[2024-11-13 06:01:58,201][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True}
[2024-11-13 06:01:58,202][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-13 06:01:58,202][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme'}
[2024-11-13 06:01:58,202][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_dual_peft', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2024-11-13_06-01-57.txt', 'log_interval': 5}
[2024-11-13 06:02:19,179][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-13 06:02:25,031][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-13 06:02:25,033][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-13 06:02:25,035][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-13 06:02:25,036][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-13 06:02:26,724][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-13 06:02:26,727][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-13 06:02:26,734][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-13 06:02:26,736][slam_llm.utils.train_utils][INFO] - --> w2v2 has 0.0 Million params

[2024-11-13 06:02:32,240][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-13 06:02:32,241][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-13 06:02:32,241][slam_llm.models.slam_model][INFO] - setup peft...
[2024-11-13 06:02:32,363][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-13 06:02:32,365][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-13 06:02:32,566][slam_llm.utils.train_utils][INFO] - --> Module dual
[2024-11-13 06:02:32,566][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2024-11-13 06:02:32,566][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-13 06:02:32,572][slam_llm.utils.train_utils][INFO] - --> asr has 30.806016 Million params

[2024-11-13 06:02:35,123][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-13 06:02:36,138][root][INFO] - --> Training Set Length = 2298
[2024-11-13 06:02:36,144][root][INFO] - --> Validation Set Length = 341
[2024-11-13 06:02:36,144][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-13 06:02:36,145][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-13 06:02:37,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:38,915][numexpr.utils][INFO] - Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-11-13 06:02:39,662][root][INFO] - Training Epoch: 1/10, step 0/574 completed (loss: 8.811226844787598, acc: 0.0)
[2024-11-13 06:02:39,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:40,110][root][INFO] - Training Epoch: 1/10, step 1/574 completed (loss: 8.449108123779297, acc: 0.0)
[2024-11-13 06:02:40,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:40,532][root][INFO] - Training Epoch: 1/10, step 2/574 completed (loss: 8.082276344299316, acc: 0.0)
[2024-11-13 06:02:40,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:40,994][root][INFO] - Training Epoch: 1/10, step 3/574 completed (loss: 7.622629165649414, acc: 0.02631578966975212)
[2024-11-13 06:02:41,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:41,748][root][INFO] - Training Epoch: 1/10, step 4/574 completed (loss: 7.386237144470215, acc: 0.0)
[2024-11-13 06:02:41,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:42,199][root][INFO] - Training Epoch: 1/10, step 5/574 completed (loss: 7.908690929412842, acc: 0.0)
[2024-11-13 06:02:42,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:42,638][root][INFO] - Training Epoch: 1/10, step 6/574 completed (loss: 8.179454803466797, acc: 0.020408162847161293)
[2024-11-13 06:02:42,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:43,078][root][INFO] - Training Epoch: 1/10, step 7/574 completed (loss: 8.39134407043457, acc: 0.0)
[2024-11-13 06:02:43,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:43,532][root][INFO] - Training Epoch: 1/10, step 8/574 completed (loss: 9.506363868713379, acc: 0.0)
[2024-11-13 06:02:43,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:44,056][root][INFO] - Training Epoch: 1/10, step 9/574 completed (loss: 7.789135932922363, acc: 0.0)
[2024-11-13 06:02:44,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:44,485][root][INFO] - Training Epoch: 1/10, step 10/574 completed (loss: 8.175960540771484, acc: 0.0)
[2024-11-13 06:02:44,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:44,894][root][INFO] - Training Epoch: 1/10, step 11/574 completed (loss: 7.299582481384277, acc: 0.0)
[2024-11-13 06:02:45,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:45,373][root][INFO] - Training Epoch: 1/10, step 12/574 completed (loss: 8.141666412353516, acc: 0.03030303120613098)
[2024-11-13 06:02:45,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:45,805][root][INFO] - Training Epoch: 1/10, step 13/574 completed (loss: 7.183655261993408, acc: 0.0)
[2024-11-13 06:02:46,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:46,233][root][INFO] - Training Epoch: 1/10, step 14/574 completed (loss: 7.522417068481445, acc: 0.0)
[2024-11-13 06:02:46,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:46,686][root][INFO] - Training Epoch: 1/10, step 15/574 completed (loss: 6.8035478591918945, acc: 0.020408162847161293)
[2024-11-13 06:02:46,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:47,105][root][INFO] - Training Epoch: 1/10, step 16/574 completed (loss: 9.049670219421387, acc: 0.0)
[2024-11-13 06:02:47,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:47,520][root][INFO] - Training Epoch: 1/10, step 17/574 completed (loss: 7.849672317504883, acc: 0.0)
[2024-11-13 06:02:47,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:47,948][root][INFO] - Training Epoch: 1/10, step 18/574 completed (loss: 6.548851013183594, acc: 0.02777777798473835)
[2024-11-13 06:02:48,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:48,382][root][INFO] - Training Epoch: 1/10, step 19/574 completed (loss: 8.004210472106934, acc: 0.05263157933950424)
[2024-11-13 06:02:48,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:48,791][root][INFO] - Training Epoch: 1/10, step 20/574 completed (loss: 7.494279861450195, acc: 0.03846153989434242)
[2024-11-13 06:02:49,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:49,254][root][INFO] - Training Epoch: 1/10, step 21/574 completed (loss: 6.818119049072266, acc: 0.0)
[2024-11-13 06:02:49,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:49,717][root][INFO] - Training Epoch: 1/10, step 22/574 completed (loss: 7.301790714263916, acc: 0.03999999910593033)
[2024-11-13 06:02:49,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:50,150][root][INFO] - Training Epoch: 1/10, step 23/574 completed (loss: 8.146017074584961, acc: 0.0)
[2024-11-13 06:02:50,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:50,596][root][INFO] - Training Epoch: 1/10, step 24/574 completed (loss: 8.409347534179688, acc: 0.0)
[2024-11-13 06:02:50,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:51,019][root][INFO] - Training Epoch: 1/10, step 25/574 completed (loss: 6.0477190017700195, acc: 0.03773584961891174)
[2024-11-13 06:02:51,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:51,455][root][INFO] - Training Epoch: 1/10, step 26/574 completed (loss: 5.497841835021973, acc: 0.054794520139694214)
[2024-11-13 06:02:52,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:53,446][root][INFO] - Training Epoch: 1/10, step 27/574 completed (loss: 4.122448921203613, acc: 0.1818181872367859)
[2024-11-13 06:02:53,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:53,829][root][INFO] - Training Epoch: 1/10, step 28/574 completed (loss: 6.3264479637146, acc: 0.023255813866853714)
[2024-11-13 06:02:54,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:54,233][root][INFO] - Training Epoch: 1/10, step 29/574 completed (loss: 5.439688205718994, acc: 0.14457830786705017)
[2024-11-13 06:02:54,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:54,670][root][INFO] - Training Epoch: 1/10, step 30/574 completed (loss: 5.230003356933594, acc: 0.1111111119389534)
[2024-11-13 06:02:54,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:55,072][root][INFO] - Training Epoch: 1/10, step 31/574 completed (loss: 6.886542797088623, acc: 0.0)
[2024-11-13 06:02:55,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:55,530][root][INFO] - Training Epoch: 1/10, step 32/574 completed (loss: 6.987488269805908, acc: 0.0)
[2024-11-13 06:02:55,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:56,080][root][INFO] - Training Epoch: 1/10, step 33/574 completed (loss: 7.8986711502075195, acc: 0.0)
[2024-11-13 06:02:56,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:56,557][root][INFO] - Training Epoch: 1/10, step 34/574 completed (loss: 4.942643165588379, acc: 0.1428571492433548)
[2024-11-13 06:02:56,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:56,982][root][INFO] - Training Epoch: 1/10, step 35/574 completed (loss: 5.148040294647217, acc: 0.16393442451953888)
[2024-11-13 06:02:57,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:57,459][root][INFO] - Training Epoch: 1/10, step 36/574 completed (loss: 4.988309383392334, acc: 0.1428571492433548)
[2024-11-13 06:02:57,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:57,879][root][INFO] - Training Epoch: 1/10, step 37/574 completed (loss: 6.125331878662109, acc: 0.0)
[2024-11-13 06:02:58,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:58,343][root][INFO] - Training Epoch: 1/10, step 38/574 completed (loss: 4.737957000732422, acc: 0.2183908075094223)
[2024-11-13 06:02:58,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:58,816][root][INFO] - Training Epoch: 1/10, step 39/574 completed (loss: 6.742552280426025, acc: 0.0)
[2024-11-13 06:02:59,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:59,247][root][INFO] - Training Epoch: 1/10, step 40/574 completed (loss: 6.197668075561523, acc: 0.0)
[2024-11-13 06:02:59,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:02:59,858][root][INFO] - Training Epoch: 1/10, step 41/574 completed (loss: 4.8356032371521, acc: 0.1756756752729416)
[2024-11-13 06:03:00,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:00,372][root][INFO] - Training Epoch: 1/10, step 42/574 completed (loss: 5.127464294433594, acc: 0.1538461595773697)
[2024-11-13 06:03:00,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:01,018][root][INFO] - Training Epoch: 1/10, step 43/574 completed (loss: 5.108147621154785, acc: 0.14141413569450378)
[2024-11-13 06:03:01,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:01,588][root][INFO] - Training Epoch: 1/10, step 44/574 completed (loss: 4.510633945465088, acc: 0.23711340129375458)
[2024-11-13 06:03:01,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:02,158][root][INFO] - Training Epoch: 1/10, step 45/574 completed (loss: 4.788696765899658, acc: 0.18382352590560913)
[2024-11-13 06:03:02,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:02,602][root][INFO] - Training Epoch: 1/10, step 46/574 completed (loss: 6.299820899963379, acc: 0.03846153989434242)
[2024-11-13 06:03:02,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:03,039][root][INFO] - Training Epoch: 1/10, step 47/574 completed (loss: 5.909210205078125, acc: 0.0)
[2024-11-13 06:03:03,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:03,473][root][INFO] - Training Epoch: 1/10, step 48/574 completed (loss: 6.304486274719238, acc: 0.0714285746216774)
[2024-11-13 06:03:03,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:03,886][root][INFO] - Training Epoch: 1/10, step 49/574 completed (loss: 5.369802474975586, acc: 0.02777777798473835)
[2024-11-13 06:03:04,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:04,336][root][INFO] - Training Epoch: 1/10, step 50/574 completed (loss: 4.652285575866699, acc: 0.24561403691768646)
[2024-11-13 06:03:04,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:04,762][root][INFO] - Training Epoch: 1/10, step 51/574 completed (loss: 4.970456123352051, acc: 0.1269841343164444)
[2024-11-13 06:03:04,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:05,228][root][INFO] - Training Epoch: 1/10, step 52/574 completed (loss: 5.093414306640625, acc: 0.15492957830429077)
[2024-11-13 06:03:05,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:05,911][root][INFO] - Training Epoch: 1/10, step 53/574 completed (loss: 4.404682636260986, acc: 0.2266666740179062)
[2024-11-13 06:03:06,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:06,426][root][INFO] - Training Epoch: 1/10, step 54/574 completed (loss: 5.34060525894165, acc: 0.13513512909412384)
[2024-11-13 06:03:06,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:06,816][root][INFO] - Training Epoch: 1/10, step 55/574 completed (loss: 6.019804000854492, acc: 0.0)
[2024-11-13 06:03:10,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:11,695][root][INFO] - Training Epoch: 1/10, step 56/574 completed (loss: 3.121422529220581, acc: 0.3788395822048187)
[2024-11-13 06:03:12,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:13,659][root][INFO] - Training Epoch: 1/10, step 57/574 completed (loss: 3.67775297164917, acc: 0.27886709570884705)
[2024-11-13 06:03:14,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:14,694][root][INFO] - Training Epoch: 1/10, step 58/574 completed (loss: 3.951601982116699, acc: 0.2954545319080353)
[2024-11-13 06:03:15,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:15,590][root][INFO] - Training Epoch: 1/10, step 59/574 completed (loss: 4.2104926109313965, acc: 0.24264705181121826)
[2024-11-13 06:03:16,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:16,482][root][INFO] - Training Epoch: 1/10, step 60/574 completed (loss: 3.952141284942627, acc: 0.21014492213726044)
[2024-11-13 06:03:16,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:17,101][root][INFO] - Training Epoch: 1/10, step 61/574 completed (loss: 4.126835823059082, acc: 0.3125)
[2024-11-13 06:03:17,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:17,587][root][INFO] - Training Epoch: 1/10, step 62/574 completed (loss: 4.93164587020874, acc: 0.029411764815449715)
[2024-11-13 06:03:17,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:18,149][root][INFO] - Training Epoch: 1/10, step 63/574 completed (loss: 4.684291362762451, acc: 0.1111111119389534)
[2024-11-13 06:03:18,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:18,661][root][INFO] - Training Epoch: 1/10, step 64/574 completed (loss: 3.855813503265381, acc: 0.21875)
[2024-11-13 06:03:18,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:19,082][root][INFO] - Training Epoch: 1/10, step 65/574 completed (loss: 3.8021199703216553, acc: 0.24137930572032928)
[2024-11-13 06:03:19,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:19,517][root][INFO] - Training Epoch: 1/10, step 66/574 completed (loss: 3.9861342906951904, acc: 0.2321428507566452)
[2024-11-13 06:03:19,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:19,971][root][INFO] - Training Epoch: 1/10, step 67/574 completed (loss: 4.027034759521484, acc: 0.13333334028720856)
[2024-11-13 06:03:20,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:20,364][root][INFO] - Training Epoch: 1/10, step 68/574 completed (loss: 4.885772705078125, acc: 0.07999999821186066)
[2024-11-13 06:03:20,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:20,837][root][INFO] - Training Epoch: 1/10, step 69/574 completed (loss: 4.8697004318237305, acc: 0.0833333358168602)
[2024-11-13 06:03:21,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:21,309][root][INFO] - Training Epoch: 1/10, step 70/574 completed (loss: 5.044125080108643, acc: 0.12121212482452393)
[2024-11-13 06:03:21,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:21,774][root][INFO] - Training Epoch: 1/10, step 71/574 completed (loss: 4.073944568634033, acc: 0.22058823704719543)
[2024-11-13 06:03:21,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:22,198][root][INFO] - Training Epoch: 1/10, step 72/574 completed (loss: 3.56555438041687, acc: 0.2380952388048172)
[2024-11-13 06:03:22,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:22,617][root][INFO] - Training Epoch: 1/10, step 73/574 completed (loss: 4.0046916007995605, acc: 0.22564102709293365)
[2024-11-13 06:03:22,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:22,990][root][INFO] - Training Epoch: 1/10, step 74/574 completed (loss: 4.9459333419799805, acc: 0.10204081982374191)
[2024-11-13 06:03:23,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:23,440][root][INFO] - Training Epoch: 1/10, step 75/574 completed (loss: 4.398948669433594, acc: 0.11194030195474625)
[2024-11-13 06:03:23,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:23,989][root][INFO] - Training Epoch: 1/10, step 76/574 completed (loss: 3.767613649368286, acc: 0.2737226188182831)
[2024-11-13 06:03:24,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:24,431][root][INFO] - Training Epoch: 1/10, step 77/574 completed (loss: 5.140418529510498, acc: 0.1428571492433548)
[2024-11-13 06:03:24,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:24,871][root][INFO] - Training Epoch: 1/10, step 78/574 completed (loss: 4.91889762878418, acc: 0.125)
[2024-11-13 06:03:25,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:25,309][root][INFO] - Training Epoch: 1/10, step 79/574 completed (loss: 4.1672186851501465, acc: 0.12121212482452393)
[2024-11-13 06:03:25,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:25,771][root][INFO] - Training Epoch: 1/10, step 80/574 completed (loss: 4.438751697540283, acc: 0.07692307978868484)
[2024-11-13 06:03:26,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:26,240][root][INFO] - Training Epoch: 1/10, step 81/574 completed (loss: 4.391235828399658, acc: 0.11538461595773697)
[2024-11-13 06:03:26,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:26,696][root][INFO] - Training Epoch: 1/10, step 82/574 completed (loss: 4.230152130126953, acc: 0.19230769574642181)
[2024-11-13 06:03:26,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:27,162][root][INFO] - Training Epoch: 1/10, step 83/574 completed (loss: 3.4824347496032715, acc: 0.21875)
[2024-11-13 06:03:27,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:27,571][root][INFO] - Training Epoch: 1/10, step 84/574 completed (loss: 3.9876327514648438, acc: 0.23188406229019165)
[2024-11-13 06:03:27,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:27,993][root][INFO] - Training Epoch: 1/10, step 85/574 completed (loss: 3.630939245223999, acc: 0.20000000298023224)
[2024-11-13 06:03:28,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:28,431][root][INFO] - Training Epoch: 1/10, step 86/574 completed (loss: 4.3192033767700195, acc: 0.21739129722118378)
[2024-11-13 06:03:28,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:29,170][root][INFO] - Training Epoch: 1/10, step 87/574 completed (loss: 4.209620475769043, acc: 0.2199999988079071)
[2024-11-13 06:03:29,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:29,629][root][INFO] - Training Epoch: 1/10, step 88/574 completed (loss: 3.2945799827575684, acc: 0.28155338764190674)
[2024-11-13 06:03:30,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:31,489][root][INFO] - Training Epoch: 1/10, step 89/574 completed (loss: 3.4388716220855713, acc: 0.3349514603614807)
[2024-11-13 06:03:32,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:32,797][root][INFO] - Training Epoch: 1/10, step 90/574 completed (loss: 3.614513874053955, acc: 0.2849462330341339)
[2024-11-13 06:03:33,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:34,060][root][INFO] - Training Epoch: 1/10, step 91/574 completed (loss: 3.2245922088623047, acc: 0.3836206793785095)
[2024-11-13 06:03:34,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:35,236][root][INFO] - Training Epoch: 1/10, step 92/574 completed (loss: 3.5595970153808594, acc: 0.31578946113586426)
[2024-11-13 06:03:36,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:36,846][root][INFO] - Training Epoch: 1/10, step 93/574 completed (loss: 3.8488214015960693, acc: 0.1881188154220581)
[2024-11-13 06:03:37,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:37,299][root][INFO] - Training Epoch: 1/10, step 94/574 completed (loss: 3.892112970352173, acc: 0.19354838132858276)
[2024-11-13 06:03:37,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:37,756][root][INFO] - Training Epoch: 1/10, step 95/574 completed (loss: 4.0727996826171875, acc: 0.17391304671764374)
[2024-11-13 06:03:38,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:38,281][root][INFO] - Training Epoch: 1/10, step 96/574 completed (loss: 3.954690933227539, acc: 0.18487395346164703)
[2024-11-13 06:03:38,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:38,762][root][INFO] - Training Epoch: 1/10, step 97/574 completed (loss: 3.576787233352661, acc: 0.20192307233810425)
[2024-11-13 06:03:39,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:39,324][root][INFO] - Training Epoch: 1/10, step 98/574 completed (loss: 3.7361297607421875, acc: 0.21897810697555542)
[2024-11-13 06:03:39,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:39,764][root][INFO] - Training Epoch: 1/10, step 99/574 completed (loss: 4.05220890045166, acc: 0.17910447716712952)
[2024-11-13 06:03:40,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:40,219][root][INFO] - Training Epoch: 1/10, step 100/574 completed (loss: 4.488713264465332, acc: 0.05000000074505806)
[2024-11-13 06:03:40,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:40,722][root][INFO] - Training Epoch: 1/10, step 101/574 completed (loss: 3.4758238792419434, acc: 0.22727273404598236)
[2024-11-13 06:03:40,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:41,198][root][INFO] - Training Epoch: 1/10, step 102/574 completed (loss: 3.0399329662323, acc: 0.21739129722118378)
[2024-11-13 06:03:41,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:41,653][root][INFO] - Training Epoch: 1/10, step 103/574 completed (loss: 3.3172860145568848, acc: 0.1818181872367859)
[2024-11-13 06:03:41,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:42,111][root][INFO] - Training Epoch: 1/10, step 104/574 completed (loss: 3.5987749099731445, acc: 0.13793103396892548)
[2024-11-13 06:03:42,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:42,542][root][INFO] - Training Epoch: 1/10, step 105/574 completed (loss: 3.737295389175415, acc: 0.13953489065170288)
[2024-11-13 06:03:42,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:42,984][root][INFO] - Training Epoch: 1/10, step 106/574 completed (loss: 3.041149854660034, acc: 0.2800000011920929)
[2024-11-13 06:03:43,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:43,409][root][INFO] - Training Epoch: 1/10, step 107/574 completed (loss: 3.759768009185791, acc: 0.1764705926179886)
[2024-11-13 06:03:43,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:43,864][root][INFO] - Training Epoch: 1/10, step 108/574 completed (loss: 3.4725379943847656, acc: 0.23076923191547394)
[2024-11-13 06:03:44,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:44,339][root][INFO] - Training Epoch: 1/10, step 109/574 completed (loss: 3.657033681869507, acc: 0.190476194024086)
[2024-11-13 06:03:44,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:44,829][root][INFO] - Training Epoch: 1/10, step 110/574 completed (loss: 3.7584478855133057, acc: 0.3384615480899811)
[2024-11-13 06:03:45,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:45,427][root][INFO] - Training Epoch: 1/10, step 111/574 completed (loss: 3.713848114013672, acc: 0.21052631735801697)
[2024-11-13 06:03:45,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:45,912][root][INFO] - Training Epoch: 1/10, step 112/574 completed (loss: 3.267192840576172, acc: 0.3333333432674408)
[2024-11-13 06:03:46,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:46,319][root][INFO] - Training Epoch: 1/10, step 113/574 completed (loss: 3.8285863399505615, acc: 0.3076923191547394)
[2024-11-13 06:03:46,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:46,818][root][INFO] - Training Epoch: 1/10, step 114/574 completed (loss: 3.300170660018921, acc: 0.3265306055545807)
[2024-11-13 06:03:47,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:47,275][root][INFO] - Training Epoch: 1/10, step 115/574 completed (loss: 3.3283121585845947, acc: 0.22727273404598236)
[2024-11-13 06:03:47,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:47,739][root][INFO] - Training Epoch: 1/10, step 116/574 completed (loss: 3.0344655513763428, acc: 0.2857142984867096)
[2024-11-13 06:03:47,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:48,196][root][INFO] - Training Epoch: 1/10, step 117/574 completed (loss: 3.1397769451141357, acc: 0.3333333432674408)
[2024-11-13 06:03:48,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:48,641][root][INFO] - Training Epoch: 1/10, step 118/574 completed (loss: 3.1871025562286377, acc: 0.4032258093357086)
[2024-11-13 06:03:49,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:49,985][root][INFO] - Training Epoch: 1/10, step 119/574 completed (loss: 2.9551520347595215, acc: 0.35361215472221375)
[2024-11-13 06:03:50,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:50,430][root][INFO] - Training Epoch: 1/10, step 120/574 completed (loss: 3.0284485816955566, acc: 0.3866666555404663)
[2024-11-13 06:03:50,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:51,036][root][INFO] - Training Epoch: 1/10, step 121/574 completed (loss: 3.489457130432129, acc: 0.2884615361690521)
[2024-11-13 06:03:51,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:51,429][root][INFO] - Training Epoch: 1/10, step 122/574 completed (loss: 3.9039018154144287, acc: 0.0833333358168602)
[2024-11-13 06:03:51,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:51,820][root][INFO] - Training Epoch: 1/10, step 123/574 completed (loss: 2.9923512935638428, acc: 0.21052631735801697)
[2024-11-13 06:03:52,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:52,286][root][INFO] - Training Epoch: 1/10, step 124/574 completed (loss: 3.355330228805542, acc: 0.24539877474308014)
[2024-11-13 06:03:52,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:52,813][root][INFO] - Training Epoch: 1/10, step 125/574 completed (loss: 2.6722252368927, acc: 0.4097222089767456)
[2024-11-13 06:03:53,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:53,327][root][INFO] - Training Epoch: 1/10, step 126/574 completed (loss: 3.1395528316497803, acc: 0.20000000298023224)
[2024-11-13 06:03:53,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:53,779][root][INFO] - Training Epoch: 1/10, step 127/574 completed (loss: 3.0510365962982178, acc: 0.2261904776096344)
[2024-11-13 06:03:53,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:54,213][root][INFO] - Training Epoch: 1/10, step 128/574 completed (loss: 3.1933705806732178, acc: 0.2974359095096588)
[2024-11-13 06:03:54,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:54,798][root][INFO] - Training Epoch: 1/10, step 129/574 completed (loss: 2.632866621017456, acc: 0.3970588147640228)
[2024-11-13 06:03:54,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:55,201][root][INFO] - Training Epoch: 1/10, step 130/574 completed (loss: 3.478053569793701, acc: 0.19230769574642181)
[2024-11-13 06:03:55,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:55,649][root][INFO] - Training Epoch: 1/10, step 131/574 completed (loss: 2.8498737812042236, acc: 0.30434781312942505)
[2024-11-13 06:03:55,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:56,083][root][INFO] - Training Epoch: 1/10, step 132/574 completed (loss: 3.619281768798828, acc: 0.1875)
[2024-11-13 06:03:56,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:56,615][root][INFO] - Training Epoch: 1/10, step 133/574 completed (loss: 2.6338117122650146, acc: 0.3478260934352875)
[2024-11-13 06:03:56,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:57,055][root][INFO] - Training Epoch: 1/10, step 134/574 completed (loss: 2.832475423812866, acc: 0.17142857611179352)
[2024-11-13 06:03:57,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:57,471][root][INFO] - Training Epoch: 1/10, step 135/574 completed (loss: 2.7189979553222656, acc: 0.3076923191547394)
[2024-11-13 06:03:57,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:57,853][root][INFO] - Training Epoch: 1/10, step 136/574 completed (loss: 3.290003776550293, acc: 0.190476194024086)
[2024-11-13 06:03:58,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:58,237][root][INFO] - Training Epoch: 1/10, step 137/574 completed (loss: 2.34859037399292, acc: 0.46666666865348816)
[2024-11-13 06:03:58,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:58,651][root][INFO] - Training Epoch: 1/10, step 138/574 completed (loss: 2.3459982872009277, acc: 0.30434781312942505)
[2024-11-13 06:03:58,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:59,120][root][INFO] - Training Epoch: 1/10, step 139/574 completed (loss: 3.5332772731781006, acc: 0.2857142984867096)
[2024-11-13 06:03:59,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:59,573][root][INFO] - Training Epoch: 1/10, step 140/574 completed (loss: 3.1255903244018555, acc: 0.19230769574642181)
[2024-11-13 06:03:59,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:03:59,945][root][INFO] - Training Epoch: 1/10, step 141/574 completed (loss: 3.327813148498535, acc: 0.22580644488334656)
[2024-11-13 06:04:00,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:00,427][root][INFO] - Training Epoch: 1/10, step 142/574 completed (loss: 2.9003357887268066, acc: 0.21621622145175934)
[2024-11-13 06:04:01,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:02,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:02,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:03,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:03,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:04,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:04,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:05,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:05,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:06,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:06,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:07,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:07,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:08,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:08,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:09,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:09,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:10,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:10,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:11,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:12,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:12,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:13,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:13,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:14,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:14,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:15,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:15,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:16,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:16,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:17,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:17,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:18,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:19,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:19,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:20,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:20,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:21,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:21,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:22,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:22,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:23,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:23,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:24,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:24,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:25,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:26,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:26,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:27,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:27,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:28,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:28,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:29,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:29,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:30,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:30,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:31,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:31,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:32,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:32,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:32,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:33,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:34,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:35,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:35,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:36,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:36,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:37,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:37,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:38,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:39,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:39,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:40,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:40,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:41,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:41,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:42,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:42,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:43,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:43,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:44,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:44,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:45,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:45,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:46,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:47,198][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(20.6159, device='cuda:0') eval_epoch_loss=tensor(3.0261, device='cuda:0') eval_epoch_acc=tensor(0.2790, device='cuda:0')
[2024-11-13 06:04:47,200][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:04:47,200][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:04:47,658][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_1_step_143_loss_3.0260632038116455/model.pt
[2024-11-13 06:04:47,662][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:04:47,663][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 3.0260632038116455
[2024-11-13 06:04:47,663][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.27903246879577637
[2024-11-13 06:04:48,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:48,522][root][INFO] - Training Epoch: 1/10, step 143/574 completed (loss: 2.4665353298187256, acc: 0.41228070855140686)
[2024-11-13 06:04:48,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:49,015][root][INFO] - Training Epoch: 1/10, step 144/574 completed (loss: 2.316765546798706, acc: 0.44029849767684937)
[2024-11-13 06:04:49,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:49,529][root][INFO] - Training Epoch: 1/10, step 145/574 completed (loss: 2.8668832778930664, acc: 0.2857142984867096)
[2024-11-13 06:04:49,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:50,218][root][INFO] - Training Epoch: 1/10, step 146/574 completed (loss: 2.679302215576172, acc: 0.2978723347187042)
[2024-11-13 06:04:50,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:50,612][root][INFO] - Training Epoch: 1/10, step 147/574 completed (loss: 2.4014928340911865, acc: 0.4285714328289032)
[2024-11-13 06:04:50,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:51,148][root][INFO] - Training Epoch: 1/10, step 148/574 completed (loss: 3.3217928409576416, acc: 0.2142857164144516)
[2024-11-13 06:04:51,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:51,586][root][INFO] - Training Epoch: 1/10, step 149/574 completed (loss: 2.467597484588623, acc: 0.3478260934352875)
[2024-11-13 06:04:51,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:51,992][root][INFO] - Training Epoch: 1/10, step 150/574 completed (loss: 3.026014804840088, acc: 0.24137930572032928)
[2024-11-13 06:04:52,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:52,466][root][INFO] - Training Epoch: 1/10, step 151/574 completed (loss: 2.6400399208068848, acc: 0.30434781312942505)
[2024-11-13 06:04:52,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:52,920][root][INFO] - Training Epoch: 1/10, step 152/574 completed (loss: 2.6050381660461426, acc: 0.2711864411830902)
[2024-11-13 06:04:53,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:53,374][root][INFO] - Training Epoch: 1/10, step 153/574 completed (loss: 2.9753434658050537, acc: 0.3333333432674408)
[2024-11-13 06:04:53,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:53,792][root][INFO] - Training Epoch: 1/10, step 154/574 completed (loss: 2.7131567001342773, acc: 0.31081080436706543)
[2024-11-13 06:04:54,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:54,251][root][INFO] - Training Epoch: 1/10, step 155/574 completed (loss: 2.7272517681121826, acc: 0.4642857015132904)
[2024-11-13 06:04:54,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:54,707][root][INFO] - Training Epoch: 1/10, step 156/574 completed (loss: 2.180757522583008, acc: 0.43478259444236755)
[2024-11-13 06:04:54,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:55,141][root][INFO] - Training Epoch: 1/10, step 157/574 completed (loss: 2.708214521408081, acc: 0.2631579041481018)
[2024-11-13 06:04:57,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:58,074][root][INFO] - Training Epoch: 1/10, step 158/574 completed (loss: 3.1333024501800537, acc: 0.3243243098258972)
[2024-11-13 06:04:58,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:58,530][root][INFO] - Training Epoch: 1/10, step 159/574 completed (loss: 2.856304407119751, acc: 0.31481480598449707)
[2024-11-13 06:04:58,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:04:59,186][root][INFO] - Training Epoch: 1/10, step 160/574 completed (loss: 2.8837602138519287, acc: 0.3604651093482971)
[2024-11-13 06:04:59,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:00,145][root][INFO] - Training Epoch: 1/10, step 161/574 completed (loss: 2.697282552719116, acc: 0.38823530077934265)
[2024-11-13 06:05:00,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:01,027][root][INFO] - Training Epoch: 1/10, step 162/574 completed (loss: 2.9281039237976074, acc: 0.30337077379226685)
[2024-11-13 06:05:01,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:01,525][root][INFO] - Training Epoch: 1/10, step 163/574 completed (loss: 2.548623561859131, acc: 0.4545454680919647)
[2024-11-13 06:05:01,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:01,959][root][INFO] - Training Epoch: 1/10, step 164/574 completed (loss: 2.592463254928589, acc: 0.3333333432674408)
[2024-11-13 06:05:02,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:02,378][root][INFO] - Training Epoch: 1/10, step 165/574 completed (loss: 3.174147844314575, acc: 0.24137930572032928)
[2024-11-13 06:05:02,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:02,813][root][INFO] - Training Epoch: 1/10, step 166/574 completed (loss: 2.1163506507873535, acc: 0.4897959232330322)
[2024-11-13 06:05:03,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:03,250][root][INFO] - Training Epoch: 1/10, step 167/574 completed (loss: 2.6265649795532227, acc: 0.30000001192092896)
[2024-11-13 06:05:03,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:03,854][root][INFO] - Training Epoch: 1/10, step 168/574 completed (loss: 2.6075055599212646, acc: 0.4027777910232544)
[2024-11-13 06:05:04,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:04,313][root][INFO] - Training Epoch: 1/10, step 169/574 completed (loss: 2.56947922706604, acc: 0.29411765933036804)
[2024-11-13 06:05:05,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:06,085][root][INFO] - Training Epoch: 1/10, step 170/574 completed (loss: 3.110450267791748, acc: 0.3493150770664215)
[2024-11-13 06:05:06,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:06,534][root][INFO] - Training Epoch: 1/10, step 171/574 completed (loss: 2.373488187789917, acc: 0.4166666567325592)
[2024-11-13 06:05:06,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:06,974][root][INFO] - Training Epoch: 1/10, step 172/574 completed (loss: 2.9134914875030518, acc: 0.18518517911434174)
[2024-11-13 06:05:07,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:07,450][root][INFO] - Training Epoch: 1/10, step 173/574 completed (loss: 2.7648985385894775, acc: 0.2857142984867096)
[2024-11-13 06:05:07,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:08,301][root][INFO] - Training Epoch: 1/10, step 174/574 completed (loss: 2.2872567176818848, acc: 0.44247788190841675)
[2024-11-13 06:05:08,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:08,701][root][INFO] - Training Epoch: 1/10, step 175/574 completed (loss: 2.7212460041046143, acc: 0.3188405930995941)
[2024-11-13 06:05:08,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:09,192][root][INFO] - Training Epoch: 1/10, step 176/574 completed (loss: 2.788417100906372, acc: 0.27272728085517883)
[2024-11-13 06:05:10,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:10,711][root][INFO] - Training Epoch: 1/10, step 177/574 completed (loss: 2.8710522651672363, acc: 0.30534350872039795)
[2024-11-13 06:05:11,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:11,795][root][INFO] - Training Epoch: 1/10, step 178/574 completed (loss: 3.0310299396514893, acc: 0.22962963581085205)
[2024-11-13 06:05:12,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:12,273][root][INFO] - Training Epoch: 1/10, step 179/574 completed (loss: 2.6213717460632324, acc: 0.32786884903907776)
[2024-11-13 06:05:12,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:12,774][root][INFO] - Training Epoch: 1/10, step 180/574 completed (loss: 2.5232760906219482, acc: 0.3333333432674408)
[2024-11-13 06:05:13,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:13,260][root][INFO] - Training Epoch: 1/10, step 181/574 completed (loss: 2.3158321380615234, acc: 0.4000000059604645)
[2024-11-13 06:05:13,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:13,747][root][INFO] - Training Epoch: 1/10, step 182/574 completed (loss: 2.918539524078369, acc: 0.2142857164144516)
[2024-11-13 06:05:13,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:14,191][root][INFO] - Training Epoch: 1/10, step 183/574 completed (loss: 2.9571456909179688, acc: 0.26829269528388977)
[2024-11-13 06:05:14,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:14,695][root][INFO] - Training Epoch: 1/10, step 184/574 completed (loss: 2.948925495147705, acc: 0.3111782371997833)
[2024-11-13 06:05:14,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:15,136][root][INFO] - Training Epoch: 1/10, step 185/574 completed (loss: 2.9853856563568115, acc: 0.24783861637115479)
[2024-11-13 06:05:15,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:15,879][root][INFO] - Training Epoch: 1/10, step 186/574 completed (loss: 2.99778413772583, acc: 0.2562499940395355)
[2024-11-13 06:05:16,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:16,677][root][INFO] - Training Epoch: 1/10, step 187/574 completed (loss: 2.7071332931518555, acc: 0.31707316637039185)
[2024-11-13 06:05:16,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:17,253][root][INFO] - Training Epoch: 1/10, step 188/574 completed (loss: 2.7128124237060547, acc: 0.32384341955184937)
[2024-11-13 06:05:17,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:17,763][root][INFO] - Training Epoch: 1/10, step 189/574 completed (loss: 3.053539752960205, acc: 0.2800000011920929)
[2024-11-13 06:05:18,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:18,690][root][INFO] - Training Epoch: 1/10, step 190/574 completed (loss: 2.692331552505493, acc: 0.3139534890651703)
[2024-11-13 06:05:19,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:20,038][root][INFO] - Training Epoch: 1/10, step 191/574 completed (loss: 2.722839117050171, acc: 0.341269850730896)
[2024-11-13 06:05:20,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:21,556][root][INFO] - Training Epoch: 1/10, step 192/574 completed (loss: 2.642610549926758, acc: 0.34090909361839294)
[2024-11-13 06:05:22,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:22,848][root][INFO] - Training Epoch: 1/10, step 193/574 completed (loss: 2.2878780364990234, acc: 0.47058823704719543)
[2024-11-13 06:05:23,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:24,690][root][INFO] - Training Epoch: 1/10, step 194/574 completed (loss: 2.300326347351074, acc: 0.40123456716537476)
[2024-11-13 06:05:25,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:26,299][root][INFO] - Training Epoch: 1/10, step 195/574 completed (loss: 2.372004270553589, acc: 0.4193548262119293)
[2024-11-13 06:05:26,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:26,740][root][INFO] - Training Epoch: 1/10, step 196/574 completed (loss: 2.3180325031280518, acc: 0.4285714328289032)
[2024-11-13 06:05:26,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:27,239][root][INFO] - Training Epoch: 1/10, step 197/574 completed (loss: 2.819464683532715, acc: 0.20000000298023224)
[2024-11-13 06:05:27,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:27,695][root][INFO] - Training Epoch: 1/10, step 198/574 completed (loss: 3.154909372329712, acc: 0.27941176295280457)
[2024-11-13 06:05:27,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:28,147][root][INFO] - Training Epoch: 1/10, step 199/574 completed (loss: 2.529974937438965, acc: 0.3897058963775635)
[2024-11-13 06:05:28,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:28,685][root][INFO] - Training Epoch: 1/10, step 200/574 completed (loss: 2.796048164367676, acc: 0.31355932354927063)
[2024-11-13 06:05:28,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:29,173][root][INFO] - Training Epoch: 1/10, step 201/574 completed (loss: 2.6731982231140137, acc: 0.35820895433425903)
[2024-11-13 06:05:29,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:29,677][root][INFO] - Training Epoch: 1/10, step 202/574 completed (loss: 2.8420469760894775, acc: 0.3009708821773529)
[2024-11-13 06:05:29,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:30,143][root][INFO] - Training Epoch: 1/10, step 203/574 completed (loss: 2.7550790309906006, acc: 0.3174603283405304)
[2024-11-13 06:05:30,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:30,627][root][INFO] - Training Epoch: 1/10, step 204/574 completed (loss: 2.650880813598633, acc: 0.2967033088207245)
[2024-11-13 06:05:30,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:31,087][root][INFO] - Training Epoch: 1/10, step 205/574 completed (loss: 2.6947760581970215, acc: 0.31838566064834595)
[2024-11-13 06:05:31,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:31,671][root][INFO] - Training Epoch: 1/10, step 206/574 completed (loss: 2.598328113555908, acc: 0.3937007784843445)
[2024-11-13 06:05:31,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:32,189][root][INFO] - Training Epoch: 1/10, step 207/574 completed (loss: 2.6238853931427, acc: 0.3448275923728943)
[2024-11-13 06:05:32,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:32,690][root][INFO] - Training Epoch: 1/10, step 208/574 completed (loss: 2.4144396781921387, acc: 0.4275362193584442)
[2024-11-13 06:05:32,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:33,203][root][INFO] - Training Epoch: 1/10, step 209/574 completed (loss: 2.772280693054199, acc: 0.287937730550766)
[2024-11-13 06:05:33,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:33,695][root][INFO] - Training Epoch: 1/10, step 210/574 completed (loss: 2.8073487281799316, acc: 0.260869562625885)
[2024-11-13 06:05:33,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:34,149][root][INFO] - Training Epoch: 1/10, step 211/574 completed (loss: 2.716428756713867, acc: 0.3478260934352875)
[2024-11-13 06:05:34,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:34,568][root][INFO] - Training Epoch: 1/10, step 212/574 completed (loss: 2.824492931365967, acc: 0.1428571492433548)
[2024-11-13 06:05:34,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:34,998][root][INFO] - Training Epoch: 1/10, step 213/574 completed (loss: 2.473198652267456, acc: 0.27659574151039124)
[2024-11-13 06:05:35,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:36,324][root][INFO] - Training Epoch: 1/10, step 214/574 completed (loss: 2.618030309677124, acc: 0.32307693362236023)
[2024-11-13 06:05:36,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:36,849][root][INFO] - Training Epoch: 1/10, step 215/574 completed (loss: 2.456521511077881, acc: 0.3243243098258972)
[2024-11-13 06:05:37,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:37,380][root][INFO] - Training Epoch: 1/10, step 216/574 completed (loss: 2.589839458465576, acc: 0.3604651093482971)
[2024-11-13 06:05:37,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:38,257][root][INFO] - Training Epoch: 1/10, step 217/574 completed (loss: 2.5154595375061035, acc: 0.36936935782432556)
[2024-11-13 06:05:38,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:38,828][root][INFO] - Training Epoch: 1/10, step 218/574 completed (loss: 2.5154716968536377, acc: 0.36666667461395264)
[2024-11-13 06:05:39,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:39,189][root][INFO] - Training Epoch: 1/10, step 219/574 completed (loss: 2.1526708602905273, acc: 0.4545454680919647)
[2024-11-13 06:05:39,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:39,485][root][INFO] - Training Epoch: 1/10, step 220/574 completed (loss: 1.9947130680084229, acc: 0.37037035822868347)
[2024-11-13 06:05:39,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:39,861][root][INFO] - Training Epoch: 1/10, step 221/574 completed (loss: 2.034554958343506, acc: 0.3199999928474426)
[2024-11-13 06:05:40,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:40,321][root][INFO] - Training Epoch: 1/10, step 222/574 completed (loss: 2.612558603286743, acc: 0.2884615361690521)
[2024-11-13 06:05:41,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:41,637][root][INFO] - Training Epoch: 1/10, step 223/574 completed (loss: 2.304647445678711, acc: 0.42934781312942505)
[2024-11-13 06:05:42,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:42,530][root][INFO] - Training Epoch: 1/10, step 224/574 completed (loss: 2.5868732929229736, acc: 0.3636363744735718)
[2024-11-13 06:05:42,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:43,220][root][INFO] - Training Epoch: 1/10, step 225/574 completed (loss: 2.7918853759765625, acc: 0.27659574151039124)
[2024-11-13 06:05:43,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:43,713][root][INFO] - Training Epoch: 1/10, step 226/574 completed (loss: 3.031878709793091, acc: 0.2830188572406769)
[2024-11-13 06:05:43,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:44,210][root][INFO] - Training Epoch: 1/10, step 227/574 completed (loss: 2.502471446990967, acc: 0.36666667461395264)
[2024-11-13 06:05:44,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:44,634][root][INFO] - Training Epoch: 1/10, step 228/574 completed (loss: 2.0943124294281006, acc: 0.44186046719551086)
[2024-11-13 06:05:44,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:45,076][root][INFO] - Training Epoch: 1/10, step 229/574 completed (loss: 2.2925684452056885, acc: 0.2666666805744171)
[2024-11-13 06:05:45,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:45,648][root][INFO] - Training Epoch: 1/10, step 230/574 completed (loss: 2.8679909706115723, acc: 0.2526315748691559)
[2024-11-13 06:05:45,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:46,160][root][INFO] - Training Epoch: 1/10, step 231/574 completed (loss: 2.436180353164673, acc: 0.3444444537162781)
[2024-11-13 06:05:46,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:46,860][root][INFO] - Training Epoch: 1/10, step 232/574 completed (loss: 2.2047431468963623, acc: 0.5)
[2024-11-13 06:05:47,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:47,640][root][INFO] - Training Epoch: 1/10, step 233/574 completed (loss: 2.255958080291748, acc: 0.46330276131629944)
[2024-11-13 06:05:48,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:48,358][root][INFO] - Training Epoch: 1/10, step 234/574 completed (loss: 2.2151081562042236, acc: 0.4076923131942749)
[2024-11-13 06:05:48,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:48,827][root][INFO] - Training Epoch: 1/10, step 235/574 completed (loss: 1.8811856508255005, acc: 0.4736842215061188)
[2024-11-13 06:05:49,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:49,244][root][INFO] - Training Epoch: 1/10, step 236/574 completed (loss: 2.109112501144409, acc: 0.375)
[2024-11-13 06:05:49,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:49,752][root][INFO] - Training Epoch: 1/10, step 237/574 completed (loss: 2.5534327030181885, acc: 0.27272728085517883)
[2024-11-13 06:05:50,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:50,254][root][INFO] - Training Epoch: 1/10, step 238/574 completed (loss: 1.9944496154785156, acc: 0.48148149251937866)
[2024-11-13 06:05:50,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:50,698][root][INFO] - Training Epoch: 1/10, step 239/574 completed (loss: 2.2095751762390137, acc: 0.34285715222358704)
[2024-11-13 06:05:50,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:51,209][root][INFO] - Training Epoch: 1/10, step 240/574 completed (loss: 2.0034117698669434, acc: 0.47727271914482117)
[2024-11-13 06:05:51,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:51,636][root][INFO] - Training Epoch: 1/10, step 241/574 completed (loss: 2.2583608627319336, acc: 0.4545454680919647)
[2024-11-13 06:05:52,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:52,611][root][INFO] - Training Epoch: 1/10, step 242/574 completed (loss: 2.32236385345459, acc: 0.3709677457809448)
[2024-11-13 06:05:53,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:53,529][root][INFO] - Training Epoch: 1/10, step 243/574 completed (loss: 2.227066993713379, acc: 0.3863636255264282)
[2024-11-13 06:05:53,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:53,995][root][INFO] - Training Epoch: 1/10, step 244/574 completed (loss: 1.8574148416519165, acc: 0.4285714328289032)
[2024-11-13 06:05:54,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:54,410][root][INFO] - Training Epoch: 1/10, step 245/574 completed (loss: 2.362673044204712, acc: 0.3461538553237915)
[2024-11-13 06:05:54,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:54,877][root][INFO] - Training Epoch: 1/10, step 246/574 completed (loss: 2.9229607582092285, acc: 0.25806450843811035)
[2024-11-13 06:05:55,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:55,285][root][INFO] - Training Epoch: 1/10, step 247/574 completed (loss: 1.9751083850860596, acc: 0.44999998807907104)
[2024-11-13 06:05:55,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:55,796][root][INFO] - Training Epoch: 1/10, step 248/574 completed (loss: 2.4274239540100098, acc: 0.37837839126586914)
[2024-11-13 06:05:56,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:56,242][root][INFO] - Training Epoch: 1/10, step 249/574 completed (loss: 2.339752674102783, acc: 0.3513513505458832)
[2024-11-13 06:05:56,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:56,714][root][INFO] - Training Epoch: 1/10, step 250/574 completed (loss: 2.4033594131469727, acc: 0.3243243098258972)
[2024-11-13 06:05:57,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:57,228][root][INFO] - Training Epoch: 1/10, step 251/574 completed (loss: 2.5704877376556396, acc: 0.27941176295280457)
[2024-11-13 06:05:57,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:57,692][root][INFO] - Training Epoch: 1/10, step 252/574 completed (loss: 1.746601939201355, acc: 0.4878048896789551)
[2024-11-13 06:05:57,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:58,078][root][INFO] - Training Epoch: 1/10, step 253/574 completed (loss: 1.800254464149475, acc: 0.5600000023841858)
[2024-11-13 06:05:58,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:58,550][root][INFO] - Training Epoch: 1/10, step 254/574 completed (loss: 1.5468924045562744, acc: 0.5199999809265137)
[2024-11-13 06:05:58,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:59,067][root][INFO] - Training Epoch: 1/10, step 255/574 completed (loss: 2.473325729370117, acc: 0.32258063554763794)
[2024-11-13 06:05:59,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:05:59,507][root][INFO] - Training Epoch: 1/10, step 256/574 completed (loss: 2.552438259124756, acc: 0.2982456088066101)
[2024-11-13 06:05:59,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:00,058][root][INFO] - Training Epoch: 1/10, step 257/574 completed (loss: 2.538464069366455, acc: 0.3571428656578064)
[2024-11-13 06:06:00,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:00,522][root][INFO] - Training Epoch: 1/10, step 258/574 completed (loss: 2.4056954383850098, acc: 0.3815789520740509)
[2024-11-13 06:06:00,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:01,436][root][INFO] - Training Epoch: 1/10, step 259/574 completed (loss: 2.3471012115478516, acc: 0.38679245114326477)
[2024-11-13 06:06:01,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:02,409][root][INFO] - Training Epoch: 1/10, step 260/574 completed (loss: 2.441603899002075, acc: 0.4166666567325592)
[2024-11-13 06:06:02,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:02,859][root][INFO] - Training Epoch: 1/10, step 261/574 completed (loss: 2.3535358905792236, acc: 0.4722222089767456)
[2024-11-13 06:06:03,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:03,304][root][INFO] - Training Epoch: 1/10, step 262/574 completed (loss: 2.551867961883545, acc: 0.35483869910240173)
[2024-11-13 06:06:03,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:03,830][root][INFO] - Training Epoch: 1/10, step 263/574 completed (loss: 2.7303740978240967, acc: 0.30666667222976685)
[2024-11-13 06:06:04,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:04,247][root][INFO] - Training Epoch: 1/10, step 264/574 completed (loss: 2.629265308380127, acc: 0.3333333432674408)
[2024-11-13 06:06:05,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:05,672][root][INFO] - Training Epoch: 1/10, step 265/574 completed (loss: 2.7590649127960205, acc: 0.30399999022483826)
[2024-11-13 06:06:05,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:06,153][root][INFO] - Training Epoch: 1/10, step 266/574 completed (loss: 2.472628593444824, acc: 0.40449437499046326)
[2024-11-13 06:06:06,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:06,649][root][INFO] - Training Epoch: 1/10, step 267/574 completed (loss: 2.555792808532715, acc: 0.36486485600471497)
[2024-11-13 06:06:07,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:07,356][root][INFO] - Training Epoch: 1/10, step 268/574 completed (loss: 2.1447770595550537, acc: 0.37931033968925476)
[2024-11-13 06:06:07,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:07,733][root][INFO] - Training Epoch: 1/10, step 269/574 completed (loss: 2.310831308364868, acc: 0.3636363744735718)
[2024-11-13 06:06:07,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:08,247][root][INFO] - Training Epoch: 1/10, step 270/574 completed (loss: 1.9294848442077637, acc: 0.4545454680919647)
[2024-11-13 06:06:08,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:08,707][root][INFO] - Training Epoch: 1/10, step 271/574 completed (loss: 1.875966191291809, acc: 0.46875)
[2024-11-13 06:06:08,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:09,150][root][INFO] - Training Epoch: 1/10, step 272/574 completed (loss: 2.0912070274353027, acc: 0.4333333373069763)
[2024-11-13 06:06:09,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:09,736][root][INFO] - Training Epoch: 1/10, step 273/574 completed (loss: 2.532711982727051, acc: 0.3499999940395355)
[2024-11-13 06:06:09,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:10,204][root][INFO] - Training Epoch: 1/10, step 274/574 completed (loss: 2.171285390853882, acc: 0.4375)
[2024-11-13 06:06:10,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:10,651][root][INFO] - Training Epoch: 1/10, step 275/574 completed (loss: 1.7825571298599243, acc: 0.5)
[2024-11-13 06:06:10,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:11,135][root][INFO] - Training Epoch: 1/10, step 276/574 completed (loss: 2.2715446949005127, acc: 0.3103448152542114)
[2024-11-13 06:06:11,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:11,561][root][INFO] - Training Epoch: 1/10, step 277/574 completed (loss: 2.047088146209717, acc: 0.4000000059604645)
[2024-11-13 06:06:11,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:11,969][root][INFO] - Training Epoch: 1/10, step 278/574 completed (loss: 2.6808555126190186, acc: 0.3191489279270172)
[2024-11-13 06:06:12,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:12,431][root][INFO] - Training Epoch: 1/10, step 279/574 completed (loss: 2.2593023777008057, acc: 0.3958333432674408)
[2024-11-13 06:06:12,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:12,860][root][INFO] - Training Epoch: 1/10, step 280/574 completed (loss: 2.170276403427124, acc: 0.3863636255264282)
[2024-11-13 06:06:13,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:13,548][root][INFO] - Training Epoch: 1/10, step 281/574 completed (loss: 2.5805821418762207, acc: 0.3614457845687866)
[2024-11-13 06:06:13,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:14,046][root][INFO] - Training Epoch: 1/10, step 282/574 completed (loss: 2.54941463470459, acc: 0.3611111044883728)
[2024-11-13 06:06:14,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:14,545][root][INFO] - Training Epoch: 1/10, step 283/574 completed (loss: 2.5523242950439453, acc: 0.2368421107530594)
[2024-11-13 06:06:14,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:14,965][root][INFO] - Training Epoch: 1/10, step 284/574 completed (loss: 2.5457775592803955, acc: 0.23529411852359772)
[2024-11-13 06:06:15,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:15,382][root][INFO] - Training Epoch: 1/10, step 285/574 completed (loss: 2.4867920875549316, acc: 0.22499999403953552)
[2024-11-13 06:06:16,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:17,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:17,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:18,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:18,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:19,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:19,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:20,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:20,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:20,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:21,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:22,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:22,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:23,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:23,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:24,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:24,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:25,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:25,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:25,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:26,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:26,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:27,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:27,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:28,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:28,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:29,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:30,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:30,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:30,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:31,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:31,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:32,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:32,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:33,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:33,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:34,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:34,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:35,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:35,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:36,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:36,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:37,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:37,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:38,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:38,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:38,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:39,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:40,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:40,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:40,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:41,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:41,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:42,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:43,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:43,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:44,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:44,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:45,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:45,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:46,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:46,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:47,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:48,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:48,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:49,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:49,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:50,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:50,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:51,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:51,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:52,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:52,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:53,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:53,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:54,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:54,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:55,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:55,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:56,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:56,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:57,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:57,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:58,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:58,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:06:59,372][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(10.8259, device='cuda:0') eval_epoch_loss=tensor(2.3819, device='cuda:0') eval_epoch_acc=tensor(0.3858, device='cuda:0')
[2024-11-13 06:06:59,374][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:06:59,374][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:07:00,092][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_1_step_286_loss_2.3819382190704346/model.pt
[2024-11-13 06:07:00,102][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:07:00,103][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 2.3819382190704346
[2024-11-13 06:07:00,104][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.38577207922935486
[2024-11-13 06:07:00,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:00,589][root][INFO] - Training Epoch: 1/10, step 286/574 completed (loss: 2.3826234340667725, acc: 0.34375)
[2024-11-13 06:07:00,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:01,076][root][INFO] - Training Epoch: 1/10, step 287/574 completed (loss: 2.6144449710845947, acc: 0.2639999985694885)
[2024-11-13 06:07:01,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:01,609][root][INFO] - Training Epoch: 1/10, step 288/574 completed (loss: 2.2915313243865967, acc: 0.37362638115882874)
[2024-11-13 06:07:01,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:02,096][root][INFO] - Training Epoch: 1/10, step 289/574 completed (loss: 2.6104087829589844, acc: 0.27950310707092285)
[2024-11-13 06:07:02,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:02,533][root][INFO] - Training Epoch: 1/10, step 290/574 completed (loss: 2.680903196334839, acc: 0.32474225759506226)
[2024-11-13 06:07:02,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:02,903][root][INFO] - Training Epoch: 1/10, step 291/574 completed (loss: 1.8552467823028564, acc: 0.5909090638160706)
[2024-11-13 06:07:03,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:03,190][root][INFO] - Training Epoch: 1/10, step 292/574 completed (loss: 2.4083333015441895, acc: 0.3333333432674408)
[2024-11-13 06:07:03,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:03,548][root][INFO] - Training Epoch: 1/10, step 293/574 completed (loss: 2.021831512451172, acc: 0.5)
[2024-11-13 06:07:03,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:04,255][root][INFO] - Training Epoch: 1/10, step 294/574 completed (loss: 1.7125983238220215, acc: 0.581818163394928)
[2024-11-13 06:07:04,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:05,107][root][INFO] - Training Epoch: 1/10, step 295/574 completed (loss: 2.030151128768921, acc: 0.5)
[2024-11-13 06:07:05,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:05,542][root][INFO] - Training Epoch: 1/10, step 296/574 completed (loss: 2.456958532333374, acc: 0.4482758641242981)
[2024-11-13 06:07:05,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:05,948][root][INFO] - Training Epoch: 1/10, step 297/574 completed (loss: 2.0712506771087646, acc: 0.4444444477558136)
[2024-11-13 06:07:06,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:06,413][root][INFO] - Training Epoch: 1/10, step 298/574 completed (loss: 2.4381775856018066, acc: 0.3947368562221527)
[2024-11-13 06:07:06,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:06,800][root][INFO] - Training Epoch: 1/10, step 299/574 completed (loss: 2.503032922744751, acc: 0.4107142984867096)
[2024-11-13 06:07:06,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:07,154][root][INFO] - Training Epoch: 1/10, step 300/574 completed (loss: 2.345562219619751, acc: 0.34375)
[2024-11-13 06:07:07,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:07,615][root][INFO] - Training Epoch: 1/10, step 301/574 completed (loss: 2.574079990386963, acc: 0.3207547068595886)
[2024-11-13 06:07:07,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:08,038][root][INFO] - Training Epoch: 1/10, step 302/574 completed (loss: 1.8036164045333862, acc: 0.5660377144813538)
[2024-11-13 06:07:08,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:08,530][root][INFO] - Training Epoch: 1/10, step 303/574 completed (loss: 1.9723073244094849, acc: 0.529411792755127)
[2024-11-13 06:07:08,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:08,984][root][INFO] - Training Epoch: 1/10, step 304/574 completed (loss: 2.545942783355713, acc: 0.3125)
[2024-11-13 06:07:09,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:09,437][root][INFO] - Training Epoch: 1/10, step 305/574 completed (loss: 2.047584056854248, acc: 0.4754098355770111)
[2024-11-13 06:07:09,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:09,806][root][INFO] - Training Epoch: 1/10, step 306/574 completed (loss: 1.651067852973938, acc: 0.5333333611488342)
[2024-11-13 06:07:09,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:10,191][root][INFO] - Training Epoch: 1/10, step 307/574 completed (loss: 1.5061650276184082, acc: 0.5789473652839661)
[2024-11-13 06:07:10,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:10,575][root][INFO] - Training Epoch: 1/10, step 308/574 completed (loss: 2.629485845565796, acc: 0.36231884360313416)
[2024-11-13 06:07:10,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:11,192][root][INFO] - Training Epoch: 1/10, step 309/574 completed (loss: 2.284688711166382, acc: 0.4027777910232544)
[2024-11-13 06:07:11,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:11,687][root][INFO] - Training Epoch: 1/10, step 310/574 completed (loss: 2.3613197803497314, acc: 0.3132530152797699)
[2024-11-13 06:07:11,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:12,159][root][INFO] - Training Epoch: 1/10, step 311/574 completed (loss: 2.593965768814087, acc: 0.28205129504203796)
[2024-11-13 06:07:12,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:12,668][root][INFO] - Training Epoch: 1/10, step 312/574 completed (loss: 2.727297067642212, acc: 0.29591837525367737)
[2024-11-13 06:07:12,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:13,101][root][INFO] - Training Epoch: 1/10, step 313/574 completed (loss: 1.4148378372192383, acc: 0.7083333134651184)
[2024-11-13 06:07:13,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:13,519][root][INFO] - Training Epoch: 1/10, step 314/574 completed (loss: 1.8935881853103638, acc: 0.4583333432674408)
[2024-11-13 06:07:13,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:13,945][root][INFO] - Training Epoch: 1/10, step 315/574 completed (loss: 2.104058027267456, acc: 0.32258063554763794)
[2024-11-13 06:07:14,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:14,329][root][INFO] - Training Epoch: 1/10, step 316/574 completed (loss: 2.3069846630096436, acc: 0.35483869910240173)
[2024-11-13 06:07:14,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:14,805][root][INFO] - Training Epoch: 1/10, step 317/574 completed (loss: 2.0076820850372314, acc: 0.4776119291782379)
[2024-11-13 06:07:15,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:15,248][root][INFO] - Training Epoch: 1/10, step 318/574 completed (loss: 1.889051914215088, acc: 0.5)
[2024-11-13 06:07:15,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:15,711][root][INFO] - Training Epoch: 1/10, step 319/574 completed (loss: 2.7242813110351562, acc: 0.17777778208255768)
[2024-11-13 06:07:15,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:16,109][root][INFO] - Training Epoch: 1/10, step 320/574 completed (loss: 2.2265076637268066, acc: 0.35483869910240173)
[2024-11-13 06:07:16,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:16,575][root][INFO] - Training Epoch: 1/10, step 321/574 completed (loss: 1.5712106227874756, acc: 0.6200000047683716)
[2024-11-13 06:07:16,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:17,046][root][INFO] - Training Epoch: 1/10, step 322/574 completed (loss: 3.073143482208252, acc: 0.25925925374031067)
[2024-11-13 06:07:17,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:17,425][root][INFO] - Training Epoch: 1/10, step 323/574 completed (loss: 3.7764625549316406, acc: 0.02857142873108387)
[2024-11-13 06:07:17,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:17,844][root][INFO] - Training Epoch: 1/10, step 324/574 completed (loss: 2.948716878890991, acc: 0.1794871836900711)
[2024-11-13 06:07:18,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:18,253][root][INFO] - Training Epoch: 1/10, step 325/574 completed (loss: 2.7458150386810303, acc: 0.3658536672592163)
[2024-11-13 06:07:18,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:18,737][root][INFO] - Training Epoch: 1/10, step 326/574 completed (loss: 2.374917984008789, acc: 0.34210526943206787)
[2024-11-13 06:07:18,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:19,135][root][INFO] - Training Epoch: 1/10, step 327/574 completed (loss: 1.9001964330673218, acc: 0.42105263471603394)
[2024-11-13 06:07:19,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:19,538][root][INFO] - Training Epoch: 1/10, step 328/574 completed (loss: 1.9146875143051147, acc: 0.4285714328289032)
[2024-11-13 06:07:19,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:20,018][root][INFO] - Training Epoch: 1/10, step 329/574 completed (loss: 2.261134147644043, acc: 0.40740740299224854)
[2024-11-13 06:07:20,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:20,544][root][INFO] - Training Epoch: 1/10, step 330/574 completed (loss: 1.6184817552566528, acc: 0.5)
[2024-11-13 06:07:20,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:21,034][root][INFO] - Training Epoch: 1/10, step 331/574 completed (loss: 2.3947949409484863, acc: 0.3709677457809448)
[2024-11-13 06:07:21,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:21,572][root][INFO] - Training Epoch: 1/10, step 332/574 completed (loss: 1.9760719537734985, acc: 0.42105263471603394)
[2024-11-13 06:07:21,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:21,967][root][INFO] - Training Epoch: 1/10, step 333/574 completed (loss: 2.733301877975464, acc: 0.25)
[2024-11-13 06:07:22,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:22,411][root][INFO] - Training Epoch: 1/10, step 334/574 completed (loss: 2.032038927078247, acc: 0.46666666865348816)
[2024-11-13 06:07:22,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:22,853][root][INFO] - Training Epoch: 1/10, step 335/574 completed (loss: 2.2002272605895996, acc: 0.31578946113586426)
[2024-11-13 06:07:23,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:23,326][root][INFO] - Training Epoch: 1/10, step 336/574 completed (loss: 2.3441553115844727, acc: 0.3199999928474426)
[2024-11-13 06:07:23,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:23,757][root][INFO] - Training Epoch: 1/10, step 337/574 completed (loss: 2.5345616340637207, acc: 0.3333333432674408)
[2024-11-13 06:07:23,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:24,198][root][INFO] - Training Epoch: 1/10, step 338/574 completed (loss: 2.6328344345092773, acc: 0.3510638177394867)
[2024-11-13 06:07:24,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:24,612][root][INFO] - Training Epoch: 1/10, step 339/574 completed (loss: 2.657742977142334, acc: 0.3734939694404602)
[2024-11-13 06:07:24,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:25,100][root][INFO] - Training Epoch: 1/10, step 340/574 completed (loss: 1.897363543510437, acc: 0.6086956262588501)
[2024-11-13 06:07:25,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:25,534][root][INFO] - Training Epoch: 1/10, step 341/574 completed (loss: 2.7769999504089355, acc: 0.3333333432674408)
[2024-11-13 06:07:25,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:26,031][root][INFO] - Training Epoch: 1/10, step 342/574 completed (loss: 2.992609977722168, acc: 0.2409638613462448)
[2024-11-13 06:07:26,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:26,447][root][INFO] - Training Epoch: 1/10, step 343/574 completed (loss: 2.232234239578247, acc: 0.43396225571632385)
[2024-11-13 06:07:26,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:26,972][root][INFO] - Training Epoch: 1/10, step 344/574 completed (loss: 2.655507802963257, acc: 0.3037974536418915)
[2024-11-13 06:07:27,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:27,401][root][INFO] - Training Epoch: 1/10, step 345/574 completed (loss: 2.3461201190948486, acc: 0.37254902720451355)
[2024-11-13 06:07:27,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:27,846][root][INFO] - Training Epoch: 1/10, step 346/574 completed (loss: 2.7625863552093506, acc: 0.3283582031726837)
[2024-11-13 06:07:28,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:28,337][root][INFO] - Training Epoch: 1/10, step 347/574 completed (loss: 1.8466222286224365, acc: 0.6000000238418579)
[2024-11-13 06:07:28,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:28,718][root][INFO] - Training Epoch: 1/10, step 348/574 completed (loss: 1.8802919387817383, acc: 0.5199999809265137)
[2024-11-13 06:07:29,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:29,342][root][INFO] - Training Epoch: 1/10, step 349/574 completed (loss: 1.7300019264221191, acc: 0.6111111044883728)
[2024-11-13 06:07:29,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:29,768][root][INFO] - Training Epoch: 1/10, step 350/574 completed (loss: 2.3833415508270264, acc: 0.3720930218696594)
[2024-11-13 06:07:29,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:30,186][root][INFO] - Training Epoch: 1/10, step 351/574 completed (loss: 2.2497849464416504, acc: 0.38461539149284363)
[2024-11-13 06:07:30,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:30,704][root][INFO] - Training Epoch: 1/10, step 352/574 completed (loss: 2.245579719543457, acc: 0.3777777850627899)
[2024-11-13 06:07:30,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:31,158][root][INFO] - Training Epoch: 1/10, step 353/574 completed (loss: 1.5635148286819458, acc: 0.43478259444236755)
[2024-11-13 06:07:31,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:31,689][root][INFO] - Training Epoch: 1/10, step 354/574 completed (loss: 2.90371036529541, acc: 0.3076923191547394)
[2024-11-13 06:07:31,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:32,162][root][INFO] - Training Epoch: 1/10, step 355/574 completed (loss: 2.726576328277588, acc: 0.3186813294887543)
[2024-11-13 06:07:32,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:32,938][root][INFO] - Training Epoch: 1/10, step 356/574 completed (loss: 2.2377309799194336, acc: 0.3913043439388275)
[2024-11-13 06:07:33,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:33,404][root][INFO] - Training Epoch: 1/10, step 357/574 completed (loss: 2.472745418548584, acc: 0.29347825050354004)
[2024-11-13 06:07:33,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:33,867][root][INFO] - Training Epoch: 1/10, step 358/574 completed (loss: 2.4304757118225098, acc: 0.26530611515045166)
[2024-11-13 06:07:34,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:34,334][root][INFO] - Training Epoch: 1/10, step 359/574 completed (loss: 1.0660933256149292, acc: 0.7083333134651184)
[2024-11-13 06:07:34,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:34,770][root][INFO] - Training Epoch: 1/10, step 360/574 completed (loss: 1.9957382678985596, acc: 0.3461538553237915)
[2024-11-13 06:07:35,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:35,250][root][INFO] - Training Epoch: 1/10, step 361/574 completed (loss: 2.28031063079834, acc: 0.3658536672592163)
[2024-11-13 06:07:35,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:35,722][root][INFO] - Training Epoch: 1/10, step 362/574 completed (loss: 1.9626052379608154, acc: 0.46666666865348816)
[2024-11-13 06:07:35,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:36,157][root][INFO] - Training Epoch: 1/10, step 363/574 completed (loss: 2.498340606689453, acc: 0.3947368562221527)
[2024-11-13 06:07:36,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:36,583][root][INFO] - Training Epoch: 1/10, step 364/574 completed (loss: 2.4840891361236572, acc: 0.4146341383457184)
[2024-11-13 06:07:36,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:37,009][root][INFO] - Training Epoch: 1/10, step 365/574 completed (loss: 2.3894591331481934, acc: 0.39393940567970276)
[2024-11-13 06:07:37,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:37,423][root][INFO] - Training Epoch: 1/10, step 366/574 completed (loss: 1.0719956159591675, acc: 0.7083333134651184)
[2024-11-13 06:07:37,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:37,823][root][INFO] - Training Epoch: 1/10, step 367/574 completed (loss: 1.1065729856491089, acc: 0.739130437374115)
[2024-11-13 06:07:38,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:38,225][root][INFO] - Training Epoch: 1/10, step 368/574 completed (loss: 1.8910914659500122, acc: 0.4285714328289032)
[2024-11-13 06:07:38,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:38,682][root][INFO] - Training Epoch: 1/10, step 369/574 completed (loss: 1.9022899866104126, acc: 0.46875)
[2024-11-13 06:07:39,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:39,677][root][INFO] - Training Epoch: 1/10, step 370/574 completed (loss: 2.3471357822418213, acc: 0.38787877559661865)
[2024-11-13 06:07:40,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:41,107][root][INFO] - Training Epoch: 1/10, step 371/574 completed (loss: 1.7538734674453735, acc: 0.5660377144813538)
[2024-11-13 06:07:41,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:41,494][root][INFO] - Training Epoch: 1/10, step 372/574 completed (loss: 2.1926562786102295, acc: 0.42222222685813904)
[2024-11-13 06:07:41,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:41,926][root][INFO] - Training Epoch: 1/10, step 373/574 completed (loss: 2.211296319961548, acc: 0.4285714328289032)
[2024-11-13 06:07:42,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:42,341][root][INFO] - Training Epoch: 1/10, step 374/574 completed (loss: 1.3240889310836792, acc: 0.6285714507102966)
[2024-11-13 06:07:42,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:42,703][root][INFO] - Training Epoch: 1/10, step 375/574 completed (loss: 0.9819733500480652, acc: 0.7599999904632568)
[2024-11-13 06:07:42,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:43,070][root][INFO] - Training Epoch: 1/10, step 376/574 completed (loss: 1.1935573816299438, acc: 0.6086956262588501)
[2024-11-13 06:07:43,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:43,461][root][INFO] - Training Epoch: 1/10, step 377/574 completed (loss: 2.7277705669403076, acc: 0.2083333283662796)
[2024-11-13 06:07:43,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:43,930][root][INFO] - Training Epoch: 1/10, step 378/574 completed (loss: 2.1431472301483154, acc: 0.43157893419265747)
[2024-11-13 06:07:44,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:44,851][root][INFO] - Training Epoch: 1/10, step 379/574 completed (loss: 2.097727060317993, acc: 0.455089807510376)
[2024-11-13 06:07:45,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:45,440][root][INFO] - Training Epoch: 1/10, step 380/574 completed (loss: 1.9452913999557495, acc: 0.518796980381012)
[2024-11-13 06:07:46,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:47,459][root][INFO] - Training Epoch: 1/10, step 381/574 completed (loss: 2.0725793838500977, acc: 0.47058823704719543)
[2024-11-13 06:07:47,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:48,312][root][INFO] - Training Epoch: 1/10, step 382/574 completed (loss: 1.8162356615066528, acc: 0.5315315127372742)
[2024-11-13 06:07:48,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:48,735][root][INFO] - Training Epoch: 1/10, step 383/574 completed (loss: 1.5768245458602905, acc: 0.5714285969734192)
[2024-11-13 06:07:48,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:49,128][root][INFO] - Training Epoch: 1/10, step 384/574 completed (loss: 1.3839577436447144, acc: 0.5)
[2024-11-13 06:07:49,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:49,499][root][INFO] - Training Epoch: 1/10, step 385/574 completed (loss: 2.214446783065796, acc: 0.3125)
[2024-11-13 06:07:49,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:49,873][root][INFO] - Training Epoch: 1/10, step 386/574 completed (loss: 2.322841167449951, acc: 0.4444444477558136)
[2024-11-13 06:07:50,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:50,212][root][INFO] - Training Epoch: 1/10, step 387/574 completed (loss: 2.021561861038208, acc: 0.3947368562221527)
[2024-11-13 06:07:50,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:50,516][root][INFO] - Training Epoch: 1/10, step 388/574 completed (loss: 1.7518336772918701, acc: 0.5454545617103577)
[2024-11-13 06:07:50,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:50,829][root][INFO] - Training Epoch: 1/10, step 389/574 completed (loss: 1.9856212139129639, acc: 0.550000011920929)
[2024-11-13 06:07:50,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:51,137][root][INFO] - Training Epoch: 1/10, step 390/574 completed (loss: 1.9887285232543945, acc: 0.4761904776096344)
[2024-11-13 06:07:51,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:51,493][root][INFO] - Training Epoch: 1/10, step 391/574 completed (loss: 2.51912260055542, acc: 0.31481480598449707)
[2024-11-13 06:07:51,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:51,902][root][INFO] - Training Epoch: 1/10, step 392/574 completed (loss: 2.707677125930786, acc: 0.3786407709121704)
[2024-11-13 06:07:52,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:52,727][root][INFO] - Training Epoch: 1/10, step 393/574 completed (loss: 2.259979248046875, acc: 0.41911765933036804)
[2024-11-13 06:07:53,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:53,273][root][INFO] - Training Epoch: 1/10, step 394/574 completed (loss: 2.5929713249206543, acc: 0.35333332419395447)
[2024-11-13 06:07:53,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:53,825][root][INFO] - Training Epoch: 1/10, step 395/574 completed (loss: 2.3791074752807617, acc: 0.4305555522441864)
[2024-11-13 06:07:54,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:54,282][root][INFO] - Training Epoch: 1/10, step 396/574 completed (loss: 2.202240228652954, acc: 0.41860464215278625)
[2024-11-13 06:07:54,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:54,669][root][INFO] - Training Epoch: 1/10, step 397/574 completed (loss: 1.4840717315673828, acc: 0.5833333134651184)
[2024-11-13 06:07:54,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:55,090][root][INFO] - Training Epoch: 1/10, step 398/574 completed (loss: 1.986412763595581, acc: 0.4651162922382355)
[2024-11-13 06:07:55,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:55,503][root][INFO] - Training Epoch: 1/10, step 399/574 completed (loss: 2.1016719341278076, acc: 0.4000000059604645)
[2024-11-13 06:07:55,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:56,336][root][INFO] - Training Epoch: 1/10, step 400/574 completed (loss: 2.207313060760498, acc: 0.5)
[2024-11-13 06:07:56,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:56,798][root][INFO] - Training Epoch: 1/10, step 401/574 completed (loss: 2.1302456855773926, acc: 0.46666666865348816)
[2024-11-13 06:07:57,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:57,243][root][INFO] - Training Epoch: 1/10, step 402/574 completed (loss: 1.7043704986572266, acc: 0.6060606241226196)
[2024-11-13 06:07:57,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:57,698][root][INFO] - Training Epoch: 1/10, step 403/574 completed (loss: 2.041837215423584, acc: 0.5151515007019043)
[2024-11-13 06:07:57,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:58,178][root][INFO] - Training Epoch: 1/10, step 404/574 completed (loss: 1.8886436223983765, acc: 0.4193548262119293)
[2024-11-13 06:07:58,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:58,608][root][INFO] - Training Epoch: 1/10, step 405/574 completed (loss: 2.2443900108337402, acc: 0.40740740299224854)
[2024-11-13 06:07:58,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:59,075][root][INFO] - Training Epoch: 1/10, step 406/574 completed (loss: 1.1667691469192505, acc: 0.7599999904632568)
[2024-11-13 06:07:59,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:59,529][root][INFO] - Training Epoch: 1/10, step 407/574 completed (loss: 1.241692066192627, acc: 0.6388888955116272)
[2024-11-13 06:07:59,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:07:59,966][root][INFO] - Training Epoch: 1/10, step 408/574 completed (loss: 1.3309662342071533, acc: 0.6296296119689941)
[2024-11-13 06:08:00,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:00,406][root][INFO] - Training Epoch: 1/10, step 409/574 completed (loss: 1.4379888772964478, acc: 0.6153846383094788)
[2024-11-13 06:08:00,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:00,838][root][INFO] - Training Epoch: 1/10, step 410/574 completed (loss: 1.81657874584198, acc: 0.5344827771186829)
[2024-11-13 06:08:01,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:01,233][root][INFO] - Training Epoch: 1/10, step 411/574 completed (loss: 1.2186282873153687, acc: 0.75)
[2024-11-13 06:08:01,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:01,639][root][INFO] - Training Epoch: 1/10, step 412/574 completed (loss: 1.4044029712677002, acc: 0.6000000238418579)
[2024-11-13 06:08:01,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:02,015][root][INFO] - Training Epoch: 1/10, step 413/574 completed (loss: 1.6989600658416748, acc: 0.4545454680919647)
[2024-11-13 06:08:02,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:02,440][root][INFO] - Training Epoch: 1/10, step 414/574 completed (loss: 2.0050294399261475, acc: 0.4545454680919647)
[2024-11-13 06:08:02,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:02,972][root][INFO] - Training Epoch: 1/10, step 415/574 completed (loss: 2.2235991954803467, acc: 0.4901960790157318)
[2024-11-13 06:08:03,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:03,399][root][INFO] - Training Epoch: 1/10, step 416/574 completed (loss: 2.0314221382141113, acc: 0.5)
[2024-11-13 06:08:03,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:03,794][root][INFO] - Training Epoch: 1/10, step 417/574 completed (loss: 1.9313877820968628, acc: 0.6111111044883728)
[2024-11-13 06:08:03,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:04,225][root][INFO] - Training Epoch: 1/10, step 418/574 completed (loss: 1.9922103881835938, acc: 0.5)
[2024-11-13 06:08:04,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:04,682][root][INFO] - Training Epoch: 1/10, step 419/574 completed (loss: 2.940661907196045, acc: 0.3499999940395355)
[2024-11-13 06:08:04,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:05,120][root][INFO] - Training Epoch: 1/10, step 420/574 completed (loss: 0.7208282947540283, acc: 0.8095238208770752)
[2024-11-13 06:08:05,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:05,541][root][INFO] - Training Epoch: 1/10, step 421/574 completed (loss: 1.8957992792129517, acc: 0.4333333373069763)
[2024-11-13 06:08:05,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:05,984][root][INFO] - Training Epoch: 1/10, step 422/574 completed (loss: 1.8816947937011719, acc: 0.40625)
[2024-11-13 06:08:06,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:06,452][root][INFO] - Training Epoch: 1/10, step 423/574 completed (loss: 1.7858388423919678, acc: 0.4444444477558136)
[2024-11-13 06:08:06,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:06,884][root][INFO] - Training Epoch: 1/10, step 424/574 completed (loss: 1.7534079551696777, acc: 0.3333333432674408)
[2024-11-13 06:08:07,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:07,280][root][INFO] - Training Epoch: 1/10, step 425/574 completed (loss: 1.5024813413619995, acc: 0.5757575631141663)
[2024-11-13 06:08:07,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:07,724][root][INFO] - Training Epoch: 1/10, step 426/574 completed (loss: 0.940432608127594, acc: 0.8260869383811951)
[2024-11-13 06:08:07,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:08,174][root][INFO] - Training Epoch: 1/10, step 427/574 completed (loss: 1.32276451587677, acc: 0.6756756901741028)
[2024-11-13 06:08:08,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:08,604][root][INFO] - Training Epoch: 1/10, step 428/574 completed (loss: 1.2627965211868286, acc: 0.7407407164573669)
[2024-11-13 06:08:09,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:10,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:10,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:10,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:11,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:11,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:12,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:12,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:13,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:14,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:14,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:15,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:15,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:16,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:16,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:17,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:17,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:18,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:18,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:19,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:19,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:20,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:20,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:21,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:21,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:22,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:22,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:23,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:23,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:24,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:24,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:25,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:25,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:26,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:26,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:27,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:27,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:27,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:28,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:28,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:29,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:30,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:30,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:31,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:32,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:32,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:33,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:33,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:34,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:34,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:35,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:35,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:36,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:36,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:37,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:38,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:38,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:39,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:39,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:39,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:40,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:41,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:42,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:42,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:43,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:43,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:44,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:44,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:45,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:45,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:46,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:46,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:47,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:47,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:48,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:48,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:49,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:49,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:50,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:50,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:51,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:51,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:52,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:52,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:53,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:53,911][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(10.1284, device='cuda:0') eval_epoch_loss=tensor(2.3153, device='cuda:0') eval_epoch_acc=tensor(0.4149, device='cuda:0')
[2024-11-13 06:08:53,913][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:08:53,913][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:08:54,403][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_1_step_429_loss_2.3153440952301025/model.pt
[2024-11-13 06:08:54,426][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:08:54,427][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 2.3153440952301025
[2024-11-13 06:08:54,428][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.4148782193660736
[2024-11-13 06:08:54,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:55,006][root][INFO] - Training Epoch: 1/10, step 429/574 completed (loss: 1.7538985013961792, acc: 0.47826087474823)
[2024-11-13 06:08:55,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:55,497][root][INFO] - Training Epoch: 1/10, step 430/574 completed (loss: 0.8031637668609619, acc: 0.8518518805503845)
[2024-11-13 06:08:55,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:55,937][root][INFO] - Training Epoch: 1/10, step 431/574 completed (loss: 1.0698050260543823, acc: 0.6666666865348816)
[2024-11-13 06:08:56,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:56,319][root][INFO] - Training Epoch: 1/10, step 432/574 completed (loss: 1.6810331344604492, acc: 0.5652173757553101)
[2024-11-13 06:08:56,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:56,832][root][INFO] - Training Epoch: 1/10, step 433/574 completed (loss: 1.6756185293197632, acc: 0.6111111044883728)
[2024-11-13 06:08:57,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:57,291][root][INFO] - Training Epoch: 1/10, step 434/574 completed (loss: 0.8065745830535889, acc: 0.7599999904632568)
[2024-11-13 06:08:57,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:57,804][root][INFO] - Training Epoch: 1/10, step 435/574 completed (loss: 1.7506834268569946, acc: 0.4848484992980957)
[2024-11-13 06:08:57,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:58,214][root][INFO] - Training Epoch: 1/10, step 436/574 completed (loss: 1.4923036098480225, acc: 0.5277777910232544)
[2024-11-13 06:08:58,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:58,651][root][INFO] - Training Epoch: 1/10, step 437/574 completed (loss: 1.8902575969696045, acc: 0.5681818127632141)
[2024-11-13 06:08:58,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:58,987][root][INFO] - Training Epoch: 1/10, step 438/574 completed (loss: 0.4840075969696045, acc: 0.9047619104385376)
[2024-11-13 06:08:59,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:08:59,460][root][INFO] - Training Epoch: 1/10, step 439/574 completed (loss: 2.256784439086914, acc: 0.4871794879436493)
[2024-11-13 06:08:59,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:00,271][root][INFO] - Training Epoch: 1/10, step 440/574 completed (loss: 2.249405860900879, acc: 0.40909090638160706)
[2024-11-13 06:09:00,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:01,444][root][INFO] - Training Epoch: 1/10, step 441/574 completed (loss: 2.76883864402771, acc: 0.2879999876022339)
[2024-11-13 06:09:01,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:02,081][root][INFO] - Training Epoch: 1/10, step 442/574 completed (loss: 2.523444175720215, acc: 0.39516130089759827)
[2024-11-13 06:09:02,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:03,165][root][INFO] - Training Epoch: 1/10, step 443/574 completed (loss: 2.51190185546875, acc: 0.3631840944290161)
[2024-11-13 06:09:03,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:03,622][root][INFO] - Training Epoch: 1/10, step 444/574 completed (loss: 2.433936357498169, acc: 0.37735849618911743)
[2024-11-13 06:09:03,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:04,315][root][INFO] - Training Epoch: 1/10, step 445/574 completed (loss: 1.6585121154785156, acc: 0.5681818127632141)
[2024-11-13 06:09:04,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:04,762][root][INFO] - Training Epoch: 1/10, step 446/574 completed (loss: 1.9734601974487305, acc: 0.52173912525177)
[2024-11-13 06:09:04,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:05,225][root][INFO] - Training Epoch: 1/10, step 447/574 completed (loss: 1.8564376831054688, acc: 0.5384615659713745)
[2024-11-13 06:09:05,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:05,741][root][INFO] - Training Epoch: 1/10, step 448/574 completed (loss: 1.5843178033828735, acc: 0.6071428656578064)
[2024-11-13 06:09:05,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:06,175][root][INFO] - Training Epoch: 1/10, step 449/574 completed (loss: 2.5920581817626953, acc: 0.34328359365463257)
[2024-11-13 06:09:06,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:06,548][root][INFO] - Training Epoch: 1/10, step 450/574 completed (loss: 2.052028179168701, acc: 0.5138888955116272)
[2024-11-13 06:09:06,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:07,023][root][INFO] - Training Epoch: 1/10, step 451/574 completed (loss: 2.455470323562622, acc: 0.41304346919059753)
[2024-11-13 06:09:07,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:07,462][root][INFO] - Training Epoch: 1/10, step 452/574 completed (loss: 2.5400497913360596, acc: 0.3589743673801422)
[2024-11-13 06:09:07,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:07,895][root][INFO] - Training Epoch: 1/10, step 453/574 completed (loss: 2.466794967651367, acc: 0.44736841320991516)
[2024-11-13 06:09:08,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:08,340][root][INFO] - Training Epoch: 1/10, step 454/574 completed (loss: 1.7923073768615723, acc: 0.5510203838348389)
[2024-11-13 06:09:08,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:08,786][root][INFO] - Training Epoch: 1/10, step 455/574 completed (loss: 1.7985880374908447, acc: 0.42424243688583374)
[2024-11-13 06:09:09,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:09,263][root][INFO] - Training Epoch: 1/10, step 456/574 completed (loss: 2.4219884872436523, acc: 0.2989690601825714)
[2024-11-13 06:09:09,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:09,744][root][INFO] - Training Epoch: 1/10, step 457/574 completed (loss: 2.1680808067321777, acc: 0.3857142925262451)
[2024-11-13 06:09:10,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:10,319][root][INFO] - Training Epoch: 1/10, step 458/574 completed (loss: 2.3256771564483643, acc: 0.38372093439102173)
[2024-11-13 06:09:10,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:10,723][root][INFO] - Training Epoch: 1/10, step 459/574 completed (loss: 2.4474198818206787, acc: 0.375)
[2024-11-13 06:09:10,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:11,187][root][INFO] - Training Epoch: 1/10, step 460/574 completed (loss: 2.2863926887512207, acc: 0.37037035822868347)
[2024-11-13 06:09:11,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:11,662][root][INFO] - Training Epoch: 1/10, step 461/574 completed (loss: 1.6558839082717896, acc: 0.5555555820465088)
[2024-11-13 06:09:11,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:12,037][root][INFO] - Training Epoch: 1/10, step 462/574 completed (loss: 1.7957782745361328, acc: 0.59375)
[2024-11-13 06:09:12,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:12,501][root][INFO] - Training Epoch: 1/10, step 463/574 completed (loss: 1.7199273109436035, acc: 0.5384615659713745)
[2024-11-13 06:09:12,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:12,967][root][INFO] - Training Epoch: 1/10, step 464/574 completed (loss: 2.1699776649475098, acc: 0.43478259444236755)
[2024-11-13 06:09:13,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:13,379][root][INFO] - Training Epoch: 1/10, step 465/574 completed (loss: 2.4023425579071045, acc: 0.261904776096344)
[2024-11-13 06:09:13,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:13,829][root][INFO] - Training Epoch: 1/10, step 466/574 completed (loss: 2.521488904953003, acc: 0.2650602459907532)
[2024-11-13 06:09:14,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:14,275][root][INFO] - Training Epoch: 1/10, step 467/574 completed (loss: 2.2501089572906494, acc: 0.4234234094619751)
[2024-11-13 06:09:14,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:14,734][root][INFO] - Training Epoch: 1/10, step 468/574 completed (loss: 2.2389075756073, acc: 0.4660194218158722)
[2024-11-13 06:09:14,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:15,203][root][INFO] - Training Epoch: 1/10, step 469/574 completed (loss: 2.1644279956817627, acc: 0.39024388790130615)
[2024-11-13 06:09:15,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:15,685][root][INFO] - Training Epoch: 1/10, step 470/574 completed (loss: 1.6537760496139526, acc: 0.5416666865348816)
[2024-11-13 06:09:15,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:16,153][root][INFO] - Training Epoch: 1/10, step 471/574 completed (loss: 2.483851909637451, acc: 0.3571428656578064)
[2024-11-13 06:09:16,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:16,784][root][INFO] - Training Epoch: 1/10, step 472/574 completed (loss: 2.3163094520568848, acc: 0.38235294818878174)
[2024-11-13 06:09:17,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:17,323][root][INFO] - Training Epoch: 1/10, step 473/574 completed (loss: 2.487844944000244, acc: 0.34061136841773987)
[2024-11-13 06:09:17,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:17,729][root][INFO] - Training Epoch: 1/10, step 474/574 completed (loss: 2.4016330242156982, acc: 0.3229166567325592)
[2024-11-13 06:09:17,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:18,111][root][INFO] - Training Epoch: 1/10, step 475/574 completed (loss: 2.3168437480926514, acc: 0.38650307059288025)
[2024-11-13 06:09:18,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:18,517][root][INFO] - Training Epoch: 1/10, step 476/574 completed (loss: 2.4992518424987793, acc: 0.32374101877212524)
[2024-11-13 06:09:18,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:18,963][root][INFO] - Training Epoch: 1/10, step 477/574 completed (loss: 2.4240708351135254, acc: 0.3467336595058441)
[2024-11-13 06:09:19,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:19,292][root][INFO] - Training Epoch: 1/10, step 478/574 completed (loss: 1.6555445194244385, acc: 0.5833333134651184)
[2024-11-13 06:09:19,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:19,624][root][INFO] - Training Epoch: 1/10, step 479/574 completed (loss: 1.6696752309799194, acc: 0.5757575631141663)
[2024-11-13 06:09:19,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:20,065][root][INFO] - Training Epoch: 1/10, step 480/574 completed (loss: 1.6103132963180542, acc: 0.5185185074806213)
[2024-11-13 06:09:20,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:20,508][root][INFO] - Training Epoch: 1/10, step 481/574 completed (loss: 1.702492356300354, acc: 0.5)
[2024-11-13 06:09:20,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:20,905][root][INFO] - Training Epoch: 1/10, step 482/574 completed (loss: 1.0237269401550293, acc: 0.75)
[2024-11-13 06:09:21,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:21,486][root][INFO] - Training Epoch: 1/10, step 483/574 completed (loss: 1.6813143491744995, acc: 0.5517241358757019)
[2024-11-13 06:09:21,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:21,892][root][INFO] - Training Epoch: 1/10, step 484/574 completed (loss: 1.4408339262008667, acc: 0.6451612710952759)
[2024-11-13 06:09:22,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:22,275][root][INFO] - Training Epoch: 1/10, step 485/574 completed (loss: 1.2847697734832764, acc: 0.7894737124443054)
[2024-11-13 06:09:22,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:22,714][root][INFO] - Training Epoch: 1/10, step 486/574 completed (loss: 2.42608380317688, acc: 0.37037035822868347)
[2024-11-13 06:09:22,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:23,077][root][INFO] - Training Epoch: 1/10, step 487/574 completed (loss: 1.9991014003753662, acc: 0.4285714328289032)
[2024-11-13 06:09:23,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:23,454][root][INFO] - Training Epoch: 1/10, step 488/574 completed (loss: 1.5828529596328735, acc: 0.5)
[2024-11-13 06:09:23,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:23,887][root][INFO] - Training Epoch: 1/10, step 489/574 completed (loss: 2.108757734298706, acc: 0.3692307770252228)
[2024-11-13 06:09:24,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:24,274][root][INFO] - Training Epoch: 1/10, step 490/574 completed (loss: 1.3462263345718384, acc: 0.6333333253860474)
[2024-11-13 06:09:24,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:24,716][root][INFO] - Training Epoch: 1/10, step 491/574 completed (loss: 1.5406709909439087, acc: 0.5862069129943848)
[2024-11-13 06:09:24,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:25,123][root][INFO] - Training Epoch: 1/10, step 492/574 completed (loss: 2.149925947189331, acc: 0.47058823704719543)
[2024-11-13 06:09:25,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:25,566][root][INFO] - Training Epoch: 1/10, step 493/574 completed (loss: 1.6054154634475708, acc: 0.6206896305084229)
[2024-11-13 06:09:25,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:25,963][root][INFO] - Training Epoch: 1/10, step 494/574 completed (loss: 1.1225329637527466, acc: 0.7368420958518982)
[2024-11-13 06:09:26,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:26,359][root][INFO] - Training Epoch: 1/10, step 495/574 completed (loss: 2.992041826248169, acc: 0.21052631735801697)
[2024-11-13 06:09:26,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:26,788][root][INFO] - Training Epoch: 1/10, step 496/574 completed (loss: 2.0460333824157715, acc: 0.4642857015132904)
[2024-11-13 06:09:27,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:27,326][root][INFO] - Training Epoch: 1/10, step 497/574 completed (loss: 2.0229859352111816, acc: 0.3820224702358246)
[2024-11-13 06:09:27,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:27,700][root][INFO] - Training Epoch: 1/10, step 498/574 completed (loss: 2.379481554031372, acc: 0.37078651785850525)
[2024-11-13 06:09:27,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:28,069][root][INFO] - Training Epoch: 1/10, step 499/574 completed (loss: 2.4995322227478027, acc: 0.3617021143436432)
[2024-11-13 06:09:28,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:28,433][root][INFO] - Training Epoch: 1/10, step 500/574 completed (loss: 2.5105860233306885, acc: 0.3478260934352875)
[2024-11-13 06:09:28,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:28,786][root][INFO] - Training Epoch: 1/10, step 501/574 completed (loss: 0.9867016673088074, acc: 0.800000011920929)
[2024-11-13 06:09:28,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:29,149][root][INFO] - Training Epoch: 1/10, step 502/574 completed (loss: 1.400978684425354, acc: 0.6153846383094788)
[2024-11-13 06:09:29,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:29,559][root][INFO] - Training Epoch: 1/10, step 503/574 completed (loss: 1.312053918838501, acc: 0.6666666865348816)
[2024-11-13 06:09:29,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:29,911][root][INFO] - Training Epoch: 1/10, step 504/574 completed (loss: 1.9903700351715088, acc: 0.4444444477558136)
[2024-11-13 06:09:30,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:30,264][root][INFO] - Training Epoch: 1/10, step 505/574 completed (loss: 1.6755050420761108, acc: 0.6037735939025879)
[2024-11-13 06:09:30,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:30,622][root][INFO] - Training Epoch: 1/10, step 506/574 completed (loss: 1.5779391527175903, acc: 0.6206896305084229)
[2024-11-13 06:09:31,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:31,591][root][INFO] - Training Epoch: 1/10, step 507/574 completed (loss: 2.1035754680633545, acc: 0.4324324429035187)
[2024-11-13 06:09:31,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:32,244][root][INFO] - Training Epoch: 1/10, step 508/574 completed (loss: 2.171905755996704, acc: 0.47887325286865234)
[2024-11-13 06:09:32,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:32,594][root][INFO] - Training Epoch: 1/10, step 509/574 completed (loss: 0.36504849791526794, acc: 0.949999988079071)
[2024-11-13 06:09:32,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:32,911][root][INFO] - Training Epoch: 1/10, step 510/574 completed (loss: 0.8281598687171936, acc: 0.8333333134651184)
[2024-11-13 06:09:33,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:33,292][root][INFO] - Training Epoch: 1/10, step 511/574 completed (loss: 1.1938226222991943, acc: 0.7307692170143127)
[2024-11-13 06:09:36,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:37,465][root][INFO] - Training Epoch: 1/10, step 512/574 completed (loss: 2.2830746173858643, acc: 0.44285714626312256)
[2024-11-13 06:09:38,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:38,684][root][INFO] - Training Epoch: 1/10, step 513/574 completed (loss: 2.2220511436462402, acc: 0.4682539701461792)
[2024-11-13 06:09:38,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:39,032][root][INFO] - Training Epoch: 1/10, step 514/574 completed (loss: 1.5504379272460938, acc: 0.6428571343421936)
[2024-11-13 06:09:39,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:39,467][root][INFO] - Training Epoch: 1/10, step 515/574 completed (loss: 1.8837746381759644, acc: 0.5333333611488342)
[2024-11-13 06:09:40,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:40,635][root][INFO] - Training Epoch: 1/10, step 516/574 completed (loss: 2.0107297897338867, acc: 0.5138888955116272)
[2024-11-13 06:09:40,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:41,046][root][INFO] - Training Epoch: 1/10, step 517/574 completed (loss: 0.46016114950180054, acc: 0.8846153616905212)
[2024-11-13 06:09:41,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:41,422][root][INFO] - Training Epoch: 1/10, step 518/574 completed (loss: 2.0021636486053467, acc: 0.5161290168762207)
[2024-11-13 06:09:41,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:41,823][root][INFO] - Training Epoch: 1/10, step 519/574 completed (loss: 2.33779239654541, acc: 0.550000011920929)
[2024-11-13 06:09:41,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:42,189][root][INFO] - Training Epoch: 1/10, step 520/574 completed (loss: 1.9002503156661987, acc: 0.5555555820465088)
[2024-11-13 06:09:43,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:43,761][root][INFO] - Training Epoch: 1/10, step 521/574 completed (loss: 2.2705745697021484, acc: 0.38983049988746643)
[2024-11-13 06:09:43,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:44,231][root][INFO] - Training Epoch: 1/10, step 522/574 completed (loss: 2.3672235012054443, acc: 0.3805970251560211)
[2024-11-13 06:09:44,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:44,740][root][INFO] - Training Epoch: 1/10, step 523/574 completed (loss: 2.2838804721832275, acc: 0.38686132431030273)
[2024-11-13 06:09:45,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:45,602][root][INFO] - Training Epoch: 1/10, step 524/574 completed (loss: 2.134413719177246, acc: 0.44999998807907104)
[2024-11-13 06:09:45,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:45,977][root][INFO] - Training Epoch: 1/10, step 525/574 completed (loss: 2.124732732772827, acc: 0.46296295523643494)
[2024-11-13 06:09:46,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:46,360][root][INFO] - Training Epoch: 1/10, step 526/574 completed (loss: 2.028493881225586, acc: 0.48076921701431274)
[2024-11-13 06:09:46,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:46,681][root][INFO] - Training Epoch: 1/10, step 527/574 completed (loss: 2.2259225845336914, acc: 0.3333333432674408)
[2024-11-13 06:09:46,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:47,012][root][INFO] - Training Epoch: 1/10, step 528/574 completed (loss: 2.979982852935791, acc: 0.19672131538391113)
[2024-11-13 06:09:47,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:47,359][root][INFO] - Training Epoch: 1/10, step 529/574 completed (loss: 1.9944219589233398, acc: 0.47457626461982727)
[2024-11-13 06:09:47,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:47,702][root][INFO] - Training Epoch: 1/10, step 530/574 completed (loss: 2.5872466564178467, acc: 0.3255814015865326)
[2024-11-13 06:09:47,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:48,091][root][INFO] - Training Epoch: 1/10, step 531/574 completed (loss: 2.3630685806274414, acc: 0.3863636255264282)
[2024-11-13 06:09:48,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:48,429][root][INFO] - Training Epoch: 1/10, step 532/574 completed (loss: 2.5802018642425537, acc: 0.3207547068595886)
[2024-11-13 06:09:48,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:48,772][root][INFO] - Training Epoch: 1/10, step 533/574 completed (loss: 2.119034767150879, acc: 0.5227272510528564)
[2024-11-13 06:09:48,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:49,186][root][INFO] - Training Epoch: 1/10, step 534/574 completed (loss: 1.5003451108932495, acc: 0.6399999856948853)
[2024-11-13 06:09:49,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:49,566][root][INFO] - Training Epoch: 1/10, step 535/574 completed (loss: 1.7938144207000732, acc: 0.6000000238418579)
[2024-11-13 06:09:49,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:49,913][root][INFO] - Training Epoch: 1/10, step 536/574 completed (loss: 1.2746940851211548, acc: 0.5909090638160706)
[2024-11-13 06:09:50,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:50,463][root][INFO] - Training Epoch: 1/10, step 537/574 completed (loss: 2.0258712768554688, acc: 0.446153849363327)
[2024-11-13 06:09:50,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:50,866][root][INFO] - Training Epoch: 1/10, step 538/574 completed (loss: 1.9756405353546143, acc: 0.5)
[2024-11-13 06:09:51,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:51,384][root][INFO] - Training Epoch: 1/10, step 539/574 completed (loss: 1.3087986707687378, acc: 0.71875)
[2024-11-13 06:09:51,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:51,704][root][INFO] - Training Epoch: 1/10, step 540/574 completed (loss: 1.978560447692871, acc: 0.4545454680919647)
[2024-11-13 06:09:51,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:52,020][root][INFO] - Training Epoch: 1/10, step 541/574 completed (loss: 0.7927253842353821, acc: 0.8125)
[2024-11-13 06:09:52,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:52,306][root][INFO] - Training Epoch: 1/10, step 542/574 completed (loss: 1.1984646320343018, acc: 0.6451612710952759)
[2024-11-13 06:09:52,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:52,643][root][INFO] - Training Epoch: 1/10, step 543/574 completed (loss: 0.6202178001403809, acc: 0.9130434989929199)
[2024-11-13 06:09:52,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:52,974][root][INFO] - Training Epoch: 1/10, step 544/574 completed (loss: 2.0804524421691895, acc: 0.5)
[2024-11-13 06:09:53,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:53,395][root][INFO] - Training Epoch: 1/10, step 545/574 completed (loss: 1.6378008127212524, acc: 0.6341463327407837)
[2024-11-13 06:09:53,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:53,717][root][INFO] - Training Epoch: 1/10, step 546/574 completed (loss: 0.9857960343360901, acc: 0.7142857313156128)
[2024-11-13 06:09:53,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:54,076][root][INFO] - Training Epoch: 1/10, step 547/574 completed (loss: 1.4926676750183105, acc: 0.6315789222717285)
[2024-11-13 06:09:54,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:54,511][root][INFO] - Training Epoch: 1/10, step 548/574 completed (loss: 1.345428466796875, acc: 0.5806451439857483)
[2024-11-13 06:09:54,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:54,905][root][INFO] - Training Epoch: 1/10, step 549/574 completed (loss: 0.6346209645271301, acc: 0.8799999952316284)
[2024-11-13 06:09:55,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:55,212][root][INFO] - Training Epoch: 1/10, step 550/574 completed (loss: 1.1986079216003418, acc: 0.5454545617103577)
[2024-11-13 06:09:55,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:55,533][root][INFO] - Training Epoch: 1/10, step 551/574 completed (loss: 1.3548462390899658, acc: 0.625)
[2024-11-13 06:09:55,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:55,834][root][INFO] - Training Epoch: 1/10, step 552/574 completed (loss: 1.608404278755188, acc: 0.5)
[2024-11-13 06:09:56,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:56,268][root][INFO] - Training Epoch: 1/10, step 553/574 completed (loss: 2.469097852706909, acc: 0.34306567907333374)
[2024-11-13 06:09:56,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:56,609][root][INFO] - Training Epoch: 1/10, step 554/574 completed (loss: 2.090237617492676, acc: 0.45517241954803467)
[2024-11-13 06:09:56,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:56,981][root][INFO] - Training Epoch: 1/10, step 555/574 completed (loss: 2.8844244480133057, acc: 0.2571428716182709)
[2024-11-13 06:09:57,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:57,348][root][INFO] - Training Epoch: 1/10, step 556/574 completed (loss: 2.8448781967163086, acc: 0.21854305267333984)
[2024-11-13 06:09:57,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:57,705][root][INFO] - Training Epoch: 1/10, step 557/574 completed (loss: 2.257611036300659, acc: 0.41025641560554504)
[2024-11-13 06:09:57,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:58,047][root][INFO] - Training Epoch: 1/10, step 558/574 completed (loss: 0.5391470193862915, acc: 0.8399999737739563)
[2024-11-13 06:09:58,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:58,395][root][INFO] - Training Epoch: 1/10, step 559/574 completed (loss: 1.3388769626617432, acc: 0.692307710647583)
[2024-11-13 06:09:58,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:58,708][root][INFO] - Training Epoch: 1/10, step 560/574 completed (loss: 0.9527692198753357, acc: 0.7307692170143127)
[2024-11-13 06:09:58,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:59,130][root][INFO] - Training Epoch: 1/10, step 561/574 completed (loss: 1.7432268857955933, acc: 0.5384615659713745)
[2024-11-13 06:09:59,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:59,518][root][INFO] - Training Epoch: 1/10, step 562/574 completed (loss: 1.8600283861160278, acc: 0.5333333611488342)
[2024-11-13 06:09:59,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:09:59,848][root][INFO] - Training Epoch: 1/10, step 563/574 completed (loss: 2.164280414581299, acc: 0.4025973975658417)
[2024-11-13 06:10:00,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:00,231][root][INFO] - Training Epoch: 1/10, step 564/574 completed (loss: 1.9556865692138672, acc: 0.4583333432674408)
[2024-11-13 06:10:00,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:00,588][root][INFO] - Training Epoch: 1/10, step 565/574 completed (loss: 2.1207711696624756, acc: 0.4482758641242981)
[2024-11-13 06:10:00,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:00,921][root][INFO] - Training Epoch: 1/10, step 566/574 completed (loss: 2.1279330253601074, acc: 0.4166666567325592)
[2024-11-13 06:10:01,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:01,284][root][INFO] - Training Epoch: 1/10, step 567/574 completed (loss: 1.671338438987732, acc: 0.4736842215061188)
[2024-11-13 06:10:01,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:01,696][root][INFO] - Training Epoch: 1/10, step 568/574 completed (loss: 1.3819512128829956, acc: 0.6296296119689941)
[2024-11-13 06:10:01,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:02,243][root][INFO] - Training Epoch: 1/10, step 569/574 completed (loss: 2.140922784805298, acc: 0.4010695219039917)
[2024-11-13 06:10:02,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:02,627][root][INFO] - Training Epoch: 1/10, step 570/574 completed (loss: 1.6484670639038086, acc: 0.5645161271095276)
[2024-11-13 06:10:02,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:02,967][root][INFO] - Training Epoch: 1/10, step 571/574 completed (loss: 2.1665589809417725, acc: 0.43589743971824646)
[2024-11-13 06:10:03,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:04,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:04,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:05,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:05,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:06,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:07,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:07,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:08,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:08,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:09,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:09,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:10,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:10,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:11,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:12,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:12,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:13,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:13,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:13,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:14,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:14,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:15,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:15,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:16,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:16,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:17,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:17,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:18,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:18,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:19,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:19,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:20,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:20,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:21,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:21,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:22,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:22,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:23,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:23,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:24,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:24,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:25,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:25,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:26,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:26,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:27,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:27,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:28,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:28,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:29,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:29,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:30,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:30,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:30,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:31,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:31,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:32,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:32,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:33,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:34,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:34,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:35,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:36,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:36,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:37,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:37,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:38,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:38,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:39,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:40,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:40,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:41,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:41,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:42,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:42,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:43,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:43,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:44,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:44,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:45,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:45,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:46,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:46,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:47,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:47,941][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(5.9823, device='cuda:0') eval_epoch_loss=tensor(1.7888, device='cuda:0') eval_epoch_acc=tensor(0.5274, device='cuda:0')
[2024-11-13 06:10:47,943][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:10:47,943][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:10:48,477][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_1_step_572_loss_1.7887994050979614/model.pt
[2024-11-13 06:10:48,487][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:10:48,488][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.7887994050979614
[2024-11-13 06:10:48,488][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.5273908972740173
[2024-11-13 06:10:48,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:49,000][root][INFO] - Training Epoch: 1/10, step 572/574 completed (loss: 2.4816627502441406, acc: 0.3469387888908386)
[2024-11-13 06:10:49,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:49,512][root][INFO] - Training Epoch: 1/10, step 573/574 completed (loss: 2.36776065826416, acc: 0.35849055647850037)
[2024-11-13 06:10:50,277][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=16.7871, train_epoch_loss=2.8206, epoch time 494.1193344760686s
[2024-11-13 06:10:50,278][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-11-13 06:10:50,278][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 17 GB
[2024-11-13 06:10:50,278][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-11-13 06:10:50,278][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2024-11-13 06:10:50,278][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-13 06:10:51,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:51,316][root][INFO] - Training Epoch: 2/10, step 0/574 completed (loss: 1.3459054231643677, acc: 0.6666666865348816)
[2024-11-13 06:10:51,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:51,708][root][INFO] - Training Epoch: 2/10, step 1/574 completed (loss: 1.8777754306793213, acc: 0.6000000238418579)
[2024-11-13 06:10:51,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:52,072][root][INFO] - Training Epoch: 2/10, step 2/574 completed (loss: 2.5064609050750732, acc: 0.37837839126586914)
[2024-11-13 06:10:52,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:52,471][root][INFO] - Training Epoch: 2/10, step 3/574 completed (loss: 2.4220921993255615, acc: 0.28947368264198303)
[2024-11-13 06:10:52,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:52,854][root][INFO] - Training Epoch: 2/10, step 4/574 completed (loss: 1.73459792137146, acc: 0.5135135054588318)
[2024-11-13 06:10:53,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:53,244][root][INFO] - Training Epoch: 2/10, step 5/574 completed (loss: 1.806985855102539, acc: 0.3928571343421936)
[2024-11-13 06:10:53,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:53,657][root][INFO] - Training Epoch: 2/10, step 6/574 completed (loss: 2.1297900676727295, acc: 0.4897959232330322)
[2024-11-13 06:10:53,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:54,131][root][INFO] - Training Epoch: 2/10, step 7/574 completed (loss: 1.5906037092208862, acc: 0.5)
[2024-11-13 06:10:54,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:54,563][root][INFO] - Training Epoch: 2/10, step 8/574 completed (loss: 0.3464317321777344, acc: 0.9090909361839294)
[2024-11-13 06:10:54,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:54,921][root][INFO] - Training Epoch: 2/10, step 9/574 completed (loss: 0.9219810962677002, acc: 0.6538461446762085)
[2024-11-13 06:10:55,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:55,273][root][INFO] - Training Epoch: 2/10, step 10/574 completed (loss: 0.9980731010437012, acc: 0.7777777910232544)
[2024-11-13 06:10:55,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:55,697][root][INFO] - Training Epoch: 2/10, step 11/574 completed (loss: 1.929171085357666, acc: 0.5641025900840759)
[2024-11-13 06:10:55,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:56,100][root][INFO] - Training Epoch: 2/10, step 12/574 completed (loss: 1.4277137517929077, acc: 0.6363636255264282)
[2024-11-13 06:10:56,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:56,559][root][INFO] - Training Epoch: 2/10, step 13/574 completed (loss: 1.397486925125122, acc: 0.6086956262588501)
[2024-11-13 06:10:56,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:56,956][root][INFO] - Training Epoch: 2/10, step 14/574 completed (loss: 1.9523615837097168, acc: 0.529411792755127)
[2024-11-13 06:10:57,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:57,388][root][INFO] - Training Epoch: 2/10, step 15/574 completed (loss: 1.6565810441970825, acc: 0.5918367505073547)
[2024-11-13 06:10:57,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:57,783][root][INFO] - Training Epoch: 2/10, step 16/574 completed (loss: 0.8518540263175964, acc: 0.7894737124443054)
[2024-11-13 06:10:57,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:58,227][root][INFO] - Training Epoch: 2/10, step 17/574 completed (loss: 1.6329902410507202, acc: 0.5)
[2024-11-13 06:10:58,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:58,635][root][INFO] - Training Epoch: 2/10, step 18/574 completed (loss: 2.6090047359466553, acc: 0.4166666567325592)
[2024-11-13 06:10:58,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:59,067][root][INFO] - Training Epoch: 2/10, step 19/574 completed (loss: 1.3198076486587524, acc: 0.5263158082962036)
[2024-11-13 06:10:59,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:59,507][root][INFO] - Training Epoch: 2/10, step 20/574 completed (loss: 1.8663487434387207, acc: 0.5)
[2024-11-13 06:10:59,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:10:59,921][root][INFO] - Training Epoch: 2/10, step 21/574 completed (loss: 1.57118558883667, acc: 0.5517241358757019)
[2024-11-13 06:11:00,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:00,309][root][INFO] - Training Epoch: 2/10, step 22/574 completed (loss: 1.4146461486816406, acc: 0.6000000238418579)
[2024-11-13 06:11:00,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:00,732][root][INFO] - Training Epoch: 2/10, step 23/574 completed (loss: 1.105799674987793, acc: 0.761904776096344)
[2024-11-13 06:11:00,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:01,156][root][INFO] - Training Epoch: 2/10, step 24/574 completed (loss: 1.5787774324417114, acc: 0.625)
[2024-11-13 06:11:01,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:01,651][root][INFO] - Training Epoch: 2/10, step 25/574 completed (loss: 2.38320255279541, acc: 0.3207547068595886)
[2024-11-13 06:11:01,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:02,117][root][INFO] - Training Epoch: 2/10, step 26/574 completed (loss: 2.142008066177368, acc: 0.45205479860305786)
[2024-11-13 06:11:03,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:03,885][root][INFO] - Training Epoch: 2/10, step 27/574 completed (loss: 2.5423097610473633, acc: 0.3320158123970032)
[2024-11-13 06:11:04,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:04,266][root][INFO] - Training Epoch: 2/10, step 28/574 completed (loss: 1.97441565990448, acc: 0.3720930218696594)
[2024-11-13 06:11:04,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:04,685][root][INFO] - Training Epoch: 2/10, step 29/574 completed (loss: 1.9771406650543213, acc: 0.4337349534034729)
[2024-11-13 06:11:04,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:05,097][root][INFO] - Training Epoch: 2/10, step 30/574 completed (loss: 2.2225732803344727, acc: 0.34567901492118835)
[2024-11-13 06:11:05,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:05,547][root][INFO] - Training Epoch: 2/10, step 31/574 completed (loss: 2.2501003742218018, acc: 0.5357142686843872)
[2024-11-13 06:11:05,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:05,952][root][INFO] - Training Epoch: 2/10, step 32/574 completed (loss: 1.5000203847885132, acc: 0.5185185074806213)
[2024-11-13 06:11:06,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:06,327][root][INFO] - Training Epoch: 2/10, step 33/574 completed (loss: 1.007156491279602, acc: 0.695652186870575)
[2024-11-13 06:11:06,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:06,719][root][INFO] - Training Epoch: 2/10, step 34/574 completed (loss: 1.9462006092071533, acc: 0.529411792755127)
[2024-11-13 06:11:06,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:07,137][root][INFO] - Training Epoch: 2/10, step 35/574 completed (loss: 1.649696946144104, acc: 0.5245901346206665)
[2024-11-13 06:11:07,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:07,608][root][INFO] - Training Epoch: 2/10, step 36/574 completed (loss: 1.7972289323806763, acc: 0.460317462682724)
[2024-11-13 06:11:07,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:08,036][root][INFO] - Training Epoch: 2/10, step 37/574 completed (loss: 2.0867607593536377, acc: 0.4406779706478119)
[2024-11-13 06:11:08,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:08,481][root][INFO] - Training Epoch: 2/10, step 38/574 completed (loss: 1.5468413829803467, acc: 0.49425286054611206)
[2024-11-13 06:11:08,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:08,974][root][INFO] - Training Epoch: 2/10, step 39/574 completed (loss: 1.0412548780441284, acc: 0.761904776096344)
[2024-11-13 06:11:09,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:09,424][root][INFO] - Training Epoch: 2/10, step 40/574 completed (loss: 2.0701353549957275, acc: 0.5)
[2024-11-13 06:11:09,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:09,949][root][INFO] - Training Epoch: 2/10, step 41/574 completed (loss: 2.4757919311523438, acc: 0.37837839126586914)
[2024-11-13 06:11:10,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:10,363][root][INFO] - Training Epoch: 2/10, step 42/574 completed (loss: 2.148559093475342, acc: 0.4153846204280853)
[2024-11-13 06:11:10,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:10,964][root][INFO] - Training Epoch: 2/10, step 43/574 completed (loss: 2.1559534072875977, acc: 0.5050504803657532)
[2024-11-13 06:11:11,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:11,537][root][INFO] - Training Epoch: 2/10, step 44/574 completed (loss: 2.069760322570801, acc: 0.4845360815525055)
[2024-11-13 06:11:11,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:12,077][root][INFO] - Training Epoch: 2/10, step 45/574 completed (loss: 2.141763925552368, acc: 0.45588234066963196)
[2024-11-13 06:11:12,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:12,489][root][INFO] - Training Epoch: 2/10, step 46/574 completed (loss: 0.8198532462120056, acc: 0.7692307829856873)
[2024-11-13 06:11:12,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:12,929][root][INFO] - Training Epoch: 2/10, step 47/574 completed (loss: 0.6756845116615295, acc: 0.8518518805503845)
[2024-11-13 06:11:13,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:13,356][root][INFO] - Training Epoch: 2/10, step 48/574 completed (loss: 1.1607112884521484, acc: 0.6785714030265808)
[2024-11-13 06:11:13,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:13,809][root][INFO] - Training Epoch: 2/10, step 49/574 completed (loss: 1.0471450090408325, acc: 0.7222222089767456)
[2024-11-13 06:11:14,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:14,269][root][INFO] - Training Epoch: 2/10, step 50/574 completed (loss: 1.3572463989257812, acc: 0.6315789222717285)
[2024-11-13 06:11:14,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:14,730][root][INFO] - Training Epoch: 2/10, step 51/574 completed (loss: 1.5263571739196777, acc: 0.6349206566810608)
[2024-11-13 06:11:14,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:15,152][root][INFO] - Training Epoch: 2/10, step 52/574 completed (loss: 2.0870063304901123, acc: 0.4647887349128723)
[2024-11-13 06:11:15,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:15,795][root][INFO] - Training Epoch: 2/10, step 53/574 completed (loss: 2.432072401046753, acc: 0.47333332896232605)
[2024-11-13 06:11:15,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:16,199][root][INFO] - Training Epoch: 2/10, step 54/574 completed (loss: 1.3357007503509521, acc: 0.6486486196517944)
[2024-11-13 06:11:16,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:16,528][root][INFO] - Training Epoch: 2/10, step 55/574 completed (loss: 0.37959226965904236, acc: 0.8461538553237915)
[2024-11-13 06:11:19,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:21,051][root][INFO] - Training Epoch: 2/10, step 56/574 completed (loss: 2.010572910308838, acc: 0.5153583884239197)
[2024-11-13 06:11:22,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:22,909][root][INFO] - Training Epoch: 2/10, step 57/574 completed (loss: 2.4394500255584717, acc: 0.38562092185020447)
[2024-11-13 06:11:23,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:23,858][root][INFO] - Training Epoch: 2/10, step 58/574 completed (loss: 2.072873592376709, acc: 0.46022728085517883)
[2024-11-13 06:11:24,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:24,724][root][INFO] - Training Epoch: 2/10, step 59/574 completed (loss: 2.199429988861084, acc: 0.45588234066963196)
[2024-11-13 06:11:25,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:25,560][root][INFO] - Training Epoch: 2/10, step 60/574 completed (loss: 2.1900272369384766, acc: 0.4057970941066742)
[2024-11-13 06:11:25,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:26,128][root][INFO] - Training Epoch: 2/10, step 61/574 completed (loss: 1.8353443145751953, acc: 0.5249999761581421)
[2024-11-13 06:11:26,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:26,556][root][INFO] - Training Epoch: 2/10, step 62/574 completed (loss: 1.0265953540802002, acc: 0.7058823704719543)
[2024-11-13 06:11:26,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:26,999][root][INFO] - Training Epoch: 2/10, step 63/574 completed (loss: 1.4388035535812378, acc: 0.6111111044883728)
[2024-11-13 06:11:27,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:27,431][root][INFO] - Training Epoch: 2/10, step 64/574 completed (loss: 1.435117244720459, acc: 0.625)
[2024-11-13 06:11:27,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:27,863][root][INFO] - Training Epoch: 2/10, step 65/574 completed (loss: 0.6995921730995178, acc: 0.7931034564971924)
[2024-11-13 06:11:28,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:28,294][root][INFO] - Training Epoch: 2/10, step 66/574 completed (loss: 2.077420949935913, acc: 0.5)
[2024-11-13 06:11:28,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:28,746][root][INFO] - Training Epoch: 2/10, step 67/574 completed (loss: 1.775450587272644, acc: 0.5833333134651184)
[2024-11-13 06:11:28,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:29,274][root][INFO] - Training Epoch: 2/10, step 68/574 completed (loss: 0.5311925411224365, acc: 0.800000011920929)
[2024-11-13 06:11:29,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:29,720][root][INFO] - Training Epoch: 2/10, step 69/574 completed (loss: 1.232659935951233, acc: 0.6666666865348816)
[2024-11-13 06:11:29,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:30,102][root][INFO] - Training Epoch: 2/10, step 70/574 completed (loss: 1.5182300806045532, acc: 0.5757575631141663)
[2024-11-13 06:11:30,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:30,542][root][INFO] - Training Epoch: 2/10, step 71/574 completed (loss: 1.9497380256652832, acc: 0.5)
[2024-11-13 06:11:30,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:30,970][root][INFO] - Training Epoch: 2/10, step 72/574 completed (loss: 1.7842836380004883, acc: 0.5317460298538208)
[2024-11-13 06:11:31,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:31,425][root][INFO] - Training Epoch: 2/10, step 73/574 completed (loss: 2.238630533218384, acc: 0.4256410300731659)
[2024-11-13 06:11:31,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:31,888][root][INFO] - Training Epoch: 2/10, step 74/574 completed (loss: 1.89006769657135, acc: 0.4591836631298065)
[2024-11-13 06:11:32,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:32,312][root][INFO] - Training Epoch: 2/10, step 75/574 completed (loss: 2.1242926120758057, acc: 0.41791045665740967)
[2024-11-13 06:11:32,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:32,833][root][INFO] - Training Epoch: 2/10, step 76/574 completed (loss: 2.1317150592803955, acc: 0.4343065619468689)
[2024-11-13 06:11:33,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:33,236][root][INFO] - Training Epoch: 2/10, step 77/574 completed (loss: 0.28381800651550293, acc: 0.9047619104385376)
[2024-11-13 06:11:33,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:33,648][root][INFO] - Training Epoch: 2/10, step 78/574 completed (loss: 0.5941417813301086, acc: 0.875)
[2024-11-13 06:11:33,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:34,042][root][INFO] - Training Epoch: 2/10, step 79/574 completed (loss: 0.8820192813873291, acc: 0.7575757503509521)
[2024-11-13 06:11:34,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:34,415][root][INFO] - Training Epoch: 2/10, step 80/574 completed (loss: 0.5653027892112732, acc: 0.8461538553237915)
[2024-11-13 06:11:34,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:34,811][root][INFO] - Training Epoch: 2/10, step 81/574 completed (loss: 1.4747415781021118, acc: 0.6538461446762085)
[2024-11-13 06:11:34,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:35,232][root][INFO] - Training Epoch: 2/10, step 82/574 completed (loss: 1.7028923034667969, acc: 0.5384615659713745)
[2024-11-13 06:11:35,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:35,675][root][INFO] - Training Epoch: 2/10, step 83/574 completed (loss: 0.900026798248291, acc: 0.71875)
[2024-11-13 06:11:35,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:36,060][root][INFO] - Training Epoch: 2/10, step 84/574 completed (loss: 1.363879680633545, acc: 0.6666666865348816)
[2024-11-13 06:11:36,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:36,440][root][INFO] - Training Epoch: 2/10, step 85/574 completed (loss: 1.2588012218475342, acc: 0.6399999856948853)
[2024-11-13 06:11:36,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:36,874][root][INFO] - Training Epoch: 2/10, step 86/574 completed (loss: 0.8347905278205872, acc: 0.739130437374115)
[2024-11-13 06:11:37,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:37,557][root][INFO] - Training Epoch: 2/10, step 87/574 completed (loss: 2.0115127563476562, acc: 0.5199999809265137)
[2024-11-13 06:11:37,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:38,087][root][INFO] - Training Epoch: 2/10, step 88/574 completed (loss: 1.8079270124435425, acc: 0.582524299621582)
[2024-11-13 06:11:39,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:39,842][root][INFO] - Training Epoch: 2/10, step 89/574 completed (loss: 1.7636266946792603, acc: 0.5631067752838135)
[2024-11-13 06:11:40,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:41,133][root][INFO] - Training Epoch: 2/10, step 90/574 completed (loss: 1.9623725414276123, acc: 0.42473119497299194)
[2024-11-13 06:11:41,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:42,386][root][INFO] - Training Epoch: 2/10, step 91/574 completed (loss: 1.7384059429168701, acc: 0.5517241358757019)
[2024-11-13 06:11:42,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:43,542][root][INFO] - Training Epoch: 2/10, step 92/574 completed (loss: 1.4632340669631958, acc: 0.6000000238418579)
[2024-11-13 06:11:44,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:45,126][root][INFO] - Training Epoch: 2/10, step 93/574 completed (loss: 2.2193121910095215, acc: 0.39603960514068604)
[2024-11-13 06:11:45,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:45,480][root][INFO] - Training Epoch: 2/10, step 94/574 completed (loss: 2.0600619316101074, acc: 0.4838709533214569)
[2024-11-13 06:11:45,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:45,916][root][INFO] - Training Epoch: 2/10, step 95/574 completed (loss: 2.087141513824463, acc: 0.43478259444236755)
[2024-11-13 06:11:46,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:46,321][root][INFO] - Training Epoch: 2/10, step 96/574 completed (loss: 2.488340377807617, acc: 0.3361344635486603)
[2024-11-13 06:11:46,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:46,798][root][INFO] - Training Epoch: 2/10, step 97/574 completed (loss: 2.365915536880493, acc: 0.2884615361690521)
[2024-11-13 06:11:47,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:47,302][root][INFO] - Training Epoch: 2/10, step 98/574 completed (loss: 2.3349568843841553, acc: 0.36496350169181824)
[2024-11-13 06:11:47,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:47,772][root][INFO] - Training Epoch: 2/10, step 99/574 completed (loss: 2.341205358505249, acc: 0.3731343150138855)
[2024-11-13 06:11:47,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:48,171][root][INFO] - Training Epoch: 2/10, step 100/574 completed (loss: 1.077733039855957, acc: 0.6499999761581421)
[2024-11-13 06:11:48,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:48,584][root][INFO] - Training Epoch: 2/10, step 101/574 completed (loss: 0.5808143019676208, acc: 0.8181818127632141)
[2024-11-13 06:11:48,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:48,977][root][INFO] - Training Epoch: 2/10, step 102/574 completed (loss: 0.4956517815589905, acc: 0.9130434989929199)
[2024-11-13 06:11:49,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:49,331][root][INFO] - Training Epoch: 2/10, step 103/574 completed (loss: 0.9485712647438049, acc: 0.7727272510528564)
[2024-11-13 06:11:49,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:49,767][root][INFO] - Training Epoch: 2/10, step 104/574 completed (loss: 1.493569254875183, acc: 0.568965494632721)
[2024-11-13 06:11:49,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:50,138][root][INFO] - Training Epoch: 2/10, step 105/574 completed (loss: 1.2026065587997437, acc: 0.6744186282157898)
[2024-11-13 06:11:50,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:50,536][root][INFO] - Training Epoch: 2/10, step 106/574 completed (loss: 0.8996660709381104, acc: 0.800000011920929)
[2024-11-13 06:11:50,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:50,899][root][INFO] - Training Epoch: 2/10, step 107/574 completed (loss: 0.2505437731742859, acc: 0.9411764740943909)
[2024-11-13 06:11:51,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:51,300][root][INFO] - Training Epoch: 2/10, step 108/574 completed (loss: 0.3657737970352173, acc: 0.9230769276618958)
[2024-11-13 06:11:51,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:51,684][root][INFO] - Training Epoch: 2/10, step 109/574 completed (loss: 1.0457674264907837, acc: 0.6666666865348816)
[2024-11-13 06:11:51,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:52,084][root][INFO] - Training Epoch: 2/10, step 110/574 completed (loss: 1.3751615285873413, acc: 0.6000000238418579)
[2024-11-13 06:11:52,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:52,650][root][INFO] - Training Epoch: 2/10, step 111/574 completed (loss: 1.7374300956726074, acc: 0.4912280738353729)
[2024-11-13 06:11:52,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:53,103][root][INFO] - Training Epoch: 2/10, step 112/574 completed (loss: 1.7344955205917358, acc: 0.5438596606254578)
[2024-11-13 06:11:53,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:53,499][root][INFO] - Training Epoch: 2/10, step 113/574 completed (loss: 1.656116247177124, acc: 0.4615384638309479)
[2024-11-13 06:11:53,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:53,973][root][INFO] - Training Epoch: 2/10, step 114/574 completed (loss: 0.7865195274353027, acc: 0.7755101919174194)
[2024-11-13 06:11:54,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:54,420][root][INFO] - Training Epoch: 2/10, step 115/574 completed (loss: 0.8913585543632507, acc: 0.7272727489471436)
[2024-11-13 06:11:54,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:54,835][root][INFO] - Training Epoch: 2/10, step 116/574 completed (loss: 1.4746285676956177, acc: 0.5873016119003296)
[2024-11-13 06:11:54,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:55,211][root][INFO] - Training Epoch: 2/10, step 117/574 completed (loss: 1.7045263051986694, acc: 0.6016260385513306)
[2024-11-13 06:11:55,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:55,698][root][INFO] - Training Epoch: 2/10, step 118/574 completed (loss: 1.4799977540969849, acc: 0.6774193644523621)
[2024-11-13 06:11:56,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:57,029][root][INFO] - Training Epoch: 2/10, step 119/574 completed (loss: 1.8147865533828735, acc: 0.5095056891441345)
[2024-11-13 06:11:57,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:57,446][root][INFO] - Training Epoch: 2/10, step 120/574 completed (loss: 1.270419716835022, acc: 0.6266666650772095)
[2024-11-13 06:11:57,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:58,006][root][INFO] - Training Epoch: 2/10, step 121/574 completed (loss: 1.1594685316085815, acc: 0.7115384340286255)
[2024-11-13 06:11:58,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:58,375][root][INFO] - Training Epoch: 2/10, step 122/574 completed (loss: 0.7544167041778564, acc: 0.75)
[2024-11-13 06:11:58,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:58,749][root][INFO] - Training Epoch: 2/10, step 123/574 completed (loss: 1.0679045915603638, acc: 0.6315789222717285)
[2024-11-13 06:11:58,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:59,148][root][INFO] - Training Epoch: 2/10, step 124/574 completed (loss: 1.9672273397445679, acc: 0.44171780347824097)
[2024-11-13 06:11:59,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:59,600][root][INFO] - Training Epoch: 2/10, step 125/574 completed (loss: 1.992600917816162, acc: 0.4722222089767456)
[2024-11-13 06:11:59,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:11:59,970][root][INFO] - Training Epoch: 2/10, step 126/574 completed (loss: 2.0061416625976562, acc: 0.42500001192092896)
[2024-11-13 06:12:00,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:00,382][root][INFO] - Training Epoch: 2/10, step 127/574 completed (loss: 2.143913507461548, acc: 0.3928571343421936)
[2024-11-13 06:12:00,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:00,813][root][INFO] - Training Epoch: 2/10, step 128/574 completed (loss: 1.7081853151321411, acc: 0.5333333611488342)
[2024-11-13 06:12:01,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:01,361][root][INFO] - Training Epoch: 2/10, step 129/574 completed (loss: 1.8473845720291138, acc: 0.5)
[2024-11-13 06:12:01,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:01,713][root][INFO] - Training Epoch: 2/10, step 130/574 completed (loss: 1.0228253602981567, acc: 0.7307692170143127)
[2024-11-13 06:12:01,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:02,070][root][INFO] - Training Epoch: 2/10, step 131/574 completed (loss: 0.8406432867050171, acc: 0.782608687877655)
[2024-11-13 06:12:02,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:02,439][root][INFO] - Training Epoch: 2/10, step 132/574 completed (loss: 1.6985037326812744, acc: 0.4375)
[2024-11-13 06:12:02,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:02,795][root][INFO] - Training Epoch: 2/10, step 133/574 completed (loss: 1.7539148330688477, acc: 0.52173912525177)
[2024-11-13 06:12:02,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:03,221][root][INFO] - Training Epoch: 2/10, step 134/574 completed (loss: 1.303270936012268, acc: 0.5714285969734192)
[2024-11-13 06:12:03,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:03,608][root][INFO] - Training Epoch: 2/10, step 135/574 completed (loss: 1.1005098819732666, acc: 0.6538461446762085)
[2024-11-13 06:12:03,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:04,041][root][INFO] - Training Epoch: 2/10, step 136/574 completed (loss: 1.6750707626342773, acc: 0.5952380895614624)
[2024-11-13 06:12:04,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:04,466][root][INFO] - Training Epoch: 2/10, step 137/574 completed (loss: 1.5721664428710938, acc: 0.5666666626930237)
[2024-11-13 06:12:04,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:04,843][root][INFO] - Training Epoch: 2/10, step 138/574 completed (loss: 1.558282732963562, acc: 0.52173912525177)
[2024-11-13 06:12:05,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:05,269][root][INFO] - Training Epoch: 2/10, step 139/574 completed (loss: 2.16957688331604, acc: 0.523809552192688)
[2024-11-13 06:12:05,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:05,685][root][INFO] - Training Epoch: 2/10, step 140/574 completed (loss: 1.824966549873352, acc: 0.5)
[2024-11-13 06:12:06,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:07,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:07,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:08,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:08,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:09,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:09,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:10,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:10,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:11,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:12,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:12,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:12,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:13,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:14,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:14,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:15,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:15,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:16,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:16,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:17,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:17,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:18,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:18,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:19,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:19,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:20,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:20,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:20,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:21,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:21,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:22,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:22,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:23,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:23,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:24,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:24,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:25,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:25,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:26,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:26,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:27,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:27,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:28,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:28,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:29,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:29,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:30,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:30,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:31,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:31,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:32,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:32,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:33,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:33,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:34,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:34,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:35,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:35,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:36,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:36,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:37,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:37,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:38,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:38,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:39,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:39,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:40,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:41,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:41,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:42,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:42,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:43,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:43,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:44,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:44,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:44,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:45,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:45,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:46,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:46,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:46,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:47,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:47,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:48,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:49,094][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.3676, device='cuda:0') eval_epoch_loss=tensor(1.4742, device='cuda:0') eval_epoch_acc=tensor(0.5938, device='cuda:0')
[2024-11-13 06:12:49,095][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:12:49,095][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:12:49,512][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_2_step_141_loss_1.4742106199264526/model.pt
[2024-11-13 06:12:49,517][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:12:49,517][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 1.4742106199264526
[2024-11-13 06:12:49,518][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.5938355326652527
[2024-11-13 06:12:49,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:49,915][root][INFO] - Training Epoch: 2/10, step 141/574 completed (loss: 2.0628161430358887, acc: 0.4193548262119293)
[2024-11-13 06:12:50,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:50,344][root][INFO] - Training Epoch: 2/10, step 142/574 completed (loss: 1.7664989233016968, acc: 0.4054054021835327)
[2024-11-13 06:12:50,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:51,127][root][INFO] - Training Epoch: 2/10, step 143/574 completed (loss: 1.5752475261688232, acc: 0.4912280738353729)
[2024-11-13 06:12:51,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:51,593][root][INFO] - Training Epoch: 2/10, step 144/574 completed (loss: 1.3821187019348145, acc: 0.6492537260055542)
[2024-11-13 06:12:51,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:52,058][root][INFO] - Training Epoch: 2/10, step 145/574 completed (loss: 1.695234775543213, acc: 0.5)
[2024-11-13 06:12:52,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:52,691][root][INFO] - Training Epoch: 2/10, step 146/574 completed (loss: 1.917483925819397, acc: 0.478723406791687)
[2024-11-13 06:12:52,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:53,108][root][INFO] - Training Epoch: 2/10, step 147/574 completed (loss: 1.537345051765442, acc: 0.5571428537368774)
[2024-11-13 06:12:53,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:53,542][root][INFO] - Training Epoch: 2/10, step 148/574 completed (loss: 1.7797120809555054, acc: 0.4642857015132904)
[2024-11-13 06:12:53,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:53,981][root][INFO] - Training Epoch: 2/10, step 149/574 completed (loss: 1.3258720636367798, acc: 0.695652186870575)
[2024-11-13 06:12:54,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:54,398][root][INFO] - Training Epoch: 2/10, step 150/574 completed (loss: 1.745517373085022, acc: 0.48275861144065857)
[2024-11-13 06:12:54,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:54,857][root][INFO] - Training Epoch: 2/10, step 151/574 completed (loss: 1.9457639455795288, acc: 0.5652173757553101)
[2024-11-13 06:12:55,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:55,297][root][INFO] - Training Epoch: 2/10, step 152/574 completed (loss: 1.4393541812896729, acc: 0.5932203531265259)
[2024-11-13 06:12:55,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:55,691][root][INFO] - Training Epoch: 2/10, step 153/574 completed (loss: 2.1624598503112793, acc: 0.4385964870452881)
[2024-11-13 06:12:55,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:56,110][root][INFO] - Training Epoch: 2/10, step 154/574 completed (loss: 1.4039912223815918, acc: 0.662162184715271)
[2024-11-13 06:12:56,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:56,529][root][INFO] - Training Epoch: 2/10, step 155/574 completed (loss: 1.213686227798462, acc: 0.75)
[2024-11-13 06:12:56,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:56,984][root][INFO] - Training Epoch: 2/10, step 156/574 completed (loss: 1.0817564725875854, acc: 0.695652186870575)
[2024-11-13 06:12:57,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:12:57,383][root][INFO] - Training Epoch: 2/10, step 157/574 completed (loss: 2.266397476196289, acc: 0.42105263471603394)
[2024-11-13 06:12:59,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:00,019][root][INFO] - Training Epoch: 2/10, step 158/574 completed (loss: 1.7234820127487183, acc: 0.5)
[2024-11-13 06:13:00,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:00,393][root][INFO] - Training Epoch: 2/10, step 159/574 completed (loss: 1.9486596584320068, acc: 0.5)
[2024-11-13 06:13:00,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:00,953][root][INFO] - Training Epoch: 2/10, step 160/574 completed (loss: 2.0653154850006104, acc: 0.45348837971687317)
[2024-11-13 06:13:01,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:01,860][root][INFO] - Training Epoch: 2/10, step 161/574 completed (loss: 1.9763226509094238, acc: 0.43529412150382996)
[2024-11-13 06:13:02,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:02,717][root][INFO] - Training Epoch: 2/10, step 162/574 completed (loss: 2.0185868740081787, acc: 0.42696627974510193)
[2024-11-13 06:13:02,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:03,093][root][INFO] - Training Epoch: 2/10, step 163/574 completed (loss: 1.2045040130615234, acc: 0.7045454382896423)
[2024-11-13 06:13:03,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:03,476][root][INFO] - Training Epoch: 2/10, step 164/574 completed (loss: 1.0816221237182617, acc: 0.7142857313156128)
[2024-11-13 06:13:03,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:03,835][root][INFO] - Training Epoch: 2/10, step 165/574 completed (loss: 1.4485702514648438, acc: 0.6551724076271057)
[2024-11-13 06:13:04,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:04,236][root][INFO] - Training Epoch: 2/10, step 166/574 completed (loss: 0.8913660645484924, acc: 0.6938775777816772)
[2024-11-13 06:13:04,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:04,601][root][INFO] - Training Epoch: 2/10, step 167/574 completed (loss: 1.1084574460983276, acc: 0.699999988079071)
[2024-11-13 06:13:04,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:05,201][root][INFO] - Training Epoch: 2/10, step 168/574 completed (loss: 1.5052965879440308, acc: 0.5833333134651184)
[2024-11-13 06:13:05,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:05,674][root][INFO] - Training Epoch: 2/10, step 169/574 completed (loss: 1.8356750011444092, acc: 0.529411792755127)
[2024-11-13 06:13:06,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:07,368][root][INFO] - Training Epoch: 2/10, step 170/574 completed (loss: 2.161423683166504, acc: 0.45205479860305786)
[2024-11-13 06:13:07,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:07,825][root][INFO] - Training Epoch: 2/10, step 171/574 completed (loss: 0.9600849151611328, acc: 0.75)
[2024-11-13 06:13:07,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:08,210][root][INFO] - Training Epoch: 2/10, step 172/574 completed (loss: 1.1235265731811523, acc: 0.6296296119689941)
[2024-11-13 06:13:08,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:08,633][root][INFO] - Training Epoch: 2/10, step 173/574 completed (loss: 1.180983543395996, acc: 0.6071428656578064)
[2024-11-13 06:13:09,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:09,442][root][INFO] - Training Epoch: 2/10, step 174/574 completed (loss: 1.7569268941879272, acc: 0.5486725568771362)
[2024-11-13 06:13:09,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:09,797][root][INFO] - Training Epoch: 2/10, step 175/574 completed (loss: 1.517338514328003, acc: 0.5942028760910034)
[2024-11-13 06:13:09,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:10,189][root][INFO] - Training Epoch: 2/10, step 176/574 completed (loss: 1.5449761152267456, acc: 0.5909090638160706)
[2024-11-13 06:13:10,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:11,641][root][INFO] - Training Epoch: 2/10, step 177/574 completed (loss: 2.1369292736053467, acc: 0.39694657921791077)
[2024-11-13 06:13:12,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:12,677][root][INFO] - Training Epoch: 2/10, step 178/574 completed (loss: 1.7655514478683472, acc: 0.4888888895511627)
[2024-11-13 06:13:12,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:13,063][root][INFO] - Training Epoch: 2/10, step 179/574 completed (loss: 1.0538780689239502, acc: 0.688524603843689)
[2024-11-13 06:13:13,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:13,500][root][INFO] - Training Epoch: 2/10, step 180/574 completed (loss: 0.24258320033550262, acc: 0.9166666865348816)
[2024-11-13 06:13:13,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:13,856][root][INFO] - Training Epoch: 2/10, step 181/574 completed (loss: 0.7889702320098877, acc: 0.8399999737739563)
[2024-11-13 06:13:14,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:14,236][root][INFO] - Training Epoch: 2/10, step 182/574 completed (loss: 0.4297533929347992, acc: 0.8571428656578064)
[2024-11-13 06:13:14,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:14,619][root][INFO] - Training Epoch: 2/10, step 183/574 completed (loss: 1.2376151084899902, acc: 0.6341463327407837)
[2024-11-13 06:13:14,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:15,033][root][INFO] - Training Epoch: 2/10, step 184/574 completed (loss: 1.740837574005127, acc: 0.5468277931213379)
[2024-11-13 06:13:15,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:15,423][root][INFO] - Training Epoch: 2/10, step 185/574 completed (loss: 1.686740517616272, acc: 0.5417867302894592)
[2024-11-13 06:13:15,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:16,110][root][INFO] - Training Epoch: 2/10, step 186/574 completed (loss: 1.7324975728988647, acc: 0.5562499761581421)
[2024-11-13 06:13:16,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:16,841][root][INFO] - Training Epoch: 2/10, step 187/574 completed (loss: 1.662267804145813, acc: 0.5534709095954895)
[2024-11-13 06:13:17,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:17,414][root][INFO] - Training Epoch: 2/10, step 188/574 completed (loss: 1.6264232397079468, acc: 0.5729537606239319)
[2024-11-13 06:13:17,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:17,777][root][INFO] - Training Epoch: 2/10, step 189/574 completed (loss: 1.4211093187332153, acc: 0.7200000286102295)
[2024-11-13 06:13:18,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:18,648][root][INFO] - Training Epoch: 2/10, step 190/574 completed (loss: 1.9554604291915894, acc: 0.43023255467414856)
[2024-11-13 06:13:19,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:19,964][root][INFO] - Training Epoch: 2/10, step 191/574 completed (loss: 1.9555081129074097, acc: 0.4761904776096344)
[2024-11-13 06:13:21,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:21,670][root][INFO] - Training Epoch: 2/10, step 192/574 completed (loss: 1.6871763467788696, acc: 0.469696968793869)
[2024-11-13 06:13:22,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:22,860][root][INFO] - Training Epoch: 2/10, step 193/574 completed (loss: 1.2958589792251587, acc: 0.6470588445663452)
[2024-11-13 06:13:23,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:24,688][root][INFO] - Training Epoch: 2/10, step 194/574 completed (loss: 1.5821071863174438, acc: 0.5370370149612427)
[2024-11-13 06:13:25,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:26,387][root][INFO] - Training Epoch: 2/10, step 195/574 completed (loss: 1.4384348392486572, acc: 0.5483871102333069)
[2024-11-13 06:13:26,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:26,833][root][INFO] - Training Epoch: 2/10, step 196/574 completed (loss: 0.5126810669898987, acc: 0.8214285969734192)
[2024-11-13 06:13:27,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:27,231][root][INFO] - Training Epoch: 2/10, step 197/574 completed (loss: 1.394136905670166, acc: 0.574999988079071)
[2024-11-13 06:13:27,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:27,617][root][INFO] - Training Epoch: 2/10, step 198/574 completed (loss: 1.4198179244995117, acc: 0.6029411554336548)
[2024-11-13 06:13:27,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:28,070][root][INFO] - Training Epoch: 2/10, step 199/574 completed (loss: 1.4953595399856567, acc: 0.5661764740943909)
[2024-11-13 06:13:28,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:28,490][root][INFO] - Training Epoch: 2/10, step 200/574 completed (loss: 1.6051753759384155, acc: 0.6101694703102112)
[2024-11-13 06:13:28,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:28,964][root][INFO] - Training Epoch: 2/10, step 201/574 completed (loss: 1.6878652572631836, acc: 0.60447758436203)
[2024-11-13 06:13:29,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:29,412][root][INFO] - Training Epoch: 2/10, step 202/574 completed (loss: 1.5541744232177734, acc: 0.5339806079864502)
[2024-11-13 06:13:29,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:29,803][root][INFO] - Training Epoch: 2/10, step 203/574 completed (loss: 1.6125810146331787, acc: 0.6190476417541504)
[2024-11-13 06:13:29,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:30,164][root][INFO] - Training Epoch: 2/10, step 204/574 completed (loss: 1.1517239809036255, acc: 0.7032967209815979)
[2024-11-13 06:13:30,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:30,574][root][INFO] - Training Epoch: 2/10, step 205/574 completed (loss: 1.2742356061935425, acc: 0.6233183741569519)
[2024-11-13 06:13:30,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:31,113][root][INFO] - Training Epoch: 2/10, step 206/574 completed (loss: 1.5851771831512451, acc: 0.586614191532135)
[2024-11-13 06:13:31,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:31,530][root][INFO] - Training Epoch: 2/10, step 207/574 completed (loss: 1.202901005744934, acc: 0.681034505367279)
[2024-11-13 06:13:31,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:31,969][root][INFO] - Training Epoch: 2/10, step 208/574 completed (loss: 1.1145074367523193, acc: 0.6992753744125366)
[2024-11-13 06:13:32,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:32,438][root][INFO] - Training Epoch: 2/10, step 209/574 completed (loss: 1.3150659799575806, acc: 0.618677020072937)
[2024-11-13 06:13:32,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:32,818][root][INFO] - Training Epoch: 2/10, step 210/574 completed (loss: 1.375733733177185, acc: 0.6521739363670349)
[2024-11-13 06:13:32,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:33,186][root][INFO] - Training Epoch: 2/10, step 211/574 completed (loss: 0.6450642347335815, acc: 0.739130437374115)
[2024-11-13 06:13:33,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:33,551][root][INFO] - Training Epoch: 2/10, step 212/574 completed (loss: 1.1553715467453003, acc: 0.5714285969734192)
[2024-11-13 06:13:33,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:33,957][root][INFO] - Training Epoch: 2/10, step 213/574 completed (loss: 0.9268580675125122, acc: 0.7234042286872864)
[2024-11-13 06:13:34,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:35,057][root][INFO] - Training Epoch: 2/10, step 214/574 completed (loss: 0.9420790076255798, acc: 0.7153846025466919)
[2024-11-13 06:13:35,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:35,475][root][INFO] - Training Epoch: 2/10, step 215/574 completed (loss: 0.656217634677887, acc: 0.7837837934494019)
[2024-11-13 06:13:35,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:35,918][root][INFO] - Training Epoch: 2/10, step 216/574 completed (loss: 0.5834075808525085, acc: 0.8372092843055725)
[2024-11-13 06:13:36,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:36,734][root][INFO] - Training Epoch: 2/10, step 217/574 completed (loss: 0.8635237812995911, acc: 0.7477477192878723)
[2024-11-13 06:13:36,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:37,301][root][INFO] - Training Epoch: 2/10, step 218/574 completed (loss: 0.7199439406394958, acc: 0.800000011920929)
[2024-11-13 06:13:37,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:37,697][root][INFO] - Training Epoch: 2/10, step 219/574 completed (loss: 0.3605973720550537, acc: 0.939393937587738)
[2024-11-13 06:13:37,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:38,139][root][INFO] - Training Epoch: 2/10, step 220/574 completed (loss: 0.39677301049232483, acc: 0.8888888955116272)
[2024-11-13 06:13:38,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:38,486][root][INFO] - Training Epoch: 2/10, step 221/574 completed (loss: 0.5159180164337158, acc: 0.8799999952316284)
[2024-11-13 06:13:38,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:38,862][root][INFO] - Training Epoch: 2/10, step 222/574 completed (loss: 1.2976654767990112, acc: 0.6153846383094788)
[2024-11-13 06:13:39,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:40,071][root][INFO] - Training Epoch: 2/10, step 223/574 completed (loss: 1.2328227758407593, acc: 0.6413043737411499)
[2024-11-13 06:13:40,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:40,889][root][INFO] - Training Epoch: 2/10, step 224/574 completed (loss: 1.201146125793457, acc: 0.6534090638160706)
[2024-11-13 06:13:41,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:41,515][root][INFO] - Training Epoch: 2/10, step 225/574 completed (loss: 1.7761623859405518, acc: 0.5106382966041565)
[2024-11-13 06:13:41,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:41,974][root][INFO] - Training Epoch: 2/10, step 226/574 completed (loss: 1.1785802841186523, acc: 0.6603773832321167)
[2024-11-13 06:13:42,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:42,428][root][INFO] - Training Epoch: 2/10, step 227/574 completed (loss: 0.9194767475128174, acc: 0.75)
[2024-11-13 06:13:42,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:42,862][root][INFO] - Training Epoch: 2/10, step 228/574 completed (loss: 0.6467251777648926, acc: 0.7441860437393188)
[2024-11-13 06:13:43,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:43,236][root][INFO] - Training Epoch: 2/10, step 229/574 completed (loss: 1.734674334526062, acc: 0.5333333611488342)
[2024-11-13 06:13:43,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:43,716][root][INFO] - Training Epoch: 2/10, step 230/574 completed (loss: 2.562614679336548, acc: 0.35789474844932556)
[2024-11-13 06:13:43,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:44,085][root][INFO] - Training Epoch: 2/10, step 231/574 completed (loss: 1.9106109142303467, acc: 0.5333333611488342)
[2024-11-13 06:13:44,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:44,721][root][INFO] - Training Epoch: 2/10, step 232/574 completed (loss: 1.8479119539260864, acc: 0.5444444417953491)
[2024-11-13 06:13:45,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:45,458][root][INFO] - Training Epoch: 2/10, step 233/574 completed (loss: 1.8313325643539429, acc: 0.5229358077049255)
[2024-11-13 06:13:45,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:46,161][root][INFO] - Training Epoch: 2/10, step 234/574 completed (loss: 1.8396986722946167, acc: 0.5615384578704834)
[2024-11-13 06:13:46,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:46,629][root][INFO] - Training Epoch: 2/10, step 235/574 completed (loss: 1.2706210613250732, acc: 0.7368420958518982)
[2024-11-13 06:13:46,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:46,984][root][INFO] - Training Epoch: 2/10, step 236/574 completed (loss: 1.596995234489441, acc: 0.5)
[2024-11-13 06:13:47,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:47,356][root][INFO] - Training Epoch: 2/10, step 237/574 completed (loss: 2.5631802082061768, acc: 0.5)
[2024-11-13 06:13:47,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:47,759][root][INFO] - Training Epoch: 2/10, step 238/574 completed (loss: 1.5762006044387817, acc: 0.5925925970077515)
[2024-11-13 06:13:47,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:48,219][root][INFO] - Training Epoch: 2/10, step 239/574 completed (loss: 1.231930136680603, acc: 0.6857143044471741)
[2024-11-13 06:13:48,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:48,722][root][INFO] - Training Epoch: 2/10, step 240/574 completed (loss: 1.2662920951843262, acc: 0.6818181872367859)
[2024-11-13 06:13:48,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:49,157][root][INFO] - Training Epoch: 2/10, step 241/574 completed (loss: 1.3297882080078125, acc: 0.6590909361839294)
[2024-11-13 06:13:49,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:50,068][root][INFO] - Training Epoch: 2/10, step 242/574 completed (loss: 1.9252110719680786, acc: 0.4032258093357086)
[2024-11-13 06:13:50,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:50,901][root][INFO] - Training Epoch: 2/10, step 243/574 completed (loss: 1.6627177000045776, acc: 0.5227272510528564)
[2024-11-13 06:13:51,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:51,304][root][INFO] - Training Epoch: 2/10, step 244/574 completed (loss: 0.5399061441421509, acc: 0.8571428656578064)
[2024-11-13 06:13:51,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:51,719][root][INFO] - Training Epoch: 2/10, step 245/574 completed (loss: 1.406510829925537, acc: 0.5)
[2024-11-13 06:13:51,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:52,114][root][INFO] - Training Epoch: 2/10, step 246/574 completed (loss: 1.4818189144134521, acc: 0.6451612710952759)
[2024-11-13 06:13:52,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:52,500][root][INFO] - Training Epoch: 2/10, step 247/574 completed (loss: 0.5830172300338745, acc: 0.8500000238418579)
[2024-11-13 06:13:52,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:52,947][root][INFO] - Training Epoch: 2/10, step 248/574 completed (loss: 0.9940252304077148, acc: 0.6756756901741028)
[2024-11-13 06:13:53,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:53,333][root][INFO] - Training Epoch: 2/10, step 249/574 completed (loss: 1.0859769582748413, acc: 0.6756756901741028)
[2024-11-13 06:13:53,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:53,769][root][INFO] - Training Epoch: 2/10, step 250/574 completed (loss: 0.937362551689148, acc: 0.7567567825317383)
[2024-11-13 06:13:53,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:54,193][root][INFO] - Training Epoch: 2/10, step 251/574 completed (loss: 1.063238501548767, acc: 0.6764705777168274)
[2024-11-13 06:13:54,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:54,638][root][INFO] - Training Epoch: 2/10, step 252/574 completed (loss: 0.3342275321483612, acc: 0.9268292784690857)
[2024-11-13 06:13:54,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:55,062][root][INFO] - Training Epoch: 2/10, step 253/574 completed (loss: 0.45621275901794434, acc: 0.8799999952316284)
[2024-11-13 06:13:55,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:55,512][root][INFO] - Training Epoch: 2/10, step 254/574 completed (loss: 0.16056641936302185, acc: 1.0)
[2024-11-13 06:13:55,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:55,907][root][INFO] - Training Epoch: 2/10, step 255/574 completed (loss: 0.7729735970497131, acc: 0.774193525314331)
[2024-11-13 06:13:56,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:56,307][root][INFO] - Training Epoch: 2/10, step 256/574 completed (loss: 0.7821328639984131, acc: 0.7719298005104065)
[2024-11-13 06:13:56,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:56,740][root][INFO] - Training Epoch: 2/10, step 257/574 completed (loss: 0.8778231143951416, acc: 0.800000011920929)
[2024-11-13 06:13:56,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:57,129][root][INFO] - Training Epoch: 2/10, step 258/574 completed (loss: 0.7597004771232605, acc: 0.8026315569877625)
[2024-11-13 06:13:57,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:57,999][root][INFO] - Training Epoch: 2/10, step 259/574 completed (loss: 1.1730905771255493, acc: 0.6603773832321167)
[2024-11-13 06:13:58,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:58,891][root][INFO] - Training Epoch: 2/10, step 260/574 completed (loss: 1.2627745866775513, acc: 0.6666666865348816)
[2024-11-13 06:13:59,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:59,325][root][INFO] - Training Epoch: 2/10, step 261/574 completed (loss: 0.7356818318367004, acc: 0.75)
[2024-11-13 06:13:59,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:13:59,758][root][INFO] - Training Epoch: 2/10, step 262/574 completed (loss: 1.0789259672164917, acc: 0.7419354915618896)
[2024-11-13 06:13:59,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:00,199][root][INFO] - Training Epoch: 2/10, step 263/574 completed (loss: 2.0545601844787598, acc: 0.4933333396911621)
[2024-11-13 06:14:00,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:00,630][root][INFO] - Training Epoch: 2/10, step 264/574 completed (loss: 1.3217816352844238, acc: 0.6041666865348816)
[2024-11-13 06:14:01,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:01,936][root][INFO] - Training Epoch: 2/10, step 265/574 completed (loss: 2.0153632164001465, acc: 0.41600000858306885)
[2024-11-13 06:14:02,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:02,344][root][INFO] - Training Epoch: 2/10, step 266/574 completed (loss: 1.5961875915527344, acc: 0.5730336904525757)
[2024-11-13 06:14:02,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:02,802][root][INFO] - Training Epoch: 2/10, step 267/574 completed (loss: 1.6663349866867065, acc: 0.5540540814399719)
[2024-11-13 06:14:03,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:03,462][root][INFO] - Training Epoch: 2/10, step 268/574 completed (loss: 1.226416826248169, acc: 0.6206896305084229)
[2024-11-13 06:14:03,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:03,892][root][INFO] - Training Epoch: 2/10, step 269/574 completed (loss: 0.8167199492454529, acc: 0.6818181872367859)
[2024-11-13 06:14:04,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:04,274][root][INFO] - Training Epoch: 2/10, step 270/574 completed (loss: 0.42564013600349426, acc: 0.8636363744735718)
[2024-11-13 06:14:04,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:04,710][root][INFO] - Training Epoch: 2/10, step 271/574 completed (loss: 0.4848387539386749, acc: 0.78125)
[2024-11-13 06:14:04,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:05,131][root][INFO] - Training Epoch: 2/10, step 272/574 completed (loss: 0.43859535455703735, acc: 0.8999999761581421)
[2024-11-13 06:14:05,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:05,641][root][INFO] - Training Epoch: 2/10, step 273/574 completed (loss: 1.2510961294174194, acc: 0.6666666865348816)
[2024-11-13 06:14:05,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:06,030][root][INFO] - Training Epoch: 2/10, step 274/574 completed (loss: 0.7862226963043213, acc: 0.78125)
[2024-11-13 06:14:06,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:06,428][root][INFO] - Training Epoch: 2/10, step 275/574 completed (loss: 0.6789163947105408, acc: 0.8666666746139526)
[2024-11-13 06:14:06,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:06,890][root][INFO] - Training Epoch: 2/10, step 276/574 completed (loss: 0.8821632266044617, acc: 0.7241379022598267)
[2024-11-13 06:14:07,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:07,291][root][INFO] - Training Epoch: 2/10, step 277/574 completed (loss: 0.43497657775878906, acc: 0.8799999952316284)
[2024-11-13 06:14:07,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:07,672][root][INFO] - Training Epoch: 2/10, step 278/574 completed (loss: 1.5004186630249023, acc: 0.5531914830207825)
[2024-11-13 06:14:07,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:08,111][root][INFO] - Training Epoch: 2/10, step 279/574 completed (loss: 1.0121368169784546, acc: 0.7291666865348816)
[2024-11-13 06:14:08,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:08,484][root][INFO] - Training Epoch: 2/10, step 280/574 completed (loss: 0.5614483952522278, acc: 0.9090909361839294)
[2024-11-13 06:14:08,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:09,060][root][INFO] - Training Epoch: 2/10, step 281/574 completed (loss: 1.2310817241668701, acc: 0.6987951993942261)
[2024-11-13 06:14:09,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:09,525][root][INFO] - Training Epoch: 2/10, step 282/574 completed (loss: 1.5248016119003296, acc: 0.5648148059844971)
[2024-11-13 06:14:09,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:09,877][root][INFO] - Training Epoch: 2/10, step 283/574 completed (loss: 1.415116548538208, acc: 0.6315789222717285)
[2024-11-13 06:14:10,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:11,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:11,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:12,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:12,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:13,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:13,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:14,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:14,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:15,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:15,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:16,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:16,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:17,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:17,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:18,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:18,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:19,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:19,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:20,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:21,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:21,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:22,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:22,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:23,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:23,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:24,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:24,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:25,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:25,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:26,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:26,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:27,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:27,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:28,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:28,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:29,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:29,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:30,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:30,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:31,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:31,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:32,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:32,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:33,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:33,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:34,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:35,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:35,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:35,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:36,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:36,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:37,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:37,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:38,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:38,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:39,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:39,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:40,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:40,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:41,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:42,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:42,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:43,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:43,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:44,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:44,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:45,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:46,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:46,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:47,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:47,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:48,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:48,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:49,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:49,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:50,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:50,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:51,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:52,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:52,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:53,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:53,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:54,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:54,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:55,296][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.8999, device='cuda:0') eval_epoch_loss=tensor(1.0647, device='cuda:0') eval_epoch_acc=tensor(0.6929, device='cuda:0')
[2024-11-13 06:14:55,298][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:14:55,298][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:14:55,693][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_2_step_284_loss_1.064674973487854/model.pt
[2024-11-13 06:14:55,701][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:14:55,701][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 1.064674973487854
[2024-11-13 06:14:55,702][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.6929380297660828
[2024-11-13 06:14:55,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:56,149][root][INFO] - Training Epoch: 2/10, step 284/574 completed (loss: 1.2355492115020752, acc: 0.7352941036224365)
[2024-11-13 06:14:56,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:56,554][root][INFO] - Training Epoch: 2/10, step 285/574 completed (loss: 0.7745351195335388, acc: 0.7749999761581421)
[2024-11-13 06:14:56,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:56,948][root][INFO] - Training Epoch: 2/10, step 286/574 completed (loss: 1.1231567859649658, acc: 0.65625)
[2024-11-13 06:14:57,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:57,366][root][INFO] - Training Epoch: 2/10, step 287/574 completed (loss: 1.448914885520935, acc: 0.5839999914169312)
[2024-11-13 06:14:57,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:57,787][root][INFO] - Training Epoch: 2/10, step 288/574 completed (loss: 0.9136740565299988, acc: 0.7472527623176575)
[2024-11-13 06:14:57,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:58,178][root][INFO] - Training Epoch: 2/10, step 289/574 completed (loss: 1.1006931066513062, acc: 0.6832298040390015)
[2024-11-13 06:14:58,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:58,656][root][INFO] - Training Epoch: 2/10, step 290/574 completed (loss: 1.3605107069015503, acc: 0.6649484634399414)
[2024-11-13 06:14:58,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:59,038][root][INFO] - Training Epoch: 2/10, step 291/574 completed (loss: 0.5450130701065063, acc: 0.8181818127632141)
[2024-11-13 06:14:59,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:59,474][root][INFO] - Training Epoch: 2/10, step 292/574 completed (loss: 1.255152702331543, acc: 0.5714285969734192)
[2024-11-13 06:14:59,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:14:59,915][root][INFO] - Training Epoch: 2/10, step 293/574 completed (loss: 0.8154387474060059, acc: 0.7413793206214905)
[2024-11-13 06:15:00,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:00,677][root][INFO] - Training Epoch: 2/10, step 294/574 completed (loss: 1.0144059658050537, acc: 0.6909090876579285)
[2024-11-13 06:15:01,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:01,512][root][INFO] - Training Epoch: 2/10, step 295/574 completed (loss: 1.3338292837142944, acc: 0.6597937941551208)
[2024-11-13 06:15:01,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:01,909][root][INFO] - Training Epoch: 2/10, step 296/574 completed (loss: 1.5370427370071411, acc: 0.568965494632721)
[2024-11-13 06:15:02,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:02,292][root][INFO] - Training Epoch: 2/10, step 297/574 completed (loss: 0.782807469367981, acc: 0.8148148059844971)
[2024-11-13 06:15:02,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:02,712][root][INFO] - Training Epoch: 2/10, step 298/574 completed (loss: 1.5245565176010132, acc: 0.6052631735801697)
[2024-11-13 06:15:02,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:03,099][root][INFO] - Training Epoch: 2/10, step 299/574 completed (loss: 0.7705613374710083, acc: 0.7857142686843872)
[2024-11-13 06:15:03,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:03,422][root][INFO] - Training Epoch: 2/10, step 300/574 completed (loss: 0.4463643729686737, acc: 0.90625)
[2024-11-13 06:15:03,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:03,810][root][INFO] - Training Epoch: 2/10, step 301/574 completed (loss: 1.186141848564148, acc: 0.7735849022865295)
[2024-11-13 06:15:03,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:04,217][root][INFO] - Training Epoch: 2/10, step 302/574 completed (loss: 0.23313334584236145, acc: 0.9433962106704712)
[2024-11-13 06:15:04,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:04,608][root][INFO] - Training Epoch: 2/10, step 303/574 completed (loss: 0.5767598152160645, acc: 0.8529411554336548)
[2024-11-13 06:15:04,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:05,014][root][INFO] - Training Epoch: 2/10, step 304/574 completed (loss: 0.9340237379074097, acc: 0.6875)
[2024-11-13 06:15:05,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:05,440][root][INFO] - Training Epoch: 2/10, step 305/574 completed (loss: 0.9501020312309265, acc: 0.7704917788505554)
[2024-11-13 06:15:05,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:05,798][root][INFO] - Training Epoch: 2/10, step 306/574 completed (loss: 0.3695589601993561, acc: 0.8999999761581421)
[2024-11-13 06:15:05,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:06,151][root][INFO] - Training Epoch: 2/10, step 307/574 completed (loss: 0.242561936378479, acc: 0.8947368264198303)
[2024-11-13 06:15:06,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:06,516][root][INFO] - Training Epoch: 2/10, step 308/574 completed (loss: 1.2353578805923462, acc: 0.695652186870575)
[2024-11-13 06:15:06,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:07,106][root][INFO] - Training Epoch: 2/10, step 309/574 completed (loss: 1.2381354570388794, acc: 0.6805555820465088)
[2024-11-13 06:15:07,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:07,471][root][INFO] - Training Epoch: 2/10, step 310/574 completed (loss: 0.9994378089904785, acc: 0.7108433842658997)
[2024-11-13 06:15:07,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:07,832][root][INFO] - Training Epoch: 2/10, step 311/574 completed (loss: 1.082485556602478, acc: 0.6410256624221802)
[2024-11-13 06:15:08,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:08,284][root][INFO] - Training Epoch: 2/10, step 312/574 completed (loss: 0.9875590205192566, acc: 0.7244898080825806)
[2024-11-13 06:15:08,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:08,714][root][INFO] - Training Epoch: 2/10, step 313/574 completed (loss: 0.14699888229370117, acc: 0.9583333134651184)
[2024-11-13 06:15:08,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:09,109][root][INFO] - Training Epoch: 2/10, step 314/574 completed (loss: 0.8873971104621887, acc: 0.75)
[2024-11-13 06:15:09,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:09,439][root][INFO] - Training Epoch: 2/10, step 315/574 completed (loss: 0.5513310432434082, acc: 0.9032257795333862)
[2024-11-13 06:15:09,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:09,840][root][INFO] - Training Epoch: 2/10, step 316/574 completed (loss: 0.8351147770881653, acc: 0.8064516186714172)
[2024-11-13 06:15:10,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:10,267][root][INFO] - Training Epoch: 2/10, step 317/574 completed (loss: 0.6091019511222839, acc: 0.8358209133148193)
[2024-11-13 06:15:10,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:10,627][root][INFO] - Training Epoch: 2/10, step 318/574 completed (loss: 0.5448181629180908, acc: 0.8461538553237915)
[2024-11-13 06:15:10,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:10,974][root][INFO] - Training Epoch: 2/10, step 319/574 completed (loss: 0.7238843441009521, acc: 0.7777777910232544)
[2024-11-13 06:15:11,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:11,376][root][INFO] - Training Epoch: 2/10, step 320/574 completed (loss: 0.5402715802192688, acc: 0.8225806355476379)
[2024-11-13 06:15:11,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:11,759][root][INFO] - Training Epoch: 2/10, step 321/574 completed (loss: 0.1523522436618805, acc: 0.9599999785423279)
[2024-11-13 06:15:11,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:12,155][root][INFO] - Training Epoch: 2/10, step 322/574 completed (loss: 1.8682141304016113, acc: 0.37037035822868347)
[2024-11-13 06:15:12,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:12,514][root][INFO] - Training Epoch: 2/10, step 323/574 completed (loss: 3.035839796066284, acc: 0.22857142984867096)
[2024-11-13 06:15:12,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:12,905][root][INFO] - Training Epoch: 2/10, step 324/574 completed (loss: 2.3790767192840576, acc: 0.3589743673801422)
[2024-11-13 06:15:13,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:13,250][root][INFO] - Training Epoch: 2/10, step 325/574 completed (loss: 2.4266726970672607, acc: 0.3658536672592163)
[2024-11-13 06:15:13,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:13,690][root][INFO] - Training Epoch: 2/10, step 326/574 completed (loss: 2.07035493850708, acc: 0.3947368562221527)
[2024-11-13 06:15:13,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:14,108][root][INFO] - Training Epoch: 2/10, step 327/574 completed (loss: 0.8937248587608337, acc: 0.7368420958518982)
[2024-11-13 06:15:14,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:14,540][root][INFO] - Training Epoch: 2/10, step 328/574 completed (loss: 0.40054959058761597, acc: 0.8571428656578064)
[2024-11-13 06:15:14,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:14,949][root][INFO] - Training Epoch: 2/10, step 329/574 completed (loss: 1.0807204246520996, acc: 0.6296296119689941)
[2024-11-13 06:15:15,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:15,306][root][INFO] - Training Epoch: 2/10, step 330/574 completed (loss: 0.41570013761520386, acc: 0.90625)
[2024-11-13 06:15:15,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:15,646][root][INFO] - Training Epoch: 2/10, step 331/574 completed (loss: 1.0085725784301758, acc: 0.6935483813285828)
[2024-11-13 06:15:15,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:16,118][root][INFO] - Training Epoch: 2/10, step 332/574 completed (loss: 0.6606674790382385, acc: 0.7894737124443054)
[2024-11-13 06:15:16,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:16,477][root][INFO] - Training Epoch: 2/10, step 333/574 completed (loss: 1.077248454093933, acc: 0.71875)
[2024-11-13 06:15:16,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:16,897][root][INFO] - Training Epoch: 2/10, step 334/574 completed (loss: 0.42362308502197266, acc: 0.8666666746139526)
[2024-11-13 06:15:17,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:17,240][root][INFO] - Training Epoch: 2/10, step 335/574 completed (loss: 0.884067952632904, acc: 0.6842105388641357)
[2024-11-13 06:15:17,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:17,621][root][INFO] - Training Epoch: 2/10, step 336/574 completed (loss: 1.5288517475128174, acc: 0.6399999856948853)
[2024-11-13 06:15:17,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:18,048][root][INFO] - Training Epoch: 2/10, step 337/574 completed (loss: 1.9234100580215454, acc: 0.48275861144065857)
[2024-11-13 06:15:18,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:18,468][root][INFO] - Training Epoch: 2/10, step 338/574 completed (loss: 1.9034658670425415, acc: 0.43617022037506104)
[2024-11-13 06:15:18,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:18,851][root][INFO] - Training Epoch: 2/10, step 339/574 completed (loss: 1.7452378273010254, acc: 0.5662650465965271)
[2024-11-13 06:15:19,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:19,302][root][INFO] - Training Epoch: 2/10, step 340/574 completed (loss: 0.5464155077934265, acc: 0.8695651888847351)
[2024-11-13 06:15:19,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:19,680][root][INFO] - Training Epoch: 2/10, step 341/574 completed (loss: 1.2583861351013184, acc: 0.6666666865348816)
[2024-11-13 06:15:19,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:20,190][root][INFO] - Training Epoch: 2/10, step 342/574 completed (loss: 1.2854094505310059, acc: 0.6626505851745605)
[2024-11-13 06:15:20,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:20,598][root][INFO] - Training Epoch: 2/10, step 343/574 completed (loss: 1.1994894742965698, acc: 0.6792452931404114)
[2024-11-13 06:15:20,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:21,012][root][INFO] - Training Epoch: 2/10, step 344/574 completed (loss: 0.8198930025100708, acc: 0.7215189933776855)
[2024-11-13 06:15:21,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:21,393][root][INFO] - Training Epoch: 2/10, step 345/574 completed (loss: 0.6727486252784729, acc: 0.843137264251709)
[2024-11-13 06:15:21,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:21,790][root][INFO] - Training Epoch: 2/10, step 346/574 completed (loss: 1.4119690656661987, acc: 0.6268656849861145)
[2024-11-13 06:15:21,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:22,140][root][INFO] - Training Epoch: 2/10, step 347/574 completed (loss: 0.5185778737068176, acc: 0.8999999761581421)
[2024-11-13 06:15:22,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:22,574][root][INFO] - Training Epoch: 2/10, step 348/574 completed (loss: 0.7244861721992493, acc: 0.7200000286102295)
[2024-11-13 06:15:22,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:23,113][root][INFO] - Training Epoch: 2/10, step 349/574 completed (loss: 1.0833488702774048, acc: 0.6388888955116272)
[2024-11-13 06:15:23,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:23,505][root][INFO] - Training Epoch: 2/10, step 350/574 completed (loss: 1.4139564037322998, acc: 0.5813953280448914)
[2024-11-13 06:15:23,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:23,965][root][INFO] - Training Epoch: 2/10, step 351/574 completed (loss: 1.1554086208343506, acc: 0.5897436141967773)
[2024-11-13 06:15:24,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:24,464][root][INFO] - Training Epoch: 2/10, step 352/574 completed (loss: 1.9458216428756714, acc: 0.5111111402511597)
[2024-11-13 06:15:24,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:24,824][root][INFO] - Training Epoch: 2/10, step 353/574 completed (loss: 0.41341590881347656, acc: 0.8695651888847351)
[2024-11-13 06:15:25,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:25,220][root][INFO] - Training Epoch: 2/10, step 354/574 completed (loss: 1.3093178272247314, acc: 0.6153846383094788)
[2024-11-13 06:15:25,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:25,663][root][INFO] - Training Epoch: 2/10, step 355/574 completed (loss: 1.941646695137024, acc: 0.49450549483299255)
[2024-11-13 06:15:26,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:26,425][root][INFO] - Training Epoch: 2/10, step 356/574 completed (loss: 1.5290770530700684, acc: 0.5478261113166809)
[2024-11-13 06:15:26,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:26,862][root][INFO] - Training Epoch: 2/10, step 357/574 completed (loss: 1.3037171363830566, acc: 0.5760869383811951)
[2024-11-13 06:15:27,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:27,274][root][INFO] - Training Epoch: 2/10, step 358/574 completed (loss: 1.373310923576355, acc: 0.6734693646430969)
[2024-11-13 06:15:27,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:27,618][root][INFO] - Training Epoch: 2/10, step 359/574 completed (loss: 0.10157390683889389, acc: 1.0)
[2024-11-13 06:15:27,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:28,017][root][INFO] - Training Epoch: 2/10, step 360/574 completed (loss: 0.7705647945404053, acc: 0.7692307829856873)
[2024-11-13 06:15:28,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:28,442][root][INFO] - Training Epoch: 2/10, step 361/574 completed (loss: 1.11485755443573, acc: 0.6585366129875183)
[2024-11-13 06:15:28,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:28,894][root][INFO] - Training Epoch: 2/10, step 362/574 completed (loss: 0.9395840167999268, acc: 0.800000011920929)
[2024-11-13 06:15:29,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:29,336][root][INFO] - Training Epoch: 2/10, step 363/574 completed (loss: 0.9586953520774841, acc: 0.7105262875556946)
[2024-11-13 06:15:29,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:29,690][root][INFO] - Training Epoch: 2/10, step 364/574 completed (loss: 0.8758689165115356, acc: 0.707317054271698)
[2024-11-13 06:15:29,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:30,114][root][INFO] - Training Epoch: 2/10, step 365/574 completed (loss: 0.5335060358047485, acc: 0.8787878751754761)
[2024-11-13 06:15:30,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:30,529][root][INFO] - Training Epoch: 2/10, step 366/574 completed (loss: 0.17449168860912323, acc: 0.9166666865348816)
[2024-11-13 06:15:30,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:30,939][root][INFO] - Training Epoch: 2/10, step 367/574 completed (loss: 0.34451407194137573, acc: 0.9130434989929199)
[2024-11-13 06:15:31,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:31,351][root][INFO] - Training Epoch: 2/10, step 368/574 completed (loss: 0.3595806956291199, acc: 0.8928571343421936)
[2024-11-13 06:15:31,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:31,834][root][INFO] - Training Epoch: 2/10, step 369/574 completed (loss: 1.3866374492645264, acc: 0.59375)
[2024-11-13 06:15:32,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:32,780][root][INFO] - Training Epoch: 2/10, step 370/574 completed (loss: 1.3305814266204834, acc: 0.6363636255264282)
[2024-11-13 06:15:33,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:34,107][root][INFO] - Training Epoch: 2/10, step 371/574 completed (loss: 0.9817871451377869, acc: 0.7264150977134705)
[2024-11-13 06:15:34,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:34,545][root][INFO] - Training Epoch: 2/10, step 372/574 completed (loss: 0.7329955101013184, acc: 0.8111110925674438)
[2024-11-13 06:15:34,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:34,998][root][INFO] - Training Epoch: 2/10, step 373/574 completed (loss: 0.6419659852981567, acc: 0.8928571343421936)
[2024-11-13 06:15:35,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:35,432][root][INFO] - Training Epoch: 2/10, step 374/574 completed (loss: 0.35868966579437256, acc: 0.9142857193946838)
[2024-11-13 06:15:35,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:35,796][root][INFO] - Training Epoch: 2/10, step 375/574 completed (loss: 0.14991602301597595, acc: 0.9200000166893005)
[2024-11-13 06:15:35,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:36,217][root][INFO] - Training Epoch: 2/10, step 376/574 completed (loss: 0.4494289457798004, acc: 0.8695651888847351)
[2024-11-13 06:15:36,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:36,608][root][INFO] - Training Epoch: 2/10, step 377/574 completed (loss: 0.7689208984375, acc: 0.7291666865348816)
[2024-11-13 06:15:36,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:37,022][root][INFO] - Training Epoch: 2/10, step 378/574 completed (loss: 0.3248751163482666, acc: 0.9157894849777222)
[2024-11-13 06:15:37,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:37,916][root][INFO] - Training Epoch: 2/10, step 379/574 completed (loss: 0.7182512283325195, acc: 0.7904191613197327)
[2024-11-13 06:15:38,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:38,571][root][INFO] - Training Epoch: 2/10, step 380/574 completed (loss: 0.6976879239082336, acc: 0.7819548845291138)
[2024-11-13 06:15:39,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:40,520][root][INFO] - Training Epoch: 2/10, step 381/574 completed (loss: 0.8730033040046692, acc: 0.7754010558128357)
[2024-11-13 06:15:40,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:41,382][root][INFO] - Training Epoch: 2/10, step 382/574 completed (loss: 0.41493120789527893, acc: 0.8558558821678162)
[2024-11-13 06:15:41,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:41,792][root][INFO] - Training Epoch: 2/10, step 383/574 completed (loss: 0.7847288846969604, acc: 0.8214285969734192)
[2024-11-13 06:15:41,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:42,182][root][INFO] - Training Epoch: 2/10, step 384/574 completed (loss: 0.4009180963039398, acc: 0.8571428656578064)
[2024-11-13 06:15:42,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:42,544][root][INFO] - Training Epoch: 2/10, step 385/574 completed (loss: 0.5302764773368835, acc: 0.84375)
[2024-11-13 06:15:42,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:42,898][root][INFO] - Training Epoch: 2/10, step 386/574 completed (loss: 0.38948559761047363, acc: 0.9444444179534912)
[2024-11-13 06:15:43,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:43,254][root][INFO] - Training Epoch: 2/10, step 387/574 completed (loss: 0.21789364516735077, acc: 0.9210526347160339)
[2024-11-13 06:15:43,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:43,630][root][INFO] - Training Epoch: 2/10, step 388/574 completed (loss: 0.07700023055076599, acc: 0.9545454382896423)
[2024-11-13 06:15:43,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:43,982][root][INFO] - Training Epoch: 2/10, step 389/574 completed (loss: 0.21939495205879211, acc: 0.949999988079071)
[2024-11-13 06:15:44,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:44,402][root][INFO] - Training Epoch: 2/10, step 390/574 completed (loss: 0.7562637329101562, acc: 0.8095238208770752)
[2024-11-13 06:15:44,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:44,771][root][INFO] - Training Epoch: 2/10, step 391/574 completed (loss: 1.7489264011383057, acc: 0.5555555820465088)
[2024-11-13 06:15:44,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:45,182][root][INFO] - Training Epoch: 2/10, step 392/574 completed (loss: 1.8164405822753906, acc: 0.5145630836486816)
[2024-11-13 06:15:45,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:46,029][root][INFO] - Training Epoch: 2/10, step 393/574 completed (loss: 1.363824486732483, acc: 0.654411792755127)
[2024-11-13 06:15:46,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:46,591][root][INFO] - Training Epoch: 2/10, step 394/574 completed (loss: 1.9368737936019897, acc: 0.5)
[2024-11-13 06:15:46,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:47,161][root][INFO] - Training Epoch: 2/10, step 395/574 completed (loss: 1.1904951333999634, acc: 0.7013888955116272)
[2024-11-13 06:15:47,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:47,590][root][INFO] - Training Epoch: 2/10, step 396/574 completed (loss: 0.8596010208129883, acc: 0.8604651093482971)
[2024-11-13 06:15:47,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:48,004][root][INFO] - Training Epoch: 2/10, step 397/574 completed (loss: 0.7804726958274841, acc: 0.8333333134651184)
[2024-11-13 06:15:48,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:48,485][root][INFO] - Training Epoch: 2/10, step 398/574 completed (loss: 0.7503589391708374, acc: 0.7209302186965942)
[2024-11-13 06:15:48,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:48,889][root][INFO] - Training Epoch: 2/10, step 399/574 completed (loss: 0.8118060827255249, acc: 0.800000011920929)
[2024-11-13 06:15:49,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:49,694][root][INFO] - Training Epoch: 2/10, step 400/574 completed (loss: 0.8895110487937927, acc: 0.779411792755127)
[2024-11-13 06:15:49,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:50,072][root][INFO] - Training Epoch: 2/10, step 401/574 completed (loss: 1.0514615774154663, acc: 0.6933333277702332)
[2024-11-13 06:15:50,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:50,423][root][INFO] - Training Epoch: 2/10, step 402/574 completed (loss: 1.0395588874816895, acc: 0.7272727489471436)
[2024-11-13 06:15:50,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:50,767][root][INFO] - Training Epoch: 2/10, step 403/574 completed (loss: 0.9042012095451355, acc: 0.7878788113594055)
[2024-11-13 06:15:50,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:51,127][root][INFO] - Training Epoch: 2/10, step 404/574 completed (loss: 0.4892366826534271, acc: 0.8709677457809448)
[2024-11-13 06:15:51,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:51,524][root][INFO] - Training Epoch: 2/10, step 405/574 completed (loss: 0.8445816040039062, acc: 0.7777777910232544)
[2024-11-13 06:15:51,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:51,979][root][INFO] - Training Epoch: 2/10, step 406/574 completed (loss: 0.5123252272605896, acc: 0.8799999952316284)
[2024-11-13 06:15:52,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:52,443][root][INFO] - Training Epoch: 2/10, step 407/574 completed (loss: 0.46668270230293274, acc: 0.8611111044883728)
[2024-11-13 06:15:52,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:52,854][root][INFO] - Training Epoch: 2/10, step 408/574 completed (loss: 0.5274761319160461, acc: 0.8888888955116272)
[2024-11-13 06:15:53,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:53,340][root][INFO] - Training Epoch: 2/10, step 409/574 completed (loss: 0.27815142273902893, acc: 0.9230769276618958)
[2024-11-13 06:15:53,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:53,776][root][INFO] - Training Epoch: 2/10, step 410/574 completed (loss: 0.40558701753616333, acc: 0.8965517282485962)
[2024-11-13 06:15:54,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:54,280][root][INFO] - Training Epoch: 2/10, step 411/574 completed (loss: 0.18101705610752106, acc: 0.9642857313156128)
[2024-11-13 06:15:54,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:54,744][root][INFO] - Training Epoch: 2/10, step 412/574 completed (loss: 0.9182747602462769, acc: 0.7333333492279053)
[2024-11-13 06:15:54,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:55,166][root][INFO] - Training Epoch: 2/10, step 413/574 completed (loss: 0.5753627419471741, acc: 0.939393937587738)
[2024-11-13 06:15:55,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:55,561][root][INFO] - Training Epoch: 2/10, step 414/574 completed (loss: 0.49226388335227966, acc: 0.8636363744735718)
[2024-11-13 06:15:55,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:55,936][root][INFO] - Training Epoch: 2/10, step 415/574 completed (loss: 1.0442028045654297, acc: 0.7058823704719543)
[2024-11-13 06:15:56,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:56,313][root][INFO] - Training Epoch: 2/10, step 416/574 completed (loss: 0.7916052937507629, acc: 0.807692289352417)
[2024-11-13 06:15:56,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:56,718][root][INFO] - Training Epoch: 2/10, step 417/574 completed (loss: 0.4800374209880829, acc: 0.8888888955116272)
[2024-11-13 06:15:56,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:57,116][root][INFO] - Training Epoch: 2/10, step 418/574 completed (loss: 0.615592896938324, acc: 0.7749999761581421)
[2024-11-13 06:15:57,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:57,465][root][INFO] - Training Epoch: 2/10, step 419/574 completed (loss: 0.9588068723678589, acc: 0.8999999761581421)
[2024-11-13 06:15:57,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:57,908][root][INFO] - Training Epoch: 2/10, step 420/574 completed (loss: 0.28698354959487915, acc: 0.9047619104385376)
[2024-11-13 06:15:58,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:58,288][root][INFO] - Training Epoch: 2/10, step 421/574 completed (loss: 0.574762225151062, acc: 0.8999999761581421)
[2024-11-13 06:15:58,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:58,687][root][INFO] - Training Epoch: 2/10, step 422/574 completed (loss: 1.126872181892395, acc: 0.6875)
[2024-11-13 06:15:58,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:59,064][root][INFO] - Training Epoch: 2/10, step 423/574 completed (loss: 0.9771113991737366, acc: 0.75)
[2024-11-13 06:15:59,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:59,416][root][INFO] - Training Epoch: 2/10, step 424/574 completed (loss: 0.7919108271598816, acc: 0.7037037014961243)
[2024-11-13 06:15:59,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:15:59,835][root][INFO] - Training Epoch: 2/10, step 425/574 completed (loss: 0.7822985649108887, acc: 0.7878788113594055)
[2024-11-13 06:16:00,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:00,269][root][INFO] - Training Epoch: 2/10, step 426/574 completed (loss: 0.7661632895469666, acc: 0.739130437374115)
[2024-11-13 06:16:01,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:01,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:02,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:02,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:02,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:03,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:03,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:04,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:05,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:05,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:06,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:06,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:07,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:07,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:08,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:09,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:09,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:10,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:10,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:10,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:11,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:11,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:12,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:12,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:13,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:13,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:14,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:14,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:15,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:15,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:16,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:16,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:17,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:17,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:18,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:18,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:19,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:19,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:20,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:20,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:21,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:21,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:22,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:22,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:22,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:23,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:24,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:24,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:24,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:25,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:25,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:26,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:26,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:27,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:27,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:28,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:28,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:29,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:29,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:30,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:30,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:31,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:31,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:32,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:32,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:33,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:33,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:34,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:35,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:35,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:36,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:36,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:37,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:37,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:38,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:38,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:39,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:39,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:40,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:40,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:40,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:41,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:41,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:42,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:43,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:43,791][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.6470, device='cuda:0') eval_epoch_loss=tensor(0.9734, device='cuda:0') eval_epoch_acc=tensor(0.7177, device='cuda:0')
[2024-11-13 06:16:43,792][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:16:43,792][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:16:44,286][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_2_step_427_loss_0.9734284281730652/model.pt
[2024-11-13 06:16:44,298][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:16:44,299][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.9734284281730652
[2024-11-13 06:16:44,300][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.7176712155342102
[2024-11-13 06:16:44,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:44,719][root][INFO] - Training Epoch: 2/10, step 427/574 completed (loss: 0.6440052390098572, acc: 0.7567567825317383)
[2024-11-13 06:16:44,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:45,163][root][INFO] - Training Epoch: 2/10, step 428/574 completed (loss: 0.24040867388248444, acc: 0.9629629850387573)
[2024-11-13 06:16:45,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:45,542][root][INFO] - Training Epoch: 2/10, step 429/574 completed (loss: 0.7226362824440002, acc: 0.8260869383811951)
[2024-11-13 06:16:45,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:45,853][root][INFO] - Training Epoch: 2/10, step 430/574 completed (loss: 0.18974027037620544, acc: 0.9629629850387573)
[2024-11-13 06:16:46,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:46,210][root][INFO] - Training Epoch: 2/10, step 431/574 completed (loss: 0.26204320788383484, acc: 0.9259259104728699)
[2024-11-13 06:16:46,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:46,569][root][INFO] - Training Epoch: 2/10, step 432/574 completed (loss: 0.5916261076927185, acc: 0.8260869383811951)
[2024-11-13 06:16:46,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:47,033][root][INFO] - Training Epoch: 2/10, step 433/574 completed (loss: 0.4433581829071045, acc: 0.8888888955116272)
[2024-11-13 06:16:47,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:47,376][root][INFO] - Training Epoch: 2/10, step 434/574 completed (loss: 0.08097667992115021, acc: 1.0)
[2024-11-13 06:16:47,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:47,755][root][INFO] - Training Epoch: 2/10, step 435/574 completed (loss: 0.1721489429473877, acc: 0.9696969985961914)
[2024-11-13 06:16:47,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:48,143][root][INFO] - Training Epoch: 2/10, step 436/574 completed (loss: 0.5415281653404236, acc: 0.8055555820465088)
[2024-11-13 06:16:48,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:48,582][root][INFO] - Training Epoch: 2/10, step 437/574 completed (loss: 0.7974012494087219, acc: 0.7954545617103577)
[2024-11-13 06:16:48,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:48,984][root][INFO] - Training Epoch: 2/10, step 438/574 completed (loss: 0.31805142760276794, acc: 0.9523809552192688)
[2024-11-13 06:16:49,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:49,343][root][INFO] - Training Epoch: 2/10, step 439/574 completed (loss: 1.1182256937026978, acc: 0.7692307829856873)
[2024-11-13 06:16:49,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:50,011][root][INFO] - Training Epoch: 2/10, step 440/574 completed (loss: 1.0654197931289673, acc: 0.7272727489471436)
[2024-11-13 06:16:50,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:51,054][root][INFO] - Training Epoch: 2/10, step 441/574 completed (loss: 1.4479526281356812, acc: 0.6079999804496765)
[2024-11-13 06:16:51,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:51,615][root][INFO] - Training Epoch: 2/10, step 442/574 completed (loss: 1.474225401878357, acc: 0.5967742204666138)
[2024-11-13 06:16:52,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:52,617][root][INFO] - Training Epoch: 2/10, step 443/574 completed (loss: 1.2226555347442627, acc: 0.6567164063453674)
[2024-11-13 06:16:52,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:53,006][root][INFO] - Training Epoch: 2/10, step 444/574 completed (loss: 0.9153352975845337, acc: 0.7547169923782349)
[2024-11-13 06:16:53,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:53,607][root][INFO] - Training Epoch: 2/10, step 445/574 completed (loss: 0.556522011756897, acc: 0.8636363744735718)
[2024-11-13 06:16:53,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:53,988][root][INFO] - Training Epoch: 2/10, step 446/574 completed (loss: 0.9395432472229004, acc: 0.782608687877655)
[2024-11-13 06:16:54,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:54,392][root][INFO] - Training Epoch: 2/10, step 447/574 completed (loss: 1.2789136171340942, acc: 0.807692289352417)
[2024-11-13 06:16:54,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:54,725][root][INFO] - Training Epoch: 2/10, step 448/574 completed (loss: 0.345196008682251, acc: 0.9285714030265808)
[2024-11-13 06:16:54,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:55,058][root][INFO] - Training Epoch: 2/10, step 449/574 completed (loss: 0.6814276576042175, acc: 0.8358209133148193)
[2024-11-13 06:16:55,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:55,504][root][INFO] - Training Epoch: 2/10, step 450/574 completed (loss: 0.21396760642528534, acc: 0.9444444179534912)
[2024-11-13 06:16:55,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:55,888][root][INFO] - Training Epoch: 2/10, step 451/574 completed (loss: 0.43732133507728577, acc: 0.8804348111152649)
[2024-11-13 06:16:56,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:56,255][root][INFO] - Training Epoch: 2/10, step 452/574 completed (loss: 0.7613420486450195, acc: 0.7948718070983887)
[2024-11-13 06:16:56,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:56,707][root][INFO] - Training Epoch: 2/10, step 453/574 completed (loss: 0.9126670360565186, acc: 0.7894737124443054)
[2024-11-13 06:16:56,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:57,094][root][INFO] - Training Epoch: 2/10, step 454/574 completed (loss: 0.6000081896781921, acc: 0.8163265585899353)
[2024-11-13 06:16:57,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:57,479][root][INFO] - Training Epoch: 2/10, step 455/574 completed (loss: 1.0079765319824219, acc: 0.6969696879386902)
[2024-11-13 06:16:57,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:57,853][root][INFO] - Training Epoch: 2/10, step 456/574 completed (loss: 1.3521004915237427, acc: 0.6391752362251282)
[2024-11-13 06:16:58,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:58,223][root][INFO] - Training Epoch: 2/10, step 457/574 completed (loss: 0.9432682394981384, acc: 0.7571428418159485)
[2024-11-13 06:16:58,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:58,723][root][INFO] - Training Epoch: 2/10, step 458/574 completed (loss: 1.1749497652053833, acc: 0.6627907156944275)
[2024-11-13 06:16:58,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:59,073][root][INFO] - Training Epoch: 2/10, step 459/574 completed (loss: 0.9128962755203247, acc: 0.75)
[2024-11-13 06:16:59,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:59,507][root][INFO] - Training Epoch: 2/10, step 460/574 completed (loss: 0.9233942627906799, acc: 0.7283950448036194)
[2024-11-13 06:16:59,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:16:59,858][root][INFO] - Training Epoch: 2/10, step 461/574 completed (loss: 1.0096840858459473, acc: 0.7222222089767456)
[2024-11-13 06:16:59,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:00,226][root][INFO] - Training Epoch: 2/10, step 462/574 completed (loss: 0.7073915004730225, acc: 0.875)
[2024-11-13 06:17:00,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:00,640][root][INFO] - Training Epoch: 2/10, step 463/574 completed (loss: 0.688927412033081, acc: 0.807692289352417)
[2024-11-13 06:17:00,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:01,010][root][INFO] - Training Epoch: 2/10, step 464/574 completed (loss: 0.9411871433258057, acc: 0.717391312122345)
[2024-11-13 06:17:01,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:01,366][root][INFO] - Training Epoch: 2/10, step 465/574 completed (loss: 0.9170132875442505, acc: 0.773809552192688)
[2024-11-13 06:17:01,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:01,743][root][INFO] - Training Epoch: 2/10, step 466/574 completed (loss: 1.4631612300872803, acc: 0.5783132314682007)
[2024-11-13 06:17:01,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:02,234][root][INFO] - Training Epoch: 2/10, step 467/574 completed (loss: 1.1365939378738403, acc: 0.7027027010917664)
[2024-11-13 06:17:02,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:02,633][root][INFO] - Training Epoch: 2/10, step 468/574 completed (loss: 1.3228485584259033, acc: 0.6796116232872009)
[2024-11-13 06:17:02,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:02,975][root][INFO] - Training Epoch: 2/10, step 469/574 completed (loss: 1.1623471975326538, acc: 0.7154471278190613)
[2024-11-13 06:17:03,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:03,333][root][INFO] - Training Epoch: 2/10, step 470/574 completed (loss: 0.6301613450050354, acc: 0.8333333134651184)
[2024-11-13 06:17:03,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:03,694][root][INFO] - Training Epoch: 2/10, step 471/574 completed (loss: 1.031946063041687, acc: 0.7142857313156128)
[2024-11-13 06:17:03,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:04,240][root][INFO] - Training Epoch: 2/10, step 472/574 completed (loss: 1.4509353637695312, acc: 0.5490196347236633)
[2024-11-13 06:17:04,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:04,703][root][INFO] - Training Epoch: 2/10, step 473/574 completed (loss: 1.3898870944976807, acc: 0.624454140663147)
[2024-11-13 06:17:04,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:05,075][root][INFO] - Training Epoch: 2/10, step 474/574 completed (loss: 1.11408531665802, acc: 0.65625)
[2024-11-13 06:17:05,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:05,505][root][INFO] - Training Epoch: 2/10, step 475/574 completed (loss: 0.8527905344963074, acc: 0.7484662532806396)
[2024-11-13 06:17:05,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:05,884][root][INFO] - Training Epoch: 2/10, step 476/574 completed (loss: 0.9202756285667419, acc: 0.6906474828720093)
[2024-11-13 06:17:06,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:06,349][root][INFO] - Training Epoch: 2/10, step 477/574 completed (loss: 1.2404286861419678, acc: 0.6482412219047546)
[2024-11-13 06:17:06,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:06,785][root][INFO] - Training Epoch: 2/10, step 478/574 completed (loss: 1.1294260025024414, acc: 0.7222222089767456)
[2024-11-13 06:17:06,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:07,196][root][INFO] - Training Epoch: 2/10, step 479/574 completed (loss: 0.8422273397445679, acc: 0.7878788113594055)
[2024-11-13 06:17:07,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:07,580][root][INFO] - Training Epoch: 2/10, step 480/574 completed (loss: 0.7785289883613586, acc: 0.7037037014961243)
[2024-11-13 06:17:07,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:08,037][root][INFO] - Training Epoch: 2/10, step 481/574 completed (loss: 0.8834379315376282, acc: 0.800000011920929)
[2024-11-13 06:17:08,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:08,421][root][INFO] - Training Epoch: 2/10, step 482/574 completed (loss: 1.085086464881897, acc: 0.699999988079071)
[2024-11-13 06:17:08,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:08,893][root][INFO] - Training Epoch: 2/10, step 483/574 completed (loss: 1.054517388343811, acc: 0.6896551847457886)
[2024-11-13 06:17:09,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:09,239][root][INFO] - Training Epoch: 2/10, step 484/574 completed (loss: 0.562699556350708, acc: 0.8387096524238586)
[2024-11-13 06:17:09,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:09,584][root][INFO] - Training Epoch: 2/10, step 485/574 completed (loss: 0.5484803915023804, acc: 0.7894737124443054)
[2024-11-13 06:17:09,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:09,931][root][INFO] - Training Epoch: 2/10, step 486/574 completed (loss: 1.4744460582733154, acc: 0.5555555820465088)
[2024-11-13 06:17:10,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:10,356][root][INFO] - Training Epoch: 2/10, step 487/574 completed (loss: 1.414608359336853, acc: 0.7142857313156128)
[2024-11-13 06:17:10,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:10,749][root][INFO] - Training Epoch: 2/10, step 488/574 completed (loss: 0.836679220199585, acc: 0.7727272510528564)
[2024-11-13 06:17:10,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:11,178][root][INFO] - Training Epoch: 2/10, step 489/574 completed (loss: 1.341459035873413, acc: 0.6615384817123413)
[2024-11-13 06:17:11,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:11,505][root][INFO] - Training Epoch: 2/10, step 490/574 completed (loss: 0.5185595750808716, acc: 0.8666666746139526)
[2024-11-13 06:17:11,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:11,857][root][INFO] - Training Epoch: 2/10, step 491/574 completed (loss: 0.8101452589035034, acc: 0.7931034564971924)
[2024-11-13 06:17:12,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:12,212][root][INFO] - Training Epoch: 2/10, step 492/574 completed (loss: 0.7997156977653503, acc: 0.7450980544090271)
[2024-11-13 06:17:12,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:12,632][root][INFO] - Training Epoch: 2/10, step 493/574 completed (loss: 0.83153235912323, acc: 0.6896551847457886)
[2024-11-13 06:17:12,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:13,013][root][INFO] - Training Epoch: 2/10, step 494/574 completed (loss: 0.642209529876709, acc: 0.8421052694320679)
[2024-11-13 06:17:13,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:13,370][root][INFO] - Training Epoch: 2/10, step 495/574 completed (loss: 1.9204896688461304, acc: 0.4736842215061188)
[2024-11-13 06:17:13,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:13,794][root][INFO] - Training Epoch: 2/10, step 496/574 completed (loss: 1.3998342752456665, acc: 0.625)
[2024-11-13 06:17:14,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:14,288][root][INFO] - Training Epoch: 2/10, step 497/574 completed (loss: 0.7817991375923157, acc: 0.8089887499809265)
[2024-11-13 06:17:14,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:14,726][root][INFO] - Training Epoch: 2/10, step 498/574 completed (loss: 1.2349239587783813, acc: 0.6516854166984558)
[2024-11-13 06:17:14,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:15,185][root][INFO] - Training Epoch: 2/10, step 499/574 completed (loss: 1.786278247833252, acc: 0.5035461187362671)
[2024-11-13 06:17:15,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:15,580][root][INFO] - Training Epoch: 2/10, step 500/574 completed (loss: 1.312636137008667, acc: 0.6521739363670349)
[2024-11-13 06:17:15,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:15,934][root][INFO] - Training Epoch: 2/10, step 501/574 completed (loss: 0.21537835896015167, acc: 0.9200000166893005)
[2024-11-13 06:17:16,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:16,268][root][INFO] - Training Epoch: 2/10, step 502/574 completed (loss: 0.24558435380458832, acc: 0.9615384340286255)
[2024-11-13 06:17:16,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:16,729][root][INFO] - Training Epoch: 2/10, step 503/574 completed (loss: 0.33945637941360474, acc: 0.9259259104728699)
[2024-11-13 06:17:16,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:17,128][root][INFO] - Training Epoch: 2/10, step 504/574 completed (loss: 0.6504490375518799, acc: 0.7777777910232544)
[2024-11-13 06:17:17,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:17,483][root][INFO] - Training Epoch: 2/10, step 505/574 completed (loss: 0.9106320142745972, acc: 0.8113207817077637)
[2024-11-13 06:17:17,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:17,859][root][INFO] - Training Epoch: 2/10, step 506/574 completed (loss: 1.03501296043396, acc: 0.7241379022598267)
[2024-11-13 06:17:18,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:18,753][root][INFO] - Training Epoch: 2/10, step 507/574 completed (loss: 1.583316445350647, acc: 0.5585585832595825)
[2024-11-13 06:17:19,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:19,396][root][INFO] - Training Epoch: 2/10, step 508/574 completed (loss: 1.1091598272323608, acc: 0.6760563254356384)
[2024-11-13 06:17:19,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:19,786][root][INFO] - Training Epoch: 2/10, step 509/574 completed (loss: 0.4306434690952301, acc: 0.8999999761581421)
[2024-11-13 06:17:19,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:20,159][root][INFO] - Training Epoch: 2/10, step 510/574 completed (loss: 0.20505018532276154, acc: 0.9333333373069763)
[2024-11-13 06:17:20,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:20,492][root][INFO] - Training Epoch: 2/10, step 511/574 completed (loss: 0.8343897461891174, acc: 0.807692289352417)
[2024-11-13 06:17:22,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:24,199][root][INFO] - Training Epoch: 2/10, step 512/574 completed (loss: 1.5082554817199707, acc: 0.6214285492897034)
[2024-11-13 06:17:24,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:25,412][root][INFO] - Training Epoch: 2/10, step 513/574 completed (loss: 0.8413203954696655, acc: 0.7698412537574768)
[2024-11-13 06:17:25,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:25,806][root][INFO] - Training Epoch: 2/10, step 514/574 completed (loss: 0.9618330001831055, acc: 0.75)
[2024-11-13 06:17:26,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:26,257][root][INFO] - Training Epoch: 2/10, step 515/574 completed (loss: 0.4394594132900238, acc: 0.8500000238418579)
[2024-11-13 06:17:26,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:27,349][root][INFO] - Training Epoch: 2/10, step 516/574 completed (loss: 0.7561706304550171, acc: 0.8333333134651184)
[2024-11-13 06:17:27,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:27,851][root][INFO] - Training Epoch: 2/10, step 517/574 completed (loss: 0.02194144017994404, acc: 1.0)
[2024-11-13 06:17:28,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:28,234][root][INFO] - Training Epoch: 2/10, step 518/574 completed (loss: 0.6564373970031738, acc: 0.8709677457809448)
[2024-11-13 06:17:28,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:28,582][root][INFO] - Training Epoch: 2/10, step 519/574 completed (loss: 0.8905194401741028, acc: 0.800000011920929)
[2024-11-13 06:17:28,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:28,937][root][INFO] - Training Epoch: 2/10, step 520/574 completed (loss: 1.1244648694992065, acc: 0.7037037014961243)
[2024-11-13 06:17:29,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:30,537][root][INFO] - Training Epoch: 2/10, step 521/574 completed (loss: 1.0246738195419312, acc: 0.6779661178588867)
[2024-11-13 06:17:30,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:30,996][root][INFO] - Training Epoch: 2/10, step 522/574 completed (loss: 0.7322021126747131, acc: 0.7985074520111084)
[2024-11-13 06:17:31,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:31,494][root][INFO] - Training Epoch: 2/10, step 523/574 completed (loss: 0.893975555896759, acc: 0.7299270033836365)
[2024-11-13 06:17:31,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:32,353][root][INFO] - Training Epoch: 2/10, step 524/574 completed (loss: 0.9714211821556091, acc: 0.7300000190734863)
[2024-11-13 06:17:32,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:32,718][root][INFO] - Training Epoch: 2/10, step 525/574 completed (loss: 0.2892131507396698, acc: 0.8888888955116272)
[2024-11-13 06:17:32,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:33,058][root][INFO] - Training Epoch: 2/10, step 526/574 completed (loss: 0.5439184904098511, acc: 0.8461538553237915)
[2024-11-13 06:17:33,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:33,494][root][INFO] - Training Epoch: 2/10, step 527/574 completed (loss: 0.6133105158805847, acc: 0.8571428656578064)
[2024-11-13 06:17:33,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:33,885][root][INFO] - Training Epoch: 2/10, step 528/574 completed (loss: 2.462919235229492, acc: 0.37704917788505554)
[2024-11-13 06:17:34,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:34,331][root][INFO] - Training Epoch: 2/10, step 529/574 completed (loss: 0.7922101616859436, acc: 0.7457627058029175)
[2024-11-13 06:17:34,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:34,695][root][INFO] - Training Epoch: 2/10, step 530/574 completed (loss: 1.8434407711029053, acc: 0.5116279125213623)
[2024-11-13 06:17:34,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:35,061][root][INFO] - Training Epoch: 2/10, step 531/574 completed (loss: 1.474918007850647, acc: 0.6136363744735718)
[2024-11-13 06:17:35,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:35,429][root][INFO] - Training Epoch: 2/10, step 532/574 completed (loss: 1.9026750326156616, acc: 0.5094339847564697)
[2024-11-13 06:17:35,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:35,764][root][INFO] - Training Epoch: 2/10, step 533/574 completed (loss: 0.9598985314369202, acc: 0.7045454382896423)
[2024-11-13 06:17:35,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:36,117][root][INFO] - Training Epoch: 2/10, step 534/574 completed (loss: 1.2943754196166992, acc: 0.6399999856948853)
[2024-11-13 06:17:36,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:36,538][root][INFO] - Training Epoch: 2/10, step 535/574 completed (loss: 0.5700033903121948, acc: 0.8500000238418579)
[2024-11-13 06:17:36,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:36,945][root][INFO] - Training Epoch: 2/10, step 536/574 completed (loss: 0.706425130367279, acc: 0.7727272510528564)
[2024-11-13 06:17:37,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:37,495][root][INFO] - Training Epoch: 2/10, step 537/574 completed (loss: 0.8787202835083008, acc: 0.7538461685180664)
[2024-11-13 06:17:37,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:37,950][root][INFO] - Training Epoch: 2/10, step 538/574 completed (loss: 0.8923473954200745, acc: 0.734375)
[2024-11-13 06:17:38,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:38,483][root][INFO] - Training Epoch: 2/10, step 539/574 completed (loss: 0.7583993673324585, acc: 0.8125)
[2024-11-13 06:17:38,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:38,849][root][INFO] - Training Epoch: 2/10, step 540/574 completed (loss: 1.1544663906097412, acc: 0.6969696879386902)
[2024-11-13 06:17:38,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:39,196][root][INFO] - Training Epoch: 2/10, step 541/574 completed (loss: 0.43976065516471863, acc: 0.875)
[2024-11-13 06:17:39,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:39,628][root][INFO] - Training Epoch: 2/10, step 542/574 completed (loss: 0.2508693039417267, acc: 0.9354838728904724)
[2024-11-13 06:17:39,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:40,022][root][INFO] - Training Epoch: 2/10, step 543/574 completed (loss: 0.21745990216732025, acc: 0.9130434989929199)
[2024-11-13 06:17:40,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:40,418][root][INFO] - Training Epoch: 2/10, step 544/574 completed (loss: 0.9168128371238708, acc: 0.6666666865348816)
[2024-11-13 06:17:40,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:40,807][root][INFO] - Training Epoch: 2/10, step 545/574 completed (loss: 0.23386453092098236, acc: 0.9512194991111755)
[2024-11-13 06:17:40,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:41,155][root][INFO] - Training Epoch: 2/10, step 546/574 completed (loss: 0.18461447954177856, acc: 0.9142857193946838)
[2024-11-13 06:17:41,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:41,558][root][INFO] - Training Epoch: 2/10, step 547/574 completed (loss: 0.43414855003356934, acc: 0.9210526347160339)
[2024-11-13 06:17:41,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:41,916][root][INFO] - Training Epoch: 2/10, step 548/574 completed (loss: 0.7024437189102173, acc: 0.8064516186714172)
[2024-11-13 06:17:42,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:42,285][root][INFO] - Training Epoch: 2/10, step 549/574 completed (loss: 0.18779657781124115, acc: 0.9599999785423279)
[2024-11-13 06:17:42,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:42,642][root][INFO] - Training Epoch: 2/10, step 550/574 completed (loss: 0.6909323334693909, acc: 0.8181818127632141)
[2024-11-13 06:17:42,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:43,011][root][INFO] - Training Epoch: 2/10, step 551/574 completed (loss: 0.4472552239894867, acc: 0.8500000238418579)
[2024-11-13 06:17:43,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:43,370][root][INFO] - Training Epoch: 2/10, step 552/574 completed (loss: 0.4353763461112976, acc: 0.8571428656578064)
[2024-11-13 06:17:43,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:43,743][root][INFO] - Training Epoch: 2/10, step 553/574 completed (loss: 0.936198890209198, acc: 0.7591241002082825)
[2024-11-13 06:17:43,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:44,157][root][INFO] - Training Epoch: 2/10, step 554/574 completed (loss: 0.7967356443405151, acc: 0.7655172348022461)
[2024-11-13 06:17:44,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:44,576][root][INFO] - Training Epoch: 2/10, step 555/574 completed (loss: 1.0264151096343994, acc: 0.7642857432365417)
[2024-11-13 06:17:44,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:45,014][root][INFO] - Training Epoch: 2/10, step 556/574 completed (loss: 0.8520526885986328, acc: 0.7682119011878967)
[2024-11-13 06:17:45,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:45,386][root][INFO] - Training Epoch: 2/10, step 557/574 completed (loss: 0.6177852153778076, acc: 0.811965823173523)
[2024-11-13 06:17:45,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:45,747][root][INFO] - Training Epoch: 2/10, step 558/574 completed (loss: 0.21878455579280853, acc: 0.9200000166893005)
[2024-11-13 06:17:45,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:46,212][root][INFO] - Training Epoch: 2/10, step 559/574 completed (loss: 0.6885931491851807, acc: 0.7307692170143127)
[2024-11-13 06:17:46,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:46,542][root][INFO] - Training Epoch: 2/10, step 560/574 completed (loss: 0.1707194447517395, acc: 0.9615384340286255)
[2024-11-13 06:17:46,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:46,911][root][INFO] - Training Epoch: 2/10, step 561/574 completed (loss: 0.8265213370323181, acc: 0.7948718070983887)
[2024-11-13 06:17:47,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:47,307][root][INFO] - Training Epoch: 2/10, step 562/574 completed (loss: 1.1309102773666382, acc: 0.6666666865348816)
[2024-11-13 06:17:47,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:47,692][root][INFO] - Training Epoch: 2/10, step 563/574 completed (loss: 0.9812284111976624, acc: 0.7532467246055603)
[2024-11-13 06:17:47,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:48,119][root][INFO] - Training Epoch: 2/10, step 564/574 completed (loss: 0.9529752135276794, acc: 0.7083333134651184)
[2024-11-13 06:17:48,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:48,524][root][INFO] - Training Epoch: 2/10, step 565/574 completed (loss: 0.7724102139472961, acc: 0.8448275923728943)
[2024-11-13 06:17:48,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:48,874][root][INFO] - Training Epoch: 2/10, step 566/574 completed (loss: 0.6026541590690613, acc: 0.8690476417541504)
[2024-11-13 06:17:49,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:49,322][root][INFO] - Training Epoch: 2/10, step 567/574 completed (loss: 0.30356764793395996, acc: 0.8947368264198303)
[2024-11-13 06:17:49,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:49,758][root][INFO] - Training Epoch: 2/10, step 568/574 completed (loss: 0.413038045167923, acc: 0.8518518805503845)
[2024-11-13 06:17:49,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:50,265][root][INFO] - Training Epoch: 2/10, step 569/574 completed (loss: 0.7064710259437561, acc: 0.7860962748527527)
[2024-11-13 06:17:51,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:51,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:51,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:52,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:52,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:53,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:53,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:54,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:54,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:55,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:55,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:56,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:56,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:57,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:57,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:58,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:58,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:59,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:17:59,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:00,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:00,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:01,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:01,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:02,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:02,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:03,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:03,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:04,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:04,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:04,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:05,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:05,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:06,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:06,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:07,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:07,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:08,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:08,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:09,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:09,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:10,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:10,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:10,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:11,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:11,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:12,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:12,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:13,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:13,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:14,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:14,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:15,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:15,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:16,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:16,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:17,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:17,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:18,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:18,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:19,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:19,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:20,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:20,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:21,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:21,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:22,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:22,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:23,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:23,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:24,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:24,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:25,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:25,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:26,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:26,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:27,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:27,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:28,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:28,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:29,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:29,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:30,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:30,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:30,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:31,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:32,120][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3366, device='cuda:0') eval_epoch_loss=tensor(0.8487, device='cuda:0') eval_epoch_acc=tensor(0.7662, device='cuda:0')
[2024-11-13 06:18:32,121][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:18:32,122][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:18:32,457][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_2_step_570_loss_0.8486912846565247/model.pt
[2024-11-13 06:18:32,462][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:18:32,463][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.8486912846565247
[2024-11-13 06:18:32,463][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.7662307620048523
[2024-11-13 06:18:32,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:32,867][root][INFO] - Training Epoch: 2/10, step 570/574 completed (loss: 0.3481394052505493, acc: 0.8870967626571655)
[2024-11-13 06:18:33,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:33,211][root][INFO] - Training Epoch: 2/10, step 571/574 completed (loss: 0.8601023554801941, acc: 0.8034188151359558)
[2024-11-13 06:18:33,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:33,550][root][INFO] - Training Epoch: 2/10, step 572/574 completed (loss: 1.060349941253662, acc: 0.6989796161651611)
[2024-11-13 06:18:33,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:33,975][root][INFO] - Training Epoch: 2/10, step 573/574 completed (loss: 1.2870992422103882, acc: 0.6540880799293518)
[2024-11-13 06:18:34,422][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=3.1747, train_epoch_loss=1.1552, epoch time 464.14259972423315s
[2024-11-13 06:18:34,422][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-11-13 06:18:34,422][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 17 GB
[2024-11-13 06:18:34,422][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-11-13 06:18:34,422][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2024-11-13 06:18:34,422][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-13 06:18:35,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:35,309][root][INFO] - Training Epoch: 3/10, step 0/574 completed (loss: 0.5273550748825073, acc: 0.8148148059844971)
[2024-11-13 06:18:35,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:35,713][root][INFO] - Training Epoch: 3/10, step 1/574 completed (loss: 0.747894287109375, acc: 0.800000011920929)
[2024-11-13 06:18:35,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:36,111][root][INFO] - Training Epoch: 3/10, step 2/574 completed (loss: 1.5527598857879639, acc: 0.5675675868988037)
[2024-11-13 06:18:36,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:36,579][root][INFO] - Training Epoch: 3/10, step 3/574 completed (loss: 0.8857935667037964, acc: 0.7894737124443054)
[2024-11-13 06:18:36,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:37,016][root][INFO] - Training Epoch: 3/10, step 4/574 completed (loss: 0.8910516500473022, acc: 0.7027027010917664)
[2024-11-13 06:18:37,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:37,371][root][INFO] - Training Epoch: 3/10, step 5/574 completed (loss: 0.4894232153892517, acc: 0.9285714030265808)
[2024-11-13 06:18:37,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:37,801][root][INFO] - Training Epoch: 3/10, step 6/574 completed (loss: 1.2272106409072876, acc: 0.6326530575752258)
[2024-11-13 06:18:37,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:38,206][root][INFO] - Training Epoch: 3/10, step 7/574 completed (loss: 0.7739124298095703, acc: 0.8333333134651184)
[2024-11-13 06:18:38,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:38,625][root][INFO] - Training Epoch: 3/10, step 8/574 completed (loss: 0.14638476073741913, acc: 0.9545454382896423)
[2024-11-13 06:18:38,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:38,997][root][INFO] - Training Epoch: 3/10, step 9/574 completed (loss: 0.20047415792942047, acc: 0.9615384340286255)
[2024-11-13 06:18:39,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:39,347][root][INFO] - Training Epoch: 3/10, step 10/574 completed (loss: 0.338584303855896, acc: 0.8888888955116272)
[2024-11-13 06:18:39,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:39,812][root][INFO] - Training Epoch: 3/10, step 11/574 completed (loss: 0.998455822467804, acc: 0.7435897588729858)
[2024-11-13 06:18:39,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:40,224][root][INFO] - Training Epoch: 3/10, step 12/574 completed (loss: 0.4136652946472168, acc: 0.8484848737716675)
[2024-11-13 06:18:40,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:40,665][root][INFO] - Training Epoch: 3/10, step 13/574 completed (loss: 0.7122987508773804, acc: 0.760869562625885)
[2024-11-13 06:18:40,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:41,045][root][INFO] - Training Epoch: 3/10, step 14/574 completed (loss: 0.5467785000801086, acc: 0.843137264251709)
[2024-11-13 06:18:41,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:41,410][root][INFO] - Training Epoch: 3/10, step 15/574 completed (loss: 1.0404479503631592, acc: 0.7551020383834839)
[2024-11-13 06:18:41,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:41,823][root][INFO] - Training Epoch: 3/10, step 16/574 completed (loss: 0.38031741976737976, acc: 0.8947368264198303)
[2024-11-13 06:18:42,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:42,251][root][INFO] - Training Epoch: 3/10, step 17/574 completed (loss: 0.6513980031013489, acc: 0.7916666865348816)
[2024-11-13 06:18:42,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:42,703][root][INFO] - Training Epoch: 3/10, step 18/574 completed (loss: 1.1829969882965088, acc: 0.6944444179534912)
[2024-11-13 06:18:42,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:43,084][root][INFO] - Training Epoch: 3/10, step 19/574 completed (loss: 0.5853676795959473, acc: 0.7894737124443054)
[2024-11-13 06:18:43,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:43,466][root][INFO] - Training Epoch: 3/10, step 20/574 completed (loss: 0.7017669081687927, acc: 0.8461538553237915)
[2024-11-13 06:18:43,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:43,890][root][INFO] - Training Epoch: 3/10, step 21/574 completed (loss: 0.8380828499794006, acc: 0.7586206793785095)
[2024-11-13 06:18:44,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:44,220][root][INFO] - Training Epoch: 3/10, step 22/574 completed (loss: 0.8647237420082092, acc: 0.8399999737739563)
[2024-11-13 06:18:44,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:44,601][root][INFO] - Training Epoch: 3/10, step 23/574 completed (loss: 0.683028519153595, acc: 0.8571428656578064)
[2024-11-13 06:18:44,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:45,030][root][INFO] - Training Epoch: 3/10, step 24/574 completed (loss: 0.3975251317024231, acc: 0.875)
[2024-11-13 06:18:45,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:45,461][root][INFO] - Training Epoch: 3/10, step 25/574 completed (loss: 1.2654544115066528, acc: 0.6792452931404114)
[2024-11-13 06:18:45,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:45,814][root][INFO] - Training Epoch: 3/10, step 26/574 completed (loss: 1.158677577972412, acc: 0.6575342416763306)
[2024-11-13 06:18:46,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:47,539][root][INFO] - Training Epoch: 3/10, step 27/574 completed (loss: 1.3765697479248047, acc: 0.6166008114814758)
[2024-11-13 06:18:47,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:47,945][root][INFO] - Training Epoch: 3/10, step 28/574 completed (loss: 0.6947525143623352, acc: 0.8372092843055725)
[2024-11-13 06:18:48,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:48,380][root][INFO] - Training Epoch: 3/10, step 29/574 completed (loss: 0.9824474453926086, acc: 0.7469879388809204)
[2024-11-13 06:18:48,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:48,803][root][INFO] - Training Epoch: 3/10, step 30/574 completed (loss: 1.107437252998352, acc: 0.7037037014961243)
[2024-11-13 06:18:48,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:49,165][root][INFO] - Training Epoch: 3/10, step 31/574 completed (loss: 0.937088668346405, acc: 0.7142857313156128)
[2024-11-13 06:18:49,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:49,585][root][INFO] - Training Epoch: 3/10, step 32/574 completed (loss: 0.4644165635108948, acc: 0.8518518805503845)
[2024-11-13 06:18:49,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:49,948][root][INFO] - Training Epoch: 3/10, step 33/574 completed (loss: 0.1853296309709549, acc: 0.9130434989929199)
[2024-11-13 06:18:50,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:50,370][root][INFO] - Training Epoch: 3/10, step 34/574 completed (loss: 0.7886739373207092, acc: 0.7983193397521973)
[2024-11-13 06:18:50,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:50,789][root][INFO] - Training Epoch: 3/10, step 35/574 completed (loss: 0.702664315700531, acc: 0.8360655903816223)
[2024-11-13 06:18:50,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:51,207][root][INFO] - Training Epoch: 3/10, step 36/574 completed (loss: 0.6575703024864197, acc: 0.8095238208770752)
[2024-11-13 06:18:51,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:51,626][root][INFO] - Training Epoch: 3/10, step 37/574 completed (loss: 0.9630122780799866, acc: 0.7627118825912476)
[2024-11-13 06:18:51,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:52,067][root][INFO] - Training Epoch: 3/10, step 38/574 completed (loss: 0.5133717656135559, acc: 0.8735632300376892)
[2024-11-13 06:18:52,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:52,461][root][INFO] - Training Epoch: 3/10, step 39/574 completed (loss: 0.564454197883606, acc: 0.761904776096344)
[2024-11-13 06:18:52,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:52,847][root][INFO] - Training Epoch: 3/10, step 40/574 completed (loss: 0.9576697945594788, acc: 0.6538461446762085)
[2024-11-13 06:18:53,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:53,326][root][INFO] - Training Epoch: 3/10, step 41/574 completed (loss: 0.9446113109588623, acc: 0.6216216087341309)
[2024-11-13 06:18:53,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:53,748][root][INFO] - Training Epoch: 3/10, step 42/574 completed (loss: 1.4351967573165894, acc: 0.6000000238418579)
[2024-11-13 06:18:54,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:54,310][root][INFO] - Training Epoch: 3/10, step 43/574 completed (loss: 0.8473979830741882, acc: 0.7676767706871033)
[2024-11-13 06:18:54,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:54,894][root][INFO] - Training Epoch: 3/10, step 44/574 completed (loss: 0.8237546682357788, acc: 0.7938144207000732)
[2024-11-13 06:18:55,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:55,444][root][INFO] - Training Epoch: 3/10, step 45/574 completed (loss: 0.8914623260498047, acc: 0.7279411554336548)
[2024-11-13 06:18:55,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:55,872][root][INFO] - Training Epoch: 3/10, step 46/574 completed (loss: 0.23241733014583588, acc: 0.9615384340286255)
[2024-11-13 06:18:56,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:56,287][root][INFO] - Training Epoch: 3/10, step 47/574 completed (loss: 0.3681148886680603, acc: 0.9629629850387573)
[2024-11-13 06:18:56,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:56,683][root][INFO] - Training Epoch: 3/10, step 48/574 completed (loss: 0.48142051696777344, acc: 0.9285714030265808)
[2024-11-13 06:18:56,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:57,087][root][INFO] - Training Epoch: 3/10, step 49/574 completed (loss: 0.40054112672805786, acc: 0.8888888955116272)
[2024-11-13 06:18:57,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:57,510][root][INFO] - Training Epoch: 3/10, step 50/574 completed (loss: 0.9531630277633667, acc: 0.719298243522644)
[2024-11-13 06:18:57,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:57,913][root][INFO] - Training Epoch: 3/10, step 51/574 completed (loss: 1.088358759880066, acc: 0.7460317611694336)
[2024-11-13 06:18:58,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:58,346][root][INFO] - Training Epoch: 3/10, step 52/574 completed (loss: 1.599581003189087, acc: 0.5211267471313477)
[2024-11-13 06:18:58,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:58,979][root][INFO] - Training Epoch: 3/10, step 53/574 completed (loss: 1.7905561923980713, acc: 0.4866666793823242)
[2024-11-13 06:18:59,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:59,431][root][INFO] - Training Epoch: 3/10, step 54/574 completed (loss: 0.9910707473754883, acc: 0.8108108043670654)
[2024-11-13 06:18:59,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:18:59,861][root][INFO] - Training Epoch: 3/10, step 55/574 completed (loss: 0.18950289487838745, acc: 0.9230769276618958)
[2024-11-13 06:19:02,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:04,355][root][INFO] - Training Epoch: 3/10, step 56/574 completed (loss: 1.464945912361145, acc: 0.61774742603302)
[2024-11-13 06:19:05,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:06,205][root][INFO] - Training Epoch: 3/10, step 57/574 completed (loss: 1.5998084545135498, acc: 0.5620915293693542)
[2024-11-13 06:19:06,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:07,146][root][INFO] - Training Epoch: 3/10, step 58/574 completed (loss: 1.1778758764266968, acc: 0.6761363744735718)
[2024-11-13 06:19:07,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:08,008][root][INFO] - Training Epoch: 3/10, step 59/574 completed (loss: 0.8145151138305664, acc: 0.75)
[2024-11-13 06:19:08,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:08,845][root][INFO] - Training Epoch: 3/10, step 60/574 completed (loss: 1.0612244606018066, acc: 0.7101449370384216)
[2024-11-13 06:19:09,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:09,419][root][INFO] - Training Epoch: 3/10, step 61/574 completed (loss: 1.0472673177719116, acc: 0.6875)
[2024-11-13 06:19:09,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:09,769][root][INFO] - Training Epoch: 3/10, step 62/574 completed (loss: 0.6611118316650391, acc: 0.7941176295280457)
[2024-11-13 06:19:09,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:10,200][root][INFO] - Training Epoch: 3/10, step 63/574 completed (loss: 0.3000793755054474, acc: 0.9444444179534912)
[2024-11-13 06:19:10,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:10,655][root][INFO] - Training Epoch: 3/10, step 64/574 completed (loss: 0.34668102860450745, acc: 0.890625)
[2024-11-13 06:19:10,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:11,000][root][INFO] - Training Epoch: 3/10, step 65/574 completed (loss: 0.2748863995075226, acc: 0.8965517282485962)
[2024-11-13 06:19:11,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:11,401][root][INFO] - Training Epoch: 3/10, step 66/574 completed (loss: 1.2294318675994873, acc: 0.6964285969734192)
[2024-11-13 06:19:11,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:11,800][root][INFO] - Training Epoch: 3/10, step 67/574 completed (loss: 0.7203028798103333, acc: 0.8166666626930237)
[2024-11-13 06:19:11,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:12,226][root][INFO] - Training Epoch: 3/10, step 68/574 completed (loss: 0.1495705395936966, acc: 0.9200000166893005)
[2024-11-13 06:19:12,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:12,592][root][INFO] - Training Epoch: 3/10, step 69/574 completed (loss: 1.052401065826416, acc: 0.75)
[2024-11-13 06:19:12,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:12,954][root][INFO] - Training Epoch: 3/10, step 70/574 completed (loss: 1.0688139200210571, acc: 0.6969696879386902)
[2024-11-13 06:19:13,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:13,322][root][INFO] - Training Epoch: 3/10, step 71/574 completed (loss: 1.462981104850769, acc: 0.6102941036224365)
[2024-11-13 06:19:13,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:13,745][root][INFO] - Training Epoch: 3/10, step 72/574 completed (loss: 1.1559545993804932, acc: 0.6666666865348816)
[2024-11-13 06:19:13,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:14,096][root][INFO] - Training Epoch: 3/10, step 73/574 completed (loss: 1.8955094814300537, acc: 0.5384615659713745)
[2024-11-13 06:19:14,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:14,525][root][INFO] - Training Epoch: 3/10, step 74/574 completed (loss: 1.4763966798782349, acc: 0.6122449040412903)
[2024-11-13 06:19:14,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:14,926][root][INFO] - Training Epoch: 3/10, step 75/574 completed (loss: 1.5065832138061523, acc: 0.6268656849861145)
[2024-11-13 06:19:15,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:15,425][root][INFO] - Training Epoch: 3/10, step 76/574 completed (loss: 1.6818593740463257, acc: 0.5328466892242432)
[2024-11-13 06:19:15,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:15,823][root][INFO] - Training Epoch: 3/10, step 77/574 completed (loss: 0.06261362135410309, acc: 1.0)
[2024-11-13 06:19:15,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:16,189][root][INFO] - Training Epoch: 3/10, step 78/574 completed (loss: 0.40164777636528015, acc: 0.875)
[2024-11-13 06:19:16,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:16,593][root][INFO] - Training Epoch: 3/10, step 79/574 completed (loss: 0.38608112931251526, acc: 0.9090909361839294)
[2024-11-13 06:19:16,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:16,998][root][INFO] - Training Epoch: 3/10, step 80/574 completed (loss: 0.30253466963768005, acc: 0.8846153616905212)
[2024-11-13 06:19:17,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:17,387][root][INFO] - Training Epoch: 3/10, step 81/574 completed (loss: 1.0036430358886719, acc: 0.7115384340286255)
[2024-11-13 06:19:17,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:17,826][root][INFO] - Training Epoch: 3/10, step 82/574 completed (loss: 1.157319188117981, acc: 0.7115384340286255)
[2024-11-13 06:19:18,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:18,246][root][INFO] - Training Epoch: 3/10, step 83/574 completed (loss: 0.4860130846500397, acc: 0.875)
[2024-11-13 06:19:18,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:18,687][root][INFO] - Training Epoch: 3/10, step 84/574 completed (loss: 0.6724733710289001, acc: 0.782608687877655)
[2024-11-13 06:19:18,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:19,105][root][INFO] - Training Epoch: 3/10, step 85/574 completed (loss: 0.7380419969558716, acc: 0.7799999713897705)
[2024-11-13 06:19:19,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:19,554][root][INFO] - Training Epoch: 3/10, step 86/574 completed (loss: 0.30790436267852783, acc: 0.9130434989929199)
[2024-11-13 06:19:19,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:20,206][root][INFO] - Training Epoch: 3/10, step 87/574 completed (loss: 1.3174597024917603, acc: 0.6000000238418579)
[2024-11-13 06:19:20,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:20,670][root][INFO] - Training Epoch: 3/10, step 88/574 completed (loss: 1.268857479095459, acc: 0.6893203854560852)
[2024-11-13 06:19:21,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:22,400][root][INFO] - Training Epoch: 3/10, step 89/574 completed (loss: 1.106282353401184, acc: 0.7281553149223328)
[2024-11-13 06:19:23,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:23,676][root][INFO] - Training Epoch: 3/10, step 90/574 completed (loss: 1.4902329444885254, acc: 0.5967742204666138)
[2024-11-13 06:19:24,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:24,919][root][INFO] - Training Epoch: 3/10, step 91/574 completed (loss: 1.090381145477295, acc: 0.7112069129943848)
[2024-11-13 06:19:25,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:26,070][root][INFO] - Training Epoch: 3/10, step 92/574 completed (loss: 1.0244450569152832, acc: 0.7052631378173828)
[2024-11-13 06:19:26,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:27,652][root][INFO] - Training Epoch: 3/10, step 93/574 completed (loss: 1.8065040111541748, acc: 0.5148515105247498)
[2024-11-13 06:19:27,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:28,018][root][INFO] - Training Epoch: 3/10, step 94/574 completed (loss: 1.3237600326538086, acc: 0.6451612710952759)
[2024-11-13 06:19:28,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:28,412][root][INFO] - Training Epoch: 3/10, step 95/574 completed (loss: 1.25669264793396, acc: 0.6666666865348816)
[2024-11-13 06:19:28,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:28,807][root][INFO] - Training Epoch: 3/10, step 96/574 completed (loss: 1.4790971279144287, acc: 0.5462185144424438)
[2024-11-13 06:19:29,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:29,259][root][INFO] - Training Epoch: 3/10, step 97/574 completed (loss: 1.6152949333190918, acc: 0.5384615659713745)
[2024-11-13 06:19:29,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:29,759][root][INFO] - Training Epoch: 3/10, step 98/574 completed (loss: 1.7720341682434082, acc: 0.48175182938575745)
[2024-11-13 06:19:29,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:30,171][root][INFO] - Training Epoch: 3/10, step 99/574 completed (loss: 1.9118447303771973, acc: 0.4776119291782379)
[2024-11-13 06:19:30,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:30,576][root][INFO] - Training Epoch: 3/10, step 100/574 completed (loss: 0.6869763135910034, acc: 0.75)
[2024-11-13 06:19:30,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:31,037][root][INFO] - Training Epoch: 3/10, step 101/574 completed (loss: 0.06784648448228836, acc: 1.0)
[2024-11-13 06:19:31,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:31,421][root][INFO] - Training Epoch: 3/10, step 102/574 completed (loss: 0.24778705835342407, acc: 0.8695651888847351)
[2024-11-13 06:19:31,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:31,817][root][INFO] - Training Epoch: 3/10, step 103/574 completed (loss: 0.32021692395210266, acc: 0.8863636255264282)
[2024-11-13 06:19:32,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:32,259][root][INFO] - Training Epoch: 3/10, step 104/574 completed (loss: 0.8331705331802368, acc: 0.7586206793785095)
[2024-11-13 06:19:32,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:32,648][root][INFO] - Training Epoch: 3/10, step 105/574 completed (loss: 0.5118973255157471, acc: 0.8372092843055725)
[2024-11-13 06:19:32,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:33,049][root][INFO] - Training Epoch: 3/10, step 106/574 completed (loss: 0.5103023648262024, acc: 0.8399999737739563)
[2024-11-13 06:19:33,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:33,522][root][INFO] - Training Epoch: 3/10, step 107/574 completed (loss: 0.04781222715973854, acc: 1.0)
[2024-11-13 06:19:33,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:33,988][root][INFO] - Training Epoch: 3/10, step 108/574 completed (loss: 0.11637939512729645, acc: 0.9615384340286255)
[2024-11-13 06:19:34,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:34,397][root][INFO] - Training Epoch: 3/10, step 109/574 completed (loss: 0.37637263536453247, acc: 0.9047619104385376)
[2024-11-13 06:19:34,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:34,840][root][INFO] - Training Epoch: 3/10, step 110/574 completed (loss: 0.5613363981246948, acc: 0.8307692408561707)
[2024-11-13 06:19:35,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:35,390][root][INFO] - Training Epoch: 3/10, step 111/574 completed (loss: 0.8951799869537354, acc: 0.7543859481811523)
[2024-11-13 06:19:35,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:35,837][root][INFO] - Training Epoch: 3/10, step 112/574 completed (loss: 1.1478711366653442, acc: 0.6666666865348816)
[2024-11-13 06:19:36,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:36,266][root][INFO] - Training Epoch: 3/10, step 113/574 completed (loss: 0.6131049990653992, acc: 0.7948718070983887)
[2024-11-13 06:19:36,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:36,735][root][INFO] - Training Epoch: 3/10, step 114/574 completed (loss: 0.4338301718235016, acc: 0.8367347121238708)
[2024-11-13 06:19:36,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:37,102][root][INFO] - Training Epoch: 3/10, step 115/574 completed (loss: 0.116362564265728, acc: 0.9545454382896423)
[2024-11-13 06:19:37,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:37,530][root][INFO] - Training Epoch: 3/10, step 116/574 completed (loss: 0.9925987124443054, acc: 0.7460317611694336)
[2024-11-13 06:19:37,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:37,901][root][INFO] - Training Epoch: 3/10, step 117/574 completed (loss: 0.890049159526825, acc: 0.772357702255249)
[2024-11-13 06:19:38,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:38,290][root][INFO] - Training Epoch: 3/10, step 118/574 completed (loss: 0.7382848858833313, acc: 0.7580645084381104)
[2024-11-13 06:19:38,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:39,567][root][INFO] - Training Epoch: 3/10, step 119/574 completed (loss: 1.2549201250076294, acc: 0.6463878154754639)
[2024-11-13 06:19:39,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:40,002][root][INFO] - Training Epoch: 3/10, step 120/574 completed (loss: 0.5960354208946228, acc: 0.800000011920929)
[2024-11-13 06:19:40,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:40,561][root][INFO] - Training Epoch: 3/10, step 121/574 completed (loss: 0.8796873092651367, acc: 0.75)
[2024-11-13 06:19:40,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:40,927][root][INFO] - Training Epoch: 3/10, step 122/574 completed (loss: 0.42871811985969543, acc: 0.8333333134651184)
[2024-11-13 06:19:41,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:41,323][root][INFO] - Training Epoch: 3/10, step 123/574 completed (loss: 0.601351261138916, acc: 0.8421052694320679)
[2024-11-13 06:19:41,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:41,728][root][INFO] - Training Epoch: 3/10, step 124/574 completed (loss: 1.4127413034439087, acc: 0.5766870975494385)
[2024-11-13 06:19:41,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:42,176][root][INFO] - Training Epoch: 3/10, step 125/574 completed (loss: 1.6012743711471558, acc: 0.6111111044883728)
[2024-11-13 06:19:42,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:42,606][root][INFO] - Training Epoch: 3/10, step 126/574 completed (loss: 1.5123919248580933, acc: 0.5666666626930237)
[2024-11-13 06:19:42,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:43,054][root][INFO] - Training Epoch: 3/10, step 127/574 completed (loss: 1.6099613904953003, acc: 0.5357142686843872)
[2024-11-13 06:19:43,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:43,474][root][INFO] - Training Epoch: 3/10, step 128/574 completed (loss: 1.2063465118408203, acc: 0.6615384817123413)
[2024-11-13 06:19:43,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:44,013][root][INFO] - Training Epoch: 3/10, step 129/574 completed (loss: 1.366866111755371, acc: 0.6617646813392639)
[2024-11-13 06:19:44,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:44,416][root][INFO] - Training Epoch: 3/10, step 130/574 completed (loss: 0.9191965460777283, acc: 0.7307692170143127)
[2024-11-13 06:19:44,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:44,846][root][INFO] - Training Epoch: 3/10, step 131/574 completed (loss: 0.39916348457336426, acc: 0.8260869383811951)
[2024-11-13 06:19:45,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:45,276][root][INFO] - Training Epoch: 3/10, step 132/574 completed (loss: 0.770817756652832, acc: 0.8125)
[2024-11-13 06:19:45,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:45,722][root][INFO] - Training Epoch: 3/10, step 133/574 completed (loss: 1.1513432264328003, acc: 0.6521739363670349)
[2024-11-13 06:19:45,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:46,154][root][INFO] - Training Epoch: 3/10, step 134/574 completed (loss: 0.9453784227371216, acc: 0.6285714507102966)
[2024-11-13 06:19:46,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:46,556][root][INFO] - Training Epoch: 3/10, step 135/574 completed (loss: 0.6050137877464294, acc: 0.7692307829856873)
[2024-11-13 06:19:46,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:46,939][root][INFO] - Training Epoch: 3/10, step 136/574 completed (loss: 1.0927419662475586, acc: 0.7142857313156128)
[2024-11-13 06:19:47,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:47,366][root][INFO] - Training Epoch: 3/10, step 137/574 completed (loss: 1.1754246950149536, acc: 0.699999988079071)
[2024-11-13 06:19:47,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:47,787][root][INFO] - Training Epoch: 3/10, step 138/574 completed (loss: 0.9270240664482117, acc: 0.695652186870575)
[2024-11-13 06:19:48,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:49,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:49,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:50,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:50,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:51,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:51,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:52,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:52,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:53,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:53,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:54,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:55,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:55,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:56,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:56,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:57,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:57,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:58,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:58,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:58,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:59,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:19:59,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:00,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:00,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:01,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:01,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:02,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:02,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:02,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:03,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:04,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:04,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:04,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:05,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:05,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:06,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:06,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:07,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:07,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:08,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:08,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:09,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:09,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:10,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:10,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:11,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:12,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:12,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:12,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:13,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:13,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:14,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:14,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:15,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:15,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:16,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:16,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:17,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:17,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:18,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:18,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:19,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:19,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:20,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:20,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:21,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:21,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:22,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:23,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:23,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:24,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:24,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:25,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:25,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:26,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:26,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:27,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:27,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:28,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:28,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:29,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:29,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:29,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:30,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:31,084][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3846, device='cuda:0') eval_epoch_loss=tensor(0.8690, device='cuda:0') eval_epoch_acc=tensor(0.7573, device='cuda:0')
[2024-11-13 06:20:31,086][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:20:31,086][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:20:31,474][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_3_step_139_loss_0.8690188527107239/model.pt
[2024-11-13 06:20:31,483][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:20:31,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:31,925][root][INFO] - Training Epoch: 3/10, step 139/574 completed (loss: 1.0369211435317993, acc: 0.761904776096344)
[2024-11-13 06:20:32,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:32,349][root][INFO] - Training Epoch: 3/10, step 140/574 completed (loss: 1.0595085620880127, acc: 0.6538461446762085)
[2024-11-13 06:20:32,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:32,759][root][INFO] - Training Epoch: 3/10, step 141/574 completed (loss: 1.091259241104126, acc: 0.7096773982048035)
[2024-11-13 06:20:32,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:33,220][root][INFO] - Training Epoch: 3/10, step 142/574 completed (loss: 0.8332341313362122, acc: 0.7567567825317383)
[2024-11-13 06:20:33,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:34,004][root][INFO] - Training Epoch: 3/10, step 143/574 completed (loss: 1.033940315246582, acc: 0.6842105388641357)
[2024-11-13 06:20:34,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:34,447][root][INFO] - Training Epoch: 3/10, step 144/574 completed (loss: 1.1391370296478271, acc: 0.7089552283287048)
[2024-11-13 06:20:34,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:34,875][root][INFO] - Training Epoch: 3/10, step 145/574 completed (loss: 0.9751687049865723, acc: 0.704081654548645)
[2024-11-13 06:20:35,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:35,513][root][INFO] - Training Epoch: 3/10, step 146/574 completed (loss: 1.2552015781402588, acc: 0.6063829660415649)
[2024-11-13 06:20:35,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:35,939][root][INFO] - Training Epoch: 3/10, step 147/574 completed (loss: 0.9612302780151367, acc: 0.6714285612106323)
[2024-11-13 06:20:36,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:36,305][root][INFO] - Training Epoch: 3/10, step 148/574 completed (loss: 0.9839614033699036, acc: 0.5357142686843872)
[2024-11-13 06:20:36,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:36,721][root][INFO] - Training Epoch: 3/10, step 149/574 completed (loss: 0.893843948841095, acc: 0.739130437374115)
[2024-11-13 06:20:36,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:37,124][root][INFO] - Training Epoch: 3/10, step 150/574 completed (loss: 1.0844451189041138, acc: 0.7586206793785095)
[2024-11-13 06:20:37,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:37,486][root][INFO] - Training Epoch: 3/10, step 151/574 completed (loss: 1.3945831060409546, acc: 0.695652186870575)
[2024-11-13 06:20:37,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:37,919][root][INFO] - Training Epoch: 3/10, step 152/574 completed (loss: 0.9573649764060974, acc: 0.7627118825912476)
[2024-11-13 06:20:38,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:38,296][root][INFO] - Training Epoch: 3/10, step 153/574 completed (loss: 1.396048665046692, acc: 0.6315789222717285)
[2024-11-13 06:20:38,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:38,701][root][INFO] - Training Epoch: 3/10, step 154/574 completed (loss: 0.7592371106147766, acc: 0.7837837934494019)
[2024-11-13 06:20:38,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:39,060][root][INFO] - Training Epoch: 3/10, step 155/574 completed (loss: 0.6566843390464783, acc: 0.8571428656578064)
[2024-11-13 06:20:39,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:39,542][root][INFO] - Training Epoch: 3/10, step 156/574 completed (loss: 0.5980004072189331, acc: 0.9130434989929199)
[2024-11-13 06:20:39,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:39,990][root][INFO] - Training Epoch: 3/10, step 157/574 completed (loss: 2.4178483486175537, acc: 0.31578946113586426)
[2024-11-13 06:20:41,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:42,653][root][INFO] - Training Epoch: 3/10, step 158/574 completed (loss: 1.2027510404586792, acc: 0.5810810923576355)
[2024-11-13 06:20:42,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:43,021][root][INFO] - Training Epoch: 3/10, step 159/574 completed (loss: 1.6961259841918945, acc: 0.48148149251937866)
[2024-11-13 06:20:43,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:43,579][root][INFO] - Training Epoch: 3/10, step 160/574 completed (loss: 1.5680112838745117, acc: 0.5581395626068115)
[2024-11-13 06:20:44,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:44,489][root][INFO] - Training Epoch: 3/10, step 161/574 completed (loss: 1.6617381572723389, acc: 0.529411792755127)
[2024-11-13 06:20:44,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:45,367][root][INFO] - Training Epoch: 3/10, step 162/574 completed (loss: 1.6732585430145264, acc: 0.550561785697937)
[2024-11-13 06:20:45,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:45,771][root][INFO] - Training Epoch: 3/10, step 163/574 completed (loss: 0.5318546891212463, acc: 0.8636363744735718)
[2024-11-13 06:20:45,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:46,141][root][INFO] - Training Epoch: 3/10, step 164/574 completed (loss: 0.5176842212677002, acc: 0.9047619104385376)
[2024-11-13 06:20:46,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:46,573][root][INFO] - Training Epoch: 3/10, step 165/574 completed (loss: 0.8685613870620728, acc: 0.7241379022598267)
[2024-11-13 06:20:46,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:47,057][root][INFO] - Training Epoch: 3/10, step 166/574 completed (loss: 0.3570539951324463, acc: 0.8775510191917419)
[2024-11-13 06:20:47,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:47,493][root][INFO] - Training Epoch: 3/10, step 167/574 completed (loss: 0.48928725719451904, acc: 0.8199999928474426)
[2024-11-13 06:20:47,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:48,009][root][INFO] - Training Epoch: 3/10, step 168/574 completed (loss: 0.6914081573486328, acc: 0.7638888955116272)
[2024-11-13 06:20:48,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:48,460][root][INFO] - Training Epoch: 3/10, step 169/574 completed (loss: 1.4333113431930542, acc: 0.6960784196853638)
[2024-11-13 06:20:49,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:50,161][root][INFO] - Training Epoch: 3/10, step 170/574 completed (loss: 1.641294002532959, acc: 0.5753424763679504)
[2024-11-13 06:20:50,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:50,590][root][INFO] - Training Epoch: 3/10, step 171/574 completed (loss: 0.33518335223197937, acc: 0.9583333134651184)
[2024-11-13 06:20:50,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:51,005][root][INFO] - Training Epoch: 3/10, step 172/574 completed (loss: 0.9905776381492615, acc: 0.7407407164573669)
[2024-11-13 06:20:51,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:51,343][root][INFO] - Training Epoch: 3/10, step 173/574 completed (loss: 0.6579315066337585, acc: 0.8928571343421936)
[2024-11-13 06:20:51,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:52,143][root][INFO] - Training Epoch: 3/10, step 174/574 completed (loss: 1.213514804840088, acc: 0.6814159154891968)
[2024-11-13 06:20:52,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:52,553][root][INFO] - Training Epoch: 3/10, step 175/574 completed (loss: 1.120460033416748, acc: 0.6666666865348816)
[2024-11-13 06:20:52,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:53,016][root][INFO] - Training Epoch: 3/10, step 176/574 completed (loss: 0.6762722134590149, acc: 0.7954545617103577)
[2024-11-13 06:20:53,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:54,477][root][INFO] - Training Epoch: 3/10, step 177/574 completed (loss: 1.593180775642395, acc: 0.580152690410614)
[2024-11-13 06:20:55,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:55,527][root][INFO] - Training Epoch: 3/10, step 178/574 completed (loss: 1.1681876182556152, acc: 0.644444465637207)
[2024-11-13 06:20:55,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:55,930][root][INFO] - Training Epoch: 3/10, step 179/574 completed (loss: 0.7502228617668152, acc: 0.7868852615356445)
[2024-11-13 06:20:56,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:56,365][root][INFO] - Training Epoch: 3/10, step 180/574 completed (loss: 0.05238690599799156, acc: 1.0)
[2024-11-13 06:20:56,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:56,777][root][INFO] - Training Epoch: 3/10, step 181/574 completed (loss: 0.3778337836265564, acc: 0.8799999952316284)
[2024-11-13 06:20:56,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:57,186][root][INFO] - Training Epoch: 3/10, step 182/574 completed (loss: 0.27035120129585266, acc: 0.9642857313156128)
[2024-11-13 06:20:57,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:57,637][root][INFO] - Training Epoch: 3/10, step 183/574 completed (loss: 0.5816138386726379, acc: 0.8048780560493469)
[2024-11-13 06:20:57,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:58,107][root][INFO] - Training Epoch: 3/10, step 184/574 completed (loss: 0.8135256767272949, acc: 0.7885196208953857)
[2024-11-13 06:20:58,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:58,584][root][INFO] - Training Epoch: 3/10, step 185/574 completed (loss: 0.946847677230835, acc: 0.7492795586585999)
[2024-11-13 06:20:58,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:20:59,288][root][INFO] - Training Epoch: 3/10, step 186/574 completed (loss: 1.0513535737991333, acc: 0.706250011920929)
[2024-11-13 06:20:59,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:00,033][root][INFO] - Training Epoch: 3/10, step 187/574 completed (loss: 0.9816818833351135, acc: 0.7504690289497375)
[2024-11-13 06:21:00,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:00,571][root][INFO] - Training Epoch: 3/10, step 188/574 completed (loss: 0.9594911336898804, acc: 0.7188612222671509)
[2024-11-13 06:21:00,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:00,946][root][INFO] - Training Epoch: 3/10, step 189/574 completed (loss: 0.7555713057518005, acc: 0.800000011920929)
[2024-11-13 06:21:01,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:01,777][root][INFO] - Training Epoch: 3/10, step 190/574 completed (loss: 1.1687384843826294, acc: 0.6627907156944275)
[2024-11-13 06:21:02,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:03,045][root][INFO] - Training Epoch: 3/10, step 191/574 completed (loss: 1.560152530670166, acc: 0.5158730149269104)
[2024-11-13 06:21:03,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:04,528][root][INFO] - Training Epoch: 3/10, step 192/574 completed (loss: 1.2025816440582275, acc: 0.6363636255264282)
[2024-11-13 06:21:05,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:05,703][root][INFO] - Training Epoch: 3/10, step 193/574 completed (loss: 0.9092089533805847, acc: 0.7411764860153198)
[2024-11-13 06:21:06,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:07,459][root][INFO] - Training Epoch: 3/10, step 194/574 completed (loss: 1.2761517763137817, acc: 0.6172839403152466)
[2024-11-13 06:21:08,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:09,006][root][INFO] - Training Epoch: 3/10, step 195/574 completed (loss: 0.7023981213569641, acc: 0.8064516186714172)
[2024-11-13 06:21:09,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:09,425][root][INFO] - Training Epoch: 3/10, step 196/574 completed (loss: 0.25590601563453674, acc: 0.9642857313156128)
[2024-11-13 06:21:09,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:09,870][root][INFO] - Training Epoch: 3/10, step 197/574 completed (loss: 1.1056468486785889, acc: 0.75)
[2024-11-13 06:21:10,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:10,360][root][INFO] - Training Epoch: 3/10, step 198/574 completed (loss: 1.1751199960708618, acc: 0.6176470518112183)
[2024-11-13 06:21:10,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:10,767][root][INFO] - Training Epoch: 3/10, step 199/574 completed (loss: 1.1673136949539185, acc: 0.6985294222831726)
[2024-11-13 06:21:10,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:11,157][root][INFO] - Training Epoch: 3/10, step 200/574 completed (loss: 0.928016722202301, acc: 0.7372881174087524)
[2024-11-13 06:21:11,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:11,597][root][INFO] - Training Epoch: 3/10, step 201/574 completed (loss: 1.1400492191314697, acc: 0.7238805890083313)
[2024-11-13 06:21:11,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:12,046][root][INFO] - Training Epoch: 3/10, step 202/574 completed (loss: 1.0348279476165771, acc: 0.7475728392601013)
[2024-11-13 06:21:12,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:12,456][root][INFO] - Training Epoch: 3/10, step 203/574 completed (loss: 1.0661406517028809, acc: 0.6666666865348816)
[2024-11-13 06:21:12,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:12,831][root][INFO] - Training Epoch: 3/10, step 204/574 completed (loss: 0.5023522973060608, acc: 0.901098906993866)
[2024-11-13 06:21:13,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:13,270][root][INFO] - Training Epoch: 3/10, step 205/574 completed (loss: 0.640640377998352, acc: 0.8295964002609253)
[2024-11-13 06:21:13,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:13,798][root][INFO] - Training Epoch: 3/10, step 206/574 completed (loss: 0.8787225484848022, acc: 0.748031497001648)
[2024-11-13 06:21:13,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:14,230][root][INFO] - Training Epoch: 3/10, step 207/574 completed (loss: 0.674204409122467, acc: 0.8146551847457886)
[2024-11-13 06:21:14,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:14,688][root][INFO] - Training Epoch: 3/10, step 208/574 completed (loss: 0.6942857503890991, acc: 0.8079710006713867)
[2024-11-13 06:21:14,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:15,154][root][INFO] - Training Epoch: 3/10, step 209/574 completed (loss: 0.6872984766960144, acc: 0.8249027132987976)
[2024-11-13 06:21:15,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:15,568][root][INFO] - Training Epoch: 3/10, step 210/574 completed (loss: 0.8232792019844055, acc: 0.79347825050354)
[2024-11-13 06:21:15,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:15,973][root][INFO] - Training Epoch: 3/10, step 211/574 completed (loss: 0.21354898810386658, acc: 0.95652174949646)
[2024-11-13 06:21:16,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:16,397][root][INFO] - Training Epoch: 3/10, step 212/574 completed (loss: 0.2610311210155487, acc: 0.9642857313156128)
[2024-11-13 06:21:16,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:16,783][root][INFO] - Training Epoch: 3/10, step 213/574 completed (loss: 0.3385116159915924, acc: 0.8936170339584351)
[2024-11-13 06:21:17,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:17,864][root][INFO] - Training Epoch: 3/10, step 214/574 completed (loss: 0.3413480818271637, acc: 0.892307698726654)
[2024-11-13 06:21:18,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:18,338][root][INFO] - Training Epoch: 3/10, step 215/574 completed (loss: 0.24978245794773102, acc: 0.9459459185600281)
[2024-11-13 06:21:18,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:18,826][root][INFO] - Training Epoch: 3/10, step 216/574 completed (loss: 0.28656065464019775, acc: 0.930232584476471)
[2024-11-13 06:21:19,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:19,627][root][INFO] - Training Epoch: 3/10, step 217/574 completed (loss: 0.35443469882011414, acc: 0.9009009003639221)
[2024-11-13 06:21:19,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:20,163][root][INFO] - Training Epoch: 3/10, step 218/574 completed (loss: 0.24892757833003998, acc: 0.9111111164093018)
[2024-11-13 06:21:20,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:20,571][root][INFO] - Training Epoch: 3/10, step 219/574 completed (loss: 0.2882562279701233, acc: 0.939393937587738)
[2024-11-13 06:21:20,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:20,965][root][INFO] - Training Epoch: 3/10, step 220/574 completed (loss: 0.06402796506881714, acc: 1.0)
[2024-11-13 06:21:21,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:21,388][root][INFO] - Training Epoch: 3/10, step 221/574 completed (loss: 0.33480358123779297, acc: 0.8799999952316284)
[2024-11-13 06:21:21,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:21,786][root][INFO] - Training Epoch: 3/10, step 222/574 completed (loss: 0.8803158402442932, acc: 0.7307692170143127)
[2024-11-13 06:21:22,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:22,962][root][INFO] - Training Epoch: 3/10, step 223/574 completed (loss: 0.5161178112030029, acc: 0.8369565010070801)
[2024-11-13 06:21:23,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:23,769][root][INFO] - Training Epoch: 3/10, step 224/574 completed (loss: 0.7564224004745483, acc: 0.7954545617103577)
[2024-11-13 06:21:24,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:24,393][root][INFO] - Training Epoch: 3/10, step 225/574 completed (loss: 1.0828627347946167, acc: 0.7340425252914429)
[2024-11-13 06:21:24,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:24,845][root][INFO] - Training Epoch: 3/10, step 226/574 completed (loss: 0.777076780796051, acc: 0.7735849022865295)
[2024-11-13 06:21:25,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:25,288][root][INFO] - Training Epoch: 3/10, step 227/574 completed (loss: 0.42749807238578796, acc: 0.8999999761581421)
[2024-11-13 06:21:25,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:25,713][root][INFO] - Training Epoch: 3/10, step 228/574 completed (loss: 0.33986321091651917, acc: 0.8837209343910217)
[2024-11-13 06:21:25,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:26,095][root][INFO] - Training Epoch: 3/10, step 229/574 completed (loss: 1.3137710094451904, acc: 0.5333333611488342)
[2024-11-13 06:21:26,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:26,568][root][INFO] - Training Epoch: 3/10, step 230/574 completed (loss: 2.203986883163452, acc: 0.42105263471603394)
[2024-11-13 06:21:26,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:27,053][root][INFO] - Training Epoch: 3/10, step 231/574 completed (loss: 1.6707053184509277, acc: 0.5666666626930237)
[2024-11-13 06:21:27,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:27,642][root][INFO] - Training Epoch: 3/10, step 232/574 completed (loss: 1.523167371749878, acc: 0.6166666746139526)
[2024-11-13 06:21:27,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:28,355][root][INFO] - Training Epoch: 3/10, step 233/574 completed (loss: 1.7407941818237305, acc: 0.5550458431243896)
[2024-11-13 06:21:28,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:29,048][root][INFO] - Training Epoch: 3/10, step 234/574 completed (loss: 1.5572853088378906, acc: 0.5846154093742371)
[2024-11-13 06:21:29,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:29,481][root][INFO] - Training Epoch: 3/10, step 235/574 completed (loss: 0.5524910092353821, acc: 0.7894737124443054)
[2024-11-13 06:21:29,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:29,869][root][INFO] - Training Epoch: 3/10, step 236/574 completed (loss: 0.7694478034973145, acc: 0.7083333134651184)
[2024-11-13 06:21:30,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:30,287][root][INFO] - Training Epoch: 3/10, step 237/574 completed (loss: 1.1705384254455566, acc: 0.6363636255264282)
[2024-11-13 06:21:30,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:30,689][root][INFO] - Training Epoch: 3/10, step 238/574 completed (loss: 0.9443163871765137, acc: 0.7777777910232544)
[2024-11-13 06:21:30,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:31,040][root][INFO] - Training Epoch: 3/10, step 239/574 completed (loss: 0.8511145114898682, acc: 0.7714285850524902)
[2024-11-13 06:21:31,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:31,460][root][INFO] - Training Epoch: 3/10, step 240/574 completed (loss: 1.1967800855636597, acc: 0.5909090638160706)
[2024-11-13 06:21:31,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:31,845][root][INFO] - Training Epoch: 3/10, step 241/574 completed (loss: 0.8426650762557983, acc: 0.75)
[2024-11-13 06:21:32,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:32,734][root][INFO] - Training Epoch: 3/10, step 242/574 completed (loss: 1.5248197317123413, acc: 0.5645161271095276)
[2024-11-13 06:21:33,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:33,539][root][INFO] - Training Epoch: 3/10, step 243/574 completed (loss: 1.22600257396698, acc: 0.7045454382896423)
[2024-11-13 06:21:33,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:33,884][root][INFO] - Training Epoch: 3/10, step 244/574 completed (loss: 0.4395560622215271, acc: 0.9047619104385376)
[2024-11-13 06:21:34,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:34,250][root][INFO] - Training Epoch: 3/10, step 245/574 completed (loss: 0.4028746485710144, acc: 0.8461538553237915)
[2024-11-13 06:21:34,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:34,622][root][INFO] - Training Epoch: 3/10, step 246/574 completed (loss: 0.6501708030700684, acc: 0.8387096524238586)
[2024-11-13 06:21:34,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:35,048][root][INFO] - Training Epoch: 3/10, step 247/574 completed (loss: 0.5254504084587097, acc: 0.800000011920929)
[2024-11-13 06:21:35,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:35,509][root][INFO] - Training Epoch: 3/10, step 248/574 completed (loss: 0.6936625838279724, acc: 0.837837815284729)
[2024-11-13 06:21:35,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:35,949][root][INFO] - Training Epoch: 3/10, step 249/574 completed (loss: 0.5636271238327026, acc: 0.8648648858070374)
[2024-11-13 06:21:36,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:36,404][root][INFO] - Training Epoch: 3/10, step 250/574 completed (loss: 0.4419030249118805, acc: 0.8648648858070374)
[2024-11-13 06:21:36,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:36,821][root][INFO] - Training Epoch: 3/10, step 251/574 completed (loss: 0.5212628245353699, acc: 0.8382353186607361)
[2024-11-13 06:21:36,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:37,253][root][INFO] - Training Epoch: 3/10, step 252/574 completed (loss: 0.2844228148460388, acc: 0.8536585569381714)
[2024-11-13 06:21:37,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:37,596][root][INFO] - Training Epoch: 3/10, step 253/574 completed (loss: 0.10142721235752106, acc: 1.0)
[2024-11-13 06:21:37,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:38,002][root][INFO] - Training Epoch: 3/10, step 254/574 completed (loss: 0.08425819128751755, acc: 1.0)
[2024-11-13 06:21:38,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:38,410][root][INFO] - Training Epoch: 3/10, step 255/574 completed (loss: 0.18924586474895477, acc: 0.9354838728904724)
[2024-11-13 06:21:38,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:38,798][root][INFO] - Training Epoch: 3/10, step 256/574 completed (loss: 0.5536822080612183, acc: 0.9122806787490845)
[2024-11-13 06:21:38,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:39,182][root][INFO] - Training Epoch: 3/10, step 257/574 completed (loss: 0.330583393573761, acc: 0.8999999761581421)
[2024-11-13 06:21:39,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:39,560][root][INFO] - Training Epoch: 3/10, step 258/574 completed (loss: 0.36768463253974915, acc: 0.9078947305679321)
[2024-11-13 06:21:39,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:40,405][root][INFO] - Training Epoch: 3/10, step 259/574 completed (loss: 0.6988463401794434, acc: 0.8113207817077637)
[2024-11-13 06:21:40,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:41,281][root][INFO] - Training Epoch: 3/10, step 260/574 completed (loss: 0.7085159420967102, acc: 0.800000011920929)
[2024-11-13 06:21:41,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:41,606][root][INFO] - Training Epoch: 3/10, step 261/574 completed (loss: 0.26563259959220886, acc: 0.8888888955116272)
[2024-11-13 06:21:41,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:42,041][root][INFO] - Training Epoch: 3/10, step 262/574 completed (loss: 0.6491245031356812, acc: 0.8064516186714172)
[2024-11-13 06:21:42,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:42,451][root][INFO] - Training Epoch: 3/10, step 263/574 completed (loss: 1.4598643779754639, acc: 0.5866666436195374)
[2024-11-13 06:21:42,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:42,895][root][INFO] - Training Epoch: 3/10, step 264/574 completed (loss: 1.0369328260421753, acc: 0.5833333134651184)
[2024-11-13 06:21:43,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:44,255][root][INFO] - Training Epoch: 3/10, step 265/574 completed (loss: 1.6420146226882935, acc: 0.5680000185966492)
[2024-11-13 06:21:44,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:44,631][root][INFO] - Training Epoch: 3/10, step 266/574 completed (loss: 1.3138749599456787, acc: 0.6292135119438171)
[2024-11-13 06:21:44,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:45,088][root][INFO] - Training Epoch: 3/10, step 267/574 completed (loss: 1.2883952856063843, acc: 0.6486486196517944)
[2024-11-13 06:21:45,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:45,752][root][INFO] - Training Epoch: 3/10, step 268/574 completed (loss: 0.7846542596817017, acc: 0.7931034564971924)
[2024-11-13 06:21:45,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:46,116][root][INFO] - Training Epoch: 3/10, step 269/574 completed (loss: 0.11823637783527374, acc: 0.9545454382896423)
[2024-11-13 06:21:46,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:46,529][root][INFO] - Training Epoch: 3/10, step 270/574 completed (loss: 0.24596582353115082, acc: 0.9545454382896423)
[2024-11-13 06:21:46,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:46,886][root][INFO] - Training Epoch: 3/10, step 271/574 completed (loss: 0.12113862484693527, acc: 0.9375)
[2024-11-13 06:21:47,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:47,294][root][INFO] - Training Epoch: 3/10, step 272/574 completed (loss: 0.16753597557544708, acc: 0.9666666388511658)
[2024-11-13 06:21:47,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:47,826][root][INFO] - Training Epoch: 3/10, step 273/574 completed (loss: 0.6338197588920593, acc: 0.8166666626930237)
[2024-11-13 06:21:48,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:48,262][root][INFO] - Training Epoch: 3/10, step 274/574 completed (loss: 0.3033076226711273, acc: 0.9375)
[2024-11-13 06:21:48,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:48,656][root][INFO] - Training Epoch: 3/10, step 275/574 completed (loss: 0.41453754901885986, acc: 0.9333333373069763)
[2024-11-13 06:21:48,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:49,034][root][INFO] - Training Epoch: 3/10, step 276/574 completed (loss: 0.28492820262908936, acc: 0.9655172228813171)
[2024-11-13 06:21:49,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:49,461][root][INFO] - Training Epoch: 3/10, step 277/574 completed (loss: 0.28998202085494995, acc: 0.9599999785423279)
[2024-11-13 06:21:49,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:49,931][root][INFO] - Training Epoch: 3/10, step 278/574 completed (loss: 0.7689292430877686, acc: 0.7659574747085571)
[2024-11-13 06:21:50,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:50,356][root][INFO] - Training Epoch: 3/10, step 279/574 completed (loss: 0.6315038800239563, acc: 0.7916666865348816)
[2024-11-13 06:21:50,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:50,777][root][INFO] - Training Epoch: 3/10, step 280/574 completed (loss: 0.3351016640663147, acc: 0.9318181872367859)
[2024-11-13 06:21:51,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:51,370][root][INFO] - Training Epoch: 3/10, step 281/574 completed (loss: 1.055273175239563, acc: 0.7108433842658997)
[2024-11-13 06:21:52,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:52,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:52,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:53,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:53,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:54,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:54,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:55,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:55,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:56,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:57,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:57,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:58,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:58,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:59,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:21:59,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:00,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:00,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:00,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:01,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:01,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:02,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:02,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:03,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:03,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:04,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:04,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:05,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:05,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:06,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:06,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:07,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:07,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:08,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:08,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:09,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:09,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:10,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:10,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:11,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:11,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:12,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:12,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:13,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:13,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:14,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:14,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:14,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:15,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:16,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:16,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:16,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:17,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:17,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:18,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:18,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:19,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:19,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:20,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:20,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:21,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:22,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:22,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:23,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:23,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:24,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:24,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:25,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:26,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:26,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:27,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:27,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:28,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:28,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:29,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:29,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:29,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:30,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:30,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:31,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:31,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:32,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:32,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:33,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:33,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:34,542][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1673, device='cuda:0') eval_epoch_loss=tensor(0.7735, device='cuda:0') eval_epoch_acc=tensor(0.7852, device='cuda:0')
[2024-11-13 06:22:34,543][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:22:34,544][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:22:34,868][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_3_step_282_loss_0.773464560508728/model.pt
[2024-11-13 06:22:34,873][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:22:34,874][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 0.773464560508728
[2024-11-13 06:22:34,874][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.78523850440979
[2024-11-13 06:22:35,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:35,385][root][INFO] - Training Epoch: 3/10, step 282/574 completed (loss: 1.234886884689331, acc: 0.6296296119689941)
[2024-11-13 06:22:35,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:35,799][root][INFO] - Training Epoch: 3/10, step 283/574 completed (loss: 0.430042564868927, acc: 0.8947368264198303)
[2024-11-13 06:22:35,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:36,211][root][INFO] - Training Epoch: 3/10, step 284/574 completed (loss: 0.7270314693450928, acc: 0.7647058963775635)
[2024-11-13 06:22:36,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:36,692][root][INFO] - Training Epoch: 3/10, step 285/574 completed (loss: 0.4517783522605896, acc: 0.875)
[2024-11-13 06:22:36,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:37,124][root][INFO] - Training Epoch: 3/10, step 286/574 completed (loss: 0.7307259440422058, acc: 0.8046875)
[2024-11-13 06:22:37,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:37,515][root][INFO] - Training Epoch: 3/10, step 287/574 completed (loss: 0.9109030365943909, acc: 0.7760000228881836)
[2024-11-13 06:22:37,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:37,940][root][INFO] - Training Epoch: 3/10, step 288/574 completed (loss: 0.6409426331520081, acc: 0.8131868243217468)
[2024-11-13 06:22:38,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:38,357][root][INFO] - Training Epoch: 3/10, step 289/574 completed (loss: 0.6998171806335449, acc: 0.7950310707092285)
[2024-11-13 06:22:38,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:38,806][root][INFO] - Training Epoch: 3/10, step 290/574 completed (loss: 0.924869179725647, acc: 0.7731958627700806)
[2024-11-13 06:22:39,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:39,239][root][INFO] - Training Epoch: 3/10, step 291/574 completed (loss: 0.2597576379776001, acc: 0.9090909361839294)
[2024-11-13 06:22:39,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:39,634][root][INFO] - Training Epoch: 3/10, step 292/574 completed (loss: 0.568245530128479, acc: 0.8571428656578064)
[2024-11-13 06:22:39,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:40,070][root][INFO] - Training Epoch: 3/10, step 293/574 completed (loss: 0.39564257860183716, acc: 0.8965517282485962)
[2024-11-13 06:22:40,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:40,734][root][INFO] - Training Epoch: 3/10, step 294/574 completed (loss: 0.6306940913200378, acc: 0.8363636136054993)
[2024-11-13 06:22:41,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:41,538][root][INFO] - Training Epoch: 3/10, step 295/574 completed (loss: 0.8287835121154785, acc: 0.7371134161949158)
[2024-11-13 06:22:41,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:41,927][root][INFO] - Training Epoch: 3/10, step 296/574 completed (loss: 0.764755368232727, acc: 0.7758620977401733)
[2024-11-13 06:22:42,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:42,354][root][INFO] - Training Epoch: 3/10, step 297/574 completed (loss: 0.3667789101600647, acc: 0.8888888955116272)
[2024-11-13 06:22:42,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:42,771][root][INFO] - Training Epoch: 3/10, step 298/574 completed (loss: 0.596133291721344, acc: 0.7631579041481018)
[2024-11-13 06:22:42,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:43,187][root][INFO] - Training Epoch: 3/10, step 299/574 completed (loss: 0.48848041892051697, acc: 0.8571428656578064)
[2024-11-13 06:22:43,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:43,582][root][INFO] - Training Epoch: 3/10, step 300/574 completed (loss: 0.07269342243671417, acc: 1.0)
[2024-11-13 06:22:43,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:43,987][root][INFO] - Training Epoch: 3/10, step 301/574 completed (loss: 0.3562375605106354, acc: 0.8867924809455872)
[2024-11-13 06:22:44,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:44,412][root][INFO] - Training Epoch: 3/10, step 302/574 completed (loss: 0.0511770099401474, acc: 1.0)
[2024-11-13 06:22:44,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:44,805][root][INFO] - Training Epoch: 3/10, step 303/574 completed (loss: 0.18520811200141907, acc: 0.970588207244873)
[2024-11-13 06:22:44,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:45,163][root][INFO] - Training Epoch: 3/10, step 304/574 completed (loss: 0.37133753299713135, acc: 0.9375)
[2024-11-13 06:22:45,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:45,583][root][INFO] - Training Epoch: 3/10, step 305/574 completed (loss: 0.5993436574935913, acc: 0.8524590134620667)
[2024-11-13 06:22:45,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:45,975][root][INFO] - Training Epoch: 3/10, step 306/574 completed (loss: 0.08673456311225891, acc: 1.0)
[2024-11-13 06:22:46,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:46,305][root][INFO] - Training Epoch: 3/10, step 307/574 completed (loss: 0.014640952460467815, acc: 1.0)
[2024-11-13 06:22:46,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:46,676][root][INFO] - Training Epoch: 3/10, step 308/574 completed (loss: 0.5594623684883118, acc: 0.8695651888847351)
[2024-11-13 06:22:46,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:47,256][root][INFO] - Training Epoch: 3/10, step 309/574 completed (loss: 0.5557733774185181, acc: 0.8611111044883728)
[2024-11-13 06:22:47,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:47,648][root][INFO] - Training Epoch: 3/10, step 310/574 completed (loss: 0.4377844035625458, acc: 0.8433734774589539)
[2024-11-13 06:22:47,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:48,024][root][INFO] - Training Epoch: 3/10, step 311/574 completed (loss: 0.699190616607666, acc: 0.8333333134651184)
[2024-11-13 06:22:48,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:48,469][root][INFO] - Training Epoch: 3/10, step 312/574 completed (loss: 0.44226136803627014, acc: 0.8775510191917419)
[2024-11-13 06:22:48,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:48,816][root][INFO] - Training Epoch: 3/10, step 313/574 completed (loss: 0.02619754709303379, acc: 1.0)
[2024-11-13 06:22:48,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:49,183][root][INFO] - Training Epoch: 3/10, step 314/574 completed (loss: 0.2955133020877838, acc: 0.875)
[2024-11-13 06:22:49,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:49,567][root][INFO] - Training Epoch: 3/10, step 315/574 completed (loss: 0.4029463231563568, acc: 0.8709677457809448)
[2024-11-13 06:22:49,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:49,896][root][INFO] - Training Epoch: 3/10, step 316/574 completed (loss: 0.536159098148346, acc: 0.8387096524238586)
[2024-11-13 06:22:50,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:50,303][root][INFO] - Training Epoch: 3/10, step 317/574 completed (loss: 0.3698863685131073, acc: 0.8805969953536987)
[2024-11-13 06:22:50,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:50,768][root][INFO] - Training Epoch: 3/10, step 318/574 completed (loss: 0.20722416043281555, acc: 0.942307710647583)
[2024-11-13 06:22:50,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:51,153][root][INFO] - Training Epoch: 3/10, step 319/574 completed (loss: 0.2967330813407898, acc: 0.8888888955116272)
[2024-11-13 06:22:51,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:51,510][root][INFO] - Training Epoch: 3/10, step 320/574 completed (loss: 0.2950974702835083, acc: 0.9032257795333862)
[2024-11-13 06:22:51,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:51,864][root][INFO] - Training Epoch: 3/10, step 321/574 completed (loss: 0.0844150260090828, acc: 0.9599999785423279)
[2024-11-13 06:22:52,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:52,255][root][INFO] - Training Epoch: 3/10, step 322/574 completed (loss: 1.5168700218200684, acc: 0.6296296119689941)
[2024-11-13 06:22:52,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:52,614][root][INFO] - Training Epoch: 3/10, step 323/574 completed (loss: 1.7773184776306152, acc: 0.48571428656578064)
[2024-11-13 06:22:52,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:52,999][root][INFO] - Training Epoch: 3/10, step 324/574 completed (loss: 1.215843915939331, acc: 0.692307710647583)
[2024-11-13 06:22:53,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:53,357][root][INFO] - Training Epoch: 3/10, step 325/574 completed (loss: 1.9799531698226929, acc: 0.4390243887901306)
[2024-11-13 06:22:53,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:53,744][root][INFO] - Training Epoch: 3/10, step 326/574 completed (loss: 1.305616855621338, acc: 0.6052631735801697)
[2024-11-13 06:22:53,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:54,088][root][INFO] - Training Epoch: 3/10, step 327/574 completed (loss: 0.2613564431667328, acc: 0.9473684430122375)
[2024-11-13 06:22:54,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:54,437][root][INFO] - Training Epoch: 3/10, step 328/574 completed (loss: 0.13461337983608246, acc: 0.9285714030265808)
[2024-11-13 06:22:54,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:54,848][root][INFO] - Training Epoch: 3/10, step 329/574 completed (loss: 0.2402375191450119, acc: 0.9259259104728699)
[2024-11-13 06:22:55,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:55,241][root][INFO] - Training Epoch: 3/10, step 330/574 completed (loss: 0.1836586445569992, acc: 0.9375)
[2024-11-13 06:22:55,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:55,622][root][INFO] - Training Epoch: 3/10, step 331/574 completed (loss: 0.5803437829017639, acc: 0.8548387289047241)
[2024-11-13 06:22:55,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:56,093][root][INFO] - Training Epoch: 3/10, step 332/574 completed (loss: 0.3303930461406708, acc: 0.9122806787490845)
[2024-11-13 06:22:56,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:56,447][root][INFO] - Training Epoch: 3/10, step 333/574 completed (loss: 0.49779191613197327, acc: 0.875)
[2024-11-13 06:22:56,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:56,839][root][INFO] - Training Epoch: 3/10, step 334/574 completed (loss: 0.3061661124229431, acc: 0.8999999761581421)
[2024-11-13 06:22:56,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:57,184][root][INFO] - Training Epoch: 3/10, step 335/574 completed (loss: 0.4469958245754242, acc: 0.7894737124443054)
[2024-11-13 06:22:57,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:57,561][root][INFO] - Training Epoch: 3/10, step 336/574 completed (loss: 0.9657671451568604, acc: 0.7599999904632568)
[2024-11-13 06:22:57,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:57,976][root][INFO] - Training Epoch: 3/10, step 337/574 completed (loss: 1.5025110244750977, acc: 0.540229856967926)
[2024-11-13 06:22:58,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:58,396][root][INFO] - Training Epoch: 3/10, step 338/574 completed (loss: 1.3595863580703735, acc: 0.563829779624939)
[2024-11-13 06:22:58,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:58,819][root][INFO] - Training Epoch: 3/10, step 339/574 completed (loss: 1.5559648275375366, acc: 0.6144578456878662)
[2024-11-13 06:22:58,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:59,190][root][INFO] - Training Epoch: 3/10, step 340/574 completed (loss: 0.22331024706363678, acc: 0.9130434989929199)
[2024-11-13 06:22:59,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:22:59,604][root][INFO] - Training Epoch: 3/10, step 341/574 completed (loss: 0.7281847596168518, acc: 0.8717948794364929)
[2024-11-13 06:22:59,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:00,092][root][INFO] - Training Epoch: 3/10, step 342/574 completed (loss: 0.7307326197624207, acc: 0.8192771077156067)
[2024-11-13 06:23:00,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:00,488][root][INFO] - Training Epoch: 3/10, step 343/574 completed (loss: 0.6981369853019714, acc: 0.8113207817077637)
[2024-11-13 06:23:00,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:00,877][root][INFO] - Training Epoch: 3/10, step 344/574 completed (loss: 0.5440360307693481, acc: 0.8607594966888428)
[2024-11-13 06:23:01,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:01,300][root][INFO] - Training Epoch: 3/10, step 345/574 completed (loss: 0.43700265884399414, acc: 0.8235294222831726)
[2024-11-13 06:23:01,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:01,646][root][INFO] - Training Epoch: 3/10, step 346/574 completed (loss: 0.7416159510612488, acc: 0.8208954930305481)
[2024-11-13 06:23:01,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:02,068][root][INFO] - Training Epoch: 3/10, step 347/574 completed (loss: 0.041998717933893204, acc: 1.0)
[2024-11-13 06:23:02,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:02,474][root][INFO] - Training Epoch: 3/10, step 348/574 completed (loss: 0.22911466658115387, acc: 0.9200000166893005)
[2024-11-13 06:23:02,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:03,001][root][INFO] - Training Epoch: 3/10, step 349/574 completed (loss: 0.7139776349067688, acc: 0.8611111044883728)
[2024-11-13 06:23:03,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:03,435][root][INFO] - Training Epoch: 3/10, step 350/574 completed (loss: 0.8661565780639648, acc: 0.6976743936538696)
[2024-11-13 06:23:03,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:03,859][root][INFO] - Training Epoch: 3/10, step 351/574 completed (loss: 0.756831705570221, acc: 0.7948718070983887)
[2024-11-13 06:23:04,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:04,368][root][INFO] - Training Epoch: 3/10, step 352/574 completed (loss: 1.475193977355957, acc: 0.6000000238418579)
[2024-11-13 06:23:04,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:04,797][root][INFO] - Training Epoch: 3/10, step 353/574 completed (loss: 0.22309131920337677, acc: 0.95652174949646)
[2024-11-13 06:23:05,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:05,260][root][INFO] - Training Epoch: 3/10, step 354/574 completed (loss: 0.7984423041343689, acc: 0.807692289352417)
[2024-11-13 06:23:05,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:05,680][root][INFO] - Training Epoch: 3/10, step 355/574 completed (loss: 1.2747946977615356, acc: 0.6703296899795532)
[2024-11-13 06:23:06,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:06,429][root][INFO] - Training Epoch: 3/10, step 356/574 completed (loss: 0.9196813106536865, acc: 0.8086956739425659)
[2024-11-13 06:23:06,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:06,858][root][INFO] - Training Epoch: 3/10, step 357/574 completed (loss: 0.7702486515045166, acc: 0.79347825050354)
[2024-11-13 06:23:07,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:07,333][root][INFO] - Training Epoch: 3/10, step 358/574 completed (loss: 0.8377101421356201, acc: 0.7755101919174194)
[2024-11-13 06:23:07,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:07,726][root][INFO] - Training Epoch: 3/10, step 359/574 completed (loss: 0.019338762387633324, acc: 1.0)
[2024-11-13 06:23:07,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:08,081][root][INFO] - Training Epoch: 3/10, step 360/574 completed (loss: 0.40143436193466187, acc: 0.8461538553237915)
[2024-11-13 06:23:08,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:08,424][root][INFO] - Training Epoch: 3/10, step 361/574 completed (loss: 0.6386712193489075, acc: 0.8048780560493469)
[2024-11-13 06:23:08,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:08,799][root][INFO] - Training Epoch: 3/10, step 362/574 completed (loss: 0.7634490132331848, acc: 0.8222222328186035)
[2024-11-13 06:23:08,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:09,198][root][INFO] - Training Epoch: 3/10, step 363/574 completed (loss: 0.37254878878593445, acc: 0.8947368264198303)
[2024-11-13 06:23:09,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:09,595][root][INFO] - Training Epoch: 3/10, step 364/574 completed (loss: 0.4340863525867462, acc: 0.8292682766914368)
[2024-11-13 06:23:09,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:10,008][root][INFO] - Training Epoch: 3/10, step 365/574 completed (loss: 0.23696516454219818, acc: 0.9090909361839294)
[2024-11-13 06:23:10,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:10,370][root][INFO] - Training Epoch: 3/10, step 366/574 completed (loss: 0.037269335240125656, acc: 1.0)
[2024-11-13 06:23:10,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:10,785][root][INFO] - Training Epoch: 3/10, step 367/574 completed (loss: 0.038243185728788376, acc: 1.0)
[2024-11-13 06:23:10,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:11,167][root][INFO] - Training Epoch: 3/10, step 368/574 completed (loss: 0.15472856163978577, acc: 0.9642857313156128)
[2024-11-13 06:23:11,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:11,566][root][INFO] - Training Epoch: 3/10, step 369/574 completed (loss: 0.49713951349258423, acc: 0.90625)
[2024-11-13 06:23:12,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:12,501][root][INFO] - Training Epoch: 3/10, step 370/574 completed (loss: 0.8988460898399353, acc: 0.7575757503509521)
[2024-11-13 06:23:13,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:13,824][root][INFO] - Training Epoch: 3/10, step 371/574 completed (loss: 0.5085557699203491, acc: 0.8679245114326477)
[2024-11-13 06:23:14,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:14,246][root][INFO] - Training Epoch: 3/10, step 372/574 completed (loss: 0.4564555585384369, acc: 0.8888888955116272)
[2024-11-13 06:23:14,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:14,633][root][INFO] - Training Epoch: 3/10, step 373/574 completed (loss: 0.20721228420734406, acc: 0.9107142686843872)
[2024-11-13 06:23:14,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:15,045][root][INFO] - Training Epoch: 3/10, step 374/574 completed (loss: 0.21860122680664062, acc: 0.9142857193946838)
[2024-11-13 06:23:15,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:15,457][root][INFO] - Training Epoch: 3/10, step 375/574 completed (loss: 0.04103361442685127, acc: 1.0)
[2024-11-13 06:23:15,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:15,873][root][INFO] - Training Epoch: 3/10, step 376/574 completed (loss: 0.13512149453163147, acc: 0.95652174949646)
[2024-11-13 06:23:16,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:16,345][root][INFO] - Training Epoch: 3/10, step 377/574 completed (loss: 0.3453649580478668, acc: 0.9166666865348816)
[2024-11-13 06:23:16,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:16,784][root][INFO] - Training Epoch: 3/10, step 378/574 completed (loss: 0.1214551031589508, acc: 0.9684210419654846)
[2024-11-13 06:23:17,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:17,655][root][INFO] - Training Epoch: 3/10, step 379/574 completed (loss: 0.4325525760650635, acc: 0.886227548122406)
[2024-11-13 06:23:17,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:18,198][root][INFO] - Training Epoch: 3/10, step 380/574 completed (loss: 0.414986789226532, acc: 0.8947368264198303)
[2024-11-13 06:23:19,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:20,106][root][INFO] - Training Epoch: 3/10, step 381/574 completed (loss: 0.6036564707756042, acc: 0.8395721912384033)
[2024-11-13 06:23:20,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:20,953][root][INFO] - Training Epoch: 3/10, step 382/574 completed (loss: 0.17706434428691864, acc: 0.9459459185600281)
[2024-11-13 06:23:21,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:21,377][root][INFO] - Training Epoch: 3/10, step 383/574 completed (loss: 0.3225558400154114, acc: 0.8928571343421936)
[2024-11-13 06:23:21,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:21,732][root][INFO] - Training Epoch: 3/10, step 384/574 completed (loss: 0.14520122110843658, acc: 0.9642857313156128)
[2024-11-13 06:23:21,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:22,135][root][INFO] - Training Epoch: 3/10, step 385/574 completed (loss: 0.1604585349559784, acc: 0.96875)
[2024-11-13 06:23:22,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:22,519][root][INFO] - Training Epoch: 3/10, step 386/574 completed (loss: 0.054171424359083176, acc: 0.9722222089767456)
[2024-11-13 06:23:22,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:22,912][root][INFO] - Training Epoch: 3/10, step 387/574 completed (loss: 0.05415546894073486, acc: 0.9736841917037964)
[2024-11-13 06:23:23,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:23,325][root][INFO] - Training Epoch: 3/10, step 388/574 completed (loss: 0.020240293815732002, acc: 1.0)
[2024-11-13 06:23:23,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:23,756][root][INFO] - Training Epoch: 3/10, step 389/574 completed (loss: 0.10012663900852203, acc: 0.949999988079071)
[2024-11-13 06:23:23,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:24,136][root][INFO] - Training Epoch: 3/10, step 390/574 completed (loss: 0.43936192989349365, acc: 0.9047619104385376)
[2024-11-13 06:23:24,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:24,561][root][INFO] - Training Epoch: 3/10, step 391/574 completed (loss: 1.3642504215240479, acc: 0.6851851940155029)
[2024-11-13 06:23:24,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:24,989][root][INFO] - Training Epoch: 3/10, step 392/574 completed (loss: 1.1372238397598267, acc: 0.6699029207229614)
[2024-11-13 06:23:25,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:25,764][root][INFO] - Training Epoch: 3/10, step 393/574 completed (loss: 1.1774142980575562, acc: 0.720588207244873)
[2024-11-13 06:23:25,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:26,260][root][INFO] - Training Epoch: 3/10, step 394/574 completed (loss: 0.9613072872161865, acc: 0.7133333086967468)
[2024-11-13 06:23:26,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:26,780][root][INFO] - Training Epoch: 3/10, step 395/574 completed (loss: 0.8252286911010742, acc: 0.7777777910232544)
[2024-11-13 06:23:26,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:27,190][root][INFO] - Training Epoch: 3/10, step 396/574 completed (loss: 0.468614399433136, acc: 0.8604651093482971)
[2024-11-13 06:23:27,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:27,593][root][INFO] - Training Epoch: 3/10, step 397/574 completed (loss: 0.27967211604118347, acc: 0.9166666865348816)
[2024-11-13 06:23:27,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:28,039][root][INFO] - Training Epoch: 3/10, step 398/574 completed (loss: 0.3302095830440521, acc: 0.8837209343910217)
[2024-11-13 06:23:28,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:28,485][root][INFO] - Training Epoch: 3/10, step 399/574 completed (loss: 0.3516238331794739, acc: 0.9200000166893005)
[2024-11-13 06:23:28,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:29,281][root][INFO] - Training Epoch: 3/10, step 400/574 completed (loss: 0.43284252285957336, acc: 0.8676470518112183)
[2024-11-13 06:23:29,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:29,711][root][INFO] - Training Epoch: 3/10, step 401/574 completed (loss: 0.5758686661720276, acc: 0.8399999737739563)
[2024-11-13 06:23:29,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:30,080][root][INFO] - Training Epoch: 3/10, step 402/574 completed (loss: 0.6696946024894714, acc: 0.8181818127632141)
[2024-11-13 06:23:30,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:30,476][root][INFO] - Training Epoch: 3/10, step 403/574 completed (loss: 0.36260440945625305, acc: 0.8787878751754761)
[2024-11-13 06:23:30,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:30,906][root][INFO] - Training Epoch: 3/10, step 404/574 completed (loss: 0.1543985903263092, acc: 0.9354838728904724)
[2024-11-13 06:23:31,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:31,263][root][INFO] - Training Epoch: 3/10, step 405/574 completed (loss: 0.3227989077568054, acc: 0.9259259104728699)
[2024-11-13 06:23:31,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:31,622][root][INFO] - Training Epoch: 3/10, step 406/574 completed (loss: 0.1598694920539856, acc: 0.9599999785423279)
[2024-11-13 06:23:31,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:32,011][root][INFO] - Training Epoch: 3/10, step 407/574 completed (loss: 0.22316353023052216, acc: 0.9166666865348816)
[2024-11-13 06:23:32,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:32,437][root][INFO] - Training Epoch: 3/10, step 408/574 completed (loss: 0.2246170938014984, acc: 0.9259259104728699)
[2024-11-13 06:23:32,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:32,831][root][INFO] - Training Epoch: 3/10, step 409/574 completed (loss: 0.049000322818756104, acc: 1.0)
[2024-11-13 06:23:32,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:33,189][root][INFO] - Training Epoch: 3/10, step 410/574 completed (loss: 0.22290359437465668, acc: 0.9482758641242981)
[2024-11-13 06:23:33,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:33,562][root][INFO] - Training Epoch: 3/10, step 411/574 completed (loss: 0.33423444628715515, acc: 0.9285714030265808)
[2024-11-13 06:23:33,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:34,011][root][INFO] - Training Epoch: 3/10, step 412/574 completed (loss: 0.3522559106349945, acc: 0.9333333373069763)
[2024-11-13 06:23:34,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:34,390][root][INFO] - Training Epoch: 3/10, step 413/574 completed (loss: 0.13599471747875214, acc: 0.9696969985961914)
[2024-11-13 06:23:34,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:34,779][root][INFO] - Training Epoch: 3/10, step 414/574 completed (loss: 0.029572743922472, acc: 1.0)
[2024-11-13 06:23:34,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:35,149][root][INFO] - Training Epoch: 3/10, step 415/574 completed (loss: 0.5577201843261719, acc: 0.8039215803146362)
[2024-11-13 06:23:35,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:35,514][root][INFO] - Training Epoch: 3/10, step 416/574 completed (loss: 0.3742671310901642, acc: 0.8461538553237915)
[2024-11-13 06:23:35,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:35,868][root][INFO] - Training Epoch: 3/10, step 417/574 completed (loss: 0.4181115925312042, acc: 0.8333333134651184)
[2024-11-13 06:23:36,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:36,267][root][INFO] - Training Epoch: 3/10, step 418/574 completed (loss: 0.2962856888771057, acc: 0.8999999761581421)
[2024-11-13 06:23:36,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:36,686][root][INFO] - Training Epoch: 3/10, step 419/574 completed (loss: 0.19902697205543518, acc: 0.949999988079071)
[2024-11-13 06:23:36,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:37,107][root][INFO] - Training Epoch: 3/10, step 420/574 completed (loss: 0.12977634370326996, acc: 0.9523809552192688)
[2024-11-13 06:23:37,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:37,507][root][INFO] - Training Epoch: 3/10, step 421/574 completed (loss: 0.31652334332466125, acc: 0.9333333373069763)
[2024-11-13 06:23:37,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:37,879][root][INFO] - Training Epoch: 3/10, step 422/574 completed (loss: 0.4923396706581116, acc: 0.84375)
[2024-11-13 06:23:38,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:38,243][root][INFO] - Training Epoch: 3/10, step 423/574 completed (loss: 0.48099714517593384, acc: 0.8888888955116272)
[2024-11-13 06:23:38,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:38,593][root][INFO] - Training Epoch: 3/10, step 424/574 completed (loss: 0.15544287860393524, acc: 0.9629629850387573)
[2024-11-13 06:23:39,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:39,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:40,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:40,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:41,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:41,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:41,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:42,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:42,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:43,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:44,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:44,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:45,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:45,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:46,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:46,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:47,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:47,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:48,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:48,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:49,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:49,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:50,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:50,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:51,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:51,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:52,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:52,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:53,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:53,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:54,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:54,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:55,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:55,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:56,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:56,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:57,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:57,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:58,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:58,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:59,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:59,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:23:59,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:00,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:00,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:01,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:01,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:02,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:02,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:03,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:03,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:04,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:04,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:05,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:05,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:05,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:06,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:06,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:07,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:07,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:08,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:09,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:09,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:10,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:10,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:11,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:11,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:12,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:12,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:13,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:13,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:14,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:14,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:15,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:15,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:16,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:16,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:17,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:17,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:18,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:18,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:19,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:20,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:20,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:21,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:21,865][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1659, device='cuda:0') eval_epoch_loss=tensor(0.7728, device='cuda:0') eval_epoch_acc=tensor(0.7896, device='cuda:0')
[2024-11-13 06:24:21,866][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:24:21,866][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:24:22,310][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_3_step_425_loss_0.772824764251709/model.pt
[2024-11-13 06:24:22,319][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:24:22,320][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 0.772824764251709
[2024-11-13 06:24:22,321][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.7895928025245667
[2024-11-13 06:24:22,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:22,788][root][INFO] - Training Epoch: 3/10, step 425/574 completed (loss: 0.18094375729560852, acc: 0.9090909361839294)
[2024-11-13 06:24:22,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:23,171][root][INFO] - Training Epoch: 3/10, step 426/574 completed (loss: 0.07521064579486847, acc: 0.95652174949646)
[2024-11-13 06:24:23,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:23,560][root][INFO] - Training Epoch: 3/10, step 427/574 completed (loss: 0.1472071260213852, acc: 0.9459459185600281)
[2024-11-13 06:24:23,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:24,005][root][INFO] - Training Epoch: 3/10, step 428/574 completed (loss: 0.05193951353430748, acc: 1.0)
[2024-11-13 06:24:24,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:24,419][root][INFO] - Training Epoch: 3/10, step 429/574 completed (loss: 0.11349689215421677, acc: 0.95652174949646)
[2024-11-13 06:24:24,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:24,838][root][INFO] - Training Epoch: 3/10, step 430/574 completed (loss: 0.007702184375375509, acc: 1.0)
[2024-11-13 06:24:25,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:25,308][root][INFO] - Training Epoch: 3/10, step 431/574 completed (loss: 0.06082940474152565, acc: 0.9629629850387573)
[2024-11-13 06:24:25,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:25,739][root][INFO] - Training Epoch: 3/10, step 432/574 completed (loss: 0.192733034491539, acc: 0.95652174949646)
[2024-11-13 06:24:25,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:26,192][root][INFO] - Training Epoch: 3/10, step 433/574 completed (loss: 0.29808151721954346, acc: 0.9166666865348816)
[2024-11-13 06:24:26,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:26,693][root][INFO] - Training Epoch: 3/10, step 434/574 completed (loss: 0.008847326971590519, acc: 1.0)
[2024-11-13 06:24:26,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:27,111][root][INFO] - Training Epoch: 3/10, step 435/574 completed (loss: 0.03979961574077606, acc: 1.0)
[2024-11-13 06:24:27,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:27,545][root][INFO] - Training Epoch: 3/10, step 436/574 completed (loss: 0.2856159508228302, acc: 0.8888888955116272)
[2024-11-13 06:24:27,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:27,924][root][INFO] - Training Epoch: 3/10, step 437/574 completed (loss: 0.26868006587028503, acc: 0.9318181872367859)
[2024-11-13 06:24:28,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:28,277][root][INFO] - Training Epoch: 3/10, step 438/574 completed (loss: 0.013809926807880402, acc: 1.0)
[2024-11-13 06:24:28,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:28,649][root][INFO] - Training Epoch: 3/10, step 439/574 completed (loss: 0.6674293279647827, acc: 0.8717948794364929)
[2024-11-13 06:24:28,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:29,326][root][INFO] - Training Epoch: 3/10, step 440/574 completed (loss: 0.4903746545314789, acc: 0.8484848737716675)
[2024-11-13 06:24:29,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:30,396][root][INFO] - Training Epoch: 3/10, step 441/574 completed (loss: 0.9941582083702087, acc: 0.7039999961853027)
[2024-11-13 06:24:30,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:30,961][root][INFO] - Training Epoch: 3/10, step 442/574 completed (loss: 1.219293475151062, acc: 0.6935483813285828)
[2024-11-13 06:24:31,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:31,973][root][INFO] - Training Epoch: 3/10, step 443/574 completed (loss: 0.8540070056915283, acc: 0.7661691308021545)
[2024-11-13 06:24:32,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:32,317][root][INFO] - Training Epoch: 3/10, step 444/574 completed (loss: 0.44876813888549805, acc: 0.8867924809455872)
[2024-11-13 06:24:32,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:32,877][root][INFO] - Training Epoch: 3/10, step 445/574 completed (loss: 0.2610585391521454, acc: 0.9090909361839294)
[2024-11-13 06:24:33,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:33,209][root][INFO] - Training Epoch: 3/10, step 446/574 completed (loss: 0.44948405027389526, acc: 0.9130434989929199)
[2024-11-13 06:24:33,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:33,580][root][INFO] - Training Epoch: 3/10, step 447/574 completed (loss: 0.4015372097492218, acc: 0.8846153616905212)
[2024-11-13 06:24:33,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:33,971][root][INFO] - Training Epoch: 3/10, step 448/574 completed (loss: 0.30374982953071594, acc: 0.9285714030265808)
[2024-11-13 06:24:34,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:34,381][root][INFO] - Training Epoch: 3/10, step 449/574 completed (loss: 0.37605613470077515, acc: 0.9104477763175964)
[2024-11-13 06:24:34,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:34,784][root][INFO] - Training Epoch: 3/10, step 450/574 completed (loss: 0.11539679020643234, acc: 0.9583333134651184)
[2024-11-13 06:24:34,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:35,153][root][INFO] - Training Epoch: 3/10, step 451/574 completed (loss: 0.15470054745674133, acc: 0.967391312122345)
[2024-11-13 06:24:35,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:35,560][root][INFO] - Training Epoch: 3/10, step 452/574 completed (loss: 0.44242197275161743, acc: 0.8846153616905212)
[2024-11-13 06:24:35,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:35,947][root][INFO] - Training Epoch: 3/10, step 453/574 completed (loss: 0.5233787894248962, acc: 0.8552631735801697)
[2024-11-13 06:24:36,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:36,311][root][INFO] - Training Epoch: 3/10, step 454/574 completed (loss: 0.3161555826663971, acc: 0.918367326259613)
[2024-11-13 06:24:36,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:36,652][root][INFO] - Training Epoch: 3/10, step 455/574 completed (loss: 0.3974049389362335, acc: 0.9090909361839294)
[2024-11-13 06:24:36,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:37,017][root][INFO] - Training Epoch: 3/10, step 456/574 completed (loss: 0.8722971677780151, acc: 0.7731958627700806)
[2024-11-13 06:24:37,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:37,379][root][INFO] - Training Epoch: 3/10, step 457/574 completed (loss: 0.22438344359397888, acc: 0.9142857193946838)
[2024-11-13 06:24:37,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:37,879][root][INFO] - Training Epoch: 3/10, step 458/574 completed (loss: 0.8335175514221191, acc: 0.7790697813034058)
[2024-11-13 06:24:38,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:38,228][root][INFO] - Training Epoch: 3/10, step 459/574 completed (loss: 0.39431077241897583, acc: 0.8928571343421936)
[2024-11-13 06:24:38,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:38,573][root][INFO] - Training Epoch: 3/10, step 460/574 completed (loss: 0.7429556846618652, acc: 0.790123462677002)
[2024-11-13 06:24:38,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:39,013][root][INFO] - Training Epoch: 3/10, step 461/574 completed (loss: 0.6118296384811401, acc: 0.8055555820465088)
[2024-11-13 06:24:39,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:39,376][root][INFO] - Training Epoch: 3/10, step 462/574 completed (loss: 0.15018469095230103, acc: 0.96875)
[2024-11-13 06:24:39,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:39,754][root][INFO] - Training Epoch: 3/10, step 463/574 completed (loss: 0.3952704668045044, acc: 0.8461538553237915)
[2024-11-13 06:24:39,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:40,104][root][INFO] - Training Epoch: 3/10, step 464/574 completed (loss: 0.4262729585170746, acc: 0.8695651888847351)
[2024-11-13 06:24:40,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:40,470][root][INFO] - Training Epoch: 3/10, step 465/574 completed (loss: 0.4967145025730133, acc: 0.8571428656578064)
[2024-11-13 06:24:40,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:40,886][root][INFO] - Training Epoch: 3/10, step 466/574 completed (loss: 0.8791567087173462, acc: 0.7710843086242676)
[2024-11-13 06:24:41,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:41,324][root][INFO] - Training Epoch: 3/10, step 467/574 completed (loss: 0.43119561672210693, acc: 0.8828828930854797)
[2024-11-13 06:24:41,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:41,726][root][INFO] - Training Epoch: 3/10, step 468/574 completed (loss: 1.0241305828094482, acc: 0.737864077091217)
[2024-11-13 06:24:41,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:42,108][root][INFO] - Training Epoch: 3/10, step 469/574 completed (loss: 0.8549761176109314, acc: 0.7642276287078857)
[2024-11-13 06:24:42,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:42,457][root][INFO] - Training Epoch: 3/10, step 470/574 completed (loss: 0.18305189907550812, acc: 0.9583333134651184)
[2024-11-13 06:24:42,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:42,859][root][INFO] - Training Epoch: 3/10, step 471/574 completed (loss: 0.48264047503471375, acc: 0.8571428656578064)
[2024-11-13 06:24:43,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:43,427][root][INFO] - Training Epoch: 3/10, step 472/574 completed (loss: 0.6871513724327087, acc: 0.7843137383460999)
[2024-11-13 06:24:43,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:43,864][root][INFO] - Training Epoch: 3/10, step 473/574 completed (loss: 0.9358826279640198, acc: 0.7379912734031677)
[2024-11-13 06:24:44,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:44,237][root][INFO] - Training Epoch: 3/10, step 474/574 completed (loss: 0.8082044124603271, acc: 0.7708333134651184)
[2024-11-13 06:24:44,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:44,650][root][INFO] - Training Epoch: 3/10, step 475/574 completed (loss: 0.5588793754577637, acc: 0.803680956363678)
[2024-11-13 06:24:44,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:45,109][root][INFO] - Training Epoch: 3/10, step 476/574 completed (loss: 0.5336729288101196, acc: 0.8489208817481995)
[2024-11-13 06:24:45,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:45,550][root][INFO] - Training Epoch: 3/10, step 477/574 completed (loss: 0.852039098739624, acc: 0.7537688612937927)
[2024-11-13 06:24:45,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:45,967][root][INFO] - Training Epoch: 3/10, step 478/574 completed (loss: 0.6524331569671631, acc: 0.7777777910232544)
[2024-11-13 06:24:46,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:46,328][root][INFO] - Training Epoch: 3/10, step 479/574 completed (loss: 0.4540703594684601, acc: 0.7878788113594055)
[2024-11-13 06:24:46,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:46,658][root][INFO] - Training Epoch: 3/10, step 480/574 completed (loss: 0.29763245582580566, acc: 0.9259259104728699)
[2024-11-13 06:24:46,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:47,037][root][INFO] - Training Epoch: 3/10, step 481/574 completed (loss: 0.48163318634033203, acc: 0.75)
[2024-11-13 06:24:47,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:47,420][root][INFO] - Training Epoch: 3/10, step 482/574 completed (loss: 0.5038930773735046, acc: 0.8999999761581421)
[2024-11-13 06:24:47,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:47,921][root][INFO] - Training Epoch: 3/10, step 483/574 completed (loss: 0.9275851249694824, acc: 0.7931034564971924)
[2024-11-13 06:24:48,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:48,276][root][INFO] - Training Epoch: 3/10, step 484/574 completed (loss: 0.1828158050775528, acc: 0.9354838728904724)
[2024-11-13 06:24:48,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:48,629][root][INFO] - Training Epoch: 3/10, step 485/574 completed (loss: 0.45282474160194397, acc: 0.8947368264198303)
[2024-11-13 06:24:48,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:49,042][root][INFO] - Training Epoch: 3/10, step 486/574 completed (loss: 0.7592694163322449, acc: 0.7777777910232544)
[2024-11-13 06:24:49,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:49,420][root][INFO] - Training Epoch: 3/10, step 487/574 completed (loss: 0.5182766318321228, acc: 0.8095238208770752)
[2024-11-13 06:24:49,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:49,803][root][INFO] - Training Epoch: 3/10, step 488/574 completed (loss: 0.2993603050708771, acc: 0.9090909361839294)
[2024-11-13 06:24:49,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:50,241][root][INFO] - Training Epoch: 3/10, step 489/574 completed (loss: 1.0812008380889893, acc: 0.6307692527770996)
[2024-11-13 06:24:50,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:50,595][root][INFO] - Training Epoch: 3/10, step 490/574 completed (loss: 0.20069915056228638, acc: 0.9333333373069763)
[2024-11-13 06:24:50,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:50,950][root][INFO] - Training Epoch: 3/10, step 491/574 completed (loss: 0.4487724006175995, acc: 0.8965517282485962)
[2024-11-13 06:24:51,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:51,275][root][INFO] - Training Epoch: 3/10, step 492/574 completed (loss: 0.4755602180957794, acc: 0.8039215803146362)
[2024-11-13 06:24:51,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:51,626][root][INFO] - Training Epoch: 3/10, step 493/574 completed (loss: 0.5988989472389221, acc: 0.8275862336158752)
[2024-11-13 06:24:51,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:51,981][root][INFO] - Training Epoch: 3/10, step 494/574 completed (loss: 0.5030602216720581, acc: 0.9473684430122375)
[2024-11-13 06:24:52,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:52,342][root][INFO] - Training Epoch: 3/10, step 495/574 completed (loss: 0.4583120346069336, acc: 0.8947368264198303)
[2024-11-13 06:24:52,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:52,778][root][INFO] - Training Epoch: 3/10, step 496/574 completed (loss: 0.8082976937294006, acc: 0.7767857313156128)
[2024-11-13 06:24:52,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:53,278][root][INFO] - Training Epoch: 3/10, step 497/574 completed (loss: 0.4616844058036804, acc: 0.8426966071128845)
[2024-11-13 06:24:53,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:53,714][root][INFO] - Training Epoch: 3/10, step 498/574 completed (loss: 0.8739187121391296, acc: 0.7078651785850525)
[2024-11-13 06:24:53,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:54,105][root][INFO] - Training Epoch: 3/10, step 499/574 completed (loss: 1.3079445362091064, acc: 0.652482271194458)
[2024-11-13 06:24:54,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:54,510][root][INFO] - Training Epoch: 3/10, step 500/574 completed (loss: 0.9359192848205566, acc: 0.760869562625885)
[2024-11-13 06:24:54,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:54,898][root][INFO] - Training Epoch: 3/10, step 501/574 completed (loss: 0.017079852521419525, acc: 1.0)
[2024-11-13 06:24:55,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:55,280][root][INFO] - Training Epoch: 3/10, step 502/574 completed (loss: 0.08718141168355942, acc: 0.9615384340286255)
[2024-11-13 06:24:55,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:55,673][root][INFO] - Training Epoch: 3/10, step 503/574 completed (loss: 0.13823066651821136, acc: 0.9629629850387573)
[2024-11-13 06:24:55,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:56,054][root][INFO] - Training Epoch: 3/10, step 504/574 completed (loss: 0.22061549127101898, acc: 0.8888888955116272)
[2024-11-13 06:24:56,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:56,538][root][INFO] - Training Epoch: 3/10, step 505/574 completed (loss: 0.6382623910903931, acc: 0.8301886916160583)
[2024-11-13 06:24:56,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:56,899][root][INFO] - Training Epoch: 3/10, step 506/574 completed (loss: 0.6398040056228638, acc: 0.7586206793785095)
[2024-11-13 06:24:57,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:57,794][root][INFO] - Training Epoch: 3/10, step 507/574 completed (loss: 1.2666544914245605, acc: 0.6576576828956604)
[2024-11-13 06:24:58,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:58,436][root][INFO] - Training Epoch: 3/10, step 508/574 completed (loss: 0.9191032648086548, acc: 0.8028169274330139)
[2024-11-13 06:24:58,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:58,841][root][INFO] - Training Epoch: 3/10, step 509/574 completed (loss: 0.5694829225540161, acc: 0.8999999761581421)
[2024-11-13 06:24:59,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:59,218][root][INFO] - Training Epoch: 3/10, step 510/574 completed (loss: 0.31472069025039673, acc: 0.8999999761581421)
[2024-11-13 06:24:59,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:24:59,552][root][INFO] - Training Epoch: 3/10, step 511/574 completed (loss: 0.3375571370124817, acc: 0.9230769276618958)
[2024-11-13 06:25:01,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:03,296][root][INFO] - Training Epoch: 3/10, step 512/574 completed (loss: 1.2087262868881226, acc: 0.6928571462631226)
[2024-11-13 06:25:03,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:04,500][root][INFO] - Training Epoch: 3/10, step 513/574 completed (loss: 0.5967615246772766, acc: 0.8095238208770752)
[2024-11-13 06:25:04,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:04,837][root][INFO] - Training Epoch: 3/10, step 514/574 completed (loss: 0.6045059561729431, acc: 0.8928571343421936)
[2024-11-13 06:25:04,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:05,198][root][INFO] - Training Epoch: 3/10, step 515/574 completed (loss: 0.2083793580532074, acc: 0.9166666865348816)
[2024-11-13 06:25:05,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:06,268][root][INFO] - Training Epoch: 3/10, step 516/574 completed (loss: 0.7656251192092896, acc: 0.8333333134651184)
[2024-11-13 06:25:06,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:06,706][root][INFO] - Training Epoch: 3/10, step 517/574 completed (loss: 0.038395658135414124, acc: 1.0)
[2024-11-13 06:25:06,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:07,106][root][INFO] - Training Epoch: 3/10, step 518/574 completed (loss: 0.20742091536521912, acc: 0.9354838728904724)
[2024-11-13 06:25:07,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:07,543][root][INFO] - Training Epoch: 3/10, step 519/574 completed (loss: 0.26090574264526367, acc: 0.8999999761581421)
[2024-11-13 06:25:07,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:07,930][root][INFO] - Training Epoch: 3/10, step 520/574 completed (loss: 0.3956005871295929, acc: 0.9259259104728699)
[2024-11-13 06:25:08,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:09,528][root][INFO] - Training Epoch: 3/10, step 521/574 completed (loss: 0.9602492451667786, acc: 0.7033898234367371)
[2024-11-13 06:25:09,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:09,978][root][INFO] - Training Epoch: 3/10, step 522/574 completed (loss: 0.37299662828445435, acc: 0.9029850959777832)
[2024-11-13 06:25:10,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:10,454][root][INFO] - Training Epoch: 3/10, step 523/574 completed (loss: 0.5510600805282593, acc: 0.8540145754814148)
[2024-11-13 06:25:10,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:11,300][root][INFO] - Training Epoch: 3/10, step 524/574 completed (loss: 0.816469669342041, acc: 0.7549999952316284)
[2024-11-13 06:25:11,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:11,685][root][INFO] - Training Epoch: 3/10, step 525/574 completed (loss: 0.08524652570486069, acc: 0.9814814925193787)
[2024-11-13 06:25:11,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:12,099][root][INFO] - Training Epoch: 3/10, step 526/574 completed (loss: 0.31677430868148804, acc: 0.8653846383094788)
[2024-11-13 06:25:12,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:12,457][root][INFO] - Training Epoch: 3/10, step 527/574 completed (loss: 0.35039496421813965, acc: 0.9047619104385376)
[2024-11-13 06:25:12,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:12,849][root][INFO] - Training Epoch: 3/10, step 528/574 completed (loss: 1.8685675859451294, acc: 0.4754098355770111)
[2024-11-13 06:25:12,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:13,198][root][INFO] - Training Epoch: 3/10, step 529/574 completed (loss: 0.37361809611320496, acc: 0.8983050584793091)
[2024-11-13 06:25:13,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:13,555][root][INFO] - Training Epoch: 3/10, step 530/574 completed (loss: 1.5199753046035767, acc: 0.5581395626068115)
[2024-11-13 06:25:13,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:13,911][root][INFO] - Training Epoch: 3/10, step 531/574 completed (loss: 0.8497191071510315, acc: 0.75)
[2024-11-13 06:25:14,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:14,268][root][INFO] - Training Epoch: 3/10, step 532/574 completed (loss: 1.3640629053115845, acc: 0.7547169923782349)
[2024-11-13 06:25:14,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:14,625][root][INFO] - Training Epoch: 3/10, step 533/574 completed (loss: 0.7424957156181335, acc: 0.75)
[2024-11-13 06:25:14,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:14,958][root][INFO] - Training Epoch: 3/10, step 534/574 completed (loss: 0.598915159702301, acc: 0.8399999737739563)
[2024-11-13 06:25:15,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:15,311][root][INFO] - Training Epoch: 3/10, step 535/574 completed (loss: 0.4948086142539978, acc: 0.8999999761581421)
[2024-11-13 06:25:15,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:15,654][root][INFO] - Training Epoch: 3/10, step 536/574 completed (loss: 0.2570503056049347, acc: 0.9090909361839294)
[2024-11-13 06:25:15,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:16,195][root][INFO] - Training Epoch: 3/10, step 537/574 completed (loss: 0.5854753255844116, acc: 0.8461538553237915)
[2024-11-13 06:25:16,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:16,577][root][INFO] - Training Epoch: 3/10, step 538/574 completed (loss: 0.5713695883750916, acc: 0.796875)
[2024-11-13 06:25:16,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:17,112][root][INFO] - Training Epoch: 3/10, step 539/574 completed (loss: 0.6819808483123779, acc: 0.78125)
[2024-11-13 06:25:17,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:17,462][root][INFO] - Training Epoch: 3/10, step 540/574 completed (loss: 0.8563445210456848, acc: 0.8181818127632141)
[2024-11-13 06:25:17,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:17,810][root][INFO] - Training Epoch: 3/10, step 541/574 completed (loss: 0.39725834131240845, acc: 0.8125)
[2024-11-13 06:25:17,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:18,151][root][INFO] - Training Epoch: 3/10, step 542/574 completed (loss: 0.07387963682413101, acc: 1.0)
[2024-11-13 06:25:18,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:18,500][root][INFO] - Training Epoch: 3/10, step 543/574 completed (loss: 0.2706891596317291, acc: 0.95652174949646)
[2024-11-13 06:25:18,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:18,867][root][INFO] - Training Epoch: 3/10, step 544/574 completed (loss: 0.2519751191139221, acc: 0.9333333373069763)
[2024-11-13 06:25:19,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:19,230][root][INFO] - Training Epoch: 3/10, step 545/574 completed (loss: 0.12082895636558533, acc: 0.9756097793579102)
[2024-11-13 06:25:19,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:19,586][root][INFO] - Training Epoch: 3/10, step 546/574 completed (loss: 0.37591660022735596, acc: 0.9428571462631226)
[2024-11-13 06:25:19,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:19,937][root][INFO] - Training Epoch: 3/10, step 547/574 completed (loss: 0.21793267130851746, acc: 0.9736841917037964)
[2024-11-13 06:25:20,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:20,292][root][INFO] - Training Epoch: 3/10, step 548/574 completed (loss: 0.10519450902938843, acc: 0.9677419066429138)
[2024-11-13 06:25:20,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:20,643][root][INFO] - Training Epoch: 3/10, step 549/574 completed (loss: 0.05143950879573822, acc: 1.0)
[2024-11-13 06:25:20,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:21,008][root][INFO] - Training Epoch: 3/10, step 550/574 completed (loss: 0.4126713275909424, acc: 0.8787878751754761)
[2024-11-13 06:25:21,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:21,358][root][INFO] - Training Epoch: 3/10, step 551/574 completed (loss: 0.23983576893806458, acc: 0.925000011920929)
[2024-11-13 06:25:21,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:21,716][root][INFO] - Training Epoch: 3/10, step 552/574 completed (loss: 0.3132704794406891, acc: 0.8714285492897034)
[2024-11-13 06:25:21,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:22,178][root][INFO] - Training Epoch: 3/10, step 553/574 completed (loss: 0.6417928338050842, acc: 0.8102189898490906)
[2024-11-13 06:25:22,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:22,641][root][INFO] - Training Epoch: 3/10, step 554/574 completed (loss: 0.45436763763427734, acc: 0.8620689511299133)
[2024-11-13 06:25:22,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:23,041][root][INFO] - Training Epoch: 3/10, step 555/574 completed (loss: 0.6388902068138123, acc: 0.8214285969734192)
[2024-11-13 06:25:23,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:23,422][root][INFO] - Training Epoch: 3/10, step 556/574 completed (loss: 0.5716617703437805, acc: 0.8278145790100098)
[2024-11-13 06:25:23,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:23,823][root][INFO] - Training Epoch: 3/10, step 557/574 completed (loss: 0.28601187467575073, acc: 0.8888888955116272)
[2024-11-13 06:25:23,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:24,199][root][INFO] - Training Epoch: 3/10, step 558/574 completed (loss: 0.09783484786748886, acc: 1.0)
[2024-11-13 06:25:24,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:24,554][root][INFO] - Training Epoch: 3/10, step 559/574 completed (loss: 0.20390425622463226, acc: 0.9615384340286255)
[2024-11-13 06:25:24,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:24,913][root][INFO] - Training Epoch: 3/10, step 560/574 completed (loss: 0.06993575394153595, acc: 1.0)
[2024-11-13 06:25:25,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:25,272][root][INFO] - Training Epoch: 3/10, step 561/574 completed (loss: 0.3419695496559143, acc: 0.9487179517745972)
[2024-11-13 06:25:25,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:25,668][root][INFO] - Training Epoch: 3/10, step 562/574 completed (loss: 0.6138331294059753, acc: 0.8111110925674438)
[2024-11-13 06:25:25,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:26,051][root][INFO] - Training Epoch: 3/10, step 563/574 completed (loss: 0.5208715200424194, acc: 0.8571428656578064)
[2024-11-13 06:25:26,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:26,514][root][INFO] - Training Epoch: 3/10, step 564/574 completed (loss: 0.4199921190738678, acc: 0.8333333134651184)
[2024-11-13 06:25:26,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:26,895][root][INFO] - Training Epoch: 3/10, step 565/574 completed (loss: 0.49933236837387085, acc: 0.8793103694915771)
[2024-11-13 06:25:27,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:27,259][root][INFO] - Training Epoch: 3/10, step 566/574 completed (loss: 0.41195979714393616, acc: 0.9285714030265808)
[2024-11-13 06:25:27,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:27,587][root][INFO] - Training Epoch: 3/10, step 567/574 completed (loss: 0.0622568316757679, acc: 1.0)
[2024-11-13 06:25:28,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:28,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:29,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:29,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:30,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:30,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:31,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:31,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:32,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:32,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:33,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:33,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:34,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:34,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:35,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:36,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:36,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:37,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:37,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:37,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:38,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:38,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:39,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:39,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:40,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:41,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:41,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:42,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:42,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:42,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:43,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:43,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:44,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:44,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:45,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:45,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:46,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:46,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:47,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:47,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:48,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:48,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:49,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:49,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:50,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:50,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:51,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:51,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:52,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:52,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:53,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:53,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:54,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:54,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:55,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:55,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:55,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:56,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:56,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:57,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:57,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:58,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:59,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:25:59,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:00,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:00,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:01,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:01,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:02,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:03,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:03,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:04,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:04,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:05,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:05,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:06,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:06,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:06,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:07,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:07,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:08,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:08,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:09,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:09,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:10,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:11,008][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3849, device='cuda:0') eval_epoch_loss=tensor(0.8691, device='cuda:0') eval_epoch_acc=tensor(0.7808, device='cuda:0')
[2024-11-13 06:26:11,009][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:26:11,010][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:26:11,442][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_3_step_568_loss_0.8691489100456238/model.pt
[2024-11-13 06:26:11,451][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:26:11,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:11,912][root][INFO] - Training Epoch: 3/10, step 568/574 completed (loss: 0.05746022239327431, acc: 1.0)
[2024-11-13 06:26:12,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:12,434][root][INFO] - Training Epoch: 3/10, step 569/574 completed (loss: 0.4752870798110962, acc: 0.855614960193634)
[2024-11-13 06:26:12,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:12,837][root][INFO] - Training Epoch: 3/10, step 570/574 completed (loss: 0.09941689670085907, acc: 0.9677419066429138)
[2024-11-13 06:26:13,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:13,197][root][INFO] - Training Epoch: 3/10, step 571/574 completed (loss: 0.5580542683601379, acc: 0.8547008633613586)
[2024-11-13 06:26:13,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:13,548][root][INFO] - Training Epoch: 3/10, step 572/574 completed (loss: 0.7220920920372009, acc: 0.795918345451355)
[2024-11-13 06:26:13,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:13,970][root][INFO] - Training Epoch: 3/10, step 573/574 completed (loss: 0.9084315299987793, acc: 0.7672955989837646)
[2024-11-13 06:26:14,383][slam_llm.utils.train_utils][INFO] - Epoch 3: train_perplexity=1.9487, train_epoch_loss=0.6671, epoch time 459.95928941108286s
[2024-11-13 06:26:14,383][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2024-11-13 06:26:14,383][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 17 GB
[2024-11-13 06:26:14,383][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2024-11-13 06:26:14,383][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 9
[2024-11-13 06:26:14,383][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-13 06:26:14,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:15,291][root][INFO] - Training Epoch: 4/10, step 0/574 completed (loss: 0.16747063398361206, acc: 0.9259259104728699)
[2024-11-13 06:26:15,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:15,751][root][INFO] - Training Epoch: 4/10, step 1/574 completed (loss: 0.3651115298271179, acc: 0.9200000166893005)
[2024-11-13 06:26:15,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:16,179][root][INFO] - Training Epoch: 4/10, step 2/574 completed (loss: 0.8589290976524353, acc: 0.7837837934494019)
[2024-11-13 06:26:16,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:16,601][root][INFO] - Training Epoch: 4/10, step 3/574 completed (loss: 0.3957078158855438, acc: 0.8684210777282715)
[2024-11-13 06:26:16,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:17,040][root][INFO] - Training Epoch: 4/10, step 4/574 completed (loss: 0.5197556018829346, acc: 0.8108108043670654)
[2024-11-13 06:26:17,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:17,452][root][INFO] - Training Epoch: 4/10, step 5/574 completed (loss: 0.34955835342407227, acc: 0.8571428656578064)
[2024-11-13 06:26:17,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:17,861][root][INFO] - Training Epoch: 4/10, step 6/574 completed (loss: 1.1131104230880737, acc: 0.7142857313156128)
[2024-11-13 06:26:18,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:18,234][root][INFO] - Training Epoch: 4/10, step 7/574 completed (loss: 0.7450811266899109, acc: 0.8666666746139526)
[2024-11-13 06:26:18,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:18,718][root][INFO] - Training Epoch: 4/10, step 8/574 completed (loss: 0.14191283285617828, acc: 0.9545454382896423)
[2024-11-13 06:26:18,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:19,098][root][INFO] - Training Epoch: 4/10, step 9/574 completed (loss: 0.02317754551768303, acc: 1.0)
[2024-11-13 06:26:19,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:19,492][root][INFO] - Training Epoch: 4/10, step 10/574 completed (loss: 0.2159024477005005, acc: 0.8888888955116272)
[2024-11-13 06:26:19,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:19,928][root][INFO] - Training Epoch: 4/10, step 11/574 completed (loss: 0.23295150697231293, acc: 0.9230769276618958)
[2024-11-13 06:26:20,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:20,308][root][INFO] - Training Epoch: 4/10, step 12/574 completed (loss: 0.08431032299995422, acc: 0.9696969985961914)
[2024-11-13 06:26:20,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:20,745][root][INFO] - Training Epoch: 4/10, step 13/574 completed (loss: 0.3854217529296875, acc: 0.9130434989929199)
[2024-11-13 06:26:20,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:21,145][root][INFO] - Training Epoch: 4/10, step 14/574 completed (loss: 0.2699022591114044, acc: 0.9215686321258545)
[2024-11-13 06:26:21,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:21,517][root][INFO] - Training Epoch: 4/10, step 15/574 completed (loss: 0.617739737033844, acc: 0.8571428656578064)
[2024-11-13 06:26:21,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:21,907][root][INFO] - Training Epoch: 4/10, step 16/574 completed (loss: 0.40166234970092773, acc: 0.8421052694320679)
[2024-11-13 06:26:22,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:22,313][root][INFO] - Training Epoch: 4/10, step 17/574 completed (loss: 0.2754118740558624, acc: 0.9166666865348816)
[2024-11-13 06:26:22,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:22,676][root][INFO] - Training Epoch: 4/10, step 18/574 completed (loss: 0.745418906211853, acc: 0.8055555820465088)
[2024-11-13 06:26:22,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:23,012][root][INFO] - Training Epoch: 4/10, step 19/574 completed (loss: 0.18280096352100372, acc: 0.9473684430122375)
[2024-11-13 06:26:23,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:23,334][root][INFO] - Training Epoch: 4/10, step 20/574 completed (loss: 0.24316826462745667, acc: 0.9230769276618958)
[2024-11-13 06:26:23,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:23,742][root][INFO] - Training Epoch: 4/10, step 21/574 completed (loss: 0.3269341289997101, acc: 0.8965517282485962)
[2024-11-13 06:26:23,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:24,151][root][INFO] - Training Epoch: 4/10, step 22/574 completed (loss: 0.7374043464660645, acc: 0.8399999737739563)
[2024-11-13 06:26:24,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:24,553][root][INFO] - Training Epoch: 4/10, step 23/574 completed (loss: 0.628062903881073, acc: 0.8571428656578064)
[2024-11-13 06:26:24,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:24,987][root][INFO] - Training Epoch: 4/10, step 24/574 completed (loss: 0.3881409466266632, acc: 0.875)
[2024-11-13 06:26:25,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:25,349][root][INFO] - Training Epoch: 4/10, step 25/574 completed (loss: 0.842523992061615, acc: 0.7735849022865295)
[2024-11-13 06:26:25,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:25,732][root][INFO] - Training Epoch: 4/10, step 26/574 completed (loss: 0.7972087860107422, acc: 0.8082191944122314)
[2024-11-13 06:26:26,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:27,548][root][INFO] - Training Epoch: 4/10, step 27/574 completed (loss: 1.0891921520233154, acc: 0.6679841876029968)
[2024-11-13 06:26:27,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:27,961][root][INFO] - Training Epoch: 4/10, step 28/574 completed (loss: 0.5899845361709595, acc: 0.8604651093482971)
[2024-11-13 06:26:28,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:28,354][root][INFO] - Training Epoch: 4/10, step 29/574 completed (loss: 0.8861839771270752, acc: 0.7469879388809204)
[2024-11-13 06:26:28,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:28,821][root][INFO] - Training Epoch: 4/10, step 30/574 completed (loss: 0.7800714373588562, acc: 0.7407407164573669)
[2024-11-13 06:26:28,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:29,218][root][INFO] - Training Epoch: 4/10, step 31/574 completed (loss: 0.48564594984054565, acc: 0.8928571343421936)
[2024-11-13 06:26:29,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:29,637][root][INFO] - Training Epoch: 4/10, step 32/574 completed (loss: 0.2532687783241272, acc: 0.9259259104728699)
[2024-11-13 06:26:29,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:30,072][root][INFO] - Training Epoch: 4/10, step 33/574 completed (loss: 0.1548391878604889, acc: 0.95652174949646)
[2024-11-13 06:26:30,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:30,477][root][INFO] - Training Epoch: 4/10, step 34/574 completed (loss: 0.5070614814758301, acc: 0.8571428656578064)
[2024-11-13 06:26:30,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:30,878][root][INFO] - Training Epoch: 4/10, step 35/574 completed (loss: 0.34376460313796997, acc: 0.9016393423080444)
[2024-11-13 06:26:31,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:31,320][root][INFO] - Training Epoch: 4/10, step 36/574 completed (loss: 0.5355691909790039, acc: 0.8095238208770752)
[2024-11-13 06:26:31,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:31,750][root][INFO] - Training Epoch: 4/10, step 37/574 completed (loss: 0.6853753328323364, acc: 0.7796609997749329)
[2024-11-13 06:26:31,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:32,186][root][INFO] - Training Epoch: 4/10, step 38/574 completed (loss: 0.36964869499206543, acc: 0.8620689511299133)
[2024-11-13 06:26:32,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:32,556][root][INFO] - Training Epoch: 4/10, step 39/574 completed (loss: 0.2984660565853119, acc: 1.0)
[2024-11-13 06:26:32,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:32,967][root][INFO] - Training Epoch: 4/10, step 40/574 completed (loss: 0.5943105220794678, acc: 0.7692307829856873)
[2024-11-13 06:26:33,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:33,439][root][INFO] - Training Epoch: 4/10, step 41/574 completed (loss: 0.2235345095396042, acc: 0.9189189076423645)
[2024-11-13 06:26:33,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:33,822][root][INFO] - Training Epoch: 4/10, step 42/574 completed (loss: 0.6711301207542419, acc: 0.7846153974533081)
[2024-11-13 06:26:34,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:34,386][root][INFO] - Training Epoch: 4/10, step 43/574 completed (loss: 0.576146125793457, acc: 0.8282828330993652)
[2024-11-13 06:26:34,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:34,956][root][INFO] - Training Epoch: 4/10, step 44/574 completed (loss: 0.5975554585456848, acc: 0.8247422575950623)
[2024-11-13 06:26:35,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:35,498][root][INFO] - Training Epoch: 4/10, step 45/574 completed (loss: 0.6903128027915955, acc: 0.7720588445663452)
[2024-11-13 06:26:35,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:35,896][root][INFO] - Training Epoch: 4/10, step 46/574 completed (loss: 0.41014668345451355, acc: 0.8846153616905212)
[2024-11-13 06:26:36,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:36,355][root][INFO] - Training Epoch: 4/10, step 47/574 completed (loss: 0.18415601551532745, acc: 0.9629629850387573)
[2024-11-13 06:26:36,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:36,815][root][INFO] - Training Epoch: 4/10, step 48/574 completed (loss: 0.3697514235973358, acc: 0.9285714030265808)
[2024-11-13 06:26:37,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:37,255][root][INFO] - Training Epoch: 4/10, step 49/574 completed (loss: 0.04029364138841629, acc: 1.0)
[2024-11-13 06:26:37,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:37,724][root][INFO] - Training Epoch: 4/10, step 50/574 completed (loss: 0.7846004366874695, acc: 0.8070175647735596)
[2024-11-13 06:26:37,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:38,183][root][INFO] - Training Epoch: 4/10, step 51/574 completed (loss: 0.7866092324256897, acc: 0.7777777910232544)
[2024-11-13 06:26:38,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:38,645][root][INFO] - Training Epoch: 4/10, step 52/574 completed (loss: 1.0628489255905151, acc: 0.6760563254356384)
[2024-11-13 06:26:38,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:39,285][root][INFO] - Training Epoch: 4/10, step 53/574 completed (loss: 1.5668706893920898, acc: 0.54666668176651)
[2024-11-13 06:26:39,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:39,664][root][INFO] - Training Epoch: 4/10, step 54/574 completed (loss: 0.7853568196296692, acc: 0.837837815284729)
[2024-11-13 06:26:39,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:40,099][root][INFO] - Training Epoch: 4/10, step 55/574 completed (loss: 0.34130871295928955, acc: 0.9230769276618958)
[2024-11-13 06:26:43,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:44,603][root][INFO] - Training Epoch: 4/10, step 56/574 completed (loss: 0.9579357504844666, acc: 0.7269624471664429)
[2024-11-13 06:26:45,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:46,461][root][INFO] - Training Epoch: 4/10, step 57/574 completed (loss: 1.3006857633590698, acc: 0.6470588445663452)
[2024-11-13 06:26:46,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:47,394][root][INFO] - Training Epoch: 4/10, step 58/574 completed (loss: 0.9788156747817993, acc: 0.7215909361839294)
[2024-11-13 06:26:47,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:48,268][root][INFO] - Training Epoch: 4/10, step 59/574 completed (loss: 0.5310599207878113, acc: 0.8529411554336548)
[2024-11-13 06:26:48,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:49,107][root][INFO] - Training Epoch: 4/10, step 60/574 completed (loss: 0.8984952569007874, acc: 0.760869562625885)
[2024-11-13 06:26:49,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:49,676][root][INFO] - Training Epoch: 4/10, step 61/574 completed (loss: 0.7404841184616089, acc: 0.800000011920929)
[2024-11-13 06:26:49,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:50,080][root][INFO] - Training Epoch: 4/10, step 62/574 completed (loss: 0.4054623246192932, acc: 0.8823529481887817)
[2024-11-13 06:26:50,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:50,532][root][INFO] - Training Epoch: 4/10, step 63/574 completed (loss: 0.3364669680595398, acc: 0.9444444179534912)
[2024-11-13 06:26:50,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:51,018][root][INFO] - Training Epoch: 4/10, step 64/574 completed (loss: 0.21254557371139526, acc: 0.953125)
[2024-11-13 06:26:51,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:51,425][root][INFO] - Training Epoch: 4/10, step 65/574 completed (loss: 0.116740882396698, acc: 1.0)
[2024-11-13 06:26:51,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:51,874][root][INFO] - Training Epoch: 4/10, step 66/574 completed (loss: 0.7654408812522888, acc: 0.8214285969734192)
[2024-11-13 06:26:52,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:52,293][root][INFO] - Training Epoch: 4/10, step 67/574 completed (loss: 0.5774710774421692, acc: 0.800000011920929)
[2024-11-13 06:26:52,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:52,704][root][INFO] - Training Epoch: 4/10, step 68/574 completed (loss: 0.045464564114809036, acc: 1.0)
[2024-11-13 06:26:52,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:53,100][root][INFO] - Training Epoch: 4/10, step 69/574 completed (loss: 0.4804210066795349, acc: 0.8055555820465088)
[2024-11-13 06:26:53,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:53,535][root][INFO] - Training Epoch: 4/10, step 70/574 completed (loss: 0.7454372048377991, acc: 0.7878788113594055)
[2024-11-13 06:26:53,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:53,997][root][INFO] - Training Epoch: 4/10, step 71/574 completed (loss: 1.1320788860321045, acc: 0.6985294222831726)
[2024-11-13 06:26:54,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:54,467][root][INFO] - Training Epoch: 4/10, step 72/574 completed (loss: 0.9441525936126709, acc: 0.7301587462425232)
[2024-11-13 06:26:54,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:54,916][root][INFO] - Training Epoch: 4/10, step 73/574 completed (loss: 1.5498851537704468, acc: 0.5846154093742371)
[2024-11-13 06:26:55,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:55,350][root][INFO] - Training Epoch: 4/10, step 74/574 completed (loss: 1.1273051500320435, acc: 0.6836734414100647)
[2024-11-13 06:26:55,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:55,729][root][INFO] - Training Epoch: 4/10, step 75/574 completed (loss: 1.2714406251907349, acc: 0.6343283653259277)
[2024-11-13 06:26:55,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:56,260][root][INFO] - Training Epoch: 4/10, step 76/574 completed (loss: 1.4460140466690063, acc: 0.6131386756896973)
[2024-11-13 06:26:56,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:56,637][root][INFO] - Training Epoch: 4/10, step 77/574 completed (loss: 0.03296486660838127, acc: 1.0)
[2024-11-13 06:26:56,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:57,056][root][INFO] - Training Epoch: 4/10, step 78/574 completed (loss: 0.1588497757911682, acc: 0.9166666865348816)
[2024-11-13 06:26:57,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:57,415][root][INFO] - Training Epoch: 4/10, step 79/574 completed (loss: 0.1287011057138443, acc: 0.939393937587738)
[2024-11-13 06:26:57,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:57,805][root][INFO] - Training Epoch: 4/10, step 80/574 completed (loss: 0.07092858850955963, acc: 0.9615384340286255)
[2024-11-13 06:26:57,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:58,221][root][INFO] - Training Epoch: 4/10, step 81/574 completed (loss: 0.6994214653968811, acc: 0.7692307829856873)
[2024-11-13 06:26:58,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:58,579][root][INFO] - Training Epoch: 4/10, step 82/574 completed (loss: 0.631920576095581, acc: 0.7884615659713745)
[2024-11-13 06:26:58,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:58,936][root][INFO] - Training Epoch: 4/10, step 83/574 completed (loss: 0.3146730065345764, acc: 0.90625)
[2024-11-13 06:26:59,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:59,350][root][INFO] - Training Epoch: 4/10, step 84/574 completed (loss: 0.48742765188217163, acc: 0.8405796885490417)
[2024-11-13 06:26:59,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:26:59,773][root][INFO] - Training Epoch: 4/10, step 85/574 completed (loss: 0.35511985421180725, acc: 0.8999999761581421)
[2024-11-13 06:26:59,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:00,162][root][INFO] - Training Epoch: 4/10, step 86/574 completed (loss: 0.15867403149604797, acc: 0.95652174949646)
[2024-11-13 06:27:00,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:00,813][root][INFO] - Training Epoch: 4/10, step 87/574 completed (loss: 0.9567344188690186, acc: 0.6800000071525574)
[2024-11-13 06:27:01,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:01,305][root][INFO] - Training Epoch: 4/10, step 88/574 completed (loss: 1.0219430923461914, acc: 0.6990291476249695)
[2024-11-13 06:27:02,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:03,071][root][INFO] - Training Epoch: 4/10, step 89/574 completed (loss: 1.0043460130691528, acc: 0.7524271607398987)
[2024-11-13 06:27:03,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:04,347][root][INFO] - Training Epoch: 4/10, step 90/574 completed (loss: 1.2436295747756958, acc: 0.6827957034111023)
[2024-11-13 06:27:05,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:05,598][root][INFO] - Training Epoch: 4/10, step 91/574 completed (loss: 0.9242227077484131, acc: 0.7586206793785095)
[2024-11-13 06:27:06,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:06,751][root][INFO] - Training Epoch: 4/10, step 92/574 completed (loss: 0.8329190611839294, acc: 0.7684210538864136)
[2024-11-13 06:27:07,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:08,333][root][INFO] - Training Epoch: 4/10, step 93/574 completed (loss: 1.5819734334945679, acc: 0.603960394859314)
[2024-11-13 06:27:08,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:08,741][root][INFO] - Training Epoch: 4/10, step 94/574 completed (loss: 1.080483317375183, acc: 0.6612903475761414)
[2024-11-13 06:27:08,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:09,183][root][INFO] - Training Epoch: 4/10, step 95/574 completed (loss: 0.8235021233558655, acc: 0.7246376872062683)
[2024-11-13 06:27:09,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:09,595][root][INFO] - Training Epoch: 4/10, step 96/574 completed (loss: 1.1999140977859497, acc: 0.6386554837226868)
[2024-11-13 06:27:09,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:10,040][root][INFO] - Training Epoch: 4/10, step 97/574 completed (loss: 1.2110671997070312, acc: 0.625)
[2024-11-13 06:27:10,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:10,549][root][INFO] - Training Epoch: 4/10, step 98/574 completed (loss: 1.41341233253479, acc: 0.5766423344612122)
[2024-11-13 06:27:10,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:10,940][root][INFO] - Training Epoch: 4/10, step 99/574 completed (loss: 1.3964242935180664, acc: 0.5671641826629639)
[2024-11-13 06:27:11,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:11,304][root][INFO] - Training Epoch: 4/10, step 100/574 completed (loss: 0.3374786376953125, acc: 0.949999988079071)
[2024-11-13 06:27:11,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:11,679][root][INFO] - Training Epoch: 4/10, step 101/574 completed (loss: 0.017946872860193253, acc: 1.0)
[2024-11-13 06:27:11,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:12,042][root][INFO] - Training Epoch: 4/10, step 102/574 completed (loss: 0.027584843337535858, acc: 1.0)
[2024-11-13 06:27:12,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:12,427][root][INFO] - Training Epoch: 4/10, step 103/574 completed (loss: 0.15616260468959808, acc: 0.9772727489471436)
[2024-11-13 06:27:12,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:12,825][root][INFO] - Training Epoch: 4/10, step 104/574 completed (loss: 0.6112232804298401, acc: 0.8275862336158752)
[2024-11-13 06:27:12,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:13,203][root][INFO] - Training Epoch: 4/10, step 105/574 completed (loss: 0.19117756187915802, acc: 0.930232584476471)
[2024-11-13 06:27:13,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:13,625][root][INFO] - Training Epoch: 4/10, step 106/574 completed (loss: 0.33166080713272095, acc: 0.8799999952316284)
[2024-11-13 06:27:13,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:14,039][root][INFO] - Training Epoch: 4/10, step 107/574 completed (loss: 0.09360489994287491, acc: 1.0)
[2024-11-13 06:27:14,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:14,450][root][INFO] - Training Epoch: 4/10, step 108/574 completed (loss: 0.021864615380764008, acc: 1.0)
[2024-11-13 06:27:14,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:14,810][root][INFO] - Training Epoch: 4/10, step 109/574 completed (loss: 0.19140025973320007, acc: 0.9523809552192688)
[2024-11-13 06:27:15,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:15,266][root][INFO] - Training Epoch: 4/10, step 110/574 completed (loss: 0.3105260133743286, acc: 0.8769230842590332)
[2024-11-13 06:27:15,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:15,822][root][INFO] - Training Epoch: 4/10, step 111/574 completed (loss: 0.6239835023880005, acc: 0.8245614171028137)
[2024-11-13 06:27:16,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:16,267][root][INFO] - Training Epoch: 4/10, step 112/574 completed (loss: 0.8303559422492981, acc: 0.719298243522644)
[2024-11-13 06:27:16,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:16,670][root][INFO] - Training Epoch: 4/10, step 113/574 completed (loss: 0.6928461790084839, acc: 0.8205128312110901)
[2024-11-13 06:27:16,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:17,098][root][INFO] - Training Epoch: 4/10, step 114/574 completed (loss: 0.3537618815898895, acc: 0.8571428656578064)
[2024-11-13 06:27:17,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:17,447][root][INFO] - Training Epoch: 4/10, step 115/574 completed (loss: 0.0243997760117054, acc: 1.0)
[2024-11-13 06:27:17,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:17,893][root][INFO] - Training Epoch: 4/10, step 116/574 completed (loss: 0.6157881021499634, acc: 0.8571428656578064)
[2024-11-13 06:27:18,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:18,258][root][INFO] - Training Epoch: 4/10, step 117/574 completed (loss: 0.5231619477272034, acc: 0.869918704032898)
[2024-11-13 06:27:18,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:18,612][root][INFO] - Training Epoch: 4/10, step 118/574 completed (loss: 0.24551279842853546, acc: 0.9354838728904724)
[2024-11-13 06:27:19,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:19,889][root][INFO] - Training Epoch: 4/10, step 119/574 completed (loss: 0.9535965919494629, acc: 0.7224334478378296)
[2024-11-13 06:27:20,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:20,242][root][INFO] - Training Epoch: 4/10, step 120/574 completed (loss: 0.26255497336387634, acc: 0.9466666579246521)
[2024-11-13 06:27:20,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:20,796][root][INFO] - Training Epoch: 4/10, step 121/574 completed (loss: 0.4109194576740265, acc: 0.8846153616905212)
[2024-11-13 06:27:20,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:21,179][root][INFO] - Training Epoch: 4/10, step 122/574 completed (loss: 0.21312852203845978, acc: 0.9583333134651184)
[2024-11-13 06:27:21,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:21,656][root][INFO] - Training Epoch: 4/10, step 123/574 completed (loss: 0.3434763550758362, acc: 0.8947368264198303)
[2024-11-13 06:27:21,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:22,123][root][INFO] - Training Epoch: 4/10, step 124/574 completed (loss: 1.1886868476867676, acc: 0.6441717743873596)
[2024-11-13 06:27:22,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:22,586][root][INFO] - Training Epoch: 4/10, step 125/574 completed (loss: 1.2460577487945557, acc: 0.6458333134651184)
[2024-11-13 06:27:22,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:23,024][root][INFO] - Training Epoch: 4/10, step 126/574 completed (loss: 1.2502684593200684, acc: 0.6166666746139526)
[2024-11-13 06:27:23,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:23,484][root][INFO] - Training Epoch: 4/10, step 127/574 completed (loss: 1.218331217765808, acc: 0.6904761791229248)
[2024-11-13 06:27:23,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:23,955][root][INFO] - Training Epoch: 4/10, step 128/574 completed (loss: 1.0366146564483643, acc: 0.692307710647583)
[2024-11-13 06:27:24,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:24,508][root][INFO] - Training Epoch: 4/10, step 129/574 completed (loss: 1.1136388778686523, acc: 0.7279411554336548)
[2024-11-13 06:27:24,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:24,944][root][INFO] - Training Epoch: 4/10, step 130/574 completed (loss: 0.5912421941757202, acc: 0.807692289352417)
[2024-11-13 06:27:25,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:25,314][root][INFO] - Training Epoch: 4/10, step 131/574 completed (loss: 0.1285247653722763, acc: 0.95652174949646)
[2024-11-13 06:27:25,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:25,707][root][INFO] - Training Epoch: 4/10, step 132/574 completed (loss: 0.5130435228347778, acc: 0.78125)
[2024-11-13 06:27:25,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:26,103][root][INFO] - Training Epoch: 4/10, step 133/574 completed (loss: 0.5778261423110962, acc: 0.739130437374115)
[2024-11-13 06:27:26,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:26,472][root][INFO] - Training Epoch: 4/10, step 134/574 completed (loss: 0.7150565981864929, acc: 0.8285714387893677)
[2024-11-13 06:27:26,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:26,859][root][INFO] - Training Epoch: 4/10, step 135/574 completed (loss: 0.22794601321220398, acc: 0.9230769276618958)
[2024-11-13 06:27:27,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:27,238][root][INFO] - Training Epoch: 4/10, step 136/574 completed (loss: 0.5647873878479004, acc: 0.8571428656578064)
[2024-11-13 06:27:28,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:28,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:29,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:29,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:29,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:30,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:30,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:31,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:31,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:32,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:32,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:33,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:33,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:34,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:34,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:35,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:35,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:36,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:36,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:37,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:37,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:38,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:38,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:39,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:39,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:40,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:40,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:41,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:41,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:42,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:42,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:42,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:43,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:43,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:44,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:44,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:45,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:45,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:46,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:46,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:47,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:47,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:48,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:48,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:49,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:49,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:50,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:50,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:51,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:51,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:51,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:52,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:52,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:53,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:53,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:54,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:54,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:55,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:55,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:56,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:56,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:57,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:57,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:58,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:58,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:59,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:27:59,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:00,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:00,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:01,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:01,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:02,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:03,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:03,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:03,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:04,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:04,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:05,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:05,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:06,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:06,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:07,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:07,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:08,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:08,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:09,430][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1132, device='cuda:0') eval_epoch_loss=tensor(0.7482, device='cuda:0') eval_epoch_acc=tensor(0.7948, device='cuda:0')
[2024-11-13 06:28:09,431][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:28:09,432][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:28:09,852][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_137_loss_0.7482238411903381/model.pt
[2024-11-13 06:28:09,857][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:28:09,857][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 4 is 0.7482238411903381
[2024-11-13 06:28:09,858][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.7948068976402283
[2024-11-13 06:28:10,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:10,319][root][INFO] - Training Epoch: 4/10, step 137/574 completed (loss: 0.9324206113815308, acc: 0.699999988079071)
[2024-11-13 06:28:10,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:10,747][root][INFO] - Training Epoch: 4/10, step 138/574 completed (loss: 0.4771445691585541, acc: 0.8695651888847351)
[2024-11-13 06:28:10,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:11,169][root][INFO] - Training Epoch: 4/10, step 139/574 completed (loss: 0.2253226786851883, acc: 0.9047619104385376)
[2024-11-13 06:28:11,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:11,600][root][INFO] - Training Epoch: 4/10, step 140/574 completed (loss: 0.24132497608661652, acc: 0.9615384340286255)
[2024-11-13 06:28:11,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:12,069][root][INFO] - Training Epoch: 4/10, step 141/574 completed (loss: 0.3209322988986969, acc: 0.8709677457809448)
[2024-11-13 06:28:12,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:12,476][root][INFO] - Training Epoch: 4/10, step 142/574 completed (loss: 0.2987084984779358, acc: 0.8918918967247009)
[2024-11-13 06:28:12,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:13,252][root][INFO] - Training Epoch: 4/10, step 143/574 completed (loss: 0.8029497861862183, acc: 0.7631579041481018)
[2024-11-13 06:28:13,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:13,660][root][INFO] - Training Epoch: 4/10, step 144/574 completed (loss: 0.8856115937232971, acc: 0.7238805890083313)
[2024-11-13 06:28:13,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:14,075][root][INFO] - Training Epoch: 4/10, step 145/574 completed (loss: 0.5354483127593994, acc: 0.8265306353569031)
[2024-11-13 06:28:14,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:14,721][root][INFO] - Training Epoch: 4/10, step 146/574 completed (loss: 1.0405362844467163, acc: 0.6595744490623474)
[2024-11-13 06:28:14,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:15,109][root][INFO] - Training Epoch: 4/10, step 147/574 completed (loss: 0.5577636957168579, acc: 0.8428571224212646)
[2024-11-13 06:28:15,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:15,523][root][INFO] - Training Epoch: 4/10, step 148/574 completed (loss: 0.6816133260726929, acc: 0.75)
[2024-11-13 06:28:15,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:15,891][root][INFO] - Training Epoch: 4/10, step 149/574 completed (loss: 0.26695138216018677, acc: 0.95652174949646)
[2024-11-13 06:28:16,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:16,269][root][INFO] - Training Epoch: 4/10, step 150/574 completed (loss: 0.436003178358078, acc: 0.931034505367279)
[2024-11-13 06:28:16,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:16,645][root][INFO] - Training Epoch: 4/10, step 151/574 completed (loss: 0.9216185212135315, acc: 0.782608687877655)
[2024-11-13 06:28:16,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:17,116][root][INFO] - Training Epoch: 4/10, step 152/574 completed (loss: 0.7092089056968689, acc: 0.8474576473236084)
[2024-11-13 06:28:17,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:17,492][root][INFO] - Training Epoch: 4/10, step 153/574 completed (loss: 0.9827320575714111, acc: 0.7017543911933899)
[2024-11-13 06:28:17,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:17,910][root][INFO] - Training Epoch: 4/10, step 154/574 completed (loss: 0.6168996691703796, acc: 0.8648648858070374)
[2024-11-13 06:28:18,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:18,327][root][INFO] - Training Epoch: 4/10, step 155/574 completed (loss: 0.29413041472435, acc: 0.9285714030265808)
[2024-11-13 06:28:18,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:18,778][root][INFO] - Training Epoch: 4/10, step 156/574 completed (loss: 0.2659677565097809, acc: 0.8695651888847351)
[2024-11-13 06:28:18,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:19,234][root][INFO] - Training Epoch: 4/10, step 157/574 completed (loss: 1.691330909729004, acc: 0.5263158082962036)
[2024-11-13 06:28:20,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:21,882][root][INFO] - Training Epoch: 4/10, step 158/574 completed (loss: 0.9289758205413818, acc: 0.7432432174682617)
[2024-11-13 06:28:22,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:22,260][root][INFO] - Training Epoch: 4/10, step 159/574 completed (loss: 1.5277405977249146, acc: 0.5370370149612427)
[2024-11-13 06:28:22,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:22,816][root][INFO] - Training Epoch: 4/10, step 160/574 completed (loss: 1.2649726867675781, acc: 0.6395348906517029)
[2024-11-13 06:28:23,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:23,731][root][INFO] - Training Epoch: 4/10, step 161/574 completed (loss: 1.312919020652771, acc: 0.6352941393852234)
[2024-11-13 06:28:24,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:24,579][root][INFO] - Training Epoch: 4/10, step 162/574 completed (loss: 1.3728560209274292, acc: 0.6516854166984558)
[2024-11-13 06:28:24,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:25,005][root][INFO] - Training Epoch: 4/10, step 163/574 completed (loss: 0.5002833604812622, acc: 0.8409090638160706)
[2024-11-13 06:28:25,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:25,397][root][INFO] - Training Epoch: 4/10, step 164/574 completed (loss: 0.5235780477523804, acc: 0.8571428656578064)
[2024-11-13 06:28:25,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:25,804][root][INFO] - Training Epoch: 4/10, step 165/574 completed (loss: 1.0937623977661133, acc: 0.7586206793785095)
[2024-11-13 06:28:26,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:26,228][root][INFO] - Training Epoch: 4/10, step 166/574 completed (loss: 0.1723465621471405, acc: 0.9591836929321289)
[2024-11-13 06:28:26,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:26,658][root][INFO] - Training Epoch: 4/10, step 167/574 completed (loss: 0.48817893862724304, acc: 0.8799999952316284)
[2024-11-13 06:28:26,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:27,210][root][INFO] - Training Epoch: 4/10, step 168/574 completed (loss: 0.4294987916946411, acc: 0.9027777910232544)
[2024-11-13 06:28:27,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:27,614][root][INFO] - Training Epoch: 4/10, step 169/574 completed (loss: 1.0156149864196777, acc: 0.7843137383460999)
[2024-11-13 06:28:28,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:29,271][root][INFO] - Training Epoch: 4/10, step 170/574 completed (loss: 1.0961475372314453, acc: 0.698630154132843)
[2024-11-13 06:28:29,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:29,613][root][INFO] - Training Epoch: 4/10, step 171/574 completed (loss: 0.06512980908155441, acc: 1.0)
[2024-11-13 06:28:29,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:30,023][root][INFO] - Training Epoch: 4/10, step 172/574 completed (loss: 0.5054173469543457, acc: 0.8888888955116272)
[2024-11-13 06:28:30,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:30,424][root][INFO] - Training Epoch: 4/10, step 173/574 completed (loss: 0.486663281917572, acc: 0.8571428656578064)
[2024-11-13 06:28:30,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:31,250][root][INFO] - Training Epoch: 4/10, step 174/574 completed (loss: 1.0303505659103394, acc: 0.7433628439903259)
[2024-11-13 06:28:31,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:31,590][root][INFO] - Training Epoch: 4/10, step 175/574 completed (loss: 0.8458618521690369, acc: 0.782608687877655)
[2024-11-13 06:28:31,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:32,025][root][INFO] - Training Epoch: 4/10, step 176/574 completed (loss: 0.46053898334503174, acc: 0.8522727489471436)
[2024-11-13 06:28:32,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:33,530][root][INFO] - Training Epoch: 4/10, step 177/574 completed (loss: 1.3401477336883545, acc: 0.6183205842971802)
[2024-11-13 06:28:34,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:34,578][root][INFO] - Training Epoch: 4/10, step 178/574 completed (loss: 0.9703102111816406, acc: 0.7333333492279053)
[2024-11-13 06:28:34,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:35,007][root][INFO] - Training Epoch: 4/10, step 179/574 completed (loss: 0.5335720777511597, acc: 0.8360655903816223)
[2024-11-13 06:28:35,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:35,385][root][INFO] - Training Epoch: 4/10, step 180/574 completed (loss: 0.1684780865907669, acc: 0.9583333134651184)
[2024-11-13 06:28:35,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:35,749][root][INFO] - Training Epoch: 4/10, step 181/574 completed (loss: 0.13422654569149017, acc: 0.9599999785423279)
[2024-11-13 06:28:35,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:36,091][root][INFO] - Training Epoch: 4/10, step 182/574 completed (loss: 0.04831768199801445, acc: 1.0)
[2024-11-13 06:28:36,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:36,468][root][INFO] - Training Epoch: 4/10, step 183/574 completed (loss: 0.23793531954288483, acc: 0.9512194991111755)
[2024-11-13 06:28:36,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:37,002][root][INFO] - Training Epoch: 4/10, step 184/574 completed (loss: 0.6260914206504822, acc: 0.8277945518493652)
[2024-11-13 06:28:37,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:37,441][root][INFO] - Training Epoch: 4/10, step 185/574 completed (loss: 0.7333049178123474, acc: 0.7694524526596069)
[2024-11-13 06:28:37,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:38,126][root][INFO] - Training Epoch: 4/10, step 186/574 completed (loss: 0.8337116241455078, acc: 0.7437499761581421)
[2024-11-13 06:28:38,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:38,859][root][INFO] - Training Epoch: 4/10, step 187/574 completed (loss: 0.776848316192627, acc: 0.7861163020133972)
[2024-11-13 06:28:39,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:39,401][root][INFO] - Training Epoch: 4/10, step 188/574 completed (loss: 0.6855877637863159, acc: 0.8149465918540955)
[2024-11-13 06:28:39,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:39,843][root][INFO] - Training Epoch: 4/10, step 189/574 completed (loss: 0.15321113169193268, acc: 0.9599999785423279)
[2024-11-13 06:28:40,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:40,676][root][INFO] - Training Epoch: 4/10, step 190/574 completed (loss: 0.9695552587509155, acc: 0.6976743936538696)
[2024-11-13 06:28:41,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:41,942][root][INFO] - Training Epoch: 4/10, step 191/574 completed (loss: 1.2489577531814575, acc: 0.5714285969734192)
[2024-11-13 06:28:42,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:43,424][root][INFO] - Training Epoch: 4/10, step 192/574 completed (loss: 1.0301406383514404, acc: 0.6666666865348816)
[2024-11-13 06:28:44,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:44,600][root][INFO] - Training Epoch: 4/10, step 193/574 completed (loss: 0.689315140247345, acc: 0.8352941274642944)
[2024-11-13 06:28:45,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:46,359][root][INFO] - Training Epoch: 4/10, step 194/574 completed (loss: 0.9604594707489014, acc: 0.7283950448036194)
[2024-11-13 06:28:47,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:47,906][root][INFO] - Training Epoch: 4/10, step 195/574 completed (loss: 0.4657425284385681, acc: 0.8387096524238586)
[2024-11-13 06:28:48,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:48,281][root][INFO] - Training Epoch: 4/10, step 196/574 completed (loss: 0.19005826115608215, acc: 0.9285714030265808)
[2024-11-13 06:28:48,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:48,710][root][INFO] - Training Epoch: 4/10, step 197/574 completed (loss: 0.5212335586547852, acc: 0.875)
[2024-11-13 06:28:48,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:49,080][root][INFO] - Training Epoch: 4/10, step 198/574 completed (loss: 0.686335563659668, acc: 0.7647058963775635)
[2024-11-13 06:28:49,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:49,457][root][INFO] - Training Epoch: 4/10, step 199/574 completed (loss: 1.056823492050171, acc: 0.7647058963775635)
[2024-11-13 06:28:49,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:49,892][root][INFO] - Training Epoch: 4/10, step 200/574 completed (loss: 0.8201996684074402, acc: 0.7881355881690979)
[2024-11-13 06:28:50,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:50,330][root][INFO] - Training Epoch: 4/10, step 201/574 completed (loss: 0.9548711180686951, acc: 0.7686567306518555)
[2024-11-13 06:28:50,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:50,755][root][INFO] - Training Epoch: 4/10, step 202/574 completed (loss: 0.8895034790039062, acc: 0.7572815418243408)
[2024-11-13 06:28:50,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:51,125][root][INFO] - Training Epoch: 4/10, step 203/574 completed (loss: 0.608518660068512, acc: 0.841269850730896)
[2024-11-13 06:28:51,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:51,543][root][INFO] - Training Epoch: 4/10, step 204/574 completed (loss: 0.28514501452445984, acc: 0.9450549483299255)
[2024-11-13 06:28:51,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:51,994][root][INFO] - Training Epoch: 4/10, step 205/574 completed (loss: 0.4552995264530182, acc: 0.8744394779205322)
[2024-11-13 06:28:52,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:52,546][root][INFO] - Training Epoch: 4/10, step 206/574 completed (loss: 0.5045977830886841, acc: 0.8503937125205994)
[2024-11-13 06:28:52,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:52,988][root][INFO] - Training Epoch: 4/10, step 207/574 completed (loss: 0.5140030980110168, acc: 0.8491379022598267)
[2024-11-13 06:28:53,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:53,457][root][INFO] - Training Epoch: 4/10, step 208/574 completed (loss: 0.5204112529754639, acc: 0.8659420013427734)
[2024-11-13 06:28:53,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:53,928][root][INFO] - Training Epoch: 4/10, step 209/574 completed (loss: 0.4728238880634308, acc: 0.8560311198234558)
[2024-11-13 06:28:54,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:54,324][root][INFO] - Training Epoch: 4/10, step 210/574 completed (loss: 0.37175410985946655, acc: 0.9021739363670349)
[2024-11-13 06:28:54,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:54,711][root][INFO] - Training Epoch: 4/10, step 211/574 completed (loss: 0.08678817003965378, acc: 0.95652174949646)
[2024-11-13 06:28:54,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:55,103][root][INFO] - Training Epoch: 4/10, step 212/574 completed (loss: 0.17138658463954926, acc: 1.0)
[2024-11-13 06:28:55,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:55,547][root][INFO] - Training Epoch: 4/10, step 213/574 completed (loss: 0.1708187758922577, acc: 0.957446813583374)
[2024-11-13 06:28:56,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:56,616][root][INFO] - Training Epoch: 4/10, step 214/574 completed (loss: 0.21113649010658264, acc: 0.9538461565971375)
[2024-11-13 06:28:56,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:57,092][root][INFO] - Training Epoch: 4/10, step 215/574 completed (loss: 0.12736792862415314, acc: 0.9729729890823364)
[2024-11-13 06:28:57,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:57,476][root][INFO] - Training Epoch: 4/10, step 216/574 completed (loss: 0.1596194952726364, acc: 0.9186046719551086)
[2024-11-13 06:28:57,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:58,271][root][INFO] - Training Epoch: 4/10, step 217/574 completed (loss: 0.19668574631214142, acc: 0.9279279112815857)
[2024-11-13 06:28:58,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:58,808][root][INFO] - Training Epoch: 4/10, step 218/574 completed (loss: 0.14871716499328613, acc: 0.9666666388511658)
[2024-11-13 06:28:58,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:59,184][root][INFO] - Training Epoch: 4/10, step 219/574 completed (loss: 0.14069326221942902, acc: 0.9696969985961914)
[2024-11-13 06:28:59,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:59,572][root][INFO] - Training Epoch: 4/10, step 220/574 completed (loss: 0.11386651545763016, acc: 1.0)
[2024-11-13 06:28:59,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:28:59,907][root][INFO] - Training Epoch: 4/10, step 221/574 completed (loss: 0.12742865085601807, acc: 0.9599999785423279)
[2024-11-13 06:29:00,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:00,286][root][INFO] - Training Epoch: 4/10, step 222/574 completed (loss: 0.5654200911521912, acc: 0.8461538553237915)
[2024-11-13 06:29:00,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:01,460][root][INFO] - Training Epoch: 4/10, step 223/574 completed (loss: 0.4029984772205353, acc: 0.8695651888847351)
[2024-11-13 06:29:01,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:02,265][root][INFO] - Training Epoch: 4/10, step 224/574 completed (loss: 0.7064530849456787, acc: 0.8068181872367859)
[2024-11-13 06:29:02,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:02,897][root][INFO] - Training Epoch: 4/10, step 225/574 completed (loss: 0.8492051959037781, acc: 0.7234042286872864)
[2024-11-13 06:29:03,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:03,371][root][INFO] - Training Epoch: 4/10, step 226/574 completed (loss: 0.43249595165252686, acc: 0.849056601524353)
[2024-11-13 06:29:03,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:03,833][root][INFO] - Training Epoch: 4/10, step 227/574 completed (loss: 0.35338038206100464, acc: 0.8999999761581421)
[2024-11-13 06:29:04,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:04,227][root][INFO] - Training Epoch: 4/10, step 228/574 completed (loss: 0.15170423686504364, acc: 0.9767441749572754)
[2024-11-13 06:29:04,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:04,673][root][INFO] - Training Epoch: 4/10, step 229/574 completed (loss: 0.9335610270500183, acc: 0.6666666865348816)
[2024-11-13 06:29:04,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:05,155][root][INFO] - Training Epoch: 4/10, step 230/574 completed (loss: 1.981348991394043, acc: 0.49473685026168823)
[2024-11-13 06:29:05,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:05,558][root][INFO] - Training Epoch: 4/10, step 231/574 completed (loss: 1.4944164752960205, acc: 0.5888888835906982)
[2024-11-13 06:29:05,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:06,146][root][INFO] - Training Epoch: 4/10, step 232/574 completed (loss: 1.4644670486450195, acc: 0.6111111044883728)
[2024-11-13 06:29:06,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:06,854][root][INFO] - Training Epoch: 4/10, step 233/574 completed (loss: 1.6719856262207031, acc: 0.5688073635101318)
[2024-11-13 06:29:07,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:07,549][root][INFO] - Training Epoch: 4/10, step 234/574 completed (loss: 1.437469482421875, acc: 0.6153846383094788)
[2024-11-13 06:29:07,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:07,947][root][INFO] - Training Epoch: 4/10, step 235/574 completed (loss: 0.17406760156154633, acc: 0.9473684430122375)
[2024-11-13 06:29:08,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:08,346][root][INFO] - Training Epoch: 4/10, step 236/574 completed (loss: 0.21107922494411469, acc: 0.9166666865348816)
[2024-11-13 06:29:08,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:08,761][root][INFO] - Training Epoch: 4/10, step 237/574 completed (loss: 0.6536509990692139, acc: 0.8636363744735718)
[2024-11-13 06:29:08,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:09,125][root][INFO] - Training Epoch: 4/10, step 238/574 completed (loss: 0.6162545084953308, acc: 0.8888888955116272)
[2024-11-13 06:29:09,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:09,568][root][INFO] - Training Epoch: 4/10, step 239/574 completed (loss: 0.5285947322845459, acc: 0.800000011920929)
[2024-11-13 06:29:09,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:10,024][root][INFO] - Training Epoch: 4/10, step 240/574 completed (loss: 0.8394022583961487, acc: 0.8636363744735718)
[2024-11-13 06:29:10,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:10,436][root][INFO] - Training Epoch: 4/10, step 241/574 completed (loss: 0.5813328623771667, acc: 0.7727272510528564)
[2024-11-13 06:29:10,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:11,324][root][INFO] - Training Epoch: 4/10, step 242/574 completed (loss: 1.2133383750915527, acc: 0.6612903475761414)
[2024-11-13 06:29:11,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:12,135][root][INFO] - Training Epoch: 4/10, step 243/574 completed (loss: 0.7898288369178772, acc: 0.8409090638160706)
[2024-11-13 06:29:12,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:12,500][root][INFO] - Training Epoch: 4/10, step 244/574 completed (loss: 0.1722346395254135, acc: 0.9523809552192688)
[2024-11-13 06:29:12,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:12,941][root][INFO] - Training Epoch: 4/10, step 245/574 completed (loss: 0.22114409506320953, acc: 0.9230769276618958)
[2024-11-13 06:29:13,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:13,345][root][INFO] - Training Epoch: 4/10, step 246/574 completed (loss: 0.3487348258495331, acc: 0.9354838728904724)
[2024-11-13 06:29:13,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:13,757][root][INFO] - Training Epoch: 4/10, step 247/574 completed (loss: 0.21283689141273499, acc: 0.8999999761581421)
[2024-11-13 06:29:13,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:14,239][root][INFO] - Training Epoch: 4/10, step 248/574 completed (loss: 0.37192508578300476, acc: 0.9459459185600281)
[2024-11-13 06:29:14,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:14,613][root][INFO] - Training Epoch: 4/10, step 249/574 completed (loss: 0.24124398827552795, acc: 0.9459459185600281)
[2024-11-13 06:29:14,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:15,017][root][INFO] - Training Epoch: 4/10, step 250/574 completed (loss: 0.09149375557899475, acc: 0.9729729890823364)
[2024-11-13 06:29:15,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:15,445][root][INFO] - Training Epoch: 4/10, step 251/574 completed (loss: 0.2851771116256714, acc: 0.9264705777168274)
[2024-11-13 06:29:15,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:15,878][root][INFO] - Training Epoch: 4/10, step 252/574 completed (loss: 0.12165547907352448, acc: 0.9268292784690857)
[2024-11-13 06:29:16,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:16,279][root][INFO] - Training Epoch: 4/10, step 253/574 completed (loss: 0.03404809162020683, acc: 1.0)
[2024-11-13 06:29:16,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:16,712][root][INFO] - Training Epoch: 4/10, step 254/574 completed (loss: 0.08317132294178009, acc: 0.9599999785423279)
[2024-11-13 06:29:16,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:17,199][root][INFO] - Training Epoch: 4/10, step 255/574 completed (loss: 0.18089847266674042, acc: 0.9032257795333862)
[2024-11-13 06:29:17,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:17,642][root][INFO] - Training Epoch: 4/10, step 256/574 completed (loss: 0.6451131701469421, acc: 0.9122806787490845)
[2024-11-13 06:29:17,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:18,027][root][INFO] - Training Epoch: 4/10, step 257/574 completed (loss: 0.26323673129081726, acc: 0.8857142925262451)
[2024-11-13 06:29:18,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:18,419][root][INFO] - Training Epoch: 4/10, step 258/574 completed (loss: 0.22113566100597382, acc: 0.9473684430122375)
[2024-11-13 06:29:18,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:19,264][root][INFO] - Training Epoch: 4/10, step 259/574 completed (loss: 0.5206084847450256, acc: 0.8773584961891174)
[2024-11-13 06:29:19,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:20,156][root][INFO] - Training Epoch: 4/10, step 260/574 completed (loss: 0.5166700482368469, acc: 0.8666666746139526)
[2024-11-13 06:29:20,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:20,576][root][INFO] - Training Epoch: 4/10, step 261/574 completed (loss: 0.13614481687545776, acc: 0.9444444179534912)
[2024-11-13 06:29:20,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:21,001][root][INFO] - Training Epoch: 4/10, step 262/574 completed (loss: 0.5964164137840271, acc: 0.8064516186714172)
[2024-11-13 06:29:21,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:21,458][root][INFO] - Training Epoch: 4/10, step 263/574 completed (loss: 1.0820438861846924, acc: 0.7333333492279053)
[2024-11-13 06:29:21,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:21,908][root][INFO] - Training Epoch: 4/10, step 264/574 completed (loss: 0.5687997937202454, acc: 0.8333333134651184)
[2024-11-13 06:29:22,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:23,231][root][INFO] - Training Epoch: 4/10, step 265/574 completed (loss: 1.321462869644165, acc: 0.6159999966621399)
[2024-11-13 06:29:23,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:23,632][root][INFO] - Training Epoch: 4/10, step 266/574 completed (loss: 1.090950846672058, acc: 0.6629213690757751)
[2024-11-13 06:29:23,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:24,096][root][INFO] - Training Epoch: 4/10, step 267/574 completed (loss: 0.8776021599769592, acc: 0.7297297120094299)
[2024-11-13 06:29:24,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:24,751][root][INFO] - Training Epoch: 4/10, step 268/574 completed (loss: 0.6283624768257141, acc: 0.8103448152542114)
[2024-11-13 06:29:24,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:25,150][root][INFO] - Training Epoch: 4/10, step 269/574 completed (loss: 0.1687682867050171, acc: 1.0)
[2024-11-13 06:29:25,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:25,617][root][INFO] - Training Epoch: 4/10, step 270/574 completed (loss: 0.1691199392080307, acc: 0.9090909361839294)
[2024-11-13 06:29:25,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:26,011][root][INFO] - Training Epoch: 4/10, step 271/574 completed (loss: 0.053761076182127, acc: 1.0)
[2024-11-13 06:29:26,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:26,443][root][INFO] - Training Epoch: 4/10, step 272/574 completed (loss: 0.21280980110168457, acc: 0.9333333373069763)
[2024-11-13 06:29:26,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:26,919][root][INFO] - Training Epoch: 4/10, step 273/574 completed (loss: 0.427059143781662, acc: 0.8333333134651184)
[2024-11-13 06:29:27,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:27,289][root][INFO] - Training Epoch: 4/10, step 274/574 completed (loss: 0.2400124967098236, acc: 0.9375)
[2024-11-13 06:29:27,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:27,642][root][INFO] - Training Epoch: 4/10, step 275/574 completed (loss: 0.1750766634941101, acc: 0.9333333373069763)
[2024-11-13 06:29:27,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:28,009][root][INFO] - Training Epoch: 4/10, step 276/574 completed (loss: 0.06676005572080612, acc: 1.0)
[2024-11-13 06:29:28,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:28,413][root][INFO] - Training Epoch: 4/10, step 277/574 completed (loss: 0.07101663947105408, acc: 1.0)
[2024-11-13 06:29:28,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:28,839][root][INFO] - Training Epoch: 4/10, step 278/574 completed (loss: 0.4393153786659241, acc: 0.8297872543334961)
[2024-11-13 06:29:29,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:29,277][root][INFO] - Training Epoch: 4/10, step 279/574 completed (loss: 0.4697345495223999, acc: 0.8958333134651184)
[2024-11-13 06:29:30,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:30,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:31,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:31,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:31,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:32,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:32,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:33,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:34,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:34,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:35,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:35,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:36,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:36,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:37,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:38,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:38,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:39,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:39,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:40,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:40,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:40,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:41,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:41,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:42,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:42,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:43,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:43,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:44,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:44,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:45,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:45,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:46,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:46,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:47,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:47,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:48,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:48,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:49,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:49,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:50,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:50,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:50,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:51,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:51,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:52,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:52,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:53,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:53,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:54,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:54,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:55,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:55,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:56,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:56,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:57,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:57,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:58,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:58,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:59,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:29:59,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:00,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:01,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:01,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:02,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:02,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:03,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:03,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:04,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:05,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:05,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:06,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:06,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:07,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:07,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:08,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:08,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:09,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:09,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:10,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:10,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:10,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:11,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:11,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:12,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:13,250][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3320, device='cuda:0') eval_epoch_loss=tensor(0.8467, device='cuda:0') eval_epoch_acc=tensor(0.7824, device='cuda:0')
[2024-11-13 06:30:13,251][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:30:13,251][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:30:13,701][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_280_loss_0.8467076420783997/model.pt
[2024-11-13 06:30:13,707][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:30:13,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:14,111][root][INFO] - Training Epoch: 4/10, step 280/574 completed (loss: 0.06608089804649353, acc: 0.9772727489471436)
[2024-11-13 06:30:14,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:14,700][root][INFO] - Training Epoch: 4/10, step 281/574 completed (loss: 0.9069296717643738, acc: 0.759036123752594)
[2024-11-13 06:30:14,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:15,219][root][INFO] - Training Epoch: 4/10, step 282/574 completed (loss: 0.951615571975708, acc: 0.7407407164573669)
[2024-11-13 06:30:15,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:15,676][root][INFO] - Training Epoch: 4/10, step 283/574 completed (loss: 0.2002364844083786, acc: 0.9210526347160339)
[2024-11-13 06:30:15,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:16,118][root][INFO] - Training Epoch: 4/10, step 284/574 completed (loss: 0.2702556848526001, acc: 0.9117646813392639)
[2024-11-13 06:30:16,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:16,514][root][INFO] - Training Epoch: 4/10, step 285/574 completed (loss: 0.21211068332195282, acc: 0.925000011920929)
[2024-11-13 06:30:16,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:16,898][root][INFO] - Training Epoch: 4/10, step 286/574 completed (loss: 0.5371333360671997, acc: 0.8515625)
[2024-11-13 06:30:17,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:17,330][root][INFO] - Training Epoch: 4/10, step 287/574 completed (loss: 0.7092850804328918, acc: 0.8240000009536743)
[2024-11-13 06:30:17,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:17,723][root][INFO] - Training Epoch: 4/10, step 288/574 completed (loss: 0.4739903509616852, acc: 0.8351648449897766)
[2024-11-13 06:30:17,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:18,080][root][INFO] - Training Epoch: 4/10, step 289/574 completed (loss: 0.3858737647533417, acc: 0.8819875717163086)
[2024-11-13 06:30:18,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:18,503][root][INFO] - Training Epoch: 4/10, step 290/574 completed (loss: 0.6937984228134155, acc: 0.8041236996650696)
[2024-11-13 06:30:18,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:18,895][root][INFO] - Training Epoch: 4/10, step 291/574 completed (loss: 0.07275411486625671, acc: 1.0)
[2024-11-13 06:30:19,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:19,282][root][INFO] - Training Epoch: 4/10, step 292/574 completed (loss: 0.208012655377388, acc: 0.9523809552192688)
[2024-11-13 06:30:19,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:19,749][root][INFO] - Training Epoch: 4/10, step 293/574 completed (loss: 0.18209196627140045, acc: 0.982758641242981)
[2024-11-13 06:30:20,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:20,428][root][INFO] - Training Epoch: 4/10, step 294/574 completed (loss: 0.4137522280216217, acc: 0.9272727370262146)
[2024-11-13 06:30:20,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:21,251][root][INFO] - Training Epoch: 4/10, step 295/574 completed (loss: 0.6505139470100403, acc: 0.8092783689498901)
[2024-11-13 06:30:21,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:21,704][root][INFO] - Training Epoch: 4/10, step 296/574 completed (loss: 0.39014536142349243, acc: 0.8965517282485962)
[2024-11-13 06:30:21,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:22,171][root][INFO] - Training Epoch: 4/10, step 297/574 completed (loss: 0.13009493052959442, acc: 0.9629629850387573)
[2024-11-13 06:30:22,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:22,571][root][INFO] - Training Epoch: 4/10, step 298/574 completed (loss: 0.32918691635131836, acc: 0.8947368264198303)
[2024-11-13 06:30:22,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:22,981][root][INFO] - Training Epoch: 4/10, step 299/574 completed (loss: 0.23261301219463348, acc: 0.9464285969734192)
[2024-11-13 06:30:23,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:23,349][root][INFO] - Training Epoch: 4/10, step 300/574 completed (loss: 0.03866931051015854, acc: 1.0)
[2024-11-13 06:30:23,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:23,737][root][INFO] - Training Epoch: 4/10, step 301/574 completed (loss: 0.28083473443984985, acc: 0.9433962106704712)
[2024-11-13 06:30:23,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:24,118][root][INFO] - Training Epoch: 4/10, step 302/574 completed (loss: 0.035187818109989166, acc: 0.9811320900917053)
[2024-11-13 06:30:24,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:24,465][root][INFO] - Training Epoch: 4/10, step 303/574 completed (loss: 0.018472306430339813, acc: 1.0)
[2024-11-13 06:30:24,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:24,845][root][INFO] - Training Epoch: 4/10, step 304/574 completed (loss: 0.140811026096344, acc: 0.9375)
[2024-11-13 06:30:25,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:25,254][root][INFO] - Training Epoch: 4/10, step 305/574 completed (loss: 0.49940115213394165, acc: 0.8524590134620667)
[2024-11-13 06:30:25,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:25,590][root][INFO] - Training Epoch: 4/10, step 306/574 completed (loss: 0.0641464963555336, acc: 1.0)
[2024-11-13 06:30:25,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:25,946][root][INFO] - Training Epoch: 4/10, step 307/574 completed (loss: 0.0034138078335672617, acc: 1.0)
[2024-11-13 06:30:26,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:26,322][root][INFO] - Training Epoch: 4/10, step 308/574 completed (loss: 0.2778227627277374, acc: 0.9275362491607666)
[2024-11-13 06:30:26,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:26,907][root][INFO] - Training Epoch: 4/10, step 309/574 completed (loss: 0.4047571122646332, acc: 0.9027777910232544)
[2024-11-13 06:30:27,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:27,288][root][INFO] - Training Epoch: 4/10, step 310/574 completed (loss: 0.14727918803691864, acc: 0.9759036302566528)
[2024-11-13 06:30:27,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:27,676][root][INFO] - Training Epoch: 4/10, step 311/574 completed (loss: 0.31554320454597473, acc: 0.9102563858032227)
[2024-11-13 06:30:27,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:28,123][root][INFO] - Training Epoch: 4/10, step 312/574 completed (loss: 0.24586258828639984, acc: 0.9387755393981934)
[2024-11-13 06:30:28,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:28,487][root][INFO] - Training Epoch: 4/10, step 313/574 completed (loss: 0.015381983481347561, acc: 1.0)
[2024-11-13 06:30:28,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:28,836][root][INFO] - Training Epoch: 4/10, step 314/574 completed (loss: 0.05638740584254265, acc: 1.0)
[2024-11-13 06:30:28,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:29,169][root][INFO] - Training Epoch: 4/10, step 315/574 completed (loss: 0.265327125787735, acc: 0.9354838728904724)
[2024-11-13 06:30:29,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:29,519][root][INFO] - Training Epoch: 4/10, step 316/574 completed (loss: 0.3468446135520935, acc: 0.9032257795333862)
[2024-11-13 06:30:29,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:30,019][root][INFO] - Training Epoch: 4/10, step 317/574 completed (loss: 0.12326417118310928, acc: 0.9552238583564758)
[2024-11-13 06:30:30,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:30,417][root][INFO] - Training Epoch: 4/10, step 318/574 completed (loss: 0.19693781435489655, acc: 0.9519230723381042)
[2024-11-13 06:30:30,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:30,821][root][INFO] - Training Epoch: 4/10, step 319/574 completed (loss: 0.16591741144657135, acc: 0.9333333373069763)
[2024-11-13 06:30:31,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:31,249][root][INFO] - Training Epoch: 4/10, step 320/574 completed (loss: 0.14747586846351624, acc: 0.9354838728904724)
[2024-11-13 06:30:31,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:31,607][root][INFO] - Training Epoch: 4/10, step 321/574 completed (loss: 0.03403841704130173, acc: 0.9800000190734863)
[2024-11-13 06:30:31,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:32,001][root][INFO] - Training Epoch: 4/10, step 322/574 completed (loss: 0.7236810326576233, acc: 0.7407407164573669)
[2024-11-13 06:30:32,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:32,341][root][INFO] - Training Epoch: 4/10, step 323/574 completed (loss: 1.3184229135513306, acc: 0.6285714507102966)
[2024-11-13 06:30:32,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:32,703][root][INFO] - Training Epoch: 4/10, step 324/574 completed (loss: 0.8935211300849915, acc: 0.6666666865348816)
[2024-11-13 06:30:32,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:33,062][root][INFO] - Training Epoch: 4/10, step 325/574 completed (loss: 1.3431943655014038, acc: 0.5609756112098694)
[2024-11-13 06:30:33,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:33,542][root][INFO] - Training Epoch: 4/10, step 326/574 completed (loss: 0.9056968688964844, acc: 0.8157894611358643)
[2024-11-13 06:30:33,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:33,987][root][INFO] - Training Epoch: 4/10, step 327/574 completed (loss: 0.10174386948347092, acc: 1.0)
[2024-11-13 06:30:34,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:34,365][root][INFO] - Training Epoch: 4/10, step 328/574 completed (loss: 0.04865900054574013, acc: 0.9642857313156128)
[2024-11-13 06:30:34,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:34,702][root][INFO] - Training Epoch: 4/10, step 329/574 completed (loss: 0.05386914685368538, acc: 0.9629629850387573)
[2024-11-13 06:30:34,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:35,128][root][INFO] - Training Epoch: 4/10, step 330/574 completed (loss: 0.022774526849389076, acc: 1.0)
[2024-11-13 06:30:35,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:35,544][root][INFO] - Training Epoch: 4/10, step 331/574 completed (loss: 0.30850470066070557, acc: 0.9354838728904724)
[2024-11-13 06:30:35,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:36,032][root][INFO] - Training Epoch: 4/10, step 332/574 completed (loss: 0.09703719615936279, acc: 0.9824561476707458)
[2024-11-13 06:30:36,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:36,395][root][INFO] - Training Epoch: 4/10, step 333/574 completed (loss: 0.34525176882743835, acc: 0.90625)
[2024-11-13 06:30:36,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:36,777][root][INFO] - Training Epoch: 4/10, step 334/574 completed (loss: 0.050345707684755325, acc: 1.0)
[2024-11-13 06:30:36,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:37,125][root][INFO] - Training Epoch: 4/10, step 335/574 completed (loss: 0.05623212084174156, acc: 1.0)
[2024-11-13 06:30:37,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:37,507][root][INFO] - Training Epoch: 4/10, step 336/574 completed (loss: 0.8785779476165771, acc: 0.7799999713897705)
[2024-11-13 06:30:37,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:37,944][root][INFO] - Training Epoch: 4/10, step 337/574 completed (loss: 1.1354265213012695, acc: 0.6666666865348816)
[2024-11-13 06:30:38,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:38,377][root][INFO] - Training Epoch: 4/10, step 338/574 completed (loss: 1.3168703317642212, acc: 0.6276595592498779)
[2024-11-13 06:30:38,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:38,799][root][INFO] - Training Epoch: 4/10, step 339/574 completed (loss: 1.2527730464935303, acc: 0.650602400302887)
[2024-11-13 06:30:38,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:39,154][root][INFO] - Training Epoch: 4/10, step 340/574 completed (loss: 0.03941916674375534, acc: 1.0)
[2024-11-13 06:30:39,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:39,543][root][INFO] - Training Epoch: 4/10, step 341/574 completed (loss: 0.5622743368148804, acc: 0.8974359035491943)
[2024-11-13 06:30:39,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:39,968][root][INFO] - Training Epoch: 4/10, step 342/574 completed (loss: 0.3786826431751251, acc: 0.8674699068069458)
[2024-11-13 06:30:40,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:40,323][root][INFO] - Training Epoch: 4/10, step 343/574 completed (loss: 0.4261813163757324, acc: 0.8679245114326477)
[2024-11-13 06:30:40,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:40,695][root][INFO] - Training Epoch: 4/10, step 344/574 completed (loss: 0.129713773727417, acc: 0.9620253443717957)
[2024-11-13 06:30:40,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:41,085][root][INFO] - Training Epoch: 4/10, step 345/574 completed (loss: 0.0909990668296814, acc: 0.9607843160629272)
[2024-11-13 06:30:41,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:41,480][root][INFO] - Training Epoch: 4/10, step 346/574 completed (loss: 0.34432196617126465, acc: 0.9104477763175964)
[2024-11-13 06:30:41,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:41,861][root][INFO] - Training Epoch: 4/10, step 347/574 completed (loss: 0.34714239835739136, acc: 0.8999999761581421)
[2024-11-13 06:30:42,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:42,252][root][INFO] - Training Epoch: 4/10, step 348/574 completed (loss: 0.029966535046696663, acc: 1.0)
[2024-11-13 06:30:42,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:42,762][root][INFO] - Training Epoch: 4/10, step 349/574 completed (loss: 0.614149808883667, acc: 0.8055555820465088)
[2024-11-13 06:30:42,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:43,149][root][INFO] - Training Epoch: 4/10, step 350/574 completed (loss: 0.7587434649467468, acc: 0.7441860437393188)
[2024-11-13 06:30:43,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:43,564][root][INFO] - Training Epoch: 4/10, step 351/574 completed (loss: 0.28624752163887024, acc: 0.9230769276618958)
[2024-11-13 06:30:43,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:44,070][root][INFO] - Training Epoch: 4/10, step 352/574 completed (loss: 0.9373241662979126, acc: 0.7333333492279053)
[2024-11-13 06:30:44,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:44,418][root][INFO] - Training Epoch: 4/10, step 353/574 completed (loss: 0.07336971908807755, acc: 1.0)
[2024-11-13 06:30:44,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:44,780][root][INFO] - Training Epoch: 4/10, step 354/574 completed (loss: 0.3226691484451294, acc: 0.8846153616905212)
[2024-11-13 06:30:44,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:45,162][root][INFO] - Training Epoch: 4/10, step 355/574 completed (loss: 0.8374525904655457, acc: 0.7582417726516724)
[2024-11-13 06:30:45,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:45,901][root][INFO] - Training Epoch: 4/10, step 356/574 completed (loss: 0.6962893009185791, acc: 0.791304349899292)
[2024-11-13 06:30:46,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:46,273][root][INFO] - Training Epoch: 4/10, step 357/574 completed (loss: 0.5635746121406555, acc: 0.8152173757553101)
[2024-11-13 06:30:46,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:46,711][root][INFO] - Training Epoch: 4/10, step 358/574 completed (loss: 0.6567579507827759, acc: 0.795918345451355)
[2024-11-13 06:30:46,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:47,154][root][INFO] - Training Epoch: 4/10, step 359/574 completed (loss: 0.010229193605482578, acc: 1.0)
[2024-11-13 06:30:47,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:47,556][root][INFO] - Training Epoch: 4/10, step 360/574 completed (loss: 0.14076265692710876, acc: 0.9230769276618958)
[2024-11-13 06:30:47,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:47,922][root][INFO] - Training Epoch: 4/10, step 361/574 completed (loss: 0.40093180537223816, acc: 0.8780487775802612)
[2024-11-13 06:30:48,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:48,377][root][INFO] - Training Epoch: 4/10, step 362/574 completed (loss: 0.3859066367149353, acc: 0.8666666746139526)
[2024-11-13 06:30:48,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:48,865][root][INFO] - Training Epoch: 4/10, step 363/574 completed (loss: 0.20344014465808868, acc: 0.9210526347160339)
[2024-11-13 06:30:49,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:49,381][root][INFO] - Training Epoch: 4/10, step 364/574 completed (loss: 0.1570172756910324, acc: 0.9512194991111755)
[2024-11-13 06:30:49,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:49,758][root][INFO] - Training Epoch: 4/10, step 365/574 completed (loss: 0.1276565045118332, acc: 0.9696969985961914)
[2024-11-13 06:30:49,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:50,104][root][INFO] - Training Epoch: 4/10, step 366/574 completed (loss: 0.010193685069680214, acc: 1.0)
[2024-11-13 06:30:50,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:50,488][root][INFO] - Training Epoch: 4/10, step 367/574 completed (loss: 0.013815229758620262, acc: 1.0)
[2024-11-13 06:30:50,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:50,903][root][INFO] - Training Epoch: 4/10, step 368/574 completed (loss: 0.08651677519083023, acc: 0.9642857313156128)
[2024-11-13 06:30:51,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:51,332][root][INFO] - Training Epoch: 4/10, step 369/574 completed (loss: 0.34146934747695923, acc: 0.9375)
[2024-11-13 06:30:51,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:52,287][root][INFO] - Training Epoch: 4/10, step 370/574 completed (loss: 0.8513646125793457, acc: 0.7757575511932373)
[2024-11-13 06:30:53,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:53,608][root][INFO] - Training Epoch: 4/10, step 371/574 completed (loss: 0.41129356622695923, acc: 0.8867924809455872)
[2024-11-13 06:30:53,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:54,093][root][INFO] - Training Epoch: 4/10, step 372/574 completed (loss: 0.3808057904243469, acc: 0.9111111164093018)
[2024-11-13 06:30:54,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:54,541][root][INFO] - Training Epoch: 4/10, step 373/574 completed (loss: 0.22585247457027435, acc: 0.9464285969734192)
[2024-11-13 06:30:54,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:55,051][root][INFO] - Training Epoch: 4/10, step 374/574 completed (loss: 0.09909388422966003, acc: 0.9714285731315613)
[2024-11-13 06:30:55,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:55,501][root][INFO] - Training Epoch: 4/10, step 375/574 completed (loss: 0.04003744572401047, acc: 0.9599999785423279)
[2024-11-13 06:30:55,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:55,962][root][INFO] - Training Epoch: 4/10, step 376/574 completed (loss: 0.05634632706642151, acc: 1.0)
[2024-11-13 06:30:56,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:56,385][root][INFO] - Training Epoch: 4/10, step 377/574 completed (loss: 0.1397114247083664, acc: 0.9791666865348816)
[2024-11-13 06:30:56,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:56,790][root][INFO] - Training Epoch: 4/10, step 378/574 completed (loss: 0.07297752797603607, acc: 0.9894737005233765)
[2024-11-13 06:30:57,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:57,842][root][INFO] - Training Epoch: 4/10, step 379/574 completed (loss: 0.256152868270874, acc: 0.9221556782722473)
[2024-11-13 06:30:58,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:30:58,412][root][INFO] - Training Epoch: 4/10, step 380/574 completed (loss: 0.37422284483909607, acc: 0.9097744226455688)
[2024-11-13 06:30:59,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:00,331][root][INFO] - Training Epoch: 4/10, step 381/574 completed (loss: 0.51463383436203, acc: 0.8770053386688232)
[2024-11-13 06:31:00,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:01,205][root][INFO] - Training Epoch: 4/10, step 382/574 completed (loss: 0.13748188316822052, acc: 0.9639639854431152)
[2024-11-13 06:31:01,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:01,659][root][INFO] - Training Epoch: 4/10, step 383/574 completed (loss: 0.2653796672821045, acc: 0.9285714030265808)
[2024-11-13 06:31:01,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:02,079][root][INFO] - Training Epoch: 4/10, step 384/574 completed (loss: 0.06283026188611984, acc: 0.9642857313156128)
[2024-11-13 06:31:02,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:02,467][root][INFO] - Training Epoch: 4/10, step 385/574 completed (loss: 0.30900317430496216, acc: 0.9375)
[2024-11-13 06:31:02,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:02,913][root][INFO] - Training Epoch: 4/10, step 386/574 completed (loss: 0.01999014802277088, acc: 1.0)
[2024-11-13 06:31:03,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:03,330][root][INFO] - Training Epoch: 4/10, step 387/574 completed (loss: 0.054991215467453, acc: 0.9736841917037964)
[2024-11-13 06:31:03,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:03,773][root][INFO] - Training Epoch: 4/10, step 388/574 completed (loss: 0.006551432888954878, acc: 1.0)
[2024-11-13 06:31:03,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:04,180][root][INFO] - Training Epoch: 4/10, step 389/574 completed (loss: 0.0952034443616867, acc: 0.949999988079071)
[2024-11-13 06:31:04,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:04,608][root][INFO] - Training Epoch: 4/10, step 390/574 completed (loss: 0.21646860241889954, acc: 0.9523809552192688)
[2024-11-13 06:31:04,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:05,058][root][INFO] - Training Epoch: 4/10, step 391/574 completed (loss: 1.018825888633728, acc: 0.7407407164573669)
[2024-11-13 06:31:05,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:05,558][root][INFO] - Training Epoch: 4/10, step 392/574 completed (loss: 0.9322746992111206, acc: 0.737864077091217)
[2024-11-13 06:31:06,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:06,426][root][INFO] - Training Epoch: 4/10, step 393/574 completed (loss: 1.006617546081543, acc: 0.7647058963775635)
[2024-11-13 06:31:06,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:06,917][root][INFO] - Training Epoch: 4/10, step 394/574 completed (loss: 0.868881344795227, acc: 0.7333333492279053)
[2024-11-13 06:31:07,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:07,441][root][INFO] - Training Epoch: 4/10, step 395/574 completed (loss: 0.5960940718650818, acc: 0.8333333134651184)
[2024-11-13 06:31:07,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:07,878][root][INFO] - Training Epoch: 4/10, step 396/574 completed (loss: 0.17070941627025604, acc: 0.930232584476471)
[2024-11-13 06:31:08,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:08,258][root][INFO] - Training Epoch: 4/10, step 397/574 completed (loss: 0.11916782706975937, acc: 0.9583333134651184)
[2024-11-13 06:31:08,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:08,724][root][INFO] - Training Epoch: 4/10, step 398/574 completed (loss: 0.1839284598827362, acc: 0.930232584476471)
[2024-11-13 06:31:08,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:09,156][root][INFO] - Training Epoch: 4/10, step 399/574 completed (loss: 0.09467636793851852, acc: 0.9599999785423279)
[2024-11-13 06:31:09,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:09,997][root][INFO] - Training Epoch: 4/10, step 400/574 completed (loss: 0.22537492215633392, acc: 0.9264705777168274)
[2024-11-13 06:31:10,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:10,440][root][INFO] - Training Epoch: 4/10, step 401/574 completed (loss: 0.320144921541214, acc: 0.8666666746139526)
[2024-11-13 06:31:10,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:10,851][root][INFO] - Training Epoch: 4/10, step 402/574 completed (loss: 0.3888847231864929, acc: 0.8787878751754761)
[2024-11-13 06:31:11,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:11,303][root][INFO] - Training Epoch: 4/10, step 403/574 completed (loss: 0.1928149163722992, acc: 0.939393937587738)
[2024-11-13 06:31:11,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:11,776][root][INFO] - Training Epoch: 4/10, step 404/574 completed (loss: 0.1350133866071701, acc: 0.9677419066429138)
[2024-11-13 06:31:12,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:12,269][root][INFO] - Training Epoch: 4/10, step 405/574 completed (loss: 0.20106087625026703, acc: 0.9629629850387573)
[2024-11-13 06:31:12,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:12,671][root][INFO] - Training Epoch: 4/10, step 406/574 completed (loss: 0.09708653390407562, acc: 0.9200000166893005)
[2024-11-13 06:31:12,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:13,094][root][INFO] - Training Epoch: 4/10, step 407/574 completed (loss: 0.02997511625289917, acc: 1.0)
[2024-11-13 06:31:13,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:13,466][root][INFO] - Training Epoch: 4/10, step 408/574 completed (loss: 0.13481405377388, acc: 0.9629629850387573)
[2024-11-13 06:31:13,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:13,873][root][INFO] - Training Epoch: 4/10, step 409/574 completed (loss: 0.025708874687552452, acc: 1.0)
[2024-11-13 06:31:14,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:14,296][root][INFO] - Training Epoch: 4/10, step 410/574 completed (loss: 0.13700109720230103, acc: 0.982758641242981)
[2024-11-13 06:31:14,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:14,735][root][INFO] - Training Epoch: 4/10, step 411/574 completed (loss: 0.10449738800525665, acc: 0.9642857313156128)
[2024-11-13 06:31:14,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:15,176][root][INFO] - Training Epoch: 4/10, step 412/574 completed (loss: 0.08184615522623062, acc: 0.9666666388511658)
[2024-11-13 06:31:15,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:15,566][root][INFO] - Training Epoch: 4/10, step 413/574 completed (loss: 0.029174726456403732, acc: 1.0)
[2024-11-13 06:31:15,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:15,927][root][INFO] - Training Epoch: 4/10, step 414/574 completed (loss: 0.041288189589977264, acc: 1.0)
[2024-11-13 06:31:16,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:16,379][root][INFO] - Training Epoch: 4/10, step 415/574 completed (loss: 0.35890552401542664, acc: 0.8627451062202454)
[2024-11-13 06:31:16,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:16,767][root][INFO] - Training Epoch: 4/10, step 416/574 completed (loss: 0.06414742767810822, acc: 0.9615384340286255)
[2024-11-13 06:31:16,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:17,147][root][INFO] - Training Epoch: 4/10, step 417/574 completed (loss: 0.1493234932422638, acc: 1.0)
[2024-11-13 06:31:17,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:17,607][root][INFO] - Training Epoch: 4/10, step 418/574 completed (loss: 0.08903733640909195, acc: 0.9750000238418579)
[2024-11-13 06:31:17,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:18,046][root][INFO] - Training Epoch: 4/10, step 419/574 completed (loss: 0.06585680693387985, acc: 1.0)
[2024-11-13 06:31:18,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:18,473][root][INFO] - Training Epoch: 4/10, step 420/574 completed (loss: 0.07098130136728287, acc: 1.0)
[2024-11-13 06:31:18,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:18,958][root][INFO] - Training Epoch: 4/10, step 421/574 completed (loss: 0.2779535949230194, acc: 0.8999999761581421)
[2024-11-13 06:31:19,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:19,309][root][INFO] - Training Epoch: 4/10, step 422/574 completed (loss: 0.29704779386520386, acc: 0.90625)
[2024-11-13 06:31:20,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:20,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:21,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:21,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:22,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:22,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:23,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:23,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:24,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:24,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:25,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:25,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:26,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:26,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:27,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:27,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:28,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:28,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:29,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:29,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:30,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:30,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:31,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:31,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:32,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:32,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:33,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:33,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:34,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:34,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:35,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:35,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:36,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:36,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:37,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:37,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:38,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:38,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:39,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:39,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:40,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:40,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:41,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:41,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:42,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:42,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:43,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:43,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:44,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:44,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:44,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:45,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:45,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:46,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:46,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:47,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:47,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:47,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:48,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:49,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:49,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:50,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:50,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:51,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:51,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:52,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:53,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:53,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:54,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:54,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:55,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:56,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:56,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:56,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:57,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:57,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:58,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:58,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:59,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:31:59,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:00,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:00,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:01,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:01,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:02,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:02,643][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3755, device='cuda:0') eval_epoch_loss=tensor(0.8652, device='cuda:0') eval_epoch_acc=tensor(0.7900, device='cuda:0')
[2024-11-13 06:32:02,644][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:32:02,645][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:32:03,158][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_423_loss_0.865225076675415/model.pt
[2024-11-13 06:32:03,171][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:32:03,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:03,645][root][INFO] - Training Epoch: 4/10, step 423/574 completed (loss: 0.19697445631027222, acc: 0.9722222089767456)
[2024-11-13 06:32:03,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:04,058][root][INFO] - Training Epoch: 4/10, step 424/574 completed (loss: 0.1272946447134018, acc: 0.9259259104728699)
[2024-11-13 06:32:04,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:04,533][root][INFO] - Training Epoch: 4/10, step 425/574 completed (loss: 0.15940403938293457, acc: 0.939393937587738)
[2024-11-13 06:32:04,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:04,915][root][INFO] - Training Epoch: 4/10, step 426/574 completed (loss: 0.08434947580099106, acc: 0.95652174949646)
[2024-11-13 06:32:05,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:05,348][root][INFO] - Training Epoch: 4/10, step 427/574 completed (loss: 0.08340944349765778, acc: 1.0)
[2024-11-13 06:32:05,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:05,698][root][INFO] - Training Epoch: 4/10, step 428/574 completed (loss: 0.015329675748944283, acc: 1.0)
[2024-11-13 06:32:05,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:06,043][root][INFO] - Training Epoch: 4/10, step 429/574 completed (loss: 0.024744518101215363, acc: 1.0)
[2024-11-13 06:32:06,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:06,395][root][INFO] - Training Epoch: 4/10, step 430/574 completed (loss: 0.005335354246199131, acc: 1.0)
[2024-11-13 06:32:06,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:06,845][root][INFO] - Training Epoch: 4/10, step 431/574 completed (loss: 0.018948916345834732, acc: 1.0)
[2024-11-13 06:32:07,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:07,229][root][INFO] - Training Epoch: 4/10, step 432/574 completed (loss: 0.02318994700908661, acc: 1.0)
[2024-11-13 06:32:07,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:07,680][root][INFO] - Training Epoch: 4/10, step 433/574 completed (loss: 0.08625992387533188, acc: 0.9722222089767456)
[2024-11-13 06:32:07,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:08,113][root][INFO] - Training Epoch: 4/10, step 434/574 completed (loss: 0.036751460283994675, acc: 0.9599999785423279)
[2024-11-13 06:32:08,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:08,601][root][INFO] - Training Epoch: 4/10, step 435/574 completed (loss: 0.2750060260295868, acc: 0.939393937587738)
[2024-11-13 06:32:08,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:09,048][root][INFO] - Training Epoch: 4/10, step 436/574 completed (loss: 0.13217271864414215, acc: 0.9444444179534912)
[2024-11-13 06:32:09,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:09,448][root][INFO] - Training Epoch: 4/10, step 437/574 completed (loss: 0.11106405407190323, acc: 0.9545454382896423)
[2024-11-13 06:32:09,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:09,793][root][INFO] - Training Epoch: 4/10, step 438/574 completed (loss: 0.0013737636618316174, acc: 1.0)
[2024-11-13 06:32:09,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:10,196][root][INFO] - Training Epoch: 4/10, step 439/574 completed (loss: 0.30030909180641174, acc: 0.8974359035491943)
[2024-11-13 06:32:10,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:10,867][root][INFO] - Training Epoch: 4/10, step 440/574 completed (loss: 0.24674539268016815, acc: 0.9242424368858337)
[2024-11-13 06:32:11,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:11,933][root][INFO] - Training Epoch: 4/10, step 441/574 completed (loss: 0.6948360800743103, acc: 0.7599999904632568)
[2024-11-13 06:32:12,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:12,495][root][INFO] - Training Epoch: 4/10, step 442/574 completed (loss: 0.9971663355827332, acc: 0.7419354915618896)
[2024-11-13 06:32:13,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:13,494][root][INFO] - Training Epoch: 4/10, step 443/574 completed (loss: 0.6563049554824829, acc: 0.8159204125404358)
[2024-11-13 06:32:13,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:13,887][root][INFO] - Training Epoch: 4/10, step 444/574 completed (loss: 0.13620468974113464, acc: 0.9622641801834106)
[2024-11-13 06:32:14,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:14,481][root][INFO] - Training Epoch: 4/10, step 445/574 completed (loss: 0.10271117091178894, acc: 0.9545454382896423)
[2024-11-13 06:32:14,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:14,842][root][INFO] - Training Epoch: 4/10, step 446/574 completed (loss: 0.22776810824871063, acc: 0.9130434989929199)
[2024-11-13 06:32:14,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:15,182][root][INFO] - Training Epoch: 4/10, step 447/574 completed (loss: 0.2967440187931061, acc: 0.8461538553237915)
[2024-11-13 06:32:15,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:15,534][root][INFO] - Training Epoch: 4/10, step 448/574 completed (loss: 0.05502889305353165, acc: 1.0)
[2024-11-13 06:32:15,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:15,915][root][INFO] - Training Epoch: 4/10, step 449/574 completed (loss: 0.20836466550827026, acc: 0.9402984976768494)
[2024-11-13 06:32:16,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:16,292][root][INFO] - Training Epoch: 4/10, step 450/574 completed (loss: 0.09510473906993866, acc: 0.9722222089767456)
[2024-11-13 06:32:16,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:16,688][root][INFO] - Training Epoch: 4/10, step 451/574 completed (loss: 0.07215321809053421, acc: 0.989130437374115)
[2024-11-13 06:32:16,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:17,064][root][INFO] - Training Epoch: 4/10, step 452/574 completed (loss: 0.23342815041542053, acc: 0.9615384340286255)
[2024-11-13 06:32:17,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:17,440][root][INFO] - Training Epoch: 4/10, step 453/574 completed (loss: 0.20866446197032928, acc: 0.9605262875556946)
[2024-11-13 06:32:17,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:17,790][root][INFO] - Training Epoch: 4/10, step 454/574 completed (loss: 0.16085956990718842, acc: 0.9591836929321289)
[2024-11-13 06:32:17,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:18,131][root][INFO] - Training Epoch: 4/10, step 455/574 completed (loss: 0.08750069886445999, acc: 0.9696969985961914)
[2024-11-13 06:32:18,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:18,606][root][INFO] - Training Epoch: 4/10, step 456/574 completed (loss: 0.7372129559516907, acc: 0.8556700944900513)
[2024-11-13 06:32:18,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:19,010][root][INFO] - Training Epoch: 4/10, step 457/574 completed (loss: 0.19857414066791534, acc: 0.9285714030265808)
[2024-11-13 06:32:19,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:19,501][root][INFO] - Training Epoch: 4/10, step 458/574 completed (loss: 0.6296640634536743, acc: 0.8139534592628479)
[2024-11-13 06:32:19,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:19,854][root][INFO] - Training Epoch: 4/10, step 459/574 completed (loss: 0.23883947730064392, acc: 0.9642857313156128)
[2024-11-13 06:32:20,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:20,233][root][INFO] - Training Epoch: 4/10, step 460/574 completed (loss: 0.4083000421524048, acc: 0.8888888955116272)
[2024-11-13 06:32:20,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:20,669][root][INFO] - Training Epoch: 4/10, step 461/574 completed (loss: 0.2779582738876343, acc: 0.9444444179534912)
[2024-11-13 06:32:20,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:21,042][root][INFO] - Training Epoch: 4/10, step 462/574 completed (loss: 0.11553055047988892, acc: 0.96875)
[2024-11-13 06:32:21,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:21,388][root][INFO] - Training Epoch: 4/10, step 463/574 completed (loss: 0.14601662755012512, acc: 0.9615384340286255)
[2024-11-13 06:32:21,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:21,801][root][INFO] - Training Epoch: 4/10, step 464/574 completed (loss: 0.059681572020053864, acc: 1.0)
[2024-11-13 06:32:21,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:22,266][root][INFO] - Training Epoch: 4/10, step 465/574 completed (loss: 0.3412271738052368, acc: 0.9047619104385376)
[2024-11-13 06:32:22,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:22,618][root][INFO] - Training Epoch: 4/10, step 466/574 completed (loss: 0.6295650005340576, acc: 0.8433734774589539)
[2024-11-13 06:32:22,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:23,111][root][INFO] - Training Epoch: 4/10, step 467/574 completed (loss: 0.25616687536239624, acc: 0.9189189076423645)
[2024-11-13 06:32:23,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:23,520][root][INFO] - Training Epoch: 4/10, step 468/574 completed (loss: 0.689239501953125, acc: 0.8252426981925964)
[2024-11-13 06:32:23,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:23,903][root][INFO] - Training Epoch: 4/10, step 469/574 completed (loss: 0.697809100151062, acc: 0.8048780560493469)
[2024-11-13 06:32:24,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:24,247][root][INFO] - Training Epoch: 4/10, step 470/574 completed (loss: 0.15027748048305511, acc: 0.9583333134651184)
[2024-11-13 06:32:24,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:24,584][root][INFO] - Training Epoch: 4/10, step 471/574 completed (loss: 0.1376728117465973, acc: 0.9642857313156128)
[2024-11-13 06:32:24,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:25,152][root][INFO] - Training Epoch: 4/10, step 472/574 completed (loss: 0.6472452282905579, acc: 0.8235294222831726)
[2024-11-13 06:32:25,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:25,634][root][INFO] - Training Epoch: 4/10, step 473/574 completed (loss: 0.8298963308334351, acc: 0.7598253488540649)
[2024-11-13 06:32:25,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:26,031][root][INFO] - Training Epoch: 4/10, step 474/574 completed (loss: 0.4947190284729004, acc: 0.8854166865348816)
[2024-11-13 06:32:26,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:26,450][root][INFO] - Training Epoch: 4/10, step 475/574 completed (loss: 0.35262471437454224, acc: 0.8773006200790405)
[2024-11-13 06:32:26,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:26,821][root][INFO] - Training Epoch: 4/10, step 476/574 completed (loss: 0.40218204259872437, acc: 0.8776978254318237)
[2024-11-13 06:32:27,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:27,257][root][INFO] - Training Epoch: 4/10, step 477/574 completed (loss: 0.6921424269676208, acc: 0.7738693356513977)
[2024-11-13 06:32:27,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:27,623][root][INFO] - Training Epoch: 4/10, step 478/574 completed (loss: 0.5890004634857178, acc: 0.8333333134651184)
[2024-11-13 06:32:27,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:27,975][root][INFO] - Training Epoch: 4/10, step 479/574 completed (loss: 0.26083841919898987, acc: 0.9090909361839294)
[2024-11-13 06:32:28,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:28,338][root][INFO] - Training Epoch: 4/10, step 480/574 completed (loss: 0.21995718777179718, acc: 0.9259259104728699)
[2024-11-13 06:32:28,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:28,732][root][INFO] - Training Epoch: 4/10, step 481/574 completed (loss: 0.25968295335769653, acc: 0.8999999761581421)
[2024-11-13 06:32:28,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:29,082][root][INFO] - Training Epoch: 4/10, step 482/574 completed (loss: 0.2759779691696167, acc: 0.949999988079071)
[2024-11-13 06:32:29,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:29,587][root][INFO] - Training Epoch: 4/10, step 483/574 completed (loss: 0.5723909139633179, acc: 0.8103448152542114)
[2024-11-13 06:32:29,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:29,982][root][INFO] - Training Epoch: 4/10, step 484/574 completed (loss: 0.11599591374397278, acc: 0.9677419066429138)
[2024-11-13 06:32:30,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:30,334][root][INFO] - Training Epoch: 4/10, step 485/574 completed (loss: 0.2649158239364624, acc: 0.9473684430122375)
[2024-11-13 06:32:30,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:30,698][root][INFO] - Training Epoch: 4/10, step 486/574 completed (loss: 0.6786137819290161, acc: 0.7407407164573669)
[2024-11-13 06:32:30,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:31,072][root][INFO] - Training Epoch: 4/10, step 487/574 completed (loss: 0.2586991488933563, acc: 0.8571428656578064)
[2024-11-13 06:32:31,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:31,449][root][INFO] - Training Epoch: 4/10, step 488/574 completed (loss: 0.44263628125190735, acc: 0.9090909361839294)
[2024-11-13 06:32:31,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:31,885][root][INFO] - Training Epoch: 4/10, step 489/574 completed (loss: 0.8331392407417297, acc: 0.7384615540504456)
[2024-11-13 06:32:32,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:32,239][root][INFO] - Training Epoch: 4/10, step 490/574 completed (loss: 0.057274140417575836, acc: 1.0)
[2024-11-13 06:32:32,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:32,587][root][INFO] - Training Epoch: 4/10, step 491/574 completed (loss: 0.3090323507785797, acc: 0.8965517282485962)
[2024-11-13 06:32:32,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:32,947][root][INFO] - Training Epoch: 4/10, step 492/574 completed (loss: 0.17856664955615997, acc: 0.9411764740943909)
[2024-11-13 06:32:33,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:33,332][root][INFO] - Training Epoch: 4/10, step 493/574 completed (loss: 0.2097819596529007, acc: 0.931034505367279)
[2024-11-13 06:32:33,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:33,727][root][INFO] - Training Epoch: 4/10, step 494/574 completed (loss: 0.8359818458557129, acc: 0.8421052694320679)
[2024-11-13 06:32:33,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:34,190][root][INFO] - Training Epoch: 4/10, step 495/574 completed (loss: 0.15154880285263062, acc: 0.9473684430122375)
[2024-11-13 06:32:34,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:34,630][root][INFO] - Training Epoch: 4/10, step 496/574 completed (loss: 0.5922900438308716, acc: 0.8482142686843872)
[2024-11-13 06:32:34,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:35,131][root][INFO] - Training Epoch: 4/10, step 497/574 completed (loss: 0.32036852836608887, acc: 0.9213483333587646)
[2024-11-13 06:32:35,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:35,634][root][INFO] - Training Epoch: 4/10, step 498/574 completed (loss: 0.793576717376709, acc: 0.7752808928489685)
[2024-11-13 06:32:35,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:36,104][root][INFO] - Training Epoch: 4/10, step 499/574 completed (loss: 1.1579623222351074, acc: 0.7021276354789734)
[2024-11-13 06:32:36,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:36,558][root][INFO] - Training Epoch: 4/10, step 500/574 completed (loss: 0.909697413444519, acc: 0.739130437374115)
[2024-11-13 06:32:36,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:36,895][root][INFO] - Training Epoch: 4/10, step 501/574 completed (loss: 0.005016350653022528, acc: 1.0)
[2024-11-13 06:32:37,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:37,240][root][INFO] - Training Epoch: 4/10, step 502/574 completed (loss: 0.08247383683919907, acc: 0.9615384340286255)
[2024-11-13 06:32:37,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:37,612][root][INFO] - Training Epoch: 4/10, step 503/574 completed (loss: 0.16862370073795319, acc: 0.9259259104728699)
[2024-11-13 06:32:37,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:37,957][root][INFO] - Training Epoch: 4/10, step 504/574 completed (loss: 0.07626110315322876, acc: 1.0)
[2024-11-13 06:32:38,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:38,305][root][INFO] - Training Epoch: 4/10, step 505/574 completed (loss: 0.5413401126861572, acc: 0.8867924809455872)
[2024-11-13 06:32:38,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:38,651][root][INFO] - Training Epoch: 4/10, step 506/574 completed (loss: 0.5079426169395447, acc: 0.8620689511299133)
[2024-11-13 06:32:39,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:39,636][root][INFO] - Training Epoch: 4/10, step 507/574 completed (loss: 1.1326051950454712, acc: 0.684684693813324)
[2024-11-13 06:32:39,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:40,267][root][INFO] - Training Epoch: 4/10, step 508/574 completed (loss: 0.821475088596344, acc: 0.7887324094772339)
[2024-11-13 06:32:40,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:40,631][root][INFO] - Training Epoch: 4/10, step 509/574 completed (loss: 0.0668054074048996, acc: 1.0)
[2024-11-13 06:32:40,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:41,080][root][INFO] - Training Epoch: 4/10, step 510/574 completed (loss: 0.07304511219263077, acc: 1.0)
[2024-11-13 06:32:41,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:41,453][root][INFO] - Training Epoch: 4/10, step 511/574 completed (loss: 0.10913705080747604, acc: 0.9615384340286255)
[2024-11-13 06:32:43,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:45,130][root][INFO] - Training Epoch: 4/10, step 512/574 completed (loss: 0.9763766527175903, acc: 0.7357142567634583)
[2024-11-13 06:32:45,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:46,343][root][INFO] - Training Epoch: 4/10, step 513/574 completed (loss: 0.46113482117652893, acc: 0.8492063283920288)
[2024-11-13 06:32:46,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:46,697][root][INFO] - Training Epoch: 4/10, step 514/574 completed (loss: 0.3972131609916687, acc: 0.8928571343421936)
[2024-11-13 06:32:46,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:47,057][root][INFO] - Training Epoch: 4/10, step 515/574 completed (loss: 0.10503409057855606, acc: 0.9666666388511658)
[2024-11-13 06:32:47,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:48,123][root][INFO] - Training Epoch: 4/10, step 516/574 completed (loss: 0.6248162984848022, acc: 0.8333333134651184)
[2024-11-13 06:32:48,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:48,587][root][INFO] - Training Epoch: 4/10, step 517/574 completed (loss: 0.017555709928274155, acc: 1.0)
[2024-11-13 06:32:48,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:49,001][root][INFO] - Training Epoch: 4/10, step 518/574 completed (loss: 0.10844439268112183, acc: 0.9354838728904724)
[2024-11-13 06:32:49,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:49,417][root][INFO] - Training Epoch: 4/10, step 519/574 completed (loss: 0.13562175631523132, acc: 1.0)
[2024-11-13 06:32:49,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:49,869][root][INFO] - Training Epoch: 4/10, step 520/574 completed (loss: 0.2835957109928131, acc: 0.9259259104728699)
[2024-11-13 06:32:50,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:51,399][root][INFO] - Training Epoch: 4/10, step 521/574 completed (loss: 0.6944968700408936, acc: 0.8093220591545105)
[2024-11-13 06:32:51,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:51,879][root][INFO] - Training Epoch: 4/10, step 522/574 completed (loss: 0.3134304881095886, acc: 0.9328358173370361)
[2024-11-13 06:32:52,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:52,328][root][INFO] - Training Epoch: 4/10, step 523/574 completed (loss: 0.32638460397720337, acc: 0.8978102207183838)
[2024-11-13 06:32:52,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:53,159][root][INFO] - Training Epoch: 4/10, step 524/574 completed (loss: 0.6694368720054626, acc: 0.8149999976158142)
[2024-11-13 06:32:53,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:53,524][root][INFO] - Training Epoch: 4/10, step 525/574 completed (loss: 0.05355454981327057, acc: 1.0)
[2024-11-13 06:32:53,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:53,923][root][INFO] - Training Epoch: 4/10, step 526/574 completed (loss: 0.20762580633163452, acc: 0.9615384340286255)
[2024-11-13 06:32:54,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:54,287][root][INFO] - Training Epoch: 4/10, step 527/574 completed (loss: 0.09130102396011353, acc: 1.0)
[2024-11-13 06:32:54,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:54,660][root][INFO] - Training Epoch: 4/10, step 528/574 completed (loss: 1.237892508506775, acc: 0.5737704634666443)
[2024-11-13 06:32:54,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:55,014][root][INFO] - Training Epoch: 4/10, step 529/574 completed (loss: 0.29651153087615967, acc: 0.8983050584793091)
[2024-11-13 06:32:55,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:55,349][root][INFO] - Training Epoch: 4/10, step 530/574 completed (loss: 1.108093500137329, acc: 0.6744186282157898)
[2024-11-13 06:32:55,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:55,718][root][INFO] - Training Epoch: 4/10, step 531/574 completed (loss: 0.669192373752594, acc: 0.8181818127632141)
[2024-11-13 06:32:55,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:56,173][root][INFO] - Training Epoch: 4/10, step 532/574 completed (loss: 1.0260040760040283, acc: 0.7547169923782349)
[2024-11-13 06:32:56,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:56,568][root][INFO] - Training Epoch: 4/10, step 533/574 completed (loss: 0.3931982219219208, acc: 0.8409090638160706)
[2024-11-13 06:32:56,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:56,931][root][INFO] - Training Epoch: 4/10, step 534/574 completed (loss: 0.37098410725593567, acc: 0.9200000166893005)
[2024-11-13 06:32:57,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:57,334][root][INFO] - Training Epoch: 4/10, step 535/574 completed (loss: 0.17767837643623352, acc: 0.8999999761581421)
[2024-11-13 06:32:57,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:57,673][root][INFO] - Training Epoch: 4/10, step 536/574 completed (loss: 0.12382611632347107, acc: 0.9545454382896423)
[2024-11-13 06:32:57,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:58,214][root][INFO] - Training Epoch: 4/10, step 537/574 completed (loss: 0.5351151823997498, acc: 0.8769230842590332)
[2024-11-13 06:32:58,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:58,663][root][INFO] - Training Epoch: 4/10, step 538/574 completed (loss: 0.37963223457336426, acc: 0.921875)
[2024-11-13 06:32:58,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:59,207][root][INFO] - Training Epoch: 4/10, step 539/574 completed (loss: 0.43244469165802, acc: 0.84375)
[2024-11-13 06:32:59,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:59,578][root][INFO] - Training Epoch: 4/10, step 540/574 completed (loss: 0.6067582964897156, acc: 0.7878788113594055)
[2024-11-13 06:32:59,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:32:59,935][root][INFO] - Training Epoch: 4/10, step 541/574 completed (loss: 0.0457439087331295, acc: 1.0)
[2024-11-13 06:33:00,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:00,281][root][INFO] - Training Epoch: 4/10, step 542/574 completed (loss: 0.07332224398851395, acc: 1.0)
[2024-11-13 06:33:00,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:00,629][root][INFO] - Training Epoch: 4/10, step 543/574 completed (loss: 0.09527482092380524, acc: 0.95652174949646)
[2024-11-13 06:33:00,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:00,981][root][INFO] - Training Epoch: 4/10, step 544/574 completed (loss: 0.10464683175086975, acc: 0.9666666388511658)
[2024-11-13 06:33:01,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:01,357][root][INFO] - Training Epoch: 4/10, step 545/574 completed (loss: 0.15706126391887665, acc: 0.9512194991111755)
[2024-11-13 06:33:01,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:01,744][root][INFO] - Training Epoch: 4/10, step 546/574 completed (loss: 0.03878774493932724, acc: 0.9714285731315613)
[2024-11-13 06:33:01,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:02,129][root][INFO] - Training Epoch: 4/10, step 547/574 completed (loss: 0.047833144664764404, acc: 0.9736841917037964)
[2024-11-13 06:33:02,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:02,501][root][INFO] - Training Epoch: 4/10, step 548/574 completed (loss: 0.08711932599544525, acc: 0.9677419066429138)
[2024-11-13 06:33:02,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:02,854][root][INFO] - Training Epoch: 4/10, step 549/574 completed (loss: 0.016502968966960907, acc: 1.0)
[2024-11-13 06:33:02,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:03,196][root][INFO] - Training Epoch: 4/10, step 550/574 completed (loss: 0.3182685971260071, acc: 0.9090909361839294)
[2024-11-13 06:33:03,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:03,551][root][INFO] - Training Epoch: 4/10, step 551/574 completed (loss: 0.037042759358882904, acc: 1.0)
[2024-11-13 06:33:03,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:03,906][root][INFO] - Training Epoch: 4/10, step 552/574 completed (loss: 0.10312864929437637, acc: 0.9714285731315613)
[2024-11-13 06:33:04,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:04,266][root][INFO] - Training Epoch: 4/10, step 553/574 completed (loss: 0.4196021258831024, acc: 0.8832116723060608)
[2024-11-13 06:33:04,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:04,699][root][INFO] - Training Epoch: 4/10, step 554/574 completed (loss: 0.252983421087265, acc: 0.9241379499435425)
[2024-11-13 06:33:04,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:05,088][root][INFO] - Training Epoch: 4/10, step 555/574 completed (loss: 0.42156359553337097, acc: 0.8999999761581421)
[2024-11-13 06:33:05,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:05,458][root][INFO] - Training Epoch: 4/10, step 556/574 completed (loss: 0.43314269185066223, acc: 0.887417197227478)
[2024-11-13 06:33:05,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:05,837][root][INFO] - Training Epoch: 4/10, step 557/574 completed (loss: 0.16259519755840302, acc: 0.9487179517745972)
[2024-11-13 06:33:05,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:06,211][root][INFO] - Training Epoch: 4/10, step 558/574 completed (loss: 0.11392968893051147, acc: 0.9200000166893005)
[2024-11-13 06:33:06,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:06,584][root][INFO] - Training Epoch: 4/10, step 559/574 completed (loss: 0.39975228905677795, acc: 0.8461538553237915)
[2024-11-13 06:33:06,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:06,980][root][INFO] - Training Epoch: 4/10, step 560/574 completed (loss: 0.0218562800437212, acc: 1.0)
[2024-11-13 06:33:07,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:07,357][root][INFO] - Training Epoch: 4/10, step 561/574 completed (loss: 0.3987385332584381, acc: 0.9230769276618958)
[2024-11-13 06:33:07,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:07,740][root][INFO] - Training Epoch: 4/10, step 562/574 completed (loss: 0.30805501341819763, acc: 0.8888888955116272)
[2024-11-13 06:33:07,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:08,092][root][INFO] - Training Epoch: 4/10, step 563/574 completed (loss: 0.2962590157985687, acc: 0.8831169009208679)
[2024-11-13 06:33:08,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:08,432][root][INFO] - Training Epoch: 4/10, step 564/574 completed (loss: 0.2631339430809021, acc: 0.9166666865348816)
[2024-11-13 06:33:08,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:08,786][root][INFO] - Training Epoch: 4/10, step 565/574 completed (loss: 0.2725130021572113, acc: 0.9137930870056152)
[2024-11-13 06:33:09,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:10,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:10,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:11,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:11,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:12,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:12,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:13,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:13,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:14,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:14,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:15,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:15,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:16,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:16,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:17,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:17,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:18,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:18,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:19,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:19,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:20,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:20,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:21,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:21,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:22,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:22,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:23,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:23,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:24,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:24,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:25,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:25,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:25,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:26,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:26,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:27,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:27,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:28,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:28,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:29,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:29,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:30,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:30,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:31,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:31,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:32,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:32,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:33,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:33,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:33,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:34,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:34,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:35,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:35,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:36,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:36,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:37,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:37,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:38,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:38,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:39,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:40,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:40,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:40,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:41,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:41,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:42,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:43,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:43,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:44,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:44,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:44,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:45,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:45,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:46,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:46,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:47,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:47,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:48,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:48,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:49,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:49,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:50,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:50,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:51,384][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2915, device='cuda:0') eval_epoch_loss=tensor(0.8292, device='cuda:0') eval_epoch_acc=tensor(0.7949, device='cuda:0')
[2024-11-13 06:33:51,385][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:33:51,385][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:33:51,843][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_566_loss_0.8292079567909241/model.pt
[2024-11-13 06:33:51,854][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:33:51,854][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.7949268221855164
[2024-11-13 06:33:52,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:52,415][root][INFO] - Training Epoch: 4/10, step 566/574 completed (loss: 0.30703598260879517, acc: 0.9285714030265808)
[2024-11-13 06:33:52,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:52,874][root][INFO] - Training Epoch: 4/10, step 567/574 completed (loss: 0.028721466660499573, acc: 1.0)
[2024-11-13 06:33:53,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:53,281][root][INFO] - Training Epoch: 4/10, step 568/574 completed (loss: 0.07783287018537521, acc: 0.9629629850387573)
[2024-11-13 06:33:53,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:53,776][root][INFO] - Training Epoch: 4/10, step 569/574 completed (loss: 0.30142155289649963, acc: 0.9144384860992432)
[2024-11-13 06:33:53,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:54,216][root][INFO] - Training Epoch: 4/10, step 570/574 completed (loss: 0.08563336730003357, acc: 0.9516128897666931)
[2024-11-13 06:33:54,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:54,620][root][INFO] - Training Epoch: 4/10, step 571/574 completed (loss: 0.3987342119216919, acc: 0.9145299196243286)
[2024-11-13 06:33:54,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:54,975][root][INFO] - Training Epoch: 4/10, step 572/574 completed (loss: 0.7183611392974854, acc: 0.8367347121238708)
[2024-11-13 06:33:55,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:55,398][root][INFO] - Training Epoch: 4/10, step 573/574 completed (loss: 0.5427114367485046, acc: 0.8679245114326477)
[2024-11-13 06:33:55,813][slam_llm.utils.train_utils][INFO] - Epoch 4: train_perplexity=1.5721, train_epoch_loss=0.4524, epoch time 461.42843899317086s
[2024-11-13 06:33:55,813][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2024-11-13 06:33:55,813][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 16 GB
[2024-11-13 06:33:55,813][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2024-11-13 06:33:55,814][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 12
[2024-11-13 06:33:55,814][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-13 06:33:56,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:56,838][root][INFO] - Training Epoch: 5/10, step 0/574 completed (loss: 0.24908094108104706, acc: 0.8888888955116272)
[2024-11-13 06:33:56,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:57,175][root][INFO] - Training Epoch: 5/10, step 1/574 completed (loss: 0.2445535510778427, acc: 0.8799999952316284)
[2024-11-13 06:33:57,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:57,561][root][INFO] - Training Epoch: 5/10, step 2/574 completed (loss: 0.6366366744041443, acc: 0.8648648858070374)
[2024-11-13 06:33:57,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:57,991][root][INFO] - Training Epoch: 5/10, step 3/574 completed (loss: 0.3028888404369354, acc: 0.9210526347160339)
[2024-11-13 06:33:58,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:58,436][root][INFO] - Training Epoch: 5/10, step 4/574 completed (loss: 0.16777457296848297, acc: 0.9729729890823364)
[2024-11-13 06:33:58,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:58,822][root][INFO] - Training Epoch: 5/10, step 5/574 completed (loss: 0.05325987562537193, acc: 1.0)
[2024-11-13 06:33:59,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:59,283][root][INFO] - Training Epoch: 5/10, step 6/574 completed (loss: 0.5465137958526611, acc: 0.8163265585899353)
[2024-11-13 06:33:59,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:33:59,716][root][INFO] - Training Epoch: 5/10, step 7/574 completed (loss: 0.38554298877716064, acc: 0.9666666388511658)
[2024-11-13 06:33:59,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:00,111][root][INFO] - Training Epoch: 5/10, step 8/574 completed (loss: 0.11513517051935196, acc: 0.9545454382896423)
[2024-11-13 06:34:00,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:00,548][root][INFO] - Training Epoch: 5/10, step 9/574 completed (loss: 0.026732642203569412, acc: 1.0)
[2024-11-13 06:34:00,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:00,998][root][INFO] - Training Epoch: 5/10, step 10/574 completed (loss: 0.3215024173259735, acc: 0.9259259104728699)
[2024-11-13 06:34:01,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:01,470][root][INFO] - Training Epoch: 5/10, step 11/574 completed (loss: 0.09870607405900955, acc: 0.9487179517745972)
[2024-11-13 06:34:01,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:01,862][root][INFO] - Training Epoch: 5/10, step 12/574 completed (loss: 0.022260818630456924, acc: 1.0)
[2024-11-13 06:34:02,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:02,258][root][INFO] - Training Epoch: 5/10, step 13/574 completed (loss: 0.30595263838768005, acc: 0.8695651888847351)
[2024-11-13 06:34:02,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:02,687][root][INFO] - Training Epoch: 5/10, step 14/574 completed (loss: 0.11824411153793335, acc: 0.9607843160629272)
[2024-11-13 06:34:02,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:03,122][root][INFO] - Training Epoch: 5/10, step 15/574 completed (loss: 0.5038809776306152, acc: 0.8979591727256775)
[2024-11-13 06:34:03,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:03,546][root][INFO] - Training Epoch: 5/10, step 16/574 completed (loss: 0.1652308851480484, acc: 0.8947368264198303)
[2024-11-13 06:34:03,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:04,000][root][INFO] - Training Epoch: 5/10, step 17/574 completed (loss: 0.5992448329925537, acc: 0.875)
[2024-11-13 06:34:04,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:04,413][root][INFO] - Training Epoch: 5/10, step 18/574 completed (loss: 0.34990209341049194, acc: 0.9166666865348816)
[2024-11-13 06:34:04,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:04,772][root][INFO] - Training Epoch: 5/10, step 19/574 completed (loss: 0.20537680387496948, acc: 0.8947368264198303)
[2024-11-13 06:34:04,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:05,217][root][INFO] - Training Epoch: 5/10, step 20/574 completed (loss: 0.1320449411869049, acc: 0.9230769276618958)
[2024-11-13 06:34:05,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:05,620][root][INFO] - Training Epoch: 5/10, step 21/574 completed (loss: 0.09970840066671371, acc: 1.0)
[2024-11-13 06:34:05,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:06,014][root][INFO] - Training Epoch: 5/10, step 22/574 completed (loss: 0.14353303611278534, acc: 0.9200000166893005)
[2024-11-13 06:34:06,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:06,328][root][INFO] - Training Epoch: 5/10, step 23/574 completed (loss: 0.2002028226852417, acc: 0.9523809552192688)
[2024-11-13 06:34:06,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:06,806][root][INFO] - Training Epoch: 5/10, step 24/574 completed (loss: 0.031952813267707825, acc: 1.0)
[2024-11-13 06:34:07,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:07,257][root][INFO] - Training Epoch: 5/10, step 25/574 completed (loss: 0.39517614245414734, acc: 0.8679245114326477)
[2024-11-13 06:34:07,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:07,714][root][INFO] - Training Epoch: 5/10, step 26/574 completed (loss: 0.6801602840423584, acc: 0.7945205569267273)
[2024-11-13 06:34:08,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:09,586][root][INFO] - Training Epoch: 5/10, step 27/574 completed (loss: 0.9700157642364502, acc: 0.7154150009155273)
[2024-11-13 06:34:09,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:10,012][root][INFO] - Training Epoch: 5/10, step 28/574 completed (loss: 0.503875732421875, acc: 0.8604651093482971)
[2024-11-13 06:34:10,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:10,568][root][INFO] - Training Epoch: 5/10, step 29/574 completed (loss: 0.6073036193847656, acc: 0.8072289228439331)
[2024-11-13 06:34:10,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:11,041][root][INFO] - Training Epoch: 5/10, step 30/574 completed (loss: 0.658292829990387, acc: 0.790123462677002)
[2024-11-13 06:34:11,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:11,486][root][INFO] - Training Epoch: 5/10, step 31/574 completed (loss: 0.23591195046901703, acc: 0.8928571343421936)
[2024-11-13 06:34:11,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:11,921][root][INFO] - Training Epoch: 5/10, step 32/574 completed (loss: 0.05397268012166023, acc: 1.0)
[2024-11-13 06:34:12,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:12,334][root][INFO] - Training Epoch: 5/10, step 33/574 completed (loss: 0.06027935817837715, acc: 1.0)
[2024-11-13 06:34:12,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:12,795][root][INFO] - Training Epoch: 5/10, step 34/574 completed (loss: 0.4404151439666748, acc: 0.8655462265014648)
[2024-11-13 06:34:12,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:13,289][root][INFO] - Training Epoch: 5/10, step 35/574 completed (loss: 0.2622164189815521, acc: 0.9344262480735779)
[2024-11-13 06:34:13,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:13,759][root][INFO] - Training Epoch: 5/10, step 36/574 completed (loss: 0.30572280287742615, acc: 0.9047619104385376)
[2024-11-13 06:34:13,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:14,166][root][INFO] - Training Epoch: 5/10, step 37/574 completed (loss: 0.25205811858177185, acc: 0.9152542352676392)
[2024-11-13 06:34:14,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:14,594][root][INFO] - Training Epoch: 5/10, step 38/574 completed (loss: 0.24537654221057892, acc: 0.931034505367279)
[2024-11-13 06:34:14,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:14,961][root][INFO] - Training Epoch: 5/10, step 39/574 completed (loss: 0.08455211669206619, acc: 1.0)
[2024-11-13 06:34:15,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:15,382][root][INFO] - Training Epoch: 5/10, step 40/574 completed (loss: 0.37393543124198914, acc: 0.8461538553237915)
[2024-11-13 06:34:15,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:15,941][root][INFO] - Training Epoch: 5/10, step 41/574 completed (loss: 0.2884748578071594, acc: 0.9054054021835327)
[2024-11-13 06:34:16,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:16,404][root][INFO] - Training Epoch: 5/10, step 42/574 completed (loss: 0.8073099255561829, acc: 0.7230769395828247)
[2024-11-13 06:34:16,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:16,961][root][INFO] - Training Epoch: 5/10, step 43/574 completed (loss: 0.608428955078125, acc: 0.8282828330993652)
[2024-11-13 06:34:17,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:17,533][root][INFO] - Training Epoch: 5/10, step 44/574 completed (loss: 0.33026155829429626, acc: 0.8659793734550476)
[2024-11-13 06:34:17,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:18,074][root][INFO] - Training Epoch: 5/10, step 45/574 completed (loss: 0.4880831241607666, acc: 0.875)
[2024-11-13 06:34:18,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:18,556][root][INFO] - Training Epoch: 5/10, step 46/574 completed (loss: 0.13198022544384003, acc: 0.9230769276618958)
[2024-11-13 06:34:18,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:18,963][root][INFO] - Training Epoch: 5/10, step 47/574 completed (loss: 0.15923000872135162, acc: 0.9259259104728699)
[2024-11-13 06:34:19,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:19,381][root][INFO] - Training Epoch: 5/10, step 48/574 completed (loss: 0.17504966259002686, acc: 0.9642857313156128)
[2024-11-13 06:34:19,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:19,831][root][INFO] - Training Epoch: 5/10, step 49/574 completed (loss: 0.06751292943954468, acc: 0.9722222089767456)
[2024-11-13 06:34:20,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:20,238][root][INFO] - Training Epoch: 5/10, step 50/574 completed (loss: 0.4597158133983612, acc: 0.8421052694320679)
[2024-11-13 06:34:20,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:20,674][root][INFO] - Training Epoch: 5/10, step 51/574 completed (loss: 0.678372859954834, acc: 0.7777777910232544)
[2024-11-13 06:34:20,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:21,080][root][INFO] - Training Epoch: 5/10, step 52/574 completed (loss: 0.6847080588340759, acc: 0.7605633735656738)
[2024-11-13 06:34:21,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:21,742][root][INFO] - Training Epoch: 5/10, step 53/574 completed (loss: 1.57651948928833, acc: 0.54666668176651)
[2024-11-13 06:34:21,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:22,116][root][INFO] - Training Epoch: 5/10, step 54/574 completed (loss: 0.5676919221878052, acc: 0.7837837934494019)
[2024-11-13 06:34:22,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:22,566][root][INFO] - Training Epoch: 5/10, step 55/574 completed (loss: 0.019942542538046837, acc: 1.0)
[2024-11-13 06:34:25,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:26,993][root][INFO] - Training Epoch: 5/10, step 56/574 completed (loss: 1.01759672164917, acc: 0.6791808605194092)
[2024-11-13 06:34:27,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:28,853][root][INFO] - Training Epoch: 5/10, step 57/574 completed (loss: 1.2337497472763062, acc: 0.6535947918891907)
[2024-11-13 06:34:29,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:29,783][root][INFO] - Training Epoch: 5/10, step 58/574 completed (loss: 0.8430761098861694, acc: 0.7670454382896423)
[2024-11-13 06:34:30,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:30,649][root][INFO] - Training Epoch: 5/10, step 59/574 completed (loss: 0.3794715702533722, acc: 0.8823529481887817)
[2024-11-13 06:34:31,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:31,485][root][INFO] - Training Epoch: 5/10, step 60/574 completed (loss: 0.8675870895385742, acc: 0.760869562625885)
[2024-11-13 06:34:31,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:32,065][root][INFO] - Training Epoch: 5/10, step 61/574 completed (loss: 0.6179831624031067, acc: 0.8500000238418579)
[2024-11-13 06:34:32,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:32,555][root][INFO] - Training Epoch: 5/10, step 62/574 completed (loss: 0.2774355113506317, acc: 0.9117646813392639)
[2024-11-13 06:34:32,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:32,995][root][INFO] - Training Epoch: 5/10, step 63/574 completed (loss: 0.20090359449386597, acc: 0.9722222089767456)
[2024-11-13 06:34:33,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:33,451][root][INFO] - Training Epoch: 5/10, step 64/574 completed (loss: 0.18653489649295807, acc: 0.9375)
[2024-11-13 06:34:33,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:33,850][root][INFO] - Training Epoch: 5/10, step 65/574 completed (loss: 0.0655282661318779, acc: 1.0)
[2024-11-13 06:34:34,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:34,271][root][INFO] - Training Epoch: 5/10, step 66/574 completed (loss: 0.5647691488265991, acc: 0.8571428656578064)
[2024-11-13 06:34:34,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:34,744][root][INFO] - Training Epoch: 5/10, step 67/574 completed (loss: 0.41049832105636597, acc: 0.8666666746139526)
[2024-11-13 06:34:34,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:35,147][root][INFO] - Training Epoch: 5/10, step 68/574 completed (loss: 0.05039254203438759, acc: 1.0)
[2024-11-13 06:34:35,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:35,564][root][INFO] - Training Epoch: 5/10, step 69/574 completed (loss: 0.3846759796142578, acc: 0.8333333134651184)
[2024-11-13 06:34:35,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:35,953][root][INFO] - Training Epoch: 5/10, step 70/574 completed (loss: 0.29583925008773804, acc: 0.8787878751754761)
[2024-11-13 06:34:36,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:36,376][root][INFO] - Training Epoch: 5/10, step 71/574 completed (loss: 1.0217481851577759, acc: 0.720588207244873)
[2024-11-13 06:34:36,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:36,811][root][INFO] - Training Epoch: 5/10, step 72/574 completed (loss: 0.8463752865791321, acc: 0.8015872836112976)
[2024-11-13 06:34:36,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:37,207][root][INFO] - Training Epoch: 5/10, step 73/574 completed (loss: 1.3267338275909424, acc: 0.6358974575996399)
[2024-11-13 06:34:37,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:37,555][root][INFO] - Training Epoch: 5/10, step 74/574 completed (loss: 0.9488940238952637, acc: 0.6938775777816772)
[2024-11-13 06:34:37,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:37,937][root][INFO] - Training Epoch: 5/10, step 75/574 completed (loss: 1.1160999536514282, acc: 0.7089552283287048)
[2024-11-13 06:34:38,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:38,429][root][INFO] - Training Epoch: 5/10, step 76/574 completed (loss: 1.39482843875885, acc: 0.5839415788650513)
[2024-11-13 06:34:38,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:38,836][root][INFO] - Training Epoch: 5/10, step 77/574 completed (loss: 0.041653845459222794, acc: 1.0)
[2024-11-13 06:34:39,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:39,239][root][INFO] - Training Epoch: 5/10, step 78/574 completed (loss: 0.028375117108225822, acc: 1.0)
[2024-11-13 06:34:39,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:39,593][root][INFO] - Training Epoch: 5/10, step 79/574 completed (loss: 0.08483736217021942, acc: 0.9696969985961914)
[2024-11-13 06:34:39,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:40,005][root][INFO] - Training Epoch: 5/10, step 80/574 completed (loss: 0.025263268500566483, acc: 1.0)
[2024-11-13 06:34:40,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:40,425][root][INFO] - Training Epoch: 5/10, step 81/574 completed (loss: 0.3474450707435608, acc: 0.8461538553237915)
[2024-11-13 06:34:40,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:40,848][root][INFO] - Training Epoch: 5/10, step 82/574 completed (loss: 0.8039855360984802, acc: 0.75)
[2024-11-13 06:34:41,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:41,246][root][INFO] - Training Epoch: 5/10, step 83/574 completed (loss: 0.21448658406734467, acc: 0.9375)
[2024-11-13 06:34:41,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:41,667][root][INFO] - Training Epoch: 5/10, step 84/574 completed (loss: 0.3782652020454407, acc: 0.8695651888847351)
[2024-11-13 06:34:41,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:42,057][root][INFO] - Training Epoch: 5/10, step 85/574 completed (loss: 0.3976594805717468, acc: 0.8600000143051147)
[2024-11-13 06:34:42,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:42,466][root][INFO] - Training Epoch: 5/10, step 86/574 completed (loss: 0.196576327085495, acc: 0.9130434989929199)
[2024-11-13 06:34:42,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:43,134][root][INFO] - Training Epoch: 5/10, step 87/574 completed (loss: 0.8599063754081726, acc: 0.7200000286102295)
[2024-11-13 06:34:43,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:43,679][root][INFO] - Training Epoch: 5/10, step 88/574 completed (loss: 0.9146948456764221, acc: 0.7572815418243408)
[2024-11-13 06:34:44,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:45,428][root][INFO] - Training Epoch: 5/10, step 89/574 completed (loss: 0.8829742074012756, acc: 0.7815533876419067)
[2024-11-13 06:34:46,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:46,709][root][INFO] - Training Epoch: 5/10, step 90/574 completed (loss: 1.116589903831482, acc: 0.6881720423698425)
[2024-11-13 06:34:47,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:47,961][root][INFO] - Training Epoch: 5/10, step 91/574 completed (loss: 0.9465426206588745, acc: 0.7456896305084229)
[2024-11-13 06:34:48,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:49,118][root][INFO] - Training Epoch: 5/10, step 92/574 completed (loss: 0.790897011756897, acc: 0.7789473533630371)
[2024-11-13 06:34:49,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:50,699][root][INFO] - Training Epoch: 5/10, step 93/574 completed (loss: 1.2906150817871094, acc: 0.6534653306007385)
[2024-11-13 06:34:50,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:51,141][root][INFO] - Training Epoch: 5/10, step 94/574 completed (loss: 0.9026313424110413, acc: 0.7580645084381104)
[2024-11-13 06:34:51,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:51,544][root][INFO] - Training Epoch: 5/10, step 95/574 completed (loss: 0.6727069616317749, acc: 0.782608687877655)
[2024-11-13 06:34:51,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:51,940][root][INFO] - Training Epoch: 5/10, step 96/574 completed (loss: 1.0890226364135742, acc: 0.6554622054100037)
[2024-11-13 06:34:52,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:52,438][root][INFO] - Training Epoch: 5/10, step 97/574 completed (loss: 0.8584671020507812, acc: 0.7115384340286255)
[2024-11-13 06:34:52,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:52,985][root][INFO] - Training Epoch: 5/10, step 98/574 completed (loss: 1.188477873802185, acc: 0.6569343209266663)
[2024-11-13 06:34:53,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:53,406][root][INFO] - Training Epoch: 5/10, step 99/574 completed (loss: 0.9564495086669922, acc: 0.7164179086685181)
[2024-11-13 06:34:53,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:53,872][root][INFO] - Training Epoch: 5/10, step 100/574 completed (loss: 0.11130718141794205, acc: 1.0)
[2024-11-13 06:34:54,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:54,327][root][INFO] - Training Epoch: 5/10, step 101/574 completed (loss: 0.0047054896131157875, acc: 1.0)
[2024-11-13 06:34:54,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:54,729][root][INFO] - Training Epoch: 5/10, step 102/574 completed (loss: 0.03486163169145584, acc: 1.0)
[2024-11-13 06:34:54,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:55,103][root][INFO] - Training Epoch: 5/10, step 103/574 completed (loss: 0.0603008009493351, acc: 0.9772727489471436)
[2024-11-13 06:34:55,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:55,540][root][INFO] - Training Epoch: 5/10, step 104/574 completed (loss: 0.30292072892189026, acc: 0.9137930870056152)
[2024-11-13 06:34:55,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:55,964][root][INFO] - Training Epoch: 5/10, step 105/574 completed (loss: 0.33262044191360474, acc: 0.9069767594337463)
[2024-11-13 06:34:56,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:56,377][root][INFO] - Training Epoch: 5/10, step 106/574 completed (loss: 0.18842242658138275, acc: 0.9200000166893005)
[2024-11-13 06:34:56,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:56,800][root][INFO] - Training Epoch: 5/10, step 107/574 completed (loss: 0.04482193663716316, acc: 1.0)
[2024-11-13 06:34:56,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:57,183][root][INFO] - Training Epoch: 5/10, step 108/574 completed (loss: 0.022694438695907593, acc: 1.0)
[2024-11-13 06:34:57,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:57,551][root][INFO] - Training Epoch: 5/10, step 109/574 completed (loss: 0.12795184552669525, acc: 0.976190447807312)
[2024-11-13 06:34:57,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:58,016][root][INFO] - Training Epoch: 5/10, step 110/574 completed (loss: 0.14585402607917786, acc: 0.9384615421295166)
[2024-11-13 06:34:58,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:58,566][root][INFO] - Training Epoch: 5/10, step 111/574 completed (loss: 0.3471331000328064, acc: 0.8771929740905762)
[2024-11-13 06:34:58,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:59,042][root][INFO] - Training Epoch: 5/10, step 112/574 completed (loss: 0.6412691473960876, acc: 0.7894737124443054)
[2024-11-13 06:34:59,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:59,454][root][INFO] - Training Epoch: 5/10, step 113/574 completed (loss: 0.36036545038223267, acc: 0.8974359035491943)
[2024-11-13 06:34:59,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:34:59,943][root][INFO] - Training Epoch: 5/10, step 114/574 completed (loss: 0.17365968227386475, acc: 0.9387755393981934)
[2024-11-13 06:35:00,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:00,372][root][INFO] - Training Epoch: 5/10, step 115/574 completed (loss: 0.03130820021033287, acc: 1.0)
[2024-11-13 06:35:00,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:00,789][root][INFO] - Training Epoch: 5/10, step 116/574 completed (loss: 0.4590771794319153, acc: 0.8888888955116272)
[2024-11-13 06:35:01,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:01,262][root][INFO] - Training Epoch: 5/10, step 117/574 completed (loss: 0.41263628005981445, acc: 0.869918704032898)
[2024-11-13 06:35:01,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:01,731][root][INFO] - Training Epoch: 5/10, step 118/574 completed (loss: 0.1234285905957222, acc: 0.9516128897666931)
[2024-11-13 06:35:02,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:03,033][root][INFO] - Training Epoch: 5/10, step 119/574 completed (loss: 0.8412081003189087, acc: 0.7566539645195007)
[2024-11-13 06:35:03,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:03,483][root][INFO] - Training Epoch: 5/10, step 120/574 completed (loss: 0.21174834668636322, acc: 0.9466666579246521)
[2024-11-13 06:35:03,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:04,041][root][INFO] - Training Epoch: 5/10, step 121/574 completed (loss: 0.2375175952911377, acc: 0.9230769276618958)
[2024-11-13 06:35:04,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:04,418][root][INFO] - Training Epoch: 5/10, step 122/574 completed (loss: 0.03343716636300087, acc: 1.0)
[2024-11-13 06:35:04,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:04,775][root][INFO] - Training Epoch: 5/10, step 123/574 completed (loss: 0.14876911044120789, acc: 0.8947368264198303)
[2024-11-13 06:35:04,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:05,214][root][INFO] - Training Epoch: 5/10, step 124/574 completed (loss: 0.8203692436218262, acc: 0.7300613522529602)
[2024-11-13 06:35:05,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:05,660][root][INFO] - Training Epoch: 5/10, step 125/574 completed (loss: 1.1382561922073364, acc: 0.6736111044883728)
[2024-11-13 06:35:05,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:06,123][root][INFO] - Training Epoch: 5/10, step 126/574 completed (loss: 1.2143089771270752, acc: 0.6416666507720947)
[2024-11-13 06:35:06,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:06,541][root][INFO] - Training Epoch: 5/10, step 127/574 completed (loss: 0.9006427526473999, acc: 0.761904776096344)
[2024-11-13 06:35:06,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:06,989][root][INFO] - Training Epoch: 5/10, step 128/574 completed (loss: 0.900778591632843, acc: 0.728205144405365)
[2024-11-13 06:35:07,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:07,533][root][INFO] - Training Epoch: 5/10, step 129/574 completed (loss: 0.9514102935791016, acc: 0.7352941036224365)
[2024-11-13 06:35:07,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:07,957][root][INFO] - Training Epoch: 5/10, step 130/574 completed (loss: 0.4935075044631958, acc: 0.807692289352417)
[2024-11-13 06:35:08,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:08,373][root][INFO] - Training Epoch: 5/10, step 131/574 completed (loss: 0.20131824910640717, acc: 0.9130434989929199)
[2024-11-13 06:35:08,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:08,777][root][INFO] - Training Epoch: 5/10, step 132/574 completed (loss: 0.26094409823417664, acc: 0.84375)
[2024-11-13 06:35:08,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:09,189][root][INFO] - Training Epoch: 5/10, step 133/574 completed (loss: 0.18624071776866913, acc: 0.95652174949646)
[2024-11-13 06:35:09,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:09,603][root][INFO] - Training Epoch: 5/10, step 134/574 completed (loss: 0.24483910202980042, acc: 0.9428571462631226)
[2024-11-13 06:35:10,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:11,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:11,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:12,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:12,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:13,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:13,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:14,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:14,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:15,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:15,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:16,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:16,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:17,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:17,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:18,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:18,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:19,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:19,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:20,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:20,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:21,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:21,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:22,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:22,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:23,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:23,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:24,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:24,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:25,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:25,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:26,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:26,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:27,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:27,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:28,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:28,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:28,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:29,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:29,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:30,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:30,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:31,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:31,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:32,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:32,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:33,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:33,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:34,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:34,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:35,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:35,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:36,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:36,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:36,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:37,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:37,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:38,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:38,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:39,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:39,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:40,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:41,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:41,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:42,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:42,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:42,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:43,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:44,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:44,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:45,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:45,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:46,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:46,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:47,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:47,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:48,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:48,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:49,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:49,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:50,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:50,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:51,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:51,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:52,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:52,820][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2653, device='cuda:0') eval_epoch_loss=tensor(0.8177, device='cuda:0') eval_epoch_acc=tensor(0.7898, device='cuda:0')
[2024-11-13 06:35:52,822][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:35:52,822][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:35:53,205][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_5_step_135_loss_0.8177047967910767/model.pt
[2024-11-13 06:35:53,211][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:35:53,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:53,668][root][INFO] - Training Epoch: 5/10, step 135/574 completed (loss: 0.2537524700164795, acc: 0.9230769276618958)
[2024-11-13 06:35:53,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:54,035][root][INFO] - Training Epoch: 5/10, step 136/574 completed (loss: 0.4947020709514618, acc: 0.8571428656578064)
[2024-11-13 06:35:54,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:54,451][root][INFO] - Training Epoch: 5/10, step 137/574 completed (loss: 0.7228854894638062, acc: 0.7666666507720947)
[2024-11-13 06:35:54,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:54,843][root][INFO] - Training Epoch: 5/10, step 138/574 completed (loss: 0.12706153094768524, acc: 0.95652174949646)
[2024-11-13 06:35:54,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:55,177][root][INFO] - Training Epoch: 5/10, step 139/574 completed (loss: 0.03655247017741203, acc: 1.0)
[2024-11-13 06:35:55,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:55,530][root][INFO] - Training Epoch: 5/10, step 140/574 completed (loss: 0.11524699628353119, acc: 0.9615384340286255)
[2024-11-13 06:35:55,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:55,956][root][INFO] - Training Epoch: 5/10, step 141/574 completed (loss: 0.32322612404823303, acc: 0.8709677457809448)
[2024-11-13 06:35:56,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:56,341][root][INFO] - Training Epoch: 5/10, step 142/574 completed (loss: 0.4460843503475189, acc: 0.9459459185600281)
[2024-11-13 06:35:56,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:57,115][root][INFO] - Training Epoch: 5/10, step 143/574 completed (loss: 0.6706796884536743, acc: 0.7631579041481018)
[2024-11-13 06:35:57,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:57,567][root][INFO] - Training Epoch: 5/10, step 144/574 completed (loss: 0.7892327308654785, acc: 0.7985074520111084)
[2024-11-13 06:35:57,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:58,030][root][INFO] - Training Epoch: 5/10, step 145/574 completed (loss: 0.43931257724761963, acc: 0.8571428656578064)
[2024-11-13 06:35:58,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:58,666][root][INFO] - Training Epoch: 5/10, step 146/574 completed (loss: 0.8446263074874878, acc: 0.6808510422706604)
[2024-11-13 06:35:58,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:59,038][root][INFO] - Training Epoch: 5/10, step 147/574 completed (loss: 0.5087650418281555, acc: 0.8285714387893677)
[2024-11-13 06:35:59,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:59,388][root][INFO] - Training Epoch: 5/10, step 148/574 completed (loss: 0.6016375422477722, acc: 0.8214285969734192)
[2024-11-13 06:35:59,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:35:59,767][root][INFO] - Training Epoch: 5/10, step 149/574 completed (loss: 0.6287451386451721, acc: 0.782608687877655)
[2024-11-13 06:35:59,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:00,113][root][INFO] - Training Epoch: 5/10, step 150/574 completed (loss: 0.7022414803504944, acc: 0.7241379022598267)
[2024-11-13 06:36:00,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:00,473][root][INFO] - Training Epoch: 5/10, step 151/574 completed (loss: 0.7152864336967468, acc: 0.8478260636329651)
[2024-11-13 06:36:00,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:00,865][root][INFO] - Training Epoch: 5/10, step 152/574 completed (loss: 0.5138856768608093, acc: 0.8983050584793091)
[2024-11-13 06:36:01,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:01,186][root][INFO] - Training Epoch: 5/10, step 153/574 completed (loss: 0.5393136739730835, acc: 0.859649121761322)
[2024-11-13 06:36:01,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:01,564][root][INFO] - Training Epoch: 5/10, step 154/574 completed (loss: 0.4273941218852997, acc: 0.8918918967247009)
[2024-11-13 06:36:01,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:01,961][root][INFO] - Training Epoch: 5/10, step 155/574 completed (loss: 0.040993280708789825, acc: 1.0)
[2024-11-13 06:36:02,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:02,336][root][INFO] - Training Epoch: 5/10, step 156/574 completed (loss: 0.12545454502105713, acc: 1.0)
[2024-11-13 06:36:02,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:02,676][root][INFO] - Training Epoch: 5/10, step 157/574 completed (loss: 1.1760847568511963, acc: 0.6842105388641357)
[2024-11-13 06:36:04,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:05,294][root][INFO] - Training Epoch: 5/10, step 158/574 completed (loss: 0.9307296276092529, acc: 0.7297297120094299)
[2024-11-13 06:36:05,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:05,652][root][INFO] - Training Epoch: 5/10, step 159/574 completed (loss: 1.6497642993927002, acc: 0.5370370149612427)
[2024-11-13 06:36:05,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:06,219][root][INFO] - Training Epoch: 5/10, step 160/574 completed (loss: 1.1660358905792236, acc: 0.6860465407371521)
[2024-11-13 06:36:06,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:07,131][root][INFO] - Training Epoch: 5/10, step 161/574 completed (loss: 1.0906751155853271, acc: 0.7176470756530762)
[2024-11-13 06:36:07,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:07,975][root][INFO] - Training Epoch: 5/10, step 162/574 completed (loss: 1.1410523653030396, acc: 0.7303370833396912)
[2024-11-13 06:36:08,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:08,352][root][INFO] - Training Epoch: 5/10, step 163/574 completed (loss: 0.3646228015422821, acc: 0.8863636255264282)
[2024-11-13 06:36:08,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:08,721][root][INFO] - Training Epoch: 5/10, step 164/574 completed (loss: 0.3889327645301819, acc: 0.9047619104385376)
[2024-11-13 06:36:08,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:09,115][root][INFO] - Training Epoch: 5/10, step 165/574 completed (loss: 1.0159728527069092, acc: 0.7241379022598267)
[2024-11-13 06:36:09,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:09,524][root][INFO] - Training Epoch: 5/10, step 166/574 completed (loss: 0.2939366400241852, acc: 0.8979591727256775)
[2024-11-13 06:36:09,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:09,877][root][INFO] - Training Epoch: 5/10, step 167/574 completed (loss: 0.27577245235443115, acc: 0.9200000166893005)
[2024-11-13 06:36:10,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:10,388][root][INFO] - Training Epoch: 5/10, step 168/574 completed (loss: 0.30803659558296204, acc: 0.9027777910232544)
[2024-11-13 06:36:10,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:10,778][root][INFO] - Training Epoch: 5/10, step 169/574 completed (loss: 0.9945758581161499, acc: 0.7254902124404907)
[2024-11-13 06:36:11,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:12,430][root][INFO] - Training Epoch: 5/10, step 170/574 completed (loss: 0.8765862584114075, acc: 0.7534246444702148)
[2024-11-13 06:36:12,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:12,828][root][INFO] - Training Epoch: 5/10, step 171/574 completed (loss: 0.3814837634563446, acc: 0.9166666865348816)
[2024-11-13 06:36:13,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:13,283][root][INFO] - Training Epoch: 5/10, step 172/574 completed (loss: 0.1788007616996765, acc: 0.9629629850387573)
[2024-11-13 06:36:13,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:13,673][root][INFO] - Training Epoch: 5/10, step 173/574 completed (loss: 0.2648041546344757, acc: 0.9285714030265808)
[2024-11-13 06:36:14,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:14,485][root][INFO] - Training Epoch: 5/10, step 174/574 completed (loss: 0.9361331462860107, acc: 0.7876105904579163)
[2024-11-13 06:36:14,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:14,868][root][INFO] - Training Epoch: 5/10, step 175/574 completed (loss: 0.7297838926315308, acc: 0.7971014380455017)
[2024-11-13 06:36:15,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:15,278][root][INFO] - Training Epoch: 5/10, step 176/574 completed (loss: 0.3499247133731842, acc: 0.875)
[2024-11-13 06:36:16,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:16,739][root][INFO] - Training Epoch: 5/10, step 177/574 completed (loss: 1.119836449623108, acc: 0.694656491279602)
[2024-11-13 06:36:17,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:17,774][root][INFO] - Training Epoch: 5/10, step 178/574 completed (loss: 0.7727931141853333, acc: 0.7851851582527161)
[2024-11-13 06:36:17,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:18,183][root][INFO] - Training Epoch: 5/10, step 179/574 completed (loss: 0.3792688548564911, acc: 0.9016393423080444)
[2024-11-13 06:36:18,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:18,668][root][INFO] - Training Epoch: 5/10, step 180/574 completed (loss: 0.014161169528961182, acc: 1.0)
[2024-11-13 06:36:18,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:19,078][root][INFO] - Training Epoch: 5/10, step 181/574 completed (loss: 0.14737734198570251, acc: 0.9200000166893005)
[2024-11-13 06:36:19,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:19,517][root][INFO] - Training Epoch: 5/10, step 182/574 completed (loss: 0.12328258901834488, acc: 0.9642857313156128)
[2024-11-13 06:36:19,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:19,991][root][INFO] - Training Epoch: 5/10, step 183/574 completed (loss: 0.17210084199905396, acc: 0.9634146094322205)
[2024-11-13 06:36:20,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:20,511][root][INFO] - Training Epoch: 5/10, step 184/574 completed (loss: 0.5337780714035034, acc: 0.870090663433075)
[2024-11-13 06:36:20,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:20,906][root][INFO] - Training Epoch: 5/10, step 185/574 completed (loss: 0.6859663128852844, acc: 0.7953890562057495)
[2024-11-13 06:36:21,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:21,589][root][INFO] - Training Epoch: 5/10, step 186/574 completed (loss: 0.6393654346466064, acc: 0.800000011920929)
[2024-11-13 06:36:21,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:22,322][root][INFO] - Training Epoch: 5/10, step 187/574 completed (loss: 0.6823464035987854, acc: 0.8048780560493469)
[2024-11-13 06:36:22,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:22,867][root][INFO] - Training Epoch: 5/10, step 188/574 completed (loss: 0.6064494848251343, acc: 0.8256227970123291)
[2024-11-13 06:36:23,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:23,284][root][INFO] - Training Epoch: 5/10, step 189/574 completed (loss: 0.22726386785507202, acc: 0.9200000166893005)
[2024-11-13 06:36:23,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:24,119][root][INFO] - Training Epoch: 5/10, step 190/574 completed (loss: 0.6103668808937073, acc: 0.8023256063461304)
[2024-11-13 06:36:24,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:25,389][root][INFO] - Training Epoch: 5/10, step 191/574 completed (loss: 1.1020625829696655, acc: 0.6507936716079712)
[2024-11-13 06:36:26,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:26,874][root][INFO] - Training Epoch: 5/10, step 192/574 completed (loss: 0.9667815566062927, acc: 0.7272727489471436)
[2024-11-13 06:36:27,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:28,053][root][INFO] - Training Epoch: 5/10, step 193/574 completed (loss: 0.5329053401947021, acc: 0.8352941274642944)
[2024-11-13 06:36:29,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:29,804][root][INFO] - Training Epoch: 5/10, step 194/574 completed (loss: 0.8880490064620972, acc: 0.7530864477157593)
[2024-11-13 06:36:30,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:31,350][root][INFO] - Training Epoch: 5/10, step 195/574 completed (loss: 0.22404853999614716, acc: 0.9193548560142517)
[2024-11-13 06:36:31,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:31,732][root][INFO] - Training Epoch: 5/10, step 196/574 completed (loss: 0.10834901034832001, acc: 0.9642857313156128)
[2024-11-13 06:36:31,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:32,180][root][INFO] - Training Epoch: 5/10, step 197/574 completed (loss: 0.23079907894134521, acc: 0.925000011920929)
[2024-11-13 06:36:32,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:32,629][root][INFO] - Training Epoch: 5/10, step 198/574 completed (loss: 0.46844619512557983, acc: 0.8382353186607361)
[2024-11-13 06:36:32,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:33,070][root][INFO] - Training Epoch: 5/10, step 199/574 completed (loss: 0.8202518224716187, acc: 0.779411792755127)
[2024-11-13 06:36:33,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:33,532][root][INFO] - Training Epoch: 5/10, step 200/574 completed (loss: 0.6412907838821411, acc: 0.7796609997749329)
[2024-11-13 06:36:33,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:33,953][root][INFO] - Training Epoch: 5/10, step 201/574 completed (loss: 0.80376136302948, acc: 0.7835820913314819)
[2024-11-13 06:36:34,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:34,403][root][INFO] - Training Epoch: 5/10, step 202/574 completed (loss: 0.6889194846153259, acc: 0.8252426981925964)
[2024-11-13 06:36:34,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:34,840][root][INFO] - Training Epoch: 5/10, step 203/574 completed (loss: 0.535036027431488, acc: 0.8571428656578064)
[2024-11-13 06:36:34,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:35,282][root][INFO] - Training Epoch: 5/10, step 204/574 completed (loss: 0.18465527892112732, acc: 0.9780219793319702)
[2024-11-13 06:36:35,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:35,698][root][INFO] - Training Epoch: 5/10, step 205/574 completed (loss: 0.3035386800765991, acc: 0.9103139042854309)
[2024-11-13 06:36:35,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:36,226][root][INFO] - Training Epoch: 5/10, step 206/574 completed (loss: 0.4259643852710724, acc: 0.8543307185173035)
[2024-11-13 06:36:36,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:36,690][root][INFO] - Training Epoch: 5/10, step 207/574 completed (loss: 0.41184380650520325, acc: 0.875)
[2024-11-13 06:36:36,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:37,174][root][INFO] - Training Epoch: 5/10, step 208/574 completed (loss: 0.43630117177963257, acc: 0.8659420013427734)
[2024-11-13 06:36:37,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:37,648][root][INFO] - Training Epoch: 5/10, step 209/574 completed (loss: 0.33719539642333984, acc: 0.8910505771636963)
[2024-11-13 06:36:37,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:38,099][root][INFO] - Training Epoch: 5/10, step 210/574 completed (loss: 0.2521781325340271, acc: 0.945652186870575)
[2024-11-13 06:36:38,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:38,537][root][INFO] - Training Epoch: 5/10, step 211/574 completed (loss: 0.0598893016576767, acc: 1.0)
[2024-11-13 06:36:38,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:38,973][root][INFO] - Training Epoch: 5/10, step 212/574 completed (loss: 0.03647313266992569, acc: 1.0)
[2024-11-13 06:36:39,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:39,426][root][INFO] - Training Epoch: 5/10, step 213/574 completed (loss: 0.18177379667758942, acc: 0.957446813583374)
[2024-11-13 06:36:39,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:40,469][root][INFO] - Training Epoch: 5/10, step 214/574 completed (loss: 0.16607283055782318, acc: 0.9538461565971375)
[2024-11-13 06:36:40,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:40,899][root][INFO] - Training Epoch: 5/10, step 215/574 completed (loss: 0.15797929465770721, acc: 0.9459459185600281)
[2024-11-13 06:36:41,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:41,329][root][INFO] - Training Epoch: 5/10, step 216/574 completed (loss: 0.08022738248109818, acc: 0.9767441749572754)
[2024-11-13 06:36:41,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:42,127][root][INFO] - Training Epoch: 5/10, step 217/574 completed (loss: 0.11669301241636276, acc: 0.9459459185600281)
[2024-11-13 06:36:42,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:42,651][root][INFO] - Training Epoch: 5/10, step 218/574 completed (loss: 0.12627728283405304, acc: 0.9555555582046509)
[2024-11-13 06:36:42,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:43,011][root][INFO] - Training Epoch: 5/10, step 219/574 completed (loss: 0.1522226780653, acc: 0.9696969985961914)
[2024-11-13 06:36:43,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:43,450][root][INFO] - Training Epoch: 5/10, step 220/574 completed (loss: 0.09500271081924438, acc: 0.9629629850387573)
[2024-11-13 06:36:43,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:43,843][root][INFO] - Training Epoch: 5/10, step 221/574 completed (loss: 0.03432697430253029, acc: 1.0)
[2024-11-13 06:36:44,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:44,284][root][INFO] - Training Epoch: 5/10, step 222/574 completed (loss: 0.5193356275558472, acc: 0.8461538553237915)
[2024-11-13 06:36:44,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:45,474][root][INFO] - Training Epoch: 5/10, step 223/574 completed (loss: 0.33868199586868286, acc: 0.89673912525177)
[2024-11-13 06:36:45,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:46,280][root][INFO] - Training Epoch: 5/10, step 224/574 completed (loss: 0.49355360865592957, acc: 0.8409090638160706)
[2024-11-13 06:36:46,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:46,895][root][INFO] - Training Epoch: 5/10, step 225/574 completed (loss: 0.6681983470916748, acc: 0.7872340679168701)
[2024-11-13 06:36:47,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:47,334][root][INFO] - Training Epoch: 5/10, step 226/574 completed (loss: 0.4237329363822937, acc: 0.8679245114326477)
[2024-11-13 06:36:47,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:47,749][root][INFO] - Training Epoch: 5/10, step 227/574 completed (loss: 0.15058733522891998, acc: 0.9666666388511658)
[2024-11-13 06:36:47,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:48,167][root][INFO] - Training Epoch: 5/10, step 228/574 completed (loss: 0.1588897705078125, acc: 0.9534883499145508)
[2024-11-13 06:36:48,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:48,567][root][INFO] - Training Epoch: 5/10, step 229/574 completed (loss: 0.42914876341819763, acc: 0.8999999761581421)
[2024-11-13 06:36:48,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:49,039][root][INFO] - Training Epoch: 5/10, step 230/574 completed (loss: 1.726393699645996, acc: 0.557894766330719)
[2024-11-13 06:36:49,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:49,463][root][INFO] - Training Epoch: 5/10, step 231/574 completed (loss: 1.3835508823394775, acc: 0.6000000238418579)
[2024-11-13 06:36:49,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:50,045][root][INFO] - Training Epoch: 5/10, step 232/574 completed (loss: 1.2657538652420044, acc: 0.6555555462837219)
[2024-11-13 06:36:50,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:50,754][root][INFO] - Training Epoch: 5/10, step 233/574 completed (loss: 1.6190561056137085, acc: 0.5779816508293152)
[2024-11-13 06:36:51,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:51,445][root][INFO] - Training Epoch: 5/10, step 234/574 completed (loss: 1.166396141052246, acc: 0.6692307591438293)
[2024-11-13 06:36:51,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:51,790][root][INFO] - Training Epoch: 5/10, step 235/574 completed (loss: 0.07715480029582977, acc: 0.9473684430122375)
[2024-11-13 06:36:51,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:52,135][root][INFO] - Training Epoch: 5/10, step 236/574 completed (loss: 0.07969311624765396, acc: 0.9583333134651184)
[2024-11-13 06:36:52,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:52,541][root][INFO] - Training Epoch: 5/10, step 237/574 completed (loss: 0.2753922641277313, acc: 0.9090909361839294)
[2024-11-13 06:36:52,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:52,956][root][INFO] - Training Epoch: 5/10, step 238/574 completed (loss: 0.5135298371315002, acc: 0.8888888955116272)
[2024-11-13 06:36:53,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:53,363][root][INFO] - Training Epoch: 5/10, step 239/574 completed (loss: 0.3880947232246399, acc: 0.8857142925262451)
[2024-11-13 06:36:53,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:53,786][root][INFO] - Training Epoch: 5/10, step 240/574 completed (loss: 0.626209020614624, acc: 0.8409090638160706)
[2024-11-13 06:36:53,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:54,191][root][INFO] - Training Epoch: 5/10, step 241/574 completed (loss: 0.48705416917800903, acc: 0.8863636255264282)
[2024-11-13 06:36:54,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:55,083][root][INFO] - Training Epoch: 5/10, step 242/574 completed (loss: 0.9472681283950806, acc: 0.7096773982048035)
[2024-11-13 06:36:55,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:55,893][root][INFO] - Training Epoch: 5/10, step 243/574 completed (loss: 0.6572365164756775, acc: 0.7954545617103577)
[2024-11-13 06:36:56,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:56,283][root][INFO] - Training Epoch: 5/10, step 244/574 completed (loss: 0.021215802058577538, acc: 1.0)
[2024-11-13 06:36:56,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:56,638][root][INFO] - Training Epoch: 5/10, step 245/574 completed (loss: 0.11980447173118591, acc: 0.9615384340286255)
[2024-11-13 06:36:56,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:56,987][root][INFO] - Training Epoch: 5/10, step 246/574 completed (loss: 0.1508423089981079, acc: 0.9677419066429138)
[2024-11-13 06:36:57,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:57,335][root][INFO] - Training Epoch: 5/10, step 247/574 completed (loss: 0.1688174456357956, acc: 0.8999999761581421)
[2024-11-13 06:36:57,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:57,778][root][INFO] - Training Epoch: 5/10, step 248/574 completed (loss: 0.142965629696846, acc: 0.9729729890823364)
[2024-11-13 06:36:57,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:58,133][root][INFO] - Training Epoch: 5/10, step 249/574 completed (loss: 0.2891107201576233, acc: 0.9189189076423645)
[2024-11-13 06:36:58,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:58,506][root][INFO] - Training Epoch: 5/10, step 250/574 completed (loss: 0.029882734641432762, acc: 1.0)
[2024-11-13 06:36:58,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:58,927][root][INFO] - Training Epoch: 5/10, step 251/574 completed (loss: 0.19835665822029114, acc: 0.9558823704719543)
[2024-11-13 06:36:59,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:59,327][root][INFO] - Training Epoch: 5/10, step 252/574 completed (loss: 0.14582814276218414, acc: 0.9512194991111755)
[2024-11-13 06:36:59,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:36:59,679][root][INFO] - Training Epoch: 5/10, step 253/574 completed (loss: 0.051398228853940964, acc: 0.9599999785423279)
[2024-11-13 06:36:59,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:00,034][root][INFO] - Training Epoch: 5/10, step 254/574 completed (loss: 0.007269618101418018, acc: 1.0)
[2024-11-13 06:37:00,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:00,385][root][INFO] - Training Epoch: 5/10, step 255/574 completed (loss: 0.03918752819299698, acc: 1.0)
[2024-11-13 06:37:00,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:00,740][root][INFO] - Training Epoch: 5/10, step 256/574 completed (loss: 0.32884880900382996, acc: 0.9298245906829834)
[2024-11-13 06:37:00,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:01,072][root][INFO] - Training Epoch: 5/10, step 257/574 completed (loss: 0.08180800080299377, acc: 0.9714285731315613)
[2024-11-13 06:37:01,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:01,498][root][INFO] - Training Epoch: 5/10, step 258/574 completed (loss: 0.26062461733818054, acc: 0.9473684430122375)
[2024-11-13 06:37:01,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:02,343][root][INFO] - Training Epoch: 5/10, step 259/574 completed (loss: 0.4434901177883148, acc: 0.8962264060974121)
[2024-11-13 06:37:02,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:03,221][root][INFO] - Training Epoch: 5/10, step 260/574 completed (loss: 0.5343866944313049, acc: 0.8583333492279053)
[2024-11-13 06:37:03,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:03,571][root][INFO] - Training Epoch: 5/10, step 261/574 completed (loss: 0.04281678795814514, acc: 1.0)
[2024-11-13 06:37:03,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:03,961][root][INFO] - Training Epoch: 5/10, step 262/574 completed (loss: 0.2031421959400177, acc: 0.9354838728904724)
[2024-11-13 06:37:04,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:04,386][root][INFO] - Training Epoch: 5/10, step 263/574 completed (loss: 0.8429088592529297, acc: 0.8133333325386047)
[2024-11-13 06:37:04,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:04,745][root][INFO] - Training Epoch: 5/10, step 264/574 completed (loss: 0.26383593678474426, acc: 0.9583333134651184)
[2024-11-13 06:37:05,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:06,042][root][INFO] - Training Epoch: 5/10, step 265/574 completed (loss: 1.1478517055511475, acc: 0.6399999856948853)
[2024-11-13 06:37:06,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:06,444][root][INFO] - Training Epoch: 5/10, step 266/574 completed (loss: 0.9086589813232422, acc: 0.7303370833396912)
[2024-11-13 06:37:06,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:06,888][root][INFO] - Training Epoch: 5/10, step 267/574 completed (loss: 0.6257060170173645, acc: 0.7972972989082336)
[2024-11-13 06:37:07,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:07,557][root][INFO] - Training Epoch: 5/10, step 268/574 completed (loss: 0.34574466943740845, acc: 0.8965517282485962)
[2024-11-13 06:37:07,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:07,994][root][INFO] - Training Epoch: 5/10, step 269/574 completed (loss: 0.011463070288300514, acc: 1.0)
[2024-11-13 06:37:08,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:08,365][root][INFO] - Training Epoch: 5/10, step 270/574 completed (loss: 0.11113005131483078, acc: 0.9545454382896423)
[2024-11-13 06:37:08,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:08,747][root][INFO] - Training Epoch: 5/10, step 271/574 completed (loss: 0.08227259665727615, acc: 0.96875)
[2024-11-13 06:37:08,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:09,080][root][INFO] - Training Epoch: 5/10, step 272/574 completed (loss: 0.13245993852615356, acc: 0.9666666388511658)
[2024-11-13 06:37:09,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:09,595][root][INFO] - Training Epoch: 5/10, step 273/574 completed (loss: 0.3669948875904083, acc: 0.9333333373069763)
[2024-11-13 06:37:09,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:09,940][root][INFO] - Training Epoch: 5/10, step 274/574 completed (loss: 0.1053633838891983, acc: 0.96875)
[2024-11-13 06:37:10,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:10,317][root][INFO] - Training Epoch: 5/10, step 275/574 completed (loss: 0.25256314873695374, acc: 0.9666666388511658)
[2024-11-13 06:37:10,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:10,678][root][INFO] - Training Epoch: 5/10, step 276/574 completed (loss: 0.41219666600227356, acc: 0.8965517282485962)
[2024-11-13 06:37:10,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:11,034][root][INFO] - Training Epoch: 5/10, step 277/574 completed (loss: 0.13734088838100433, acc: 0.9599999785423279)
[2024-11-13 06:37:12,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:12,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:13,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:13,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:14,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:14,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:15,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:15,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:16,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:16,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:17,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:18,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:18,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:19,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:19,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:20,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:20,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:21,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:21,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:22,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:22,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:23,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:23,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:24,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:24,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:25,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:25,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:26,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:26,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:27,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:27,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:28,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:28,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:29,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:29,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:30,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:30,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:30,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:31,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:31,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:32,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:32,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:33,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:33,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:34,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:34,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:35,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:35,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:36,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:36,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:37,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:37,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:37,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:38,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:38,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:39,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:39,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:40,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:40,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:41,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:41,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:42,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:43,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:43,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:44,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:44,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:45,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:45,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:46,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:46,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:47,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:47,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:48,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:48,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:49,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:49,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:50,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:50,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:51,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:51,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:51,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:52,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:52,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:53,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:54,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:54,817][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3206, device='cuda:0') eval_epoch_loss=tensor(0.8418, device='cuda:0') eval_epoch_acc=tensor(0.8001, device='cuda:0')
[2024-11-13 06:37:54,819][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:37:54,819][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:37:55,289][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_5_step_278_loss_0.8418465256690979/model.pt
[2024-11-13 06:37:55,294][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:37:55,294][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 5 is 0.8000533580780029
[2024-11-13 06:37:55,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:55,679][root][INFO] - Training Epoch: 5/10, step 278/574 completed (loss: 0.49930471181869507, acc: 0.8723404407501221)
[2024-11-13 06:37:55,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:56,080][root][INFO] - Training Epoch: 5/10, step 279/574 completed (loss: 0.33488941192626953, acc: 0.8958333134651184)
[2024-11-13 06:37:56,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:56,471][root][INFO] - Training Epoch: 5/10, step 280/574 completed (loss: 0.026577649638056755, acc: 1.0)
[2024-11-13 06:37:56,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:57,050][root][INFO] - Training Epoch: 5/10, step 281/574 completed (loss: 0.6493377685546875, acc: 0.8192771077156067)
[2024-11-13 06:37:57,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:57,519][root][INFO] - Training Epoch: 5/10, step 282/574 completed (loss: 0.7489119172096252, acc: 0.7870370149612427)
[2024-11-13 06:37:57,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:57,985][root][INFO] - Training Epoch: 5/10, step 283/574 completed (loss: 0.2528379261493683, acc: 0.9210526347160339)
[2024-11-13 06:37:58,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:58,403][root][INFO] - Training Epoch: 5/10, step 284/574 completed (loss: 0.06641064584255219, acc: 1.0)
[2024-11-13 06:37:58,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:58,887][root][INFO] - Training Epoch: 5/10, step 285/574 completed (loss: 0.18284374475479126, acc: 0.949999988079071)
[2024-11-13 06:37:59,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:59,293][root][INFO] - Training Epoch: 5/10, step 286/574 completed (loss: 0.47514066100120544, acc: 0.8359375)
[2024-11-13 06:37:59,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:37:59,700][root][INFO] - Training Epoch: 5/10, step 287/574 completed (loss: 0.5802139043807983, acc: 0.8240000009536743)
[2024-11-13 06:37:59,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:00,120][root][INFO] - Training Epoch: 5/10, step 288/574 completed (loss: 0.2611066699028015, acc: 0.8901098966598511)
[2024-11-13 06:38:00,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:00,537][root][INFO] - Training Epoch: 5/10, step 289/574 completed (loss: 0.47374269366264343, acc: 0.8695651888847351)
[2024-11-13 06:38:00,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:00,981][root][INFO] - Training Epoch: 5/10, step 290/574 completed (loss: 0.5684996843338013, acc: 0.8144329786300659)
[2024-11-13 06:38:01,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:01,389][root][INFO] - Training Epoch: 5/10, step 291/574 completed (loss: 0.07431206107139587, acc: 0.9545454382896423)
[2024-11-13 06:38:01,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:01,841][root][INFO] - Training Epoch: 5/10, step 292/574 completed (loss: 0.2359839379787445, acc: 0.9285714030265808)
[2024-11-13 06:38:02,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:02,275][root][INFO] - Training Epoch: 5/10, step 293/574 completed (loss: 0.21544642746448517, acc: 0.9137930870056152)
[2024-11-13 06:38:02,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:02,956][root][INFO] - Training Epoch: 5/10, step 294/574 completed (loss: 0.27089831233024597, acc: 0.9272727370262146)
[2024-11-13 06:38:03,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:03,765][root][INFO] - Training Epoch: 5/10, step 295/574 completed (loss: 0.621380627155304, acc: 0.8041236996650696)
[2024-11-13 06:38:03,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:04,180][root][INFO] - Training Epoch: 5/10, step 296/574 completed (loss: 0.3908000886440277, acc: 0.8793103694915771)
[2024-11-13 06:38:04,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:04,586][root][INFO] - Training Epoch: 5/10, step 297/574 completed (loss: 0.05887625366449356, acc: 1.0)
[2024-11-13 06:38:04,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:04,973][root][INFO] - Training Epoch: 5/10, step 298/574 completed (loss: 0.23854172229766846, acc: 0.8947368264198303)
[2024-11-13 06:38:05,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:05,419][root][INFO] - Training Epoch: 5/10, step 299/574 completed (loss: 0.14843718707561493, acc: 0.9464285969734192)
[2024-11-13 06:38:05,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:05,788][root][INFO] - Training Epoch: 5/10, step 300/574 completed (loss: 0.0399068184196949, acc: 1.0)
[2024-11-13 06:38:05,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:06,199][root][INFO] - Training Epoch: 5/10, step 301/574 completed (loss: 0.17996607720851898, acc: 0.9245283007621765)
[2024-11-13 06:38:06,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:06,579][root][INFO] - Training Epoch: 5/10, step 302/574 completed (loss: 0.07714394479990005, acc: 0.9811320900917053)
[2024-11-13 06:38:06,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:06,976][root][INFO] - Training Epoch: 5/10, step 303/574 completed (loss: 0.05905876308679581, acc: 0.970588207244873)
[2024-11-13 06:38:07,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:07,316][root][INFO] - Training Epoch: 5/10, step 304/574 completed (loss: 0.11053621023893356, acc: 0.96875)
[2024-11-13 06:38:07,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:07,725][root][INFO] - Training Epoch: 5/10, step 305/574 completed (loss: 0.5423879623413086, acc: 0.8524590134620667)
[2024-11-13 06:38:07,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:08,090][root][INFO] - Training Epoch: 5/10, step 306/574 completed (loss: 0.04727881774306297, acc: 1.0)
[2024-11-13 06:38:08,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:08,493][root][INFO] - Training Epoch: 5/10, step 307/574 completed (loss: 0.006245109252631664, acc: 1.0)
[2024-11-13 06:38:08,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:08,972][root][INFO] - Training Epoch: 5/10, step 308/574 completed (loss: 0.3705682158470154, acc: 0.8985507488250732)
[2024-11-13 06:38:09,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:09,559][root][INFO] - Training Epoch: 5/10, step 309/574 completed (loss: 0.1691010594367981, acc: 0.9444444179534912)
[2024-11-13 06:38:09,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:09,952][root][INFO] - Training Epoch: 5/10, step 310/574 completed (loss: 0.33788877725601196, acc: 0.9277108311653137)
[2024-11-13 06:38:10,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:10,390][root][INFO] - Training Epoch: 5/10, step 311/574 completed (loss: 0.3011821508407593, acc: 0.9102563858032227)
[2024-11-13 06:38:10,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:10,855][root][INFO] - Training Epoch: 5/10, step 312/574 completed (loss: 0.09472864866256714, acc: 0.9591836929321289)
[2024-11-13 06:38:11,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:11,275][root][INFO] - Training Epoch: 5/10, step 313/574 completed (loss: 0.025110334157943726, acc: 1.0)
[2024-11-13 06:38:11,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:11,727][root][INFO] - Training Epoch: 5/10, step 314/574 completed (loss: 0.03627562150359154, acc: 1.0)
[2024-11-13 06:38:11,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:12,124][root][INFO] - Training Epoch: 5/10, step 315/574 completed (loss: 0.22851607203483582, acc: 0.9677419066429138)
[2024-11-13 06:38:12,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:12,585][root][INFO] - Training Epoch: 5/10, step 316/574 completed (loss: 0.5775324702262878, acc: 0.8709677457809448)
[2024-11-13 06:38:12,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:13,006][root][INFO] - Training Epoch: 5/10, step 317/574 completed (loss: 0.09195017069578171, acc: 0.9850746393203735)
[2024-11-13 06:38:13,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:13,443][root][INFO] - Training Epoch: 5/10, step 318/574 completed (loss: 0.12793929874897003, acc: 0.9615384340286255)
[2024-11-13 06:38:13,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:13,790][root][INFO] - Training Epoch: 5/10, step 319/574 completed (loss: 0.15367554128170013, acc: 0.9555555582046509)
[2024-11-13 06:38:13,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:14,145][root][INFO] - Training Epoch: 5/10, step 320/574 completed (loss: 0.08233374357223511, acc: 0.9838709831237793)
[2024-11-13 06:38:14,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:14,498][root][INFO] - Training Epoch: 5/10, step 321/574 completed (loss: 0.018150106072425842, acc: 1.0)
[2024-11-13 06:38:14,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:14,923][root][INFO] - Training Epoch: 5/10, step 322/574 completed (loss: 0.7697278261184692, acc: 0.7777777910232544)
[2024-11-13 06:38:15,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:15,340][root][INFO] - Training Epoch: 5/10, step 323/574 completed (loss: 0.9085832238197327, acc: 0.7714285850524902)
[2024-11-13 06:38:15,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:15,790][root][INFO] - Training Epoch: 5/10, step 324/574 completed (loss: 0.4982464015483856, acc: 0.8205128312110901)
[2024-11-13 06:38:15,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:16,204][root][INFO] - Training Epoch: 5/10, step 325/574 completed (loss: 0.9471808671951294, acc: 0.7317073345184326)
[2024-11-13 06:38:16,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:16,603][root][INFO] - Training Epoch: 5/10, step 326/574 completed (loss: 0.6838808655738831, acc: 0.7894737124443054)
[2024-11-13 06:38:16,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:17,060][root][INFO] - Training Epoch: 5/10, step 327/574 completed (loss: 0.056725163012742996, acc: 1.0)
[2024-11-13 06:38:17,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:17,515][root][INFO] - Training Epoch: 5/10, step 328/574 completed (loss: 0.009726460091769695, acc: 1.0)
[2024-11-13 06:38:17,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:18,006][root][INFO] - Training Epoch: 5/10, step 329/574 completed (loss: 0.14266827702522278, acc: 0.9629629850387573)
[2024-11-13 06:38:18,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:18,391][root][INFO] - Training Epoch: 5/10, step 330/574 completed (loss: 0.07051797956228256, acc: 0.96875)
[2024-11-13 06:38:18,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:18,822][root][INFO] - Training Epoch: 5/10, step 331/574 completed (loss: 0.28931036591529846, acc: 0.9193548560142517)
[2024-11-13 06:38:19,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:19,307][root][INFO] - Training Epoch: 5/10, step 332/574 completed (loss: 0.08507991582155228, acc: 0.9473684430122375)
[2024-11-13 06:38:19,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:19,726][root][INFO] - Training Epoch: 5/10, step 333/574 completed (loss: 0.13046540319919586, acc: 0.9375)
[2024-11-13 06:38:19,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:20,212][root][INFO] - Training Epoch: 5/10, step 334/574 completed (loss: 0.04148634895682335, acc: 1.0)
[2024-11-13 06:38:20,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:20,653][root][INFO] - Training Epoch: 5/10, step 335/574 completed (loss: 0.2032504826784134, acc: 0.8947368264198303)
[2024-11-13 06:38:20,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:21,117][root][INFO] - Training Epoch: 5/10, step 336/574 completed (loss: 0.6638903021812439, acc: 0.800000011920929)
[2024-11-13 06:38:21,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:21,605][root][INFO] - Training Epoch: 5/10, step 337/574 completed (loss: 0.9593690633773804, acc: 0.6896551847457886)
[2024-11-13 06:38:21,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:22,071][root][INFO] - Training Epoch: 5/10, step 338/574 completed (loss: 1.1354987621307373, acc: 0.6489361524581909)
[2024-11-13 06:38:22,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:22,450][root][INFO] - Training Epoch: 5/10, step 339/574 completed (loss: 0.9727970957756042, acc: 0.7228915691375732)
[2024-11-13 06:38:22,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:22,869][root][INFO] - Training Epoch: 5/10, step 340/574 completed (loss: 0.08657665550708771, acc: 0.95652174949646)
[2024-11-13 06:38:23,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:23,293][root][INFO] - Training Epoch: 5/10, step 341/574 completed (loss: 0.19053462147712708, acc: 0.9487179517745972)
[2024-11-13 06:38:23,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:23,722][root][INFO] - Training Epoch: 5/10, step 342/574 completed (loss: 0.21156078577041626, acc: 0.9397590160369873)
[2024-11-13 06:38:23,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:24,111][root][INFO] - Training Epoch: 5/10, step 343/574 completed (loss: 0.3340155780315399, acc: 0.8867924809455872)
[2024-11-13 06:38:24,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:24,539][root][INFO] - Training Epoch: 5/10, step 344/574 completed (loss: 0.15927600860595703, acc: 0.949367105960846)
[2024-11-13 06:38:24,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:24,917][root][INFO] - Training Epoch: 5/10, step 345/574 completed (loss: 0.04346010088920593, acc: 0.9803921580314636)
[2024-11-13 06:38:25,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:25,298][root][INFO] - Training Epoch: 5/10, step 346/574 completed (loss: 0.42353150248527527, acc: 0.8805969953536987)
[2024-11-13 06:38:25,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:25,726][root][INFO] - Training Epoch: 5/10, step 347/574 completed (loss: 0.08757016807794571, acc: 0.949999988079071)
[2024-11-13 06:38:25,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:26,163][root][INFO] - Training Epoch: 5/10, step 348/574 completed (loss: 0.2738177478313446, acc: 0.8799999952316284)
[2024-11-13 06:38:26,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:26,689][root][INFO] - Training Epoch: 5/10, step 349/574 completed (loss: 0.5086772441864014, acc: 0.8611111044883728)
[2024-11-13 06:38:26,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:27,057][root][INFO] - Training Epoch: 5/10, step 350/574 completed (loss: 0.4820403754711151, acc: 0.8604651093482971)
[2024-11-13 06:38:27,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:27,496][root][INFO] - Training Epoch: 5/10, step 351/574 completed (loss: 0.1477424055337906, acc: 0.9487179517745972)
[2024-11-13 06:38:27,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:27,997][root][INFO] - Training Epoch: 5/10, step 352/574 completed (loss: 0.6952354311943054, acc: 0.7555555701255798)
[2024-11-13 06:38:28,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:28,351][root][INFO] - Training Epoch: 5/10, step 353/574 completed (loss: 0.006543524097651243, acc: 1.0)
[2024-11-13 06:38:28,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:28,693][root][INFO] - Training Epoch: 5/10, step 354/574 completed (loss: 0.14483949542045593, acc: 0.9615384340286255)
[2024-11-13 06:38:28,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:29,155][root][INFO] - Training Epoch: 5/10, step 355/574 completed (loss: 0.5884702205657959, acc: 0.8351648449897766)
[2024-11-13 06:38:29,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:29,921][root][INFO] - Training Epoch: 5/10, step 356/574 completed (loss: 0.5153111815452576, acc: 0.852173924446106)
[2024-11-13 06:38:30,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:30,392][root][INFO] - Training Epoch: 5/10, step 357/574 completed (loss: 0.3931494951248169, acc: 0.8695651888847351)
[2024-11-13 06:38:30,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:30,826][root][INFO] - Training Epoch: 5/10, step 358/574 completed (loss: 0.45272234082221985, acc: 0.8979591727256775)
[2024-11-13 06:38:30,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:31,184][root][INFO] - Training Epoch: 5/10, step 359/574 completed (loss: 0.00312909041531384, acc: 1.0)
[2024-11-13 06:38:31,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:31,630][root][INFO] - Training Epoch: 5/10, step 360/574 completed (loss: 0.10466291010379791, acc: 0.9230769276618958)
[2024-11-13 06:38:31,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:32,056][root][INFO] - Training Epoch: 5/10, step 361/574 completed (loss: 0.17592261731624603, acc: 0.9024389982223511)
[2024-11-13 06:38:32,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:32,487][root][INFO] - Training Epoch: 5/10, step 362/574 completed (loss: 0.2657108008861542, acc: 0.9111111164093018)
[2024-11-13 06:38:32,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:32,857][root][INFO] - Training Epoch: 5/10, step 363/574 completed (loss: 0.06105222925543785, acc: 0.9868420958518982)
[2024-11-13 06:38:33,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:33,263][root][INFO] - Training Epoch: 5/10, step 364/574 completed (loss: 0.05207962542772293, acc: 1.0)
[2024-11-13 06:38:33,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:33,656][root][INFO] - Training Epoch: 5/10, step 365/574 completed (loss: 0.05515962839126587, acc: 0.9696969985961914)
[2024-11-13 06:38:33,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:34,084][root][INFO] - Training Epoch: 5/10, step 366/574 completed (loss: 0.037549253553152084, acc: 1.0)
[2024-11-13 06:38:34,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:34,463][root][INFO] - Training Epoch: 5/10, step 367/574 completed (loss: 0.14307160675525665, acc: 0.95652174949646)
[2024-11-13 06:38:34,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:34,825][root][INFO] - Training Epoch: 5/10, step 368/574 completed (loss: 0.14971724152565002, acc: 0.9285714030265808)
[2024-11-13 06:38:35,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:35,246][root][INFO] - Training Epoch: 5/10, step 369/574 completed (loss: 0.14682309329509735, acc: 0.96875)
[2024-11-13 06:38:35,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:36,178][root][INFO] - Training Epoch: 5/10, step 370/574 completed (loss: 0.5427324175834656, acc: 0.8545454740524292)
[2024-11-13 06:38:36,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:37,498][root][INFO] - Training Epoch: 5/10, step 371/574 completed (loss: 0.2656903862953186, acc: 0.9528301954269409)
[2024-11-13 06:38:37,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:37,934][root][INFO] - Training Epoch: 5/10, step 372/574 completed (loss: 0.23654791712760925, acc: 0.9111111164093018)
[2024-11-13 06:38:38,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:38,352][root][INFO] - Training Epoch: 5/10, step 373/574 completed (loss: 0.1525019109249115, acc: 0.9285714030265808)
[2024-11-13 06:38:38,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:38,747][root][INFO] - Training Epoch: 5/10, step 374/574 completed (loss: 0.035060979425907135, acc: 1.0)
[2024-11-13 06:38:38,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:39,086][root][INFO] - Training Epoch: 5/10, step 375/574 completed (loss: 0.05460609868168831, acc: 0.9599999785423279)
[2024-11-13 06:38:39,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:39,484][root][INFO] - Training Epoch: 5/10, step 376/574 completed (loss: 0.01982775330543518, acc: 1.0)
[2024-11-13 06:38:39,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:39,927][root][INFO] - Training Epoch: 5/10, step 377/574 completed (loss: 0.12103092670440674, acc: 0.9791666865348816)
[2024-11-13 06:38:40,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:40,371][root][INFO] - Training Epoch: 5/10, step 378/574 completed (loss: 0.04690055176615715, acc: 0.9789473414421082)
[2024-11-13 06:38:40,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:41,231][root][INFO] - Training Epoch: 5/10, step 379/574 completed (loss: 0.21695537865161896, acc: 0.9341317415237427)
[2024-11-13 06:38:41,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:41,830][root][INFO] - Training Epoch: 5/10, step 380/574 completed (loss: 0.2935045063495636, acc: 0.9172932505607605)
[2024-11-13 06:38:43,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:43,789][root][INFO] - Training Epoch: 5/10, step 381/574 completed (loss: 0.5196136832237244, acc: 0.855614960193634)
[2024-11-13 06:38:44,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:44,660][root][INFO] - Training Epoch: 5/10, step 382/574 completed (loss: 0.08746343851089478, acc: 0.9729729890823364)
[2024-11-13 06:38:44,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:45,081][root][INFO] - Training Epoch: 5/10, step 383/574 completed (loss: 0.1415880173444748, acc: 0.9642857313156128)
[2024-11-13 06:38:45,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:45,463][root][INFO] - Training Epoch: 5/10, step 384/574 completed (loss: 0.08785884082317352, acc: 0.9642857313156128)
[2024-11-13 06:38:45,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:45,865][root][INFO] - Training Epoch: 5/10, step 385/574 completed (loss: 0.30590128898620605, acc: 0.90625)
[2024-11-13 06:38:46,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:46,275][root][INFO] - Training Epoch: 5/10, step 386/574 completed (loss: 0.004423686768859625, acc: 1.0)
[2024-11-13 06:38:46,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:46,664][root][INFO] - Training Epoch: 5/10, step 387/574 completed (loss: 0.03809001296758652, acc: 0.9736841917037964)
[2024-11-13 06:38:46,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:47,072][root][INFO] - Training Epoch: 5/10, step 388/574 completed (loss: 0.05045376718044281, acc: 0.9545454382896423)
[2024-11-13 06:38:47,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:47,568][root][INFO] - Training Epoch: 5/10, step 389/574 completed (loss: 0.015754450112581253, acc: 1.0)
[2024-11-13 06:38:47,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:47,976][root][INFO] - Training Epoch: 5/10, step 390/574 completed (loss: 0.09008857607841492, acc: 1.0)
[2024-11-13 06:38:48,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:48,412][root][INFO] - Training Epoch: 5/10, step 391/574 completed (loss: 0.5757861137390137, acc: 0.7592592835426331)
[2024-11-13 06:38:48,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:48,831][root][INFO] - Training Epoch: 5/10, step 392/574 completed (loss: 0.778068482875824, acc: 0.7475728392601013)
[2024-11-13 06:38:49,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:49,592][root][INFO] - Training Epoch: 5/10, step 393/574 completed (loss: 0.7782548666000366, acc: 0.8382353186607361)
[2024-11-13 06:38:49,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:50,066][root][INFO] - Training Epoch: 5/10, step 394/574 completed (loss: 0.718097448348999, acc: 0.7933333516120911)
[2024-11-13 06:38:50,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:50,591][root][INFO] - Training Epoch: 5/10, step 395/574 completed (loss: 0.5244075655937195, acc: 0.8263888955116272)
[2024-11-13 06:38:50,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:50,988][root][INFO] - Training Epoch: 5/10, step 396/574 completed (loss: 0.26151230931282043, acc: 0.8604651093482971)
[2024-11-13 06:38:51,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:51,422][root][INFO] - Training Epoch: 5/10, step 397/574 completed (loss: 0.014266195707023144, acc: 1.0)
[2024-11-13 06:38:51,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:51,813][root][INFO] - Training Epoch: 5/10, step 398/574 completed (loss: 0.12963782250881195, acc: 0.9767441749572754)
[2024-11-13 06:38:51,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:52,213][root][INFO] - Training Epoch: 5/10, step 399/574 completed (loss: 0.02320706658065319, acc: 1.0)
[2024-11-13 06:38:52,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:53,008][root][INFO] - Training Epoch: 5/10, step 400/574 completed (loss: 0.3230556845664978, acc: 0.9117646813392639)
[2024-11-13 06:38:53,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:53,443][root][INFO] - Training Epoch: 5/10, step 401/574 completed (loss: 0.35298025608062744, acc: 0.8666666746139526)
[2024-11-13 06:38:53,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:53,859][root][INFO] - Training Epoch: 5/10, step 402/574 completed (loss: 0.40033572912216187, acc: 0.939393937587738)
[2024-11-13 06:38:54,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:54,270][root][INFO] - Training Epoch: 5/10, step 403/574 completed (loss: 0.46156784892082214, acc: 0.9090909361839294)
[2024-11-13 06:38:54,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:54,687][root][INFO] - Training Epoch: 5/10, step 404/574 completed (loss: 0.3201606571674347, acc: 0.9677419066429138)
[2024-11-13 06:38:54,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:55,072][root][INFO] - Training Epoch: 5/10, step 405/574 completed (loss: 0.20345626771450043, acc: 0.9629629850387573)
[2024-11-13 06:38:55,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:55,468][root][INFO] - Training Epoch: 5/10, step 406/574 completed (loss: 0.07864104956388474, acc: 0.9599999785423279)
[2024-11-13 06:38:55,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:55,882][root][INFO] - Training Epoch: 5/10, step 407/574 completed (loss: 0.021959461271762848, acc: 1.0)
[2024-11-13 06:38:56,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:56,327][root][INFO] - Training Epoch: 5/10, step 408/574 completed (loss: 0.04010610282421112, acc: 1.0)
[2024-11-13 06:38:56,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:56,741][root][INFO] - Training Epoch: 5/10, step 409/574 completed (loss: 0.03296946734189987, acc: 1.0)
[2024-11-13 06:38:56,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:57,105][root][INFO] - Training Epoch: 5/10, step 410/574 completed (loss: 0.15835757553577423, acc: 0.9655172228813171)
[2024-11-13 06:38:57,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:57,553][root][INFO] - Training Epoch: 5/10, step 411/574 completed (loss: 0.04178941994905472, acc: 1.0)
[2024-11-13 06:38:57,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:57,932][root][INFO] - Training Epoch: 5/10, step 412/574 completed (loss: 0.059345562011003494, acc: 0.9666666388511658)
[2024-11-13 06:38:58,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:58,277][root][INFO] - Training Epoch: 5/10, step 413/574 completed (loss: 0.09118607640266418, acc: 0.939393937587738)
[2024-11-13 06:38:58,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:58,628][root][INFO] - Training Epoch: 5/10, step 414/574 completed (loss: 0.014383560046553612, acc: 1.0)
[2024-11-13 06:38:58,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:59,050][root][INFO] - Training Epoch: 5/10, step 415/574 completed (loss: 0.292783260345459, acc: 0.9019607901573181)
[2024-11-13 06:38:59,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:59,429][root][INFO] - Training Epoch: 5/10, step 416/574 completed (loss: 0.07818849384784698, acc: 1.0)
[2024-11-13 06:38:59,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:38:59,856][root][INFO] - Training Epoch: 5/10, step 417/574 completed (loss: 0.33767759799957275, acc: 0.8888888955116272)
[2024-11-13 06:39:00,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:00,260][root][INFO] - Training Epoch: 5/10, step 418/574 completed (loss: 0.17471332848072052, acc: 0.9750000238418579)
[2024-11-13 06:39:00,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:00,604][root][INFO] - Training Epoch: 5/10, step 419/574 completed (loss: 0.1976148635149002, acc: 0.8999999761581421)
[2024-11-13 06:39:00,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:00,925][root][INFO] - Training Epoch: 5/10, step 420/574 completed (loss: 0.018740786239504814, acc: 1.0)
[2024-11-13 06:39:01,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:02,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:02,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:03,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:03,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:04,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:04,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:05,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:05,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:06,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:06,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:07,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:07,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:08,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:08,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:09,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:09,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:10,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:10,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:11,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:11,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:12,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:12,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:13,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:13,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:14,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:14,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:15,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:15,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:16,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:16,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:17,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:17,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:17,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:18,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:18,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:19,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:19,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:20,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:20,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:21,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:21,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:22,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:22,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:23,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:23,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:24,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:24,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:25,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:25,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:26,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:26,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:27,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:27,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:28,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:28,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:29,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:29,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:30,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:30,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:31,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:31,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:32,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:33,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:33,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:34,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:34,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:35,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:35,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:36,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:36,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:37,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:37,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:38,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:38,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:39,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:39,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:39,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:40,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:40,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:41,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:41,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:42,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:43,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:43,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:44,290][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4239, device='cuda:0') eval_epoch_loss=tensor(0.8854, device='cuda:0') eval_epoch_acc=tensor(0.7884, device='cuda:0')
[2024-11-13 06:39:44,291][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:39:44,292][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:39:44,837][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_5_step_421_loss_0.8853794932365417/model.pt
[2024-11-13 06:39:44,847][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:39:45,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:45,286][root][INFO] - Training Epoch: 5/10, step 421/574 completed (loss: 0.17436036467552185, acc: 0.9333333373069763)
[2024-11-13 06:39:45,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:45,696][root][INFO] - Training Epoch: 5/10, step 422/574 completed (loss: 0.16628479957580566, acc: 0.96875)
[2024-11-13 06:39:45,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:46,148][root][INFO] - Training Epoch: 5/10, step 423/574 completed (loss: 0.18856297433376312, acc: 0.9444444179534912)
[2024-11-13 06:39:46,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:46,543][root][INFO] - Training Epoch: 5/10, step 424/574 completed (loss: 0.13182562589645386, acc: 0.9629629850387573)
[2024-11-13 06:39:46,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:46,968][root][INFO] - Training Epoch: 5/10, step 425/574 completed (loss: 0.10825320333242416, acc: 0.9696969985961914)
[2024-11-13 06:39:47,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:47,381][root][INFO] - Training Epoch: 5/10, step 426/574 completed (loss: 0.011943935416638851, acc: 1.0)
[2024-11-13 06:39:47,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:47,760][root][INFO] - Training Epoch: 5/10, step 427/574 completed (loss: 0.13493961095809937, acc: 0.9459459185600281)
[2024-11-13 06:39:47,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:48,149][root][INFO] - Training Epoch: 5/10, step 428/574 completed (loss: 0.05375276505947113, acc: 1.0)
[2024-11-13 06:39:48,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:48,548][root][INFO] - Training Epoch: 5/10, step 429/574 completed (loss: 0.4380423128604889, acc: 0.9130434989929199)
[2024-11-13 06:39:48,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:48,965][root][INFO] - Training Epoch: 5/10, step 430/574 completed (loss: 0.17604146897792816, acc: 0.9629629850387573)
[2024-11-13 06:39:49,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:49,376][root][INFO] - Training Epoch: 5/10, step 431/574 completed (loss: 0.09192785620689392, acc: 0.9629629850387573)
[2024-11-13 06:39:49,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:49,720][root][INFO] - Training Epoch: 5/10, step 432/574 completed (loss: 0.02442312054336071, acc: 1.0)
[2024-11-13 06:39:49,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:50,180][root][INFO] - Training Epoch: 5/10, step 433/574 completed (loss: 0.2270815670490265, acc: 0.9166666865348816)
[2024-11-13 06:39:50,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:50,555][root][INFO] - Training Epoch: 5/10, step 434/574 completed (loss: 0.013059570454061031, acc: 1.0)
[2024-11-13 06:39:50,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:50,905][root][INFO] - Training Epoch: 5/10, step 435/574 completed (loss: 0.03467497602105141, acc: 1.0)
[2024-11-13 06:39:51,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:51,273][root][INFO] - Training Epoch: 5/10, step 436/574 completed (loss: 0.15307025611400604, acc: 0.9444444179534912)
[2024-11-13 06:39:51,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:51,654][root][INFO] - Training Epoch: 5/10, step 437/574 completed (loss: 0.05806078016757965, acc: 1.0)
[2024-11-13 06:39:51,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:52,013][root][INFO] - Training Epoch: 5/10, step 438/574 completed (loss: 0.00600787065923214, acc: 1.0)
[2024-11-13 06:39:52,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:52,396][root][INFO] - Training Epoch: 5/10, step 439/574 completed (loss: 0.10128938406705856, acc: 0.9743589758872986)
[2024-11-13 06:39:52,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:53,047][root][INFO] - Training Epoch: 5/10, step 440/574 completed (loss: 0.372871071100235, acc: 0.8939393758773804)
[2024-11-13 06:39:53,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:54,110][root][INFO] - Training Epoch: 5/10, step 441/574 completed (loss: 0.9685429334640503, acc: 0.7200000286102295)
[2024-11-13 06:39:54,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:54,668][root][INFO] - Training Epoch: 5/10, step 442/574 completed (loss: 0.9564712643623352, acc: 0.7177419066429138)
[2024-11-13 06:39:55,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:55,661][root][INFO] - Training Epoch: 5/10, step 443/574 completed (loss: 0.5829267501831055, acc: 0.8208954930305481)
[2024-11-13 06:39:55,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:56,075][root][INFO] - Training Epoch: 5/10, step 444/574 completed (loss: 0.15269294381141663, acc: 0.9433962106704712)
[2024-11-13 06:39:56,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:56,668][root][INFO] - Training Epoch: 5/10, step 445/574 completed (loss: 0.13324368000030518, acc: 0.9545454382896423)
[2024-11-13 06:39:56,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:57,113][root][INFO] - Training Epoch: 5/10, step 446/574 completed (loss: 0.2406020313501358, acc: 0.9130434989929199)
[2024-11-13 06:39:57,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:57,491][root][INFO] - Training Epoch: 5/10, step 447/574 completed (loss: 0.10888499766588211, acc: 0.9615384340286255)
[2024-11-13 06:39:57,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:57,853][root][INFO] - Training Epoch: 5/10, step 448/574 completed (loss: 0.07062231004238129, acc: 0.9642857313156128)
[2024-11-13 06:39:58,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:58,261][root][INFO] - Training Epoch: 5/10, step 449/574 completed (loss: 0.08602854609489441, acc: 0.9552238583564758)
[2024-11-13 06:39:58,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:58,707][root][INFO] - Training Epoch: 5/10, step 450/574 completed (loss: 0.07581065595149994, acc: 0.9722222089767456)
[2024-11-13 06:39:58,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:59,113][root][INFO] - Training Epoch: 5/10, step 451/574 completed (loss: 0.19743451476097107, acc: 0.95652174949646)
[2024-11-13 06:39:59,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:59,571][root][INFO] - Training Epoch: 5/10, step 452/574 completed (loss: 0.24750438332557678, acc: 0.9230769276618958)
[2024-11-13 06:39:59,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:39:59,948][root][INFO] - Training Epoch: 5/10, step 453/574 completed (loss: 0.3880634009838104, acc: 0.8815789222717285)
[2024-11-13 06:40:00,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:00,387][root][INFO] - Training Epoch: 5/10, step 454/574 completed (loss: 0.5732228755950928, acc: 0.8979591727256775)
[2024-11-13 06:40:00,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:00,802][root][INFO] - Training Epoch: 5/10, step 455/574 completed (loss: 0.35045889019966125, acc: 0.9090909361839294)
[2024-11-13 06:40:01,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:01,261][root][INFO] - Training Epoch: 5/10, step 456/574 completed (loss: 0.6651298999786377, acc: 0.7938144207000732)
[2024-11-13 06:40:01,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:01,641][root][INFO] - Training Epoch: 5/10, step 457/574 completed (loss: 0.13332924246788025, acc: 0.9571428298950195)
[2024-11-13 06:40:01,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:02,139][root][INFO] - Training Epoch: 5/10, step 458/574 completed (loss: 0.47927021980285645, acc: 0.8372092843055725)
[2024-11-13 06:40:02,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:02,501][root][INFO] - Training Epoch: 5/10, step 459/574 completed (loss: 0.19864623248577118, acc: 0.9285714030265808)
[2024-11-13 06:40:02,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:02,921][root][INFO] - Training Epoch: 5/10, step 460/574 completed (loss: 0.3802315890789032, acc: 0.8641975522041321)
[2024-11-13 06:40:03,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:03,324][root][INFO] - Training Epoch: 5/10, step 461/574 completed (loss: 0.47610628604888916, acc: 0.8333333134651184)
[2024-11-13 06:40:03,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:03,702][root][INFO] - Training Epoch: 5/10, step 462/574 completed (loss: 0.057816337794065475, acc: 1.0)
[2024-11-13 06:40:03,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:04,054][root][INFO] - Training Epoch: 5/10, step 463/574 completed (loss: 0.2985652983188629, acc: 0.9230769276618958)
[2024-11-13 06:40:04,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:04,500][root][INFO] - Training Epoch: 5/10, step 464/574 completed (loss: 0.17647096514701843, acc: 0.95652174949646)
[2024-11-13 06:40:04,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:04,890][root][INFO] - Training Epoch: 5/10, step 465/574 completed (loss: 0.4108727276325226, acc: 0.8690476417541504)
[2024-11-13 06:40:05,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:05,345][root][INFO] - Training Epoch: 5/10, step 466/574 completed (loss: 0.42669057846069336, acc: 0.891566276550293)
[2024-11-13 06:40:05,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:05,865][root][INFO] - Training Epoch: 5/10, step 467/574 completed (loss: 0.18341045081615448, acc: 0.954954981803894)
[2024-11-13 06:40:06,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:06,279][root][INFO] - Training Epoch: 5/10, step 468/574 completed (loss: 0.6923147439956665, acc: 0.844660222530365)
[2024-11-13 06:40:06,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:06,702][root][INFO] - Training Epoch: 5/10, step 469/574 completed (loss: 0.46693548560142517, acc: 0.8780487775802612)
[2024-11-13 06:40:06,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:07,136][root][INFO] - Training Epoch: 5/10, step 470/574 completed (loss: 0.09102433174848557, acc: 0.9583333134651184)
[2024-11-13 06:40:07,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:07,568][root][INFO] - Training Epoch: 5/10, step 471/574 completed (loss: 0.04052441194653511, acc: 1.0)
[2024-11-13 06:40:07,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:08,125][root][INFO] - Training Epoch: 5/10, step 472/574 completed (loss: 0.6006951928138733, acc: 0.843137264251709)
[2024-11-13 06:40:08,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:08,614][root][INFO] - Training Epoch: 5/10, step 473/574 completed (loss: 0.8087577223777771, acc: 0.7336244583129883)
[2024-11-13 06:40:08,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:09,053][root][INFO] - Training Epoch: 5/10, step 474/574 completed (loss: 0.4892929494380951, acc: 0.8541666865348816)
[2024-11-13 06:40:09,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:09,457][root][INFO] - Training Epoch: 5/10, step 475/574 completed (loss: 0.4611831307411194, acc: 0.8773006200790405)
[2024-11-13 06:40:09,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:09,835][root][INFO] - Training Epoch: 5/10, step 476/574 completed (loss: 0.3921810984611511, acc: 0.8848921060562134)
[2024-11-13 06:40:10,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:10,292][root][INFO] - Training Epoch: 5/10, step 477/574 completed (loss: 0.6235888600349426, acc: 0.8341708779335022)
[2024-11-13 06:40:10,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:10,710][root][INFO] - Training Epoch: 5/10, step 478/574 completed (loss: 0.1946960687637329, acc: 0.9722222089767456)
[2024-11-13 06:40:10,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:11,084][root][INFO] - Training Epoch: 5/10, step 479/574 completed (loss: 0.3876595199108124, acc: 0.8787878751754761)
[2024-11-13 06:40:11,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:11,483][root][INFO] - Training Epoch: 5/10, step 480/574 completed (loss: 0.3820876479148865, acc: 0.8518518805503845)
[2024-11-13 06:40:11,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:11,865][root][INFO] - Training Epoch: 5/10, step 481/574 completed (loss: 0.4172188341617584, acc: 0.8999999761581421)
[2024-11-13 06:40:12,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:12,252][root][INFO] - Training Epoch: 5/10, step 482/574 completed (loss: 0.594777524471283, acc: 0.8999999761581421)
[2024-11-13 06:40:12,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:12,749][root][INFO] - Training Epoch: 5/10, step 483/574 completed (loss: 0.49623703956604004, acc: 0.8448275923728943)
[2024-11-13 06:40:12,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:13,112][root][INFO] - Training Epoch: 5/10, step 484/574 completed (loss: 0.0675705075263977, acc: 1.0)
[2024-11-13 06:40:13,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:13,479][root][INFO] - Training Epoch: 5/10, step 485/574 completed (loss: 0.298185795545578, acc: 0.8947368264198303)
[2024-11-13 06:40:13,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:13,935][root][INFO] - Training Epoch: 5/10, step 486/574 completed (loss: 0.43985068798065186, acc: 0.8518518805503845)
[2024-11-13 06:40:14,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:14,345][root][INFO] - Training Epoch: 5/10, step 487/574 completed (loss: 0.49009159207344055, acc: 0.8095238208770752)
[2024-11-13 06:40:14,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:14,739][root][INFO] - Training Epoch: 5/10, step 488/574 completed (loss: 0.11373510956764221, acc: 0.9545454382896423)
[2024-11-13 06:40:14,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:15,211][root][INFO] - Training Epoch: 5/10, step 489/574 completed (loss: 0.7557076811790466, acc: 0.800000011920929)
[2024-11-13 06:40:15,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:15,602][root][INFO] - Training Epoch: 5/10, step 490/574 completed (loss: 0.05844607576727867, acc: 1.0)
[2024-11-13 06:40:15,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:15,997][root][INFO] - Training Epoch: 5/10, step 491/574 completed (loss: 0.1133808046579361, acc: 0.931034505367279)
[2024-11-13 06:40:16,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:16,405][root][INFO] - Training Epoch: 5/10, step 492/574 completed (loss: 0.41777312755584717, acc: 0.8627451062202454)
[2024-11-13 06:40:16,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:16,763][root][INFO] - Training Epoch: 5/10, step 493/574 completed (loss: 0.10386577993631363, acc: 0.931034505367279)
[2024-11-13 06:40:16,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:17,134][root][INFO] - Training Epoch: 5/10, step 494/574 completed (loss: 0.37098604440689087, acc: 0.8947368264198303)
[2024-11-13 06:40:17,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:17,544][root][INFO] - Training Epoch: 5/10, step 495/574 completed (loss: 0.3276498019695282, acc: 0.8947368264198303)
[2024-11-13 06:40:17,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:18,063][root][INFO] - Training Epoch: 5/10, step 496/574 completed (loss: 0.5508762001991272, acc: 0.8660714030265808)
[2024-11-13 06:40:18,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:18,561][root][INFO] - Training Epoch: 5/10, step 497/574 completed (loss: 0.21298673748970032, acc: 0.9550561904907227)
[2024-11-13 06:40:18,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:18,969][root][INFO] - Training Epoch: 5/10, step 498/574 completed (loss: 0.5421121120452881, acc: 0.8202247023582458)
[2024-11-13 06:40:19,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:19,419][root][INFO] - Training Epoch: 5/10, step 499/574 completed (loss: 1.0740407705307007, acc: 0.6808510422706604)
[2024-11-13 06:40:19,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:19,790][root][INFO] - Training Epoch: 5/10, step 500/574 completed (loss: 0.5998836159706116, acc: 0.8478260636329651)
[2024-11-13 06:40:19,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:20,148][root][INFO] - Training Epoch: 5/10, step 501/574 completed (loss: 0.025877444073557854, acc: 1.0)
[2024-11-13 06:40:20,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:20,568][root][INFO] - Training Epoch: 5/10, step 502/574 completed (loss: 0.26168590784072876, acc: 0.9615384340286255)
[2024-11-13 06:40:20,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:20,943][root][INFO] - Training Epoch: 5/10, step 503/574 completed (loss: 0.02810141257941723, acc: 1.0)
[2024-11-13 06:40:21,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:21,319][root][INFO] - Training Epoch: 5/10, step 504/574 completed (loss: 0.13386857509613037, acc: 0.9629629850387573)
[2024-11-13 06:40:21,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:21,731][root][INFO] - Training Epoch: 5/10, step 505/574 completed (loss: 0.4998515844345093, acc: 0.9056603908538818)
[2024-11-13 06:40:21,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:22,168][root][INFO] - Training Epoch: 5/10, step 506/574 completed (loss: 0.6376804113388062, acc: 0.8965517282485962)
[2024-11-13 06:40:22,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:23,116][root][INFO] - Training Epoch: 5/10, step 507/574 completed (loss: 1.0171098709106445, acc: 0.7477477192878723)
[2024-11-13 06:40:23,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:23,756][root][INFO] - Training Epoch: 5/10, step 508/574 completed (loss: 0.667552649974823, acc: 0.8309859037399292)
[2024-11-13 06:40:24,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:24,250][root][INFO] - Training Epoch: 5/10, step 509/574 completed (loss: 0.1262279897928238, acc: 0.8999999761581421)
[2024-11-13 06:40:24,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:24,667][root][INFO] - Training Epoch: 5/10, step 510/574 completed (loss: 0.027990108355879784, acc: 1.0)
[2024-11-13 06:40:24,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:25,131][root][INFO] - Training Epoch: 5/10, step 511/574 completed (loss: 0.19753853976726532, acc: 0.9615384340286255)
[2024-11-13 06:40:27,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:28,911][root][INFO] - Training Epoch: 5/10, step 512/574 completed (loss: 0.9753623008728027, acc: 0.7357142567634583)
[2024-11-13 06:40:29,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:30,123][root][INFO] - Training Epoch: 5/10, step 513/574 completed (loss: 0.24264216423034668, acc: 0.9365079402923584)
[2024-11-13 06:40:30,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:30,484][root][INFO] - Training Epoch: 5/10, step 514/574 completed (loss: 0.2841790020465851, acc: 0.9285714030265808)
[2024-11-13 06:40:30,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:30,881][root][INFO] - Training Epoch: 5/10, step 515/574 completed (loss: 0.12248395383358002, acc: 0.9333333373069763)
[2024-11-13 06:40:31,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:31,950][root][INFO] - Training Epoch: 5/10, step 516/574 completed (loss: 0.4081491529941559, acc: 0.875)
[2024-11-13 06:40:32,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:32,303][root][INFO] - Training Epoch: 5/10, step 517/574 completed (loss: 0.005634679924696684, acc: 1.0)
[2024-11-13 06:40:32,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:32,673][root][INFO] - Training Epoch: 5/10, step 518/574 completed (loss: 0.02940467558801174, acc: 1.0)
[2024-11-13 06:40:32,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:33,025][root][INFO] - Training Epoch: 5/10, step 519/574 completed (loss: 0.2852056324481964, acc: 0.949999988079071)
[2024-11-13 06:40:33,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:33,443][root][INFO] - Training Epoch: 5/10, step 520/574 completed (loss: 0.18682724237442017, acc: 0.9259259104728699)
[2024-11-13 06:40:34,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:34,996][root][INFO] - Training Epoch: 5/10, step 521/574 completed (loss: 0.7060274481773376, acc: 0.7711864113807678)
[2024-11-13 06:40:35,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:35,545][root][INFO] - Training Epoch: 5/10, step 522/574 completed (loss: 0.32561394572257996, acc: 0.9402984976768494)
[2024-11-13 06:40:35,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:36,014][root][INFO] - Training Epoch: 5/10, step 523/574 completed (loss: 0.27029848098754883, acc: 0.9124087691307068)
[2024-11-13 06:40:36,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:36,867][root][INFO] - Training Epoch: 5/10, step 524/574 completed (loss: 0.6454562544822693, acc: 0.8299999833106995)
[2024-11-13 06:40:37,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:37,239][root][INFO] - Training Epoch: 5/10, step 525/574 completed (loss: 0.07861699163913727, acc: 0.9629629850387573)
[2024-11-13 06:40:37,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:37,659][root][INFO] - Training Epoch: 5/10, step 526/574 completed (loss: 0.1263006031513214, acc: 0.9615384340286255)
[2024-11-13 06:40:37,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:38,079][root][INFO] - Training Epoch: 5/10, step 527/574 completed (loss: 0.09597865492105484, acc: 0.9523809552192688)
[2024-11-13 06:40:38,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:38,536][root][INFO] - Training Epoch: 5/10, step 528/574 completed (loss: 0.9885182976722717, acc: 0.6557376980781555)
[2024-11-13 06:40:38,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:38,926][root][INFO] - Training Epoch: 5/10, step 529/574 completed (loss: 0.192045196890831, acc: 0.9491525292396545)
[2024-11-13 06:40:39,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:39,271][root][INFO] - Training Epoch: 5/10, step 530/574 completed (loss: 0.9036632180213928, acc: 0.6744186282157898)
[2024-11-13 06:40:39,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:39,632][root][INFO] - Training Epoch: 5/10, step 531/574 completed (loss: 0.3819933831691742, acc: 0.9318181872367859)
[2024-11-13 06:40:39,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:39,999][root][INFO] - Training Epoch: 5/10, step 532/574 completed (loss: 0.773857593536377, acc: 0.7735849022865295)
[2024-11-13 06:40:40,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:40,489][root][INFO] - Training Epoch: 5/10, step 533/574 completed (loss: 0.3879055678844452, acc: 0.8863636255264282)
[2024-11-13 06:40:40,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:40,909][root][INFO] - Training Epoch: 5/10, step 534/574 completed (loss: 0.18961507081985474, acc: 0.9599999785423279)
[2024-11-13 06:40:41,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:41,276][root][INFO] - Training Epoch: 5/10, step 535/574 completed (loss: 0.1267053633928299, acc: 0.8999999761581421)
[2024-11-13 06:40:41,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:41,589][root][INFO] - Training Epoch: 5/10, step 536/574 completed (loss: 0.06509492546319962, acc: 1.0)
[2024-11-13 06:40:41,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:42,106][root][INFO] - Training Epoch: 5/10, step 537/574 completed (loss: 0.47496575117111206, acc: 0.892307698726654)
[2024-11-13 06:40:42,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:42,508][root][INFO] - Training Epoch: 5/10, step 538/574 completed (loss: 0.37648412585258484, acc: 0.875)
[2024-11-13 06:40:42,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:43,044][root][INFO] - Training Epoch: 5/10, step 539/574 completed (loss: 0.21499712765216827, acc: 0.96875)
[2024-11-13 06:40:43,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:43,403][root][INFO] - Training Epoch: 5/10, step 540/574 completed (loss: 0.32184216380119324, acc: 0.939393937587738)
[2024-11-13 06:40:43,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:43,753][root][INFO] - Training Epoch: 5/10, step 541/574 completed (loss: 0.10959302634000778, acc: 0.9375)
[2024-11-13 06:40:43,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:44,100][root][INFO] - Training Epoch: 5/10, step 542/574 completed (loss: 0.17421172559261322, acc: 0.9677419066429138)
[2024-11-13 06:40:44,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:44,471][root][INFO] - Training Epoch: 5/10, step 543/574 completed (loss: 0.09329051524400711, acc: 0.95652174949646)
[2024-11-13 06:40:44,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:44,869][root][INFO] - Training Epoch: 5/10, step 544/574 completed (loss: 0.17228247225284576, acc: 0.9333333373069763)
[2024-11-13 06:40:45,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:45,237][root][INFO] - Training Epoch: 5/10, step 545/574 completed (loss: 0.12793363630771637, acc: 0.9512194991111755)
[2024-11-13 06:40:45,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:45,648][root][INFO] - Training Epoch: 5/10, step 546/574 completed (loss: 0.037604231387376785, acc: 1.0)
[2024-11-13 06:40:45,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:46,002][root][INFO] - Training Epoch: 5/10, step 547/574 completed (loss: 0.1636575162410736, acc: 0.9736841917037964)
[2024-11-13 06:40:46,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:46,344][root][INFO] - Training Epoch: 5/10, step 548/574 completed (loss: 0.07415427267551422, acc: 0.9677419066429138)
[2024-11-13 06:40:46,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:46,762][root][INFO] - Training Epoch: 5/10, step 549/574 completed (loss: 0.016179313883185387, acc: 1.0)
[2024-11-13 06:40:46,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:47,144][root][INFO] - Training Epoch: 5/10, step 550/574 completed (loss: 0.07691251486539841, acc: 1.0)
[2024-11-13 06:40:47,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:47,527][root][INFO] - Training Epoch: 5/10, step 551/574 completed (loss: 0.04329835996031761, acc: 1.0)
[2024-11-13 06:40:47,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:47,913][root][INFO] - Training Epoch: 5/10, step 552/574 completed (loss: 0.1187594011425972, acc: 0.9714285731315613)
[2024-11-13 06:40:48,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:48,354][root][INFO] - Training Epoch: 5/10, step 553/574 completed (loss: 0.3932377099990845, acc: 0.8759124279022217)
[2024-11-13 06:40:48,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:48,742][root][INFO] - Training Epoch: 5/10, step 554/574 completed (loss: 0.2296876311302185, acc: 0.9172413945198059)
[2024-11-13 06:40:48,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:49,155][root][INFO] - Training Epoch: 5/10, step 555/574 completed (loss: 0.4563415050506592, acc: 0.8714285492897034)
[2024-11-13 06:40:49,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:49,500][root][INFO] - Training Epoch: 5/10, step 556/574 completed (loss: 0.41446027159690857, acc: 0.8741722106933594)
[2024-11-13 06:40:49,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:49,879][root][INFO] - Training Epoch: 5/10, step 557/574 completed (loss: 0.17072971165180206, acc: 0.9487179517745972)
[2024-11-13 06:40:50,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:50,227][root][INFO] - Training Epoch: 5/10, step 558/574 completed (loss: 0.11973864585161209, acc: 0.9599999785423279)
[2024-11-13 06:40:50,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:50,602][root][INFO] - Training Epoch: 5/10, step 559/574 completed (loss: 0.12475783377885818, acc: 0.9615384340286255)
[2024-11-13 06:40:50,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:50,993][root][INFO] - Training Epoch: 5/10, step 560/574 completed (loss: 0.04989737272262573, acc: 0.9615384340286255)
[2024-11-13 06:40:51,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:51,376][root][INFO] - Training Epoch: 5/10, step 561/574 completed (loss: 0.05491163954138756, acc: 0.9743589758872986)
[2024-11-13 06:40:51,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:51,763][root][INFO] - Training Epoch: 5/10, step 562/574 completed (loss: 0.24397704005241394, acc: 0.9111111164093018)
[2024-11-13 06:40:51,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:52,150][root][INFO] - Training Epoch: 5/10, step 563/574 completed (loss: 0.40133199095726013, acc: 0.9090909361839294)
[2024-11-13 06:40:53,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:53,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:54,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:54,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:55,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:55,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:56,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:56,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:57,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:57,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:58,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:58,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:59,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:40:59,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:00,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:00,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:01,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:01,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:02,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:02,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:03,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:03,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:04,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:05,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:05,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:05,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:06,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:06,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:07,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:07,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:08,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:08,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:09,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:09,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:10,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:10,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:11,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:11,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:12,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:12,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:13,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:13,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:14,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:14,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:15,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:16,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:16,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:17,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:17,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:18,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:18,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:19,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:19,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:20,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:20,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:21,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:21,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:22,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:22,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:23,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:23,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:24,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:24,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:25,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:25,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:26,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:26,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:27,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:27,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:28,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:28,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:29,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:29,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:30,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:30,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:31,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:31,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:32,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:32,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:33,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:33,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:34,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:34,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:34,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:35,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:36,313][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4086, device='cuda:0') eval_epoch_loss=tensor(0.8790, device='cuda:0') eval_epoch_acc=tensor(0.7954, device='cuda:0')
[2024-11-13 06:41:36,315][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:41:36,315][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:41:36,759][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_5_step_564_loss_0.8790475726127625/model.pt
[2024-11-13 06:41:36,770][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:41:36,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:37,220][root][INFO] - Training Epoch: 5/10, step 564/574 completed (loss: 0.18509747087955475, acc: 0.9583333134651184)
[2024-11-13 06:41:37,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:37,627][root][INFO] - Training Epoch: 5/10, step 565/574 completed (loss: 0.18861913681030273, acc: 0.9482758641242981)
[2024-11-13 06:41:37,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:38,041][root][INFO] - Training Epoch: 5/10, step 566/574 completed (loss: 0.23414795100688934, acc: 0.9642857313156128)
[2024-11-13 06:41:38,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:38,403][root][INFO] - Training Epoch: 5/10, step 567/574 completed (loss: 0.12118512392044067, acc: 0.9736841917037964)
[2024-11-13 06:41:38,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:38,760][root][INFO] - Training Epoch: 5/10, step 568/574 completed (loss: 0.06381632387638092, acc: 0.9629629850387573)
[2024-11-13 06:41:38,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:39,266][root][INFO] - Training Epoch: 5/10, step 569/574 completed (loss: 0.25029635429382324, acc: 0.9144384860992432)
[2024-11-13 06:41:39,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:39,673][root][INFO] - Training Epoch: 5/10, step 570/574 completed (loss: 0.05934610590338707, acc: 0.9677419066429138)
[2024-11-13 06:41:39,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:40,061][root][INFO] - Training Epoch: 5/10, step 571/574 completed (loss: 0.19284440577030182, acc: 0.94017094373703)
[2024-11-13 06:41:40,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:40,421][root][INFO] - Training Epoch: 5/10, step 572/574 completed (loss: 0.49914029240608215, acc: 0.8520408272743225)
[2024-11-13 06:41:40,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:40,851][root][INFO] - Training Epoch: 5/10, step 573/574 completed (loss: 0.4161231517791748, acc: 0.8553459048271179)
[2024-11-13 06:41:41,405][slam_llm.utils.train_utils][INFO] - Epoch 5: train_perplexity=1.4414, train_epoch_loss=0.3656, epoch time 465.5904118940234s
[2024-11-13 06:41:41,405][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2024-11-13 06:41:41,405][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 17 GB
[2024-11-13 06:41:41,406][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2024-11-13 06:41:41,406][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 15
[2024-11-13 06:41:41,406][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-13 06:41:42,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:42,290][root][INFO] - Training Epoch: 6/10, step 0/574 completed (loss: 0.16413013637065887, acc: 0.9629629850387573)
[2024-11-13 06:41:42,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:42,698][root][INFO] - Training Epoch: 6/10, step 1/574 completed (loss: 0.17423202097415924, acc: 0.9200000166893005)
[2024-11-13 06:41:42,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:43,102][root][INFO] - Training Epoch: 6/10, step 2/574 completed (loss: 0.5197283029556274, acc: 0.8918918967247009)
[2024-11-13 06:41:43,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:43,544][root][INFO] - Training Epoch: 6/10, step 3/574 completed (loss: 0.15645532310009003, acc: 0.9736841917037964)
[2024-11-13 06:41:43,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:43,977][root][INFO] - Training Epoch: 6/10, step 4/574 completed (loss: 0.12030471116304398, acc: 0.9729729890823364)
[2024-11-13 06:41:44,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:44,385][root][INFO] - Training Epoch: 6/10, step 5/574 completed (loss: 0.0929672047495842, acc: 0.9285714030265808)
[2024-11-13 06:41:44,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:44,784][root][INFO] - Training Epoch: 6/10, step 6/574 completed (loss: 0.5458990335464478, acc: 0.8163265585899353)
[2024-11-13 06:41:44,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:45,156][root][INFO] - Training Epoch: 6/10, step 7/574 completed (loss: 0.43942415714263916, acc: 0.9333333373069763)
[2024-11-13 06:41:45,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:45,613][root][INFO] - Training Epoch: 6/10, step 8/574 completed (loss: 0.00785621628165245, acc: 1.0)
[2024-11-13 06:41:45,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:46,044][root][INFO] - Training Epoch: 6/10, step 9/574 completed (loss: 0.0049683027900755405, acc: 1.0)
[2024-11-13 06:41:46,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:46,412][root][INFO] - Training Epoch: 6/10, step 10/574 completed (loss: 0.02351897768676281, acc: 1.0)
[2024-11-13 06:41:46,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:46,791][root][INFO] - Training Epoch: 6/10, step 11/574 completed (loss: 0.14571928977966309, acc: 0.9487179517745972)
[2024-11-13 06:41:46,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:47,179][root][INFO] - Training Epoch: 6/10, step 12/574 completed (loss: 0.07011067867279053, acc: 0.9696969985961914)
[2024-11-13 06:41:47,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:47,594][root][INFO] - Training Epoch: 6/10, step 13/574 completed (loss: 0.18051938712596893, acc: 0.9347826242446899)
[2024-11-13 06:41:47,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:47,985][root][INFO] - Training Epoch: 6/10, step 14/574 completed (loss: 0.05068017914891243, acc: 1.0)
[2024-11-13 06:41:48,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:48,422][root][INFO] - Training Epoch: 6/10, step 15/574 completed (loss: 0.17952664196491241, acc: 0.9591836929321289)
[2024-11-13 06:41:48,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:48,835][root][INFO] - Training Epoch: 6/10, step 16/574 completed (loss: 0.021272573620080948, acc: 1.0)
[2024-11-13 06:41:48,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:49,205][root][INFO] - Training Epoch: 6/10, step 17/574 completed (loss: 0.19243992865085602, acc: 0.9166666865348816)
[2024-11-13 06:41:49,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:49,512][root][INFO] - Training Epoch: 6/10, step 18/574 completed (loss: 0.287265419960022, acc: 0.9444444179534912)
[2024-11-13 06:41:49,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:49,869][root][INFO] - Training Epoch: 6/10, step 19/574 completed (loss: 0.07829317450523376, acc: 0.9473684430122375)
[2024-11-13 06:41:50,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:50,269][root][INFO] - Training Epoch: 6/10, step 20/574 completed (loss: 0.4476250112056732, acc: 0.8846153616905212)
[2024-11-13 06:41:50,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:50,676][root][INFO] - Training Epoch: 6/10, step 21/574 completed (loss: 0.3149128556251526, acc: 0.8965517282485962)
[2024-11-13 06:41:50,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:51,076][root][INFO] - Training Epoch: 6/10, step 22/574 completed (loss: 0.28511515259742737, acc: 0.9200000166893005)
[2024-11-13 06:41:51,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:51,503][root][INFO] - Training Epoch: 6/10, step 23/574 completed (loss: 0.08992180973291397, acc: 0.9523809552192688)
[2024-11-13 06:41:51,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:51,891][root][INFO] - Training Epoch: 6/10, step 24/574 completed (loss: 0.02525775320827961, acc: 1.0)
[2024-11-13 06:41:52,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:52,312][root][INFO] - Training Epoch: 6/10, step 25/574 completed (loss: 0.6001118421554565, acc: 0.8301886916160583)
[2024-11-13 06:41:52,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:52,728][root][INFO] - Training Epoch: 6/10, step 26/574 completed (loss: 0.675419270992279, acc: 0.7808219194412231)
[2024-11-13 06:41:53,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:54,527][root][INFO] - Training Epoch: 6/10, step 27/574 completed (loss: 0.9081634879112244, acc: 0.7588932514190674)
[2024-11-13 06:41:54,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:54,882][root][INFO] - Training Epoch: 6/10, step 28/574 completed (loss: 0.20845894515514374, acc: 0.9534883499145508)
[2024-11-13 06:41:55,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:55,252][root][INFO] - Training Epoch: 6/10, step 29/574 completed (loss: 0.3861868381500244, acc: 0.8674699068069458)
[2024-11-13 06:41:55,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:55,640][root][INFO] - Training Epoch: 6/10, step 30/574 completed (loss: 0.49298232793807983, acc: 0.8641975522041321)
[2024-11-13 06:41:55,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:56,061][root][INFO] - Training Epoch: 6/10, step 31/574 completed (loss: 0.24256041646003723, acc: 0.8571428656578064)
[2024-11-13 06:41:56,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:56,427][root][INFO] - Training Epoch: 6/10, step 32/574 completed (loss: 0.6040774583816528, acc: 0.8888888955116272)
[2024-11-13 06:41:56,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:56,868][root][INFO] - Training Epoch: 6/10, step 33/574 completed (loss: 0.02669057808816433, acc: 1.0)
[2024-11-13 06:41:57,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:57,290][root][INFO] - Training Epoch: 6/10, step 34/574 completed (loss: 0.42919886112213135, acc: 0.8823529481887817)
[2024-11-13 06:41:57,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:57,712][root][INFO] - Training Epoch: 6/10, step 35/574 completed (loss: 0.3376927971839905, acc: 0.9344262480735779)
[2024-11-13 06:41:57,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:58,135][root][INFO] - Training Epoch: 6/10, step 36/574 completed (loss: 0.292539119720459, acc: 0.8888888955116272)
[2024-11-13 06:41:58,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:58,530][root][INFO] - Training Epoch: 6/10, step 37/574 completed (loss: 0.28685688972473145, acc: 0.8983050584793091)
[2024-11-13 06:41:58,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:58,970][root][INFO] - Training Epoch: 6/10, step 38/574 completed (loss: 0.18742138147354126, acc: 0.954023003578186)
[2024-11-13 06:41:59,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:59,304][root][INFO] - Training Epoch: 6/10, step 39/574 completed (loss: 0.08006040006875992, acc: 1.0)
[2024-11-13 06:41:59,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:41:59,639][root][INFO] - Training Epoch: 6/10, step 40/574 completed (loss: 0.16059178113937378, acc: 0.9615384340286255)
[2024-11-13 06:41:59,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:00,088][root][INFO] - Training Epoch: 6/10, step 41/574 completed (loss: 0.28476399183273315, acc: 0.9324324131011963)
[2024-11-13 06:42:00,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:00,562][root][INFO] - Training Epoch: 6/10, step 42/574 completed (loss: 0.4441046118736267, acc: 0.8307692408561707)
[2024-11-13 06:42:00,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:01,125][root][INFO] - Training Epoch: 6/10, step 43/574 completed (loss: 0.34431472420692444, acc: 0.8989899158477783)
[2024-11-13 06:42:01,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:01,692][root][INFO] - Training Epoch: 6/10, step 44/574 completed (loss: 0.4908304512500763, acc: 0.8556700944900513)
[2024-11-13 06:42:01,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:02,237][root][INFO] - Training Epoch: 6/10, step 45/574 completed (loss: 0.33928778767585754, acc: 0.8970588445663452)
[2024-11-13 06:42:02,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:02,600][root][INFO] - Training Epoch: 6/10, step 46/574 completed (loss: 0.18626642227172852, acc: 0.9230769276618958)
[2024-11-13 06:42:02,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:03,022][root][INFO] - Training Epoch: 6/10, step 47/574 completed (loss: 0.1187467947602272, acc: 0.9259259104728699)
[2024-11-13 06:42:03,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:03,431][root][INFO] - Training Epoch: 6/10, step 48/574 completed (loss: 0.10156156122684479, acc: 0.9642857313156128)
[2024-11-13 06:42:03,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:03,793][root][INFO] - Training Epoch: 6/10, step 49/574 completed (loss: 0.07275271415710449, acc: 1.0)
[2024-11-13 06:42:03,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:04,170][root][INFO] - Training Epoch: 6/10, step 50/574 completed (loss: 0.3392811715602875, acc: 0.8947368264198303)
[2024-11-13 06:42:04,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:04,611][root][INFO] - Training Epoch: 6/10, step 51/574 completed (loss: 0.4094468355178833, acc: 0.9047619104385376)
[2024-11-13 06:42:04,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:05,034][root][INFO] - Training Epoch: 6/10, step 52/574 completed (loss: 0.5478509664535522, acc: 0.8591549396514893)
[2024-11-13 06:42:05,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:05,689][root][INFO] - Training Epoch: 6/10, step 53/574 completed (loss: 1.4045345783233643, acc: 0.5799999833106995)
[2024-11-13 06:42:05,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:06,102][root][INFO] - Training Epoch: 6/10, step 54/574 completed (loss: 0.3856634199619293, acc: 0.837837815284729)
[2024-11-13 06:42:06,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:06,538][root][INFO] - Training Epoch: 6/10, step 55/574 completed (loss: 0.19738195836544037, acc: 0.9230769276618958)
[2024-11-13 06:42:09,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:10,991][root][INFO] - Training Epoch: 6/10, step 56/574 completed (loss: 0.8084387183189392, acc: 0.7303754091262817)
[2024-11-13 06:42:11,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:12,841][root][INFO] - Training Epoch: 6/10, step 57/574 completed (loss: 1.2720988988876343, acc: 0.6623093485832214)
[2024-11-13 06:42:13,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:13,773][root][INFO] - Training Epoch: 6/10, step 58/574 completed (loss: 0.7046825885772705, acc: 0.8011363744735718)
[2024-11-13 06:42:14,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:14,629][root][INFO] - Training Epoch: 6/10, step 59/574 completed (loss: 0.36062201857566833, acc: 0.8897058963775635)
[2024-11-13 06:42:15,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:15,464][root][INFO] - Training Epoch: 6/10, step 60/574 completed (loss: 0.6754562854766846, acc: 0.804347813129425)
[2024-11-13 06:42:15,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:16,035][root][INFO] - Training Epoch: 6/10, step 61/574 completed (loss: 0.5432552695274353, acc: 0.8500000238418579)
[2024-11-13 06:42:16,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:16,402][root][INFO] - Training Epoch: 6/10, step 62/574 completed (loss: 0.13625696301460266, acc: 0.9411764740943909)
[2024-11-13 06:42:16,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:16,846][root][INFO] - Training Epoch: 6/10, step 63/574 completed (loss: 0.07341176271438599, acc: 1.0)
[2024-11-13 06:42:17,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:17,264][root][INFO] - Training Epoch: 6/10, step 64/574 completed (loss: 0.12898944318294525, acc: 0.9375)
[2024-11-13 06:42:17,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:17,668][root][INFO] - Training Epoch: 6/10, step 65/574 completed (loss: 0.0983920693397522, acc: 0.9655172228813171)
[2024-11-13 06:42:17,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:18,073][root][INFO] - Training Epoch: 6/10, step 66/574 completed (loss: 0.4737692177295685, acc: 0.8928571343421936)
[2024-11-13 06:42:18,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:18,469][root][INFO] - Training Epoch: 6/10, step 67/574 completed (loss: 0.3135644197463989, acc: 0.8333333134651184)
[2024-11-13 06:42:18,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:18,877][root][INFO] - Training Epoch: 6/10, step 68/574 completed (loss: 0.10683897137641907, acc: 0.9599999785423279)
[2024-11-13 06:42:19,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:19,284][root][INFO] - Training Epoch: 6/10, step 69/574 completed (loss: 0.2561957538127899, acc: 0.8888888955116272)
[2024-11-13 06:42:19,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:19,697][root][INFO] - Training Epoch: 6/10, step 70/574 completed (loss: 0.28383588790893555, acc: 0.9090909361839294)
[2024-11-13 06:42:19,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:20,116][root][INFO] - Training Epoch: 6/10, step 71/574 completed (loss: 0.823841392993927, acc: 0.720588207244873)
[2024-11-13 06:42:20,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:20,522][root][INFO] - Training Epoch: 6/10, step 72/574 completed (loss: 0.6374908089637756, acc: 0.8015872836112976)
[2024-11-13 06:42:20,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:20,913][root][INFO] - Training Epoch: 6/10, step 73/574 completed (loss: 1.2478315830230713, acc: 0.6461538672447205)
[2024-11-13 06:42:21,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:21,277][root][INFO] - Training Epoch: 6/10, step 74/574 completed (loss: 0.8869263529777527, acc: 0.7448979616165161)
[2024-11-13 06:42:21,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:21,665][root][INFO] - Training Epoch: 6/10, step 75/574 completed (loss: 0.9953886866569519, acc: 0.7164179086685181)
[2024-11-13 06:42:21,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:22,172][root][INFO] - Training Epoch: 6/10, step 76/574 completed (loss: 1.3242741823196411, acc: 0.6167883276939392)
[2024-11-13 06:42:22,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:22,551][root][INFO] - Training Epoch: 6/10, step 77/574 completed (loss: 0.006436957512050867, acc: 1.0)
[2024-11-13 06:42:22,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:22,964][root][INFO] - Training Epoch: 6/10, step 78/574 completed (loss: 0.21705812215805054, acc: 0.9583333134651184)
[2024-11-13 06:42:23,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:23,384][root][INFO] - Training Epoch: 6/10, step 79/574 completed (loss: 0.11220793426036835, acc: 0.9696969985961914)
[2024-11-13 06:42:23,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:23,822][root][INFO] - Training Epoch: 6/10, step 80/574 completed (loss: 0.024866610765457153, acc: 1.0)
[2024-11-13 06:42:24,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:24,264][root][INFO] - Training Epoch: 6/10, step 81/574 completed (loss: 0.22370941936969757, acc: 0.9038461446762085)
[2024-11-13 06:42:24,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:24,739][root][INFO] - Training Epoch: 6/10, step 82/574 completed (loss: 0.41109931468963623, acc: 0.9038461446762085)
[2024-11-13 06:42:24,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:25,148][root][INFO] - Training Epoch: 6/10, step 83/574 completed (loss: 0.2390417605638504, acc: 0.90625)
[2024-11-13 06:42:25,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:25,534][root][INFO] - Training Epoch: 6/10, step 84/574 completed (loss: 0.2525959610939026, acc: 0.9420289993286133)
[2024-11-13 06:42:25,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:26,003][root][INFO] - Training Epoch: 6/10, step 85/574 completed (loss: 0.14746226370334625, acc: 0.9800000190734863)
[2024-11-13 06:42:26,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:26,446][root][INFO] - Training Epoch: 6/10, step 86/574 completed (loss: 0.0965641662478447, acc: 0.95652174949646)
[2024-11-13 06:42:26,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:27,111][root][INFO] - Training Epoch: 6/10, step 87/574 completed (loss: 0.6626524329185486, acc: 0.7599999904632568)
[2024-11-13 06:42:27,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:27,574][root][INFO] - Training Epoch: 6/10, step 88/574 completed (loss: 0.8181881904602051, acc: 0.7961165308952332)
[2024-11-13 06:42:28,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:29,342][root][INFO] - Training Epoch: 6/10, step 89/574 completed (loss: 0.7484424710273743, acc: 0.8058252334594727)
[2024-11-13 06:42:30,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:30,624][root][INFO] - Training Epoch: 6/10, step 90/574 completed (loss: 1.0598952770233154, acc: 0.6720430254936218)
[2024-11-13 06:42:31,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:31,872][root][INFO] - Training Epoch: 6/10, step 91/574 completed (loss: 0.7499374747276306, acc: 0.8103448152542114)
[2024-11-13 06:42:32,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:33,026][root][INFO] - Training Epoch: 6/10, step 92/574 completed (loss: 0.6647742390632629, acc: 0.821052610874176)
[2024-11-13 06:42:33,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:34,615][root][INFO] - Training Epoch: 6/10, step 93/574 completed (loss: 0.9346024394035339, acc: 0.7029703259468079)
[2024-11-13 06:42:34,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:35,055][root][INFO] - Training Epoch: 6/10, step 94/574 completed (loss: 0.6509462594985962, acc: 0.8064516186714172)
[2024-11-13 06:42:35,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:35,495][root][INFO] - Training Epoch: 6/10, step 95/574 completed (loss: 0.5101421475410461, acc: 0.8260869383811951)
[2024-11-13 06:42:35,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:35,910][root][INFO] - Training Epoch: 6/10, step 96/574 completed (loss: 0.7720345258712769, acc: 0.7478991746902466)
[2024-11-13 06:42:36,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:36,345][root][INFO] - Training Epoch: 6/10, step 97/574 completed (loss: 0.8233324289321899, acc: 0.75)
[2024-11-13 06:42:36,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:36,841][root][INFO] - Training Epoch: 6/10, step 98/574 completed (loss: 0.9750384092330933, acc: 0.6861313581466675)
[2024-11-13 06:42:37,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:37,256][root][INFO] - Training Epoch: 6/10, step 99/574 completed (loss: 0.8416202068328857, acc: 0.7164179086685181)
[2024-11-13 06:42:37,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:37,675][root][INFO] - Training Epoch: 6/10, step 100/574 completed (loss: 0.1217506155371666, acc: 1.0)
[2024-11-13 06:42:37,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:38,074][root][INFO] - Training Epoch: 6/10, step 101/574 completed (loss: 0.004042783286422491, acc: 1.0)
[2024-11-13 06:42:38,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:38,496][root][INFO] - Training Epoch: 6/10, step 102/574 completed (loss: 0.024086011573672295, acc: 1.0)
[2024-11-13 06:42:38,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:38,851][root][INFO] - Training Epoch: 6/10, step 103/574 completed (loss: 0.10034442692995071, acc: 0.9772727489471436)
[2024-11-13 06:42:39,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:39,226][root][INFO] - Training Epoch: 6/10, step 104/574 completed (loss: 0.2943955063819885, acc: 0.8965517282485962)
[2024-11-13 06:42:39,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:39,638][root][INFO] - Training Epoch: 6/10, step 105/574 completed (loss: 0.06342804431915283, acc: 0.9767441749572754)
[2024-11-13 06:42:39,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:40,063][root][INFO] - Training Epoch: 6/10, step 106/574 completed (loss: 0.15729598701000214, acc: 0.9599999785423279)
[2024-11-13 06:42:40,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:40,457][root][INFO] - Training Epoch: 6/10, step 107/574 completed (loss: 0.015085253864526749, acc: 1.0)
[2024-11-13 06:42:40,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:40,892][root][INFO] - Training Epoch: 6/10, step 108/574 completed (loss: 0.006130577996373177, acc: 1.0)
[2024-11-13 06:42:41,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:41,318][root][INFO] - Training Epoch: 6/10, step 109/574 completed (loss: 0.17960727214813232, acc: 0.9285714030265808)
[2024-11-13 06:42:41,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:41,725][root][INFO] - Training Epoch: 6/10, step 110/574 completed (loss: 0.11859623342752457, acc: 0.9692307710647583)
[2024-11-13 06:42:41,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:42,271][root][INFO] - Training Epoch: 6/10, step 111/574 completed (loss: 0.33104923367500305, acc: 0.8947368264198303)
[2024-11-13 06:42:42,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:42,719][root][INFO] - Training Epoch: 6/10, step 112/574 completed (loss: 0.33258360624313354, acc: 0.9298245906829834)
[2024-11-13 06:42:42,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:43,121][root][INFO] - Training Epoch: 6/10, step 113/574 completed (loss: 0.22154732048511505, acc: 0.9487179517745972)
[2024-11-13 06:42:43,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:43,647][root][INFO] - Training Epoch: 6/10, step 114/574 completed (loss: 0.25414371490478516, acc: 0.918367326259613)
[2024-11-13 06:42:43,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:44,052][root][INFO] - Training Epoch: 6/10, step 115/574 completed (loss: 0.015089685097336769, acc: 1.0)
[2024-11-13 06:42:44,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:44,455][root][INFO] - Training Epoch: 6/10, step 116/574 completed (loss: 0.5835970044136047, acc: 0.8095238208770752)
[2024-11-13 06:42:44,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:44,922][root][INFO] - Training Epoch: 6/10, step 117/574 completed (loss: 0.4629981219768524, acc: 0.8455284833908081)
[2024-11-13 06:42:45,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:45,359][root][INFO] - Training Epoch: 6/10, step 118/574 completed (loss: 0.11246778070926666, acc: 0.9516128897666931)
[2024-11-13 06:42:46,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:46,655][root][INFO] - Training Epoch: 6/10, step 119/574 completed (loss: 0.7126198410987854, acc: 0.8174905180931091)
[2024-11-13 06:42:46,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:47,094][root][INFO] - Training Epoch: 6/10, step 120/574 completed (loss: 0.2297201305627823, acc: 0.8933333158493042)
[2024-11-13 06:42:47,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:47,647][root][INFO] - Training Epoch: 6/10, step 121/574 completed (loss: 0.21479295194149017, acc: 0.9230769276618958)
[2024-11-13 06:42:47,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:48,021][root][INFO] - Training Epoch: 6/10, step 122/574 completed (loss: 0.013313259929418564, acc: 1.0)
[2024-11-13 06:42:48,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:48,426][root][INFO] - Training Epoch: 6/10, step 123/574 completed (loss: 0.04205476865172386, acc: 1.0)
[2024-11-13 06:42:48,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:48,822][root][INFO] - Training Epoch: 6/10, step 124/574 completed (loss: 0.7312722206115723, acc: 0.7484662532806396)
[2024-11-13 06:42:49,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:49,331][root][INFO] - Training Epoch: 6/10, step 125/574 completed (loss: 1.0107790231704712, acc: 0.7152777910232544)
[2024-11-13 06:42:49,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:49,841][root][INFO] - Training Epoch: 6/10, step 126/574 completed (loss: 0.942500114440918, acc: 0.699999988079071)
[2024-11-13 06:42:50,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:50,273][root][INFO] - Training Epoch: 6/10, step 127/574 completed (loss: 0.746057391166687, acc: 0.7857142686843872)
[2024-11-13 06:42:50,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:50,705][root][INFO] - Training Epoch: 6/10, step 128/574 completed (loss: 0.7170053124427795, acc: 0.8153846263885498)
[2024-11-13 06:42:50,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:51,245][root][INFO] - Training Epoch: 6/10, step 129/574 completed (loss: 0.6615997552871704, acc: 0.8308823704719543)
[2024-11-13 06:42:51,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:51,537][root][INFO] - Training Epoch: 6/10, step 130/574 completed (loss: 0.2591668665409088, acc: 0.9230769276618958)
[2024-11-13 06:42:51,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:51,964][root][INFO] - Training Epoch: 6/10, step 131/574 completed (loss: 0.06905750930309296, acc: 1.0)
[2024-11-13 06:42:52,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:52,371][root][INFO] - Training Epoch: 6/10, step 132/574 completed (loss: 0.11541665345430374, acc: 0.96875)
[2024-11-13 06:42:53,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:53,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:54,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:54,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:55,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:55,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:56,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:56,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:57,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:57,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:58,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:58,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:59,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:42:59,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:00,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:00,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:01,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:01,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:02,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:02,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:02,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:03,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:03,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:04,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:04,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:05,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:05,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:06,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:07,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:07,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:08,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:08,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:09,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:09,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:09,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:10,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:10,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:11,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:11,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:12,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:13,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:13,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:13,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:14,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:14,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:15,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:15,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:16,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:16,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:17,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:17,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:18,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:18,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:18,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:19,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:19,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:20,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:20,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:21,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:21,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:22,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:22,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:23,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:24,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:24,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:24,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:25,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:26,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:26,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:27,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:27,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:28,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:28,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:28,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:29,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:29,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:30,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:30,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:31,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:31,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:32,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:32,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:33,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:33,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:33,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:34,722][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2010, device='cuda:0') eval_epoch_loss=tensor(0.7889, device='cuda:0') eval_epoch_acc=tensor(0.8103, device='cuda:0')
[2024-11-13 06:43:34,723][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:43:34,724][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:43:35,168][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_6_step_133_loss_0.7888891696929932/model.pt
[2024-11-13 06:43:35,173][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:43:35,174][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 6 is 0.8103231191635132
[2024-11-13 06:43:35,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:35,611][root][INFO] - Training Epoch: 6/10, step 133/574 completed (loss: 0.13034766912460327, acc: 0.95652174949646)
[2024-11-13 06:43:35,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:36,053][root][INFO] - Training Epoch: 6/10, step 134/574 completed (loss: 0.21327173709869385, acc: 0.9142857193946838)
[2024-11-13 06:43:36,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:36,460][root][INFO] - Training Epoch: 6/10, step 135/574 completed (loss: 0.06855209171772003, acc: 1.0)
[2024-11-13 06:43:36,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:36,868][root][INFO] - Training Epoch: 6/10, step 136/574 completed (loss: 0.14204466342926025, acc: 0.976190447807312)
[2024-11-13 06:43:37,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:37,224][root][INFO] - Training Epoch: 6/10, step 137/574 completed (loss: 0.46280133724212646, acc: 0.8666666746139526)
[2024-11-13 06:43:37,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:37,579][root][INFO] - Training Epoch: 6/10, step 138/574 completed (loss: 0.05187477171421051, acc: 1.0)
[2024-11-13 06:43:37,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:37,978][root][INFO] - Training Epoch: 6/10, step 139/574 completed (loss: 0.1375010758638382, acc: 0.9047619104385376)
[2024-11-13 06:43:38,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:38,349][root][INFO] - Training Epoch: 6/10, step 140/574 completed (loss: 0.16591916978359222, acc: 0.9230769276618958)
[2024-11-13 06:43:38,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:38,759][root][INFO] - Training Epoch: 6/10, step 141/574 completed (loss: 0.1834137737751007, acc: 0.9354838728904724)
[2024-11-13 06:43:38,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:39,226][root][INFO] - Training Epoch: 6/10, step 142/574 completed (loss: 0.164855495095253, acc: 0.9459459185600281)
[2024-11-13 06:43:39,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:40,003][root][INFO] - Training Epoch: 6/10, step 143/574 completed (loss: 0.8053244948387146, acc: 0.7631579041481018)
[2024-11-13 06:43:40,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:40,424][root][INFO] - Training Epoch: 6/10, step 144/574 completed (loss: 0.6493993997573853, acc: 0.7985074520111084)
[2024-11-13 06:43:40,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:40,857][root][INFO] - Training Epoch: 6/10, step 145/574 completed (loss: 0.3862786293029785, acc: 0.8775510191917419)
[2024-11-13 06:43:41,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:41,497][root][INFO] - Training Epoch: 6/10, step 146/574 completed (loss: 0.6032940745353699, acc: 0.8404255509376526)
[2024-11-13 06:43:41,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:41,914][root][INFO] - Training Epoch: 6/10, step 147/574 completed (loss: 0.4940909147262573, acc: 0.8428571224212646)
[2024-11-13 06:43:42,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:42,260][root][INFO] - Training Epoch: 6/10, step 148/574 completed (loss: 0.06047895550727844, acc: 1.0)
[2024-11-13 06:43:42,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:42,678][root][INFO] - Training Epoch: 6/10, step 149/574 completed (loss: 0.7310006618499756, acc: 0.8260869383811951)
[2024-11-13 06:43:42,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:43,051][root][INFO] - Training Epoch: 6/10, step 150/574 completed (loss: 0.10033861547708511, acc: 1.0)
[2024-11-13 06:43:43,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:43,431][root][INFO] - Training Epoch: 6/10, step 151/574 completed (loss: 0.4003762900829315, acc: 0.8913043737411499)
[2024-11-13 06:43:43,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:43,846][root][INFO] - Training Epoch: 6/10, step 152/574 completed (loss: 0.4048580527305603, acc: 0.8474576473236084)
[2024-11-13 06:43:44,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:44,239][root][INFO] - Training Epoch: 6/10, step 153/574 completed (loss: 0.6734572052955627, acc: 0.7894737124443054)
[2024-11-13 06:43:44,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:44,631][root][INFO] - Training Epoch: 6/10, step 154/574 completed (loss: 0.5086988806724548, acc: 0.8108108043670654)
[2024-11-13 06:43:44,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:45,010][root][INFO] - Training Epoch: 6/10, step 155/574 completed (loss: 0.1481298953294754, acc: 0.9642857313156128)
[2024-11-13 06:43:45,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:45,425][root][INFO] - Training Epoch: 6/10, step 156/574 completed (loss: 0.0795479565858841, acc: 0.95652174949646)
[2024-11-13 06:43:45,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:45,825][root][INFO] - Training Epoch: 6/10, step 157/574 completed (loss: 0.7339007258415222, acc: 0.8421052694320679)
[2024-11-13 06:43:47,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:48,480][root][INFO] - Training Epoch: 6/10, step 158/574 completed (loss: 0.5624578595161438, acc: 0.7837837934494019)
[2024-11-13 06:43:48,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:48,905][root][INFO] - Training Epoch: 6/10, step 159/574 completed (loss: 0.8813714385032654, acc: 0.7222222089767456)
[2024-11-13 06:43:49,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:49,468][root][INFO] - Training Epoch: 6/10, step 160/574 completed (loss: 0.8734450340270996, acc: 0.8023256063461304)
[2024-11-13 06:43:49,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:50,377][root][INFO] - Training Epoch: 6/10, step 161/574 completed (loss: 1.009642481803894, acc: 0.7058823704719543)
[2024-11-13 06:43:50,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:51,225][root][INFO] - Training Epoch: 6/10, step 162/574 completed (loss: 0.9726941585540771, acc: 0.7303370833396912)
[2024-11-13 06:43:51,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:51,655][root][INFO] - Training Epoch: 6/10, step 163/574 completed (loss: 0.28323790431022644, acc: 0.9090909361839294)
[2024-11-13 06:43:51,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:52,024][root][INFO] - Training Epoch: 6/10, step 164/574 completed (loss: 0.47240638732910156, acc: 0.9047619104385376)
[2024-11-13 06:43:52,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:52,378][root][INFO] - Training Epoch: 6/10, step 165/574 completed (loss: 0.4760451912879944, acc: 0.8965517282485962)
[2024-11-13 06:43:52,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:52,793][root][INFO] - Training Epoch: 6/10, step 166/574 completed (loss: 0.07345224171876907, acc: 0.9795918464660645)
[2024-11-13 06:43:52,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:53,175][root][INFO] - Training Epoch: 6/10, step 167/574 completed (loss: 0.0917515754699707, acc: 0.9800000190734863)
[2024-11-13 06:43:53,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:53,685][root][INFO] - Training Epoch: 6/10, step 168/574 completed (loss: 0.190662682056427, acc: 0.9444444179534912)
[2024-11-13 06:43:53,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:54,129][root][INFO] - Training Epoch: 6/10, step 169/574 completed (loss: 0.8636729121208191, acc: 0.7941176295280457)
[2024-11-13 06:43:55,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:55,797][root][INFO] - Training Epoch: 6/10, step 170/574 completed (loss: 0.7324105501174927, acc: 0.8219178318977356)
[2024-11-13 06:43:55,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:56,162][root][INFO] - Training Epoch: 6/10, step 171/574 completed (loss: 0.05462907627224922, acc: 0.9583333134651184)
[2024-11-13 06:43:56,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:56,556][root][INFO] - Training Epoch: 6/10, step 172/574 completed (loss: 0.23584841191768646, acc: 0.9259259104728699)
[2024-11-13 06:43:56,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:57,017][root][INFO] - Training Epoch: 6/10, step 173/574 completed (loss: 0.31799712777137756, acc: 0.8928571343421936)
[2024-11-13 06:43:57,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:57,827][root][INFO] - Training Epoch: 6/10, step 174/574 completed (loss: 0.850903332233429, acc: 0.7876105904579163)
[2024-11-13 06:43:58,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:58,251][root][INFO] - Training Epoch: 6/10, step 175/574 completed (loss: 0.5742049813270569, acc: 0.8405796885490417)
[2024-11-13 06:43:58,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:43:58,684][root][INFO] - Training Epoch: 6/10, step 176/574 completed (loss: 0.26262572407722473, acc: 0.9090909361839294)
[2024-11-13 06:43:59,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:00,138][root][INFO] - Training Epoch: 6/10, step 177/574 completed (loss: 1.2432905435562134, acc: 0.6564885377883911)
[2024-11-13 06:44:00,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:01,189][root][INFO] - Training Epoch: 6/10, step 178/574 completed (loss: 0.6720686554908752, acc: 0.7851851582527161)
[2024-11-13 06:44:01,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:01,611][root][INFO] - Training Epoch: 6/10, step 179/574 completed (loss: 0.2560977041721344, acc: 0.9016393423080444)
[2024-11-13 06:44:01,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:01,963][root][INFO] - Training Epoch: 6/10, step 180/574 completed (loss: 0.024312442168593407, acc: 1.0)
[2024-11-13 06:44:02,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:02,378][root][INFO] - Training Epoch: 6/10, step 181/574 completed (loss: 0.03337610885500908, acc: 1.0)
[2024-11-13 06:44:02,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:02,802][root][INFO] - Training Epoch: 6/10, step 182/574 completed (loss: 0.1421700417995453, acc: 0.9642857313156128)
[2024-11-13 06:44:03,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:03,210][root][INFO] - Training Epoch: 6/10, step 183/574 completed (loss: 0.14238116145133972, acc: 0.9634146094322205)
[2024-11-13 06:44:03,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:03,615][root][INFO] - Training Epoch: 6/10, step 184/574 completed (loss: 0.5614417791366577, acc: 0.8338368535041809)
[2024-11-13 06:44:03,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:04,020][root][INFO] - Training Epoch: 6/10, step 185/574 completed (loss: 0.595280647277832, acc: 0.8097983002662659)
[2024-11-13 06:44:04,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:04,708][root][INFO] - Training Epoch: 6/10, step 186/574 completed (loss: 0.5751358270645142, acc: 0.8374999761581421)
[2024-11-13 06:44:05,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:05,448][root][INFO] - Training Epoch: 6/10, step 187/574 completed (loss: 0.5958431363105774, acc: 0.8461538553237915)
[2024-11-13 06:44:05,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:05,986][root][INFO] - Training Epoch: 6/10, step 188/574 completed (loss: 0.5316259860992432, acc: 0.8505337834358215)
[2024-11-13 06:44:06,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:06,353][root][INFO] - Training Epoch: 6/10, step 189/574 completed (loss: 0.15319259464740753, acc: 0.9599999785423279)
[2024-11-13 06:44:06,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:07,183][root][INFO] - Training Epoch: 6/10, step 190/574 completed (loss: 0.6962522268295288, acc: 0.8139534592628479)
[2024-11-13 06:44:07,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:08,449][root][INFO] - Training Epoch: 6/10, step 191/574 completed (loss: 0.9518948197364807, acc: 0.7460317611694336)
[2024-11-13 06:44:09,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:09,930][root][INFO] - Training Epoch: 6/10, step 192/574 completed (loss: 0.9074182510375977, acc: 0.7196969985961914)
[2024-11-13 06:44:10,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:11,117][root][INFO] - Training Epoch: 6/10, step 193/574 completed (loss: 0.47680431604385376, acc: 0.8823529481887817)
[2024-11-13 06:44:12,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:12,877][root][INFO] - Training Epoch: 6/10, step 194/574 completed (loss: 0.7425179481506348, acc: 0.7654321193695068)
[2024-11-13 06:44:13,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:14,419][root][INFO] - Training Epoch: 6/10, step 195/574 completed (loss: 0.333709180355072, acc: 0.8870967626571655)
[2024-11-13 06:44:14,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:14,807][root][INFO] - Training Epoch: 6/10, step 196/574 completed (loss: 0.12850984930992126, acc: 0.9642857313156128)
[2024-11-13 06:44:15,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:15,237][root][INFO] - Training Epoch: 6/10, step 197/574 completed (loss: 0.29830777645111084, acc: 0.8999999761581421)
[2024-11-13 06:44:15,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:15,690][root][INFO] - Training Epoch: 6/10, step 198/574 completed (loss: 0.3245824873447418, acc: 0.9264705777168274)
[2024-11-13 06:44:15,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:16,100][root][INFO] - Training Epoch: 6/10, step 199/574 completed (loss: 0.6571733355522156, acc: 0.8161764740943909)
[2024-11-13 06:44:16,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:16,441][root][INFO] - Training Epoch: 6/10, step 200/574 completed (loss: 0.5215821862220764, acc: 0.8559321761131287)
[2024-11-13 06:44:16,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:16,835][root][INFO] - Training Epoch: 6/10, step 201/574 completed (loss: 0.6271536350250244, acc: 0.8283582329750061)
[2024-11-13 06:44:17,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:17,286][root][INFO] - Training Epoch: 6/10, step 202/574 completed (loss: 0.6197432279586792, acc: 0.8155339956283569)
[2024-11-13 06:44:17,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:17,667][root][INFO] - Training Epoch: 6/10, step 203/574 completed (loss: 0.3745008707046509, acc: 0.8730158805847168)
[2024-11-13 06:44:17,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:18,096][root][INFO] - Training Epoch: 6/10, step 204/574 completed (loss: 0.17541946470737457, acc: 0.9450549483299255)
[2024-11-13 06:44:18,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:18,538][root][INFO] - Training Epoch: 6/10, step 205/574 completed (loss: 0.2833070158958435, acc: 0.9058296084403992)
[2024-11-13 06:44:18,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:19,068][root][INFO] - Training Epoch: 6/10, step 206/574 completed (loss: 0.583129346370697, acc: 0.8031495809555054)
[2024-11-13 06:44:19,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:19,508][root][INFO] - Training Epoch: 6/10, step 207/574 completed (loss: 0.27558282017707825, acc: 0.9137930870056152)
[2024-11-13 06:44:19,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:19,924][root][INFO] - Training Epoch: 6/10, step 208/574 completed (loss: 0.3764471113681793, acc: 0.8840579986572266)
[2024-11-13 06:44:20,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:20,395][root][INFO] - Training Epoch: 6/10, step 209/574 completed (loss: 0.3012133538722992, acc: 0.8988326787948608)
[2024-11-13 06:44:20,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:20,794][root][INFO] - Training Epoch: 6/10, step 210/574 completed (loss: 0.14922858774662018, acc: 0.967391312122345)
[2024-11-13 06:44:21,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:21,210][root][INFO] - Training Epoch: 6/10, step 211/574 completed (loss: 0.1675829142332077, acc: 0.95652174949646)
[2024-11-13 06:44:21,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:21,577][root][INFO] - Training Epoch: 6/10, step 212/574 completed (loss: 0.10438557714223862, acc: 1.0)
[2024-11-13 06:44:21,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:22,044][root][INFO] - Training Epoch: 6/10, step 213/574 completed (loss: 0.11985678225755692, acc: 0.978723406791687)
[2024-11-13 06:44:22,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:23,102][root][INFO] - Training Epoch: 6/10, step 214/574 completed (loss: 0.24175000190734863, acc: 0.9384615421295166)
[2024-11-13 06:44:23,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:23,519][root][INFO] - Training Epoch: 6/10, step 215/574 completed (loss: 0.10847780108451843, acc: 0.9594594836235046)
[2024-11-13 06:44:23,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:23,931][root][INFO] - Training Epoch: 6/10, step 216/574 completed (loss: 0.10658236593008041, acc: 0.9534883499145508)
[2024-11-13 06:44:24,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:24,728][root][INFO] - Training Epoch: 6/10, step 217/574 completed (loss: 0.10364267975091934, acc: 0.9639639854431152)
[2024-11-13 06:44:24,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:25,262][root][INFO] - Training Epoch: 6/10, step 218/574 completed (loss: 0.15543073415756226, acc: 0.9333333373069763)
[2024-11-13 06:44:25,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:25,689][root][INFO] - Training Epoch: 6/10, step 219/574 completed (loss: 0.0879172757267952, acc: 0.9696969985961914)
[2024-11-13 06:44:25,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:26,058][root][INFO] - Training Epoch: 6/10, step 220/574 completed (loss: 0.016788873821496964, acc: 1.0)
[2024-11-13 06:44:26,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:26,381][root][INFO] - Training Epoch: 6/10, step 221/574 completed (loss: 0.06366574019193649, acc: 0.9599999785423279)
[2024-11-13 06:44:26,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:26,731][root][INFO] - Training Epoch: 6/10, step 222/574 completed (loss: 0.32036083936691284, acc: 0.9038461446762085)
[2024-11-13 06:44:27,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:27,906][root][INFO] - Training Epoch: 6/10, step 223/574 completed (loss: 0.23278319835662842, acc: 0.929347813129425)
[2024-11-13 06:44:28,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:28,714][root][INFO] - Training Epoch: 6/10, step 224/574 completed (loss: 0.5285801291465759, acc: 0.8181818127632141)
[2024-11-13 06:44:29,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:29,336][root][INFO] - Training Epoch: 6/10, step 225/574 completed (loss: 0.45928093791007996, acc: 0.8617021441459656)
[2024-11-13 06:44:29,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:29,776][root][INFO] - Training Epoch: 6/10, step 226/574 completed (loss: 0.24723634123802185, acc: 0.9245283007621765)
[2024-11-13 06:44:29,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:30,159][root][INFO] - Training Epoch: 6/10, step 227/574 completed (loss: 0.15459352731704712, acc: 0.9333333373069763)
[2024-11-13 06:44:30,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:30,521][root][INFO] - Training Epoch: 6/10, step 228/574 completed (loss: 0.23105545341968536, acc: 0.930232584476471)
[2024-11-13 06:44:30,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:30,883][root][INFO] - Training Epoch: 6/10, step 229/574 completed (loss: 0.5128090381622314, acc: 0.800000011920929)
[2024-11-13 06:44:31,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:31,367][root][INFO] - Training Epoch: 6/10, step 230/574 completed (loss: 1.4089694023132324, acc: 0.5684210658073425)
[2024-11-13 06:44:31,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:31,759][root][INFO] - Training Epoch: 6/10, step 231/574 completed (loss: 1.0230952501296997, acc: 0.6666666865348816)
[2024-11-13 06:44:32,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:32,360][root][INFO] - Training Epoch: 6/10, step 232/574 completed (loss: 1.114549994468689, acc: 0.6777777671813965)
[2024-11-13 06:44:32,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:33,070][root][INFO] - Training Epoch: 6/10, step 233/574 completed (loss: 1.4756187200546265, acc: 0.6146789193153381)
[2024-11-13 06:44:33,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:33,767][root][INFO] - Training Epoch: 6/10, step 234/574 completed (loss: 0.9893532395362854, acc: 0.6769230961799622)
[2024-11-13 06:44:33,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:34,158][root][INFO] - Training Epoch: 6/10, step 235/574 completed (loss: 0.03165816143155098, acc: 1.0)
[2024-11-13 06:44:34,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:34,520][root][INFO] - Training Epoch: 6/10, step 236/574 completed (loss: 0.20793913304805756, acc: 0.9166666865348816)
[2024-11-13 06:44:34,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:34,875][root][INFO] - Training Epoch: 6/10, step 237/574 completed (loss: 0.11624148488044739, acc: 1.0)
[2024-11-13 06:44:35,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:35,274][root][INFO] - Training Epoch: 6/10, step 238/574 completed (loss: 0.374371737241745, acc: 0.8518518805503845)
[2024-11-13 06:44:35,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:35,713][root][INFO] - Training Epoch: 6/10, step 239/574 completed (loss: 0.27115169167518616, acc: 0.8571428656578064)
[2024-11-13 06:44:35,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:36,174][root][INFO] - Training Epoch: 6/10, step 240/574 completed (loss: 0.41575053334236145, acc: 0.9090909361839294)
[2024-11-13 06:44:36,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:36,535][root][INFO] - Training Epoch: 6/10, step 241/574 completed (loss: 0.2918643355369568, acc: 0.8409090638160706)
[2024-11-13 06:44:36,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:37,420][root][INFO] - Training Epoch: 6/10, step 242/574 completed (loss: 0.7983590364456177, acc: 0.7096773982048035)
[2024-11-13 06:44:37,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:38,234][root][INFO] - Training Epoch: 6/10, step 243/574 completed (loss: 0.4386766254901886, acc: 0.8863636255264282)
[2024-11-13 06:44:38,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:38,577][root][INFO] - Training Epoch: 6/10, step 244/574 completed (loss: 0.1607874184846878, acc: 0.9047619104385376)
[2024-11-13 06:44:38,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:38,933][root][INFO] - Training Epoch: 6/10, step 245/574 completed (loss: 0.060680363327264786, acc: 1.0)
[2024-11-13 06:44:39,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:39,259][root][INFO] - Training Epoch: 6/10, step 246/574 completed (loss: 0.02553033083677292, acc: 1.0)
[2024-11-13 06:44:39,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:39,560][root][INFO] - Training Epoch: 6/10, step 247/574 completed (loss: 0.18053868412971497, acc: 0.949999988079071)
[2024-11-13 06:44:39,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:40,007][root][INFO] - Training Epoch: 6/10, step 248/574 completed (loss: 0.16838935017585754, acc: 0.9459459185600281)
[2024-11-13 06:44:40,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:40,342][root][INFO] - Training Epoch: 6/10, step 249/574 completed (loss: 0.0942906066775322, acc: 0.9459459185600281)
[2024-11-13 06:44:40,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:40,681][root][INFO] - Training Epoch: 6/10, step 250/574 completed (loss: 0.023668775334954262, acc: 1.0)
[2024-11-13 06:44:40,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:41,080][root][INFO] - Training Epoch: 6/10, step 251/574 completed (loss: 0.24978576600551605, acc: 0.9264705777168274)
[2024-11-13 06:44:41,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:41,460][root][INFO] - Training Epoch: 6/10, step 252/574 completed (loss: 0.05821104347705841, acc: 0.9756097793579102)
[2024-11-13 06:44:41,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:41,803][root][INFO] - Training Epoch: 6/10, step 253/574 completed (loss: 0.017239561304450035, acc: 1.0)
[2024-11-13 06:44:41,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:42,152][root][INFO] - Training Epoch: 6/10, step 254/574 completed (loss: 0.005751332268118858, acc: 1.0)
[2024-11-13 06:44:42,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:42,503][root][INFO] - Training Epoch: 6/10, step 255/574 completed (loss: 0.008559265173971653, acc: 1.0)
[2024-11-13 06:44:42,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:42,866][root][INFO] - Training Epoch: 6/10, step 256/574 completed (loss: 0.09572207927703857, acc: 0.9649122953414917)
[2024-11-13 06:44:43,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:43,222][root][INFO] - Training Epoch: 6/10, step 257/574 completed (loss: 0.09219029545783997, acc: 0.9714285731315613)
[2024-11-13 06:44:43,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:43,591][root][INFO] - Training Epoch: 6/10, step 258/574 completed (loss: 0.15012384951114655, acc: 0.9342105388641357)
[2024-11-13 06:44:44,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:44,440][root][INFO] - Training Epoch: 6/10, step 259/574 completed (loss: 0.30194610357284546, acc: 0.9150943160057068)
[2024-11-13 06:44:44,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:45,316][root][INFO] - Training Epoch: 6/10, step 260/574 completed (loss: 0.42675888538360596, acc: 0.875)
[2024-11-13 06:44:45,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:45,737][root][INFO] - Training Epoch: 6/10, step 261/574 completed (loss: 0.14117774367332458, acc: 0.9722222089767456)
[2024-11-13 06:44:45,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:46,100][root][INFO] - Training Epoch: 6/10, step 262/574 completed (loss: 0.08203968405723572, acc: 0.9677419066429138)
[2024-11-13 06:44:46,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:46,576][root][INFO] - Training Epoch: 6/10, step 263/574 completed (loss: 0.841103732585907, acc: 0.7733333110809326)
[2024-11-13 06:44:46,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:47,021][root][INFO] - Training Epoch: 6/10, step 264/574 completed (loss: 0.35412561893463135, acc: 0.875)
[2024-11-13 06:44:47,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:48,357][root][INFO] - Training Epoch: 6/10, step 265/574 completed (loss: 0.9365240335464478, acc: 0.7599999904632568)
[2024-11-13 06:44:48,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:48,764][root][INFO] - Training Epoch: 6/10, step 266/574 completed (loss: 0.6192355155944824, acc: 0.8089887499809265)
[2024-11-13 06:44:48,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:49,225][root][INFO] - Training Epoch: 6/10, step 267/574 completed (loss: 0.37573888897895813, acc: 0.8648648858070374)
[2024-11-13 06:44:49,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:49,897][root][INFO] - Training Epoch: 6/10, step 268/574 completed (loss: 0.36412808299064636, acc: 0.8448275923728943)
[2024-11-13 06:44:50,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:50,310][root][INFO] - Training Epoch: 6/10, step 269/574 completed (loss: 0.07205882668495178, acc: 0.9545454382896423)
[2024-11-13 06:44:50,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:50,676][root][INFO] - Training Epoch: 6/10, step 270/574 completed (loss: 0.014579378999769688, acc: 1.0)
[2024-11-13 06:44:50,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:51,088][root][INFO] - Training Epoch: 6/10, step 271/574 completed (loss: 0.007581945508718491, acc: 1.0)
[2024-11-13 06:44:51,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:51,512][root][INFO] - Training Epoch: 6/10, step 272/574 completed (loss: 0.013137278147041798, acc: 1.0)
[2024-11-13 06:44:51,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:52,030][root][INFO] - Training Epoch: 6/10, step 273/574 completed (loss: 0.16642291843891144, acc: 0.949999988079071)
[2024-11-13 06:44:52,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:52,432][root][INFO] - Training Epoch: 6/10, step 274/574 completed (loss: 0.08779508620500565, acc: 0.96875)
[2024-11-13 06:44:52,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:52,862][root][INFO] - Training Epoch: 6/10, step 275/574 completed (loss: 0.045224808156490326, acc: 1.0)
[2024-11-13 06:44:53,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:54,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:54,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:55,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:56,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:56,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:57,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:57,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:58,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:58,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:59,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:44:59,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:00,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:00,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:01,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:01,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:02,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:02,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:03,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:03,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:04,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:04,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:05,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:05,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:06,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:06,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:07,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:07,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:08,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:08,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:09,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:09,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:10,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:10,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:10,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:11,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:11,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:12,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:12,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:13,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:13,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:14,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:14,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:15,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:15,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:16,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:16,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:17,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:17,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:18,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:18,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:19,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:19,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:20,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:20,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:21,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:21,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:22,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:22,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:22,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:23,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:24,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:24,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:25,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:25,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:26,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:26,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:27,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:27,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:28,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:29,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:29,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:30,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:30,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:30,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:31,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:31,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:32,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:32,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:33,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:33,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:33,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:34,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:34,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:35,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:36,145][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3552, device='cuda:0') eval_epoch_loss=tensor(0.8566, device='cuda:0') eval_epoch_acc=tensor(0.7999, device='cuda:0')
[2024-11-13 06:45:36,146][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:45:36,147][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:45:36,550][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_6_step_276_loss_0.85662841796875/model.pt
[2024-11-13 06:45:36,555][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:45:36,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:37,005][root][INFO] - Training Epoch: 6/10, step 276/574 completed (loss: 0.19846995174884796, acc: 0.9655172228813171)
[2024-11-13 06:45:37,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:37,424][root][INFO] - Training Epoch: 6/10, step 277/574 completed (loss: 0.014690334908664227, acc: 1.0)
[2024-11-13 06:45:37,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:37,866][root][INFO] - Training Epoch: 6/10, step 278/574 completed (loss: 0.26433122158050537, acc: 0.957446813583374)
[2024-11-13 06:45:38,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:38,261][root][INFO] - Training Epoch: 6/10, step 279/574 completed (loss: 0.2568388879299164, acc: 0.8958333134651184)
[2024-11-13 06:45:38,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:38,630][root][INFO] - Training Epoch: 6/10, step 280/574 completed (loss: 0.12607066333293915, acc: 0.9545454382896423)
[2024-11-13 06:45:38,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:39,210][root][INFO] - Training Epoch: 6/10, step 281/574 completed (loss: 0.5658170580863953, acc: 0.8554216623306274)
[2024-11-13 06:45:39,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:39,678][root][INFO] - Training Epoch: 6/10, step 282/574 completed (loss: 0.718799352645874, acc: 0.7962962985038757)
[2024-11-13 06:45:39,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:40,029][root][INFO] - Training Epoch: 6/10, step 283/574 completed (loss: 0.11236227303743362, acc: 0.9473684430122375)
[2024-11-13 06:45:40,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:40,447][root][INFO] - Training Epoch: 6/10, step 284/574 completed (loss: 0.19898772239685059, acc: 0.9117646813392639)
[2024-11-13 06:45:40,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:40,901][root][INFO] - Training Epoch: 6/10, step 285/574 completed (loss: 0.10036369413137436, acc: 0.949999988079071)
[2024-11-13 06:45:41,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:41,303][root][INFO] - Training Epoch: 6/10, step 286/574 completed (loss: 0.41480737924575806, acc: 0.90625)
[2024-11-13 06:45:41,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:41,711][root][INFO] - Training Epoch: 6/10, step 287/574 completed (loss: 0.5384299159049988, acc: 0.8640000224113464)
[2024-11-13 06:45:41,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:42,085][root][INFO] - Training Epoch: 6/10, step 288/574 completed (loss: 0.1599375456571579, acc: 0.9450549483299255)
[2024-11-13 06:45:42,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:42,484][root][INFO] - Training Epoch: 6/10, step 289/574 completed (loss: 0.29952308535575867, acc: 0.8944099545478821)
[2024-11-13 06:45:42,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:42,968][root][INFO] - Training Epoch: 6/10, step 290/574 completed (loss: 0.5176845192909241, acc: 0.8350515365600586)
[2024-11-13 06:45:43,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:43,349][root][INFO] - Training Epoch: 6/10, step 291/574 completed (loss: 0.09764384478330612, acc: 0.9545454382896423)
[2024-11-13 06:45:43,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:43,774][root][INFO] - Training Epoch: 6/10, step 292/574 completed (loss: 0.2325572520494461, acc: 0.9285714030265808)
[2024-11-13 06:45:43,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:44,182][root][INFO] - Training Epoch: 6/10, step 293/574 completed (loss: 0.14621640741825104, acc: 0.9482758641242981)
[2024-11-13 06:45:44,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:44,863][root][INFO] - Training Epoch: 6/10, step 294/574 completed (loss: 0.31281542778015137, acc: 0.9090909361839294)
[2024-11-13 06:45:45,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:45,674][root][INFO] - Training Epoch: 6/10, step 295/574 completed (loss: 0.5441784262657166, acc: 0.8298969268798828)
[2024-11-13 06:45:45,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:46,129][root][INFO] - Training Epoch: 6/10, step 296/574 completed (loss: 0.3011711835861206, acc: 0.8965517282485962)
[2024-11-13 06:45:46,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:46,559][root][INFO] - Training Epoch: 6/10, step 297/574 completed (loss: 0.1777811348438263, acc: 0.9629629850387573)
[2024-11-13 06:45:46,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:46,936][root][INFO] - Training Epoch: 6/10, step 298/574 completed (loss: 0.12534821033477783, acc: 1.0)
[2024-11-13 06:45:47,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:47,286][root][INFO] - Training Epoch: 6/10, step 299/574 completed (loss: 0.24018816649913788, acc: 0.9464285969734192)
[2024-11-13 06:45:47,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:47,574][root][INFO] - Training Epoch: 6/10, step 300/574 completed (loss: 0.14885592460632324, acc: 0.90625)
[2024-11-13 06:45:47,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:47,930][root][INFO] - Training Epoch: 6/10, step 301/574 completed (loss: 0.21269209682941437, acc: 0.9433962106704712)
[2024-11-13 06:45:48,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:48,360][root][INFO] - Training Epoch: 6/10, step 302/574 completed (loss: 0.018940934911370277, acc: 1.0)
[2024-11-13 06:45:48,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:48,731][root][INFO] - Training Epoch: 6/10, step 303/574 completed (loss: 0.10344504565000534, acc: 0.970588207244873)
[2024-11-13 06:45:48,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:49,084][root][INFO] - Training Epoch: 6/10, step 304/574 completed (loss: 0.046290263533592224, acc: 0.96875)
[2024-11-13 06:45:49,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:49,463][root][INFO] - Training Epoch: 6/10, step 305/574 completed (loss: 0.29730522632598877, acc: 0.9016393423080444)
[2024-11-13 06:45:49,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:49,773][root][INFO] - Training Epoch: 6/10, step 306/574 completed (loss: 0.057731859385967255, acc: 1.0)
[2024-11-13 06:45:49,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:50,113][root][INFO] - Training Epoch: 6/10, step 307/574 completed (loss: 0.14977242052555084, acc: 0.9473684430122375)
[2024-11-13 06:45:50,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:50,482][root][INFO] - Training Epoch: 6/10, step 308/574 completed (loss: 0.11780040711164474, acc: 0.9710144996643066)
[2024-11-13 06:45:50,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:51,070][root][INFO] - Training Epoch: 6/10, step 309/574 completed (loss: 0.15512917935848236, acc: 0.9583333134651184)
[2024-11-13 06:45:51,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:51,463][root][INFO] - Training Epoch: 6/10, step 310/574 completed (loss: 0.13595570623874664, acc: 0.9638554453849792)
[2024-11-13 06:45:51,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:51,840][root][INFO] - Training Epoch: 6/10, step 311/574 completed (loss: 0.2572585940361023, acc: 0.9102563858032227)
[2024-11-13 06:45:52,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:52,288][root][INFO] - Training Epoch: 6/10, step 312/574 completed (loss: 0.1745907962322235, acc: 0.9387755393981934)
[2024-11-13 06:45:52,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:52,629][root][INFO] - Training Epoch: 6/10, step 313/574 completed (loss: 0.025138704106211662, acc: 1.0)
[2024-11-13 06:45:52,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:52,974][root][INFO] - Training Epoch: 6/10, step 314/574 completed (loss: 0.1233670637011528, acc: 0.9583333134651184)
[2024-11-13 06:45:53,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:53,340][root][INFO] - Training Epoch: 6/10, step 315/574 completed (loss: 0.14008957147598267, acc: 0.9032257795333862)
[2024-11-13 06:45:53,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:53,710][root][INFO] - Training Epoch: 6/10, step 316/574 completed (loss: 0.41627049446105957, acc: 0.9032257795333862)
[2024-11-13 06:45:53,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:54,125][root][INFO] - Training Epoch: 6/10, step 317/574 completed (loss: 0.06390417367219925, acc: 0.9850746393203735)
[2024-11-13 06:45:54,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:54,550][root][INFO] - Training Epoch: 6/10, step 318/574 completed (loss: 0.06034248322248459, acc: 0.9903846383094788)
[2024-11-13 06:45:54,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:54,954][root][INFO] - Training Epoch: 6/10, step 319/574 completed (loss: 0.1632789671421051, acc: 0.9555555582046509)
[2024-11-13 06:45:55,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:55,294][root][INFO] - Training Epoch: 6/10, step 320/574 completed (loss: 0.10561904311180115, acc: 0.9677419066429138)
[2024-11-13 06:45:55,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:55,632][root][INFO] - Training Epoch: 6/10, step 321/574 completed (loss: 0.0412217415869236, acc: 0.9800000190734863)
[2024-11-13 06:45:55,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:55,974][root][INFO] - Training Epoch: 6/10, step 322/574 completed (loss: 0.40585821866989136, acc: 0.8148148059844971)
[2024-11-13 06:45:56,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:56,320][root][INFO] - Training Epoch: 6/10, step 323/574 completed (loss: 1.0627130270004272, acc: 0.6857143044471741)
[2024-11-13 06:45:56,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:56,674][root][INFO] - Training Epoch: 6/10, step 324/574 completed (loss: 0.5157840251922607, acc: 0.8717948794364929)
[2024-11-13 06:45:56,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:57,032][root][INFO] - Training Epoch: 6/10, step 325/574 completed (loss: 0.9199005961418152, acc: 0.6341463327407837)
[2024-11-13 06:45:57,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:57,381][root][INFO] - Training Epoch: 6/10, step 326/574 completed (loss: 0.6667361259460449, acc: 0.8421052694320679)
[2024-11-13 06:45:57,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:57,725][root][INFO] - Training Epoch: 6/10, step 327/574 completed (loss: 0.4160645306110382, acc: 0.8421052694320679)
[2024-11-13 06:45:57,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:58,073][root][INFO] - Training Epoch: 6/10, step 328/574 completed (loss: 0.024898530915379524, acc: 1.0)
[2024-11-13 06:45:58,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:58,444][root][INFO] - Training Epoch: 6/10, step 329/574 completed (loss: 0.08541979640722275, acc: 0.9259259104728699)
[2024-11-13 06:45:58,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:58,798][root][INFO] - Training Epoch: 6/10, step 330/574 completed (loss: 0.017550718039274216, acc: 1.0)
[2024-11-13 06:45:58,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:59,177][root][INFO] - Training Epoch: 6/10, step 331/574 completed (loss: 0.21787719428539276, acc: 0.9193548560142517)
[2024-11-13 06:45:59,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:59,649][root][INFO] - Training Epoch: 6/10, step 332/574 completed (loss: 0.20411355793476105, acc: 0.9649122953414917)
[2024-11-13 06:45:59,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:45:59,999][root][INFO] - Training Epoch: 6/10, step 333/574 completed (loss: 0.12425326555967331, acc: 0.96875)
[2024-11-13 06:46:00,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:00,378][root][INFO] - Training Epoch: 6/10, step 334/574 completed (loss: 0.03388889878988266, acc: 1.0)
[2024-11-13 06:46:00,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:00,724][root][INFO] - Training Epoch: 6/10, step 335/574 completed (loss: 0.538952648639679, acc: 0.8947368264198303)
[2024-11-13 06:46:00,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:01,065][root][INFO] - Training Epoch: 6/10, step 336/574 completed (loss: 0.4862688481807709, acc: 0.8799999952316284)
[2024-11-13 06:46:01,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:01,476][root][INFO] - Training Epoch: 6/10, step 337/574 completed (loss: 0.8192920684814453, acc: 0.7126436829566956)
[2024-11-13 06:46:01,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:01,897][root][INFO] - Training Epoch: 6/10, step 338/574 completed (loss: 0.8481600880622864, acc: 0.7127659320831299)
[2024-11-13 06:46:02,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:02,279][root][INFO] - Training Epoch: 6/10, step 339/574 completed (loss: 0.7983301877975464, acc: 0.7469879388809204)
[2024-11-13 06:46:02,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:02,630][root][INFO] - Training Epoch: 6/10, step 340/574 completed (loss: 0.04257679358124733, acc: 1.0)
[2024-11-13 06:46:02,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:02,961][root][INFO] - Training Epoch: 6/10, step 341/574 completed (loss: 0.1174916997551918, acc: 0.9230769276618958)
[2024-11-13 06:46:03,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:03,333][root][INFO] - Training Epoch: 6/10, step 342/574 completed (loss: 0.2243262380361557, acc: 0.9397590160369873)
[2024-11-13 06:46:03,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:03,681][root][INFO] - Training Epoch: 6/10, step 343/574 completed (loss: 0.42830464243888855, acc: 0.8679245114326477)
[2024-11-13 06:46:03,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:04,031][root][INFO] - Training Epoch: 6/10, step 344/574 completed (loss: 0.12634477019309998, acc: 0.9746835231781006)
[2024-11-13 06:46:04,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:04,392][root][INFO] - Training Epoch: 6/10, step 345/574 completed (loss: 0.098948173224926, acc: 0.9607843160629272)
[2024-11-13 06:46:04,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:04,746][root][INFO] - Training Epoch: 6/10, step 346/574 completed (loss: 0.34299811720848083, acc: 0.89552241563797)
[2024-11-13 06:46:04,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:05,085][root][INFO] - Training Epoch: 6/10, step 347/574 completed (loss: 0.0837211161851883, acc: 0.949999988079071)
[2024-11-13 06:46:05,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:05,427][root][INFO] - Training Epoch: 6/10, step 348/574 completed (loss: 0.06037060171365738, acc: 1.0)
[2024-11-13 06:46:05,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:05,950][root][INFO] - Training Epoch: 6/10, step 349/574 completed (loss: 0.39436984062194824, acc: 0.9444444179534912)
[2024-11-13 06:46:06,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:06,302][root][INFO] - Training Epoch: 6/10, step 350/574 completed (loss: 0.31739288568496704, acc: 0.9069767594337463)
[2024-11-13 06:46:06,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:06,698][root][INFO] - Training Epoch: 6/10, step 351/574 completed (loss: 0.16390395164489746, acc: 0.9487179517745972)
[2024-11-13 06:46:06,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:07,180][root][INFO] - Training Epoch: 6/10, step 352/574 completed (loss: 0.24799205362796783, acc: 0.9333333373069763)
[2024-11-13 06:46:07,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:07,532][root][INFO] - Training Epoch: 6/10, step 353/574 completed (loss: 0.41013163328170776, acc: 0.9130434989929199)
[2024-11-13 06:46:07,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:07,896][root][INFO] - Training Epoch: 6/10, step 354/574 completed (loss: 0.184312641620636, acc: 0.9230769276618958)
[2024-11-13 06:46:08,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:08,315][root][INFO] - Training Epoch: 6/10, step 355/574 completed (loss: 0.43887317180633545, acc: 0.8461538553237915)
[2024-11-13 06:46:08,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:09,039][root][INFO] - Training Epoch: 6/10, step 356/574 completed (loss: 0.3448217213153839, acc: 0.8782608509063721)
[2024-11-13 06:46:09,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:09,445][root][INFO] - Training Epoch: 6/10, step 357/574 completed (loss: 0.22970370948314667, acc: 0.9347826242446899)
[2024-11-13 06:46:09,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:09,838][root][INFO] - Training Epoch: 6/10, step 358/574 completed (loss: 0.34163182973861694, acc: 0.918367326259613)
[2024-11-13 06:46:09,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:10,178][root][INFO] - Training Epoch: 6/10, step 359/574 completed (loss: 0.0038209327030926943, acc: 1.0)
[2024-11-13 06:46:10,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:10,526][root][INFO] - Training Epoch: 6/10, step 360/574 completed (loss: 0.016978969797492027, acc: 1.0)
[2024-11-13 06:46:10,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:10,912][root][INFO] - Training Epoch: 6/10, step 361/574 completed (loss: 0.2554470896720886, acc: 0.8780487775802612)
[2024-11-13 06:46:11,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:11,272][root][INFO] - Training Epoch: 6/10, step 362/574 completed (loss: 0.10331583768129349, acc: 0.9777777791023254)
[2024-11-13 06:46:11,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:11,633][root][INFO] - Training Epoch: 6/10, step 363/574 completed (loss: 0.19435277581214905, acc: 0.9605262875556946)
[2024-11-13 06:46:11,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:11,995][root][INFO] - Training Epoch: 6/10, step 364/574 completed (loss: 0.20815879106521606, acc: 0.9512194991111755)
[2024-11-13 06:46:12,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:12,343][root][INFO] - Training Epoch: 6/10, step 365/574 completed (loss: 0.06569221615791321, acc: 0.9696969985961914)
[2024-11-13 06:46:12,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:12,701][root][INFO] - Training Epoch: 6/10, step 366/574 completed (loss: 0.0038596035446971655, acc: 1.0)
[2024-11-13 06:46:12,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:13,045][root][INFO] - Training Epoch: 6/10, step 367/574 completed (loss: 0.009971928782761097, acc: 1.0)
[2024-11-13 06:46:13,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:13,380][root][INFO] - Training Epoch: 6/10, step 368/574 completed (loss: 0.09098726511001587, acc: 0.9642857313156128)
[2024-11-13 06:46:13,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:13,676][root][INFO] - Training Epoch: 6/10, step 369/574 completed (loss: 0.0975198820233345, acc: 1.0)
[2024-11-13 06:46:14,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:14,591][root][INFO] - Training Epoch: 6/10, step 370/574 completed (loss: 0.5462865233421326, acc: 0.8484848737716675)
[2024-11-13 06:46:15,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:15,944][root][INFO] - Training Epoch: 6/10, step 371/574 completed (loss: 0.30043718218803406, acc: 0.9150943160057068)
[2024-11-13 06:46:16,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:16,388][root][INFO] - Training Epoch: 6/10, step 372/574 completed (loss: 0.16575390100479126, acc: 0.9666666388511658)
[2024-11-13 06:46:16,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:16,757][root][INFO] - Training Epoch: 6/10, step 373/574 completed (loss: 0.09609613567590714, acc: 0.9642857313156128)
[2024-11-13 06:46:16,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:17,152][root][INFO] - Training Epoch: 6/10, step 374/574 completed (loss: 0.15825271606445312, acc: 0.9428571462631226)
[2024-11-13 06:46:17,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:17,502][root][INFO] - Training Epoch: 6/10, step 375/574 completed (loss: 0.007825269363820553, acc: 1.0)
[2024-11-13 06:46:17,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:17,946][root][INFO] - Training Epoch: 6/10, step 376/574 completed (loss: 0.01263114158064127, acc: 1.0)
[2024-11-13 06:46:18,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:18,308][root][INFO] - Training Epoch: 6/10, step 377/574 completed (loss: 0.24438045918941498, acc: 0.9375)
[2024-11-13 06:46:18,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:18,732][root][INFO] - Training Epoch: 6/10, step 378/574 completed (loss: 0.039924658834934235, acc: 0.9894737005233765)
[2024-11-13 06:46:19,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:19,603][root][INFO] - Training Epoch: 6/10, step 379/574 completed (loss: 0.1940898597240448, acc: 0.9281437397003174)
[2024-11-13 06:46:19,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:20,173][root][INFO] - Training Epoch: 6/10, step 380/574 completed (loss: 0.2201656550168991, acc: 0.9473684430122375)
[2024-11-13 06:46:21,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:22,100][root][INFO] - Training Epoch: 6/10, step 381/574 completed (loss: 0.4883919358253479, acc: 0.8716577291488647)
[2024-11-13 06:46:22,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:22,950][root][INFO] - Training Epoch: 6/10, step 382/574 completed (loss: 0.04416240379214287, acc: 0.9819819927215576)
[2024-11-13 06:46:23,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:23,282][root][INFO] - Training Epoch: 6/10, step 383/574 completed (loss: 0.14067354798316956, acc: 0.9285714030265808)
[2024-11-13 06:46:23,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:23,677][root][INFO] - Training Epoch: 6/10, step 384/574 completed (loss: 0.0324208065867424, acc: 0.9642857313156128)
[2024-11-13 06:46:23,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:24,075][root][INFO] - Training Epoch: 6/10, step 385/574 completed (loss: 0.03391505405306816, acc: 1.0)
[2024-11-13 06:46:24,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:24,496][root][INFO] - Training Epoch: 6/10, step 386/574 completed (loss: 0.03966522961854935, acc: 0.9722222089767456)
[2024-11-13 06:46:24,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:24,898][root][INFO] - Training Epoch: 6/10, step 387/574 completed (loss: 0.01758350245654583, acc: 1.0)
[2024-11-13 06:46:25,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:25,288][root][INFO] - Training Epoch: 6/10, step 388/574 completed (loss: 0.006666188128292561, acc: 1.0)
[2024-11-13 06:46:25,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:25,691][root][INFO] - Training Epoch: 6/10, step 389/574 completed (loss: 0.061228442937135696, acc: 1.0)
[2024-11-13 06:46:25,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:26,084][root][INFO] - Training Epoch: 6/10, step 390/574 completed (loss: 0.18231432139873505, acc: 0.9523809552192688)
[2024-11-13 06:46:26,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:26,516][root][INFO] - Training Epoch: 6/10, step 391/574 completed (loss: 0.3444148898124695, acc: 0.8888888955116272)
[2024-11-13 06:46:26,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:26,903][root][INFO] - Training Epoch: 6/10, step 392/574 completed (loss: 0.6901891231536865, acc: 0.7864077687263489)
[2024-11-13 06:46:27,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:27,664][root][INFO] - Training Epoch: 6/10, step 393/574 completed (loss: 0.6047334671020508, acc: 0.8308823704719543)
[2024-11-13 06:46:27,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:28,149][root][INFO] - Training Epoch: 6/10, step 394/574 completed (loss: 0.6771730780601501, acc: 0.8133333325386047)
[2024-11-13 06:46:28,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:28,692][root][INFO] - Training Epoch: 6/10, step 395/574 completed (loss: 0.36588340997695923, acc: 0.9027777910232544)
[2024-11-13 06:46:28,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:29,085][root][INFO] - Training Epoch: 6/10, step 396/574 completed (loss: 0.27110472321510315, acc: 0.930232584476471)
[2024-11-13 06:46:29,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:29,469][root][INFO] - Training Epoch: 6/10, step 397/574 completed (loss: 0.05982305109500885, acc: 0.9583333134651184)
[2024-11-13 06:46:29,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:29,835][root][INFO] - Training Epoch: 6/10, step 398/574 completed (loss: 0.10194887220859528, acc: 0.9767441749572754)
[2024-11-13 06:46:29,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:30,204][root][INFO] - Training Epoch: 6/10, step 399/574 completed (loss: 0.04897056519985199, acc: 1.0)
[2024-11-13 06:46:30,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:30,998][root][INFO] - Training Epoch: 6/10, step 400/574 completed (loss: 0.17615646123886108, acc: 0.9558823704719543)
[2024-11-13 06:46:31,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:31,454][root][INFO] - Training Epoch: 6/10, step 401/574 completed (loss: 0.24791522324085236, acc: 0.8933333158493042)
[2024-11-13 06:46:31,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:31,878][root][INFO] - Training Epoch: 6/10, step 402/574 completed (loss: 0.1106715276837349, acc: 0.939393937587738)
[2024-11-13 06:46:32,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:32,271][root][INFO] - Training Epoch: 6/10, step 403/574 completed (loss: 0.2904946506023407, acc: 0.9696969985961914)
[2024-11-13 06:46:32,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:32,643][root][INFO] - Training Epoch: 6/10, step 404/574 completed (loss: 0.09537401795387268, acc: 0.9677419066429138)
[2024-11-13 06:46:32,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:33,103][root][INFO] - Training Epoch: 6/10, step 405/574 completed (loss: 0.05089205130934715, acc: 1.0)
[2024-11-13 06:46:33,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:33,518][root][INFO] - Training Epoch: 6/10, step 406/574 completed (loss: 0.04780643433332443, acc: 0.9599999785423279)
[2024-11-13 06:46:33,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:33,935][root][INFO] - Training Epoch: 6/10, step 407/574 completed (loss: 0.027005519717931747, acc: 1.0)
[2024-11-13 06:46:34,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:34,323][root][INFO] - Training Epoch: 6/10, step 408/574 completed (loss: 0.010806429199874401, acc: 1.0)
[2024-11-13 06:46:34,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:34,668][root][INFO] - Training Epoch: 6/10, step 409/574 completed (loss: 0.0169194508343935, acc: 1.0)
[2024-11-13 06:46:34,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:34,994][root][INFO] - Training Epoch: 6/10, step 410/574 completed (loss: 0.022743014618754387, acc: 1.0)
[2024-11-13 06:46:35,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:35,360][root][INFO] - Training Epoch: 6/10, step 411/574 completed (loss: 0.07714015245437622, acc: 1.0)
[2024-11-13 06:46:35,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:35,710][root][INFO] - Training Epoch: 6/10, step 412/574 completed (loss: 0.16457316279411316, acc: 0.9666666388511658)
[2024-11-13 06:46:35,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:36,072][root][INFO] - Training Epoch: 6/10, step 413/574 completed (loss: 0.5771851539611816, acc: 0.8787878751754761)
[2024-11-13 06:46:36,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:36,471][root][INFO] - Training Epoch: 6/10, step 414/574 completed (loss: 0.014615673571825027, acc: 1.0)
[2024-11-13 06:46:36,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:36,906][root][INFO] - Training Epoch: 6/10, step 415/574 completed (loss: 0.33289453387260437, acc: 0.9019607901573181)
[2024-11-13 06:46:37,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:37,242][root][INFO] - Training Epoch: 6/10, step 416/574 completed (loss: 0.16225144267082214, acc: 0.9615384340286255)
[2024-11-13 06:46:37,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:37,630][root][INFO] - Training Epoch: 6/10, step 417/574 completed (loss: 0.01668849214911461, acc: 1.0)
[2024-11-13 06:46:37,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:38,056][root][INFO] - Training Epoch: 6/10, step 418/574 completed (loss: 0.06229717284440994, acc: 1.0)
[2024-11-13 06:46:38,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:39,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:39,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:40,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:40,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:41,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:41,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:42,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:42,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:43,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:44,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:44,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:45,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:45,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:46,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:46,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:47,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:47,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:48,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:48,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:49,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:49,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:50,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:50,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:51,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:51,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:52,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:52,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:53,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:53,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:54,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:54,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:55,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:55,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:56,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:56,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:57,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:57,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:57,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:58,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:58,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:59,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:46:59,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:00,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:00,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:01,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:01,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:02,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:02,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:03,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:03,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:03,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:04,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:04,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:05,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:05,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:06,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:06,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:07,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:07,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:08,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:09,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:09,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:10,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:10,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:11,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:11,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:12,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:12,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:13,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:13,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:14,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:14,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:15,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:15,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:16,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:16,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:17,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:17,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:18,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:18,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:19,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:19,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:20,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:20,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:21,445][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3320, device='cuda:0') eval_epoch_loss=tensor(0.8467, device='cuda:0') eval_epoch_acc=tensor(0.7936, device='cuda:0')
[2024-11-13 06:47:21,447][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:47:21,447][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:47:22,037][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_6_step_419_loss_0.8467289209365845/model.pt
[2024-11-13 06:47:22,043][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:47:22,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:22,462][root][INFO] - Training Epoch: 6/10, step 419/574 completed (loss: 0.2409248799085617, acc: 0.8999999761581421)
[2024-11-13 06:47:22,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:22,867][root][INFO] - Training Epoch: 6/10, step 420/574 completed (loss: 0.25030893087387085, acc: 0.9047619104385376)
[2024-11-13 06:47:23,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:23,238][root][INFO] - Training Epoch: 6/10, step 421/574 completed (loss: 0.09316999465227127, acc: 0.9666666388511658)
[2024-11-13 06:47:23,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:23,562][root][INFO] - Training Epoch: 6/10, step 422/574 completed (loss: 0.2202618569135666, acc: 0.90625)
[2024-11-13 06:47:23,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:23,931][root][INFO] - Training Epoch: 6/10, step 423/574 completed (loss: 0.18380777537822723, acc: 0.9166666865348816)
[2024-11-13 06:47:24,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:24,325][root][INFO] - Training Epoch: 6/10, step 424/574 completed (loss: 0.026165030896663666, acc: 1.0)
[2024-11-13 06:47:24,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:24,685][root][INFO] - Training Epoch: 6/10, step 425/574 completed (loss: 0.11270790547132492, acc: 0.939393937587738)
[2024-11-13 06:47:24,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:25,111][root][INFO] - Training Epoch: 6/10, step 426/574 completed (loss: 0.011182201094925404, acc: 1.0)
[2024-11-13 06:47:25,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:25,502][root][INFO] - Training Epoch: 6/10, step 427/574 completed (loss: 0.15509450435638428, acc: 0.9459459185600281)
[2024-11-13 06:47:25,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:25,844][root][INFO] - Training Epoch: 6/10, step 428/574 completed (loss: 0.27369269728660583, acc: 0.9629629850387573)
[2024-11-13 06:47:25,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:26,194][root][INFO] - Training Epoch: 6/10, step 429/574 completed (loss: 0.03275309503078461, acc: 1.0)
[2024-11-13 06:47:26,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:26,550][root][INFO] - Training Epoch: 6/10, step 430/574 completed (loss: 0.006681227590888739, acc: 1.0)
[2024-11-13 06:47:26,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:26,908][root][INFO] - Training Epoch: 6/10, step 431/574 completed (loss: 0.006426289211958647, acc: 1.0)
[2024-11-13 06:47:27,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:27,285][root][INFO] - Training Epoch: 6/10, step 432/574 completed (loss: 0.04898880794644356, acc: 1.0)
[2024-11-13 06:47:27,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:27,729][root][INFO] - Training Epoch: 6/10, step 433/574 completed (loss: 0.11587381362915039, acc: 0.9166666865348816)
[2024-11-13 06:47:27,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:28,136][root][INFO] - Training Epoch: 6/10, step 434/574 completed (loss: 0.007149697747081518, acc: 1.0)
[2024-11-13 06:47:28,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:28,503][root][INFO] - Training Epoch: 6/10, step 435/574 completed (loss: 0.10776739567518234, acc: 0.939393937587738)
[2024-11-13 06:47:28,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:28,904][root][INFO] - Training Epoch: 6/10, step 436/574 completed (loss: 0.03820737451314926, acc: 1.0)
[2024-11-13 06:47:29,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:29,299][root][INFO] - Training Epoch: 6/10, step 437/574 completed (loss: 0.025959651917219162, acc: 1.0)
[2024-11-13 06:47:29,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:29,663][root][INFO] - Training Epoch: 6/10, step 438/574 completed (loss: 0.0016159394290298223, acc: 1.0)
[2024-11-13 06:47:29,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:30,013][root][INFO] - Training Epoch: 6/10, step 439/574 completed (loss: 0.17810799181461334, acc: 0.9230769276618958)
[2024-11-13 06:47:30,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:30,699][root][INFO] - Training Epoch: 6/10, step 440/574 completed (loss: 0.31379517912864685, acc: 0.9242424368858337)
[2024-11-13 06:47:31,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:31,771][root][INFO] - Training Epoch: 6/10, step 441/574 completed (loss: 0.7071011066436768, acc: 0.8159999847412109)
[2024-11-13 06:47:32,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:32,388][root][INFO] - Training Epoch: 6/10, step 442/574 completed (loss: 0.6841294169425964, acc: 0.7983871102333069)
[2024-11-13 06:47:32,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:33,404][root][INFO] - Training Epoch: 6/10, step 443/574 completed (loss: 0.6032797694206238, acc: 0.8407959938049316)
[2024-11-13 06:47:33,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:33,782][root][INFO] - Training Epoch: 6/10, step 444/574 completed (loss: 0.10750079900026321, acc: 0.9622641801834106)
[2024-11-13 06:47:34,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:34,375][root][INFO] - Training Epoch: 6/10, step 445/574 completed (loss: 0.07173829525709152, acc: 0.9545454382896423)
[2024-11-13 06:47:34,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:34,842][root][INFO] - Training Epoch: 6/10, step 446/574 completed (loss: 0.27591216564178467, acc: 0.9130434989929199)
[2024-11-13 06:47:35,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:35,236][root][INFO] - Training Epoch: 6/10, step 447/574 completed (loss: 0.02608037181198597, acc: 1.0)
[2024-11-13 06:47:35,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:35,690][root][INFO] - Training Epoch: 6/10, step 448/574 completed (loss: 0.012372864410281181, acc: 1.0)
[2024-11-13 06:47:35,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:36,120][root][INFO] - Training Epoch: 6/10, step 449/574 completed (loss: 0.26809099316596985, acc: 0.9552238583564758)
[2024-11-13 06:47:36,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:36,590][root][INFO] - Training Epoch: 6/10, step 450/574 completed (loss: 0.08819237351417542, acc: 0.9722222089767456)
[2024-11-13 06:47:36,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:36,987][root][INFO] - Training Epoch: 6/10, step 451/574 completed (loss: 0.028082484379410744, acc: 1.0)
[2024-11-13 06:47:37,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:37,358][root][INFO] - Training Epoch: 6/10, step 452/574 completed (loss: 0.2374502569437027, acc: 0.9358974099159241)
[2024-11-13 06:47:37,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:37,747][root][INFO] - Training Epoch: 6/10, step 453/574 completed (loss: 0.3222833573818207, acc: 0.9078947305679321)
[2024-11-13 06:47:37,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:38,173][root][INFO] - Training Epoch: 6/10, step 454/574 completed (loss: 0.06066016107797623, acc: 0.9795918464660645)
[2024-11-13 06:47:38,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:38,632][root][INFO] - Training Epoch: 6/10, step 455/574 completed (loss: 0.05327489972114563, acc: 0.9696969985961914)
[2024-11-13 06:47:38,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:39,102][root][INFO] - Training Epoch: 6/10, step 456/574 completed (loss: 0.5413603782653809, acc: 0.8350515365600586)
[2024-11-13 06:47:39,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:39,556][root][INFO] - Training Epoch: 6/10, step 457/574 completed (loss: 0.04791213572025299, acc: 0.9857142567634583)
[2024-11-13 06:47:39,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:40,073][root][INFO] - Training Epoch: 6/10, step 458/574 completed (loss: 0.4558214545249939, acc: 0.8837209343910217)
[2024-11-13 06:47:40,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:40,483][root][INFO] - Training Epoch: 6/10, step 459/574 completed (loss: 0.13130034506320953, acc: 0.9464285969734192)
[2024-11-13 06:47:40,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:40,868][root][INFO] - Training Epoch: 6/10, step 460/574 completed (loss: 0.16513490676879883, acc: 0.9382715821266174)
[2024-11-13 06:47:41,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:41,259][root][INFO] - Training Epoch: 6/10, step 461/574 completed (loss: 0.17147357761859894, acc: 0.9166666865348816)
[2024-11-13 06:47:41,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:41,622][root][INFO] - Training Epoch: 6/10, step 462/574 completed (loss: 0.01043083518743515, acc: 1.0)
[2024-11-13 06:47:41,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:42,022][root][INFO] - Training Epoch: 6/10, step 463/574 completed (loss: 0.1413249522447586, acc: 0.9230769276618958)
[2024-11-13 06:47:42,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:42,442][root][INFO] - Training Epoch: 6/10, step 464/574 completed (loss: 0.15159203112125397, acc: 0.95652174949646)
[2024-11-13 06:47:42,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:42,932][root][INFO] - Training Epoch: 6/10, step 465/574 completed (loss: 0.30097267031669617, acc: 0.8809523582458496)
[2024-11-13 06:47:43,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:43,336][root][INFO] - Training Epoch: 6/10, step 466/574 completed (loss: 0.5133638978004456, acc: 0.8795180916786194)
[2024-11-13 06:47:43,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:43,712][root][INFO] - Training Epoch: 6/10, step 467/574 completed (loss: 0.16051164269447327, acc: 0.9639639854431152)
[2024-11-13 06:47:43,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:44,101][root][INFO] - Training Epoch: 6/10, step 468/574 completed (loss: 0.4816073179244995, acc: 0.844660222530365)
[2024-11-13 06:47:44,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:44,497][root][INFO] - Training Epoch: 6/10, step 469/574 completed (loss: 0.4199824929237366, acc: 0.8861788511276245)
[2024-11-13 06:47:44,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:44,871][root][INFO] - Training Epoch: 6/10, step 470/574 completed (loss: 0.25641772150993347, acc: 0.9583333134651184)
[2024-11-13 06:47:45,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:45,228][root][INFO] - Training Epoch: 6/10, step 471/574 completed (loss: 0.1537749022245407, acc: 0.9285714030265808)
[2024-11-13 06:47:45,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:45,792][root][INFO] - Training Epoch: 6/10, step 472/574 completed (loss: 0.40582960844039917, acc: 0.8529411554336548)
[2024-11-13 06:47:45,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:46,256][root][INFO] - Training Epoch: 6/10, step 473/574 completed (loss: 0.729710578918457, acc: 0.7903929948806763)
[2024-11-13 06:47:46,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:46,635][root][INFO] - Training Epoch: 6/10, step 474/574 completed (loss: 0.21644674241542816, acc: 0.9270833134651184)
[2024-11-13 06:47:46,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:47,039][root][INFO] - Training Epoch: 6/10, step 475/574 completed (loss: 0.2259903848171234, acc: 0.9141104221343994)
[2024-11-13 06:47:47,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:47,399][root][INFO] - Training Epoch: 6/10, step 476/574 completed (loss: 0.3004875183105469, acc: 0.8920863270759583)
[2024-11-13 06:47:47,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:47,879][root][INFO] - Training Epoch: 6/10, step 477/574 completed (loss: 0.5852380990982056, acc: 0.80402010679245)
[2024-11-13 06:47:48,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:48,272][root][INFO] - Training Epoch: 6/10, step 478/574 completed (loss: 0.0887053832411766, acc: 1.0)
[2024-11-13 06:47:48,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:48,628][root][INFO] - Training Epoch: 6/10, step 479/574 completed (loss: 0.23909594118595123, acc: 0.8787878751754761)
[2024-11-13 06:47:48,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:49,070][root][INFO] - Training Epoch: 6/10, step 480/574 completed (loss: 0.15160082280635834, acc: 0.9259259104728699)
[2024-11-13 06:47:49,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:49,522][root][INFO] - Training Epoch: 6/10, step 481/574 completed (loss: 0.3561997413635254, acc: 0.8500000238418579)
[2024-11-13 06:47:49,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:49,943][root][INFO] - Training Epoch: 6/10, step 482/574 completed (loss: 0.19181929528713226, acc: 0.8999999761581421)
[2024-11-13 06:47:50,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:50,448][root][INFO] - Training Epoch: 6/10, step 483/574 completed (loss: 0.41547679901123047, acc: 0.8275862336158752)
[2024-11-13 06:47:50,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:50,871][root][INFO] - Training Epoch: 6/10, step 484/574 completed (loss: 0.2898170053958893, acc: 0.9354838728904724)
[2024-11-13 06:47:51,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:51,301][root][INFO] - Training Epoch: 6/10, step 485/574 completed (loss: 0.19058997929096222, acc: 0.8947368264198303)
[2024-11-13 06:47:51,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:51,682][root][INFO] - Training Epoch: 6/10, step 486/574 completed (loss: 0.20092986524105072, acc: 0.9629629850387573)
[2024-11-13 06:47:51,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:52,029][root][INFO] - Training Epoch: 6/10, step 487/574 completed (loss: 0.15354542434215546, acc: 0.9047619104385376)
[2024-11-13 06:47:52,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:52,375][root][INFO] - Training Epoch: 6/10, step 488/574 completed (loss: 0.2944793105125427, acc: 0.9090909361839294)
[2024-11-13 06:47:52,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:52,820][root][INFO] - Training Epoch: 6/10, step 489/574 completed (loss: 0.7576385140419006, acc: 0.8153846263885498)
[2024-11-13 06:47:52,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:53,215][root][INFO] - Training Epoch: 6/10, step 490/574 completed (loss: 0.0959075316786766, acc: 0.9666666388511658)
[2024-11-13 06:47:53,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:53,567][root][INFO] - Training Epoch: 6/10, step 491/574 completed (loss: 0.301423043012619, acc: 0.8965517282485962)
[2024-11-13 06:47:53,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:53,926][root][INFO] - Training Epoch: 6/10, step 492/574 completed (loss: 0.32092127203941345, acc: 0.8823529481887817)
[2024-11-13 06:47:54,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:54,282][root][INFO] - Training Epoch: 6/10, step 493/574 completed (loss: 0.1455666869878769, acc: 0.931034505367279)
[2024-11-13 06:47:54,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:54,689][root][INFO] - Training Epoch: 6/10, step 494/574 completed (loss: 0.5295186042785645, acc: 0.8947368264198303)
[2024-11-13 06:47:54,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:55,054][root][INFO] - Training Epoch: 6/10, step 495/574 completed (loss: 0.16569527983665466, acc: 0.9473684430122375)
[2024-11-13 06:47:55,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:55,475][root][INFO] - Training Epoch: 6/10, step 496/574 completed (loss: 0.5644229054450989, acc: 0.8571428656578064)
[2024-11-13 06:47:55,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:55,968][root][INFO] - Training Epoch: 6/10, step 497/574 completed (loss: 0.527393102645874, acc: 0.8876404762268066)
[2024-11-13 06:47:56,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:56,363][root][INFO] - Training Epoch: 6/10, step 498/574 completed (loss: 0.4989670217037201, acc: 0.8314606547355652)
[2024-11-13 06:47:56,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:56,754][root][INFO] - Training Epoch: 6/10, step 499/574 completed (loss: 0.9063990712165833, acc: 0.7446808218955994)
[2024-11-13 06:47:56,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:57,134][root][INFO] - Training Epoch: 6/10, step 500/574 completed (loss: 0.5342117547988892, acc: 0.8804348111152649)
[2024-11-13 06:47:57,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:57,495][root][INFO] - Training Epoch: 6/10, step 501/574 completed (loss: 0.01714218594133854, acc: 1.0)
[2024-11-13 06:47:57,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:57,903][root][INFO] - Training Epoch: 6/10, step 502/574 completed (loss: 0.0040597086772322655, acc: 1.0)
[2024-11-13 06:47:58,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:58,255][root][INFO] - Training Epoch: 6/10, step 503/574 completed (loss: 0.27015358209609985, acc: 0.9629629850387573)
[2024-11-13 06:47:58,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:58,607][root][INFO] - Training Epoch: 6/10, step 504/574 completed (loss: 0.19652892649173737, acc: 0.9259259104728699)
[2024-11-13 06:47:58,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:58,974][root][INFO] - Training Epoch: 6/10, step 505/574 completed (loss: 0.2658071219921112, acc: 0.9245283007621765)
[2024-11-13 06:47:59,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:47:59,356][root][INFO] - Training Epoch: 6/10, step 506/574 completed (loss: 0.4169856011867523, acc: 0.8965517282485962)
[2024-11-13 06:47:59,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:00,265][root][INFO] - Training Epoch: 6/10, step 507/574 completed (loss: 0.8712133169174194, acc: 0.7477477192878723)
[2024-11-13 06:48:00,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:00,900][root][INFO] - Training Epoch: 6/10, step 508/574 completed (loss: 0.47468045353889465, acc: 0.8450704216957092)
[2024-11-13 06:48:01,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:01,247][root][INFO] - Training Epoch: 6/10, step 509/574 completed (loss: 0.05196957662701607, acc: 1.0)
[2024-11-13 06:48:01,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:01,592][root][INFO] - Training Epoch: 6/10, step 510/574 completed (loss: 0.0300718005746603, acc: 1.0)
[2024-11-13 06:48:01,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:01,952][root][INFO] - Training Epoch: 6/10, step 511/574 completed (loss: 0.10744166374206543, acc: 0.9230769276618958)
[2024-11-13 06:48:04,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:05,617][root][INFO] - Training Epoch: 6/10, step 512/574 completed (loss: 0.7792280316352844, acc: 0.7642857432365417)
[2024-11-13 06:48:06,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:06,820][root][INFO] - Training Epoch: 6/10, step 513/574 completed (loss: 0.3256286084651947, acc: 0.89682537317276)
[2024-11-13 06:48:06,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:07,181][root][INFO] - Training Epoch: 6/10, step 514/574 completed (loss: 0.2571275532245636, acc: 0.8928571343421936)
[2024-11-13 06:48:07,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:07,558][root][INFO] - Training Epoch: 6/10, step 515/574 completed (loss: 0.061490241438150406, acc: 0.9833333492279053)
[2024-11-13 06:48:08,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:08,626][root][INFO] - Training Epoch: 6/10, step 516/574 completed (loss: 0.33587244153022766, acc: 0.875)
[2024-11-13 06:48:08,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:08,981][root][INFO] - Training Epoch: 6/10, step 517/574 completed (loss: 0.011922725476324558, acc: 1.0)
[2024-11-13 06:48:09,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:09,331][root][INFO] - Training Epoch: 6/10, step 518/574 completed (loss: 0.0782446637749672, acc: 0.9677419066429138)
[2024-11-13 06:48:09,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:09,655][root][INFO] - Training Epoch: 6/10, step 519/574 completed (loss: 0.1569160670042038, acc: 0.949999988079071)
[2024-11-13 06:48:09,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:09,993][root][INFO] - Training Epoch: 6/10, step 520/574 completed (loss: 0.2520523965358734, acc: 0.9259259104728699)
[2024-11-13 06:48:10,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:11,591][root][INFO] - Training Epoch: 6/10, step 521/574 completed (loss: 0.6606918573379517, acc: 0.8008474707603455)
[2024-11-13 06:48:11,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:12,010][root][INFO] - Training Epoch: 6/10, step 522/574 completed (loss: 0.2597860097885132, acc: 0.9029850959777832)
[2024-11-13 06:48:12,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:12,496][root][INFO] - Training Epoch: 6/10, step 523/574 completed (loss: 0.2290169596672058, acc: 0.9197080135345459)
[2024-11-13 06:48:12,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:13,329][root][INFO] - Training Epoch: 6/10, step 524/574 completed (loss: 0.5704562664031982, acc: 0.8550000190734863)
[2024-11-13 06:48:13,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:13,698][root][INFO] - Training Epoch: 6/10, step 525/574 completed (loss: 0.023683154955506325, acc: 1.0)
[2024-11-13 06:48:13,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:14,083][root][INFO] - Training Epoch: 6/10, step 526/574 completed (loss: 0.22481149435043335, acc: 0.9230769276618958)
[2024-11-13 06:48:14,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:14,423][root][INFO] - Training Epoch: 6/10, step 527/574 completed (loss: 0.11218719184398651, acc: 0.9523809552192688)
[2024-11-13 06:48:14,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:14,794][root][INFO] - Training Epoch: 6/10, step 528/574 completed (loss: 0.6962876319885254, acc: 0.7704917788505554)
[2024-11-13 06:48:14,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:15,157][root][INFO] - Training Epoch: 6/10, step 529/574 completed (loss: 0.16018624603748322, acc: 0.9491525292396545)
[2024-11-13 06:48:15,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:15,542][root][INFO] - Training Epoch: 6/10, step 530/574 completed (loss: 0.7782910466194153, acc: 0.7906976938247681)
[2024-11-13 06:48:15,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:15,934][root][INFO] - Training Epoch: 6/10, step 531/574 completed (loss: 0.3050546944141388, acc: 0.8863636255264282)
[2024-11-13 06:48:16,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:16,287][root][INFO] - Training Epoch: 6/10, step 532/574 completed (loss: 0.6613025069236755, acc: 0.7735849022865295)
[2024-11-13 06:48:16,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:16,618][root][INFO] - Training Epoch: 6/10, step 533/574 completed (loss: 0.26838418841362, acc: 0.9318181872367859)
[2024-11-13 06:48:16,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:16,958][root][INFO] - Training Epoch: 6/10, step 534/574 completed (loss: 0.16362467408180237, acc: 0.9599999785423279)
[2024-11-13 06:48:17,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:17,315][root][INFO] - Training Epoch: 6/10, step 535/574 completed (loss: 0.07151880860328674, acc: 0.949999988079071)
[2024-11-13 06:48:17,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:17,635][root][INFO] - Training Epoch: 6/10, step 536/574 completed (loss: 0.01493261381983757, acc: 1.0)
[2024-11-13 06:48:17,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:18,173][root][INFO] - Training Epoch: 6/10, step 537/574 completed (loss: 0.317635178565979, acc: 0.9076923131942749)
[2024-11-13 06:48:18,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:18,580][root][INFO] - Training Epoch: 6/10, step 538/574 completed (loss: 0.17520920932292938, acc: 0.9375)
[2024-11-13 06:48:18,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:19,123][root][INFO] - Training Epoch: 6/10, step 539/574 completed (loss: 0.09739118814468384, acc: 1.0)
[2024-11-13 06:48:19,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:19,523][root][INFO] - Training Epoch: 6/10, step 540/574 completed (loss: 0.4334130883216858, acc: 0.8787878751754761)
[2024-11-13 06:48:19,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:19,870][root][INFO] - Training Epoch: 6/10, step 541/574 completed (loss: 0.09539421647787094, acc: 1.0)
[2024-11-13 06:48:20,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:20,215][root][INFO] - Training Epoch: 6/10, step 542/574 completed (loss: 0.096072256565094, acc: 0.9677419066429138)
[2024-11-13 06:48:20,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:20,559][root][INFO] - Training Epoch: 6/10, step 543/574 completed (loss: 0.06557577848434448, acc: 0.95652174949646)
[2024-11-13 06:48:20,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:20,916][root][INFO] - Training Epoch: 6/10, step 544/574 completed (loss: 0.11127202212810516, acc: 0.9666666388511658)
[2024-11-13 06:48:21,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:21,315][root][INFO] - Training Epoch: 6/10, step 545/574 completed (loss: 0.04202786460518837, acc: 0.9756097793579102)
[2024-11-13 06:48:21,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:21,658][root][INFO] - Training Epoch: 6/10, step 546/574 completed (loss: 0.008693471550941467, acc: 1.0)
[2024-11-13 06:48:21,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:22,009][root][INFO] - Training Epoch: 6/10, step 547/574 completed (loss: 0.020583435893058777, acc: 1.0)
[2024-11-13 06:48:22,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:22,428][root][INFO] - Training Epoch: 6/10, step 548/574 completed (loss: 0.0812799483537674, acc: 0.9677419066429138)
[2024-11-13 06:48:22,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:22,790][root][INFO] - Training Epoch: 6/10, step 549/574 completed (loss: 0.00600283220410347, acc: 1.0)
[2024-11-13 06:48:22,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:23,195][root][INFO] - Training Epoch: 6/10, step 550/574 completed (loss: 0.1670462042093277, acc: 0.939393937587738)
[2024-11-13 06:48:23,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:23,556][root][INFO] - Training Epoch: 6/10, step 551/574 completed (loss: 0.048876941204071045, acc: 0.9750000238418579)
[2024-11-13 06:48:23,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:23,945][root][INFO] - Training Epoch: 6/10, step 552/574 completed (loss: 0.03915480151772499, acc: 1.0)
[2024-11-13 06:48:24,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:24,302][root][INFO] - Training Epoch: 6/10, step 553/574 completed (loss: 0.25022512674331665, acc: 0.9051094651222229)
[2024-11-13 06:48:24,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:24,669][root][INFO] - Training Epoch: 6/10, step 554/574 completed (loss: 0.1507055163383484, acc: 0.9655172228813171)
[2024-11-13 06:48:24,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:25,028][root][INFO] - Training Epoch: 6/10, step 555/574 completed (loss: 0.3156249225139618, acc: 0.9071428775787354)
[2024-11-13 06:48:25,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:25,409][root][INFO] - Training Epoch: 6/10, step 556/574 completed (loss: 0.2222871035337448, acc: 0.9337748289108276)
[2024-11-13 06:48:25,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:25,767][root][INFO] - Training Epoch: 6/10, step 557/574 completed (loss: 0.1994006335735321, acc: 0.9487179517745972)
[2024-11-13 06:48:25,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:26,108][root][INFO] - Training Epoch: 6/10, step 558/574 completed (loss: 0.2478804737329483, acc: 0.9599999785423279)
[2024-11-13 06:48:26,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:26,466][root][INFO] - Training Epoch: 6/10, step 559/574 completed (loss: 0.06660256534814835, acc: 0.9615384340286255)
[2024-11-13 06:48:26,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:26,816][root][INFO] - Training Epoch: 6/10, step 560/574 completed (loss: 0.3907029330730438, acc: 0.9230769276618958)
[2024-11-13 06:48:26,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:27,165][root][INFO] - Training Epoch: 6/10, step 561/574 completed (loss: 0.17507553100585938, acc: 0.9487179517745972)
[2024-11-13 06:48:28,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:28,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:28,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:29,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:29,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:30,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:30,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:31,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:31,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:32,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:33,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:33,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:34,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:34,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:35,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:35,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:36,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:36,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:36,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:37,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:37,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:38,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:38,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:39,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:39,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:40,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:40,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:41,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:41,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:42,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:42,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:43,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:43,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:44,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:44,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:45,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:45,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:45,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:46,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:46,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:47,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:47,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:48,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:48,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:49,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:49,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:50,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:50,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:51,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:51,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:52,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:52,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:52,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:53,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:54,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:54,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:55,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:55,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:55,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:56,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:56,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:57,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:58,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:58,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:58,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:59,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:48:59,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:00,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:00,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:01,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:01,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:02,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:02,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:03,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:03,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:03,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:04,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:05,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:05,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:05,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:06,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:06,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:07,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:07,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:08,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:09,192][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.6027, device='cuda:0') eval_epoch_loss=tensor(0.9566, device='cuda:0') eval_epoch_acc=tensor(0.7954, device='cuda:0')
[2024-11-13 06:49:09,193][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:49:09,193][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:49:09,649][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_6_step_562_loss_0.9565608501434326/model.pt
[2024-11-13 06:49:09,656][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:49:09,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:10,139][root][INFO] - Training Epoch: 6/10, step 562/574 completed (loss: 0.2989688217639923, acc: 0.9333333373069763)
[2024-11-13 06:49:10,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:10,507][root][INFO] - Training Epoch: 6/10, step 563/574 completed (loss: 0.2475426197052002, acc: 0.948051929473877)
[2024-11-13 06:49:10,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:10,908][root][INFO] - Training Epoch: 6/10, step 564/574 completed (loss: 0.09547170996665955, acc: 0.9583333134651184)
[2024-11-13 06:49:11,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:11,325][root][INFO] - Training Epoch: 6/10, step 565/574 completed (loss: 0.10637635737657547, acc: 0.9482758641242981)
[2024-11-13 06:49:11,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:11,740][root][INFO] - Training Epoch: 6/10, step 566/574 completed (loss: 0.07936754822731018, acc: 0.988095223903656)
[2024-11-13 06:49:11,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:12,203][root][INFO] - Training Epoch: 6/10, step 567/574 completed (loss: 0.01647246442735195, acc: 1.0)
[2024-11-13 06:49:12,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:12,570][root][INFO] - Training Epoch: 6/10, step 568/574 completed (loss: 0.03819585219025612, acc: 1.0)
[2024-11-13 06:49:12,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:13,049][root][INFO] - Training Epoch: 6/10, step 569/574 completed (loss: 0.2676132619380951, acc: 0.9251337051391602)
[2024-11-13 06:49:13,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:13,468][root][INFO] - Training Epoch: 6/10, step 570/574 completed (loss: 0.01651320792734623, acc: 1.0)
[2024-11-13 06:49:13,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:13,871][root][INFO] - Training Epoch: 6/10, step 571/574 completed (loss: 0.1920221596956253, acc: 0.9572649598121643)
[2024-11-13 06:49:14,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:14,271][root][INFO] - Training Epoch: 6/10, step 572/574 completed (loss: 0.4534344971179962, acc: 0.8826530575752258)
[2024-11-13 06:49:14,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:14,692][root][INFO] - Training Epoch: 6/10, step 573/574 completed (loss: 0.37221652269363403, acc: 0.893081784248352)
[2024-11-13 06:49:15,131][slam_llm.utils.train_utils][INFO] - Epoch 6: train_perplexity=1.3489, train_epoch_loss=0.2993, epoch time 453.72398277558386s
[2024-11-13 06:49:15,131][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2024-11-13 06:49:15,131][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 17 GB
[2024-11-13 06:49:15,131][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2024-11-13 06:49:15,131][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 18
[2024-11-13 06:49:15,131][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-13 06:49:15,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:16,045][root][INFO] - Training Epoch: 7/10, step 0/574 completed (loss: 0.14521433413028717, acc: 0.9629629850387573)
[2024-11-13 06:49:16,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:16,416][root][INFO] - Training Epoch: 7/10, step 1/574 completed (loss: 0.14592762291431427, acc: 0.9599999785423279)
[2024-11-13 06:49:16,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:16,780][root][INFO] - Training Epoch: 7/10, step 2/574 completed (loss: 0.5020450353622437, acc: 0.8918918967247009)
[2024-11-13 06:49:17,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:17,235][root][INFO] - Training Epoch: 7/10, step 3/574 completed (loss: 0.12644733488559723, acc: 0.9736841917037964)
[2024-11-13 06:49:17,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:17,671][root][INFO] - Training Epoch: 7/10, step 4/574 completed (loss: 0.16865740716457367, acc: 0.9459459185600281)
[2024-11-13 06:49:17,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:18,003][root][INFO] - Training Epoch: 7/10, step 5/574 completed (loss: 0.07696601003408432, acc: 0.9642857313156128)
[2024-11-13 06:49:18,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:18,431][root][INFO] - Training Epoch: 7/10, step 6/574 completed (loss: 0.5112113952636719, acc: 0.8163265585899353)
[2024-11-13 06:49:18,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:18,878][root][INFO] - Training Epoch: 7/10, step 7/574 completed (loss: 0.2763526141643524, acc: 0.9333333373069763)
[2024-11-13 06:49:19,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:19,291][root][INFO] - Training Epoch: 7/10, step 8/574 completed (loss: 0.009891713038086891, acc: 1.0)
[2024-11-13 06:49:19,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:19,747][root][INFO] - Training Epoch: 7/10, step 9/574 completed (loss: 0.028350425884127617, acc: 1.0)
[2024-11-13 06:49:19,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:20,161][root][INFO] - Training Epoch: 7/10, step 10/574 completed (loss: 0.17964620888233185, acc: 0.8888888955116272)
[2024-11-13 06:49:20,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:20,546][root][INFO] - Training Epoch: 7/10, step 11/574 completed (loss: 0.1933407485485077, acc: 0.9230769276618958)
[2024-11-13 06:49:20,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:20,964][root][INFO] - Training Epoch: 7/10, step 12/574 completed (loss: 0.01580326445400715, acc: 1.0)
[2024-11-13 06:49:21,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:21,299][root][INFO] - Training Epoch: 7/10, step 13/574 completed (loss: 0.1108270138502121, acc: 0.95652174949646)
[2024-11-13 06:49:21,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:21,676][root][INFO] - Training Epoch: 7/10, step 14/574 completed (loss: 0.07162139564752579, acc: 0.9607843160629272)
[2024-11-13 06:49:21,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:22,085][root][INFO] - Training Epoch: 7/10, step 15/574 completed (loss: 0.1605680137872696, acc: 0.918367326259613)
[2024-11-13 06:49:22,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:22,499][root][INFO] - Training Epoch: 7/10, step 16/574 completed (loss: 0.3658696711063385, acc: 0.8947368264198303)
[2024-11-13 06:49:22,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:22,922][root][INFO] - Training Epoch: 7/10, step 17/574 completed (loss: 0.2918260097503662, acc: 0.9166666865348816)
[2024-11-13 06:49:23,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:23,310][root][INFO] - Training Epoch: 7/10, step 18/574 completed (loss: 0.1710883527994156, acc: 0.9722222089767456)
[2024-11-13 06:49:23,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:23,729][root][INFO] - Training Epoch: 7/10, step 19/574 completed (loss: 0.12557239830493927, acc: 0.9473684430122375)
[2024-11-13 06:49:23,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:24,134][root][INFO] - Training Epoch: 7/10, step 20/574 completed (loss: 0.06596438586711884, acc: 0.9615384340286255)
[2024-11-13 06:49:24,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:24,525][root][INFO] - Training Epoch: 7/10, step 21/574 completed (loss: 0.15643134713172913, acc: 0.931034505367279)
[2024-11-13 06:49:24,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:24,889][root][INFO] - Training Epoch: 7/10, step 22/574 completed (loss: 0.29556939005851746, acc: 0.8799999952316284)
[2024-11-13 06:49:25,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:25,247][root][INFO] - Training Epoch: 7/10, step 23/574 completed (loss: 0.21709945797920227, acc: 0.9523809552192688)
[2024-11-13 06:49:25,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:25,629][root][INFO] - Training Epoch: 7/10, step 24/574 completed (loss: 0.04694976657629013, acc: 1.0)
[2024-11-13 06:49:25,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:26,088][root][INFO] - Training Epoch: 7/10, step 25/574 completed (loss: 0.6599898338317871, acc: 0.7924528121948242)
[2024-11-13 06:49:26,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:26,526][root][INFO] - Training Epoch: 7/10, step 26/574 completed (loss: 0.49760901927948, acc: 0.7945205569267273)
[2024-11-13 06:49:27,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:28,326][root][INFO] - Training Epoch: 7/10, step 27/574 completed (loss: 0.9067158699035645, acc: 0.747035562992096)
[2024-11-13 06:49:28,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:28,757][root][INFO] - Training Epoch: 7/10, step 28/574 completed (loss: 0.16908274590969086, acc: 0.9767441749572754)
[2024-11-13 06:49:28,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:29,157][root][INFO] - Training Epoch: 7/10, step 29/574 completed (loss: 0.39665332436561584, acc: 0.891566276550293)
[2024-11-13 06:49:29,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:29,538][root][INFO] - Training Epoch: 7/10, step 30/574 completed (loss: 0.6022921204566956, acc: 0.8148148059844971)
[2024-11-13 06:49:29,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:29,913][root][INFO] - Training Epoch: 7/10, step 31/574 completed (loss: 0.09245496988296509, acc: 1.0)
[2024-11-13 06:49:30,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:30,279][root][INFO] - Training Epoch: 7/10, step 32/574 completed (loss: 0.049475859850645065, acc: 1.0)
[2024-11-13 06:49:30,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:30,687][root][INFO] - Training Epoch: 7/10, step 33/574 completed (loss: 0.09534893929958344, acc: 0.95652174949646)
[2024-11-13 06:49:30,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:31,155][root][INFO] - Training Epoch: 7/10, step 34/574 completed (loss: 0.35147756338119507, acc: 0.924369752407074)
[2024-11-13 06:49:31,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:31,617][root][INFO] - Training Epoch: 7/10, step 35/574 completed (loss: 0.2751578390598297, acc: 0.9180327653884888)
[2024-11-13 06:49:31,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:32,097][root][INFO] - Training Epoch: 7/10, step 36/574 completed (loss: 0.42867451906204224, acc: 0.841269850730896)
[2024-11-13 06:49:32,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:32,528][root][INFO] - Training Epoch: 7/10, step 37/574 completed (loss: 0.3074676990509033, acc: 0.8983050584793091)
[2024-11-13 06:49:32,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:33,005][root][INFO] - Training Epoch: 7/10, step 38/574 completed (loss: 0.18594597280025482, acc: 0.954023003578186)
[2024-11-13 06:49:33,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:33,397][root][INFO] - Training Epoch: 7/10, step 39/574 completed (loss: 0.3011868894100189, acc: 0.9047619104385376)
[2024-11-13 06:49:33,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:33,834][root][INFO] - Training Epoch: 7/10, step 40/574 completed (loss: 0.5508042573928833, acc: 0.807692289352417)
[2024-11-13 06:49:34,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:34,304][root][INFO] - Training Epoch: 7/10, step 41/574 completed (loss: 0.12757356464862823, acc: 0.9459459185600281)
[2024-11-13 06:49:34,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:34,733][root][INFO] - Training Epoch: 7/10, step 42/574 completed (loss: 0.4463004767894745, acc: 0.8461538553237915)
[2024-11-13 06:49:35,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:35,272][root][INFO] - Training Epoch: 7/10, step 43/574 completed (loss: 0.26578906178474426, acc: 0.9494949579238892)
[2024-11-13 06:49:35,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:35,858][root][INFO] - Training Epoch: 7/10, step 44/574 completed (loss: 0.265621155500412, acc: 0.8969072103500366)
[2024-11-13 06:49:36,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:36,410][root][INFO] - Training Epoch: 7/10, step 45/574 completed (loss: 0.3124438226222992, acc: 0.8970588445663452)
[2024-11-13 06:49:36,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:36,833][root][INFO] - Training Epoch: 7/10, step 46/574 completed (loss: 0.03575288876891136, acc: 1.0)
[2024-11-13 06:49:37,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:37,247][root][INFO] - Training Epoch: 7/10, step 47/574 completed (loss: 0.08585916459560394, acc: 0.9629629850387573)
[2024-11-13 06:49:37,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:37,602][root][INFO] - Training Epoch: 7/10, step 48/574 completed (loss: 0.23979847133159637, acc: 0.9285714030265808)
[2024-11-13 06:49:37,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:38,031][root][INFO] - Training Epoch: 7/10, step 49/574 completed (loss: 0.1986134946346283, acc: 0.9444444179534912)
[2024-11-13 06:49:38,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:38,457][root][INFO] - Training Epoch: 7/10, step 50/574 completed (loss: 0.34768548607826233, acc: 0.9298245906829834)
[2024-11-13 06:49:38,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:38,836][root][INFO] - Training Epoch: 7/10, step 51/574 completed (loss: 0.24819286167621613, acc: 0.9365079402923584)
[2024-11-13 06:49:38,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:39,232][root][INFO] - Training Epoch: 7/10, step 52/574 completed (loss: 0.3894544839859009, acc: 0.8450704216957092)
[2024-11-13 06:49:39,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:39,868][root][INFO] - Training Epoch: 7/10, step 53/574 completed (loss: 1.2598603963851929, acc: 0.6466666460037231)
[2024-11-13 06:49:39,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:40,297][root][INFO] - Training Epoch: 7/10, step 54/574 completed (loss: 0.23655638098716736, acc: 0.8918918967247009)
[2024-11-13 06:49:40,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:40,700][root][INFO] - Training Epoch: 7/10, step 55/574 completed (loss: 0.027769867330789566, acc: 1.0)
[2024-11-13 06:49:43,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:45,152][root][INFO] - Training Epoch: 7/10, step 56/574 completed (loss: 0.7630154490470886, acc: 0.7747440338134766)
[2024-11-13 06:49:46,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:47,011][root][INFO] - Training Epoch: 7/10, step 57/574 completed (loss: 1.097142219543457, acc: 0.686274528503418)
[2024-11-13 06:49:47,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:47,944][root][INFO] - Training Epoch: 7/10, step 58/574 completed (loss: 0.5450084209442139, acc: 0.8068181872367859)
[2024-11-13 06:49:48,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:48,807][root][INFO] - Training Epoch: 7/10, step 59/574 completed (loss: 0.21531431376934052, acc: 0.9264705777168274)
[2024-11-13 06:49:49,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:49,651][root][INFO] - Training Epoch: 7/10, step 60/574 completed (loss: 0.582224428653717, acc: 0.8260869383811951)
[2024-11-13 06:49:49,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:50,225][root][INFO] - Training Epoch: 7/10, step 61/574 completed (loss: 0.4023962616920471, acc: 0.8500000238418579)
[2024-11-13 06:49:50,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:50,582][root][INFO] - Training Epoch: 7/10, step 62/574 completed (loss: 0.08837608993053436, acc: 0.970588207244873)
[2024-11-13 06:49:50,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:51,043][root][INFO] - Training Epoch: 7/10, step 63/574 completed (loss: 0.12420400977134705, acc: 0.9722222089767456)
[2024-11-13 06:49:51,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:51,527][root][INFO] - Training Epoch: 7/10, step 64/574 completed (loss: 0.1280072033405304, acc: 0.96875)
[2024-11-13 06:49:51,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:51,914][root][INFO] - Training Epoch: 7/10, step 65/574 completed (loss: 0.09116940945386887, acc: 0.9655172228813171)
[2024-11-13 06:49:52,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:52,348][root][INFO] - Training Epoch: 7/10, step 66/574 completed (loss: 0.2968611717224121, acc: 0.9285714030265808)
[2024-11-13 06:49:52,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:52,770][root][INFO] - Training Epoch: 7/10, step 67/574 completed (loss: 0.2709941267967224, acc: 0.9333333373069763)
[2024-11-13 06:49:52,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:53,187][root][INFO] - Training Epoch: 7/10, step 68/574 completed (loss: 0.01612614095211029, acc: 1.0)
[2024-11-13 06:49:53,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:53,618][root][INFO] - Training Epoch: 7/10, step 69/574 completed (loss: 0.13128159940242767, acc: 0.9444444179534912)
[2024-11-13 06:49:53,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:54,064][root][INFO] - Training Epoch: 7/10, step 70/574 completed (loss: 0.24302835762500763, acc: 0.9090909361839294)
[2024-11-13 06:49:54,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:54,504][root][INFO] - Training Epoch: 7/10, step 71/574 completed (loss: 0.6390907764434814, acc: 0.779411792755127)
[2024-11-13 06:49:54,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:54,919][root][INFO] - Training Epoch: 7/10, step 72/574 completed (loss: 0.5630937814712524, acc: 0.8253968358039856)
[2024-11-13 06:49:55,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:55,383][root][INFO] - Training Epoch: 7/10, step 73/574 completed (loss: 0.9719697833061218, acc: 0.728205144405365)
[2024-11-13 06:49:55,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:55,850][root][INFO] - Training Epoch: 7/10, step 74/574 completed (loss: 0.6241723895072937, acc: 0.8367347121238708)
[2024-11-13 06:49:56,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:56,291][root][INFO] - Training Epoch: 7/10, step 75/574 completed (loss: 0.9726378917694092, acc: 0.746268630027771)
[2024-11-13 06:49:56,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:56,819][root][INFO] - Training Epoch: 7/10, step 76/574 completed (loss: 1.3433343172073364, acc: 0.6423357725143433)
[2024-11-13 06:49:56,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:57,200][root][INFO] - Training Epoch: 7/10, step 77/574 completed (loss: 0.005078264512121677, acc: 1.0)
[2024-11-13 06:49:57,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:57,561][root][INFO] - Training Epoch: 7/10, step 78/574 completed (loss: 0.1606956273317337, acc: 0.9166666865348816)
[2024-11-13 06:49:57,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:57,953][root][INFO] - Training Epoch: 7/10, step 79/574 completed (loss: 0.10956484079360962, acc: 0.9696969985961914)
[2024-11-13 06:49:58,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:58,374][root][INFO] - Training Epoch: 7/10, step 80/574 completed (loss: 0.11209069192409515, acc: 0.9230769276618958)
[2024-11-13 06:49:58,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:58,809][root][INFO] - Training Epoch: 7/10, step 81/574 completed (loss: 0.17106281220912933, acc: 0.9615384340286255)
[2024-11-13 06:49:58,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:59,177][root][INFO] - Training Epoch: 7/10, step 82/574 completed (loss: 0.26071447134017944, acc: 0.942307710647583)
[2024-11-13 06:49:59,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:59,548][root][INFO] - Training Epoch: 7/10, step 83/574 completed (loss: 0.11750432103872299, acc: 0.96875)
[2024-11-13 06:49:59,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:49:59,975][root][INFO] - Training Epoch: 7/10, step 84/574 completed (loss: 0.1562950313091278, acc: 0.95652174949646)
[2024-11-13 06:50:00,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:00,403][root][INFO] - Training Epoch: 7/10, step 85/574 completed (loss: 0.1041991114616394, acc: 1.0)
[2024-11-13 06:50:00,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:00,786][root][INFO] - Training Epoch: 7/10, step 86/574 completed (loss: 0.0641089454293251, acc: 1.0)
[2024-11-13 06:50:01,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:01,440][root][INFO] - Training Epoch: 7/10, step 87/574 completed (loss: 0.5255546569824219, acc: 0.8399999737739563)
[2024-11-13 06:50:01,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:01,858][root][INFO] - Training Epoch: 7/10, step 88/574 completed (loss: 0.7141662836074829, acc: 0.8155339956283569)
[2024-11-13 06:50:02,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:03,565][root][INFO] - Training Epoch: 7/10, step 89/574 completed (loss: 0.7125840187072754, acc: 0.8009708523750305)
[2024-11-13 06:50:04,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:04,881][root][INFO] - Training Epoch: 7/10, step 90/574 completed (loss: 0.9294305443763733, acc: 0.725806474685669)
[2024-11-13 06:50:05,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:06,126][root][INFO] - Training Epoch: 7/10, step 91/574 completed (loss: 0.6469541192054749, acc: 0.8103448152542114)
[2024-11-13 06:50:06,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:07,279][root][INFO] - Training Epoch: 7/10, step 92/574 completed (loss: 0.5704171657562256, acc: 0.8526315689086914)
[2024-11-13 06:50:08,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:08,860][root][INFO] - Training Epoch: 7/10, step 93/574 completed (loss: 0.951850950717926, acc: 0.7326732873916626)
[2024-11-13 06:50:08,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:09,232][root][INFO] - Training Epoch: 7/10, step 94/574 completed (loss: 0.5265228152275085, acc: 0.8225806355476379)
[2024-11-13 06:50:09,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:09,684][root][INFO] - Training Epoch: 7/10, step 95/574 completed (loss: 0.2849254310131073, acc: 0.9275362491607666)
[2024-11-13 06:50:09,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:10,135][root][INFO] - Training Epoch: 7/10, step 96/574 completed (loss: 0.7289075255393982, acc: 0.7647058963775635)
[2024-11-13 06:50:10,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:10,565][root][INFO] - Training Epoch: 7/10, step 97/574 completed (loss: 0.6848748922348022, acc: 0.75)
[2024-11-13 06:50:10,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:11,064][root][INFO] - Training Epoch: 7/10, step 98/574 completed (loss: 0.8524986505508423, acc: 0.7518247961997986)
[2024-11-13 06:50:11,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:11,425][root][INFO] - Training Epoch: 7/10, step 99/574 completed (loss: 0.4794030785560608, acc: 0.8507462739944458)
[2024-11-13 06:50:11,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:11,836][root][INFO] - Training Epoch: 7/10, step 100/574 completed (loss: 0.18310758471488953, acc: 0.949999988079071)
[2024-11-13 06:50:12,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:12,224][root][INFO] - Training Epoch: 7/10, step 101/574 completed (loss: 0.006304081063717604, acc: 1.0)
[2024-11-13 06:50:12,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:12,590][root][INFO] - Training Epoch: 7/10, step 102/574 completed (loss: 0.04137932509183884, acc: 1.0)
[2024-11-13 06:50:12,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:13,006][root][INFO] - Training Epoch: 7/10, step 103/574 completed (loss: 0.017246268689632416, acc: 1.0)
[2024-11-13 06:50:13,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:13,416][root][INFO] - Training Epoch: 7/10, step 104/574 completed (loss: 0.07417472451925278, acc: 1.0)
[2024-11-13 06:50:13,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:13,818][root][INFO] - Training Epoch: 7/10, step 105/574 completed (loss: 0.016611767932772636, acc: 1.0)
[2024-11-13 06:50:14,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:14,179][root][INFO] - Training Epoch: 7/10, step 106/574 completed (loss: 0.11919461935758591, acc: 0.9599999785423279)
[2024-11-13 06:50:14,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:14,598][root][INFO] - Training Epoch: 7/10, step 107/574 completed (loss: 0.004119753837585449, acc: 1.0)
[2024-11-13 06:50:14,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:14,957][root][INFO] - Training Epoch: 7/10, step 108/574 completed (loss: 0.0027157426811754704, acc: 1.0)
[2024-11-13 06:50:15,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:15,361][root][INFO] - Training Epoch: 7/10, step 109/574 completed (loss: 0.3059839606285095, acc: 0.9047619104385376)
[2024-11-13 06:50:15,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:15,779][root][INFO] - Training Epoch: 7/10, step 110/574 completed (loss: 0.13395467400550842, acc: 0.9384615421295166)
[2024-11-13 06:50:16,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:16,326][root][INFO] - Training Epoch: 7/10, step 111/574 completed (loss: 0.3066819906234741, acc: 0.9298245906829834)
[2024-11-13 06:50:16,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:16,800][root][INFO] - Training Epoch: 7/10, step 112/574 completed (loss: 0.284349650144577, acc: 0.9122806787490845)
[2024-11-13 06:50:16,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:17,205][root][INFO] - Training Epoch: 7/10, step 113/574 completed (loss: 0.15242479741573334, acc: 0.9743589758872986)
[2024-11-13 06:50:17,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:17,678][root][INFO] - Training Epoch: 7/10, step 114/574 completed (loss: 0.09941484779119492, acc: 0.9795918464660645)
[2024-11-13 06:50:17,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:18,033][root][INFO] - Training Epoch: 7/10, step 115/574 completed (loss: 0.0763736143708229, acc: 1.0)
[2024-11-13 06:50:18,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:18,493][root][INFO] - Training Epoch: 7/10, step 116/574 completed (loss: 0.3422194719314575, acc: 0.8888888955116272)
[2024-11-13 06:50:18,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:18,974][root][INFO] - Training Epoch: 7/10, step 117/574 completed (loss: 0.40661925077438354, acc: 0.8861788511276245)
[2024-11-13 06:50:19,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:19,431][root][INFO] - Training Epoch: 7/10, step 118/574 completed (loss: 0.09997875243425369, acc: 0.9838709831237793)
[2024-11-13 06:50:20,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:20,714][root][INFO] - Training Epoch: 7/10, step 119/574 completed (loss: 0.6366609930992126, acc: 0.8136882185935974)
[2024-11-13 06:50:20,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:21,197][root][INFO] - Training Epoch: 7/10, step 120/574 completed (loss: 0.09509462118148804, acc: 0.9733333587646484)
[2024-11-13 06:50:21,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:21,761][root][INFO] - Training Epoch: 7/10, step 121/574 completed (loss: 0.29551732540130615, acc: 0.942307710647583)
[2024-11-13 06:50:21,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:22,128][root][INFO] - Training Epoch: 7/10, step 122/574 completed (loss: 0.05277184769511223, acc: 0.9583333134651184)
[2024-11-13 06:50:22,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:22,475][root][INFO] - Training Epoch: 7/10, step 123/574 completed (loss: 0.2902524471282959, acc: 0.9473684430122375)
[2024-11-13 06:50:22,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:22,876][root][INFO] - Training Epoch: 7/10, step 124/574 completed (loss: 0.6741599440574646, acc: 0.8098159432411194)
[2024-11-13 06:50:23,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:23,400][root][INFO] - Training Epoch: 7/10, step 125/574 completed (loss: 0.8176901340484619, acc: 0.7638888955116272)
[2024-11-13 06:50:23,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:23,799][root][INFO] - Training Epoch: 7/10, step 126/574 completed (loss: 0.6481741070747375, acc: 0.7749999761581421)
[2024-11-13 06:50:23,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:24,209][root][INFO] - Training Epoch: 7/10, step 127/574 completed (loss: 0.569682240486145, acc: 0.8095238208770752)
[2024-11-13 06:50:24,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:24,629][root][INFO] - Training Epoch: 7/10, step 128/574 completed (loss: 0.633111834526062, acc: 0.8051282167434692)
[2024-11-13 06:50:24,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:25,173][root][INFO] - Training Epoch: 7/10, step 129/574 completed (loss: 0.5629991888999939, acc: 0.8235294222831726)
[2024-11-13 06:50:25,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:25,533][root][INFO] - Training Epoch: 7/10, step 130/574 completed (loss: 0.21541152894496918, acc: 0.9230769276618958)
[2024-11-13 06:50:26,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:27,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:27,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:28,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:28,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:29,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:29,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:29,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:30,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:30,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:31,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:31,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:32,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:33,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:33,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:33,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:34,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:34,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:35,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:35,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:36,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:36,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:37,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:37,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:37,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:38,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:38,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:39,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:39,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:40,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:40,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:41,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:41,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:41,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:42,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:42,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:43,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:43,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:44,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:44,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:45,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:45,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:46,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:46,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:47,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:47,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:48,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:48,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:49,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:49,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:49,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:50,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:51,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:51,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:51,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:52,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:52,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:53,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:53,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:54,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:54,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:55,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:56,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:56,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:57,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:57,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:57,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:58,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:59,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:50:59,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:00,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:00,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:01,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:01,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:01,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:02,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:02,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:03,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:03,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:04,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:04,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:04,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:05,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:06,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:06,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:07,250][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2877, device='cuda:0') eval_epoch_loss=tensor(0.8276, device='cuda:0') eval_epoch_acc=tensor(0.8035, device='cuda:0')
[2024-11-13 06:51:07,251][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:51:07,251][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:51:07,676][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_7_step_131_loss_0.8275540471076965/model.pt
[2024-11-13 06:51:07,684][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:51:07,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:08,036][root][INFO] - Training Epoch: 7/10, step 131/574 completed (loss: 0.2540726959705353, acc: 0.9130434989929199)
[2024-11-13 06:51:08,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:08,376][root][INFO] - Training Epoch: 7/10, step 132/574 completed (loss: 0.17090098559856415, acc: 0.96875)
[2024-11-13 06:51:08,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:08,720][root][INFO] - Training Epoch: 7/10, step 133/574 completed (loss: 0.18319134414196014, acc: 0.95652174949646)
[2024-11-13 06:51:08,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:09,141][root][INFO] - Training Epoch: 7/10, step 134/574 completed (loss: 0.250817209482193, acc: 0.9142857193946838)
[2024-11-13 06:51:09,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:09,518][root][INFO] - Training Epoch: 7/10, step 135/574 completed (loss: 0.1708589494228363, acc: 0.9230769276618958)
[2024-11-13 06:51:09,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:09,893][root][INFO] - Training Epoch: 7/10, step 136/574 completed (loss: 0.3229686915874481, acc: 0.9047619104385376)
[2024-11-13 06:51:10,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:10,303][root][INFO] - Training Epoch: 7/10, step 137/574 completed (loss: 0.25825440883636475, acc: 0.9666666388511658)
[2024-11-13 06:51:10,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:10,707][root][INFO] - Training Epoch: 7/10, step 138/574 completed (loss: 0.23874686658382416, acc: 0.8695651888847351)
[2024-11-13 06:51:10,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:11,118][root][INFO] - Training Epoch: 7/10, step 139/574 completed (loss: 0.025523023679852486, acc: 1.0)
[2024-11-13 06:51:11,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:11,555][root][INFO] - Training Epoch: 7/10, step 140/574 completed (loss: 0.060944873839616776, acc: 1.0)
[2024-11-13 06:51:11,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:11,981][root][INFO] - Training Epoch: 7/10, step 141/574 completed (loss: 0.11196280270814896, acc: 0.9677419066429138)
[2024-11-13 06:51:12,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:12,369][root][INFO] - Training Epoch: 7/10, step 142/574 completed (loss: 0.10100128501653671, acc: 0.9459459185600281)
[2024-11-13 06:51:12,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:13,143][root][INFO] - Training Epoch: 7/10, step 143/574 completed (loss: 0.45829257369041443, acc: 0.8333333134651184)
[2024-11-13 06:51:13,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:13,598][root][INFO] - Training Epoch: 7/10, step 144/574 completed (loss: 0.6491107940673828, acc: 0.8432835936546326)
[2024-11-13 06:51:13,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:14,024][root][INFO] - Training Epoch: 7/10, step 145/574 completed (loss: 0.2472018003463745, acc: 0.918367326259613)
[2024-11-13 06:51:14,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:14,659][root][INFO] - Training Epoch: 7/10, step 146/574 completed (loss: 0.5511258840560913, acc: 0.8297872543334961)
[2024-11-13 06:51:14,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:15,078][root][INFO] - Training Epoch: 7/10, step 147/574 completed (loss: 0.14544688165187836, acc: 0.9571428298950195)
[2024-11-13 06:51:15,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:15,485][root][INFO] - Training Epoch: 7/10, step 148/574 completed (loss: 0.15630993247032166, acc: 0.9642857313156128)
[2024-11-13 06:51:15,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:15,860][root][INFO] - Training Epoch: 7/10, step 149/574 completed (loss: 0.2041478306055069, acc: 0.95652174949646)
[2024-11-13 06:51:16,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:16,274][root][INFO] - Training Epoch: 7/10, step 150/574 completed (loss: 0.26353633403778076, acc: 0.931034505367279)
[2024-11-13 06:51:16,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:16,760][root][INFO] - Training Epoch: 7/10, step 151/574 completed (loss: 0.35598716139793396, acc: 0.9130434989929199)
[2024-11-13 06:51:16,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:17,215][root][INFO] - Training Epoch: 7/10, step 152/574 completed (loss: 0.41809794306755066, acc: 0.8983050584793091)
[2024-11-13 06:51:17,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:17,622][root][INFO] - Training Epoch: 7/10, step 153/574 completed (loss: 0.2070256620645523, acc: 0.9298245906829834)
[2024-11-13 06:51:17,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:18,055][root][INFO] - Training Epoch: 7/10, step 154/574 completed (loss: 0.20752224326133728, acc: 0.9459459185600281)
[2024-11-13 06:51:18,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:18,412][root][INFO] - Training Epoch: 7/10, step 155/574 completed (loss: 0.08699791133403778, acc: 1.0)
[2024-11-13 06:51:18,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:18,823][root][INFO] - Training Epoch: 7/10, step 156/574 completed (loss: 0.029608165845274925, acc: 1.0)
[2024-11-13 06:51:19,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:19,290][root][INFO] - Training Epoch: 7/10, step 157/574 completed (loss: 0.8359450101852417, acc: 0.8421052694320679)
[2024-11-13 06:51:20,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:21,908][root][INFO] - Training Epoch: 7/10, step 158/574 completed (loss: 0.5384331941604614, acc: 0.8918918967247009)
[2024-11-13 06:51:22,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:22,330][root][INFO] - Training Epoch: 7/10, step 159/574 completed (loss: 0.4292547106742859, acc: 0.8703703880310059)
[2024-11-13 06:51:22,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:22,886][root][INFO] - Training Epoch: 7/10, step 160/574 completed (loss: 0.8373351097106934, acc: 0.7441860437393188)
[2024-11-13 06:51:23,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:23,794][root][INFO] - Training Epoch: 7/10, step 161/574 completed (loss: 0.7875646948814392, acc: 0.7411764860153198)
[2024-11-13 06:51:24,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:24,640][root][INFO] - Training Epoch: 7/10, step 162/574 completed (loss: 0.8763816952705383, acc: 0.7528089880943298)
[2024-11-13 06:51:24,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:25,050][root][INFO] - Training Epoch: 7/10, step 163/574 completed (loss: 0.24431151151657104, acc: 0.8863636255264282)
[2024-11-13 06:51:25,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:25,472][root][INFO] - Training Epoch: 7/10, step 164/574 completed (loss: 0.13104009628295898, acc: 0.9523809552192688)
[2024-11-13 06:51:25,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:25,840][root][INFO] - Training Epoch: 7/10, step 165/574 completed (loss: 0.659511923789978, acc: 0.7586206793785095)
[2024-11-13 06:51:26,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:26,247][root][INFO] - Training Epoch: 7/10, step 166/574 completed (loss: 0.07450548559427261, acc: 0.9795918464660645)
[2024-11-13 06:51:26,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:26,669][root][INFO] - Training Epoch: 7/10, step 167/574 completed (loss: 0.15274757146835327, acc: 0.9399999976158142)
[2024-11-13 06:51:26,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:27,209][root][INFO] - Training Epoch: 7/10, step 168/574 completed (loss: 0.1549055576324463, acc: 0.9166666865348816)
[2024-11-13 06:51:27,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:27,596][root][INFO] - Training Epoch: 7/10, step 169/574 completed (loss: 0.8368821740150452, acc: 0.7941176295280457)
[2024-11-13 06:51:28,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:29,249][root][INFO] - Training Epoch: 7/10, step 170/574 completed (loss: 0.6977376937866211, acc: 0.801369845867157)
[2024-11-13 06:51:29,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:29,637][root][INFO] - Training Epoch: 7/10, step 171/574 completed (loss: 0.053425710648298264, acc: 1.0)
[2024-11-13 06:51:29,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:30,007][root][INFO] - Training Epoch: 7/10, step 172/574 completed (loss: 0.4404275417327881, acc: 0.8518518805503845)
[2024-11-13 06:51:30,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:30,369][root][INFO] - Training Epoch: 7/10, step 173/574 completed (loss: 0.1380620002746582, acc: 1.0)
[2024-11-13 06:51:30,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:31,188][root][INFO] - Training Epoch: 7/10, step 174/574 completed (loss: 0.7952935695648193, acc: 0.7876105904579163)
[2024-11-13 06:51:31,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:31,612][root][INFO] - Training Epoch: 7/10, step 175/574 completed (loss: 0.4683009088039398, acc: 0.8405796885490417)
[2024-11-13 06:51:31,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:32,112][root][INFO] - Training Epoch: 7/10, step 176/574 completed (loss: 0.3730589747428894, acc: 0.8636363744735718)
[2024-11-13 06:51:32,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:33,566][root][INFO] - Training Epoch: 7/10, step 177/574 completed (loss: 0.9414879679679871, acc: 0.7862595319747925)
[2024-11-13 06:51:34,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:34,601][root][INFO] - Training Epoch: 7/10, step 178/574 completed (loss: 0.6052068471908569, acc: 0.8592592477798462)
[2024-11-13 06:51:34,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:35,026][root][INFO] - Training Epoch: 7/10, step 179/574 completed (loss: 0.2982226610183716, acc: 0.8852459192276001)
[2024-11-13 06:51:35,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:35,425][root][INFO] - Training Epoch: 7/10, step 180/574 completed (loss: 0.0721530020236969, acc: 0.9583333134651184)
[2024-11-13 06:51:35,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:35,830][root][INFO] - Training Epoch: 7/10, step 181/574 completed (loss: 0.07880143821239471, acc: 0.9599999785423279)
[2024-11-13 06:51:36,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:36,235][root][INFO] - Training Epoch: 7/10, step 182/574 completed (loss: 0.03176148980855942, acc: 1.0)
[2024-11-13 06:51:36,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:36,659][root][INFO] - Training Epoch: 7/10, step 183/574 completed (loss: 0.1261005997657776, acc: 0.9512194991111755)
[2024-11-13 06:51:36,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:37,127][root][INFO] - Training Epoch: 7/10, step 184/574 completed (loss: 0.3774717450141907, acc: 0.903323233127594)
[2024-11-13 06:51:37,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:37,588][root][INFO] - Training Epoch: 7/10, step 185/574 completed (loss: 0.5567862391471863, acc: 0.8587896227836609)
[2024-11-13 06:51:37,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:38,276][root][INFO] - Training Epoch: 7/10, step 186/574 completed (loss: 0.5069850087165833, acc: 0.831250011920929)
[2024-11-13 06:51:38,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:39,009][root][INFO] - Training Epoch: 7/10, step 187/574 completed (loss: 0.5768256187438965, acc: 0.8348968029022217)
[2024-11-13 06:51:39,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:39,553][root][INFO] - Training Epoch: 7/10, step 188/574 completed (loss: 0.42231643199920654, acc: 0.871886134147644)
[2024-11-13 06:51:39,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:39,971][root][INFO] - Training Epoch: 7/10, step 189/574 completed (loss: 0.08823242783546448, acc: 0.9599999785423279)
[2024-11-13 06:51:40,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:40,812][root][INFO] - Training Epoch: 7/10, step 190/574 completed (loss: 0.4749544858932495, acc: 0.8720930218696594)
[2024-11-13 06:51:41,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:42,079][root][INFO] - Training Epoch: 7/10, step 191/574 completed (loss: 0.8340004682540894, acc: 0.7539682388305664)
[2024-11-13 06:51:42,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:43,560][root][INFO] - Training Epoch: 7/10, step 192/574 completed (loss: 0.7492156624794006, acc: 0.7878788113594055)
[2024-11-13 06:51:44,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:44,734][root][INFO] - Training Epoch: 7/10, step 193/574 completed (loss: 0.3928208649158478, acc: 0.8588235378265381)
[2024-11-13 06:51:45,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:46,484][root][INFO] - Training Epoch: 7/10, step 194/574 completed (loss: 0.728103518486023, acc: 0.790123462677002)
[2024-11-13 06:51:47,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:48,025][root][INFO] - Training Epoch: 7/10, step 195/574 completed (loss: 0.1681395322084427, acc: 0.9677419066429138)
[2024-11-13 06:51:48,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:48,438][root][INFO] - Training Epoch: 7/10, step 196/574 completed (loss: 0.23853692412376404, acc: 0.9285714030265808)
[2024-11-13 06:51:48,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:48,897][root][INFO] - Training Epoch: 7/10, step 197/574 completed (loss: 0.16574351489543915, acc: 0.925000011920929)
[2024-11-13 06:51:49,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:49,296][root][INFO] - Training Epoch: 7/10, step 198/574 completed (loss: 0.2917912006378174, acc: 0.9264705777168274)
[2024-11-13 06:51:49,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:49,659][root][INFO] - Training Epoch: 7/10, step 199/574 completed (loss: 0.5553936958312988, acc: 0.8602941036224365)
[2024-11-13 06:51:49,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:50,059][root][INFO] - Training Epoch: 7/10, step 200/574 completed (loss: 0.5012009143829346, acc: 0.8389830589294434)
[2024-11-13 06:51:50,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:50,438][root][INFO] - Training Epoch: 7/10, step 201/574 completed (loss: 0.6211104393005371, acc: 0.7761194109916687)
[2024-11-13 06:51:50,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:50,866][root][INFO] - Training Epoch: 7/10, step 202/574 completed (loss: 0.5336657166481018, acc: 0.8349514603614807)
[2024-11-13 06:51:51,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:51,172][root][INFO] - Training Epoch: 7/10, step 203/574 completed (loss: 0.24605290591716766, acc: 0.9047619104385376)
[2024-11-13 06:51:51,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:51,515][root][INFO] - Training Epoch: 7/10, step 204/574 completed (loss: 0.13634024560451508, acc: 0.9670329689979553)
[2024-11-13 06:51:51,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:51,914][root][INFO] - Training Epoch: 7/10, step 205/574 completed (loss: 0.2709704637527466, acc: 0.9013453125953674)
[2024-11-13 06:51:52,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:52,458][root][INFO] - Training Epoch: 7/10, step 206/574 completed (loss: 0.40190282464027405, acc: 0.874015748500824)
[2024-11-13 06:51:52,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:52,869][root][INFO] - Training Epoch: 7/10, step 207/574 completed (loss: 0.23197373747825623, acc: 0.9568965435028076)
[2024-11-13 06:51:53,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:53,307][root][INFO] - Training Epoch: 7/10, step 208/574 completed (loss: 0.3412781059741974, acc: 0.8985507488250732)
[2024-11-13 06:51:53,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:53,775][root][INFO] - Training Epoch: 7/10, step 209/574 completed (loss: 0.26316747069358826, acc: 0.9105058312416077)
[2024-11-13 06:51:53,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:54,150][root][INFO] - Training Epoch: 7/10, step 210/574 completed (loss: 0.1994095891714096, acc: 0.9347826242446899)
[2024-11-13 06:51:54,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:54,521][root][INFO] - Training Epoch: 7/10, step 211/574 completed (loss: 0.0676787719130516, acc: 0.95652174949646)
[2024-11-13 06:51:54,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:54,892][root][INFO] - Training Epoch: 7/10, step 212/574 completed (loss: 0.0437677800655365, acc: 1.0)
[2024-11-13 06:51:55,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:55,263][root][INFO] - Training Epoch: 7/10, step 213/574 completed (loss: 0.07892151176929474, acc: 1.0)
[2024-11-13 06:51:55,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:56,309][root][INFO] - Training Epoch: 7/10, step 214/574 completed (loss: 0.19218336045742035, acc: 0.9615384340286255)
[2024-11-13 06:51:56,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:56,715][root][INFO] - Training Epoch: 7/10, step 215/574 completed (loss: 0.07324645668268204, acc: 0.9729729890823364)
[2024-11-13 06:51:56,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:57,112][root][INFO] - Training Epoch: 7/10, step 216/574 completed (loss: 0.06698401272296906, acc: 0.9651162624359131)
[2024-11-13 06:51:57,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:57,914][root][INFO] - Training Epoch: 7/10, step 217/574 completed (loss: 0.14002035558223724, acc: 0.9639639854431152)
[2024-11-13 06:51:58,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:58,450][root][INFO] - Training Epoch: 7/10, step 218/574 completed (loss: 0.11665911227464676, acc: 0.9555555582046509)
[2024-11-13 06:51:58,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:58,799][root][INFO] - Training Epoch: 7/10, step 219/574 completed (loss: 0.02617521397769451, acc: 1.0)
[2024-11-13 06:51:58,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:59,157][root][INFO] - Training Epoch: 7/10, step 220/574 completed (loss: 0.02455759234726429, acc: 1.0)
[2024-11-13 06:51:59,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:59,544][root][INFO] - Training Epoch: 7/10, step 221/574 completed (loss: 0.1045365110039711, acc: 0.9599999785423279)
[2024-11-13 06:51:59,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:51:59,948][root][INFO] - Training Epoch: 7/10, step 222/574 completed (loss: 0.5755401849746704, acc: 0.7884615659713745)
[2024-11-13 06:52:00,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:01,123][root][INFO] - Training Epoch: 7/10, step 223/574 completed (loss: 0.3920139670372009, acc: 0.8913043737411499)
[2024-11-13 06:52:01,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:01,932][root][INFO] - Training Epoch: 7/10, step 224/574 completed (loss: 0.510570764541626, acc: 0.8579545617103577)
[2024-11-13 06:52:02,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:02,567][root][INFO] - Training Epoch: 7/10, step 225/574 completed (loss: 0.5707519054412842, acc: 0.8085106611251831)
[2024-11-13 06:52:02,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:03,026][root][INFO] - Training Epoch: 7/10, step 226/574 completed (loss: 0.18806758522987366, acc: 0.9056603908538818)
[2024-11-13 06:52:03,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:03,442][root][INFO] - Training Epoch: 7/10, step 227/574 completed (loss: 0.13588739931583405, acc: 0.9666666388511658)
[2024-11-13 06:52:03,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:03,824][root][INFO] - Training Epoch: 7/10, step 228/574 completed (loss: 0.1859413981437683, acc: 0.930232584476471)
[2024-11-13 06:52:03,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:04,198][root][INFO] - Training Epoch: 7/10, step 229/574 completed (loss: 0.31501784920692444, acc: 0.8999999761581421)
[2024-11-13 06:52:04,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:04,666][root][INFO] - Training Epoch: 7/10, step 230/574 completed (loss: 1.1505612134933472, acc: 0.6526315808296204)
[2024-11-13 06:52:04,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:05,028][root][INFO] - Training Epoch: 7/10, step 231/574 completed (loss: 0.8133119344711304, acc: 0.7111111283302307)
[2024-11-13 06:52:05,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:05,613][root][INFO] - Training Epoch: 7/10, step 232/574 completed (loss: 1.1490952968597412, acc: 0.6666666865348816)
[2024-11-13 06:52:05,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:06,324][root][INFO] - Training Epoch: 7/10, step 233/574 completed (loss: 1.3961553573608398, acc: 0.646789014339447)
[2024-11-13 06:52:06,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:07,011][root][INFO] - Training Epoch: 7/10, step 234/574 completed (loss: 0.9629610180854797, acc: 0.7153846025466919)
[2024-11-13 06:52:07,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:07,404][root][INFO] - Training Epoch: 7/10, step 235/574 completed (loss: 0.02310267649590969, acc: 1.0)
[2024-11-13 06:52:07,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:07,764][root][INFO] - Training Epoch: 7/10, step 236/574 completed (loss: 0.024447256699204445, acc: 1.0)
[2024-11-13 06:52:07,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:08,118][root][INFO] - Training Epoch: 7/10, step 237/574 completed (loss: 0.38824328780174255, acc: 0.8636363744735718)
[2024-11-13 06:52:08,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:08,489][root][INFO] - Training Epoch: 7/10, step 238/574 completed (loss: 0.18853676319122314, acc: 0.9259259104728699)
[2024-11-13 06:52:08,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:08,867][root][INFO] - Training Epoch: 7/10, step 239/574 completed (loss: 0.12699010968208313, acc: 0.9714285731315613)
[2024-11-13 06:52:09,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:09,330][root][INFO] - Training Epoch: 7/10, step 240/574 completed (loss: 0.23836968839168549, acc: 0.9545454382896423)
[2024-11-13 06:52:09,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:09,729][root][INFO] - Training Epoch: 7/10, step 241/574 completed (loss: 0.1520119607448578, acc: 0.9318181872367859)
[2024-11-13 06:52:10,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:10,618][root][INFO] - Training Epoch: 7/10, step 242/574 completed (loss: 0.4591220021247864, acc: 0.8387096524238586)
[2024-11-13 06:52:11,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:11,419][root][INFO] - Training Epoch: 7/10, step 243/574 completed (loss: 0.5964788198471069, acc: 0.8636363744735718)
[2024-11-13 06:52:11,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:11,758][root][INFO] - Training Epoch: 7/10, step 244/574 completed (loss: 0.011919102631509304, acc: 1.0)
[2024-11-13 06:52:11,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:12,097][root][INFO] - Training Epoch: 7/10, step 245/574 completed (loss: 0.17896080017089844, acc: 0.9230769276618958)
[2024-11-13 06:52:12,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:12,440][root][INFO] - Training Epoch: 7/10, step 246/574 completed (loss: 0.16120901703834534, acc: 0.9677419066429138)
[2024-11-13 06:52:12,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:12,785][root][INFO] - Training Epoch: 7/10, step 247/574 completed (loss: 0.018188107758760452, acc: 1.0)
[2024-11-13 06:52:12,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:13,231][root][INFO] - Training Epoch: 7/10, step 248/574 completed (loss: 0.09768736362457275, acc: 0.9729729890823364)
[2024-11-13 06:52:13,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:13,580][root][INFO] - Training Epoch: 7/10, step 249/574 completed (loss: 0.11618924885988235, acc: 0.9729729890823364)
[2024-11-13 06:52:13,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:13,955][root][INFO] - Training Epoch: 7/10, step 250/574 completed (loss: 0.01452193409204483, acc: 1.0)
[2024-11-13 06:52:14,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:14,359][root][INFO] - Training Epoch: 7/10, step 251/574 completed (loss: 0.23394721746444702, acc: 0.8970588445663452)
[2024-11-13 06:52:14,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:14,725][root][INFO] - Training Epoch: 7/10, step 252/574 completed (loss: 0.09216651320457458, acc: 0.9756097793579102)
[2024-11-13 06:52:14,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:15,067][root][INFO] - Training Epoch: 7/10, step 253/574 completed (loss: 0.025233253836631775, acc: 1.0)
[2024-11-13 06:52:15,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:15,411][root][INFO] - Training Epoch: 7/10, step 254/574 completed (loss: 0.0026252411771565676, acc: 1.0)
[2024-11-13 06:52:15,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:15,772][root][INFO] - Training Epoch: 7/10, step 255/574 completed (loss: 0.005215751472860575, acc: 1.0)
[2024-11-13 06:52:15,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:16,140][root][INFO] - Training Epoch: 7/10, step 256/574 completed (loss: 0.05017392709851265, acc: 0.9649122953414917)
[2024-11-13 06:52:16,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:16,506][root][INFO] - Training Epoch: 7/10, step 257/574 completed (loss: 0.02283543162047863, acc: 1.0)
[2024-11-13 06:52:16,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:16,872][root][INFO] - Training Epoch: 7/10, step 258/574 completed (loss: 0.09698144346475601, acc: 0.9605262875556946)
[2024-11-13 06:52:17,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:17,717][root][INFO] - Training Epoch: 7/10, step 259/574 completed (loss: 0.3816002607345581, acc: 0.9056603908538818)
[2024-11-13 06:52:18,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:18,597][root][INFO] - Training Epoch: 7/10, step 260/574 completed (loss: 0.2923232913017273, acc: 0.8916666507720947)
[2024-11-13 06:52:18,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:18,944][root][INFO] - Training Epoch: 7/10, step 261/574 completed (loss: 0.011547835543751717, acc: 1.0)
[2024-11-13 06:52:19,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:19,288][root][INFO] - Training Epoch: 7/10, step 262/574 completed (loss: 0.059315551072359085, acc: 1.0)
[2024-11-13 06:52:19,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:19,723][root][INFO] - Training Epoch: 7/10, step 263/574 completed (loss: 0.5542976260185242, acc: 0.8266666531562805)
[2024-11-13 06:52:19,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:20,102][root][INFO] - Training Epoch: 7/10, step 264/574 completed (loss: 0.29346683621406555, acc: 0.875)
[2024-11-13 06:52:20,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:21,411][root][INFO] - Training Epoch: 7/10, step 265/574 completed (loss: 0.6641822457313538, acc: 0.8320000171661377)
[2024-11-13 06:52:21,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:21,807][root][INFO] - Training Epoch: 7/10, step 266/574 completed (loss: 0.5405181050300598, acc: 0.8202247023582458)
[2024-11-13 06:52:22,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:22,247][root][INFO] - Training Epoch: 7/10, step 267/574 completed (loss: 0.31282657384872437, acc: 0.9189189076423645)
[2024-11-13 06:52:22,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:22,902][root][INFO] - Training Epoch: 7/10, step 268/574 completed (loss: 0.1695995181798935, acc: 0.9482758641242981)
[2024-11-13 06:52:23,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:23,323][root][INFO] - Training Epoch: 7/10, step 269/574 completed (loss: 0.02645346336066723, acc: 1.0)
[2024-11-13 06:52:23,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:23,729][root][INFO] - Training Epoch: 7/10, step 270/574 completed (loss: 0.005697284825146198, acc: 1.0)
[2024-11-13 06:52:23,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:24,074][root][INFO] - Training Epoch: 7/10, step 271/574 completed (loss: 0.08957848697900772, acc: 0.9375)
[2024-11-13 06:52:24,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:24,443][root][INFO] - Training Epoch: 7/10, step 272/574 completed (loss: 0.02985392138361931, acc: 1.0)
[2024-11-13 06:52:24,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:24,954][root][INFO] - Training Epoch: 7/10, step 273/574 completed (loss: 0.2179701328277588, acc: 0.9666666388511658)
[2024-11-13 06:52:25,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:26,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:26,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:27,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:27,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:28,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:28,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:29,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:29,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:30,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:30,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:31,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:31,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:32,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:33,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:33,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:34,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:34,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:34,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:35,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:35,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:36,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:36,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:37,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:37,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:38,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:38,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:39,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:39,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:40,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:40,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:41,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:41,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:42,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:42,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:43,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:43,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:44,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:44,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:45,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:45,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:45,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:46,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:46,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:47,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:47,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:48,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:48,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:49,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:49,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:50,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:50,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:51,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:51,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:52,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:52,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:53,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:53,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:54,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:54,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:55,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:55,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:56,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:56,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:57,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:57,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:58,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:58,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:52:59,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:00,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:00,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:00,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:01,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:02,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:02,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:03,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:03,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:03,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:04,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:04,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:05,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:05,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:06,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:06,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:07,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:07,946][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4191, device='cuda:0') eval_epoch_loss=tensor(0.8834, device='cuda:0') eval_epoch_acc=tensor(0.8106, device='cuda:0')
[2024-11-13 06:53:07,947][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:53:07,947][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:53:08,356][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_7_step_274_loss_0.8833797574043274/model.pt
[2024-11-13 06:53:08,375][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:53:08,375][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 7 is 0.810599684715271
[2024-11-13 06:53:08,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:08,824][root][INFO] - Training Epoch: 7/10, step 274/574 completed (loss: 0.045741211622953415, acc: 0.96875)
[2024-11-13 06:53:08,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:09,215][root][INFO] - Training Epoch: 7/10, step 275/574 completed (loss: 0.00478563504293561, acc: 1.0)
[2024-11-13 06:53:09,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:09,580][root][INFO] - Training Epoch: 7/10, step 276/574 completed (loss: 0.1967417299747467, acc: 0.931034505367279)
[2024-11-13 06:53:09,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:10,035][root][INFO] - Training Epoch: 7/10, step 277/574 completed (loss: 0.011939072981476784, acc: 1.0)
[2024-11-13 06:53:10,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:10,512][root][INFO] - Training Epoch: 7/10, step 278/574 completed (loss: 0.19650898873806, acc: 0.8936170339584351)
[2024-11-13 06:53:10,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:10,977][root][INFO] - Training Epoch: 7/10, step 279/574 completed (loss: 0.19407780468463898, acc: 0.9583333134651184)
[2024-11-13 06:53:11,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:11,427][root][INFO] - Training Epoch: 7/10, step 280/574 completed (loss: 0.03674481809139252, acc: 0.9772727489471436)
[2024-11-13 06:53:11,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:12,060][root][INFO] - Training Epoch: 7/10, step 281/574 completed (loss: 0.3839675784111023, acc: 0.8795180916786194)
[2024-11-13 06:53:12,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:12,545][root][INFO] - Training Epoch: 7/10, step 282/574 completed (loss: 0.5955591797828674, acc: 0.7962962985038757)
[2024-11-13 06:53:12,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:12,929][root][INFO] - Training Epoch: 7/10, step 283/574 completed (loss: 0.021547939628362656, acc: 1.0)
[2024-11-13 06:53:13,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:13,354][root][INFO] - Training Epoch: 7/10, step 284/574 completed (loss: 0.20852959156036377, acc: 0.9117646813392639)
[2024-11-13 06:53:13,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:13,829][root][INFO] - Training Epoch: 7/10, step 285/574 completed (loss: 0.43759283423423767, acc: 0.8999999761581421)
[2024-11-13 06:53:14,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:14,270][root][INFO] - Training Epoch: 7/10, step 286/574 completed (loss: 0.327007919549942, acc: 0.9375)
[2024-11-13 06:53:14,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:14,724][root][INFO] - Training Epoch: 7/10, step 287/574 completed (loss: 0.42949965596199036, acc: 0.8799999952316284)
[2024-11-13 06:53:14,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:15,056][root][INFO] - Training Epoch: 7/10, step 288/574 completed (loss: 0.35403504967689514, acc: 0.8681318759918213)
[2024-11-13 06:53:15,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:15,413][root][INFO] - Training Epoch: 7/10, step 289/574 completed (loss: 0.2136206328868866, acc: 0.9378882050514221)
[2024-11-13 06:53:15,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:15,837][root][INFO] - Training Epoch: 7/10, step 290/574 completed (loss: 0.4870820641517639, acc: 0.8298969268798828)
[2024-11-13 06:53:16,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:16,266][root][INFO] - Training Epoch: 7/10, step 291/574 completed (loss: 0.021492866799235344, acc: 1.0)
[2024-11-13 06:53:16,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:16,615][root][INFO] - Training Epoch: 7/10, step 292/574 completed (loss: 0.2230532318353653, acc: 0.9285714030265808)
[2024-11-13 06:53:16,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:17,112][root][INFO] - Training Epoch: 7/10, step 293/574 completed (loss: 0.08528877049684525, acc: 0.982758641242981)
[2024-11-13 06:53:17,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:17,811][root][INFO] - Training Epoch: 7/10, step 294/574 completed (loss: 0.20194263756275177, acc: 0.9272727370262146)
[2024-11-13 06:53:18,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:18,624][root][INFO] - Training Epoch: 7/10, step 295/574 completed (loss: 0.45762792229652405, acc: 0.8505154848098755)
[2024-11-13 06:53:18,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:19,018][root][INFO] - Training Epoch: 7/10, step 296/574 completed (loss: 0.1540510356426239, acc: 0.9655172228813171)
[2024-11-13 06:53:19,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:19,408][root][INFO] - Training Epoch: 7/10, step 297/574 completed (loss: 0.10891427099704742, acc: 0.9629629850387573)
[2024-11-13 06:53:19,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:19,778][root][INFO] - Training Epoch: 7/10, step 298/574 completed (loss: 0.16933400928974152, acc: 0.9473684430122375)
[2024-11-13 06:53:19,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:20,200][root][INFO] - Training Epoch: 7/10, step 299/574 completed (loss: 0.05496295168995857, acc: 0.9821428656578064)
[2024-11-13 06:53:20,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:20,642][root][INFO] - Training Epoch: 7/10, step 300/574 completed (loss: 0.008468511514365673, acc: 1.0)
[2024-11-13 06:53:20,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:21,062][root][INFO] - Training Epoch: 7/10, step 301/574 completed (loss: 0.07548635452985764, acc: 0.9622641801834106)
[2024-11-13 06:53:21,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:21,423][root][INFO] - Training Epoch: 7/10, step 302/574 completed (loss: 0.012715130113065243, acc: 1.0)
[2024-11-13 06:53:21,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:21,822][root][INFO] - Training Epoch: 7/10, step 303/574 completed (loss: 0.02816222421824932, acc: 1.0)
[2024-11-13 06:53:21,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:22,221][root][INFO] - Training Epoch: 7/10, step 304/574 completed (loss: 0.115303173661232, acc: 0.96875)
[2024-11-13 06:53:22,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:22,629][root][INFO] - Training Epoch: 7/10, step 305/574 completed (loss: 0.20326316356658936, acc: 0.9344262480735779)
[2024-11-13 06:53:22,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:23,022][root][INFO] - Training Epoch: 7/10, step 306/574 completed (loss: 0.08219621330499649, acc: 0.9666666388511658)
[2024-11-13 06:53:23,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:23,478][root][INFO] - Training Epoch: 7/10, step 307/574 completed (loss: 0.07184620946645737, acc: 0.9473684430122375)
[2024-11-13 06:53:23,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:23,940][root][INFO] - Training Epoch: 7/10, step 308/574 completed (loss: 0.12463803589344025, acc: 0.9710144996643066)
[2024-11-13 06:53:24,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:24,544][root][INFO] - Training Epoch: 7/10, step 309/574 completed (loss: 0.13444802165031433, acc: 0.9444444179534912)
[2024-11-13 06:53:24,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:24,965][root][INFO] - Training Epoch: 7/10, step 310/574 completed (loss: 0.1465756893157959, acc: 0.9638554453849792)
[2024-11-13 06:53:25,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:25,356][root][INFO] - Training Epoch: 7/10, step 311/574 completed (loss: 0.2964058816432953, acc: 0.8589743375778198)
[2024-11-13 06:53:25,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:25,806][root][INFO] - Training Epoch: 7/10, step 312/574 completed (loss: 0.046239838004112244, acc: 0.9897959232330322)
[2024-11-13 06:53:25,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:26,207][root][INFO] - Training Epoch: 7/10, step 313/574 completed (loss: 0.012626678682863712, acc: 1.0)
[2024-11-13 06:53:26,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:26,626][root][INFO] - Training Epoch: 7/10, step 314/574 completed (loss: 0.06423811614513397, acc: 0.9583333134651184)
[2024-11-13 06:53:26,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:27,019][root][INFO] - Training Epoch: 7/10, step 315/574 completed (loss: 0.02278725430369377, acc: 1.0)
[2024-11-13 06:53:27,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:27,413][root][INFO] - Training Epoch: 7/10, step 316/574 completed (loss: 0.3546893298625946, acc: 0.8709677457809448)
[2024-11-13 06:53:27,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:27,858][root][INFO] - Training Epoch: 7/10, step 317/574 completed (loss: 0.0872504711151123, acc: 0.9701492786407471)
[2024-11-13 06:53:28,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:28,240][root][INFO] - Training Epoch: 7/10, step 318/574 completed (loss: 0.08908084034919739, acc: 0.9711538553237915)
[2024-11-13 06:53:28,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:28,604][root][INFO] - Training Epoch: 7/10, step 319/574 completed (loss: 0.06868847459554672, acc: 0.9555555582046509)
[2024-11-13 06:53:28,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:29,012][root][INFO] - Training Epoch: 7/10, step 320/574 completed (loss: 0.0189313106238842, acc: 1.0)
[2024-11-13 06:53:29,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:29,397][root][INFO] - Training Epoch: 7/10, step 321/574 completed (loss: 0.007930886000394821, acc: 1.0)
[2024-11-13 06:53:29,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:29,746][root][INFO] - Training Epoch: 7/10, step 322/574 completed (loss: 0.6728591918945312, acc: 0.8148148059844971)
[2024-11-13 06:53:29,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:30,118][root][INFO] - Training Epoch: 7/10, step 323/574 completed (loss: 0.4060676693916321, acc: 0.8571428656578064)
[2024-11-13 06:53:30,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:30,555][root][INFO] - Training Epoch: 7/10, step 324/574 completed (loss: 0.38979825377464294, acc: 0.8461538553237915)
[2024-11-13 06:53:30,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:30,925][root][INFO] - Training Epoch: 7/10, step 325/574 completed (loss: 0.6573359370231628, acc: 0.8292682766914368)
[2024-11-13 06:53:31,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:31,341][root][INFO] - Training Epoch: 7/10, step 326/574 completed (loss: 0.35261377692222595, acc: 0.8684210777282715)
[2024-11-13 06:53:31,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:31,723][root][INFO] - Training Epoch: 7/10, step 327/574 completed (loss: 0.05337664484977722, acc: 1.0)
[2024-11-13 06:53:31,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:32,119][root][INFO] - Training Epoch: 7/10, step 328/574 completed (loss: 0.02624465897679329, acc: 1.0)
[2024-11-13 06:53:32,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:32,533][root][INFO] - Training Epoch: 7/10, step 329/574 completed (loss: 0.020194116979837418, acc: 1.0)
[2024-11-13 06:53:32,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:33,015][root][INFO] - Training Epoch: 7/10, step 330/574 completed (loss: 0.03961750119924545, acc: 1.0)
[2024-11-13 06:53:33,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:33,444][root][INFO] - Training Epoch: 7/10, step 331/574 completed (loss: 0.08508442342281342, acc: 0.9516128897666931)
[2024-11-13 06:53:33,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:33,931][root][INFO] - Training Epoch: 7/10, step 332/574 completed (loss: 0.049856871366500854, acc: 0.9824561476707458)
[2024-11-13 06:53:34,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:34,336][root][INFO] - Training Epoch: 7/10, step 333/574 completed (loss: 0.08616581559181213, acc: 0.9375)
[2024-11-13 06:53:34,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:34,778][root][INFO] - Training Epoch: 7/10, step 334/574 completed (loss: 0.09662869572639465, acc: 0.9666666388511658)
[2024-11-13 06:53:34,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:35,134][root][INFO] - Training Epoch: 7/10, step 335/574 completed (loss: 0.02205902524292469, acc: 1.0)
[2024-11-13 06:53:35,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:35,527][root][INFO] - Training Epoch: 7/10, step 336/574 completed (loss: 0.24652495980262756, acc: 0.8600000143051147)
[2024-11-13 06:53:35,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:35,954][root][INFO] - Training Epoch: 7/10, step 337/574 completed (loss: 0.5869453549385071, acc: 0.8160919547080994)
[2024-11-13 06:53:36,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:36,383][root][INFO] - Training Epoch: 7/10, step 338/574 completed (loss: 0.6504670977592468, acc: 0.7978723645210266)
[2024-11-13 06:53:36,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:36,772][root][INFO] - Training Epoch: 7/10, step 339/574 completed (loss: 0.6008886098861694, acc: 0.8072289228439331)
[2024-11-13 06:53:36,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:37,123][root][INFO] - Training Epoch: 7/10, step 340/574 completed (loss: 0.016450857743620872, acc: 1.0)
[2024-11-13 06:53:37,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:37,533][root][INFO] - Training Epoch: 7/10, step 341/574 completed (loss: 0.10239415615797043, acc: 0.9743589758872986)
[2024-11-13 06:53:37,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:37,936][root][INFO] - Training Epoch: 7/10, step 342/574 completed (loss: 0.1714201271533966, acc: 0.9397590160369873)
[2024-11-13 06:53:38,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:38,345][root][INFO] - Training Epoch: 7/10, step 343/574 completed (loss: 0.26145413517951965, acc: 0.9433962106704712)
[2024-11-13 06:53:38,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:38,721][root][INFO] - Training Epoch: 7/10, step 344/574 completed (loss: 0.12747785449028015, acc: 0.9620253443717957)
[2024-11-13 06:53:38,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:39,105][root][INFO] - Training Epoch: 7/10, step 345/574 completed (loss: 0.03741401061415672, acc: 0.9803921580314636)
[2024-11-13 06:53:39,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:39,519][root][INFO] - Training Epoch: 7/10, step 346/574 completed (loss: 0.16845011711120605, acc: 0.9253731369972229)
[2024-11-13 06:53:39,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:39,934][root][INFO] - Training Epoch: 7/10, step 347/574 completed (loss: 0.0056074149906635284, acc: 1.0)
[2024-11-13 06:53:40,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:40,306][root][INFO] - Training Epoch: 7/10, step 348/574 completed (loss: 0.09291355311870575, acc: 0.9599999785423279)
[2024-11-13 06:53:40,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:40,831][root][INFO] - Training Epoch: 7/10, step 349/574 completed (loss: 0.3127751052379608, acc: 0.9166666865348816)
[2024-11-13 06:53:40,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:41,215][root][INFO] - Training Epoch: 7/10, step 350/574 completed (loss: 0.3321796953678131, acc: 0.8837209343910217)
[2024-11-13 06:53:41,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:41,644][root][INFO] - Training Epoch: 7/10, step 351/574 completed (loss: 0.04363372549414635, acc: 1.0)
[2024-11-13 06:53:41,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:42,137][root][INFO] - Training Epoch: 7/10, step 352/574 completed (loss: 0.2721119523048401, acc: 0.8888888955116272)
[2024-11-13 06:53:42,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:42,547][root][INFO] - Training Epoch: 7/10, step 353/574 completed (loss: 0.008451888337731361, acc: 1.0)
[2024-11-13 06:53:42,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:42,932][root][INFO] - Training Epoch: 7/10, step 354/574 completed (loss: 0.09418118000030518, acc: 0.9615384340286255)
[2024-11-13 06:53:43,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:43,395][root][INFO] - Training Epoch: 7/10, step 355/574 completed (loss: 0.40506279468536377, acc: 0.8571428656578064)
[2024-11-13 06:53:43,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:44,123][root][INFO] - Training Epoch: 7/10, step 356/574 completed (loss: 0.36496657133102417, acc: 0.9130434989929199)
[2024-11-13 06:53:44,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:44,534][root][INFO] - Training Epoch: 7/10, step 357/574 completed (loss: 0.26448020339012146, acc: 0.8913043737411499)
[2024-11-13 06:53:44,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:44,936][root][INFO] - Training Epoch: 7/10, step 358/574 completed (loss: 0.5032223463058472, acc: 0.8979591727256775)
[2024-11-13 06:53:45,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:45,313][root][INFO] - Training Epoch: 7/10, step 359/574 completed (loss: 0.002404163358733058, acc: 1.0)
[2024-11-13 06:53:45,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:45,708][root][INFO] - Training Epoch: 7/10, step 360/574 completed (loss: 0.03932550176978111, acc: 1.0)
[2024-11-13 06:53:45,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:46,134][root][INFO] - Training Epoch: 7/10, step 361/574 completed (loss: 0.08791448175907135, acc: 0.9512194991111755)
[2024-11-13 06:53:46,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:46,568][root][INFO] - Training Epoch: 7/10, step 362/574 completed (loss: 0.38663843274116516, acc: 0.8888888955116272)
[2024-11-13 06:53:46,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:47,001][root][INFO] - Training Epoch: 7/10, step 363/574 completed (loss: 0.0777478963136673, acc: 0.9868420958518982)
[2024-11-13 06:53:47,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:47,424][root][INFO] - Training Epoch: 7/10, step 364/574 completed (loss: 0.08193425834178925, acc: 0.9756097793579102)
[2024-11-13 06:53:47,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:47,791][root][INFO] - Training Epoch: 7/10, step 365/574 completed (loss: 0.06389329582452774, acc: 0.9696969985961914)
[2024-11-13 06:53:47,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:48,242][root][INFO] - Training Epoch: 7/10, step 366/574 completed (loss: 0.006961436942219734, acc: 1.0)
[2024-11-13 06:53:48,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:48,643][root][INFO] - Training Epoch: 7/10, step 367/574 completed (loss: 0.03350292518734932, acc: 1.0)
[2024-11-13 06:53:48,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:49,046][root][INFO] - Training Epoch: 7/10, step 368/574 completed (loss: 0.01570870913565159, acc: 1.0)
[2024-11-13 06:53:49,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:49,495][root][INFO] - Training Epoch: 7/10, step 369/574 completed (loss: 0.1567133218050003, acc: 0.9375)
[2024-11-13 06:53:50,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:50,468][root][INFO] - Training Epoch: 7/10, step 370/574 completed (loss: 0.45707860589027405, acc: 0.8545454740524292)
[2024-11-13 06:53:51,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:51,807][root][INFO] - Training Epoch: 7/10, step 371/574 completed (loss: 0.23880819976329803, acc: 0.9056603908538818)
[2024-11-13 06:53:51,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:52,220][root][INFO] - Training Epoch: 7/10, step 372/574 completed (loss: 0.16352790594100952, acc: 0.9555555582046509)
[2024-11-13 06:53:52,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:52,642][root][INFO] - Training Epoch: 7/10, step 373/574 completed (loss: 0.12412748485803604, acc: 0.9642857313156128)
[2024-11-13 06:53:52,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:53,074][root][INFO] - Training Epoch: 7/10, step 374/574 completed (loss: 0.04505845159292221, acc: 0.9714285731315613)
[2024-11-13 06:53:53,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:53,455][root][INFO] - Training Epoch: 7/10, step 375/574 completed (loss: 0.0030230488628149033, acc: 1.0)
[2024-11-13 06:53:53,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:53,899][root][INFO] - Training Epoch: 7/10, step 376/574 completed (loss: 0.006150220986455679, acc: 1.0)
[2024-11-13 06:53:54,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:54,304][root][INFO] - Training Epoch: 7/10, step 377/574 completed (loss: 0.11584053188562393, acc: 0.9375)
[2024-11-13 06:53:54,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:54,780][root][INFO] - Training Epoch: 7/10, step 378/574 completed (loss: 0.11897654086351395, acc: 0.9789473414421082)
[2024-11-13 06:53:55,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:55,690][root][INFO] - Training Epoch: 7/10, step 379/574 completed (loss: 0.23873887956142426, acc: 0.9221556782722473)
[2024-11-13 06:53:55,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:56,259][root][INFO] - Training Epoch: 7/10, step 380/574 completed (loss: 0.16500768065452576, acc: 0.932330846786499)
[2024-11-13 06:53:57,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:58,103][root][INFO] - Training Epoch: 7/10, step 381/574 completed (loss: 0.41276392340660095, acc: 0.8877005577087402)
[2024-11-13 06:53:58,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:58,956][root][INFO] - Training Epoch: 7/10, step 382/574 completed (loss: 0.0499359592795372, acc: 0.9909909963607788)
[2024-11-13 06:53:59,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:59,328][root][INFO] - Training Epoch: 7/10, step 383/574 completed (loss: 0.36431723833084106, acc: 0.8928571343421936)
[2024-11-13 06:53:59,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:53:59,760][root][INFO] - Training Epoch: 7/10, step 384/574 completed (loss: 0.04008026793599129, acc: 1.0)
[2024-11-13 06:53:59,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:00,150][root][INFO] - Training Epoch: 7/10, step 385/574 completed (loss: 0.009158721193671227, acc: 1.0)
[2024-11-13 06:54:00,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:00,565][root][INFO] - Training Epoch: 7/10, step 386/574 completed (loss: 0.032847460359334946, acc: 0.9722222089767456)
[2024-11-13 06:54:00,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:00,996][root][INFO] - Training Epoch: 7/10, step 387/574 completed (loss: 0.04598458111286163, acc: 1.0)
[2024-11-13 06:54:01,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:01,421][root][INFO] - Training Epoch: 7/10, step 388/574 completed (loss: 0.005307032726705074, acc: 1.0)
[2024-11-13 06:54:01,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:01,859][root][INFO] - Training Epoch: 7/10, step 389/574 completed (loss: 0.017428383231163025, acc: 1.0)
[2024-11-13 06:54:02,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:02,263][root][INFO] - Training Epoch: 7/10, step 390/574 completed (loss: 0.9541678428649902, acc: 0.9047619104385376)
[2024-11-13 06:54:02,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:02,696][root][INFO] - Training Epoch: 7/10, step 391/574 completed (loss: 0.2612263560295105, acc: 0.9259259104728699)
[2024-11-13 06:54:02,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:03,138][root][INFO] - Training Epoch: 7/10, step 392/574 completed (loss: 0.4346153140068054, acc: 0.8737863898277283)
[2024-11-13 06:54:03,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:03,903][root][INFO] - Training Epoch: 7/10, step 393/574 completed (loss: 0.4559784233570099, acc: 0.8676470518112183)
[2024-11-13 06:54:04,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:04,397][root][INFO] - Training Epoch: 7/10, step 394/574 completed (loss: 0.466816246509552, acc: 0.8533333539962769)
[2024-11-13 06:54:04,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:04,919][root][INFO] - Training Epoch: 7/10, step 395/574 completed (loss: 0.3108787536621094, acc: 0.8680555820465088)
[2024-11-13 06:54:05,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:05,319][root][INFO] - Training Epoch: 7/10, step 396/574 completed (loss: 0.06038527190685272, acc: 1.0)
[2024-11-13 06:54:05,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:05,699][root][INFO] - Training Epoch: 7/10, step 397/574 completed (loss: 0.018178721889853477, acc: 1.0)
[2024-11-13 06:54:05,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:06,093][root][INFO] - Training Epoch: 7/10, step 398/574 completed (loss: 0.11413401365280151, acc: 0.9534883499145508)
[2024-11-13 06:54:06,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:06,435][root][INFO] - Training Epoch: 7/10, step 399/574 completed (loss: 0.16815192997455597, acc: 0.9200000166893005)
[2024-11-13 06:54:06,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:07,239][root][INFO] - Training Epoch: 7/10, step 400/574 completed (loss: 0.24570000171661377, acc: 0.8970588445663452)
[2024-11-13 06:54:07,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:07,614][root][INFO] - Training Epoch: 7/10, step 401/574 completed (loss: 0.18651117384433746, acc: 0.9066666960716248)
[2024-11-13 06:54:07,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:07,984][root][INFO] - Training Epoch: 7/10, step 402/574 completed (loss: 0.19117464125156403, acc: 0.8787878751754761)
[2024-11-13 06:54:08,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:08,328][root][INFO] - Training Epoch: 7/10, step 403/574 completed (loss: 0.16568230092525482, acc: 0.9090909361839294)
[2024-11-13 06:54:08,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:08,676][root][INFO] - Training Epoch: 7/10, step 404/574 completed (loss: 0.049994274973869324, acc: 1.0)
[2024-11-13 06:54:08,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:09,020][root][INFO] - Training Epoch: 7/10, step 405/574 completed (loss: 0.003190854098647833, acc: 1.0)
[2024-11-13 06:54:09,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:09,365][root][INFO] - Training Epoch: 7/10, step 406/574 completed (loss: 0.0061970055103302, acc: 1.0)
[2024-11-13 06:54:09,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:09,714][root][INFO] - Training Epoch: 7/10, step 407/574 completed (loss: 0.01065070554614067, acc: 1.0)
[2024-11-13 06:54:09,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:10,057][root][INFO] - Training Epoch: 7/10, step 408/574 completed (loss: 0.044197265058755875, acc: 1.0)
[2024-11-13 06:54:10,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:10,396][root][INFO] - Training Epoch: 7/10, step 409/574 completed (loss: 0.02704988792538643, acc: 1.0)
[2024-11-13 06:54:10,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:10,823][root][INFO] - Training Epoch: 7/10, step 410/574 completed (loss: 0.07217314094305038, acc: 0.9655172228813171)
[2024-11-13 06:54:10,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:11,195][root][INFO] - Training Epoch: 7/10, step 411/574 completed (loss: 0.003693552687764168, acc: 1.0)
[2024-11-13 06:54:11,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:11,632][root][INFO] - Training Epoch: 7/10, step 412/574 completed (loss: 0.060066502541303635, acc: 0.9666666388511658)
[2024-11-13 06:54:11,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:12,021][root][INFO] - Training Epoch: 7/10, step 413/574 completed (loss: 0.15017329156398773, acc: 0.939393937587738)
[2024-11-13 06:54:12,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:12,365][root][INFO] - Training Epoch: 7/10, step 414/574 completed (loss: 0.04135299101471901, acc: 1.0)
[2024-11-13 06:54:12,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:12,743][root][INFO] - Training Epoch: 7/10, step 415/574 completed (loss: 0.13781307637691498, acc: 0.9411764740943909)
[2024-11-13 06:54:12,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:13,091][root][INFO] - Training Epoch: 7/10, step 416/574 completed (loss: 0.15256842970848083, acc: 0.9615384340286255)
[2024-11-13 06:54:13,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:14,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:14,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:15,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:15,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:16,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:16,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:17,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:17,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:18,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:18,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:19,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:19,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:20,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:20,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:21,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:21,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:22,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:22,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:23,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:23,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:24,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:24,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:25,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:26,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:26,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:26,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:27,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:27,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:28,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:28,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:29,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:29,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:30,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:30,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:31,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:31,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:32,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:32,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:33,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:34,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:34,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:35,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:35,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:36,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:36,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:36,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:37,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:37,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:38,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:38,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:39,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:39,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:40,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:40,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:41,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:41,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:42,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:42,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:43,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:43,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:44,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:45,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:45,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:46,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:46,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:47,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:47,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:48,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:49,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:49,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:50,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:50,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:51,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:51,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:52,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:52,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:53,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:53,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:54,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:54,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:55,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:55,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:56,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:56,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:57,259][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5325, device='cuda:0') eval_epoch_loss=tensor(0.9292, device='cuda:0') eval_epoch_acc=tensor(0.7946, device='cuda:0')
[2024-11-13 06:54:57,261][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:54:57,261][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:54:57,792][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_7_step_417_loss_0.9292177557945251/model.pt
[2024-11-13 06:54:57,798][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:54:57,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:58,233][root][INFO] - Training Epoch: 7/10, step 417/574 completed (loss: 0.10325250774621964, acc: 0.9444444179534912)
[2024-11-13 06:54:58,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:58,645][root][INFO] - Training Epoch: 7/10, step 418/574 completed (loss: 0.033108558505773544, acc: 0.9750000238418579)
[2024-11-13 06:54:58,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:59,133][root][INFO] - Training Epoch: 7/10, step 419/574 completed (loss: 0.04813075810670853, acc: 1.0)
[2024-11-13 06:54:59,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:54:59,645][root][INFO] - Training Epoch: 7/10, step 420/574 completed (loss: 0.005701962858438492, acc: 1.0)
[2024-11-13 06:54:59,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:00,044][root][INFO] - Training Epoch: 7/10, step 421/574 completed (loss: 0.022223882377147675, acc: 1.0)
[2024-11-13 06:55:00,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:00,473][root][INFO] - Training Epoch: 7/10, step 422/574 completed (loss: 0.01754394732415676, acc: 1.0)
[2024-11-13 06:55:00,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:00,981][root][INFO] - Training Epoch: 7/10, step 423/574 completed (loss: 0.24928270280361176, acc: 0.9166666865348816)
[2024-11-13 06:55:01,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:01,371][root][INFO] - Training Epoch: 7/10, step 424/574 completed (loss: 0.06646464020013809, acc: 0.9629629850387573)
[2024-11-13 06:55:01,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:01,768][root][INFO] - Training Epoch: 7/10, step 425/574 completed (loss: 0.019942205399274826, acc: 1.0)
[2024-11-13 06:55:01,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:02,160][root][INFO] - Training Epoch: 7/10, step 426/574 completed (loss: 0.0029151341877877712, acc: 1.0)
[2024-11-13 06:55:02,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:02,552][root][INFO] - Training Epoch: 7/10, step 427/574 completed (loss: 0.010646799579262733, acc: 1.0)
[2024-11-13 06:55:02,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:02,950][root][INFO] - Training Epoch: 7/10, step 428/574 completed (loss: 0.0208264347165823, acc: 1.0)
[2024-11-13 06:55:03,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:03,298][root][INFO] - Training Epoch: 7/10, step 429/574 completed (loss: 0.004774302244186401, acc: 1.0)
[2024-11-13 06:55:03,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:03,765][root][INFO] - Training Epoch: 7/10, step 430/574 completed (loss: 0.004247348755598068, acc: 1.0)
[2024-11-13 06:55:03,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:04,196][root][INFO] - Training Epoch: 7/10, step 431/574 completed (loss: 0.003417362691834569, acc: 1.0)
[2024-11-13 06:55:04,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:04,608][root][INFO] - Training Epoch: 7/10, step 432/574 completed (loss: 0.0012231360888108611, acc: 1.0)
[2024-11-13 06:55:04,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:05,104][root][INFO] - Training Epoch: 7/10, step 433/574 completed (loss: 0.05294227972626686, acc: 0.9722222089767456)
[2024-11-13 06:55:05,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:05,466][root][INFO] - Training Epoch: 7/10, step 434/574 completed (loss: 0.025742284953594208, acc: 1.0)
[2024-11-13 06:55:05,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:05,851][root][INFO] - Training Epoch: 7/10, step 435/574 completed (loss: 0.08691130578517914, acc: 0.9696969985961914)
[2024-11-13 06:55:06,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:06,248][root][INFO] - Training Epoch: 7/10, step 436/574 completed (loss: 0.1619555503129959, acc: 0.9722222089767456)
[2024-11-13 06:55:06,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:06,694][root][INFO] - Training Epoch: 7/10, step 437/574 completed (loss: 0.04070476442575455, acc: 1.0)
[2024-11-13 06:55:06,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:07,085][root][INFO] - Training Epoch: 7/10, step 438/574 completed (loss: 0.043658144772052765, acc: 0.9523809552192688)
[2024-11-13 06:55:07,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:07,512][root][INFO] - Training Epoch: 7/10, step 439/574 completed (loss: 0.09289295226335526, acc: 0.9743589758872986)
[2024-11-13 06:55:07,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:08,180][root][INFO] - Training Epoch: 7/10, step 440/574 completed (loss: 0.14678707718849182, acc: 0.9545454382896423)
[2024-11-13 06:55:08,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:09,250][root][INFO] - Training Epoch: 7/10, step 441/574 completed (loss: 0.5081242322921753, acc: 0.8240000009536743)
[2024-11-13 06:55:09,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:09,815][root][INFO] - Training Epoch: 7/10, step 442/574 completed (loss: 0.7355831265449524, acc: 0.774193525314331)
[2024-11-13 06:55:10,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:10,813][root][INFO] - Training Epoch: 7/10, step 443/574 completed (loss: 0.522343099117279, acc: 0.8606964945793152)
[2024-11-13 06:55:10,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:11,195][root][INFO] - Training Epoch: 7/10, step 444/574 completed (loss: 0.1666140854358673, acc: 0.9245283007621765)
[2024-11-13 06:55:11,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:11,796][root][INFO] - Training Epoch: 7/10, step 445/574 completed (loss: 0.06790972501039505, acc: 0.9772727489471436)
[2024-11-13 06:55:11,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:12,225][root][INFO] - Training Epoch: 7/10, step 446/574 completed (loss: 0.009801854379475117, acc: 1.0)
[2024-11-13 06:55:12,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:12,611][root][INFO] - Training Epoch: 7/10, step 447/574 completed (loss: 0.0783899649977684, acc: 1.0)
[2024-11-13 06:55:12,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:13,034][root][INFO] - Training Epoch: 7/10, step 448/574 completed (loss: 0.16279026865959167, acc: 0.9285714030265808)
[2024-11-13 06:55:13,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:13,461][root][INFO] - Training Epoch: 7/10, step 449/574 completed (loss: 0.0884600356221199, acc: 0.9850746393203735)
[2024-11-13 06:55:13,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:13,848][root][INFO] - Training Epoch: 7/10, step 450/574 completed (loss: 0.09031765908002853, acc: 0.9722222089767456)
[2024-11-13 06:55:14,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:14,313][root][INFO] - Training Epoch: 7/10, step 451/574 completed (loss: 0.05501794442534447, acc: 0.97826087474823)
[2024-11-13 06:55:14,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:14,697][root][INFO] - Training Epoch: 7/10, step 452/574 completed (loss: 0.14837057888507843, acc: 0.9487179517745972)
[2024-11-13 06:55:14,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:15,062][root][INFO] - Training Epoch: 7/10, step 453/574 completed (loss: 0.25961899757385254, acc: 0.9210526347160339)
[2024-11-13 06:55:15,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:15,512][root][INFO] - Training Epoch: 7/10, step 454/574 completed (loss: 0.03772382065653801, acc: 0.9795918464660645)
[2024-11-13 06:55:15,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:15,927][root][INFO] - Training Epoch: 7/10, step 455/574 completed (loss: 0.14498089253902435, acc: 0.939393937587738)
[2024-11-13 06:55:16,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:16,351][root][INFO] - Training Epoch: 7/10, step 456/574 completed (loss: 0.4540599584579468, acc: 0.907216489315033)
[2024-11-13 06:55:16,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:16,837][root][INFO] - Training Epoch: 7/10, step 457/574 completed (loss: 0.05949228256940842, acc: 0.9857142567634583)
[2024-11-13 06:55:17,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:17,343][root][INFO] - Training Epoch: 7/10, step 458/574 completed (loss: 0.31708574295043945, acc: 0.9069767594337463)
[2024-11-13 06:55:17,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:17,734][root][INFO] - Training Epoch: 7/10, step 459/574 completed (loss: 0.054414983838796616, acc: 0.9642857313156128)
[2024-11-13 06:55:17,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:18,130][root][INFO] - Training Epoch: 7/10, step 460/574 completed (loss: 0.07068368792533875, acc: 0.9629629850387573)
[2024-11-13 06:55:18,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:18,501][root][INFO] - Training Epoch: 7/10, step 461/574 completed (loss: 0.10853806138038635, acc: 0.9444444179534912)
[2024-11-13 06:55:18,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:18,909][root][INFO] - Training Epoch: 7/10, step 462/574 completed (loss: 0.009836177341639996, acc: 1.0)
[2024-11-13 06:55:19,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:19,305][root][INFO] - Training Epoch: 7/10, step 463/574 completed (loss: 0.030135048553347588, acc: 1.0)
[2024-11-13 06:55:19,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:19,704][root][INFO] - Training Epoch: 7/10, step 464/574 completed (loss: 0.15471266210079193, acc: 0.9347826242446899)
[2024-11-13 06:55:19,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:20,078][root][INFO] - Training Epoch: 7/10, step 465/574 completed (loss: 0.1605333536863327, acc: 0.9642857313156128)
[2024-11-13 06:55:20,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:20,441][root][INFO] - Training Epoch: 7/10, step 466/574 completed (loss: 0.4407161474227905, acc: 0.8795180916786194)
[2024-11-13 06:55:20,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:20,888][root][INFO] - Training Epoch: 7/10, step 467/574 completed (loss: 0.037624187767505646, acc: 1.0)
[2024-11-13 06:55:21,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:21,324][root][INFO] - Training Epoch: 7/10, step 468/574 completed (loss: 0.35799312591552734, acc: 0.9126213788986206)
[2024-11-13 06:55:21,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:21,769][root][INFO] - Training Epoch: 7/10, step 469/574 completed (loss: 0.3656250834465027, acc: 0.9024389982223511)
[2024-11-13 06:55:21,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:22,137][root][INFO] - Training Epoch: 7/10, step 470/574 completed (loss: 0.04787707328796387, acc: 1.0)
[2024-11-13 06:55:22,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:22,560][root][INFO] - Training Epoch: 7/10, step 471/574 completed (loss: 0.09914474934339523, acc: 0.9642857313156128)
[2024-11-13 06:55:22,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:23,128][root][INFO] - Training Epoch: 7/10, step 472/574 completed (loss: 0.2195860892534256, acc: 0.9215686321258545)
[2024-11-13 06:55:23,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:23,595][root][INFO] - Training Epoch: 7/10, step 473/574 completed (loss: 0.5887833833694458, acc: 0.8340611457824707)
[2024-11-13 06:55:23,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:23,969][root][INFO] - Training Epoch: 7/10, step 474/574 completed (loss: 0.2811133861541748, acc: 0.8958333134651184)
[2024-11-13 06:55:24,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:24,409][root][INFO] - Training Epoch: 7/10, step 475/574 completed (loss: 0.25037482380867004, acc: 0.9263803958892822)
[2024-11-13 06:55:24,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:24,846][root][INFO] - Training Epoch: 7/10, step 476/574 completed (loss: 0.22241421043872833, acc: 0.9208633303642273)
[2024-11-13 06:55:25,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:25,288][root][INFO] - Training Epoch: 7/10, step 477/574 completed (loss: 0.4242137670516968, acc: 0.8542713522911072)
[2024-11-13 06:55:25,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:25,696][root][INFO] - Training Epoch: 7/10, step 478/574 completed (loss: 0.12684832513332367, acc: 0.9722222089767456)
[2024-11-13 06:55:25,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:26,093][root][INFO] - Training Epoch: 7/10, step 479/574 completed (loss: 0.11442030966281891, acc: 0.9696969985961914)
[2024-11-13 06:55:26,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:26,460][root][INFO] - Training Epoch: 7/10, step 480/574 completed (loss: 0.20257550477981567, acc: 0.9629629850387573)
[2024-11-13 06:55:26,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:26,864][root][INFO] - Training Epoch: 7/10, step 481/574 completed (loss: 0.07643376290798187, acc: 1.0)
[2024-11-13 06:55:27,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:27,324][root][INFO] - Training Epoch: 7/10, step 482/574 completed (loss: 0.18741124868392944, acc: 0.949999988079071)
[2024-11-13 06:55:27,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:27,850][root][INFO] - Training Epoch: 7/10, step 483/574 completed (loss: 0.21531686186790466, acc: 0.9137930870056152)
[2024-11-13 06:55:28,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:28,237][root][INFO] - Training Epoch: 7/10, step 484/574 completed (loss: 0.01598541997373104, acc: 1.0)
[2024-11-13 06:55:28,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:28,575][root][INFO] - Training Epoch: 7/10, step 485/574 completed (loss: 0.02574109472334385, acc: 1.0)
[2024-11-13 06:55:28,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:28,945][root][INFO] - Training Epoch: 7/10, step 486/574 completed (loss: 0.11244560778141022, acc: 0.9629629850387573)
[2024-11-13 06:55:29,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:29,292][root][INFO] - Training Epoch: 7/10, step 487/574 completed (loss: 1.2268614768981934, acc: 0.8571428656578064)
[2024-11-13 06:55:29,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:29,738][root][INFO] - Training Epoch: 7/10, step 488/574 completed (loss: 0.5500103831291199, acc: 0.9090909361839294)
[2024-11-13 06:55:29,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:30,185][root][INFO] - Training Epoch: 7/10, step 489/574 completed (loss: 0.38383790850639343, acc: 0.9076923131942749)
[2024-11-13 06:55:30,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:30,587][root][INFO] - Training Epoch: 7/10, step 490/574 completed (loss: 0.15723933279514313, acc: 0.9666666388511658)
[2024-11-13 06:55:30,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:30,993][root][INFO] - Training Epoch: 7/10, step 491/574 completed (loss: 0.10043377429246902, acc: 0.9655172228813171)
[2024-11-13 06:55:31,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:31,347][root][INFO] - Training Epoch: 7/10, step 492/574 completed (loss: 0.12447405606508255, acc: 0.9411764740943909)
[2024-11-13 06:55:31,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:31,789][root][INFO] - Training Epoch: 7/10, step 493/574 completed (loss: 0.048443753272295, acc: 1.0)
[2024-11-13 06:55:31,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:32,251][root][INFO] - Training Epoch: 7/10, step 494/574 completed (loss: 0.26713621616363525, acc: 0.8947368264198303)
[2024-11-13 06:55:32,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:32,644][root][INFO] - Training Epoch: 7/10, step 495/574 completed (loss: 0.09625862538814545, acc: 0.9473684430122375)
[2024-11-13 06:55:32,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:33,064][root][INFO] - Training Epoch: 7/10, step 496/574 completed (loss: 0.44227153062820435, acc: 0.875)
[2024-11-13 06:55:33,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:33,573][root][INFO] - Training Epoch: 7/10, step 497/574 completed (loss: 0.18315047025680542, acc: 0.9550561904907227)
[2024-11-13 06:55:33,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:33,957][root][INFO] - Training Epoch: 7/10, step 498/574 completed (loss: 0.33538106083869934, acc: 0.8539325594902039)
[2024-11-13 06:55:34,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:34,353][root][INFO] - Training Epoch: 7/10, step 499/574 completed (loss: 0.7340193390846252, acc: 0.8156028389930725)
[2024-11-13 06:55:34,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:34,742][root][INFO] - Training Epoch: 7/10, step 500/574 completed (loss: 0.4241234362125397, acc: 0.8586956262588501)
[2024-11-13 06:55:34,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:35,175][root][INFO] - Training Epoch: 7/10, step 501/574 completed (loss: 0.07710987329483032, acc: 0.9599999785423279)
[2024-11-13 06:55:35,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:35,604][root][INFO] - Training Epoch: 7/10, step 502/574 completed (loss: 0.011406541801989079, acc: 1.0)
[2024-11-13 06:55:35,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:35,989][root][INFO] - Training Epoch: 7/10, step 503/574 completed (loss: 0.2342965304851532, acc: 0.9259259104728699)
[2024-11-13 06:55:36,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:36,393][root][INFO] - Training Epoch: 7/10, step 504/574 completed (loss: 0.0064581031911075115, acc: 1.0)
[2024-11-13 06:55:36,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:36,750][root][INFO] - Training Epoch: 7/10, step 505/574 completed (loss: 0.2983536720275879, acc: 0.8867924809455872)
[2024-11-13 06:55:36,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:37,096][root][INFO] - Training Epoch: 7/10, step 506/574 completed (loss: 0.7103865146636963, acc: 0.8275862336158752)
[2024-11-13 06:55:37,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:37,997][root][INFO] - Training Epoch: 7/10, step 507/574 completed (loss: 0.8644909262657166, acc: 0.6936936974525452)
[2024-11-13 06:55:38,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:38,623][root][INFO] - Training Epoch: 7/10, step 508/574 completed (loss: 0.4600127935409546, acc: 0.8732394576072693)
[2024-11-13 06:55:38,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:38,989][root][INFO] - Training Epoch: 7/10, step 509/574 completed (loss: 0.17965999245643616, acc: 0.949999988079071)
[2024-11-13 06:55:39,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:39,324][root][INFO] - Training Epoch: 7/10, step 510/574 completed (loss: 0.18217052519321442, acc: 0.8999999761581421)
[2024-11-13 06:55:39,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:39,672][root][INFO] - Training Epoch: 7/10, step 511/574 completed (loss: 0.23173801600933075, acc: 0.9230769276618958)
[2024-11-13 06:55:42,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:43,395][root][INFO] - Training Epoch: 7/10, step 512/574 completed (loss: 0.8992598652839661, acc: 0.7642857432365417)
[2024-11-13 06:55:44,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:44,631][root][INFO] - Training Epoch: 7/10, step 513/574 completed (loss: 0.3588433563709259, acc: 0.8809523582458496)
[2024-11-13 06:55:44,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:44,982][root][INFO] - Training Epoch: 7/10, step 514/574 completed (loss: 0.2289852797985077, acc: 0.9285714030265808)
[2024-11-13 06:55:45,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:45,405][root][INFO] - Training Epoch: 7/10, step 515/574 completed (loss: 0.04382198676466942, acc: 1.0)
[2024-11-13 06:55:45,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:46,473][root][INFO] - Training Epoch: 7/10, step 516/574 completed (loss: 0.3758559226989746, acc: 0.9027777910232544)
[2024-11-13 06:55:46,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:46,828][root][INFO] - Training Epoch: 7/10, step 517/574 completed (loss: 0.006730351131409407, acc: 1.0)
[2024-11-13 06:55:46,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:47,169][root][INFO] - Training Epoch: 7/10, step 518/574 completed (loss: 0.09601731598377228, acc: 0.9354838728904724)
[2024-11-13 06:55:47,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:47,484][root][INFO] - Training Epoch: 7/10, step 519/574 completed (loss: 0.4815448820590973, acc: 0.949999988079071)
[2024-11-13 06:55:47,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:47,834][root][INFO] - Training Epoch: 7/10, step 520/574 completed (loss: 0.09041070938110352, acc: 0.9259259104728699)
[2024-11-13 06:55:48,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:49,390][root][INFO] - Training Epoch: 7/10, step 521/574 completed (loss: 0.6476752758026123, acc: 0.7923728823661804)
[2024-11-13 06:55:49,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:49,839][root][INFO] - Training Epoch: 7/10, step 522/574 completed (loss: 0.3401511609554291, acc: 0.888059675693512)
[2024-11-13 06:55:50,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:50,325][root][INFO] - Training Epoch: 7/10, step 523/574 completed (loss: 0.2982223331928253, acc: 0.9051094651222229)
[2024-11-13 06:55:50,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:51,156][root][INFO] - Training Epoch: 7/10, step 524/574 completed (loss: 0.6777068376541138, acc: 0.8050000071525574)
[2024-11-13 06:55:51,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:51,509][root][INFO] - Training Epoch: 7/10, step 525/574 completed (loss: 0.02092304266989231, acc: 1.0)
[2024-11-13 06:55:51,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:51,868][root][INFO] - Training Epoch: 7/10, step 526/574 completed (loss: 0.18467117846012115, acc: 0.942307710647583)
[2024-11-13 06:55:51,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:52,211][root][INFO] - Training Epoch: 7/10, step 527/574 completed (loss: 0.20493298768997192, acc: 0.9523809552192688)
[2024-11-13 06:55:52,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:52,596][root][INFO] - Training Epoch: 7/10, step 528/574 completed (loss: 0.7309625744819641, acc: 0.7377049326896667)
[2024-11-13 06:55:52,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:52,990][root][INFO] - Training Epoch: 7/10, step 529/574 completed (loss: 0.13157187402248383, acc: 0.9661017060279846)
[2024-11-13 06:55:53,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:53,385][root][INFO] - Training Epoch: 7/10, step 530/574 completed (loss: 0.5744406580924988, acc: 0.8139534592628479)
[2024-11-13 06:55:53,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:53,751][root][INFO] - Training Epoch: 7/10, step 531/574 completed (loss: 0.19910269975662231, acc: 0.9318181872367859)
[2024-11-13 06:55:53,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:54,105][root][INFO] - Training Epoch: 7/10, step 532/574 completed (loss: 0.5557351112365723, acc: 0.8301886916160583)
[2024-11-13 06:55:54,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:54,459][root][INFO] - Training Epoch: 7/10, step 533/574 completed (loss: 0.26401910185813904, acc: 0.9090909361839294)
[2024-11-13 06:55:54,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:54,814][root][INFO] - Training Epoch: 7/10, step 534/574 completed (loss: 0.25484606623649597, acc: 0.8799999952316284)
[2024-11-13 06:55:54,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:55,127][root][INFO] - Training Epoch: 7/10, step 535/574 completed (loss: 0.10323876142501831, acc: 0.949999988079071)
[2024-11-13 06:55:55,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:55,472][root][INFO] - Training Epoch: 7/10, step 536/574 completed (loss: 0.1563454121351242, acc: 0.9090909361839294)
[2024-11-13 06:55:55,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:56,010][root][INFO] - Training Epoch: 7/10, step 537/574 completed (loss: 0.1428980678319931, acc: 0.9692307710647583)
[2024-11-13 06:55:56,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:56,459][root][INFO] - Training Epoch: 7/10, step 538/574 completed (loss: 0.24591723084449768, acc: 0.9375)
[2024-11-13 06:55:56,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:56,994][root][INFO] - Training Epoch: 7/10, step 539/574 completed (loss: 0.24384671449661255, acc: 0.9375)
[2024-11-13 06:55:57,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:57,347][root][INFO] - Training Epoch: 7/10, step 540/574 completed (loss: 0.13971877098083496, acc: 0.9696969985961914)
[2024-11-13 06:55:57,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:57,694][root][INFO] - Training Epoch: 7/10, step 541/574 completed (loss: 0.03554647043347359, acc: 1.0)
[2024-11-13 06:55:57,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:58,039][root][INFO] - Training Epoch: 7/10, step 542/574 completed (loss: 0.004561335779726505, acc: 1.0)
[2024-11-13 06:55:58,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:58,401][root][INFO] - Training Epoch: 7/10, step 543/574 completed (loss: 0.011008954606950283, acc: 1.0)
[2024-11-13 06:55:58,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:58,799][root][INFO] - Training Epoch: 7/10, step 544/574 completed (loss: 0.07561901956796646, acc: 0.9666666388511658)
[2024-11-13 06:55:58,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:59,163][root][INFO] - Training Epoch: 7/10, step 545/574 completed (loss: 0.02642265520989895, acc: 1.0)
[2024-11-13 06:55:59,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:59,513][root][INFO] - Training Epoch: 7/10, step 546/574 completed (loss: 0.07090262323617935, acc: 0.9428571462631226)
[2024-11-13 06:55:59,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:55:59,875][root][INFO] - Training Epoch: 7/10, step 547/574 completed (loss: 0.059985529631376266, acc: 0.9736841917037964)
[2024-11-13 06:56:00,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:00,219][root][INFO] - Training Epoch: 7/10, step 548/574 completed (loss: 0.08505088835954666, acc: 0.9677419066429138)
[2024-11-13 06:56:00,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:00,584][root][INFO] - Training Epoch: 7/10, step 549/574 completed (loss: 0.02028469555079937, acc: 1.0)
[2024-11-13 06:56:00,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:00,970][root][INFO] - Training Epoch: 7/10, step 550/574 completed (loss: 0.025959134101867676, acc: 1.0)
[2024-11-13 06:56:01,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:01,317][root][INFO] - Training Epoch: 7/10, step 551/574 completed (loss: 0.02769516035914421, acc: 1.0)
[2024-11-13 06:56:01,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:01,734][root][INFO] - Training Epoch: 7/10, step 552/574 completed (loss: 0.14518992602825165, acc: 0.9571428298950195)
[2024-11-13 06:56:01,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:02,133][root][INFO] - Training Epoch: 7/10, step 553/574 completed (loss: 0.381745845079422, acc: 0.8759124279022217)
[2024-11-13 06:56:02,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:02,563][root][INFO] - Training Epoch: 7/10, step 554/574 completed (loss: 0.20788580179214478, acc: 0.931034505367279)
[2024-11-13 06:56:02,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:02,925][root][INFO] - Training Epoch: 7/10, step 555/574 completed (loss: 0.33265629410743713, acc: 0.8999999761581421)
[2024-11-13 06:56:03,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:03,323][root][INFO] - Training Epoch: 7/10, step 556/574 completed (loss: 0.30259332060813904, acc: 0.9139072895050049)
[2024-11-13 06:56:03,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:03,699][root][INFO] - Training Epoch: 7/10, step 557/574 completed (loss: 0.05403416231274605, acc: 0.9829059839248657)
[2024-11-13 06:56:03,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:04,037][root][INFO] - Training Epoch: 7/10, step 558/574 completed (loss: 0.06559476256370544, acc: 0.9599999785423279)
[2024-11-13 06:56:04,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:04,391][root][INFO] - Training Epoch: 7/10, step 559/574 completed (loss: 0.24285928905010223, acc: 0.8846153616905212)
[2024-11-13 06:56:05,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:05,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:06,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:06,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:07,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:07,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:08,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:08,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:09,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:09,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:10,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:10,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:11,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:11,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:12,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:12,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:13,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:13,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:14,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:14,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:15,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:15,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:16,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:16,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:16,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:17,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:17,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:18,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:18,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:19,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:19,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:20,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:20,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:21,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:21,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:22,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:22,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:23,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:24,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:24,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:24,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:25,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:25,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:26,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:27,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:27,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:28,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:28,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:28,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:29,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:29,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:30,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:30,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:31,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:31,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:32,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:32,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:33,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:33,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:34,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:34,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:35,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:35,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:36,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:36,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:37,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:37,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:38,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:39,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:39,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:40,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:40,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:41,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:41,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:42,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:42,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:43,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:43,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:43,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:44,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:44,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:45,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:45,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:46,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:46,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:47,390][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.6202, device='cuda:0') eval_epoch_loss=tensor(0.9633, device='cuda:0') eval_epoch_acc=tensor(0.7965, device='cuda:0')
[2024-11-13 06:56:47,391][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:56:47,391][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:56:47,855][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_7_step_560_loss_0.9632651209831238/model.pt
[2024-11-13 06:56:47,863][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:56:48,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:48,315][root][INFO] - Training Epoch: 7/10, step 560/574 completed (loss: 0.010911514051258564, acc: 1.0)
[2024-11-13 06:56:48,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:48,759][root][INFO] - Training Epoch: 7/10, step 561/574 completed (loss: 0.07254482060670853, acc: 0.9743589758872986)
[2024-11-13 06:56:48,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:49,189][root][INFO] - Training Epoch: 7/10, step 562/574 completed (loss: 0.399402916431427, acc: 0.8666666746139526)
[2024-11-13 06:56:49,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:49,609][root][INFO] - Training Epoch: 7/10, step 563/574 completed (loss: 0.21551378071308136, acc: 0.9610389471054077)
[2024-11-13 06:56:49,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:49,956][root][INFO] - Training Epoch: 7/10, step 564/574 completed (loss: 0.09019327908754349, acc: 0.9791666865348816)
[2024-11-13 06:56:50,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:50,280][root][INFO] - Training Epoch: 7/10, step 565/574 completed (loss: 0.13947539031505585, acc: 0.931034505367279)
[2024-11-13 06:56:50,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:50,720][root][INFO] - Training Epoch: 7/10, step 566/574 completed (loss: 0.16373099386692047, acc: 0.9642857313156128)
[2024-11-13 06:56:50,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:51,133][root][INFO] - Training Epoch: 7/10, step 567/574 completed (loss: 0.038249753415584564, acc: 1.0)
[2024-11-13 06:56:51,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:51,564][root][INFO] - Training Epoch: 7/10, step 568/574 completed (loss: 0.08092840760946274, acc: 0.9629629850387573)
[2024-11-13 06:56:51,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:52,061][root][INFO] - Training Epoch: 7/10, step 569/574 completed (loss: 0.27949613332748413, acc: 0.9197860956192017)
[2024-11-13 06:56:52,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:52,483][root][INFO] - Training Epoch: 7/10, step 570/574 completed (loss: 0.18541131913661957, acc: 0.9516128897666931)
[2024-11-13 06:56:52,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:52,882][root][INFO] - Training Epoch: 7/10, step 571/574 completed (loss: 0.14473313093185425, acc: 0.9487179517745972)
[2024-11-13 06:56:53,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:53,311][root][INFO] - Training Epoch: 7/10, step 572/574 completed (loss: 0.459179162979126, acc: 0.8826530575752258)
[2024-11-13 06:56:53,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:53,732][root][INFO] - Training Epoch: 7/10, step 573/574 completed (loss: 0.26993370056152344, acc: 0.9119496941566467)
[2024-11-13 06:56:54,172][slam_llm.utils.train_utils][INFO] - Epoch 7: train_perplexity=1.2864, train_epoch_loss=0.2518, epoch time 459.0393012482673s
[2024-11-13 06:56:54,172][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2024-11-13 06:56:54,172][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 17 GB
[2024-11-13 06:56:54,172][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2024-11-13 06:56:54,172][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 21
[2024-11-13 06:56:54,173][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-13 06:56:54,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:55,124][root][INFO] - Training Epoch: 8/10, step 0/574 completed (loss: 0.05986618623137474, acc: 0.9629629850387573)
[2024-11-13 06:56:55,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:55,612][root][INFO] - Training Epoch: 8/10, step 1/574 completed (loss: 0.01567990332841873, acc: 1.0)
[2024-11-13 06:56:55,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:56,053][root][INFO] - Training Epoch: 8/10, step 2/574 completed (loss: 0.4115205407142639, acc: 0.8918918967247009)
[2024-11-13 06:56:56,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:56,556][root][INFO] - Training Epoch: 8/10, step 3/574 completed (loss: 0.025935303419828415, acc: 1.0)
[2024-11-13 06:56:56,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:56,971][root][INFO] - Training Epoch: 8/10, step 4/574 completed (loss: 0.20964016020298004, acc: 0.9189189076423645)
[2024-11-13 06:56:57,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:57,404][root][INFO] - Training Epoch: 8/10, step 5/574 completed (loss: 0.10864966362714767, acc: 0.9642857313156128)
[2024-11-13 06:56:57,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:57,835][root][INFO] - Training Epoch: 8/10, step 6/574 completed (loss: 0.4204053580760956, acc: 0.8775510191917419)
[2024-11-13 06:56:57,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:58,261][root][INFO] - Training Epoch: 8/10, step 7/574 completed (loss: 0.07942165434360504, acc: 0.9666666388511658)
[2024-11-13 06:56:58,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:58,738][root][INFO] - Training Epoch: 8/10, step 8/574 completed (loss: 0.01024006400257349, acc: 1.0)
[2024-11-13 06:56:58,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:59,202][root][INFO] - Training Epoch: 8/10, step 9/574 completed (loss: 0.008892706595361233, acc: 1.0)
[2024-11-13 06:56:59,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:56:59,626][root][INFO] - Training Epoch: 8/10, step 10/574 completed (loss: 0.013107752427458763, acc: 1.0)
[2024-11-13 06:56:59,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:00,033][root][INFO] - Training Epoch: 8/10, step 11/574 completed (loss: 0.2082422524690628, acc: 0.9487179517745972)
[2024-11-13 06:57:00,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:00,435][root][INFO] - Training Epoch: 8/10, step 12/574 completed (loss: 0.011223243549466133, acc: 1.0)
[2024-11-13 06:57:00,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:00,812][root][INFO] - Training Epoch: 8/10, step 13/574 completed (loss: 0.14984990656375885, acc: 0.95652174949646)
[2024-11-13 06:57:01,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:01,274][root][INFO] - Training Epoch: 8/10, step 14/574 completed (loss: 0.14204968512058258, acc: 0.9607843160629272)
[2024-11-13 06:57:01,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:01,714][root][INFO] - Training Epoch: 8/10, step 15/574 completed (loss: 0.11731566488742828, acc: 0.9591836929321289)
[2024-11-13 06:57:01,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:02,103][root][INFO] - Training Epoch: 8/10, step 16/574 completed (loss: 0.023735875263810158, acc: 1.0)
[2024-11-13 06:57:02,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:02,453][root][INFO] - Training Epoch: 8/10, step 17/574 completed (loss: 0.014068013988435268, acc: 1.0)
[2024-11-13 06:57:02,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:02,885][root][INFO] - Training Epoch: 8/10, step 18/574 completed (loss: 0.26101386547088623, acc: 0.8611111044883728)
[2024-11-13 06:57:03,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:03,298][root][INFO] - Training Epoch: 8/10, step 19/574 completed (loss: 0.04022523760795593, acc: 1.0)
[2024-11-13 06:57:03,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:03,706][root][INFO] - Training Epoch: 8/10, step 20/574 completed (loss: 0.08552047610282898, acc: 0.9615384340286255)
[2024-11-13 06:57:03,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:04,197][root][INFO] - Training Epoch: 8/10, step 21/574 completed (loss: 0.04402103275060654, acc: 1.0)
[2024-11-13 06:57:04,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:04,606][root][INFO] - Training Epoch: 8/10, step 22/574 completed (loss: 0.27888739109039307, acc: 0.9200000166893005)
[2024-11-13 06:57:04,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:05,035][root][INFO] - Training Epoch: 8/10, step 23/574 completed (loss: 0.041727107018232346, acc: 1.0)
[2024-11-13 06:57:05,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:05,394][root][INFO] - Training Epoch: 8/10, step 24/574 completed (loss: 0.01663852110505104, acc: 1.0)
[2024-11-13 06:57:05,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:05,733][root][INFO] - Training Epoch: 8/10, step 25/574 completed (loss: 0.18664894998073578, acc: 0.9433962106704712)
[2024-11-13 06:57:05,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:06,150][root][INFO] - Training Epoch: 8/10, step 26/574 completed (loss: 0.36348438262939453, acc: 0.8493150472640991)
[2024-11-13 06:57:07,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:07,930][root][INFO] - Training Epoch: 8/10, step 27/574 completed (loss: 0.7438196539878845, acc: 0.7509881258010864)
[2024-11-13 06:57:08,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:08,348][root][INFO] - Training Epoch: 8/10, step 28/574 completed (loss: 0.15966743230819702, acc: 0.9069767594337463)
[2024-11-13 06:57:08,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:08,800][root][INFO] - Training Epoch: 8/10, step 29/574 completed (loss: 0.33849069476127625, acc: 0.891566276550293)
[2024-11-13 06:57:08,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:09,276][root][INFO] - Training Epoch: 8/10, step 30/574 completed (loss: 0.4026883542537689, acc: 0.8641975522041321)
[2024-11-13 06:57:09,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:09,686][root][INFO] - Training Epoch: 8/10, step 31/574 completed (loss: 0.12305832654237747, acc: 0.9642857313156128)
[2024-11-13 06:57:09,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:10,057][root][INFO] - Training Epoch: 8/10, step 32/574 completed (loss: 0.022915616631507874, acc: 1.0)
[2024-11-13 06:57:10,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:10,420][root][INFO] - Training Epoch: 8/10, step 33/574 completed (loss: 0.011121148243546486, acc: 1.0)
[2024-11-13 06:57:10,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:10,824][root][INFO] - Training Epoch: 8/10, step 34/574 completed (loss: 0.1615927815437317, acc: 0.9411764740943909)
[2024-11-13 06:57:10,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:11,192][root][INFO] - Training Epoch: 8/10, step 35/574 completed (loss: 0.1104937493801117, acc: 0.9672130942344666)
[2024-11-13 06:57:11,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:11,650][root][INFO] - Training Epoch: 8/10, step 36/574 completed (loss: 0.22630614042282104, acc: 0.920634925365448)
[2024-11-13 06:57:11,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:12,025][root][INFO] - Training Epoch: 8/10, step 37/574 completed (loss: 0.12344043701887131, acc: 0.9491525292396545)
[2024-11-13 06:57:12,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:12,485][root][INFO] - Training Epoch: 8/10, step 38/574 completed (loss: 0.16306063532829285, acc: 0.954023003578186)
[2024-11-13 06:57:12,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:12,858][root][INFO] - Training Epoch: 8/10, step 39/574 completed (loss: 0.018763167783617973, acc: 1.0)
[2024-11-13 06:57:12,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:13,264][root][INFO] - Training Epoch: 8/10, step 40/574 completed (loss: 0.22224368155002594, acc: 0.9230769276618958)
[2024-11-13 06:57:13,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:13,787][root][INFO] - Training Epoch: 8/10, step 41/574 completed (loss: 0.25175970792770386, acc: 0.9459459185600281)
[2024-11-13 06:57:14,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:14,263][root][INFO] - Training Epoch: 8/10, step 42/574 completed (loss: 0.15798649191856384, acc: 0.9384615421295166)
[2024-11-13 06:57:14,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:14,798][root][INFO] - Training Epoch: 8/10, step 43/574 completed (loss: 0.3831510543823242, acc: 0.9090909361839294)
[2024-11-13 06:57:15,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:15,365][root][INFO] - Training Epoch: 8/10, step 44/574 completed (loss: 0.3586248755455017, acc: 0.8556700944900513)
[2024-11-13 06:57:15,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:15,901][root][INFO] - Training Epoch: 8/10, step 45/574 completed (loss: 0.22819048166275024, acc: 0.9338235259056091)
[2024-11-13 06:57:16,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:16,312][root][INFO] - Training Epoch: 8/10, step 46/574 completed (loss: 0.061586715281009674, acc: 0.9615384340286255)
[2024-11-13 06:57:16,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:16,762][root][INFO] - Training Epoch: 8/10, step 47/574 completed (loss: 0.01647518016397953, acc: 1.0)
[2024-11-13 06:57:16,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:17,197][root][INFO] - Training Epoch: 8/10, step 48/574 completed (loss: 0.09157740324735641, acc: 0.9285714030265808)
[2024-11-13 06:57:17,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:17,607][root][INFO] - Training Epoch: 8/10, step 49/574 completed (loss: 0.12336080521345139, acc: 0.9722222089767456)
[2024-11-13 06:57:17,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:18,049][root][INFO] - Training Epoch: 8/10, step 50/574 completed (loss: 0.25555476546287537, acc: 0.9122806787490845)
[2024-11-13 06:57:18,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:18,526][root][INFO] - Training Epoch: 8/10, step 51/574 completed (loss: 0.20228859782218933, acc: 0.920634925365448)
[2024-11-13 06:57:18,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:18,903][root][INFO] - Training Epoch: 8/10, step 52/574 completed (loss: 0.3231644332408905, acc: 0.8732394576072693)
[2024-11-13 06:57:19,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:19,536][root][INFO] - Training Epoch: 8/10, step 53/574 completed (loss: 1.013744592666626, acc: 0.6800000071525574)
[2024-11-13 06:57:19,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:20,002][root][INFO] - Training Epoch: 8/10, step 54/574 completed (loss: 0.17275039851665497, acc: 0.9189189076423645)
[2024-11-13 06:57:20,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:20,466][root][INFO] - Training Epoch: 8/10, step 55/574 completed (loss: 0.03786519169807434, acc: 0.9615384340286255)
[2024-11-13 06:57:23,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:24,971][root][INFO] - Training Epoch: 8/10, step 56/574 completed (loss: 0.7196376919746399, acc: 0.7679181098937988)
[2024-11-13 06:57:25,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:26,832][root][INFO] - Training Epoch: 8/10, step 57/574 completed (loss: 1.0849653482437134, acc: 0.6928104758262634)
[2024-11-13 06:57:27,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:27,768][root][INFO] - Training Epoch: 8/10, step 58/574 completed (loss: 0.5410134792327881, acc: 0.8352272510528564)
[2024-11-13 06:57:28,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:28,630][root][INFO] - Training Epoch: 8/10, step 59/574 completed (loss: 0.1342701017856598, acc: 0.9485294222831726)
[2024-11-13 06:57:29,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:29,467][root][INFO] - Training Epoch: 8/10, step 60/574 completed (loss: 0.475056529045105, acc: 0.8405796885490417)
[2024-11-13 06:57:29,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:30,043][root][INFO] - Training Epoch: 8/10, step 61/574 completed (loss: 0.3518725633621216, acc: 0.862500011920929)
[2024-11-13 06:57:30,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:30,413][root][INFO] - Training Epoch: 8/10, step 62/574 completed (loss: 0.10725557804107666, acc: 0.9411764740943909)
[2024-11-13 06:57:30,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:30,854][root][INFO] - Training Epoch: 8/10, step 63/574 completed (loss: 0.02853233367204666, acc: 1.0)
[2024-11-13 06:57:31,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:31,301][root][INFO] - Training Epoch: 8/10, step 64/574 completed (loss: 0.02250288799405098, acc: 1.0)
[2024-11-13 06:57:31,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:31,765][root][INFO] - Training Epoch: 8/10, step 65/574 completed (loss: 0.014036639593541622, acc: 1.0)
[2024-11-13 06:57:31,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:32,159][root][INFO] - Training Epoch: 8/10, step 66/574 completed (loss: 0.31422552466392517, acc: 0.9464285969734192)
[2024-11-13 06:57:32,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:32,583][root][INFO] - Training Epoch: 8/10, step 67/574 completed (loss: 0.16456995904445648, acc: 0.949999988079071)
[2024-11-13 06:57:32,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:33,000][root][INFO] - Training Epoch: 8/10, step 68/574 completed (loss: 0.02642432227730751, acc: 1.0)
[2024-11-13 06:57:33,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:33,429][root][INFO] - Training Epoch: 8/10, step 69/574 completed (loss: 0.04244132339954376, acc: 1.0)
[2024-11-13 06:57:33,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:33,900][root][INFO] - Training Epoch: 8/10, step 70/574 completed (loss: 0.12082536518573761, acc: 0.939393937587738)
[2024-11-13 06:57:34,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:34,318][root][INFO] - Training Epoch: 8/10, step 71/574 completed (loss: 0.5038471221923828, acc: 0.8382353186607361)
[2024-11-13 06:57:34,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:34,645][root][INFO] - Training Epoch: 8/10, step 72/574 completed (loss: 0.3239687979221344, acc: 0.89682537317276)
[2024-11-13 06:57:34,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:35,036][root][INFO] - Training Epoch: 8/10, step 73/574 completed (loss: 0.9013353586196899, acc: 0.7435897588729858)
[2024-11-13 06:57:35,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:35,528][root][INFO] - Training Epoch: 8/10, step 74/574 completed (loss: 0.4491269588470459, acc: 0.8469387888908386)
[2024-11-13 06:57:35,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:35,918][root][INFO] - Training Epoch: 8/10, step 75/574 completed (loss: 0.6841687560081482, acc: 0.8358209133148193)
[2024-11-13 06:57:36,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:36,410][root][INFO] - Training Epoch: 8/10, step 76/574 completed (loss: 1.0775645971298218, acc: 0.6934306621551514)
[2024-11-13 06:57:36,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:36,754][root][INFO] - Training Epoch: 8/10, step 77/574 completed (loss: 0.014681236818432808, acc: 1.0)
[2024-11-13 06:57:36,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:37,160][root][INFO] - Training Epoch: 8/10, step 78/574 completed (loss: 0.0030558425933122635, acc: 1.0)
[2024-11-13 06:57:37,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:37,558][root][INFO] - Training Epoch: 8/10, step 79/574 completed (loss: 0.3278614282608032, acc: 0.939393937587738)
[2024-11-13 06:57:37,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:38,008][root][INFO] - Training Epoch: 8/10, step 80/574 completed (loss: 0.0025273694191128016, acc: 1.0)
[2024-11-13 06:57:38,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:38,414][root][INFO] - Training Epoch: 8/10, step 81/574 completed (loss: 0.12403690814971924, acc: 0.9807692170143127)
[2024-11-13 06:57:38,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:38,786][root][INFO] - Training Epoch: 8/10, step 82/574 completed (loss: 0.1839984506368637, acc: 0.9038461446762085)
[2024-11-13 06:57:38,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:39,193][root][INFO] - Training Epoch: 8/10, step 83/574 completed (loss: 0.12101870775222778, acc: 0.9375)
[2024-11-13 06:57:39,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:39,661][root][INFO] - Training Epoch: 8/10, step 84/574 completed (loss: 0.09382707625627518, acc: 0.9855072498321533)
[2024-11-13 06:57:39,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:40,090][root][INFO] - Training Epoch: 8/10, step 85/574 completed (loss: 0.08422870934009552, acc: 0.9599999785423279)
[2024-11-13 06:57:40,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:40,532][root][INFO] - Training Epoch: 8/10, step 86/574 completed (loss: 0.017852334305644035, acc: 1.0)
[2024-11-13 06:57:40,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:41,186][root][INFO] - Training Epoch: 8/10, step 87/574 completed (loss: 0.4706158936023712, acc: 0.800000011920929)
[2024-11-13 06:57:41,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:41,666][root][INFO] - Training Epoch: 8/10, step 88/574 completed (loss: 0.6591359376907349, acc: 0.844660222530365)
[2024-11-13 06:57:42,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:43,426][root][INFO] - Training Epoch: 8/10, step 89/574 completed (loss: 0.5798683166503906, acc: 0.8398058414459229)
[2024-11-13 06:57:44,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:44,702][root][INFO] - Training Epoch: 8/10, step 90/574 completed (loss: 0.684995174407959, acc: 0.7956989407539368)
[2024-11-13 06:57:45,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:45,948][root][INFO] - Training Epoch: 8/10, step 91/574 completed (loss: 0.5760839581489563, acc: 0.8318965435028076)
[2024-11-13 06:57:46,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:47,104][root][INFO] - Training Epoch: 8/10, step 92/574 completed (loss: 0.47799795866012573, acc: 0.8526315689086914)
[2024-11-13 06:57:47,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:48,688][root][INFO] - Training Epoch: 8/10, step 93/574 completed (loss: 0.724381148815155, acc: 0.7425742745399475)
[2024-11-13 06:57:48,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:49,035][root][INFO] - Training Epoch: 8/10, step 94/574 completed (loss: 0.28672876954078674, acc: 0.8870967626571655)
[2024-11-13 06:57:49,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:49,412][root][INFO] - Training Epoch: 8/10, step 95/574 completed (loss: 0.1674085110425949, acc: 0.95652174949646)
[2024-11-13 06:57:49,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:49,808][root][INFO] - Training Epoch: 8/10, step 96/574 completed (loss: 0.5405587553977966, acc: 0.831932783126831)
[2024-11-13 06:57:49,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:50,201][root][INFO] - Training Epoch: 8/10, step 97/574 completed (loss: 0.4389062225818634, acc: 0.8557692170143127)
[2024-11-13 06:57:50,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:50,698][root][INFO] - Training Epoch: 8/10, step 98/574 completed (loss: 0.780133843421936, acc: 0.7445255517959595)
[2024-11-13 06:57:50,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:51,062][root][INFO] - Training Epoch: 8/10, step 99/574 completed (loss: 0.3312486708164215, acc: 0.9104477763175964)
[2024-11-13 06:57:51,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:51,408][root][INFO] - Training Epoch: 8/10, step 100/574 completed (loss: 0.025689557194709778, acc: 1.0)
[2024-11-13 06:57:51,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:51,757][root][INFO] - Training Epoch: 8/10, step 101/574 completed (loss: 0.007216510362923145, acc: 1.0)
[2024-11-13 06:57:51,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:52,106][root][INFO] - Training Epoch: 8/10, step 102/574 completed (loss: 0.09249869734048843, acc: 0.95652174949646)
[2024-11-13 06:57:52,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:52,443][root][INFO] - Training Epoch: 8/10, step 103/574 completed (loss: 0.015858687460422516, acc: 1.0)
[2024-11-13 06:57:52,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:52,813][root][INFO] - Training Epoch: 8/10, step 104/574 completed (loss: 0.088363416492939, acc: 0.9655172228813171)
[2024-11-13 06:57:52,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:53,177][root][INFO] - Training Epoch: 8/10, step 105/574 completed (loss: 0.040381599217653275, acc: 0.9767441749572754)
[2024-11-13 06:57:53,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:53,529][root][INFO] - Training Epoch: 8/10, step 106/574 completed (loss: 0.05621234327554703, acc: 1.0)
[2024-11-13 06:57:53,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:53,927][root][INFO] - Training Epoch: 8/10, step 107/574 completed (loss: 0.0033113881945610046, acc: 1.0)
[2024-11-13 06:57:54,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:54,237][root][INFO] - Training Epoch: 8/10, step 108/574 completed (loss: 0.03447692468762398, acc: 0.9615384340286255)
[2024-11-13 06:57:54,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:54,611][root][INFO] - Training Epoch: 8/10, step 109/574 completed (loss: 0.17532402276992798, acc: 0.9523809552192688)
[2024-11-13 06:57:54,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:55,017][root][INFO] - Training Epoch: 8/10, step 110/574 completed (loss: 0.05862587317824364, acc: 0.9846153855323792)
[2024-11-13 06:57:55,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:55,574][root][INFO] - Training Epoch: 8/10, step 111/574 completed (loss: 0.08681000024080276, acc: 0.9824561476707458)
[2024-11-13 06:57:55,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:56,023][root][INFO] - Training Epoch: 8/10, step 112/574 completed (loss: 0.2581917941570282, acc: 0.859649121761322)
[2024-11-13 06:57:56,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:56,364][root][INFO] - Training Epoch: 8/10, step 113/574 completed (loss: 0.07976148277521133, acc: 0.9743589758872986)
[2024-11-13 06:57:56,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:56,835][root][INFO] - Training Epoch: 8/10, step 114/574 completed (loss: 0.21370072662830353, acc: 0.918367326259613)
[2024-11-13 06:57:56,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:57,185][root][INFO] - Training Epoch: 8/10, step 115/574 completed (loss: 0.0022765607573091984, acc: 1.0)
[2024-11-13 06:57:57,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:57,626][root][INFO] - Training Epoch: 8/10, step 116/574 completed (loss: 0.24873289465904236, acc: 0.9365079402923584)
[2024-11-13 06:57:57,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:58,053][root][INFO] - Training Epoch: 8/10, step 117/574 completed (loss: 0.12835060060024261, acc: 0.9512194991111755)
[2024-11-13 06:57:58,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:58,429][root][INFO] - Training Epoch: 8/10, step 118/574 completed (loss: 0.12876926362514496, acc: 0.9677419066429138)
[2024-11-13 06:57:59,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:57:59,731][root][INFO] - Training Epoch: 8/10, step 119/574 completed (loss: 0.585390567779541, acc: 0.8326995968818665)
[2024-11-13 06:57:59,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:00,132][root][INFO] - Training Epoch: 8/10, step 120/574 completed (loss: 0.12748849391937256, acc: 0.9599999785423279)
[2024-11-13 06:58:00,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:00,689][root][INFO] - Training Epoch: 8/10, step 121/574 completed (loss: 0.11420105397701263, acc: 0.942307710647583)
[2024-11-13 06:58:00,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:01,043][root][INFO] - Training Epoch: 8/10, step 122/574 completed (loss: 0.13145588338375092, acc: 0.9583333134651184)
[2024-11-13 06:58:01,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:01,431][root][INFO] - Training Epoch: 8/10, step 123/574 completed (loss: 0.21664857864379883, acc: 0.8947368264198303)
[2024-11-13 06:58:01,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:01,836][root][INFO] - Training Epoch: 8/10, step 124/574 completed (loss: 0.5964757800102234, acc: 0.8220859169960022)
[2024-11-13 06:58:02,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:02,289][root][INFO] - Training Epoch: 8/10, step 125/574 completed (loss: 0.7748053073883057, acc: 0.8055555820465088)
[2024-11-13 06:58:02,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:02,678][root][INFO] - Training Epoch: 8/10, step 126/574 completed (loss: 0.601614773273468, acc: 0.7833333611488342)
[2024-11-13 06:58:02,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:03,115][root][INFO] - Training Epoch: 8/10, step 127/574 completed (loss: 0.4225099980831146, acc: 0.8690476417541504)
[2024-11-13 06:58:03,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:03,551][root][INFO] - Training Epoch: 8/10, step 128/574 completed (loss: 0.44239211082458496, acc: 0.8512820601463318)
[2024-11-13 06:58:04,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:05,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:05,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:06,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:06,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:07,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:07,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:08,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:08,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:09,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:10,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:10,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:11,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:11,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:12,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:12,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:13,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:13,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:14,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:14,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:15,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:15,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:16,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:16,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:17,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:17,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:18,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:18,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:19,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:19,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:20,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:20,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:21,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:21,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:22,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:22,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:23,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:23,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:24,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:24,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:25,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:25,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:26,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:26,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:27,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:27,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:27,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:28,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:28,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:29,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:29,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:30,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:30,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:31,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:31,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:32,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:32,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:33,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:33,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:34,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:34,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:35,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:35,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:36,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:36,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:37,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:37,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:37,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:38,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:39,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:39,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:40,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:40,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:41,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:41,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:41,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:42,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:42,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:43,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:43,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:44,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:45,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:45,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:46,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:46,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:47,417][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3080, device='cuda:0') eval_epoch_loss=tensor(0.8364, device='cuda:0') eval_epoch_acc=tensor(0.8135, device='cuda:0')
[2024-11-13 06:58:47,419][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 06:58:47,419][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 06:58:48,244][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_8_step_129_loss_0.8363800048828125/model.pt
[2024-11-13 06:58:48,256][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 06:58:48,257][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 8 is 0.8134953379631042
[2024-11-13 06:58:48,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:48,861][root][INFO] - Training Epoch: 8/10, step 129/574 completed (loss: 0.4782693386077881, acc: 0.8823529481887817)
[2024-11-13 06:58:49,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:49,256][root][INFO] - Training Epoch: 8/10, step 130/574 completed (loss: 0.1359749138355255, acc: 0.9230769276618958)
[2024-11-13 06:58:49,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:49,665][root][INFO] - Training Epoch: 8/10, step 131/574 completed (loss: 0.033344119787216187, acc: 1.0)
[2024-11-13 06:58:49,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:50,071][root][INFO] - Training Epoch: 8/10, step 132/574 completed (loss: 0.051415715366601944, acc: 1.0)
[2024-11-13 06:58:50,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:50,488][root][INFO] - Training Epoch: 8/10, step 133/574 completed (loss: 0.2642204761505127, acc: 0.9130434989929199)
[2024-11-13 06:58:50,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:50,890][root][INFO] - Training Epoch: 8/10, step 134/574 completed (loss: 0.08698555827140808, acc: 0.9714285731315613)
[2024-11-13 06:58:51,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:51,294][root][INFO] - Training Epoch: 8/10, step 135/574 completed (loss: 0.0460546538233757, acc: 0.9615384340286255)
[2024-11-13 06:58:51,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:51,722][root][INFO] - Training Epoch: 8/10, step 136/574 completed (loss: 0.1096968874335289, acc: 0.976190447807312)
[2024-11-13 06:58:51,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:52,173][root][INFO] - Training Epoch: 8/10, step 137/574 completed (loss: 0.13954657316207886, acc: 0.9666666388511658)
[2024-11-13 06:58:52,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:52,577][root][INFO] - Training Epoch: 8/10, step 138/574 completed (loss: 0.6034427881240845, acc: 0.8695651888847351)
[2024-11-13 06:58:52,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:53,002][root][INFO] - Training Epoch: 8/10, step 139/574 completed (loss: 0.03550024703145027, acc: 1.0)
[2024-11-13 06:58:53,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:53,403][root][INFO] - Training Epoch: 8/10, step 140/574 completed (loss: 0.13741101324558258, acc: 0.9615384340286255)
[2024-11-13 06:58:53,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:53,811][root][INFO] - Training Epoch: 8/10, step 141/574 completed (loss: 0.08726946264505386, acc: 0.9677419066429138)
[2024-11-13 06:58:54,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:54,238][root][INFO] - Training Epoch: 8/10, step 142/574 completed (loss: 0.03765119984745979, acc: 1.0)
[2024-11-13 06:58:54,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:55,015][root][INFO] - Training Epoch: 8/10, step 143/574 completed (loss: 0.4386911988258362, acc: 0.859649121761322)
[2024-11-13 06:58:55,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:55,442][root][INFO] - Training Epoch: 8/10, step 144/574 completed (loss: 0.4557673931121826, acc: 0.8731343150138855)
[2024-11-13 06:58:55,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:55,918][root][INFO] - Training Epoch: 8/10, step 145/574 completed (loss: 0.3902612030506134, acc: 0.8979591727256775)
[2024-11-13 06:58:56,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:56,551][root][INFO] - Training Epoch: 8/10, step 146/574 completed (loss: 0.411074697971344, acc: 0.8617021441459656)
[2024-11-13 06:58:56,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:56,929][root][INFO] - Training Epoch: 8/10, step 147/574 completed (loss: 0.16053220629692078, acc: 0.9571428298950195)
[2024-11-13 06:58:57,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:57,390][root][INFO] - Training Epoch: 8/10, step 148/574 completed (loss: 0.4533071219921112, acc: 0.8928571343421936)
[2024-11-13 06:58:57,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:57,779][root][INFO] - Training Epoch: 8/10, step 149/574 completed (loss: 0.03178158402442932, acc: 1.0)
[2024-11-13 06:58:57,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:58,187][root][INFO] - Training Epoch: 8/10, step 150/574 completed (loss: 0.06879205256700516, acc: 1.0)
[2024-11-13 06:58:58,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:58,620][root][INFO] - Training Epoch: 8/10, step 151/574 completed (loss: 0.34444573521614075, acc: 0.8913043737411499)
[2024-11-13 06:58:58,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:59,034][root][INFO] - Training Epoch: 8/10, step 152/574 completed (loss: 0.2973971962928772, acc: 0.8983050584793091)
[2024-11-13 06:58:59,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:59,450][root][INFO] - Training Epoch: 8/10, step 153/574 completed (loss: 0.24630001187324524, acc: 0.9298245906829834)
[2024-11-13 06:58:59,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:58:59,952][root][INFO] - Training Epoch: 8/10, step 154/574 completed (loss: 0.4869427978992462, acc: 0.8513513803482056)
[2024-11-13 06:59:00,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:00,335][root][INFO] - Training Epoch: 8/10, step 155/574 completed (loss: 0.02588827721774578, acc: 1.0)
[2024-11-13 06:59:00,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:00,759][root][INFO] - Training Epoch: 8/10, step 156/574 completed (loss: 0.19665096700191498, acc: 0.9130434989929199)
[2024-11-13 06:59:00,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:01,186][root][INFO] - Training Epoch: 8/10, step 157/574 completed (loss: 0.5284584164619446, acc: 0.7894737124443054)
[2024-11-13 06:59:02,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:03,809][root][INFO] - Training Epoch: 8/10, step 158/574 completed (loss: 0.38063034415245056, acc: 0.8648648858070374)
[2024-11-13 06:59:04,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:04,227][root][INFO] - Training Epoch: 8/10, step 159/574 completed (loss: 0.3803604543209076, acc: 0.8518518805503845)
[2024-11-13 06:59:04,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:04,779][root][INFO] - Training Epoch: 8/10, step 160/574 completed (loss: 0.5450500249862671, acc: 0.8488371968269348)
[2024-11-13 06:59:05,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:05,682][root][INFO] - Training Epoch: 8/10, step 161/574 completed (loss: 0.4873361885547638, acc: 0.8352941274642944)
[2024-11-13 06:59:06,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:06,529][root][INFO] - Training Epoch: 8/10, step 162/574 completed (loss: 0.5676087141036987, acc: 0.8426966071128845)
[2024-11-13 06:59:06,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:06,969][root][INFO] - Training Epoch: 8/10, step 163/574 completed (loss: 0.07255158573389053, acc: 0.9772727489471436)
[2024-11-13 06:59:07,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:07,389][root][INFO] - Training Epoch: 8/10, step 164/574 completed (loss: 0.15349826216697693, acc: 0.9523809552192688)
[2024-11-13 06:59:07,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:07,798][root][INFO] - Training Epoch: 8/10, step 165/574 completed (loss: 0.4622112214565277, acc: 0.931034505367279)
[2024-11-13 06:59:07,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:08,246][root][INFO] - Training Epoch: 8/10, step 166/574 completed (loss: 0.07106323540210724, acc: 0.9795918464660645)
[2024-11-13 06:59:08,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:08,640][root][INFO] - Training Epoch: 8/10, step 167/574 completed (loss: 0.1319119781255722, acc: 0.9599999785423279)
[2024-11-13 06:59:08,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:09,184][root][INFO] - Training Epoch: 8/10, step 168/574 completed (loss: 0.27050766348838806, acc: 0.9166666865348816)
[2024-11-13 06:59:09,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:09,636][root][INFO] - Training Epoch: 8/10, step 169/574 completed (loss: 0.730513334274292, acc: 0.813725471496582)
[2024-11-13 06:59:10,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:11,295][root][INFO] - Training Epoch: 8/10, step 170/574 completed (loss: 0.4622267186641693, acc: 0.8561643958091736)
[2024-11-13 06:59:11,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:11,721][root][INFO] - Training Epoch: 8/10, step 171/574 completed (loss: 0.01595245860517025, acc: 1.0)
[2024-11-13 06:59:11,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:12,165][root][INFO] - Training Epoch: 8/10, step 172/574 completed (loss: 0.21842524409294128, acc: 0.9259259104728699)
[2024-11-13 06:59:12,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:12,580][root][INFO] - Training Epoch: 8/10, step 173/574 completed (loss: 0.14803089201450348, acc: 0.9642857313156128)
[2024-11-13 06:59:12,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:13,387][root][INFO] - Training Epoch: 8/10, step 174/574 completed (loss: 0.7579330205917358, acc: 0.7876105904579163)
[2024-11-13 06:59:13,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:13,791][root][INFO] - Training Epoch: 8/10, step 175/574 completed (loss: 0.2621731758117676, acc: 0.8985507488250732)
[2024-11-13 06:59:13,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:14,205][root][INFO] - Training Epoch: 8/10, step 176/574 completed (loss: 0.24216385185718536, acc: 0.9204545617103577)
[2024-11-13 06:59:14,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:15,655][root][INFO] - Training Epoch: 8/10, step 177/574 completed (loss: 0.7417013645172119, acc: 0.7709923386573792)
[2024-11-13 06:59:16,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:16,693][root][INFO] - Training Epoch: 8/10, step 178/574 completed (loss: 0.59117192029953, acc: 0.8370370268821716)
[2024-11-13 06:59:16,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:17,100][root][INFO] - Training Epoch: 8/10, step 179/574 completed (loss: 0.11710967868566513, acc: 0.9508196711540222)
[2024-11-13 06:59:17,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:17,476][root][INFO] - Training Epoch: 8/10, step 180/574 completed (loss: 0.0945165678858757, acc: 0.9583333134651184)
[2024-11-13 06:59:17,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:17,915][root][INFO] - Training Epoch: 8/10, step 181/574 completed (loss: 0.03904968872666359, acc: 0.9599999785423279)
[2024-11-13 06:59:18,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:18,277][root][INFO] - Training Epoch: 8/10, step 182/574 completed (loss: 0.05137201026082039, acc: 1.0)
[2024-11-13 06:59:18,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:18,736][root][INFO] - Training Epoch: 8/10, step 183/574 completed (loss: 0.0782574787735939, acc: 0.9878048896789551)
[2024-11-13 06:59:18,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:19,172][root][INFO] - Training Epoch: 8/10, step 184/574 completed (loss: 0.39169713854789734, acc: 0.8882175087928772)
[2024-11-13 06:59:19,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:19,635][root][INFO] - Training Epoch: 8/10, step 185/574 completed (loss: 0.466757208108902, acc: 0.8674351572990417)
[2024-11-13 06:59:19,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:20,323][root][INFO] - Training Epoch: 8/10, step 186/574 completed (loss: 0.5007660388946533, acc: 0.8531249761581421)
[2024-11-13 06:59:20,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:21,054][root][INFO] - Training Epoch: 8/10, step 187/574 completed (loss: 0.6084687113761902, acc: 0.8236397504806519)
[2024-11-13 06:59:21,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:21,599][root][INFO] - Training Epoch: 8/10, step 188/574 completed (loss: 0.44656503200531006, acc: 0.8790035843849182)
[2024-11-13 06:59:21,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:22,053][root][INFO] - Training Epoch: 8/10, step 189/574 completed (loss: 0.08469662815332413, acc: 0.9599999785423279)
[2024-11-13 06:59:22,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:22,899][root][INFO] - Training Epoch: 8/10, step 190/574 completed (loss: 0.3398045301437378, acc: 0.8837209343910217)
[2024-11-13 06:59:23,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:24,190][root][INFO] - Training Epoch: 8/10, step 191/574 completed (loss: 0.799079954624176, acc: 0.7301587462425232)
[2024-11-13 06:59:25,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:25,671][root][INFO] - Training Epoch: 8/10, step 192/574 completed (loss: 0.6326831579208374, acc: 0.8030303120613098)
[2024-11-13 06:59:26,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:26,846][root][INFO] - Training Epoch: 8/10, step 193/574 completed (loss: 0.3397991359233856, acc: 0.8941176533699036)
[2024-11-13 06:59:27,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:28,602][root][INFO] - Training Epoch: 8/10, step 194/574 completed (loss: 0.6433587670326233, acc: 0.8209876418113708)
[2024-11-13 06:59:29,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:30,146][root][INFO] - Training Epoch: 8/10, step 195/574 completed (loss: 0.18206007778644562, acc: 0.9677419066429138)
[2024-11-13 06:59:30,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:30,596][root][INFO] - Training Epoch: 8/10, step 196/574 completed (loss: 0.13112816214561462, acc: 0.9285714030265808)
[2024-11-13 06:59:30,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:31,067][root][INFO] - Training Epoch: 8/10, step 197/574 completed (loss: 0.23794999718666077, acc: 0.949999988079071)
[2024-11-13 06:59:31,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:31,573][root][INFO] - Training Epoch: 8/10, step 198/574 completed (loss: 0.29303550720214844, acc: 0.8823529481887817)
[2024-11-13 06:59:31,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:31,980][root][INFO] - Training Epoch: 8/10, step 199/574 completed (loss: 0.5488306879997253, acc: 0.8382353186607361)
[2024-11-13 06:59:32,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:32,435][root][INFO] - Training Epoch: 8/10, step 200/574 completed (loss: 0.29671233892440796, acc: 0.9152542352676392)
[2024-11-13 06:59:32,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:32,865][root][INFO] - Training Epoch: 8/10, step 201/574 completed (loss: 0.5955075621604919, acc: 0.7910447716712952)
[2024-11-13 06:59:33,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:33,329][root][INFO] - Training Epoch: 8/10, step 202/574 completed (loss: 0.3613150417804718, acc: 0.8640776872634888)
[2024-11-13 06:59:33,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:33,768][root][INFO] - Training Epoch: 8/10, step 203/574 completed (loss: 0.27315279841423035, acc: 0.9047619104385376)
[2024-11-13 06:59:33,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:34,171][root][INFO] - Training Epoch: 8/10, step 204/574 completed (loss: 0.08283643424510956, acc: 0.9780219793319702)
[2024-11-13 06:59:34,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:34,614][root][INFO] - Training Epoch: 8/10, step 205/574 completed (loss: 0.20367872714996338, acc: 0.9327354431152344)
[2024-11-13 06:59:34,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:35,149][root][INFO] - Training Epoch: 8/10, step 206/574 completed (loss: 0.25509172677993774, acc: 0.913385808467865)
[2024-11-13 06:59:35,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:35,553][root][INFO] - Training Epoch: 8/10, step 207/574 completed (loss: 0.2735634446144104, acc: 0.9353448152542114)
[2024-11-13 06:59:35,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:35,987][root][INFO] - Training Epoch: 8/10, step 208/574 completed (loss: 0.3134205639362335, acc: 0.9021739363670349)
[2024-11-13 06:59:36,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:36,455][root][INFO] - Training Epoch: 8/10, step 209/574 completed (loss: 0.2987862229347229, acc: 0.8988326787948608)
[2024-11-13 06:59:36,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:36,874][root][INFO] - Training Epoch: 8/10, step 210/574 completed (loss: 0.15583081543445587, acc: 0.9347826242446899)
[2024-11-13 06:59:37,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:37,243][root][INFO] - Training Epoch: 8/10, step 211/574 completed (loss: 0.011321749538183212, acc: 1.0)
[2024-11-13 06:59:37,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:37,666][root][INFO] - Training Epoch: 8/10, step 212/574 completed (loss: 0.024541040882468224, acc: 1.0)
[2024-11-13 06:59:37,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:38,113][root][INFO] - Training Epoch: 8/10, step 213/574 completed (loss: 0.1402442753314972, acc: 0.957446813583374)
[2024-11-13 06:59:38,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:39,169][root][INFO] - Training Epoch: 8/10, step 214/574 completed (loss: 0.17616598308086395, acc: 0.9615384340286255)
[2024-11-13 06:59:39,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:39,633][root][INFO] - Training Epoch: 8/10, step 215/574 completed (loss: 0.10279735177755356, acc: 0.9594594836235046)
[2024-11-13 06:59:39,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:40,065][root][INFO] - Training Epoch: 8/10, step 216/574 completed (loss: 0.09053578227758408, acc: 0.9767441749572754)
[2024-11-13 06:59:40,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:40,869][root][INFO] - Training Epoch: 8/10, step 217/574 completed (loss: 0.12178287655115128, acc: 0.954954981803894)
[2024-11-13 06:59:41,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:41,413][root][INFO] - Training Epoch: 8/10, step 218/574 completed (loss: 0.07536618411540985, acc: 0.9777777791023254)
[2024-11-13 06:59:41,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:41,833][root][INFO] - Training Epoch: 8/10, step 219/574 completed (loss: 0.03300732001662254, acc: 1.0)
[2024-11-13 06:59:42,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:42,287][root][INFO] - Training Epoch: 8/10, step 220/574 completed (loss: 0.03385158255696297, acc: 0.9629629850387573)
[2024-11-13 06:59:42,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:42,696][root][INFO] - Training Epoch: 8/10, step 221/574 completed (loss: 0.012497179210186005, acc: 1.0)
[2024-11-13 06:59:42,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:43,116][root][INFO] - Training Epoch: 8/10, step 222/574 completed (loss: 0.30058231949806213, acc: 0.9230769276618958)
[2024-11-13 06:59:43,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:44,294][root][INFO] - Training Epoch: 8/10, step 223/574 completed (loss: 0.252701997756958, acc: 0.9239130616188049)
[2024-11-13 06:59:44,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:45,108][root][INFO] - Training Epoch: 8/10, step 224/574 completed (loss: 0.2865554690361023, acc: 0.9090909361839294)
[2024-11-13 06:59:45,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:45,735][root][INFO] - Training Epoch: 8/10, step 225/574 completed (loss: 0.34637537598609924, acc: 0.8404255509376526)
[2024-11-13 06:59:45,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:46,197][root][INFO] - Training Epoch: 8/10, step 226/574 completed (loss: 0.10790760815143585, acc: 0.9622641801834106)
[2024-11-13 06:59:46,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:46,650][root][INFO] - Training Epoch: 8/10, step 227/574 completed (loss: 0.15175797045230865, acc: 0.9333333373069763)
[2024-11-13 06:59:46,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:47,067][root][INFO] - Training Epoch: 8/10, step 228/574 completed (loss: 0.1422206461429596, acc: 0.930232584476471)
[2024-11-13 06:59:47,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:47,464][root][INFO] - Training Epoch: 8/10, step 229/574 completed (loss: 0.3684581220149994, acc: 0.8999999761581421)
[2024-11-13 06:59:47,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:47,928][root][INFO] - Training Epoch: 8/10, step 230/574 completed (loss: 1.072014331817627, acc: 0.6631578803062439)
[2024-11-13 06:59:48,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:48,379][root][INFO] - Training Epoch: 8/10, step 231/574 completed (loss: 0.6877192854881287, acc: 0.7777777910232544)
[2024-11-13 06:59:48,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:48,966][root][INFO] - Training Epoch: 8/10, step 232/574 completed (loss: 0.8916991353034973, acc: 0.7444444298744202)
[2024-11-13 06:59:49,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:49,682][root][INFO] - Training Epoch: 8/10, step 233/574 completed (loss: 1.224774718284607, acc: 0.642201840877533)
[2024-11-13 06:59:50,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:50,366][root][INFO] - Training Epoch: 8/10, step 234/574 completed (loss: 0.7393560409545898, acc: 0.8230769038200378)
[2024-11-13 06:59:50,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:50,776][root][INFO] - Training Epoch: 8/10, step 235/574 completed (loss: 0.01892777718603611, acc: 1.0)
[2024-11-13 06:59:51,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:51,248][root][INFO] - Training Epoch: 8/10, step 236/574 completed (loss: 0.03358028456568718, acc: 1.0)
[2024-11-13 06:59:51,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:51,644][root][INFO] - Training Epoch: 8/10, step 237/574 completed (loss: 0.25413063168525696, acc: 0.9545454382896423)
[2024-11-13 06:59:51,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:52,034][root][INFO] - Training Epoch: 8/10, step 238/574 completed (loss: 0.373793363571167, acc: 0.9259259104728699)
[2024-11-13 06:59:52,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:52,473][root][INFO] - Training Epoch: 8/10, step 239/574 completed (loss: 0.2651256322860718, acc: 0.9142857193946838)
[2024-11-13 06:59:52,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:52,922][root][INFO] - Training Epoch: 8/10, step 240/574 completed (loss: 0.35998010635375977, acc: 0.9090909361839294)
[2024-11-13 06:59:53,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:53,301][root][INFO] - Training Epoch: 8/10, step 241/574 completed (loss: 0.10989906638860703, acc: 0.9545454382896423)
[2024-11-13 06:59:53,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:54,191][root][INFO] - Training Epoch: 8/10, step 242/574 completed (loss: 0.2982051372528076, acc: 0.8870967626571655)
[2024-11-13 06:59:54,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:54,995][root][INFO] - Training Epoch: 8/10, step 243/574 completed (loss: 0.22644411027431488, acc: 0.9318181872367859)
[2024-11-13 06:59:55,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:55,392][root][INFO] - Training Epoch: 8/10, step 244/574 completed (loss: 0.0017143267905339599, acc: 1.0)
[2024-11-13 06:59:55,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:55,744][root][INFO] - Training Epoch: 8/10, step 245/574 completed (loss: 0.4024812877178192, acc: 0.8461538553237915)
[2024-11-13 06:59:55,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:56,178][root][INFO] - Training Epoch: 8/10, step 246/574 completed (loss: 0.0627976730465889, acc: 0.9677419066429138)
[2024-11-13 06:59:56,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:56,578][root][INFO] - Training Epoch: 8/10, step 247/574 completed (loss: 0.03711359575390816, acc: 1.0)
[2024-11-13 06:59:56,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:57,031][root][INFO] - Training Epoch: 8/10, step 248/574 completed (loss: 0.04558563977479935, acc: 1.0)
[2024-11-13 06:59:57,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:57,425][root][INFO] - Training Epoch: 8/10, step 249/574 completed (loss: 0.13183940947055817, acc: 0.9459459185600281)
[2024-11-13 06:59:57,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:57,870][root][INFO] - Training Epoch: 8/10, step 250/574 completed (loss: 0.013059287331998348, acc: 1.0)
[2024-11-13 06:59:58,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:58,345][root][INFO] - Training Epoch: 8/10, step 251/574 completed (loss: 0.061444029211997986, acc: 1.0)
[2024-11-13 06:59:58,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:58,788][root][INFO] - Training Epoch: 8/10, step 252/574 completed (loss: 0.016820283606648445, acc: 1.0)
[2024-11-13 06:59:58,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:59,170][root][INFO] - Training Epoch: 8/10, step 253/574 completed (loss: 0.017383301630616188, acc: 1.0)
[2024-11-13 06:59:59,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:59,593][root][INFO] - Training Epoch: 8/10, step 254/574 completed (loss: 0.0029787400271743536, acc: 1.0)
[2024-11-13 06:59:59,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 06:59:59,978][root][INFO] - Training Epoch: 8/10, step 255/574 completed (loss: 0.0430961549282074, acc: 0.9677419066429138)
[2024-11-13 07:00:00,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:00,388][root][INFO] - Training Epoch: 8/10, step 256/574 completed (loss: 0.10086899995803833, acc: 0.9824561476707458)
[2024-11-13 07:00:00,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:00,814][root][INFO] - Training Epoch: 8/10, step 257/574 completed (loss: 0.09196356683969498, acc: 0.9714285731315613)
[2024-11-13 07:00:00,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:01,204][root][INFO] - Training Epoch: 8/10, step 258/574 completed (loss: 0.09430505335330963, acc: 0.9605262875556946)
[2024-11-13 07:00:01,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:02,050][root][INFO] - Training Epoch: 8/10, step 259/574 completed (loss: 0.18582803010940552, acc: 0.9339622855186462)
[2024-11-13 07:00:02,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:02,926][root][INFO] - Training Epoch: 8/10, step 260/574 completed (loss: 0.22686797380447388, acc: 0.9333333373069763)
[2024-11-13 07:00:03,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:03,275][root][INFO] - Training Epoch: 8/10, step 261/574 completed (loss: 0.19481214880943298, acc: 0.9444444179534912)
[2024-11-13 07:00:03,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:03,613][root][INFO] - Training Epoch: 8/10, step 262/574 completed (loss: 0.20210735499858856, acc: 0.9677419066429138)
[2024-11-13 07:00:03,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:04,044][root][INFO] - Training Epoch: 8/10, step 263/574 completed (loss: 0.3588814437389374, acc: 0.8933333158493042)
[2024-11-13 07:00:04,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:04,408][root][INFO] - Training Epoch: 8/10, step 264/574 completed (loss: 0.1581611931324005, acc: 0.9791666865348816)
[2024-11-13 07:00:05,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:05,747][root][INFO] - Training Epoch: 8/10, step 265/574 completed (loss: 0.8022403120994568, acc: 0.7680000066757202)
[2024-11-13 07:00:05,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:06,147][root][INFO] - Training Epoch: 8/10, step 266/574 completed (loss: 0.3165615200996399, acc: 0.8876404762268066)
[2024-11-13 07:00:06,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:06,588][root][INFO] - Training Epoch: 8/10, step 267/574 completed (loss: 0.3318144679069519, acc: 0.9189189076423645)
[2024-11-13 07:00:06,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:07,254][root][INFO] - Training Epoch: 8/10, step 268/574 completed (loss: 0.2603515684604645, acc: 0.931034505367279)
[2024-11-13 07:00:07,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:07,598][root][INFO] - Training Epoch: 8/10, step 269/574 completed (loss: 0.022552773356437683, acc: 1.0)
[2024-11-13 07:00:07,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:07,951][root][INFO] - Training Epoch: 8/10, step 270/574 completed (loss: 0.027620304375886917, acc: 1.0)
[2024-11-13 07:00:08,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:08,305][root][INFO] - Training Epoch: 8/10, step 271/574 completed (loss: 0.03613289073109627, acc: 1.0)
[2024-11-13 07:00:09,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:09,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:10,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:10,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:11,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:12,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:12,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:12,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:13,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:14,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:14,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:15,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:15,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:16,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:17,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:17,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:18,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:18,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:19,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:19,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:19,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:20,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:20,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:21,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:22,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:22,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:23,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:23,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:24,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:24,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:24,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:25,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:26,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:26,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:27,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:27,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:27,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:28,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:29,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:29,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:30,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:30,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:31,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:31,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:32,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:32,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:33,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:33,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:34,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:34,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:34,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:35,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:35,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:36,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:36,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:37,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:37,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:37,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:38,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:38,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:39,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:40,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:40,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:41,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:41,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:42,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:42,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:43,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:44,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:44,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:44,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:45,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:46,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:46,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:47,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:47,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:48,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:48,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:49,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:49,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:50,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:50,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:51,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:51,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:52,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:52,891][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4529, device='cuda:0') eval_epoch_loss=tensor(0.8973, device='cuda:0') eval_epoch_acc=tensor(0.8113, device='cuda:0')
[2024-11-13 07:00:52,892][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 07:00:52,893][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 07:00:53,348][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_8_step_272_loss_0.8972610831260681/model.pt
[2024-11-13 07:00:53,360][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 07:00:53,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:53,870][root][INFO] - Training Epoch: 8/10, step 272/574 completed (loss: 0.05631481483578682, acc: 0.9666666388511658)
[2024-11-13 07:00:54,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:54,400][root][INFO] - Training Epoch: 8/10, step 273/574 completed (loss: 0.10585067421197891, acc: 0.9833333492279053)
[2024-11-13 07:00:54,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:54,769][root][INFO] - Training Epoch: 8/10, step 274/574 completed (loss: 0.10356014221906662, acc: 0.96875)
[2024-11-13 07:00:54,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:55,209][root][INFO] - Training Epoch: 8/10, step 275/574 completed (loss: 0.24273435771465302, acc: 0.9333333373069763)
[2024-11-13 07:00:55,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:55,661][root][INFO] - Training Epoch: 8/10, step 276/574 completed (loss: 0.08452775329351425, acc: 0.9655172228813171)
[2024-11-13 07:00:55,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:56,094][root][INFO] - Training Epoch: 8/10, step 277/574 completed (loss: 0.005463866516947746, acc: 1.0)
[2024-11-13 07:00:56,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:56,502][root][INFO] - Training Epoch: 8/10, step 278/574 completed (loss: 0.143866166472435, acc: 0.914893627166748)
[2024-11-13 07:00:56,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:56,894][root][INFO] - Training Epoch: 8/10, step 279/574 completed (loss: 0.21040906012058258, acc: 0.9375)
[2024-11-13 07:00:57,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:57,261][root][INFO] - Training Epoch: 8/10, step 280/574 completed (loss: 0.02126399427652359, acc: 1.0)
[2024-11-13 07:00:57,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:57,886][root][INFO] - Training Epoch: 8/10, step 281/574 completed (loss: 0.22845406830310822, acc: 0.9518072009086609)
[2024-11-13 07:00:58,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:58,364][root][INFO] - Training Epoch: 8/10, step 282/574 completed (loss: 0.5527904033660889, acc: 0.8240740895271301)
[2024-11-13 07:00:58,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:58,785][root][INFO] - Training Epoch: 8/10, step 283/574 completed (loss: 0.036084968596696854, acc: 1.0)
[2024-11-13 07:00:58,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:59,202][root][INFO] - Training Epoch: 8/10, step 284/574 completed (loss: 0.12591253221035004, acc: 0.9411764740943909)
[2024-11-13 07:00:59,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:00:59,652][root][INFO] - Training Epoch: 8/10, step 285/574 completed (loss: 0.15786275267601013, acc: 0.925000011920929)
[2024-11-13 07:00:59,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:00,102][root][INFO] - Training Epoch: 8/10, step 286/574 completed (loss: 0.2461334615945816, acc: 0.921875)
[2024-11-13 07:01:00,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:00,559][root][INFO] - Training Epoch: 8/10, step 287/574 completed (loss: 0.20478491485118866, acc: 0.9359999895095825)
[2024-11-13 07:01:00,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:00,960][root][INFO] - Training Epoch: 8/10, step 288/574 completed (loss: 0.17858248949050903, acc: 0.9450549483299255)
[2024-11-13 07:01:01,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:01,341][root][INFO] - Training Epoch: 8/10, step 289/574 completed (loss: 0.14898185431957245, acc: 0.9378882050514221)
[2024-11-13 07:01:01,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:01,794][root][INFO] - Training Epoch: 8/10, step 290/574 completed (loss: 0.3643629848957062, acc: 0.8865979313850403)
[2024-11-13 07:01:02,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:02,235][root][INFO] - Training Epoch: 8/10, step 291/574 completed (loss: 0.005342078395187855, acc: 1.0)
[2024-11-13 07:01:02,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:02,666][root][INFO] - Training Epoch: 8/10, step 292/574 completed (loss: 0.4125288426876068, acc: 0.9285714030265808)
[2024-11-13 07:01:02,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:03,132][root][INFO] - Training Epoch: 8/10, step 293/574 completed (loss: 0.15244385600090027, acc: 0.9482758641242981)
[2024-11-13 07:01:03,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:03,829][root][INFO] - Training Epoch: 8/10, step 294/574 completed (loss: 0.24104228615760803, acc: 0.8727272748947144)
[2024-11-13 07:01:04,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:04,640][root][INFO] - Training Epoch: 8/10, step 295/574 completed (loss: 0.5374780297279358, acc: 0.8402062058448792)
[2024-11-13 07:01:04,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:05,073][root][INFO] - Training Epoch: 8/10, step 296/574 completed (loss: 0.10024093836545944, acc: 0.982758641242981)
[2024-11-13 07:01:05,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:05,539][root][INFO] - Training Epoch: 8/10, step 297/574 completed (loss: 0.07625875622034073, acc: 0.9629629850387573)
[2024-11-13 07:01:05,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:05,956][root][INFO] - Training Epoch: 8/10, step 298/574 completed (loss: 0.17829735577106476, acc: 0.9210526347160339)
[2024-11-13 07:01:06,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:06,367][root][INFO] - Training Epoch: 8/10, step 299/574 completed (loss: 0.08183842152357101, acc: 0.9642857313156128)
[2024-11-13 07:01:06,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:06,769][root][INFO] - Training Epoch: 8/10, step 300/574 completed (loss: 0.011979826726019382, acc: 1.0)
[2024-11-13 07:01:06,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:07,211][root][INFO] - Training Epoch: 8/10, step 301/574 completed (loss: 0.09550917893648148, acc: 0.9622641801834106)
[2024-11-13 07:01:07,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:07,624][root][INFO] - Training Epoch: 8/10, step 302/574 completed (loss: 0.05889100581407547, acc: 0.9622641801834106)
[2024-11-13 07:01:07,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:08,016][root][INFO] - Training Epoch: 8/10, step 303/574 completed (loss: 0.021901223808526993, acc: 1.0)
[2024-11-13 07:01:08,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:08,404][root][INFO] - Training Epoch: 8/10, step 304/574 completed (loss: 0.052644871175289154, acc: 1.0)
[2024-11-13 07:01:08,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:08,864][root][INFO] - Training Epoch: 8/10, step 305/574 completed (loss: 0.16806642711162567, acc: 0.9180327653884888)
[2024-11-13 07:01:09,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:09,304][root][INFO] - Training Epoch: 8/10, step 306/574 completed (loss: 0.17683841288089752, acc: 0.9666666388511658)
[2024-11-13 07:01:09,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:09,751][root][INFO] - Training Epoch: 8/10, step 307/574 completed (loss: 0.07163746654987335, acc: 0.9473684430122375)
[2024-11-13 07:01:09,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:10,130][root][INFO] - Training Epoch: 8/10, step 308/574 completed (loss: 0.0912480279803276, acc: 0.9855072498321533)
[2024-11-13 07:01:10,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:10,710][root][INFO] - Training Epoch: 8/10, step 309/574 completed (loss: 0.05233639478683472, acc: 1.0)
[2024-11-13 07:01:10,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:11,087][root][INFO] - Training Epoch: 8/10, step 310/574 completed (loss: 0.11180920153856277, acc: 0.9759036302566528)
[2024-11-13 07:01:11,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:11,471][root][INFO] - Training Epoch: 8/10, step 311/574 completed (loss: 0.23779945075511932, acc: 0.9358974099159241)
[2024-11-13 07:01:11,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:11,916][root][INFO] - Training Epoch: 8/10, step 312/574 completed (loss: 0.13405728340148926, acc: 0.9489796161651611)
[2024-11-13 07:01:12,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:12,337][root][INFO] - Training Epoch: 8/10, step 313/574 completed (loss: 0.00546666607260704, acc: 1.0)
[2024-11-13 07:01:12,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:12,708][root][INFO] - Training Epoch: 8/10, step 314/574 completed (loss: 0.18705330789089203, acc: 0.9583333134651184)
[2024-11-13 07:01:12,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:13,058][root][INFO] - Training Epoch: 8/10, step 315/574 completed (loss: 0.09274578094482422, acc: 0.9677419066429138)
[2024-11-13 07:01:13,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:13,425][root][INFO] - Training Epoch: 8/10, step 316/574 completed (loss: 0.17557847499847412, acc: 0.9677419066429138)
[2024-11-13 07:01:13,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:13,864][root][INFO] - Training Epoch: 8/10, step 317/574 completed (loss: 0.13059912621974945, acc: 0.9701492786407471)
[2024-11-13 07:01:14,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:14,310][root][INFO] - Training Epoch: 8/10, step 318/574 completed (loss: 0.05645016208291054, acc: 0.9903846383094788)
[2024-11-13 07:01:14,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:14,719][root][INFO] - Training Epoch: 8/10, step 319/574 completed (loss: 0.03573780506849289, acc: 0.9777777791023254)
[2024-11-13 07:01:14,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:15,142][root][INFO] - Training Epoch: 8/10, step 320/574 completed (loss: 0.07076850533485413, acc: 0.9677419066429138)
[2024-11-13 07:01:15,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:15,611][root][INFO] - Training Epoch: 8/10, step 321/574 completed (loss: 0.010118433274328709, acc: 1.0)
[2024-11-13 07:01:15,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:16,063][root][INFO] - Training Epoch: 8/10, step 322/574 completed (loss: 0.49898168444633484, acc: 0.8148148059844971)
[2024-11-13 07:01:16,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:16,477][root][INFO] - Training Epoch: 8/10, step 323/574 completed (loss: 0.25923973321914673, acc: 0.9714285731315613)
[2024-11-13 07:01:16,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:16,906][root][INFO] - Training Epoch: 8/10, step 324/574 completed (loss: 0.49055761098861694, acc: 0.8974359035491943)
[2024-11-13 07:01:17,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:17,355][root][INFO] - Training Epoch: 8/10, step 325/574 completed (loss: 0.4616319537162781, acc: 0.8780487775802612)
[2024-11-13 07:01:17,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:17,764][root][INFO] - Training Epoch: 8/10, step 326/574 completed (loss: 0.3236048221588135, acc: 0.8421052694320679)
[2024-11-13 07:01:17,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:18,175][root][INFO] - Training Epoch: 8/10, step 327/574 completed (loss: 0.01471018511801958, acc: 1.0)
[2024-11-13 07:01:18,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:18,602][root][INFO] - Training Epoch: 8/10, step 328/574 completed (loss: 0.10930418223142624, acc: 0.9642857313156128)
[2024-11-13 07:01:18,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:19,085][root][INFO] - Training Epoch: 8/10, step 329/574 completed (loss: 0.026918482035398483, acc: 1.0)
[2024-11-13 07:01:19,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:19,491][root][INFO] - Training Epoch: 8/10, step 330/574 completed (loss: 0.051913727074861526, acc: 0.96875)
[2024-11-13 07:01:19,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:19,912][root][INFO] - Training Epoch: 8/10, step 331/574 completed (loss: 0.13447391986846924, acc: 0.9354838728904724)
[2024-11-13 07:01:20,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:20,391][root][INFO] - Training Epoch: 8/10, step 332/574 completed (loss: 0.04314477741718292, acc: 0.9824561476707458)
[2024-11-13 07:01:20,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:20,794][root][INFO] - Training Epoch: 8/10, step 333/574 completed (loss: 0.034172847867012024, acc: 1.0)
[2024-11-13 07:01:21,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:21,239][root][INFO] - Training Epoch: 8/10, step 334/574 completed (loss: 0.3929024934768677, acc: 0.9666666388511658)
[2024-11-13 07:01:21,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:21,692][root][INFO] - Training Epoch: 8/10, step 335/574 completed (loss: 0.18026569485664368, acc: 0.8947368264198303)
[2024-11-13 07:01:21,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:22,099][root][INFO] - Training Epoch: 8/10, step 336/574 completed (loss: 0.21767200529575348, acc: 0.8999999761581421)
[2024-11-13 07:01:22,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:22,541][root][INFO] - Training Epoch: 8/10, step 337/574 completed (loss: 0.386046439409256, acc: 0.8505747318267822)
[2024-11-13 07:01:22,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:23,007][root][INFO] - Training Epoch: 8/10, step 338/574 completed (loss: 0.691241979598999, acc: 0.7659574747085571)
[2024-11-13 07:01:23,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:23,382][root][INFO] - Training Epoch: 8/10, step 339/574 completed (loss: 0.5353233814239502, acc: 0.8795180916786194)
[2024-11-13 07:01:23,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:23,827][root][INFO] - Training Epoch: 8/10, step 340/574 completed (loss: 0.01032277476042509, acc: 1.0)
[2024-11-13 07:01:24,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:24,190][root][INFO] - Training Epoch: 8/10, step 341/574 completed (loss: 0.0443720705807209, acc: 1.0)
[2024-11-13 07:01:24,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:24,616][root][INFO] - Training Epoch: 8/10, step 342/574 completed (loss: 0.15090227127075195, acc: 0.9397590160369873)
[2024-11-13 07:01:24,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:25,039][root][INFO] - Training Epoch: 8/10, step 343/574 completed (loss: 0.26555177569389343, acc: 0.9245283007621765)
[2024-11-13 07:01:25,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:25,428][root][INFO] - Training Epoch: 8/10, step 344/574 completed (loss: 0.14316049218177795, acc: 0.9367088675498962)
[2024-11-13 07:01:25,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:25,784][root][INFO] - Training Epoch: 8/10, step 345/574 completed (loss: 0.04595883563160896, acc: 0.9803921580314636)
[2024-11-13 07:01:25,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:26,207][root][INFO] - Training Epoch: 8/10, step 346/574 completed (loss: 0.09473136812448502, acc: 0.9552238583564758)
[2024-11-13 07:01:26,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:26,660][root][INFO] - Training Epoch: 8/10, step 347/574 completed (loss: 0.031743623316287994, acc: 1.0)
[2024-11-13 07:01:26,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:27,092][root][INFO] - Training Epoch: 8/10, step 348/574 completed (loss: 0.07679787278175354, acc: 0.9599999785423279)
[2024-11-13 07:01:27,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:27,614][root][INFO] - Training Epoch: 8/10, step 349/574 completed (loss: 0.33402755856513977, acc: 0.8888888955116272)
[2024-11-13 07:01:27,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:28,051][root][INFO] - Training Epoch: 8/10, step 350/574 completed (loss: 0.3399938642978668, acc: 0.8372092843055725)
[2024-11-13 07:01:28,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:28,500][root][INFO] - Training Epoch: 8/10, step 351/574 completed (loss: 0.05176081880927086, acc: 0.9743589758872986)
[2024-11-13 07:01:28,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:28,997][root][INFO] - Training Epoch: 8/10, step 352/574 completed (loss: 0.31392812728881836, acc: 0.9111111164093018)
[2024-11-13 07:01:29,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:29,440][root][INFO] - Training Epoch: 8/10, step 353/574 completed (loss: 0.03737815469503403, acc: 1.0)
[2024-11-13 07:01:29,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:29,840][root][INFO] - Training Epoch: 8/10, step 354/574 completed (loss: 0.1762181669473648, acc: 0.9230769276618958)
[2024-11-13 07:01:30,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:30,290][root][INFO] - Training Epoch: 8/10, step 355/574 completed (loss: 0.30260542035102844, acc: 0.9340659379959106)
[2024-11-13 07:01:30,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:31,030][root][INFO] - Training Epoch: 8/10, step 356/574 completed (loss: 0.3836377263069153, acc: 0.8695651888847351)
[2024-11-13 07:01:31,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:31,511][root][INFO] - Training Epoch: 8/10, step 357/574 completed (loss: 0.2100914716720581, acc: 0.945652186870575)
[2024-11-13 07:01:31,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:31,927][root][INFO] - Training Epoch: 8/10, step 358/574 completed (loss: 0.2484046220779419, acc: 0.8979591727256775)
[2024-11-13 07:01:32,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:32,295][root][INFO] - Training Epoch: 8/10, step 359/574 completed (loss: 0.0020954953506588936, acc: 1.0)
[2024-11-13 07:01:32,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:32,647][root][INFO] - Training Epoch: 8/10, step 360/574 completed (loss: 0.023160208016633987, acc: 1.0)
[2024-11-13 07:01:32,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:32,995][root][INFO] - Training Epoch: 8/10, step 361/574 completed (loss: 0.05784660577774048, acc: 1.0)
[2024-11-13 07:01:33,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:33,363][root][INFO] - Training Epoch: 8/10, step 362/574 completed (loss: 0.10115361958742142, acc: 0.9555555582046509)
[2024-11-13 07:01:33,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:33,730][root][INFO] - Training Epoch: 8/10, step 363/574 completed (loss: 0.1399555802345276, acc: 0.9473684430122375)
[2024-11-13 07:01:33,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:34,166][root][INFO] - Training Epoch: 8/10, step 364/574 completed (loss: 0.1050989106297493, acc: 0.9756097793579102)
[2024-11-13 07:01:34,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:34,552][root][INFO] - Training Epoch: 8/10, step 365/574 completed (loss: 0.05729392543435097, acc: 1.0)
[2024-11-13 07:01:34,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:34,905][root][INFO] - Training Epoch: 8/10, step 366/574 completed (loss: 0.0018785255961120129, acc: 1.0)
[2024-11-13 07:01:35,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:35,284][root][INFO] - Training Epoch: 8/10, step 367/574 completed (loss: 0.07205728441476822, acc: 0.95652174949646)
[2024-11-13 07:01:35,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:35,704][root][INFO] - Training Epoch: 8/10, step 368/574 completed (loss: 0.034934867173433304, acc: 0.9642857313156128)
[2024-11-13 07:01:35,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:36,092][root][INFO] - Training Epoch: 8/10, step 369/574 completed (loss: 0.0749206691980362, acc: 0.96875)
[2024-11-13 07:01:36,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:36,998][root][INFO] - Training Epoch: 8/10, step 370/574 completed (loss: 0.5803545117378235, acc: 0.8363636136054993)
[2024-11-13 07:01:37,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:38,315][root][INFO] - Training Epoch: 8/10, step 371/574 completed (loss: 0.20831774175167084, acc: 0.9528301954269409)
[2024-11-13 07:01:38,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:38,726][root][INFO] - Training Epoch: 8/10, step 372/574 completed (loss: 0.1565280556678772, acc: 0.9555555582046509)
[2024-11-13 07:01:38,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:39,113][root][INFO] - Training Epoch: 8/10, step 373/574 completed (loss: 0.07296008616685867, acc: 0.9642857313156128)
[2024-11-13 07:01:39,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:39,536][root][INFO] - Training Epoch: 8/10, step 374/574 completed (loss: 0.05411376431584358, acc: 0.9714285731315613)
[2024-11-13 07:01:39,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:39,916][root][INFO] - Training Epoch: 8/10, step 375/574 completed (loss: 0.002348964335396886, acc: 1.0)
[2024-11-13 07:01:40,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:40,256][root][INFO] - Training Epoch: 8/10, step 376/574 completed (loss: 0.005522747524082661, acc: 1.0)
[2024-11-13 07:01:40,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:40,602][root][INFO] - Training Epoch: 8/10, step 377/574 completed (loss: 0.06366336345672607, acc: 0.9791666865348816)
[2024-11-13 07:01:40,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:40,997][root][INFO] - Training Epoch: 8/10, step 378/574 completed (loss: 0.07456199079751968, acc: 0.9684210419654846)
[2024-11-13 07:01:41,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:41,857][root][INFO] - Training Epoch: 8/10, step 379/574 completed (loss: 0.19190524518489838, acc: 0.940119743347168)
[2024-11-13 07:01:42,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:42,416][root][INFO] - Training Epoch: 8/10, step 380/574 completed (loss: 0.14638403058052063, acc: 0.9548872113227844)
[2024-11-13 07:01:43,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:44,311][root][INFO] - Training Epoch: 8/10, step 381/574 completed (loss: 0.4593302011489868, acc: 0.8609625697135925)
[2024-11-13 07:01:44,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:45,162][root][INFO] - Training Epoch: 8/10, step 382/574 completed (loss: 0.06483837962150574, acc: 0.9729729890823364)
[2024-11-13 07:01:45,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:45,536][root][INFO] - Training Epoch: 8/10, step 383/574 completed (loss: 0.04543730989098549, acc: 1.0)
[2024-11-13 07:01:45,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:45,933][root][INFO] - Training Epoch: 8/10, step 384/574 completed (loss: 0.07257360219955444, acc: 0.9642857313156128)
[2024-11-13 07:01:46,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:46,276][root][INFO] - Training Epoch: 8/10, step 385/574 completed (loss: 0.018960565328598022, acc: 1.0)
[2024-11-13 07:01:46,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:46,598][root][INFO] - Training Epoch: 8/10, step 386/574 completed (loss: 0.009937739931046963, acc: 1.0)
[2024-11-13 07:01:46,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:46,970][root][INFO] - Training Epoch: 8/10, step 387/574 completed (loss: 0.0027411228511482477, acc: 1.0)
[2024-11-13 07:01:47,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:47,374][root][INFO] - Training Epoch: 8/10, step 388/574 completed (loss: 0.0014968202449381351, acc: 1.0)
[2024-11-13 07:01:47,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:47,745][root][INFO] - Training Epoch: 8/10, step 389/574 completed (loss: 0.0069026099517941475, acc: 1.0)
[2024-11-13 07:01:47,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:48,095][root][INFO] - Training Epoch: 8/10, step 390/574 completed (loss: 0.18605245649814606, acc: 0.9047619104385376)
[2024-11-13 07:01:48,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:48,506][root][INFO] - Training Epoch: 8/10, step 391/574 completed (loss: 0.23417364060878754, acc: 0.9074074029922485)
[2024-11-13 07:01:48,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:48,933][root][INFO] - Training Epoch: 8/10, step 392/574 completed (loss: 0.40206456184387207, acc: 0.8640776872634888)
[2024-11-13 07:01:49,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:49,695][root][INFO] - Training Epoch: 8/10, step 393/574 completed (loss: 0.41091638803482056, acc: 0.8602941036224365)
[2024-11-13 07:01:49,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:50,184][root][INFO] - Training Epoch: 8/10, step 394/574 completed (loss: 0.4129481315612793, acc: 0.8866666555404663)
[2024-11-13 07:01:50,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:50,707][root][INFO] - Training Epoch: 8/10, step 395/574 completed (loss: 0.25547710061073303, acc: 0.9166666865348816)
[2024-11-13 07:01:50,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:51,072][root][INFO] - Training Epoch: 8/10, step 396/574 completed (loss: 0.17935790121555328, acc: 0.9534883499145508)
[2024-11-13 07:01:51,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:51,424][root][INFO] - Training Epoch: 8/10, step 397/574 completed (loss: 0.047688230872154236, acc: 0.9583333134651184)
[2024-11-13 07:01:51,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:51,801][root][INFO] - Training Epoch: 8/10, step 398/574 completed (loss: 0.2572421729564667, acc: 0.930232584476471)
[2024-11-13 07:01:51,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:52,149][root][INFO] - Training Epoch: 8/10, step 399/574 completed (loss: 0.005918965209275484, acc: 1.0)
[2024-11-13 07:01:52,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:52,949][root][INFO] - Training Epoch: 8/10, step 400/574 completed (loss: 0.11520042270421982, acc: 0.970588207244873)
[2024-11-13 07:01:53,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:53,332][root][INFO] - Training Epoch: 8/10, step 401/574 completed (loss: 0.10023238509893417, acc: 0.9599999785423279)
[2024-11-13 07:01:53,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:53,729][root][INFO] - Training Epoch: 8/10, step 402/574 completed (loss: 0.05546882003545761, acc: 1.0)
[2024-11-13 07:01:53,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:54,072][root][INFO] - Training Epoch: 8/10, step 403/574 completed (loss: 0.010106310248374939, acc: 1.0)
[2024-11-13 07:01:54,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:54,427][root][INFO] - Training Epoch: 8/10, step 404/574 completed (loss: 0.14857298135757446, acc: 0.9677419066429138)
[2024-11-13 07:01:54,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:54,806][root][INFO] - Training Epoch: 8/10, step 405/574 completed (loss: 0.004503878299146891, acc: 1.0)
[2024-11-13 07:01:54,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:55,146][root][INFO] - Training Epoch: 8/10, step 406/574 completed (loss: 0.013775620609521866, acc: 1.0)
[2024-11-13 07:01:55,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:55,500][root][INFO] - Training Epoch: 8/10, step 407/574 completed (loss: 0.005294524133205414, acc: 1.0)
[2024-11-13 07:01:55,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:55,852][root][INFO] - Training Epoch: 8/10, step 408/574 completed (loss: 0.009376495145261288, acc: 1.0)
[2024-11-13 07:01:55,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:56,186][root][INFO] - Training Epoch: 8/10, step 409/574 completed (loss: 0.010554148815572262, acc: 1.0)
[2024-11-13 07:01:56,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:56,550][root][INFO] - Training Epoch: 8/10, step 410/574 completed (loss: 0.09445055574178696, acc: 0.9655172228813171)
[2024-11-13 07:01:56,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:56,986][root][INFO] - Training Epoch: 8/10, step 411/574 completed (loss: 0.056483153253793716, acc: 0.9642857313156128)
[2024-11-13 07:01:57,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:57,357][root][INFO] - Training Epoch: 8/10, step 412/574 completed (loss: 0.008605489507317543, acc: 1.0)
[2024-11-13 07:01:57,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:57,699][root][INFO] - Training Epoch: 8/10, step 413/574 completed (loss: 0.11561533063650131, acc: 0.9696969985961914)
[2024-11-13 07:01:57,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:58,047][root][INFO] - Training Epoch: 8/10, step 414/574 completed (loss: 0.006043448578566313, acc: 1.0)
[2024-11-13 07:01:59,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:59,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:01:59,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:00,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:01,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:01,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:01,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:02,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:03,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:03,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:04,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:04,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:05,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:05,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:06,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:06,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:07,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:07,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:08,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:08,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:09,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:09,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:10,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:10,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:10,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:11,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:11,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:12,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:12,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:13,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:13,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:14,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:14,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:15,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:15,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:16,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:16,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:17,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:17,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:18,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:18,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:19,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:19,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:20,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:20,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:21,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:21,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:22,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:22,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:23,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:23,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:24,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:24,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:24,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:25,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:25,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:26,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:26,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:27,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:27,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:28,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:29,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:29,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:30,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:30,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:31,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:31,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:32,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:32,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:33,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:34,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:34,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:34,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:35,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:35,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:36,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:36,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:37,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:37,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:38,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:38,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:39,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:39,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:40,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:40,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:41,507][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4910, device='cuda:0') eval_epoch_loss=tensor(0.9127, device='cuda:0') eval_epoch_acc=tensor(0.8003, device='cuda:0')
[2024-11-13 07:02:41,509][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 07:02:41,510][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 07:02:41,956][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_8_step_415_loss_0.9126866459846497/model.pt
[2024-11-13 07:02:41,961][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 07:02:42,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:42,453][root][INFO] - Training Epoch: 8/10, step 415/574 completed (loss: 0.05228571221232414, acc: 0.9803921580314636)
[2024-11-13 07:02:42,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:42,880][root][INFO] - Training Epoch: 8/10, step 416/574 completed (loss: 0.08765526115894318, acc: 0.9615384340286255)
[2024-11-13 07:02:43,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:43,301][root][INFO] - Training Epoch: 8/10, step 417/574 completed (loss: 0.14089711010456085, acc: 0.9444444179534912)
[2024-11-13 07:02:43,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:43,732][root][INFO] - Training Epoch: 8/10, step 418/574 completed (loss: 0.039619095623493195, acc: 1.0)
[2024-11-13 07:02:43,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:44,185][root][INFO] - Training Epoch: 8/10, step 419/574 completed (loss: 0.028425225988030434, acc: 1.0)
[2024-11-13 07:02:44,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:44,596][root][INFO] - Training Epoch: 8/10, step 420/574 completed (loss: 0.11281869560480118, acc: 0.9047619104385376)
[2024-11-13 07:02:44,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:45,037][root][INFO] - Training Epoch: 8/10, step 421/574 completed (loss: 0.07662737369537354, acc: 0.9666666388511658)
[2024-11-13 07:02:45,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:45,469][root][INFO] - Training Epoch: 8/10, step 422/574 completed (loss: 0.012723464518785477, acc: 1.0)
[2024-11-13 07:02:45,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:45,891][root][INFO] - Training Epoch: 8/10, step 423/574 completed (loss: 0.09206246584653854, acc: 0.9722222089767456)
[2024-11-13 07:02:46,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:46,298][root][INFO] - Training Epoch: 8/10, step 424/574 completed (loss: 0.06905004382133484, acc: 0.9629629850387573)
[2024-11-13 07:02:46,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:46,758][root][INFO] - Training Epoch: 8/10, step 425/574 completed (loss: 0.055459484457969666, acc: 0.9696969985961914)
[2024-11-13 07:02:46,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:47,104][root][INFO] - Training Epoch: 8/10, step 426/574 completed (loss: 0.0029963974375277758, acc: 1.0)
[2024-11-13 07:02:47,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:47,537][root][INFO] - Training Epoch: 8/10, step 427/574 completed (loss: 0.09243231266736984, acc: 0.9729729890823364)
[2024-11-13 07:02:47,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:47,951][root][INFO] - Training Epoch: 8/10, step 428/574 completed (loss: 0.0855381190776825, acc: 0.9629629850387573)
[2024-11-13 07:02:48,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:48,319][root][INFO] - Training Epoch: 8/10, step 429/574 completed (loss: 0.014766217209398746, acc: 1.0)
[2024-11-13 07:02:48,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:48,726][root][INFO] - Training Epoch: 8/10, step 430/574 completed (loss: 0.0015139708993956447, acc: 1.0)
[2024-11-13 07:02:48,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:49,117][root][INFO] - Training Epoch: 8/10, step 431/574 completed (loss: 0.0029503589030355215, acc: 1.0)
[2024-11-13 07:02:49,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:49,522][root][INFO] - Training Epoch: 8/10, step 432/574 completed (loss: 0.019487867131829262, acc: 1.0)
[2024-11-13 07:02:49,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:50,012][root][INFO] - Training Epoch: 8/10, step 433/574 completed (loss: 0.07996103167533875, acc: 0.9722222089767456)
[2024-11-13 07:02:50,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:50,379][root][INFO] - Training Epoch: 8/10, step 434/574 completed (loss: 0.005203657317906618, acc: 1.0)
[2024-11-13 07:02:50,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:50,738][root][INFO] - Training Epoch: 8/10, step 435/574 completed (loss: 0.005995663348585367, acc: 1.0)
[2024-11-13 07:02:50,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:51,157][root][INFO] - Training Epoch: 8/10, step 436/574 completed (loss: 0.05515841022133827, acc: 0.9722222089767456)
[2024-11-13 07:02:51,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:51,570][root][INFO] - Training Epoch: 8/10, step 437/574 completed (loss: 0.0627656877040863, acc: 0.9545454382896423)
[2024-11-13 07:02:51,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:51,997][root][INFO] - Training Epoch: 8/10, step 438/574 completed (loss: 0.0007635732763446867, acc: 1.0)
[2024-11-13 07:02:52,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:52,404][root][INFO] - Training Epoch: 8/10, step 439/574 completed (loss: 0.03209162503480911, acc: 0.9743589758872986)
[2024-11-13 07:02:52,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:53,099][root][INFO] - Training Epoch: 8/10, step 440/574 completed (loss: 0.23834985494613647, acc: 0.9545454382896423)
[2024-11-13 07:02:53,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:54,169][root][INFO] - Training Epoch: 8/10, step 441/574 completed (loss: 0.4285139739513397, acc: 0.8560000061988831)
[2024-11-13 07:02:54,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:54,741][root][INFO] - Training Epoch: 8/10, step 442/574 completed (loss: 0.5096033215522766, acc: 0.8467742204666138)
[2024-11-13 07:02:55,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:55,747][root][INFO] - Training Epoch: 8/10, step 443/574 completed (loss: 0.3934997618198395, acc: 0.8756219148635864)
[2024-11-13 07:02:55,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:56,182][root][INFO] - Training Epoch: 8/10, step 444/574 completed (loss: 0.06104649603366852, acc: 0.9811320900917053)
[2024-11-13 07:02:56,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:56,775][root][INFO] - Training Epoch: 8/10, step 445/574 completed (loss: 0.057918619364500046, acc: 1.0)
[2024-11-13 07:02:56,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:57,211][root][INFO] - Training Epoch: 8/10, step 446/574 completed (loss: 0.022378111258149147, acc: 1.0)
[2024-11-13 07:02:57,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:57,600][root][INFO] - Training Epoch: 8/10, step 447/574 completed (loss: 0.4736168682575226, acc: 0.9230769276618958)
[2024-11-13 07:02:57,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:57,982][root][INFO] - Training Epoch: 8/10, step 448/574 completed (loss: 0.0272785983979702, acc: 1.0)
[2024-11-13 07:02:58,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:58,394][root][INFO] - Training Epoch: 8/10, step 449/574 completed (loss: 0.035510215908288956, acc: 0.9850746393203735)
[2024-11-13 07:02:58,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:58,827][root][INFO] - Training Epoch: 8/10, step 450/574 completed (loss: 0.07590670883655548, acc: 0.9722222089767456)
[2024-11-13 07:02:59,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:59,248][root][INFO] - Training Epoch: 8/10, step 451/574 completed (loss: 0.06400517374277115, acc: 0.97826087474823)
[2024-11-13 07:02:59,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:02:59,679][root][INFO] - Training Epoch: 8/10, step 452/574 completed (loss: 0.2829456627368927, acc: 0.9487179517745972)
[2024-11-13 07:02:59,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:00,030][root][INFO] - Training Epoch: 8/10, step 453/574 completed (loss: 0.15537568926811218, acc: 0.9605262875556946)
[2024-11-13 07:03:00,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:00,462][root][INFO] - Training Epoch: 8/10, step 454/574 completed (loss: 0.06149465590715408, acc: 0.9795918464660645)
[2024-11-13 07:03:00,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:00,867][root][INFO] - Training Epoch: 8/10, step 455/574 completed (loss: 0.22649933397769928, acc: 0.9090909361839294)
[2024-11-13 07:03:01,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:01,293][root][INFO] - Training Epoch: 8/10, step 456/574 completed (loss: 0.2630036771297455, acc: 0.9175257682800293)
[2024-11-13 07:03:01,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:01,696][root][INFO] - Training Epoch: 8/10, step 457/574 completed (loss: 0.04259138181805611, acc: 0.9857142567634583)
[2024-11-13 07:03:01,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:02,205][root][INFO] - Training Epoch: 8/10, step 458/574 completed (loss: 0.18228793144226074, acc: 0.9476743936538696)
[2024-11-13 07:03:02,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:02,625][root][INFO] - Training Epoch: 8/10, step 459/574 completed (loss: 0.09094638377428055, acc: 0.9821428656578064)
[2024-11-13 07:03:02,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:03,028][root][INFO] - Training Epoch: 8/10, step 460/574 completed (loss: 0.0802328959107399, acc: 0.9753086566925049)
[2024-11-13 07:03:03,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:03,394][root][INFO] - Training Epoch: 8/10, step 461/574 completed (loss: 0.14932462573051453, acc: 0.9444444179534912)
[2024-11-13 07:03:03,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:03,793][root][INFO] - Training Epoch: 8/10, step 462/574 completed (loss: 0.007914219982922077, acc: 1.0)
[2024-11-13 07:03:04,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:04,235][root][INFO] - Training Epoch: 8/10, step 463/574 completed (loss: 0.01843877322971821, acc: 1.0)
[2024-11-13 07:03:04,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:04,613][root][INFO] - Training Epoch: 8/10, step 464/574 completed (loss: 0.0660194605588913, acc: 0.97826087474823)
[2024-11-13 07:03:04,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:04,986][root][INFO] - Training Epoch: 8/10, step 465/574 completed (loss: 0.13204911351203918, acc: 0.976190447807312)
[2024-11-13 07:03:05,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:05,369][root][INFO] - Training Epoch: 8/10, step 466/574 completed (loss: 0.28554192185401917, acc: 0.9277108311653137)
[2024-11-13 07:03:05,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:05,788][root][INFO] - Training Epoch: 8/10, step 467/574 completed (loss: 0.06918106228113174, acc: 0.9729729890823364)
[2024-11-13 07:03:05,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:06,211][root][INFO] - Training Epoch: 8/10, step 468/574 completed (loss: 0.22390708327293396, acc: 0.9320388436317444)
[2024-11-13 07:03:06,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:06,632][root][INFO] - Training Epoch: 8/10, step 469/574 completed (loss: 0.24415943026542664, acc: 0.9268292784690857)
[2024-11-13 07:03:06,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:07,030][root][INFO] - Training Epoch: 8/10, step 470/574 completed (loss: 0.021279804408550262, acc: 1.0)
[2024-11-13 07:03:07,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:07,488][root][INFO] - Training Epoch: 8/10, step 471/574 completed (loss: 0.08355724811553955, acc: 0.9642857313156128)
[2024-11-13 07:03:07,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:08,069][root][INFO] - Training Epoch: 8/10, step 472/574 completed (loss: 0.17601080238819122, acc: 0.9215686321258545)
[2024-11-13 07:03:08,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:08,538][root][INFO] - Training Epoch: 8/10, step 473/574 completed (loss: 0.5777726769447327, acc: 0.8296943306922913)
[2024-11-13 07:03:08,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:08,902][root][INFO] - Training Epoch: 8/10, step 474/574 completed (loss: 0.13357429206371307, acc: 0.9583333134651184)
[2024-11-13 07:03:09,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:09,359][root][INFO] - Training Epoch: 8/10, step 475/574 completed (loss: 0.1724032610654831, acc: 0.9570552110671997)
[2024-11-13 07:03:09,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:09,751][root][INFO] - Training Epoch: 8/10, step 476/574 completed (loss: 0.1871223747730255, acc: 0.935251772403717)
[2024-11-13 07:03:09,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:10,203][root][INFO] - Training Epoch: 8/10, step 477/574 completed (loss: 0.40695106983184814, acc: 0.8743718862533569)
[2024-11-13 07:03:10,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:10,578][root][INFO] - Training Epoch: 8/10, step 478/574 completed (loss: 0.09047755599021912, acc: 0.9722222089767456)
[2024-11-13 07:03:10,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:10,975][root][INFO] - Training Epoch: 8/10, step 479/574 completed (loss: 0.2220698893070221, acc: 0.939393937587738)
[2024-11-13 07:03:11,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:11,390][root][INFO] - Training Epoch: 8/10, step 480/574 completed (loss: 0.15698392689228058, acc: 0.9259259104728699)
[2024-11-13 07:03:11,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:11,794][root][INFO] - Training Epoch: 8/10, step 481/574 completed (loss: 0.03547653928399086, acc: 1.0)
[2024-11-13 07:03:11,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:12,123][root][INFO] - Training Epoch: 8/10, step 482/574 completed (loss: 0.06617213040590286, acc: 1.0)
[2024-11-13 07:03:12,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:12,633][root][INFO] - Training Epoch: 8/10, step 483/574 completed (loss: 0.2827903628349304, acc: 0.9137930870056152)
[2024-11-13 07:03:12,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:13,067][root][INFO] - Training Epoch: 8/10, step 484/574 completed (loss: 0.07906582206487656, acc: 0.9677419066429138)
[2024-11-13 07:03:13,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:13,489][root][INFO] - Training Epoch: 8/10, step 485/574 completed (loss: 0.045415375381708145, acc: 1.0)
[2024-11-13 07:03:13,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:13,902][root][INFO] - Training Epoch: 8/10, step 486/574 completed (loss: 0.07963717728853226, acc: 0.9629629850387573)
[2024-11-13 07:03:14,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:14,335][root][INFO] - Training Epoch: 8/10, step 487/574 completed (loss: 0.06286083161830902, acc: 1.0)
[2024-11-13 07:03:14,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:14,758][root][INFO] - Training Epoch: 8/10, step 488/574 completed (loss: 0.05011574923992157, acc: 1.0)
[2024-11-13 07:03:14,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:15,219][root][INFO] - Training Epoch: 8/10, step 489/574 completed (loss: 0.3041231632232666, acc: 0.9230769276618958)
[2024-11-13 07:03:15,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:15,612][root][INFO] - Training Epoch: 8/10, step 490/574 completed (loss: 0.10999398678541183, acc: 0.9666666388511658)
[2024-11-13 07:03:15,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:16,099][root][INFO] - Training Epoch: 8/10, step 491/574 completed (loss: 0.040130048990249634, acc: 1.0)
[2024-11-13 07:03:16,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:16,501][root][INFO] - Training Epoch: 8/10, step 492/574 completed (loss: 0.06457073241472244, acc: 0.9803921580314636)
[2024-11-13 07:03:16,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:16,876][root][INFO] - Training Epoch: 8/10, step 493/574 completed (loss: 0.1398196816444397, acc: 0.9655172228813171)
[2024-11-13 07:03:17,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:17,307][root][INFO] - Training Epoch: 8/10, step 494/574 completed (loss: 0.007718994282186031, acc: 1.0)
[2024-11-13 07:03:17,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:17,742][root][INFO] - Training Epoch: 8/10, step 495/574 completed (loss: 0.14072108268737793, acc: 0.9473684430122375)
[2024-11-13 07:03:17,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:18,163][root][INFO] - Training Epoch: 8/10, step 496/574 completed (loss: 0.3901464343070984, acc: 0.875)
[2024-11-13 07:03:18,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:18,656][root][INFO] - Training Epoch: 8/10, step 497/574 completed (loss: 0.16240403056144714, acc: 0.9550561904907227)
[2024-11-13 07:03:18,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:19,017][root][INFO] - Training Epoch: 8/10, step 498/574 completed (loss: 0.3132258653640747, acc: 0.8539325594902039)
[2024-11-13 07:03:19,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:19,398][root][INFO] - Training Epoch: 8/10, step 499/574 completed (loss: 0.7117012143135071, acc: 0.8085106611251831)
[2024-11-13 07:03:19,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:19,832][root][INFO] - Training Epoch: 8/10, step 500/574 completed (loss: 0.40986621379852295, acc: 0.8804348111152649)
[2024-11-13 07:03:20,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:20,254][root][INFO] - Training Epoch: 8/10, step 501/574 completed (loss: 0.0420692004263401, acc: 1.0)
[2024-11-13 07:03:20,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:20,664][root][INFO] - Training Epoch: 8/10, step 502/574 completed (loss: 0.043956104665994644, acc: 1.0)
[2024-11-13 07:03:20,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:21,121][root][INFO] - Training Epoch: 8/10, step 503/574 completed (loss: 0.023443499580025673, acc: 1.0)
[2024-11-13 07:03:21,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:21,495][root][INFO] - Training Epoch: 8/10, step 504/574 completed (loss: 0.0043625738471746445, acc: 1.0)
[2024-11-13 07:03:21,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:21,917][root][INFO] - Training Epoch: 8/10, step 505/574 completed (loss: 0.05387881398200989, acc: 1.0)
[2024-11-13 07:03:22,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:22,325][root][INFO] - Training Epoch: 8/10, step 506/574 completed (loss: 0.30973899364471436, acc: 0.8965517282485962)
[2024-11-13 07:03:22,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:23,222][root][INFO] - Training Epoch: 8/10, step 507/574 completed (loss: 0.5605644583702087, acc: 0.8288288116455078)
[2024-11-13 07:03:23,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:23,854][root][INFO] - Training Epoch: 8/10, step 508/574 completed (loss: 0.2931980490684509, acc: 0.9295774698257446)
[2024-11-13 07:03:24,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:24,232][root][INFO] - Training Epoch: 8/10, step 509/574 completed (loss: 0.015234962105751038, acc: 1.0)
[2024-11-13 07:03:24,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:24,611][root][INFO] - Training Epoch: 8/10, step 510/574 completed (loss: 0.03973052278161049, acc: 1.0)
[2024-11-13 07:03:24,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:25,007][root][INFO] - Training Epoch: 8/10, step 511/574 completed (loss: 0.1981787383556366, acc: 0.9615384340286255)
[2024-11-13 07:03:27,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:28,794][root][INFO] - Training Epoch: 8/10, step 512/574 completed (loss: 0.7066028118133545, acc: 0.8285714387893677)
[2024-11-13 07:03:29,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:29,992][root][INFO] - Training Epoch: 8/10, step 513/574 completed (loss: 0.1499403864145279, acc: 0.9523809552192688)
[2024-11-13 07:03:30,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:30,387][root][INFO] - Training Epoch: 8/10, step 514/574 completed (loss: 0.18606020510196686, acc: 0.9285714030265808)
[2024-11-13 07:03:30,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:30,801][root][INFO] - Training Epoch: 8/10, step 515/574 completed (loss: 0.041580624878406525, acc: 0.9666666388511658)
[2024-11-13 07:03:31,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:31,874][root][INFO] - Training Epoch: 8/10, step 516/574 completed (loss: 0.20626288652420044, acc: 0.9166666865348816)
[2024-11-13 07:03:32,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:32,307][root][INFO] - Training Epoch: 8/10, step 517/574 completed (loss: 0.0011927953455597162, acc: 1.0)
[2024-11-13 07:03:32,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:32,671][root][INFO] - Training Epoch: 8/10, step 518/574 completed (loss: 0.058284662663936615, acc: 0.9677419066429138)
[2024-11-13 07:03:32,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:33,121][root][INFO] - Training Epoch: 8/10, step 519/574 completed (loss: 0.028154810890555382, acc: 1.0)
[2024-11-13 07:03:33,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:33,514][root][INFO] - Training Epoch: 8/10, step 520/574 completed (loss: 0.036914899945259094, acc: 1.0)
[2024-11-13 07:03:34,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:35,058][root][INFO] - Training Epoch: 8/10, step 521/574 completed (loss: 0.6101210713386536, acc: 0.8262711763381958)
[2024-11-13 07:03:35,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:35,522][root][INFO] - Training Epoch: 8/10, step 522/574 completed (loss: 0.16885140538215637, acc: 0.9477611780166626)
[2024-11-13 07:03:35,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:35,996][root][INFO] - Training Epoch: 8/10, step 523/574 completed (loss: 0.26178592443466187, acc: 0.9343065619468689)
[2024-11-13 07:03:36,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:36,830][root][INFO] - Training Epoch: 8/10, step 524/574 completed (loss: 0.4544198215007782, acc: 0.8700000047683716)
[2024-11-13 07:03:37,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:37,236][root][INFO] - Training Epoch: 8/10, step 525/574 completed (loss: 0.04881973937153816, acc: 0.9814814925193787)
[2024-11-13 07:03:37,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:37,608][root][INFO] - Training Epoch: 8/10, step 526/574 completed (loss: 0.08619681000709534, acc: 0.9615384340286255)
[2024-11-13 07:03:37,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:38,042][root][INFO] - Training Epoch: 8/10, step 527/574 completed (loss: 0.13216735422611237, acc: 0.9523809552192688)
[2024-11-13 07:03:38,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:38,472][root][INFO] - Training Epoch: 8/10, step 528/574 completed (loss: 0.2534460425376892, acc: 0.8852459192276001)
[2024-11-13 07:03:38,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:38,852][root][INFO] - Training Epoch: 8/10, step 529/574 completed (loss: 0.10895450413227081, acc: 0.9830508232116699)
[2024-11-13 07:03:39,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:39,302][root][INFO] - Training Epoch: 8/10, step 530/574 completed (loss: 0.5659162998199463, acc: 0.7906976938247681)
[2024-11-13 07:03:39,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:39,731][root][INFO] - Training Epoch: 8/10, step 531/574 completed (loss: 0.26061657071113586, acc: 0.9545454382896423)
[2024-11-13 07:03:39,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:40,145][root][INFO] - Training Epoch: 8/10, step 532/574 completed (loss: 0.36425653100013733, acc: 0.9056603908538818)
[2024-11-13 07:03:40,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:40,544][root][INFO] - Training Epoch: 8/10, step 533/574 completed (loss: 0.10900305956602097, acc: 0.9772727489471436)
[2024-11-13 07:03:40,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:40,948][root][INFO] - Training Epoch: 8/10, step 534/574 completed (loss: 0.0424056351184845, acc: 1.0)
[2024-11-13 07:03:41,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:41,309][root][INFO] - Training Epoch: 8/10, step 535/574 completed (loss: 0.050179462879896164, acc: 1.0)
[2024-11-13 07:03:41,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:41,720][root][INFO] - Training Epoch: 8/10, step 536/574 completed (loss: 0.09917575865983963, acc: 0.9545454382896423)
[2024-11-13 07:03:41,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:42,266][root][INFO] - Training Epoch: 8/10, step 537/574 completed (loss: 0.08495593070983887, acc: 0.9846153855323792)
[2024-11-13 07:03:42,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:42,675][root][INFO] - Training Epoch: 8/10, step 538/574 completed (loss: 0.21255753934383392, acc: 0.9375)
[2024-11-13 07:03:42,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:43,199][root][INFO] - Training Epoch: 8/10, step 539/574 completed (loss: 0.25106221437454224, acc: 0.90625)
[2024-11-13 07:03:43,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:43,625][root][INFO] - Training Epoch: 8/10, step 540/574 completed (loss: 0.22009481489658356, acc: 0.9090909361839294)
[2024-11-13 07:03:43,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:43,987][root][INFO] - Training Epoch: 8/10, step 541/574 completed (loss: 0.06411448121070862, acc: 0.9375)
[2024-11-13 07:03:44,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:44,417][root][INFO] - Training Epoch: 8/10, step 542/574 completed (loss: 0.05885598063468933, acc: 0.9677419066429138)
[2024-11-13 07:03:44,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:44,817][root][INFO] - Training Epoch: 8/10, step 543/574 completed (loss: 0.005595372524112463, acc: 1.0)
[2024-11-13 07:03:44,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:45,201][root][INFO] - Training Epoch: 8/10, step 544/574 completed (loss: 0.011200019158422947, acc: 1.0)
[2024-11-13 07:03:45,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:45,571][root][INFO] - Training Epoch: 8/10, step 545/574 completed (loss: 0.04577862471342087, acc: 0.9756097793579102)
[2024-11-13 07:03:45,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:45,933][root][INFO] - Training Epoch: 8/10, step 546/574 completed (loss: 0.00437765521928668, acc: 1.0)
[2024-11-13 07:03:46,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:46,279][root][INFO] - Training Epoch: 8/10, step 547/574 completed (loss: 0.010830220766365528, acc: 1.0)
[2024-11-13 07:03:46,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:46,631][root][INFO] - Training Epoch: 8/10, step 548/574 completed (loss: 0.0032707543577998877, acc: 1.0)
[2024-11-13 07:03:46,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:47,006][root][INFO] - Training Epoch: 8/10, step 549/574 completed (loss: 0.18448781967163086, acc: 0.9200000166893005)
[2024-11-13 07:03:47,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:47,345][root][INFO] - Training Epoch: 8/10, step 550/574 completed (loss: 0.14676064252853394, acc: 0.9696969985961914)
[2024-11-13 07:03:47,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:47,693][root][INFO] - Training Epoch: 8/10, step 551/574 completed (loss: 0.004955942742526531, acc: 1.0)
[2024-11-13 07:03:47,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:48,116][root][INFO] - Training Epoch: 8/10, step 552/574 completed (loss: 0.05607691407203674, acc: 0.9857142567634583)
[2024-11-13 07:03:48,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:48,511][root][INFO] - Training Epoch: 8/10, step 553/574 completed (loss: 0.2822657525539398, acc: 0.9197080135345459)
[2024-11-13 07:03:48,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:48,878][root][INFO] - Training Epoch: 8/10, step 554/574 completed (loss: 0.06116560101509094, acc: 0.9793103337287903)
[2024-11-13 07:03:49,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:49,252][root][INFO] - Training Epoch: 8/10, step 555/574 completed (loss: 0.16622181236743927, acc: 0.9428571462631226)
[2024-11-13 07:03:49,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:49,637][root][INFO] - Training Epoch: 8/10, step 556/574 completed (loss: 0.1779291331768036, acc: 0.940397322177887)
[2024-11-13 07:03:49,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:50,007][root][INFO] - Training Epoch: 8/10, step 557/574 completed (loss: 0.08857837319374084, acc: 0.9572649598121643)
[2024-11-13 07:03:50,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:51,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:51,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:52,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:52,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:53,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:53,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:54,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:54,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:55,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:55,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:56,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:56,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:57,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:58,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:58,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:59,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:59,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:03:59,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:00,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:00,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:01,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:01,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:02,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:02,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:03,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:03,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:04,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:04,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:05,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:05,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:06,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:06,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:07,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:07,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:08,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:08,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:09,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:09,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:09,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:10,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:10,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:11,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:11,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:12,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:12,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:13,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:13,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:14,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:14,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:15,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:15,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:16,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:16,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:17,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:17,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:17,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:18,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:18,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:19,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:19,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:20,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:21,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:21,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:22,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:22,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:23,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:23,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:24,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:24,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:25,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:25,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:26,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:26,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:27,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:27,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:28,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:28,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:29,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:29,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:30,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:30,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:31,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:31,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:32,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:32,673][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5991, device='cuda:0') eval_epoch_loss=tensor(0.9552, device='cuda:0') eval_epoch_acc=tensor(0.8100, device='cuda:0')
[2024-11-13 07:04:32,675][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 07:04:32,675][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 07:04:33,087][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_8_step_558_loss_0.955180823802948/model.pt
[2024-11-13 07:04:33,092][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 07:04:33,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:33,471][root][INFO] - Training Epoch: 8/10, step 558/574 completed (loss: 0.004229116719216108, acc: 1.0)
[2024-11-13 07:04:33,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:33,873][root][INFO] - Training Epoch: 8/10, step 559/574 completed (loss: 0.0910242423415184, acc: 0.9230769276618958)
[2024-11-13 07:04:34,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:34,226][root][INFO] - Training Epoch: 8/10, step 560/574 completed (loss: 0.004966127220541239, acc: 1.0)
[2024-11-13 07:04:34,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:34,582][root][INFO] - Training Epoch: 8/10, step 561/574 completed (loss: 0.013799428939819336, acc: 1.0)
[2024-11-13 07:04:34,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:34,961][root][INFO] - Training Epoch: 8/10, step 562/574 completed (loss: 0.22728699445724487, acc: 0.9444444179534912)
[2024-11-13 07:04:35,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:35,329][root][INFO] - Training Epoch: 8/10, step 563/574 completed (loss: 0.1336272805929184, acc: 0.948051929473877)
[2024-11-13 07:04:35,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:35,726][root][INFO] - Training Epoch: 8/10, step 564/574 completed (loss: 0.1906111240386963, acc: 0.9375)
[2024-11-13 07:04:35,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:36,098][root][INFO] - Training Epoch: 8/10, step 565/574 completed (loss: 0.043057601898908615, acc: 1.0)
[2024-11-13 07:04:36,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:36,449][root][INFO] - Training Epoch: 8/10, step 566/574 completed (loss: 0.05036572739481926, acc: 0.976190447807312)
[2024-11-13 07:04:36,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:36,789][root][INFO] - Training Epoch: 8/10, step 567/574 completed (loss: 0.039827823638916016, acc: 0.9736841917037964)
[2024-11-13 07:04:36,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:37,119][root][INFO] - Training Epoch: 8/10, step 568/574 completed (loss: 0.01564067415893078, acc: 1.0)
[2024-11-13 07:04:37,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:37,643][root][INFO] - Training Epoch: 8/10, step 569/574 completed (loss: 0.23703765869140625, acc: 0.9411764740943909)
[2024-11-13 07:04:37,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:38,026][root][INFO] - Training Epoch: 8/10, step 570/574 completed (loss: 0.02843417041003704, acc: 0.9838709831237793)
[2024-11-13 07:04:38,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:38,383][root][INFO] - Training Epoch: 8/10, step 571/574 completed (loss: 0.13778796792030334, acc: 0.9743589758872986)
[2024-11-13 07:04:38,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:38,755][root][INFO] - Training Epoch: 8/10, step 572/574 completed (loss: 0.2503494322299957, acc: 0.9030612111091614)
[2024-11-13 07:04:38,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:39,173][root][INFO] - Training Epoch: 8/10, step 573/574 completed (loss: 0.24231575429439545, acc: 0.9245283007621765)
[2024-11-13 07:04:39,724][slam_llm.utils.train_utils][INFO] - Epoch 8: train_perplexity=1.2180, train_epoch_loss=0.1972, epoch time 465.54989057406783s
[2024-11-13 07:04:39,724][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2024-11-13 07:04:39,724][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 16 GB
[2024-11-13 07:04:39,724][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2024-11-13 07:04:39,725][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 24
[2024-11-13 07:04:39,725][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-13 07:04:40,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:40,828][root][INFO] - Training Epoch: 9/10, step 0/574 completed (loss: 0.007228675298392773, acc: 1.0)
[2024-11-13 07:04:40,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:41,204][root][INFO] - Training Epoch: 9/10, step 1/574 completed (loss: 0.27388811111450195, acc: 0.8799999952316284)
[2024-11-13 07:04:41,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:41,597][root][INFO] - Training Epoch: 9/10, step 2/574 completed (loss: 0.1851198673248291, acc: 0.9189189076423645)
[2024-11-13 07:04:41,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:42,026][root][INFO] - Training Epoch: 9/10, step 3/574 completed (loss: 0.0908154845237732, acc: 0.9736841917037964)
[2024-11-13 07:04:42,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:42,430][root][INFO] - Training Epoch: 9/10, step 4/574 completed (loss: 0.10562431067228317, acc: 0.9459459185600281)
[2024-11-13 07:04:42,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:42,813][root][INFO] - Training Epoch: 9/10, step 5/574 completed (loss: 0.12196134030818939, acc: 0.9642857313156128)
[2024-11-13 07:04:42,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:43,194][root][INFO] - Training Epoch: 9/10, step 6/574 completed (loss: 0.10140249878168106, acc: 0.9591836929321289)
[2024-11-13 07:04:43,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:43,549][root][INFO] - Training Epoch: 9/10, step 7/574 completed (loss: 0.04149828851222992, acc: 1.0)
[2024-11-13 07:04:43,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:43,923][root][INFO] - Training Epoch: 9/10, step 8/574 completed (loss: 0.0207012128084898, acc: 1.0)
[2024-11-13 07:04:44,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:44,277][root][INFO] - Training Epoch: 9/10, step 9/574 completed (loss: 0.006262754090130329, acc: 1.0)
[2024-11-13 07:04:44,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:44,652][root][INFO] - Training Epoch: 9/10, step 10/574 completed (loss: 0.07598604261875153, acc: 0.9629629850387573)
[2024-11-13 07:04:44,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:45,010][root][INFO] - Training Epoch: 9/10, step 11/574 completed (loss: 0.08949083834886551, acc: 0.9743589758872986)
[2024-11-13 07:04:45,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:45,374][root][INFO] - Training Epoch: 9/10, step 12/574 completed (loss: 0.08518026769161224, acc: 0.9696969985961914)
[2024-11-13 07:04:45,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:45,806][root][INFO] - Training Epoch: 9/10, step 13/574 completed (loss: 0.0867512971162796, acc: 0.95652174949646)
[2024-11-13 07:04:45,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:46,261][root][INFO] - Training Epoch: 9/10, step 14/574 completed (loss: 0.09876859933137894, acc: 0.9803921580314636)
[2024-11-13 07:04:46,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:46,701][root][INFO] - Training Epoch: 9/10, step 15/574 completed (loss: 0.04430842027068138, acc: 0.9795918464660645)
[2024-11-13 07:04:46,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:47,068][root][INFO] - Training Epoch: 9/10, step 16/574 completed (loss: 0.01404030341655016, acc: 1.0)
[2024-11-13 07:04:47,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:47,423][root][INFO] - Training Epoch: 9/10, step 17/574 completed (loss: 0.005648658145219088, acc: 1.0)
[2024-11-13 07:04:47,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:47,831][root][INFO] - Training Epoch: 9/10, step 18/574 completed (loss: 0.11246183514595032, acc: 0.9722222089767456)
[2024-11-13 07:04:47,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:48,204][root][INFO] - Training Epoch: 9/10, step 19/574 completed (loss: 0.18103012442588806, acc: 0.9473684430122375)
[2024-11-13 07:04:48,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:48,616][root][INFO] - Training Epoch: 9/10, step 20/574 completed (loss: 0.12302816659212112, acc: 0.9230769276618958)
[2024-11-13 07:04:48,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:49,025][root][INFO] - Training Epoch: 9/10, step 21/574 completed (loss: 0.052803169935941696, acc: 0.9655172228813171)
[2024-11-13 07:04:49,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:49,386][root][INFO] - Training Epoch: 9/10, step 22/574 completed (loss: 0.05332997813820839, acc: 0.9599999785423279)
[2024-11-13 07:04:49,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:49,811][root][INFO] - Training Epoch: 9/10, step 23/574 completed (loss: 0.01898599974811077, acc: 1.0)
[2024-11-13 07:04:49,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:50,207][root][INFO] - Training Epoch: 9/10, step 24/574 completed (loss: 0.20080681145191193, acc: 0.9375)
[2024-11-13 07:04:50,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:50,587][root][INFO] - Training Epoch: 9/10, step 25/574 completed (loss: 0.2816447913646698, acc: 0.8867924809455872)
[2024-11-13 07:04:50,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:50,989][root][INFO] - Training Epoch: 9/10, step 26/574 completed (loss: 0.27259689569473267, acc: 0.9178082346916199)
[2024-11-13 07:04:52,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:52,799][root][INFO] - Training Epoch: 9/10, step 27/574 completed (loss: 0.8041410446166992, acc: 0.8023715615272522)
[2024-11-13 07:04:52,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:53,142][root][INFO] - Training Epoch: 9/10, step 28/574 completed (loss: 0.10655955225229263, acc: 0.9534883499145508)
[2024-11-13 07:04:53,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:53,532][root][INFO] - Training Epoch: 9/10, step 29/574 completed (loss: 0.31857094168663025, acc: 0.9036144614219666)
[2024-11-13 07:04:53,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:53,950][root][INFO] - Training Epoch: 9/10, step 30/574 completed (loss: 0.25769704580307007, acc: 0.9135802388191223)
[2024-11-13 07:04:54,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:54,356][root][INFO] - Training Epoch: 9/10, step 31/574 completed (loss: 0.048108987510204315, acc: 1.0)
[2024-11-13 07:04:54,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:54,718][root][INFO] - Training Epoch: 9/10, step 32/574 completed (loss: 0.45862579345703125, acc: 0.9629629850387573)
[2024-11-13 07:04:54,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:55,088][root][INFO] - Training Epoch: 9/10, step 33/574 completed (loss: 0.09150635451078415, acc: 0.95652174949646)
[2024-11-13 07:04:55,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:55,483][root][INFO] - Training Epoch: 9/10, step 34/574 completed (loss: 0.21683858335018158, acc: 0.924369752407074)
[2024-11-13 07:04:55,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:55,962][root][INFO] - Training Epoch: 9/10, step 35/574 completed (loss: 0.09048531204462051, acc: 0.9836065769195557)
[2024-11-13 07:04:56,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:56,379][root][INFO] - Training Epoch: 9/10, step 36/574 completed (loss: 0.3279593288898468, acc: 0.920634925365448)
[2024-11-13 07:04:56,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:56,750][root][INFO] - Training Epoch: 9/10, step 37/574 completed (loss: 0.1491508036851883, acc: 0.9491525292396545)
[2024-11-13 07:04:56,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:57,159][root][INFO] - Training Epoch: 9/10, step 38/574 completed (loss: 0.09627332538366318, acc: 0.977011501789093)
[2024-11-13 07:04:57,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:57,527][root][INFO] - Training Epoch: 9/10, step 39/574 completed (loss: 0.2109925001859665, acc: 0.9523809552192688)
[2024-11-13 07:04:57,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:57,908][root][INFO] - Training Epoch: 9/10, step 40/574 completed (loss: 0.09776034951210022, acc: 0.9615384340286255)
[2024-11-13 07:04:58,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:58,354][root][INFO] - Training Epoch: 9/10, step 41/574 completed (loss: 0.11160159856081009, acc: 0.9594594836235046)
[2024-11-13 07:04:58,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:58,800][root][INFO] - Training Epoch: 9/10, step 42/574 completed (loss: 0.3109733760356903, acc: 0.9230769276618958)
[2024-11-13 07:04:59,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:04:59,428][root][INFO] - Training Epoch: 9/10, step 43/574 completed (loss: 0.28617939352989197, acc: 0.9292929172515869)
[2024-11-13 07:04:59,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:00,003][root][INFO] - Training Epoch: 9/10, step 44/574 completed (loss: 0.23587298393249512, acc: 0.9278350472450256)
[2024-11-13 07:05:00,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:00,553][root][INFO] - Training Epoch: 9/10, step 45/574 completed (loss: 0.18462109565734863, acc: 0.9338235259056091)
[2024-11-13 07:05:00,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:00,955][root][INFO] - Training Epoch: 9/10, step 46/574 completed (loss: 0.05449674651026726, acc: 1.0)
[2024-11-13 07:05:01,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:01,356][root][INFO] - Training Epoch: 9/10, step 47/574 completed (loss: 0.0037830020301043987, acc: 1.0)
[2024-11-13 07:05:01,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:01,746][root][INFO] - Training Epoch: 9/10, step 48/574 completed (loss: 0.13051357865333557, acc: 0.9642857313156128)
[2024-11-13 07:05:01,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:02,145][root][INFO] - Training Epoch: 9/10, step 49/574 completed (loss: 0.005007789935916662, acc: 1.0)
[2024-11-13 07:05:02,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:02,526][root][INFO] - Training Epoch: 9/10, step 50/574 completed (loss: 0.2549695670604706, acc: 0.9122806787490845)
[2024-11-13 07:05:02,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:02,929][root][INFO] - Training Epoch: 9/10, step 51/574 completed (loss: 0.182025745511055, acc: 0.9523809552192688)
[2024-11-13 07:05:03,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:03,313][root][INFO] - Training Epoch: 9/10, step 52/574 completed (loss: 0.3664206266403198, acc: 0.8591549396514893)
[2024-11-13 07:05:03,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:03,951][root][INFO] - Training Epoch: 9/10, step 53/574 completed (loss: 0.6974859833717346, acc: 0.7933333516120911)
[2024-11-13 07:05:04,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:04,307][root][INFO] - Training Epoch: 9/10, step 54/574 completed (loss: 0.07593762129545212, acc: 0.9729729890823364)
[2024-11-13 07:05:04,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:04,670][root][INFO] - Training Epoch: 9/10, step 55/574 completed (loss: 0.13179031014442444, acc: 0.9615384340286255)
[2024-11-13 07:05:07,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:09,172][root][INFO] - Training Epoch: 9/10, step 56/574 completed (loss: 0.7311072945594788, acc: 0.7781569957733154)
[2024-11-13 07:05:10,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:11,021][root][INFO] - Training Epoch: 9/10, step 57/574 completed (loss: 0.961952805519104, acc: 0.7211328744888306)
[2024-11-13 07:05:11,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:11,952][root][INFO] - Training Epoch: 9/10, step 58/574 completed (loss: 0.5028669834136963, acc: 0.8409090638160706)
[2024-11-13 07:05:12,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:12,816][root][INFO] - Training Epoch: 9/10, step 59/574 completed (loss: 0.16454164683818817, acc: 0.9632353186607361)
[2024-11-13 07:05:13,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:13,661][root][INFO] - Training Epoch: 9/10, step 60/574 completed (loss: 0.45841798186302185, acc: 0.8768116235733032)
[2024-11-13 07:05:13,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:14,233][root][INFO] - Training Epoch: 9/10, step 61/574 completed (loss: 0.18892544507980347, acc: 0.925000011920929)
[2024-11-13 07:05:14,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:14,590][root][INFO] - Training Epoch: 9/10, step 62/574 completed (loss: 0.16435115039348602, acc: 0.970588207244873)
[2024-11-13 07:05:14,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:14,976][root][INFO] - Training Epoch: 9/10, step 63/574 completed (loss: 0.07662815600633621, acc: 0.9444444179534912)
[2024-11-13 07:05:15,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:15,437][root][INFO] - Training Epoch: 9/10, step 64/574 completed (loss: 0.043934520334005356, acc: 0.984375)
[2024-11-13 07:05:15,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:15,906][root][INFO] - Training Epoch: 9/10, step 65/574 completed (loss: 0.04435013607144356, acc: 0.9655172228813171)
[2024-11-13 07:05:16,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:16,333][root][INFO] - Training Epoch: 9/10, step 66/574 completed (loss: 0.2570171356201172, acc: 0.9285714030265808)
[2024-11-13 07:05:16,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:16,760][root][INFO] - Training Epoch: 9/10, step 67/574 completed (loss: 0.08213035017251968, acc: 0.9833333492279053)
[2024-11-13 07:05:16,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:17,138][root][INFO] - Training Epoch: 9/10, step 68/574 completed (loss: 0.006183993071317673, acc: 1.0)
[2024-11-13 07:05:17,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:17,591][root][INFO] - Training Epoch: 9/10, step 69/574 completed (loss: 0.19799202680587769, acc: 0.9166666865348816)
[2024-11-13 07:05:17,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:17,997][root][INFO] - Training Epoch: 9/10, step 70/574 completed (loss: 0.18581710755825043, acc: 0.9090909361839294)
[2024-11-13 07:05:18,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:18,432][root][INFO] - Training Epoch: 9/10, step 71/574 completed (loss: 0.4927237033843994, acc: 0.8676470518112183)
[2024-11-13 07:05:18,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:18,825][root][INFO] - Training Epoch: 9/10, step 72/574 completed (loss: 0.39551806449890137, acc: 0.8730158805847168)
[2024-11-13 07:05:18,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:19,231][root][INFO] - Training Epoch: 9/10, step 73/574 completed (loss: 0.8621236681938171, acc: 0.7333333492279053)
[2024-11-13 07:05:19,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:19,606][root][INFO] - Training Epoch: 9/10, step 74/574 completed (loss: 0.4716818630695343, acc: 0.8367347121238708)
[2024-11-13 07:05:19,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:19,995][root][INFO] - Training Epoch: 9/10, step 75/574 completed (loss: 0.6022590398788452, acc: 0.8358209133148193)
[2024-11-13 07:05:20,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:20,500][root][INFO] - Training Epoch: 9/10, step 76/574 completed (loss: 1.1653335094451904, acc: 0.6751824617385864)
[2024-11-13 07:05:20,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:20,914][root][INFO] - Training Epoch: 9/10, step 77/574 completed (loss: 0.013274431228637695, acc: 1.0)
[2024-11-13 07:05:21,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:21,334][root][INFO] - Training Epoch: 9/10, step 78/574 completed (loss: 0.002980055520310998, acc: 1.0)
[2024-11-13 07:05:21,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:21,768][root][INFO] - Training Epoch: 9/10, step 79/574 completed (loss: 0.032904792577028275, acc: 1.0)
[2024-11-13 07:05:21,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:22,177][root][INFO] - Training Epoch: 9/10, step 80/574 completed (loss: 0.008213440887629986, acc: 1.0)
[2024-11-13 07:05:22,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:22,587][root][INFO] - Training Epoch: 9/10, step 81/574 completed (loss: 0.1062394231557846, acc: 0.9615384340286255)
[2024-11-13 07:05:22,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:22,988][root][INFO] - Training Epoch: 9/10, step 82/574 completed (loss: 0.21090342104434967, acc: 0.9230769276618958)
[2024-11-13 07:05:23,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:23,372][root][INFO] - Training Epoch: 9/10, step 83/574 completed (loss: 0.04692382365465164, acc: 1.0)
[2024-11-13 07:05:23,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:23,766][root][INFO] - Training Epoch: 9/10, step 84/574 completed (loss: 0.12651541829109192, acc: 0.95652174949646)
[2024-11-13 07:05:23,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:24,208][root][INFO] - Training Epoch: 9/10, step 85/574 completed (loss: 0.04311224818229675, acc: 1.0)
[2024-11-13 07:05:24,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:24,532][root][INFO] - Training Epoch: 9/10, step 86/574 completed (loss: 0.009507289156317711, acc: 1.0)
[2024-11-13 07:05:24,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:25,192][root][INFO] - Training Epoch: 9/10, step 87/574 completed (loss: 0.18978846073150635, acc: 0.9399999976158142)
[2024-11-13 07:05:25,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:25,618][root][INFO] - Training Epoch: 9/10, step 88/574 completed (loss: 0.4323277473449707, acc: 0.8640776872634888)
[2024-11-13 07:05:26,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:27,348][root][INFO] - Training Epoch: 9/10, step 89/574 completed (loss: 0.5891314148902893, acc: 0.8398058414459229)
[2024-11-13 07:05:28,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:28,624][root][INFO] - Training Epoch: 9/10, step 90/574 completed (loss: 0.7114611268043518, acc: 0.801075279712677)
[2024-11-13 07:05:29,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:29,877][root][INFO] - Training Epoch: 9/10, step 91/574 completed (loss: 0.4473743736743927, acc: 0.8793103694915771)
[2024-11-13 07:05:30,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:31,031][root][INFO] - Training Epoch: 9/10, step 92/574 completed (loss: 0.3664378225803375, acc: 0.8842105269432068)
[2024-11-13 07:05:31,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:32,623][root][INFO] - Training Epoch: 9/10, step 93/574 completed (loss: 0.5127025246620178, acc: 0.8415841460227966)
[2024-11-13 07:05:32,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:33,069][root][INFO] - Training Epoch: 9/10, step 94/574 completed (loss: 0.16328047215938568, acc: 0.9838709831237793)
[2024-11-13 07:05:33,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:33,546][root][INFO] - Training Epoch: 9/10, step 95/574 completed (loss: 0.2894044816493988, acc: 0.8840579986572266)
[2024-11-13 07:05:33,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:33,955][root][INFO] - Training Epoch: 9/10, step 96/574 completed (loss: 0.526911199092865, acc: 0.7899159789085388)
[2024-11-13 07:05:34,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:34,416][root][INFO] - Training Epoch: 9/10, step 97/574 completed (loss: 0.34519654512405396, acc: 0.8846153616905212)
[2024-11-13 07:05:34,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:34,916][root][INFO] - Training Epoch: 9/10, step 98/574 completed (loss: 0.5004110336303711, acc: 0.8394160866737366)
[2024-11-13 07:05:35,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:35,290][root][INFO] - Training Epoch: 9/10, step 99/574 completed (loss: 0.3866349458694458, acc: 0.9104477763175964)
[2024-11-13 07:05:35,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:35,707][root][INFO] - Training Epoch: 9/10, step 100/574 completed (loss: 0.10375352948904037, acc: 0.949999988079071)
[2024-11-13 07:05:35,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:36,105][root][INFO] - Training Epoch: 9/10, step 101/574 completed (loss: 0.003774424782022834, acc: 1.0)
[2024-11-13 07:05:36,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:36,512][root][INFO] - Training Epoch: 9/10, step 102/574 completed (loss: 0.14473699033260345, acc: 0.95652174949646)
[2024-11-13 07:05:36,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:36,930][root][INFO] - Training Epoch: 9/10, step 103/574 completed (loss: 0.005218212492763996, acc: 1.0)
[2024-11-13 07:05:37,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:37,361][root][INFO] - Training Epoch: 9/10, step 104/574 completed (loss: 0.0865185335278511, acc: 0.9655172228813171)
[2024-11-13 07:05:37,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:37,780][root][INFO] - Training Epoch: 9/10, step 105/574 completed (loss: 0.011368161998689175, acc: 1.0)
[2024-11-13 07:05:37,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:38,210][root][INFO] - Training Epoch: 9/10, step 106/574 completed (loss: 0.0213583093136549, acc: 1.0)
[2024-11-13 07:05:38,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:38,600][root][INFO] - Training Epoch: 9/10, step 107/574 completed (loss: 0.013223608955740929, acc: 1.0)
[2024-11-13 07:05:38,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:39,004][root][INFO] - Training Epoch: 9/10, step 108/574 completed (loss: 0.0016140355728566647, acc: 1.0)
[2024-11-13 07:05:39,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:39,338][root][INFO] - Training Epoch: 9/10, step 109/574 completed (loss: 0.16185399889945984, acc: 0.976190447807312)
[2024-11-13 07:05:39,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:39,727][root][INFO] - Training Epoch: 9/10, step 110/574 completed (loss: 0.05300310254096985, acc: 0.9846153855323792)
[2024-11-13 07:05:39,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:40,273][root][INFO] - Training Epoch: 9/10, step 111/574 completed (loss: 0.07610612362623215, acc: 0.9824561476707458)
[2024-11-13 07:05:40,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:40,693][root][INFO] - Training Epoch: 9/10, step 112/574 completed (loss: 0.2993723750114441, acc: 0.9298245906829834)
[2024-11-13 07:05:40,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:41,121][root][INFO] - Training Epoch: 9/10, step 113/574 completed (loss: 0.07514020800590515, acc: 1.0)
[2024-11-13 07:05:41,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:41,598][root][INFO] - Training Epoch: 9/10, step 114/574 completed (loss: 0.24851487576961517, acc: 0.9387755393981934)
[2024-11-13 07:05:41,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:41,962][root][INFO] - Training Epoch: 9/10, step 115/574 completed (loss: 0.0012835345696657896, acc: 1.0)
[2024-11-13 07:05:42,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:42,412][root][INFO] - Training Epoch: 9/10, step 116/574 completed (loss: 0.16498534381389618, acc: 0.9365079402923584)
[2024-11-13 07:05:42,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:42,856][root][INFO] - Training Epoch: 9/10, step 117/574 completed (loss: 0.13793711364269257, acc: 0.9512194991111755)
[2024-11-13 07:05:43,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:43,312][root][INFO] - Training Epoch: 9/10, step 118/574 completed (loss: 0.09874971956014633, acc: 0.9516128897666931)
[2024-11-13 07:05:43,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:44,627][root][INFO] - Training Epoch: 9/10, step 119/574 completed (loss: 0.4439505934715271, acc: 0.874524712562561)
[2024-11-13 07:05:44,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:45,057][root][INFO] - Training Epoch: 9/10, step 120/574 completed (loss: 0.026156730949878693, acc: 1.0)
[2024-11-13 07:05:45,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:45,582][root][INFO] - Training Epoch: 9/10, step 121/574 completed (loss: 0.4175068736076355, acc: 0.9230769276618958)
[2024-11-13 07:05:45,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:46,005][root][INFO] - Training Epoch: 9/10, step 122/574 completed (loss: 0.007809691596776247, acc: 1.0)
[2024-11-13 07:05:46,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:46,459][root][INFO] - Training Epoch: 9/10, step 123/574 completed (loss: 0.018533997237682343, acc: 1.0)
[2024-11-13 07:05:46,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:46,989][root][INFO] - Training Epoch: 9/10, step 124/574 completed (loss: 0.4251633584499359, acc: 0.8895705342292786)
[2024-11-13 07:05:47,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:47,435][root][INFO] - Training Epoch: 9/10, step 125/574 completed (loss: 0.49409955739974976, acc: 0.8333333134651184)
[2024-11-13 07:05:47,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:47,827][root][INFO] - Training Epoch: 9/10, step 126/574 completed (loss: 0.3983438313007355, acc: 0.9083333611488342)
[2024-11-13 07:05:48,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:49,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:49,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:50,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:50,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:51,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:51,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:52,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:52,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:53,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:53,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:54,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:54,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:55,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:55,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:56,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:56,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:57,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:57,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:58,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:59,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:05:59,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:00,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:00,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:01,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:01,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:02,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:02,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:03,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:03,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:04,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:04,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:05,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:05,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:05,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:06,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:07,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:07,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:08,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:08,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:09,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:09,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:10,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:10,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:11,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:11,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:12,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:12,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:13,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:13,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:14,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:14,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:15,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:15,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:16,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:16,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:17,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:17,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:18,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:18,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:19,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:19,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:20,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:21,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:21,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:22,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:22,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:23,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:23,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:24,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:25,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:25,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:26,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:26,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:27,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:27,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:28,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:28,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:28,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:29,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:29,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:30,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:30,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:31,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:31,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:32,407][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3011, device='cuda:0') eval_epoch_loss=tensor(0.8334, device='cuda:0') eval_epoch_acc=tensor(0.8204, device='cuda:0')
[2024-11-13 07:06:32,408][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 07:06:32,408][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 07:06:33,066][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_9_step_127_loss_0.8334057331085205/model.pt
[2024-11-13 07:06:33,072][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 07:06:33,073][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 9 is 0.8203712105751038
[2024-11-13 07:06:33,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:33,534][root][INFO] - Training Epoch: 9/10, step 127/574 completed (loss: 0.44846221804618835, acc: 0.875)
[2024-11-13 07:06:33,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:33,949][root][INFO] - Training Epoch: 9/10, step 128/574 completed (loss: 0.39541903138160706, acc: 0.8717948794364929)
[2024-11-13 07:06:34,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:34,510][root][INFO] - Training Epoch: 9/10, step 129/574 completed (loss: 0.5444609522819519, acc: 0.8161764740943909)
[2024-11-13 07:06:34,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:34,957][root][INFO] - Training Epoch: 9/10, step 130/574 completed (loss: 0.14178745448589325, acc: 0.9615384340286255)
[2024-11-13 07:06:35,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:35,386][root][INFO] - Training Epoch: 9/10, step 131/574 completed (loss: 0.14260755479335785, acc: 0.95652174949646)
[2024-11-13 07:06:35,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:35,816][root][INFO] - Training Epoch: 9/10, step 132/574 completed (loss: 0.07836536318063736, acc: 0.96875)
[2024-11-13 07:06:35,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:36,196][root][INFO] - Training Epoch: 9/10, step 133/574 completed (loss: 0.029074450954794884, acc: 1.0)
[2024-11-13 07:06:36,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:36,552][root][INFO] - Training Epoch: 9/10, step 134/574 completed (loss: 0.10015952587127686, acc: 0.9428571462631226)
[2024-11-13 07:06:36,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:36,923][root][INFO] - Training Epoch: 9/10, step 135/574 completed (loss: 0.09575842320919037, acc: 0.9615384340286255)
[2024-11-13 07:06:37,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:37,318][root][INFO] - Training Epoch: 9/10, step 136/574 completed (loss: 0.05290135741233826, acc: 0.976190447807312)
[2024-11-13 07:06:37,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:37,666][root][INFO] - Training Epoch: 9/10, step 137/574 completed (loss: 0.1412035971879959, acc: 1.0)
[2024-11-13 07:06:37,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:38,034][root][INFO] - Training Epoch: 9/10, step 138/574 completed (loss: 0.2542944848537445, acc: 0.95652174949646)
[2024-11-13 07:06:38,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:38,430][root][INFO] - Training Epoch: 9/10, step 139/574 completed (loss: 0.02136215567588806, acc: 1.0)
[2024-11-13 07:06:38,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:38,823][root][INFO] - Training Epoch: 9/10, step 140/574 completed (loss: 0.0945485308766365, acc: 0.9615384340286255)
[2024-11-13 07:06:39,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:39,283][root][INFO] - Training Epoch: 9/10, step 141/574 completed (loss: 0.12246393412351608, acc: 0.9677419066429138)
[2024-11-13 07:06:39,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:39,662][root][INFO] - Training Epoch: 9/10, step 142/574 completed (loss: 0.11174353957176208, acc: 0.9459459185600281)
[2024-11-13 07:06:40,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:40,456][root][INFO] - Training Epoch: 9/10, step 143/574 completed (loss: 0.2544715106487274, acc: 0.9210526347160339)
[2024-11-13 07:06:40,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:40,962][root][INFO] - Training Epoch: 9/10, step 144/574 completed (loss: 0.3796272575855255, acc: 0.8731343150138855)
[2024-11-13 07:06:41,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:41,415][root][INFO] - Training Epoch: 9/10, step 145/574 completed (loss: 0.21292686462402344, acc: 0.9387755393981934)
[2024-11-13 07:06:41,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:42,053][root][INFO] - Training Epoch: 9/10, step 146/574 completed (loss: 0.3191992938518524, acc: 0.8936170339584351)
[2024-11-13 07:06:42,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:42,431][root][INFO] - Training Epoch: 9/10, step 147/574 completed (loss: 0.18315301835536957, acc: 0.9428571462631226)
[2024-11-13 07:06:42,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:42,767][root][INFO] - Training Epoch: 9/10, step 148/574 completed (loss: 0.15803268551826477, acc: 0.9285714030265808)
[2024-11-13 07:06:42,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:43,141][root][INFO] - Training Epoch: 9/10, step 149/574 completed (loss: 0.05106749385595322, acc: 1.0)
[2024-11-13 07:06:43,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:43,587][root][INFO] - Training Epoch: 9/10, step 150/574 completed (loss: 0.09158925712108612, acc: 0.9655172228813171)
[2024-11-13 07:06:43,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:44,040][root][INFO] - Training Epoch: 9/10, step 151/574 completed (loss: 0.15687210857868195, acc: 0.9130434989929199)
[2024-11-13 07:06:44,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:44,470][root][INFO] - Training Epoch: 9/10, step 152/574 completed (loss: 0.11068807542324066, acc: 0.9661017060279846)
[2024-11-13 07:06:44,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:44,929][root][INFO] - Training Epoch: 9/10, step 153/574 completed (loss: 0.0798962265253067, acc: 0.9824561476707458)
[2024-11-13 07:06:45,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:45,454][root][INFO] - Training Epoch: 9/10, step 154/574 completed (loss: 0.1288212686777115, acc: 0.9594594836235046)
[2024-11-13 07:06:45,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:45,849][root][INFO] - Training Epoch: 9/10, step 155/574 completed (loss: 0.051281582564115524, acc: 0.9642857313156128)
[2024-11-13 07:06:46,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:46,239][root][INFO] - Training Epoch: 9/10, step 156/574 completed (loss: 0.2109287977218628, acc: 0.95652174949646)
[2024-11-13 07:06:46,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:46,642][root][INFO] - Training Epoch: 9/10, step 157/574 completed (loss: 0.4023533761501312, acc: 0.8947368264198303)
[2024-11-13 07:06:48,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:49,241][root][INFO] - Training Epoch: 9/10, step 158/574 completed (loss: 0.2758028507232666, acc: 0.8918918967247009)
[2024-11-13 07:06:49,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:49,655][root][INFO] - Training Epoch: 9/10, step 159/574 completed (loss: 0.37670931220054626, acc: 0.9074074029922485)
[2024-11-13 07:06:49,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:50,223][root][INFO] - Training Epoch: 9/10, step 160/574 completed (loss: 0.45736539363861084, acc: 0.8604651093482971)
[2024-11-13 07:06:50,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:51,142][root][INFO] - Training Epoch: 9/10, step 161/574 completed (loss: 0.351430207490921, acc: 0.9058823585510254)
[2024-11-13 07:06:51,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:51,988][root][INFO] - Training Epoch: 9/10, step 162/574 completed (loss: 0.5253238677978516, acc: 0.8876404762268066)
[2024-11-13 07:06:52,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:52,366][root][INFO] - Training Epoch: 9/10, step 163/574 completed (loss: 0.13411864638328552, acc: 0.9545454382896423)
[2024-11-13 07:06:52,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:52,725][root][INFO] - Training Epoch: 9/10, step 164/574 completed (loss: 0.15766774117946625, acc: 0.9523809552192688)
[2024-11-13 07:06:52,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:53,154][root][INFO] - Training Epoch: 9/10, step 165/574 completed (loss: 0.23836733400821686, acc: 0.9655172228813171)
[2024-11-13 07:06:53,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:53,617][root][INFO] - Training Epoch: 9/10, step 166/574 completed (loss: 0.03529726713895798, acc: 1.0)
[2024-11-13 07:06:53,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:53,997][root][INFO] - Training Epoch: 9/10, step 167/574 completed (loss: 0.15303117036819458, acc: 0.9399999976158142)
[2024-11-13 07:06:54,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:54,542][root][INFO] - Training Epoch: 9/10, step 168/574 completed (loss: 0.168454110622406, acc: 0.9305555820465088)
[2024-11-13 07:06:54,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:54,994][root][INFO] - Training Epoch: 9/10, step 169/574 completed (loss: 0.46352455019950867, acc: 0.8725489974021912)
[2024-11-13 07:06:55,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:56,687][root][INFO] - Training Epoch: 9/10, step 170/574 completed (loss: 0.3978685736656189, acc: 0.8630136847496033)
[2024-11-13 07:06:56,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:57,088][root][INFO] - Training Epoch: 9/10, step 171/574 completed (loss: 0.01336547452956438, acc: 1.0)
[2024-11-13 07:06:57,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:57,457][root][INFO] - Training Epoch: 9/10, step 172/574 completed (loss: 0.013720829039812088, acc: 1.0)
[2024-11-13 07:06:57,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:57,806][root][INFO] - Training Epoch: 9/10, step 173/574 completed (loss: 0.251457154750824, acc: 0.9285714030265808)
[2024-11-13 07:06:58,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:58,619][root][INFO] - Training Epoch: 9/10, step 174/574 completed (loss: 0.5289090871810913, acc: 0.8672566413879395)
[2024-11-13 07:06:58,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:59,062][root][INFO] - Training Epoch: 9/10, step 175/574 completed (loss: 0.29760000109672546, acc: 0.9130434989929199)
[2024-11-13 07:06:59,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:06:59,546][root][INFO] - Training Epoch: 9/10, step 176/574 completed (loss: 0.11691251397132874, acc: 0.9545454382896423)
[2024-11-13 07:07:00,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:01,010][root][INFO] - Training Epoch: 9/10, step 177/574 completed (loss: 0.6611849069595337, acc: 0.8244274854660034)
[2024-11-13 07:07:01,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:02,060][root][INFO] - Training Epoch: 9/10, step 178/574 completed (loss: 0.46722105145454407, acc: 0.8370370268821716)
[2024-11-13 07:07:02,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:02,497][root][INFO] - Training Epoch: 9/10, step 179/574 completed (loss: 0.0476960651576519, acc: 0.9836065769195557)
[2024-11-13 07:07:02,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:02,856][root][INFO] - Training Epoch: 9/10, step 180/574 completed (loss: 0.01286290492862463, acc: 1.0)
[2024-11-13 07:07:03,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:03,262][root][INFO] - Training Epoch: 9/10, step 181/574 completed (loss: 0.06590992212295532, acc: 0.9599999785423279)
[2024-11-13 07:07:03,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:03,634][root][INFO] - Training Epoch: 9/10, step 182/574 completed (loss: 0.027210712432861328, acc: 1.0)
[2024-11-13 07:07:03,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:04,063][root][INFO] - Training Epoch: 9/10, step 183/574 completed (loss: 0.16768532991409302, acc: 0.9390243887901306)
[2024-11-13 07:07:04,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:04,521][root][INFO] - Training Epoch: 9/10, step 184/574 completed (loss: 0.34759843349456787, acc: 0.9184290170669556)
[2024-11-13 07:07:04,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:04,917][root][INFO] - Training Epoch: 9/10, step 185/574 completed (loss: 0.5195866823196411, acc: 0.8472622632980347)
[2024-11-13 07:07:05,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:05,614][root][INFO] - Training Epoch: 9/10, step 186/574 completed (loss: 0.38831454515457153, acc: 0.890625)
[2024-11-13 07:07:05,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:06,349][root][INFO] - Training Epoch: 9/10, step 187/574 completed (loss: 0.4864911437034607, acc: 0.8630393743515015)
[2024-11-13 07:07:06,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:06,935][root][INFO] - Training Epoch: 9/10, step 188/574 completed (loss: 0.3546413481235504, acc: 0.900355875492096)
[2024-11-13 07:07:07,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:07,360][root][INFO] - Training Epoch: 9/10, step 189/574 completed (loss: 0.038745518773794174, acc: 1.0)
[2024-11-13 07:07:07,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:08,201][root][INFO] - Training Epoch: 9/10, step 190/574 completed (loss: 0.38442540168762207, acc: 0.8720930218696594)
[2024-11-13 07:07:08,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:09,477][root][INFO] - Training Epoch: 9/10, step 191/574 completed (loss: 0.6588926911354065, acc: 0.7857142686843872)
[2024-11-13 07:07:10,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:10,956][root][INFO] - Training Epoch: 9/10, step 192/574 completed (loss: 0.49887704849243164, acc: 0.8409090638160706)
[2024-11-13 07:07:11,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:12,134][root][INFO] - Training Epoch: 9/10, step 193/574 completed (loss: 0.25288423895835876, acc: 0.9058823585510254)
[2024-11-13 07:07:13,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:13,887][root][INFO] - Training Epoch: 9/10, step 194/574 completed (loss: 0.4833487570285797, acc: 0.8580247163772583)
[2024-11-13 07:07:14,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:15,432][root][INFO] - Training Epoch: 9/10, step 195/574 completed (loss: 0.0891980528831482, acc: 0.9838709831237793)
[2024-11-13 07:07:15,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:15,833][root][INFO] - Training Epoch: 9/10, step 196/574 completed (loss: 0.03740992397069931, acc: 1.0)
[2024-11-13 07:07:15,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:16,200][root][INFO] - Training Epoch: 9/10, step 197/574 completed (loss: 0.037355199456214905, acc: 1.0)
[2024-11-13 07:07:16,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:16,609][root][INFO] - Training Epoch: 9/10, step 198/574 completed (loss: 0.16950644552707672, acc: 0.9558823704719543)
[2024-11-13 07:07:16,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:17,044][root][INFO] - Training Epoch: 9/10, step 199/574 completed (loss: 0.4483950436115265, acc: 0.8602941036224365)
[2024-11-13 07:07:17,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:17,519][root][INFO] - Training Epoch: 9/10, step 200/574 completed (loss: 0.38087454438209534, acc: 0.8474576473236084)
[2024-11-13 07:07:17,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:17,963][root][INFO] - Training Epoch: 9/10, step 201/574 completed (loss: 0.3473001718521118, acc: 0.8656716346740723)
[2024-11-13 07:07:18,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:18,424][root][INFO] - Training Epoch: 9/10, step 202/574 completed (loss: 0.2819522023200989, acc: 0.9029126167297363)
[2024-11-13 07:07:18,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:18,816][root][INFO] - Training Epoch: 9/10, step 203/574 completed (loss: 0.14537720382213593, acc: 0.9841269850730896)
[2024-11-13 07:07:19,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:19,260][root][INFO] - Training Epoch: 9/10, step 204/574 completed (loss: 0.06857196986675262, acc: 0.9780219793319702)
[2024-11-13 07:07:19,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:19,722][root][INFO] - Training Epoch: 9/10, step 205/574 completed (loss: 0.18516117334365845, acc: 0.9327354431152344)
[2024-11-13 07:07:19,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:20,255][root][INFO] - Training Epoch: 9/10, step 206/574 completed (loss: 0.1787986159324646, acc: 0.9291338324546814)
[2024-11-13 07:07:20,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:20,664][root][INFO] - Training Epoch: 9/10, step 207/574 completed (loss: 0.21055160462856293, acc: 0.9396551847457886)
[2024-11-13 07:07:20,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:21,131][root][INFO] - Training Epoch: 9/10, step 208/574 completed (loss: 0.23164208233356476, acc: 0.9528985619544983)
[2024-11-13 07:07:21,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:21,614][root][INFO] - Training Epoch: 9/10, step 209/574 completed (loss: 0.20608019828796387, acc: 0.9260700345039368)
[2024-11-13 07:07:21,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:22,008][root][INFO] - Training Epoch: 9/10, step 210/574 completed (loss: 0.10341636091470718, acc: 0.967391312122345)
[2024-11-13 07:07:22,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:22,364][root][INFO] - Training Epoch: 9/10, step 211/574 completed (loss: 0.006458362098783255, acc: 1.0)
[2024-11-13 07:07:22,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:22,818][root][INFO] - Training Epoch: 9/10, step 212/574 completed (loss: 0.017892062664031982, acc: 1.0)
[2024-11-13 07:07:23,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:23,264][root][INFO] - Training Epoch: 9/10, step 213/574 completed (loss: 0.06280062347650528, acc: 0.978723406791687)
[2024-11-13 07:07:23,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:24,382][root][INFO] - Training Epoch: 9/10, step 214/574 completed (loss: 0.10232117772102356, acc: 0.9769230484962463)
[2024-11-13 07:07:24,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:24,799][root][INFO] - Training Epoch: 9/10, step 215/574 completed (loss: 0.047736264765262604, acc: 0.9864864945411682)
[2024-11-13 07:07:24,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:25,201][root][INFO] - Training Epoch: 9/10, step 216/574 completed (loss: 0.012485158629715443, acc: 1.0)
[2024-11-13 07:07:25,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:25,998][root][INFO] - Training Epoch: 9/10, step 217/574 completed (loss: 0.09593798965215683, acc: 0.9729729890823364)
[2024-11-13 07:07:26,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:26,533][root][INFO] - Training Epoch: 9/10, step 218/574 completed (loss: 0.0927298367023468, acc: 0.9555555582046509)
[2024-11-13 07:07:26,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:26,938][root][INFO] - Training Epoch: 9/10, step 219/574 completed (loss: 0.030068783089518547, acc: 1.0)
[2024-11-13 07:07:27,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:27,369][root][INFO] - Training Epoch: 9/10, step 220/574 completed (loss: 0.08039075136184692, acc: 0.9629629850387573)
[2024-11-13 07:07:27,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:27,747][root][INFO] - Training Epoch: 9/10, step 221/574 completed (loss: 0.009116786532104015, acc: 1.0)
[2024-11-13 07:07:27,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:28,123][root][INFO] - Training Epoch: 9/10, step 222/574 completed (loss: 0.122263103723526, acc: 0.942307710647583)
[2024-11-13 07:07:28,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:29,346][root][INFO] - Training Epoch: 9/10, step 223/574 completed (loss: 0.17704488337039948, acc: 0.9347826242446899)
[2024-11-13 07:07:29,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:30,154][root][INFO] - Training Epoch: 9/10, step 224/574 completed (loss: 0.3083387613296509, acc: 0.8977272510528564)
[2024-11-13 07:07:30,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:30,784][root][INFO] - Training Epoch: 9/10, step 225/574 completed (loss: 0.19844168424606323, acc: 0.9255319237709045)
[2024-11-13 07:07:30,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:31,235][root][INFO] - Training Epoch: 9/10, step 226/574 completed (loss: 0.20288580656051636, acc: 0.9245283007621765)
[2024-11-13 07:07:31,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:31,648][root][INFO] - Training Epoch: 9/10, step 227/574 completed (loss: 0.07494594156742096, acc: 0.9666666388511658)
[2024-11-13 07:07:31,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:32,089][root][INFO] - Training Epoch: 9/10, step 228/574 completed (loss: 0.06152108684182167, acc: 0.9767441749572754)
[2024-11-13 07:07:32,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:32,512][root][INFO] - Training Epoch: 9/10, step 229/574 completed (loss: 0.1326059103012085, acc: 0.9333333373069763)
[2024-11-13 07:07:32,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:32,998][root][INFO] - Training Epoch: 9/10, step 230/574 completed (loss: 0.6627752780914307, acc: 0.7789473533630371)
[2024-11-13 07:07:33,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:33,443][root][INFO] - Training Epoch: 9/10, step 231/574 completed (loss: 0.575892984867096, acc: 0.8222222328186035)
[2024-11-13 07:07:33,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:34,025][root][INFO] - Training Epoch: 9/10, step 232/574 completed (loss: 0.7711648941040039, acc: 0.7833333611488342)
[2024-11-13 07:07:34,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:34,736][root][INFO] - Training Epoch: 9/10, step 233/574 completed (loss: 1.1475859880447388, acc: 0.6926605701446533)
[2024-11-13 07:07:35,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:35,428][root][INFO] - Training Epoch: 9/10, step 234/574 completed (loss: 0.5667597651481628, acc: 0.8307692408561707)
[2024-11-13 07:07:35,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:35,830][root][INFO] - Training Epoch: 9/10, step 235/574 completed (loss: 0.2046361267566681, acc: 0.9473684430122375)
[2024-11-13 07:07:35,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:36,200][root][INFO] - Training Epoch: 9/10, step 236/574 completed (loss: 0.02629106305539608, acc: 1.0)
[2024-11-13 07:07:36,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:36,576][root][INFO] - Training Epoch: 9/10, step 237/574 completed (loss: 0.28685176372528076, acc: 0.9090909361839294)
[2024-11-13 07:07:36,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:36,961][root][INFO] - Training Epoch: 9/10, step 238/574 completed (loss: 0.22105714678764343, acc: 0.9629629850387573)
[2024-11-13 07:07:37,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:37,328][root][INFO] - Training Epoch: 9/10, step 239/574 completed (loss: 0.0598108172416687, acc: 0.9714285731315613)
[2024-11-13 07:07:37,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:37,803][root][INFO] - Training Epoch: 9/10, step 240/574 completed (loss: 0.20323839783668518, acc: 0.9090909361839294)
[2024-11-13 07:07:37,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:38,199][root][INFO] - Training Epoch: 9/10, step 241/574 completed (loss: 0.234676793217659, acc: 0.9090909361839294)
[2024-11-13 07:07:38,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:39,088][root][INFO] - Training Epoch: 9/10, step 242/574 completed (loss: 0.36693206429481506, acc: 0.8548387289047241)
[2024-11-13 07:07:39,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:39,898][root][INFO] - Training Epoch: 9/10, step 243/574 completed (loss: 0.4189375638961792, acc: 0.9090909361839294)
[2024-11-13 07:07:40,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:40,246][root][INFO] - Training Epoch: 9/10, step 244/574 completed (loss: 0.0020893376786261797, acc: 1.0)
[2024-11-13 07:07:40,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:40,643][root][INFO] - Training Epoch: 9/10, step 245/574 completed (loss: 0.18123623728752136, acc: 0.9615384340286255)
[2024-11-13 07:07:40,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:41,020][root][INFO] - Training Epoch: 9/10, step 246/574 completed (loss: 0.02795903943479061, acc: 1.0)
[2024-11-13 07:07:41,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:41,364][root][INFO] - Training Epoch: 9/10, step 247/574 completed (loss: 0.010387412272393703, acc: 1.0)
[2024-11-13 07:07:41,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:41,811][root][INFO] - Training Epoch: 9/10, step 248/574 completed (loss: 0.032760389149188995, acc: 1.0)
[2024-11-13 07:07:41,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:42,175][root][INFO] - Training Epoch: 9/10, step 249/574 completed (loss: 0.2044445127248764, acc: 0.9459459185600281)
[2024-11-13 07:07:42,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:42,557][root][INFO] - Training Epoch: 9/10, step 250/574 completed (loss: 0.02256147935986519, acc: 1.0)
[2024-11-13 07:07:42,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:42,977][root][INFO] - Training Epoch: 9/10, step 251/574 completed (loss: 0.07665753364562988, acc: 1.0)
[2024-11-13 07:07:43,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:43,406][root][INFO] - Training Epoch: 9/10, step 252/574 completed (loss: 0.046250540763139725, acc: 0.9756097793579102)
[2024-11-13 07:07:43,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:43,851][root][INFO] - Training Epoch: 9/10, step 253/574 completed (loss: 0.0034277623053640127, acc: 1.0)
[2024-11-13 07:07:44,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:44,238][root][INFO] - Training Epoch: 9/10, step 254/574 completed (loss: 0.003970967140048742, acc: 1.0)
[2024-11-13 07:07:44,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:44,692][root][INFO] - Training Epoch: 9/10, step 255/574 completed (loss: 0.25881826877593994, acc: 0.9354838728904724)
[2024-11-13 07:07:44,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:45,080][root][INFO] - Training Epoch: 9/10, step 256/574 completed (loss: 0.03267136961221695, acc: 0.9824561476707458)
[2024-11-13 07:07:45,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:45,523][root][INFO] - Training Epoch: 9/10, step 257/574 completed (loss: 0.06779991835355759, acc: 0.9714285731315613)
[2024-11-13 07:07:45,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:45,918][root][INFO] - Training Epoch: 9/10, step 258/574 completed (loss: 0.11998791247606277, acc: 0.9473684430122375)
[2024-11-13 07:07:46,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:46,763][root][INFO] - Training Epoch: 9/10, step 259/574 completed (loss: 0.12775792181491852, acc: 0.9528301954269409)
[2024-11-13 07:07:47,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:47,641][root][INFO] - Training Epoch: 9/10, step 260/574 completed (loss: 0.172664076089859, acc: 0.9583333134651184)
[2024-11-13 07:07:47,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:48,022][root][INFO] - Training Epoch: 9/10, step 261/574 completed (loss: 0.05303705483675003, acc: 0.9722222089767456)
[2024-11-13 07:07:48,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:48,439][root][INFO] - Training Epoch: 9/10, step 262/574 completed (loss: 0.0664076879620552, acc: 1.0)
[2024-11-13 07:07:48,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:48,883][root][INFO] - Training Epoch: 9/10, step 263/574 completed (loss: 0.25700443983078003, acc: 0.9200000166893005)
[2024-11-13 07:07:49,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:49,304][root][INFO] - Training Epoch: 9/10, step 264/574 completed (loss: 0.1409188061952591, acc: 0.9583333134651184)
[2024-11-13 07:07:49,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:50,597][root][INFO] - Training Epoch: 9/10, step 265/574 completed (loss: 0.6289591193199158, acc: 0.8080000281333923)
[2024-11-13 07:07:50,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:51,065][root][INFO] - Training Epoch: 9/10, step 266/574 completed (loss: 0.4709668457508087, acc: 0.8426966071128845)
[2024-11-13 07:07:51,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:51,516][root][INFO] - Training Epoch: 9/10, step 267/574 completed (loss: 0.31673479080200195, acc: 0.8918918967247009)
[2024-11-13 07:07:51,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:52,171][root][INFO] - Training Epoch: 9/10, step 268/574 completed (loss: 0.15946702659130096, acc: 0.9482758641242981)
[2024-11-13 07:07:52,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:52,530][root][INFO] - Training Epoch: 9/10, step 269/574 completed (loss: 0.2538219690322876, acc: 0.9545454382896423)
[2024-11-13 07:07:53,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:53,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:54,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:54,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:55,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:55,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:56,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:56,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:57,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:57,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:58,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:58,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:59,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:07:59,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:00,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:00,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:01,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:01,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:02,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:02,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:03,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:03,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:04,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:04,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:05,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:05,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:06,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:06,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:07,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:07,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:08,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:08,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:09,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:09,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:10,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:10,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:11,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:11,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:12,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:12,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:13,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:13,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:14,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:14,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:14,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:15,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:16,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:16,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:17,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:17,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:17,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:18,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:18,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:19,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:19,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:19,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:20,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:21,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:21,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:22,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:22,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:23,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:23,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:24,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:24,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:25,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:25,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:26,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:27,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:27,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:28,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:28,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:29,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:29,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:30,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:30,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:31,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:31,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:32,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:33,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:33,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:34,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:34,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:35,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:35,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:36,154][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4467, device='cuda:0') eval_epoch_loss=tensor(0.8948, device='cuda:0') eval_epoch_acc=tensor(0.8116, device='cuda:0')
[2024-11-13 07:08:36,155][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 07:08:36,156][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 07:08:36,576][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_9_step_270_loss_0.8947595357894897/model.pt
[2024-11-13 07:08:36,592][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 07:08:36,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:37,074][root][INFO] - Training Epoch: 9/10, step 270/574 completed (loss: 0.12267518043518066, acc: 0.9545454382896423)
[2024-11-13 07:08:37,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:37,482][root][INFO] - Training Epoch: 9/10, step 271/574 completed (loss: 0.01953985169529915, acc: 1.0)
[2024-11-13 07:08:37,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:37,921][root][INFO] - Training Epoch: 9/10, step 272/574 completed (loss: 0.0613800473511219, acc: 0.9666666388511658)
[2024-11-13 07:08:38,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:38,426][root][INFO] - Training Epoch: 9/10, step 273/574 completed (loss: 0.11218538135290146, acc: 0.9666666388511658)
[2024-11-13 07:08:38,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:38,854][root][INFO] - Training Epoch: 9/10, step 274/574 completed (loss: 0.12225442379713058, acc: 0.96875)
[2024-11-13 07:08:39,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:39,284][root][INFO] - Training Epoch: 9/10, step 275/574 completed (loss: 0.008939963765442371, acc: 1.0)
[2024-11-13 07:08:39,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:39,709][root][INFO] - Training Epoch: 9/10, step 276/574 completed (loss: 0.05912923067808151, acc: 0.9655172228813171)
[2024-11-13 07:08:39,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:40,125][root][INFO] - Training Epoch: 9/10, step 277/574 completed (loss: 0.07808170467615128, acc: 0.9599999785423279)
[2024-11-13 07:08:40,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:40,539][root][INFO] - Training Epoch: 9/10, step 278/574 completed (loss: 0.10389295220375061, acc: 0.978723406791687)
[2024-11-13 07:08:40,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:40,957][root][INFO] - Training Epoch: 9/10, step 279/574 completed (loss: 0.157363623380661, acc: 0.9583333134651184)
[2024-11-13 07:08:41,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:41,416][root][INFO] - Training Epoch: 9/10, step 280/574 completed (loss: 0.03584149479866028, acc: 1.0)
[2024-11-13 07:08:41,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:42,013][root][INFO] - Training Epoch: 9/10, step 281/574 completed (loss: 0.26514533162117004, acc: 0.9518072009086609)
[2024-11-13 07:08:42,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:42,478][root][INFO] - Training Epoch: 9/10, step 282/574 completed (loss: 0.4879053831100464, acc: 0.8425925970077515)
[2024-11-13 07:08:42,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:42,838][root][INFO] - Training Epoch: 9/10, step 283/574 completed (loss: 0.02294624038040638, acc: 1.0)
[2024-11-13 07:08:43,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:43,236][root][INFO] - Training Epoch: 9/10, step 284/574 completed (loss: 0.24427349865436554, acc: 0.9411764740943909)
[2024-11-13 07:08:43,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:43,667][root][INFO] - Training Epoch: 9/10, step 285/574 completed (loss: 0.08433940261602402, acc: 0.9750000238418579)
[2024-11-13 07:08:43,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:44,070][root][INFO] - Training Epoch: 9/10, step 286/574 completed (loss: 0.1832396537065506, acc: 0.9375)
[2024-11-13 07:08:44,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:44,495][root][INFO] - Training Epoch: 9/10, step 287/574 completed (loss: 0.16083171963691711, acc: 0.9520000219345093)
[2024-11-13 07:08:44,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:44,949][root][INFO] - Training Epoch: 9/10, step 288/574 completed (loss: 0.12531591951847076, acc: 0.9670329689979553)
[2024-11-13 07:08:45,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:45,335][root][INFO] - Training Epoch: 9/10, step 289/574 completed (loss: 0.14690014719963074, acc: 0.95652174949646)
[2024-11-13 07:08:45,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:45,755][root][INFO] - Training Epoch: 9/10, step 290/574 completed (loss: 0.26186826825141907, acc: 0.9175257682800293)
[2024-11-13 07:08:45,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:46,189][root][INFO] - Training Epoch: 9/10, step 291/574 completed (loss: 0.02915092557668686, acc: 1.0)
[2024-11-13 07:08:46,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:46,627][root][INFO] - Training Epoch: 9/10, step 292/574 completed (loss: 0.045857127755880356, acc: 1.0)
[2024-11-13 07:08:46,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:47,057][root][INFO] - Training Epoch: 9/10, step 293/574 completed (loss: 0.12161830067634583, acc: 0.9482758641242981)
[2024-11-13 07:08:47,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:47,740][root][INFO] - Training Epoch: 9/10, step 294/574 completed (loss: 0.12597545981407166, acc: 0.9636363387107849)
[2024-11-13 07:08:48,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:48,561][root][INFO] - Training Epoch: 9/10, step 295/574 completed (loss: 0.3169226348400116, acc: 0.907216489315033)
[2024-11-13 07:08:48,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:48,932][root][INFO] - Training Epoch: 9/10, step 296/574 completed (loss: 0.07345043867826462, acc: 0.982758641242981)
[2024-11-13 07:08:49,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:49,266][root][INFO] - Training Epoch: 9/10, step 297/574 completed (loss: 0.011231891810894012, acc: 1.0)
[2024-11-13 07:08:49,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:49,668][root][INFO] - Training Epoch: 9/10, step 298/574 completed (loss: 0.22625647485256195, acc: 0.9210526347160339)
[2024-11-13 07:08:49,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:50,097][root][INFO] - Training Epoch: 9/10, step 299/574 completed (loss: 0.162909135222435, acc: 0.9642857313156128)
[2024-11-13 07:08:50,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:50,550][root][INFO] - Training Epoch: 9/10, step 300/574 completed (loss: 0.03320396691560745, acc: 1.0)
[2024-11-13 07:08:50,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:50,949][root][INFO] - Training Epoch: 9/10, step 301/574 completed (loss: 0.03776973485946655, acc: 1.0)
[2024-11-13 07:08:51,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:51,371][root][INFO] - Training Epoch: 9/10, step 302/574 completed (loss: 0.14986664056777954, acc: 0.9811320900917053)
[2024-11-13 07:08:51,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:51,744][root][INFO] - Training Epoch: 9/10, step 303/574 completed (loss: 0.006587235722690821, acc: 1.0)
[2024-11-13 07:08:51,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:52,107][root][INFO] - Training Epoch: 9/10, step 304/574 completed (loss: 0.009947541169822216, acc: 1.0)
[2024-11-13 07:08:52,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:52,514][root][INFO] - Training Epoch: 9/10, step 305/574 completed (loss: 0.1676909178495407, acc: 0.9344262480735779)
[2024-11-13 07:08:52,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:52,937][root][INFO] - Training Epoch: 9/10, step 306/574 completed (loss: 0.0367182232439518, acc: 1.0)
[2024-11-13 07:08:53,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:53,325][root][INFO] - Training Epoch: 9/10, step 307/574 completed (loss: 0.005731707438826561, acc: 1.0)
[2024-11-13 07:08:53,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:53,693][root][INFO] - Training Epoch: 9/10, step 308/574 completed (loss: 0.15545764565467834, acc: 0.95652174949646)
[2024-11-13 07:08:53,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:54,267][root][INFO] - Training Epoch: 9/10, step 309/574 completed (loss: 0.0908682569861412, acc: 0.9583333134651184)
[2024-11-13 07:08:54,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:54,666][root][INFO] - Training Epoch: 9/10, step 310/574 completed (loss: 0.07709841430187225, acc: 0.9759036302566528)
[2024-11-13 07:08:54,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:55,098][root][INFO] - Training Epoch: 9/10, step 311/574 completed (loss: 0.10389619320631027, acc: 0.9743589758872986)
[2024-11-13 07:08:55,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:55,564][root][INFO] - Training Epoch: 9/10, step 312/574 completed (loss: 0.05188805237412453, acc: 0.9693877696990967)
[2024-11-13 07:08:55,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:55,935][root][INFO] - Training Epoch: 9/10, step 313/574 completed (loss: 0.0036789344158023596, acc: 1.0)
[2024-11-13 07:08:56,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:56,304][root][INFO] - Training Epoch: 9/10, step 314/574 completed (loss: 0.04465104639530182, acc: 1.0)
[2024-11-13 07:08:56,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:56,723][root][INFO] - Training Epoch: 9/10, step 315/574 completed (loss: 0.04954992234706879, acc: 1.0)
[2024-11-13 07:08:56,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:57,128][root][INFO] - Training Epoch: 9/10, step 316/574 completed (loss: 0.15132299065589905, acc: 0.9354838728904724)
[2024-11-13 07:08:57,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:57,533][root][INFO] - Training Epoch: 9/10, step 317/574 completed (loss: 0.018863001838326454, acc: 1.0)
[2024-11-13 07:08:57,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:57,957][root][INFO] - Training Epoch: 9/10, step 318/574 completed (loss: 0.05075392499566078, acc: 0.9807692170143127)
[2024-11-13 07:08:58,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:58,352][root][INFO] - Training Epoch: 9/10, step 319/574 completed (loss: 0.017093215137720108, acc: 1.0)
[2024-11-13 07:08:58,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:58,771][root][INFO] - Training Epoch: 9/10, step 320/574 completed (loss: 0.05712587758898735, acc: 0.9677419066429138)
[2024-11-13 07:08:58,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:59,183][root][INFO] - Training Epoch: 9/10, step 321/574 completed (loss: 0.009665287099778652, acc: 1.0)
[2024-11-13 07:08:59,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:08:59,606][root][INFO] - Training Epoch: 9/10, step 322/574 completed (loss: 0.3346846103668213, acc: 0.8888888955116272)
[2024-11-13 07:08:59,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:00,006][root][INFO] - Training Epoch: 9/10, step 323/574 completed (loss: 0.09131034463644028, acc: 0.9714285731315613)
[2024-11-13 07:09:00,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:00,362][root][INFO] - Training Epoch: 9/10, step 324/574 completed (loss: 0.14784157276153564, acc: 0.9487179517745972)
[2024-11-13 07:09:00,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:00,774][root][INFO] - Training Epoch: 9/10, step 325/574 completed (loss: 0.4693162143230438, acc: 0.8536585569381714)
[2024-11-13 07:09:00,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:01,183][root][INFO] - Training Epoch: 9/10, step 326/574 completed (loss: 0.10514670610427856, acc: 0.9736841917037964)
[2024-11-13 07:09:01,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:01,620][root][INFO] - Training Epoch: 9/10, step 327/574 completed (loss: 0.07461480796337128, acc: 0.9473684430122375)
[2024-11-13 07:09:01,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:02,025][root][INFO] - Training Epoch: 9/10, step 328/574 completed (loss: 0.013302628882229328, acc: 1.0)
[2024-11-13 07:09:02,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:02,457][root][INFO] - Training Epoch: 9/10, step 329/574 completed (loss: 0.0023986399173736572, acc: 1.0)
[2024-11-13 07:09:02,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:02,903][root][INFO] - Training Epoch: 9/10, step 330/574 completed (loss: 0.002755336230620742, acc: 1.0)
[2024-11-13 07:09:03,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:03,350][root][INFO] - Training Epoch: 9/10, step 331/574 completed (loss: 0.023347800597548485, acc: 1.0)
[2024-11-13 07:09:03,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:03,862][root][INFO] - Training Epoch: 9/10, step 332/574 completed (loss: 0.015689540654420853, acc: 1.0)
[2024-11-13 07:09:04,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:04,248][root][INFO] - Training Epoch: 9/10, step 333/574 completed (loss: 0.08796664327383041, acc: 1.0)
[2024-11-13 07:09:04,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:04,666][root][INFO] - Training Epoch: 9/10, step 334/574 completed (loss: 0.009302211925387383, acc: 1.0)
[2024-11-13 07:09:04,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:05,019][root][INFO] - Training Epoch: 9/10, step 335/574 completed (loss: 0.010772892273962498, acc: 1.0)
[2024-11-13 07:09:05,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:05,452][root][INFO] - Training Epoch: 9/10, step 336/574 completed (loss: 0.1554851233959198, acc: 0.9399999976158142)
[2024-11-13 07:09:05,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:05,928][root][INFO] - Training Epoch: 9/10, step 337/574 completed (loss: 0.5250815153121948, acc: 0.8390804529190063)
[2024-11-13 07:09:06,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:06,369][root][INFO] - Training Epoch: 9/10, step 338/574 completed (loss: 0.6216357350349426, acc: 0.8404255509376526)
[2024-11-13 07:09:06,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:06,768][root][INFO] - Training Epoch: 9/10, step 339/574 completed (loss: 0.35499268770217896, acc: 0.891566276550293)
[2024-11-13 07:09:06,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:07,172][root][INFO] - Training Epoch: 9/10, step 340/574 completed (loss: 0.013975027948617935, acc: 1.0)
[2024-11-13 07:09:07,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:07,514][root][INFO] - Training Epoch: 9/10, step 341/574 completed (loss: 0.006871640682220459, acc: 1.0)
[2024-11-13 07:09:07,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:07,953][root][INFO] - Training Epoch: 9/10, step 342/574 completed (loss: 0.05984083563089371, acc: 0.9759036302566528)
[2024-11-13 07:09:08,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:08,329][root][INFO] - Training Epoch: 9/10, step 343/574 completed (loss: 0.3519192934036255, acc: 0.9056603908538818)
[2024-11-13 07:09:08,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:08,768][root][INFO] - Training Epoch: 9/10, step 344/574 completed (loss: 0.04307448863983154, acc: 0.9746835231781006)
[2024-11-13 07:09:08,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:09,180][root][INFO] - Training Epoch: 9/10, step 345/574 completed (loss: 0.022116608917713165, acc: 1.0)
[2024-11-13 07:09:09,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:09,619][root][INFO] - Training Epoch: 9/10, step 346/574 completed (loss: 0.08017881959676743, acc: 0.9701492786407471)
[2024-11-13 07:09:09,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:09,973][root][INFO] - Training Epoch: 9/10, step 347/574 completed (loss: 0.006523592863231897, acc: 1.0)
[2024-11-13 07:09:10,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:10,304][root][INFO] - Training Epoch: 9/10, step 348/574 completed (loss: 0.015988826751708984, acc: 1.0)
[2024-11-13 07:09:10,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:10,835][root][INFO] - Training Epoch: 9/10, step 349/574 completed (loss: 0.2075357884168625, acc: 0.9722222089767456)
[2024-11-13 07:09:11,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:11,247][root][INFO] - Training Epoch: 9/10, step 350/574 completed (loss: 0.05621074140071869, acc: 1.0)
[2024-11-13 07:09:11,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:11,655][root][INFO] - Training Epoch: 9/10, step 351/574 completed (loss: 0.09189355373382568, acc: 0.9487179517745972)
[2024-11-13 07:09:11,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:12,155][root][INFO] - Training Epoch: 9/10, step 352/574 completed (loss: 0.1390414535999298, acc: 0.9111111164093018)
[2024-11-13 07:09:12,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:12,512][root][INFO] - Training Epoch: 9/10, step 353/574 completed (loss: 0.0023853445891290903, acc: 1.0)
[2024-11-13 07:09:12,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:12,968][root][INFO] - Training Epoch: 9/10, step 354/574 completed (loss: 0.006589361000806093, acc: 1.0)
[2024-11-13 07:09:13,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:13,483][root][INFO] - Training Epoch: 9/10, step 355/574 completed (loss: 0.3804832994937897, acc: 0.8681318759918213)
[2024-11-13 07:09:13,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:14,237][root][INFO] - Training Epoch: 9/10, step 356/574 completed (loss: 0.2018982172012329, acc: 0.9130434989929199)
[2024-11-13 07:09:14,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:14,707][root][INFO] - Training Epoch: 9/10, step 357/574 completed (loss: 0.14331042766571045, acc: 0.967391312122345)
[2024-11-13 07:09:14,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:15,150][root][INFO] - Training Epoch: 9/10, step 358/574 completed (loss: 0.33417126536369324, acc: 0.8979591727256775)
[2024-11-13 07:09:15,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:15,541][root][INFO] - Training Epoch: 9/10, step 359/574 completed (loss: 0.0011507375165820122, acc: 1.0)
[2024-11-13 07:09:15,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:15,888][root][INFO] - Training Epoch: 9/10, step 360/574 completed (loss: 0.011045264080166817, acc: 1.0)
[2024-11-13 07:09:16,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:16,296][root][INFO] - Training Epoch: 9/10, step 361/574 completed (loss: 0.1513059139251709, acc: 0.9512194991111755)
[2024-11-13 07:09:16,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:16,703][root][INFO] - Training Epoch: 9/10, step 362/574 completed (loss: 0.08852160722017288, acc: 0.9777777791023254)
[2024-11-13 07:09:16,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:17,123][root][INFO] - Training Epoch: 9/10, step 363/574 completed (loss: 0.019591044634580612, acc: 1.0)
[2024-11-13 07:09:17,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:17,540][root][INFO] - Training Epoch: 9/10, step 364/574 completed (loss: 0.02940201759338379, acc: 0.9756097793579102)
[2024-11-13 07:09:17,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:17,906][root][INFO] - Training Epoch: 9/10, step 365/574 completed (loss: 0.003798604942858219, acc: 1.0)
[2024-11-13 07:09:18,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:18,319][root][INFO] - Training Epoch: 9/10, step 366/574 completed (loss: 0.003067879006266594, acc: 1.0)
[2024-11-13 07:09:18,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:18,754][root][INFO] - Training Epoch: 9/10, step 367/574 completed (loss: 0.05553837865591049, acc: 0.95652174949646)
[2024-11-13 07:09:18,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:19,126][root][INFO] - Training Epoch: 9/10, step 368/574 completed (loss: 0.028659651055932045, acc: 1.0)
[2024-11-13 07:09:19,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:19,588][root][INFO] - Training Epoch: 9/10, step 369/574 completed (loss: 0.11970160901546478, acc: 0.96875)
[2024-11-13 07:09:20,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:20,547][root][INFO] - Training Epoch: 9/10, step 370/574 completed (loss: 0.402186781167984, acc: 0.8848484754562378)
[2024-11-13 07:09:21,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:21,879][root][INFO] - Training Epoch: 9/10, step 371/574 completed (loss: 0.06775763630867004, acc: 0.9716981053352356)
[2024-11-13 07:09:22,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:22,316][root][INFO] - Training Epoch: 9/10, step 372/574 completed (loss: 0.0961940586566925, acc: 0.9777777791023254)
[2024-11-13 07:09:22,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:22,700][root][INFO] - Training Epoch: 9/10, step 373/574 completed (loss: 0.04683973640203476, acc: 0.9821428656578064)
[2024-11-13 07:09:22,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:23,098][root][INFO] - Training Epoch: 9/10, step 374/574 completed (loss: 0.017101358622312546, acc: 1.0)
[2024-11-13 07:09:23,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:23,486][root][INFO] - Training Epoch: 9/10, step 375/574 completed (loss: 0.0007550560403615236, acc: 1.0)
[2024-11-13 07:09:23,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:23,885][root][INFO] - Training Epoch: 9/10, step 376/574 completed (loss: 0.020114118233323097, acc: 1.0)
[2024-11-13 07:09:24,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:24,345][root][INFO] - Training Epoch: 9/10, step 377/574 completed (loss: 0.017081433907151222, acc: 1.0)
[2024-11-13 07:09:24,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:24,811][root][INFO] - Training Epoch: 9/10, step 378/574 completed (loss: 0.037430983036756516, acc: 0.9789473414421082)
[2024-11-13 07:09:25,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:25,688][root][INFO] - Training Epoch: 9/10, step 379/574 completed (loss: 0.1343514323234558, acc: 0.9580838084220886)
[2024-11-13 07:09:25,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:26,248][root][INFO] - Training Epoch: 9/10, step 380/574 completed (loss: 0.11294461786746979, acc: 0.969924807548523)
[2024-11-13 07:09:27,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:28,134][root][INFO] - Training Epoch: 9/10, step 381/574 completed (loss: 0.4116849899291992, acc: 0.8877005577087402)
[2024-11-13 07:09:28,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:28,992][root][INFO] - Training Epoch: 9/10, step 382/574 completed (loss: 0.07911057770252228, acc: 0.9819819927215576)
[2024-11-13 07:09:29,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:29,416][root][INFO] - Training Epoch: 9/10, step 383/574 completed (loss: 0.014174566604197025, acc: 1.0)
[2024-11-13 07:09:29,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:29,790][root][INFO] - Training Epoch: 9/10, step 384/574 completed (loss: 0.018358485773205757, acc: 1.0)
[2024-11-13 07:09:29,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:30,112][root][INFO] - Training Epoch: 9/10, step 385/574 completed (loss: 0.003306363010779023, acc: 1.0)
[2024-11-13 07:09:30,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:30,518][root][INFO] - Training Epoch: 9/10, step 386/574 completed (loss: 0.003962590824812651, acc: 1.0)
[2024-11-13 07:09:30,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:30,929][root][INFO] - Training Epoch: 9/10, step 387/574 completed (loss: 0.006396645214408636, acc: 1.0)
[2024-11-13 07:09:31,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:31,304][root][INFO] - Training Epoch: 9/10, step 388/574 completed (loss: 0.0008945016306824982, acc: 1.0)
[2024-11-13 07:09:31,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:31,765][root][INFO] - Training Epoch: 9/10, step 389/574 completed (loss: 0.048811208456754684, acc: 1.0)
[2024-11-13 07:09:31,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:32,213][root][INFO] - Training Epoch: 9/10, step 390/574 completed (loss: 0.018579579889774323, acc: 1.0)
[2024-11-13 07:09:32,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:32,623][root][INFO] - Training Epoch: 9/10, step 391/574 completed (loss: 0.1112201064825058, acc: 0.9629629850387573)
[2024-11-13 07:09:32,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:33,011][root][INFO] - Training Epoch: 9/10, step 392/574 completed (loss: 0.47619348764419556, acc: 0.8834951519966125)
[2024-11-13 07:09:33,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:33,780][root][INFO] - Training Epoch: 9/10, step 393/574 completed (loss: 0.5556545853614807, acc: 0.8529411554336548)
[2024-11-13 07:09:33,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:34,278][root][INFO] - Training Epoch: 9/10, step 394/574 completed (loss: 0.4680110812187195, acc: 0.846666693687439)
[2024-11-13 07:09:34,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:34,802][root][INFO] - Training Epoch: 9/10, step 395/574 completed (loss: 0.2630094885826111, acc: 0.9027777910232544)
[2024-11-13 07:09:34,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:35,245][root][INFO] - Training Epoch: 9/10, step 396/574 completed (loss: 0.028932243585586548, acc: 1.0)
[2024-11-13 07:09:35,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:35,711][root][INFO] - Training Epoch: 9/10, step 397/574 completed (loss: 0.004788244143128395, acc: 1.0)
[2024-11-13 07:09:35,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:36,187][root][INFO] - Training Epoch: 9/10, step 398/574 completed (loss: 0.03225449472665787, acc: 0.9767441749572754)
[2024-11-13 07:09:36,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:36,581][root][INFO] - Training Epoch: 9/10, step 399/574 completed (loss: 0.0017811182187870145, acc: 1.0)
[2024-11-13 07:09:36,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:37,384][root][INFO] - Training Epoch: 9/10, step 400/574 completed (loss: 0.2075503021478653, acc: 0.9558823704719543)
[2024-11-13 07:09:37,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:37,810][root][INFO] - Training Epoch: 9/10, step 401/574 completed (loss: 0.1413695365190506, acc: 0.9333333373069763)
[2024-11-13 07:09:37,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:38,214][root][INFO] - Training Epoch: 9/10, step 402/574 completed (loss: 0.1729612946510315, acc: 0.9696969985961914)
[2024-11-13 07:09:38,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:38,635][root][INFO] - Training Epoch: 9/10, step 403/574 completed (loss: 0.046073030680418015, acc: 0.9696969985961914)
[2024-11-13 07:09:38,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:39,023][root][INFO] - Training Epoch: 9/10, step 404/574 completed (loss: 0.08101418614387512, acc: 0.9677419066429138)
[2024-11-13 07:09:39,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:39,364][root][INFO] - Training Epoch: 9/10, step 405/574 completed (loss: 0.40892842411994934, acc: 0.9629629850387573)
[2024-11-13 07:09:39,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:39,754][root][INFO] - Training Epoch: 9/10, step 406/574 completed (loss: 0.0007226411253213882, acc: 1.0)
[2024-11-13 07:09:39,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:40,096][root][INFO] - Training Epoch: 9/10, step 407/574 completed (loss: 0.01231275126338005, acc: 1.0)
[2024-11-13 07:09:40,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:40,454][root][INFO] - Training Epoch: 9/10, step 408/574 completed (loss: 0.009057900868356228, acc: 1.0)
[2024-11-13 07:09:40,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:40,810][root][INFO] - Training Epoch: 9/10, step 409/574 completed (loss: 0.025434691458940506, acc: 1.0)
[2024-11-13 07:09:40,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:41,195][root][INFO] - Training Epoch: 9/10, step 410/574 completed (loss: 0.012215140275657177, acc: 1.0)
[2024-11-13 07:09:41,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:41,592][root][INFO] - Training Epoch: 9/10, step 411/574 completed (loss: 0.017917869612574577, acc: 1.0)
[2024-11-13 07:09:41,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:41,958][root][INFO] - Training Epoch: 9/10, step 412/574 completed (loss: 0.0036732780281454325, acc: 1.0)
[2024-11-13 07:09:42,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:43,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:43,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:44,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:44,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:45,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:45,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:46,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:46,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:47,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:48,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:48,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:49,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:49,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:50,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:50,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:51,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:51,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:52,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:52,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:53,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:53,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:54,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:54,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:55,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:55,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:56,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:56,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:57,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:57,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:57,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:58,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:58,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:59,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:09:59,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:00,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:00,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:01,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:01,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:02,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:02,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:03,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:03,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:03,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:04,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:04,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:05,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:05,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:06,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:06,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:07,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:07,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:08,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:08,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:09,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:09,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:10,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:10,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:11,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:11,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:12,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:12,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:13,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:13,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:14,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:14,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:15,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:15,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:16,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:16,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:17,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:17,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:18,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:18,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:19,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:19,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:20,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:20,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:21,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:21,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:22,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:22,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:23,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:23,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:24,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:24,823][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5456, device='cuda:0') eval_epoch_loss=tensor(0.9344, device='cuda:0') eval_epoch_acc=tensor(0.8052, device='cuda:0')
[2024-11-13 07:10:24,824][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 07:10:24,824][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 07:10:25,207][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_9_step_413_loss_0.9343580603599548/model.pt
[2024-11-13 07:10:25,212][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 07:10:25,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:25,585][root][INFO] - Training Epoch: 9/10, step 413/574 completed (loss: 0.0020509257446974516, acc: 1.0)
[2024-11-13 07:10:25,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:26,006][root][INFO] - Training Epoch: 9/10, step 414/574 completed (loss: 0.07997900992631912, acc: 0.9545454382896423)
[2024-11-13 07:10:26,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:26,397][root][INFO] - Training Epoch: 9/10, step 415/574 completed (loss: 0.06130555272102356, acc: 0.9803921580314636)
[2024-11-13 07:10:26,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:26,781][root][INFO] - Training Epoch: 9/10, step 416/574 completed (loss: 0.014476126059889793, acc: 1.0)
[2024-11-13 07:10:26,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:27,145][root][INFO] - Training Epoch: 9/10, step 417/574 completed (loss: 0.0846489667892456, acc: 0.9444444179534912)
[2024-11-13 07:10:27,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:27,577][root][INFO] - Training Epoch: 9/10, step 418/574 completed (loss: 0.06414209306240082, acc: 0.949999988079071)
[2024-11-13 07:10:27,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:27,923][root][INFO] - Training Epoch: 9/10, step 419/574 completed (loss: 0.0467158704996109, acc: 1.0)
[2024-11-13 07:10:28,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:28,327][root][INFO] - Training Epoch: 9/10, step 420/574 completed (loss: 0.00231661694124341, acc: 1.0)
[2024-11-13 07:10:28,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:28,711][root][INFO] - Training Epoch: 9/10, step 421/574 completed (loss: 0.07442992180585861, acc: 0.9333333373069763)
[2024-11-13 07:10:28,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:29,079][root][INFO] - Training Epoch: 9/10, step 422/574 completed (loss: 0.01038913894444704, acc: 1.0)
[2024-11-13 07:10:29,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:29,506][root][INFO] - Training Epoch: 9/10, step 423/574 completed (loss: 0.11127662658691406, acc: 0.9722222089767456)
[2024-11-13 07:10:29,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:29,910][root][INFO] - Training Epoch: 9/10, step 424/574 completed (loss: 0.017362143844366074, acc: 1.0)
[2024-11-13 07:10:30,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:30,307][root][INFO] - Training Epoch: 9/10, step 425/574 completed (loss: 0.12207194417715073, acc: 0.9696969985961914)
[2024-11-13 07:10:30,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:30,708][root][INFO] - Training Epoch: 9/10, step 426/574 completed (loss: 0.009237703867256641, acc: 1.0)
[2024-11-13 07:10:30,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:31,116][root][INFO] - Training Epoch: 9/10, step 427/574 completed (loss: 0.0699160099029541, acc: 0.9729729890823364)
[2024-11-13 07:10:31,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:31,513][root][INFO] - Training Epoch: 9/10, step 428/574 completed (loss: 0.00133357138838619, acc: 1.0)
[2024-11-13 07:10:31,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:31,916][root][INFO] - Training Epoch: 9/10, step 429/574 completed (loss: 0.0070020826533436775, acc: 1.0)
[2024-11-13 07:10:32,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:32,303][root][INFO] - Training Epoch: 9/10, step 430/574 completed (loss: 0.0024553360417485237, acc: 1.0)
[2024-11-13 07:10:32,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:32,736][root][INFO] - Training Epoch: 9/10, step 431/574 completed (loss: 0.008508246392011642, acc: 1.0)
[2024-11-13 07:10:32,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:33,128][root][INFO] - Training Epoch: 9/10, step 432/574 completed (loss: 0.0018448716728016734, acc: 1.0)
[2024-11-13 07:10:33,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:33,593][root][INFO] - Training Epoch: 9/10, step 433/574 completed (loss: 0.11684167385101318, acc: 0.9722222089767456)
[2024-11-13 07:10:33,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:34,033][root][INFO] - Training Epoch: 9/10, step 434/574 completed (loss: 0.0005336732720024884, acc: 1.0)
[2024-11-13 07:10:34,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:34,410][root][INFO] - Training Epoch: 9/10, step 435/574 completed (loss: 0.0009508028160780668, acc: 1.0)
[2024-11-13 07:10:34,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:34,838][root][INFO] - Training Epoch: 9/10, step 436/574 completed (loss: 0.09080256521701813, acc: 0.9722222089767456)
[2024-11-13 07:10:34,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:35,204][root][INFO] - Training Epoch: 9/10, step 437/574 completed (loss: 0.20324145257472992, acc: 0.9772727489471436)
[2024-11-13 07:10:35,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:35,574][root][INFO] - Training Epoch: 9/10, step 438/574 completed (loss: 0.0059577589854598045, acc: 1.0)
[2024-11-13 07:10:35,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:35,953][root][INFO] - Training Epoch: 9/10, step 439/574 completed (loss: 0.06136403977870941, acc: 1.0)
[2024-11-13 07:10:36,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:36,651][root][INFO] - Training Epoch: 9/10, step 440/574 completed (loss: 0.1639459729194641, acc: 0.939393937587738)
[2024-11-13 07:10:37,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:37,714][root][INFO] - Training Epoch: 9/10, step 441/574 completed (loss: 0.363968163728714, acc: 0.8640000224113464)
[2024-11-13 07:10:37,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:38,271][root][INFO] - Training Epoch: 9/10, step 442/574 completed (loss: 0.38786131143569946, acc: 0.8870967626571655)
[2024-11-13 07:10:38,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:39,272][root][INFO] - Training Epoch: 9/10, step 443/574 completed (loss: 0.43900734186172485, acc: 0.8805969953536987)
[2024-11-13 07:10:39,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:39,723][root][INFO] - Training Epoch: 9/10, step 444/574 completed (loss: 0.04084322974085808, acc: 0.9811320900917053)
[2024-11-13 07:10:40,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:40,318][root][INFO] - Training Epoch: 9/10, step 445/574 completed (loss: 0.06496480107307434, acc: 0.9772727489471436)
[2024-11-13 07:10:40,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:40,723][root][INFO] - Training Epoch: 9/10, step 446/574 completed (loss: 0.011880398727953434, acc: 1.0)
[2024-11-13 07:10:40,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:41,116][root][INFO] - Training Epoch: 9/10, step 447/574 completed (loss: 0.20055833458900452, acc: 0.9615384340286255)
[2024-11-13 07:10:41,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:41,479][root][INFO] - Training Epoch: 9/10, step 448/574 completed (loss: 0.02839469723403454, acc: 1.0)
[2024-11-13 07:10:41,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:41,860][root][INFO] - Training Epoch: 9/10, step 449/574 completed (loss: 0.028641531243920326, acc: 0.9850746393203735)
[2024-11-13 07:10:42,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:42,308][root][INFO] - Training Epoch: 9/10, step 450/574 completed (loss: 0.01738915778696537, acc: 1.0)
[2024-11-13 07:10:42,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:42,722][root][INFO] - Training Epoch: 9/10, step 451/574 completed (loss: 0.014247954823076725, acc: 1.0)
[2024-11-13 07:10:42,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:43,096][root][INFO] - Training Epoch: 9/10, step 452/574 completed (loss: 0.06800130754709244, acc: 0.9743589758872986)
[2024-11-13 07:10:43,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:43,527][root][INFO] - Training Epoch: 9/10, step 453/574 completed (loss: 0.029601868242025375, acc: 0.9868420958518982)
[2024-11-13 07:10:43,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:43,956][root][INFO] - Training Epoch: 9/10, step 454/574 completed (loss: 0.02368963696062565, acc: 1.0)
[2024-11-13 07:10:44,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:44,352][root][INFO] - Training Epoch: 9/10, step 455/574 completed (loss: 0.0021562210749834776, acc: 1.0)
[2024-11-13 07:10:44,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:44,768][root][INFO] - Training Epoch: 9/10, step 456/574 completed (loss: 0.12947365641593933, acc: 0.9587628841400146)
[2024-11-13 07:10:44,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:45,177][root][INFO] - Training Epoch: 9/10, step 457/574 completed (loss: 0.06891405582427979, acc: 0.9714285731315613)
[2024-11-13 07:10:45,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:45,686][root][INFO] - Training Epoch: 9/10, step 458/574 completed (loss: 0.2082652449607849, acc: 0.9360465407371521)
[2024-11-13 07:10:45,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:46,107][root][INFO] - Training Epoch: 9/10, step 459/574 completed (loss: 0.05376594141125679, acc: 0.9821428656578064)
[2024-11-13 07:10:46,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:46,510][root][INFO] - Training Epoch: 9/10, step 460/574 completed (loss: 0.08796095848083496, acc: 0.9753086566925049)
[2024-11-13 07:10:46,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:46,928][root][INFO] - Training Epoch: 9/10, step 461/574 completed (loss: 0.14664366841316223, acc: 0.9444444179534912)
[2024-11-13 07:10:47,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:47,285][root][INFO] - Training Epoch: 9/10, step 462/574 completed (loss: 0.039608001708984375, acc: 1.0)
[2024-11-13 07:10:47,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:47,631][root][INFO] - Training Epoch: 9/10, step 463/574 completed (loss: 0.013119365088641644, acc: 1.0)
[2024-11-13 07:10:47,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:48,015][root][INFO] - Training Epoch: 9/10, step 464/574 completed (loss: 0.03997733071446419, acc: 1.0)
[2024-11-13 07:10:48,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:48,373][root][INFO] - Training Epoch: 9/10, step 465/574 completed (loss: 0.10498703271150589, acc: 0.9642857313156128)
[2024-11-13 07:10:48,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:48,818][root][INFO] - Training Epoch: 9/10, step 466/574 completed (loss: 0.18946810066699982, acc: 0.9518072009086609)
[2024-11-13 07:10:49,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:49,264][root][INFO] - Training Epoch: 9/10, step 467/574 completed (loss: 0.05068584904074669, acc: 0.9729729890823364)
[2024-11-13 07:10:49,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:49,670][root][INFO] - Training Epoch: 9/10, step 468/574 completed (loss: 0.1580352783203125, acc: 0.9514563083648682)
[2024-11-13 07:10:49,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:50,098][root][INFO] - Training Epoch: 9/10, step 469/574 completed (loss: 0.18681761622428894, acc: 0.9430894255638123)
[2024-11-13 07:10:50,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:50,413][root][INFO] - Training Epoch: 9/10, step 470/574 completed (loss: 0.010313079692423344, acc: 1.0)
[2024-11-13 07:10:50,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:50,813][root][INFO] - Training Epoch: 9/10, step 471/574 completed (loss: 0.08491436392068863, acc: 0.9642857313156128)
[2024-11-13 07:10:51,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:51,371][root][INFO] - Training Epoch: 9/10, step 472/574 completed (loss: 0.16168390214443207, acc: 0.9509803652763367)
[2024-11-13 07:10:51,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:51,830][root][INFO] - Training Epoch: 9/10, step 473/574 completed (loss: 0.49327927827835083, acc: 0.8602620363235474)
[2024-11-13 07:10:52,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:52,241][root][INFO] - Training Epoch: 9/10, step 474/574 completed (loss: 0.1457662135362625, acc: 0.9583333134651184)
[2024-11-13 07:10:52,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:52,597][root][INFO] - Training Epoch: 9/10, step 475/574 completed (loss: 0.10863707959651947, acc: 0.9570552110671997)
[2024-11-13 07:10:52,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:52,975][root][INFO] - Training Epoch: 9/10, step 476/574 completed (loss: 0.14322945475578308, acc: 0.9496402740478516)
[2024-11-13 07:10:53,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:53,413][root][INFO] - Training Epoch: 9/10, step 477/574 completed (loss: 0.23020508885383606, acc: 0.9396985173225403)
[2024-11-13 07:10:53,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:53,758][root][INFO] - Training Epoch: 9/10, step 478/574 completed (loss: 0.04533982649445534, acc: 1.0)
[2024-11-13 07:10:53,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:54,141][root][INFO] - Training Epoch: 9/10, step 479/574 completed (loss: 0.03191663324832916, acc: 1.0)
[2024-11-13 07:10:54,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:54,506][root][INFO] - Training Epoch: 9/10, step 480/574 completed (loss: 0.004261577036231756, acc: 1.0)
[2024-11-13 07:10:54,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:54,914][root][INFO] - Training Epoch: 9/10, step 481/574 completed (loss: 0.11894258111715317, acc: 0.949999988079071)
[2024-11-13 07:10:55,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:55,317][root][INFO] - Training Epoch: 9/10, step 482/574 completed (loss: 0.26057785749435425, acc: 0.8999999761581421)
[2024-11-13 07:10:55,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:55,818][root][INFO] - Training Epoch: 9/10, step 483/574 completed (loss: 0.20234185457229614, acc: 0.931034505367279)
[2024-11-13 07:10:55,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:56,232][root][INFO] - Training Epoch: 9/10, step 484/574 completed (loss: 0.006723036058247089, acc: 1.0)
[2024-11-13 07:10:56,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:56,588][root][INFO] - Training Epoch: 9/10, step 485/574 completed (loss: 0.10427012294530869, acc: 0.9473684430122375)
[2024-11-13 07:10:56,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:56,951][root][INFO] - Training Epoch: 9/10, step 486/574 completed (loss: 0.08639001846313477, acc: 0.9629629850387573)
[2024-11-13 07:10:57,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:57,356][root][INFO] - Training Epoch: 9/10, step 487/574 completed (loss: 0.14407049119472504, acc: 0.9523809552192688)
[2024-11-13 07:10:57,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:57,717][root][INFO] - Training Epoch: 9/10, step 488/574 completed (loss: 0.03672836720943451, acc: 1.0)
[2024-11-13 07:10:57,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:58,157][root][INFO] - Training Epoch: 9/10, step 489/574 completed (loss: 0.25689423084259033, acc: 0.9230769276618958)
[2024-11-13 07:10:58,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:58,531][root][INFO] - Training Epoch: 9/10, step 490/574 completed (loss: 0.009838356636464596, acc: 1.0)
[2024-11-13 07:10:58,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:58,876][root][INFO] - Training Epoch: 9/10, step 491/574 completed (loss: 0.044130247086286545, acc: 1.0)
[2024-11-13 07:10:59,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:59,228][root][INFO] - Training Epoch: 9/10, step 492/574 completed (loss: 0.16927719116210938, acc: 0.9803921580314636)
[2024-11-13 07:10:59,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:10:59,620][root][INFO] - Training Epoch: 9/10, step 493/574 completed (loss: 0.006568176671862602, acc: 1.0)
[2024-11-13 07:10:59,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:00,017][root][INFO] - Training Epoch: 9/10, step 494/574 completed (loss: 0.3406532108783722, acc: 0.8421052694320679)
[2024-11-13 07:11:00,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:00,462][root][INFO] - Training Epoch: 9/10, step 495/574 completed (loss: 0.4571154713630676, acc: 0.8947368264198303)
[2024-11-13 07:11:00,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:00,902][root][INFO] - Training Epoch: 9/10, step 496/574 completed (loss: 0.374931663274765, acc: 0.9017857313156128)
[2024-11-13 07:11:01,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:01,388][root][INFO] - Training Epoch: 9/10, step 497/574 completed (loss: 0.24788053333759308, acc: 0.9550561904907227)
[2024-11-13 07:11:01,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:01,844][root][INFO] - Training Epoch: 9/10, step 498/574 completed (loss: 0.31471309065818787, acc: 0.932584285736084)
[2024-11-13 07:11:02,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:02,278][root][INFO] - Training Epoch: 9/10, step 499/574 completed (loss: 0.5452763438224792, acc: 0.8439716100692749)
[2024-11-13 07:11:02,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:02,661][root][INFO] - Training Epoch: 9/10, step 500/574 completed (loss: 0.21181359887123108, acc: 0.9130434989929199)
[2024-11-13 07:11:02,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:03,027][root][INFO] - Training Epoch: 9/10, step 501/574 completed (loss: 0.009213266894221306, acc: 1.0)
[2024-11-13 07:11:03,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:03,465][root][INFO] - Training Epoch: 9/10, step 502/574 completed (loss: 0.010194293223321438, acc: 1.0)
[2024-11-13 07:11:03,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:03,857][root][INFO] - Training Epoch: 9/10, step 503/574 completed (loss: 0.038152579218149185, acc: 1.0)
[2024-11-13 07:11:04,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:04,210][root][INFO] - Training Epoch: 9/10, step 504/574 completed (loss: 0.07146738469600677, acc: 0.9629629850387573)
[2024-11-13 07:11:04,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:04,616][root][INFO] - Training Epoch: 9/10, step 505/574 completed (loss: 0.17945905029773712, acc: 0.9811320900917053)
[2024-11-13 07:11:04,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:04,964][root][INFO] - Training Epoch: 9/10, step 506/574 completed (loss: 0.42036736011505127, acc: 0.8965517282485962)
[2024-11-13 07:11:05,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:05,862][root][INFO] - Training Epoch: 9/10, step 507/574 completed (loss: 0.4846229553222656, acc: 0.8558558821678162)
[2024-11-13 07:11:06,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:06,496][root][INFO] - Training Epoch: 9/10, step 508/574 completed (loss: 0.415326327085495, acc: 0.9014084339141846)
[2024-11-13 07:11:06,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:06,921][root][INFO] - Training Epoch: 9/10, step 509/574 completed (loss: 0.0034149792045354843, acc: 1.0)
[2024-11-13 07:11:07,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:07,280][root][INFO] - Training Epoch: 9/10, step 510/574 completed (loss: 0.30831167101860046, acc: 0.8999999761581421)
[2024-11-13 07:11:07,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:07,665][root][INFO] - Training Epoch: 9/10, step 511/574 completed (loss: 0.013851339928805828, acc: 1.0)
[2024-11-13 07:11:10,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:11,307][root][INFO] - Training Epoch: 9/10, step 512/574 completed (loss: 0.6117351055145264, acc: 0.8428571224212646)
[2024-11-13 07:11:11,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:12,512][root][INFO] - Training Epoch: 9/10, step 513/574 completed (loss: 0.19105970859527588, acc: 0.9285714030265808)
[2024-11-13 07:11:12,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:12,965][root][INFO] - Training Epoch: 9/10, step 514/574 completed (loss: 0.17197655141353607, acc: 0.9285714030265808)
[2024-11-13 07:11:13,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:13,374][root][INFO] - Training Epoch: 9/10, step 515/574 completed (loss: 0.10677165538072586, acc: 0.9666666388511658)
[2024-11-13 07:11:13,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:14,439][root][INFO] - Training Epoch: 9/10, step 516/574 completed (loss: 0.1805037260055542, acc: 0.9444444179534912)
[2024-11-13 07:11:14,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:14,863][root][INFO] - Training Epoch: 9/10, step 517/574 completed (loss: 0.015722669661045074, acc: 1.0)
[2024-11-13 07:11:15,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:15,239][root][INFO] - Training Epoch: 9/10, step 518/574 completed (loss: 0.07550892978906631, acc: 0.9677419066429138)
[2024-11-13 07:11:15,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:15,604][root][INFO] - Training Epoch: 9/10, step 519/574 completed (loss: 0.11055703461170197, acc: 0.949999988079071)
[2024-11-13 07:11:15,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:16,021][root][INFO] - Training Epoch: 9/10, step 520/574 completed (loss: 0.047750696539878845, acc: 1.0)
[2024-11-13 07:11:16,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:17,556][root][INFO] - Training Epoch: 9/10, step 521/574 completed (loss: 0.45839840173721313, acc: 0.8601694703102112)
[2024-11-13 07:11:17,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:18,040][root][INFO] - Training Epoch: 9/10, step 522/574 completed (loss: 0.09564872831106186, acc: 0.9850746393203735)
[2024-11-13 07:11:18,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:18,520][root][INFO] - Training Epoch: 9/10, step 523/574 completed (loss: 0.1595713049173355, acc: 0.9416058659553528)
[2024-11-13 07:11:18,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:19,355][root][INFO] - Training Epoch: 9/10, step 524/574 completed (loss: 0.32773396372795105, acc: 0.9049999713897705)
[2024-11-13 07:11:19,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:19,730][root][INFO] - Training Epoch: 9/10, step 525/574 completed (loss: 0.017572779208421707, acc: 1.0)
[2024-11-13 07:11:19,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:20,136][root][INFO] - Training Epoch: 9/10, step 526/574 completed (loss: 0.08557793498039246, acc: 0.9615384340286255)
[2024-11-13 07:11:20,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:20,503][root][INFO] - Training Epoch: 9/10, step 527/574 completed (loss: 0.036692142486572266, acc: 1.0)
[2024-11-13 07:11:20,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:20,933][root][INFO] - Training Epoch: 9/10, step 528/574 completed (loss: 0.2575506269931793, acc: 0.9180327653884888)
[2024-11-13 07:11:21,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:21,295][root][INFO] - Training Epoch: 9/10, step 529/574 completed (loss: 0.03956705331802368, acc: 1.0)
[2024-11-13 07:11:21,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:21,658][root][INFO] - Training Epoch: 9/10, step 530/574 completed (loss: 0.42717742919921875, acc: 0.8604651093482971)
[2024-11-13 07:11:21,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:22,059][root][INFO] - Training Epoch: 9/10, step 531/574 completed (loss: 0.19521988928318024, acc: 0.9545454382896423)
[2024-11-13 07:11:22,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:22,449][root][INFO] - Training Epoch: 9/10, step 532/574 completed (loss: 0.33415982127189636, acc: 0.9056603908538818)
[2024-11-13 07:11:22,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:22,839][root][INFO] - Training Epoch: 9/10, step 533/574 completed (loss: 0.03701849281787872, acc: 1.0)
[2024-11-13 07:11:22,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:23,197][root][INFO] - Training Epoch: 9/10, step 534/574 completed (loss: 0.06453166157007217, acc: 1.0)
[2024-11-13 07:11:23,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:23,609][root][INFO] - Training Epoch: 9/10, step 535/574 completed (loss: 0.055421195924282074, acc: 1.0)
[2024-11-13 07:11:23,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:24,049][root][INFO] - Training Epoch: 9/10, step 536/574 completed (loss: 0.007647633086889982, acc: 1.0)
[2024-11-13 07:11:24,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:24,590][root][INFO] - Training Epoch: 9/10, step 537/574 completed (loss: 0.1294471174478531, acc: 0.9384615421295166)
[2024-11-13 07:11:24,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:25,029][root][INFO] - Training Epoch: 9/10, step 538/574 completed (loss: 0.1380530148744583, acc: 0.96875)
[2024-11-13 07:11:25,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:25,522][root][INFO] - Training Epoch: 9/10, step 539/574 completed (loss: 0.12744130194187164, acc: 0.96875)
[2024-11-13 07:11:25,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:25,910][root][INFO] - Training Epoch: 9/10, step 540/574 completed (loss: 0.15379321575164795, acc: 0.9696969985961914)
[2024-11-13 07:11:26,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:26,290][root][INFO] - Training Epoch: 9/10, step 541/574 completed (loss: 0.005440997425466776, acc: 1.0)
[2024-11-13 07:11:26,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:26,683][root][INFO] - Training Epoch: 9/10, step 542/574 completed (loss: 0.0023333742283284664, acc: 1.0)
[2024-11-13 07:11:26,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:27,046][root][INFO] - Training Epoch: 9/10, step 543/574 completed (loss: 0.007942491210997105, acc: 1.0)
[2024-11-13 07:11:27,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:27,448][root][INFO] - Training Epoch: 9/10, step 544/574 completed (loss: 0.009807496331632137, acc: 1.0)
[2024-11-13 07:11:27,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:27,857][root][INFO] - Training Epoch: 9/10, step 545/574 completed (loss: 0.020437495782971382, acc: 1.0)
[2024-11-13 07:11:28,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:28,245][root][INFO] - Training Epoch: 9/10, step 546/574 completed (loss: 0.10996650159358978, acc: 0.9714285731315613)
[2024-11-13 07:11:28,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:28,593][root][INFO] - Training Epoch: 9/10, step 547/574 completed (loss: 0.0012090469244867563, acc: 1.0)
[2024-11-13 07:11:28,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:29,018][root][INFO] - Training Epoch: 9/10, step 548/574 completed (loss: 0.0036856962833553553, acc: 1.0)
[2024-11-13 07:11:29,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:29,427][root][INFO] - Training Epoch: 9/10, step 549/574 completed (loss: 0.0016220425022765994, acc: 1.0)
[2024-11-13 07:11:29,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:29,772][root][INFO] - Training Epoch: 9/10, step 550/574 completed (loss: 0.04729927331209183, acc: 0.9696969985961914)
[2024-11-13 07:11:29,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:30,163][root][INFO] - Training Epoch: 9/10, step 551/574 completed (loss: 0.16340592503547668, acc: 0.9750000238418579)
[2024-11-13 07:11:30,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:30,564][root][INFO] - Training Epoch: 9/10, step 552/574 completed (loss: 0.06073752045631409, acc: 0.9857142567634583)
[2024-11-13 07:11:30,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:31,016][root][INFO] - Training Epoch: 9/10, step 553/574 completed (loss: 0.1396222561597824, acc: 0.9416058659553528)
[2024-11-13 07:11:31,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:31,412][root][INFO] - Training Epoch: 9/10, step 554/574 completed (loss: 0.09633120149374008, acc: 0.9793103337287903)
[2024-11-13 07:11:31,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:31,821][root][INFO] - Training Epoch: 9/10, step 555/574 completed (loss: 0.22918181121349335, acc: 0.9285714030265808)
[2024-11-13 07:11:32,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:33,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:33,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:34,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:34,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:35,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:35,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:36,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:36,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:37,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:37,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:38,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:39,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:39,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:40,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:40,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:41,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:41,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:42,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:42,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:43,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:43,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:44,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:44,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:45,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:45,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:46,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:46,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:47,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:47,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:48,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:48,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:49,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:49,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:50,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:50,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:51,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:52,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:52,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:53,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:53,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:54,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:54,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:55,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:55,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:56,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:56,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:57,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:57,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:58,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:58,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:59,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:11:59,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:00,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:00,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:01,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:01,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:02,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:02,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:03,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:03,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:04,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:05,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:05,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:06,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:06,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:07,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:08,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:08,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:09,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:09,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:10,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:10,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:11,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:11,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:12,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:12,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:13,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:13,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:14,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:14,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:15,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:15,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:16,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:16,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:17,491][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5478, device='cuda:0') eval_epoch_loss=tensor(0.9352, device='cuda:0') eval_epoch_acc=tensor(0.8132, device='cuda:0')
[2024-11-13 07:12:17,494][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 07:12:17,495][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 07:12:17,911][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_9_step_556_loss_0.935248076915741/model.pt
[2024-11-13 07:12:17,920][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 07:12:18,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:18,442][root][INFO] - Training Epoch: 9/10, step 556/574 completed (loss: 0.15534467995166779, acc: 0.9602649211883545)
[2024-11-13 07:12:18,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:18,902][root][INFO] - Training Epoch: 9/10, step 557/574 completed (loss: 0.12651526927947998, acc: 0.9658119678497314)
[2024-11-13 07:12:19,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:19,297][root][INFO] - Training Epoch: 9/10, step 558/574 completed (loss: 0.08428845554590225, acc: 0.9599999785423279)
[2024-11-13 07:12:19,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:19,657][root][INFO] - Training Epoch: 9/10, step 559/574 completed (loss: 0.05501699447631836, acc: 0.9615384340286255)
[2024-11-13 07:12:19,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:20,048][root][INFO] - Training Epoch: 9/10, step 560/574 completed (loss: 0.01409903820604086, acc: 1.0)
[2024-11-13 07:12:20,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:20,474][root][INFO] - Training Epoch: 9/10, step 561/574 completed (loss: 0.03229885920882225, acc: 0.9743589758872986)
[2024-11-13 07:12:20,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:20,888][root][INFO] - Training Epoch: 9/10, step 562/574 completed (loss: 0.211410790681839, acc: 0.9333333373069763)
[2024-11-13 07:12:21,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:21,290][root][INFO] - Training Epoch: 9/10, step 563/574 completed (loss: 0.04914706572890282, acc: 0.9870129823684692)
[2024-11-13 07:12:21,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:21,737][root][INFO] - Training Epoch: 9/10, step 564/574 completed (loss: 0.15991798043251038, acc: 0.9375)
[2024-11-13 07:12:21,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:22,143][root][INFO] - Training Epoch: 9/10, step 565/574 completed (loss: 0.07926696538925171, acc: 0.9655172228813171)
[2024-11-13 07:12:22,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:22,572][root][INFO] - Training Epoch: 9/10, step 566/574 completed (loss: 0.06016779690980911, acc: 0.976190447807312)
[2024-11-13 07:12:22,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:23,008][root][INFO] - Training Epoch: 9/10, step 567/574 completed (loss: 0.02308758907020092, acc: 1.0)
[2024-11-13 07:12:23,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:23,425][root][INFO] - Training Epoch: 9/10, step 568/574 completed (loss: 0.10493869334459305, acc: 0.9629629850387573)
[2024-11-13 07:12:23,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:23,937][root][INFO] - Training Epoch: 9/10, step 569/574 completed (loss: 0.09413424134254456, acc: 0.9625668525695801)
[2024-11-13 07:12:24,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:24,429][root][INFO] - Training Epoch: 9/10, step 570/574 completed (loss: 0.1511128693819046, acc: 0.9516128897666931)
[2024-11-13 07:12:24,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:24,922][root][INFO] - Training Epoch: 9/10, step 571/574 completed (loss: 0.07248980551958084, acc: 0.9829059839248657)
[2024-11-13 07:12:25,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:25,298][root][INFO] - Training Epoch: 9/10, step 572/574 completed (loss: 0.26903387904167175, acc: 0.9030612111091614)
[2024-11-13 07:12:25,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:25,730][root][INFO] - Training Epoch: 9/10, step 573/574 completed (loss: 0.21696920692920685, acc: 0.9496855139732361)
[2024-11-13 07:12:26,289][slam_llm.utils.train_utils][INFO] - Epoch 9: train_perplexity=1.1761, train_epoch_loss=0.1622, epoch time 466.5633832011372s
[2024-11-13 07:12:26,289][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2024-11-13 07:12:26,289][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 17 GB
[2024-11-13 07:12:26,289][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2024-11-13 07:12:26,290][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 27
[2024-11-13 07:12:26,290][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-13 07:12:26,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:27,219][root][INFO] - Training Epoch: 10/10, step 0/574 completed (loss: 0.026469644159078598, acc: 1.0)
[2024-11-13 07:12:27,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:27,511][root][INFO] - Training Epoch: 10/10, step 1/574 completed (loss: 0.006730548106133938, acc: 1.0)
[2024-11-13 07:12:27,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:27,899][root][INFO] - Training Epoch: 10/10, step 2/574 completed (loss: 0.06736879050731659, acc: 0.9729729890823364)
[2024-11-13 07:12:28,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:28,333][root][INFO] - Training Epoch: 10/10, step 3/574 completed (loss: 0.12054822593927383, acc: 0.9736841917037964)
[2024-11-13 07:12:28,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:28,724][root][INFO] - Training Epoch: 10/10, step 4/574 completed (loss: 0.05950574576854706, acc: 0.9729729890823364)
[2024-11-13 07:12:28,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:29,156][root][INFO] - Training Epoch: 10/10, step 5/574 completed (loss: 0.09913725405931473, acc: 0.9285714030265808)
[2024-11-13 07:12:29,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:29,572][root][INFO] - Training Epoch: 10/10, step 6/574 completed (loss: 0.0769663080573082, acc: 0.9795918464660645)
[2024-11-13 07:12:29,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:29,984][root][INFO] - Training Epoch: 10/10, step 7/574 completed (loss: 0.025789229199290276, acc: 1.0)
[2024-11-13 07:12:30,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:30,420][root][INFO] - Training Epoch: 10/10, step 8/574 completed (loss: 0.004051131661981344, acc: 1.0)
[2024-11-13 07:12:30,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:30,852][root][INFO] - Training Epoch: 10/10, step 9/574 completed (loss: 0.03420420363545418, acc: 0.9615384340286255)
[2024-11-13 07:12:30,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:31,233][root][INFO] - Training Epoch: 10/10, step 10/574 completed (loss: 0.0020716118160635233, acc: 1.0)
[2024-11-13 07:12:31,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:31,634][root][INFO] - Training Epoch: 10/10, step 11/574 completed (loss: 0.1111459881067276, acc: 0.9487179517745972)
[2024-11-13 07:12:31,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:32,064][root][INFO] - Training Epoch: 10/10, step 12/574 completed (loss: 0.013489559292793274, acc: 1.0)
[2024-11-13 07:12:32,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:32,496][root][INFO] - Training Epoch: 10/10, step 13/574 completed (loss: 0.2042667418718338, acc: 0.97826087474823)
[2024-11-13 07:12:32,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:32,900][root][INFO] - Training Epoch: 10/10, step 14/574 completed (loss: 0.020019397139549255, acc: 1.0)
[2024-11-13 07:12:33,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:33,308][root][INFO] - Training Epoch: 10/10, step 15/574 completed (loss: 0.15126366913318634, acc: 0.9795918464660645)
[2024-11-13 07:12:33,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:33,678][root][INFO] - Training Epoch: 10/10, step 16/574 completed (loss: 0.06986585259437561, acc: 0.9473684430122375)
[2024-11-13 07:12:33,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:34,042][root][INFO] - Training Epoch: 10/10, step 17/574 completed (loss: 0.013554961420595646, acc: 1.0)
[2024-11-13 07:12:34,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:34,415][root][INFO] - Training Epoch: 10/10, step 18/574 completed (loss: 0.02233477309346199, acc: 1.0)
[2024-11-13 07:12:34,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:34,797][root][INFO] - Training Epoch: 10/10, step 19/574 completed (loss: 0.023558298125863075, acc: 1.0)
[2024-11-13 07:12:34,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:35,201][root][INFO] - Training Epoch: 10/10, step 20/574 completed (loss: 0.05255575850605965, acc: 0.9615384340286255)
[2024-11-13 07:12:35,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:35,547][root][INFO] - Training Epoch: 10/10, step 21/574 completed (loss: 0.10443709790706635, acc: 0.9655172228813171)
[2024-11-13 07:12:35,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:35,958][root][INFO] - Training Epoch: 10/10, step 22/574 completed (loss: 0.0563058964908123, acc: 0.9599999785423279)
[2024-11-13 07:12:36,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:36,355][root][INFO] - Training Epoch: 10/10, step 23/574 completed (loss: 0.07030501961708069, acc: 0.9523809552192688)
[2024-11-13 07:12:36,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:36,778][root][INFO] - Training Epoch: 10/10, step 24/574 completed (loss: 0.009321649558842182, acc: 1.0)
[2024-11-13 07:12:36,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:37,216][root][INFO] - Training Epoch: 10/10, step 25/574 completed (loss: 0.11682745069265366, acc: 0.9433962106704712)
[2024-11-13 07:12:37,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:37,656][root][INFO] - Training Epoch: 10/10, step 26/574 completed (loss: 0.12877418100833893, acc: 0.9452054500579834)
[2024-11-13 07:12:38,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:39,458][root][INFO] - Training Epoch: 10/10, step 27/574 completed (loss: 0.4859957695007324, acc: 0.8458498120307922)
[2024-11-13 07:12:39,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:39,890][root][INFO] - Training Epoch: 10/10, step 28/574 completed (loss: 0.11768992245197296, acc: 0.930232584476471)
[2024-11-13 07:12:40,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:40,303][root][INFO] - Training Epoch: 10/10, step 29/574 completed (loss: 0.1650329828262329, acc: 0.9277108311653137)
[2024-11-13 07:12:40,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:40,726][root][INFO] - Training Epoch: 10/10, step 30/574 completed (loss: 0.1479874849319458, acc: 0.9629629850387573)
[2024-11-13 07:12:40,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:41,124][root][INFO] - Training Epoch: 10/10, step 31/574 completed (loss: 0.009662621654570103, acc: 1.0)
[2024-11-13 07:12:41,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:41,534][root][INFO] - Training Epoch: 10/10, step 32/574 completed (loss: 0.3113957941532135, acc: 0.9629629850387573)
[2024-11-13 07:12:41,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:41,942][root][INFO] - Training Epoch: 10/10, step 33/574 completed (loss: 0.0033413798082619905, acc: 1.0)
[2024-11-13 07:12:42,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:42,381][root][INFO] - Training Epoch: 10/10, step 34/574 completed (loss: 0.18073412775993347, acc: 0.9327731132507324)
[2024-11-13 07:12:42,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:42,839][root][INFO] - Training Epoch: 10/10, step 35/574 completed (loss: 0.15517090260982513, acc: 0.9508196711540222)
[2024-11-13 07:12:43,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:43,282][root][INFO] - Training Epoch: 10/10, step 36/574 completed (loss: 0.1432381272315979, acc: 0.9365079402923584)
[2024-11-13 07:12:43,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:43,702][root][INFO] - Training Epoch: 10/10, step 37/574 completed (loss: 0.15237590670585632, acc: 0.9152542352676392)
[2024-11-13 07:12:43,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:44,129][root][INFO] - Training Epoch: 10/10, step 38/574 completed (loss: 0.12787041068077087, acc: 0.977011501789093)
[2024-11-13 07:12:44,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:44,519][root][INFO] - Training Epoch: 10/10, step 39/574 completed (loss: 0.011469765566289425, acc: 1.0)
[2024-11-13 07:12:44,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:44,982][root][INFO] - Training Epoch: 10/10, step 40/574 completed (loss: 0.01885041780769825, acc: 1.0)
[2024-11-13 07:12:45,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:45,479][root][INFO] - Training Epoch: 10/10, step 41/574 completed (loss: 0.13189378380775452, acc: 0.9864864945411682)
[2024-11-13 07:12:45,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:45,945][root][INFO] - Training Epoch: 10/10, step 42/574 completed (loss: 0.07878781110048294, acc: 0.9692307710647583)
[2024-11-13 07:12:46,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:46,514][root][INFO] - Training Epoch: 10/10, step 43/574 completed (loss: 0.13228772580623627, acc: 0.9494949579238892)
[2024-11-13 07:12:46,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:47,096][root][INFO] - Training Epoch: 10/10, step 44/574 completed (loss: 0.13693967461585999, acc: 0.969072163105011)
[2024-11-13 07:12:47,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:47,651][root][INFO] - Training Epoch: 10/10, step 45/574 completed (loss: 0.0631852075457573, acc: 0.9852941036224365)
[2024-11-13 07:12:47,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:48,047][root][INFO] - Training Epoch: 10/10, step 46/574 completed (loss: 0.02902906946837902, acc: 1.0)
[2024-11-13 07:12:48,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:48,420][root][INFO] - Training Epoch: 10/10, step 47/574 completed (loss: 0.008636103011667728, acc: 1.0)
[2024-11-13 07:12:48,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:48,793][root][INFO] - Training Epoch: 10/10, step 48/574 completed (loss: 0.00667556980624795, acc: 1.0)
[2024-11-13 07:12:48,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:49,209][root][INFO] - Training Epoch: 10/10, step 49/574 completed (loss: 0.01755691133439541, acc: 1.0)
[2024-11-13 07:12:49,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:49,670][root][INFO] - Training Epoch: 10/10, step 50/574 completed (loss: 0.24220867455005646, acc: 0.9298245906829834)
[2024-11-13 07:12:49,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:50,106][root][INFO] - Training Epoch: 10/10, step 51/574 completed (loss: 0.1157563179731369, acc: 0.9682539701461792)
[2024-11-13 07:12:50,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:50,532][root][INFO] - Training Epoch: 10/10, step 52/574 completed (loss: 0.20914480090141296, acc: 0.9436619877815247)
[2024-11-13 07:12:50,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:51,173][root][INFO] - Training Epoch: 10/10, step 53/574 completed (loss: 0.5885522961616516, acc: 0.8066666722297668)
[2024-11-13 07:12:51,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:51,564][root][INFO] - Training Epoch: 10/10, step 54/574 completed (loss: 0.029537294059991837, acc: 1.0)
[2024-11-13 07:12:51,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:51,984][root][INFO] - Training Epoch: 10/10, step 55/574 completed (loss: 0.011279717087745667, acc: 1.0)
[2024-11-13 07:12:54,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:56,410][root][INFO] - Training Epoch: 10/10, step 56/574 completed (loss: 0.6867310404777527, acc: 0.7781569957733154)
[2024-11-13 07:12:57,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:58,274][root][INFO] - Training Epoch: 10/10, step 57/574 completed (loss: 0.8742209076881409, acc: 0.7450980544090271)
[2024-11-13 07:12:58,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:12:59,212][root][INFO] - Training Epoch: 10/10, step 58/574 completed (loss: 0.31078505516052246, acc: 0.8977272510528564)
[2024-11-13 07:12:59,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:00,068][root][INFO] - Training Epoch: 10/10, step 59/574 completed (loss: 0.111502505838871, acc: 0.970588207244873)
[2024-11-13 07:13:00,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:00,906][root][INFO] - Training Epoch: 10/10, step 60/574 completed (loss: 0.4402998089790344, acc: 0.8623188138008118)
[2024-11-13 07:13:01,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:01,476][root][INFO] - Training Epoch: 10/10, step 61/574 completed (loss: 0.16519087553024292, acc: 0.9375)
[2024-11-13 07:13:01,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:01,886][root][INFO] - Training Epoch: 10/10, step 62/574 completed (loss: 0.03625449538230896, acc: 0.970588207244873)
[2024-11-13 07:13:02,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:02,326][root][INFO] - Training Epoch: 10/10, step 63/574 completed (loss: 0.024456726387143135, acc: 1.0)
[2024-11-13 07:13:02,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:02,757][root][INFO] - Training Epoch: 10/10, step 64/574 completed (loss: 0.021008353680372238, acc: 1.0)
[2024-11-13 07:13:02,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:03,140][root][INFO] - Training Epoch: 10/10, step 65/574 completed (loss: 0.015845943242311478, acc: 1.0)
[2024-11-13 07:13:03,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:03,560][root][INFO] - Training Epoch: 10/10, step 66/574 completed (loss: 0.07778342068195343, acc: 0.9821428656578064)
[2024-11-13 07:13:03,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:03,970][root][INFO] - Training Epoch: 10/10, step 67/574 completed (loss: 0.052442409098148346, acc: 0.9666666388511658)
[2024-11-13 07:13:04,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:04,397][root][INFO] - Training Epoch: 10/10, step 68/574 completed (loss: 0.25230783224105835, acc: 0.9599999785423279)
[2024-11-13 07:13:04,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:04,807][root][INFO] - Training Epoch: 10/10, step 69/574 completed (loss: 0.29736095666885376, acc: 0.9722222089767456)
[2024-11-13 07:13:04,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:05,255][root][INFO] - Training Epoch: 10/10, step 70/574 completed (loss: 0.01790804974734783, acc: 1.0)
[2024-11-13 07:13:05,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:05,669][root][INFO] - Training Epoch: 10/10, step 71/574 completed (loss: 0.34041815996170044, acc: 0.8897058963775635)
[2024-11-13 07:13:05,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:06,094][root][INFO] - Training Epoch: 10/10, step 72/574 completed (loss: 0.31346291303634644, acc: 0.9126983880996704)
[2024-11-13 07:13:06,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:06,476][root][INFO] - Training Epoch: 10/10, step 73/574 completed (loss: 0.6801785826683044, acc: 0.7948718070983887)
[2024-11-13 07:13:06,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:06,910][root][INFO] - Training Epoch: 10/10, step 74/574 completed (loss: 0.335893839597702, acc: 0.8673469424247742)
[2024-11-13 07:13:07,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:07,304][root][INFO] - Training Epoch: 10/10, step 75/574 completed (loss: 0.3786015808582306, acc: 0.9104477763175964)
[2024-11-13 07:13:07,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:07,808][root][INFO] - Training Epoch: 10/10, step 76/574 completed (loss: 0.8993227481842041, acc: 0.7518247961997986)
[2024-11-13 07:13:07,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:08,171][root][INFO] - Training Epoch: 10/10, step 77/574 completed (loss: 0.0019794288091361523, acc: 1.0)
[2024-11-13 07:13:08,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:08,540][root][INFO] - Training Epoch: 10/10, step 78/574 completed (loss: 0.0036690400447696447, acc: 1.0)
[2024-11-13 07:13:08,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:08,981][root][INFO] - Training Epoch: 10/10, step 79/574 completed (loss: 0.005620159674435854, acc: 1.0)
[2024-11-13 07:13:09,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:09,396][root][INFO] - Training Epoch: 10/10, step 80/574 completed (loss: 0.001277961884625256, acc: 1.0)
[2024-11-13 07:13:09,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:09,794][root][INFO] - Training Epoch: 10/10, step 81/574 completed (loss: 0.04117349535226822, acc: 1.0)
[2024-11-13 07:13:09,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:10,159][root][INFO] - Training Epoch: 10/10, step 82/574 completed (loss: 0.0836179107427597, acc: 0.9615384340286255)
[2024-11-13 07:13:10,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:10,552][root][INFO] - Training Epoch: 10/10, step 83/574 completed (loss: 0.11940982192754745, acc: 0.9375)
[2024-11-13 07:13:10,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:10,958][root][INFO] - Training Epoch: 10/10, step 84/574 completed (loss: 0.042368438094854355, acc: 1.0)
[2024-11-13 07:13:11,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:11,389][root][INFO] - Training Epoch: 10/10, step 85/574 completed (loss: 0.04625360667705536, acc: 1.0)
[2024-11-13 07:13:11,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:11,817][root][INFO] - Training Epoch: 10/10, step 86/574 completed (loss: 0.04634014889597893, acc: 0.95652174949646)
[2024-11-13 07:13:12,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:12,478][root][INFO] - Training Epoch: 10/10, step 87/574 completed (loss: 0.06941373646259308, acc: 1.0)
[2024-11-13 07:13:12,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:12,915][root][INFO] - Training Epoch: 10/10, step 88/574 completed (loss: 0.2763991355895996, acc: 0.893203854560852)
[2024-11-13 07:13:13,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:14,622][root][INFO] - Training Epoch: 10/10, step 89/574 completed (loss: 0.4811604619026184, acc: 0.8543689250946045)
[2024-11-13 07:13:15,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:15,901][root][INFO] - Training Epoch: 10/10, step 90/574 completed (loss: 0.5312215089797974, acc: 0.8333333134651184)
[2024-11-13 07:13:16,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:17,152][root][INFO] - Training Epoch: 10/10, step 91/574 completed (loss: 0.45811179280281067, acc: 0.8362069129943848)
[2024-11-13 07:13:17,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:18,306][root][INFO] - Training Epoch: 10/10, step 92/574 completed (loss: 0.27897652983665466, acc: 0.9368420839309692)
[2024-11-13 07:13:19,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:19,921][root][INFO] - Training Epoch: 10/10, step 93/574 completed (loss: 0.3347473442554474, acc: 0.9207921028137207)
[2024-11-13 07:13:20,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:20,349][root][INFO] - Training Epoch: 10/10, step 94/574 completed (loss: 0.23928995430469513, acc: 0.9193548560142517)
[2024-11-13 07:13:20,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:20,782][root][INFO] - Training Epoch: 10/10, step 95/574 completed (loss: 0.0684719830751419, acc: 0.9710144996643066)
[2024-11-13 07:13:20,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:21,155][root][INFO] - Training Epoch: 10/10, step 96/574 completed (loss: 0.3836246132850647, acc: 0.8571428656578064)
[2024-11-13 07:13:21,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:21,550][root][INFO] - Training Epoch: 10/10, step 97/574 completed (loss: 0.28207147121429443, acc: 0.8942307829856873)
[2024-11-13 07:13:21,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:22,048][root][INFO] - Training Epoch: 10/10, step 98/574 completed (loss: 0.3081458508968353, acc: 0.8832116723060608)
[2024-11-13 07:13:22,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:22,428][root][INFO] - Training Epoch: 10/10, step 99/574 completed (loss: 0.2696896493434906, acc: 0.9253731369972229)
[2024-11-13 07:13:22,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:22,865][root][INFO] - Training Epoch: 10/10, step 100/574 completed (loss: 0.5816186666488647, acc: 0.949999988079071)
[2024-11-13 07:13:23,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:23,230][root][INFO] - Training Epoch: 10/10, step 101/574 completed (loss: 0.005005519837141037, acc: 1.0)
[2024-11-13 07:13:23,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:23,662][root][INFO] - Training Epoch: 10/10, step 102/574 completed (loss: 0.0014938521198928356, acc: 1.0)
[2024-11-13 07:13:23,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:24,034][root][INFO] - Training Epoch: 10/10, step 103/574 completed (loss: 0.027675040066242218, acc: 0.9772727489471436)
[2024-11-13 07:13:24,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:24,445][root][INFO] - Training Epoch: 10/10, step 104/574 completed (loss: 0.052415356040000916, acc: 0.982758641242981)
[2024-11-13 07:13:24,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:24,777][root][INFO] - Training Epoch: 10/10, step 105/574 completed (loss: 0.17502520978450775, acc: 0.9767441749572754)
[2024-11-13 07:13:24,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:25,173][root][INFO] - Training Epoch: 10/10, step 106/574 completed (loss: 0.01919868402183056, acc: 1.0)
[2024-11-13 07:13:25,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:25,577][root][INFO] - Training Epoch: 10/10, step 107/574 completed (loss: 0.0049898140132427216, acc: 1.0)
[2024-11-13 07:13:25,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:26,029][root][INFO] - Training Epoch: 10/10, step 108/574 completed (loss: 0.012564494274556637, acc: 1.0)
[2024-11-13 07:13:26,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:26,423][root][INFO] - Training Epoch: 10/10, step 109/574 completed (loss: 0.03883497416973114, acc: 1.0)
[2024-11-13 07:13:26,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:26,917][root][INFO] - Training Epoch: 10/10, step 110/574 completed (loss: 0.016141267493367195, acc: 1.0)
[2024-11-13 07:13:27,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:27,469][root][INFO] - Training Epoch: 10/10, step 111/574 completed (loss: 0.02367125265300274, acc: 1.0)
[2024-11-13 07:13:27,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:27,976][root][INFO] - Training Epoch: 10/10, step 112/574 completed (loss: 0.21736818552017212, acc: 0.9473684430122375)
[2024-11-13 07:13:28,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:28,412][root][INFO] - Training Epoch: 10/10, step 113/574 completed (loss: 0.06391595304012299, acc: 1.0)
[2024-11-13 07:13:28,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:28,913][root][INFO] - Training Epoch: 10/10, step 114/574 completed (loss: 0.11034949123859406, acc: 0.9591836929321289)
[2024-11-13 07:13:29,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:29,343][root][INFO] - Training Epoch: 10/10, step 115/574 completed (loss: 0.06621555238962173, acc: 0.9545454382896423)
[2024-11-13 07:13:29,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:29,796][root][INFO] - Training Epoch: 10/10, step 116/574 completed (loss: 0.06285078823566437, acc: 0.9841269850730896)
[2024-11-13 07:13:29,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:30,180][root][INFO] - Training Epoch: 10/10, step 117/574 completed (loss: 0.14360922574996948, acc: 0.9512194991111755)
[2024-11-13 07:13:30,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:30,558][root][INFO] - Training Epoch: 10/10, step 118/574 completed (loss: 0.043227456510066986, acc: 0.9838709831237793)
[2024-11-13 07:13:31,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:31,839][root][INFO] - Training Epoch: 10/10, step 119/574 completed (loss: 0.4467551112174988, acc: 0.8859315514564514)
[2024-11-13 07:13:32,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:32,242][root][INFO] - Training Epoch: 10/10, step 120/574 completed (loss: 0.01252161804586649, acc: 1.0)
[2024-11-13 07:13:32,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:32,801][root][INFO] - Training Epoch: 10/10, step 121/574 completed (loss: 0.14667293429374695, acc: 0.9615384340286255)
[2024-11-13 07:13:32,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:33,160][root][INFO] - Training Epoch: 10/10, step 122/574 completed (loss: 0.0029848364647477865, acc: 1.0)
[2024-11-13 07:13:33,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:33,586][root][INFO] - Training Epoch: 10/10, step 123/574 completed (loss: 0.0412185862660408, acc: 1.0)
[2024-11-13 07:13:33,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:34,004][root][INFO] - Training Epoch: 10/10, step 124/574 completed (loss: 0.28791600465774536, acc: 0.89570552110672)
[2024-11-13 07:13:35,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:35,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:36,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:36,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:37,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:37,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:38,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:38,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:39,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:39,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:40,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:40,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:41,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:41,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:42,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:42,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:43,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:43,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:44,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:44,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:45,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:45,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:46,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:46,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:47,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:47,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:48,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:48,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:49,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:49,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:50,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:50,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:50,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:51,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:51,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:52,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:53,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:53,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:53,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:54,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:54,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:55,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:55,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:55,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:56,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:56,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:57,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:57,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:58,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:58,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:13:59,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:00,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:00,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:00,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:01,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:01,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:02,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:02,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:03,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:03,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:04,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:04,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:05,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:05,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:06,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:06,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:07,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:07,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:08,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:08,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:09,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:09,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:10,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:10,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:11,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:11,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:12,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:12,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:13,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:13,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:14,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:14,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:15,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:15,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:16,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:16,939][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3981, device='cuda:0') eval_epoch_loss=tensor(0.8747, device='cuda:0') eval_epoch_acc=tensor(0.8190, device='cuda:0')
[2024-11-13 07:14:16,941][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 07:14:16,941][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 07:14:17,601][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_125_loss_0.874660313129425/model.pt
[2024-11-13 07:14:17,606][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 07:14:17,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:18,105][root][INFO] - Training Epoch: 10/10, step 125/574 completed (loss: 0.46708419919013977, acc: 0.8472222089767456)
[2024-11-13 07:14:18,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:18,550][root][INFO] - Training Epoch: 10/10, step 126/574 completed (loss: 0.3950890898704529, acc: 0.8416666388511658)
[2024-11-13 07:14:18,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:18,984][root][INFO] - Training Epoch: 10/10, step 127/574 completed (loss: 0.3776317834854126, acc: 0.8690476417541504)
[2024-11-13 07:14:19,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:19,359][root][INFO] - Training Epoch: 10/10, step 128/574 completed (loss: 0.2500954866409302, acc: 0.8974359035491943)
[2024-11-13 07:14:19,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:19,885][root][INFO] - Training Epoch: 10/10, step 129/574 completed (loss: 0.42518556118011475, acc: 0.875)
[2024-11-13 07:14:20,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:20,207][root][INFO] - Training Epoch: 10/10, step 130/574 completed (loss: 0.18833144009113312, acc: 0.9615384340286255)
[2024-11-13 07:14:20,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:20,605][root][INFO] - Training Epoch: 10/10, step 131/574 completed (loss: 0.023162005469202995, acc: 1.0)
[2024-11-13 07:14:20,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:21,021][root][INFO] - Training Epoch: 10/10, step 132/574 completed (loss: 0.22027869522571564, acc: 0.90625)
[2024-11-13 07:14:21,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:21,415][root][INFO] - Training Epoch: 10/10, step 133/574 completed (loss: 0.04592638090252876, acc: 0.95652174949646)
[2024-11-13 07:14:21,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:21,773][root][INFO] - Training Epoch: 10/10, step 134/574 completed (loss: 0.07685232907533646, acc: 0.9714285731315613)
[2024-11-13 07:14:21,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:22,178][root][INFO] - Training Epoch: 10/10, step 135/574 completed (loss: 0.21180447936058044, acc: 0.9230769276618958)
[2024-11-13 07:14:22,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:22,561][root][INFO] - Training Epoch: 10/10, step 136/574 completed (loss: 0.1102953851222992, acc: 0.976190447807312)
[2024-11-13 07:14:22,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:22,922][root][INFO] - Training Epoch: 10/10, step 137/574 completed (loss: 0.11758614331483841, acc: 0.9666666388511658)
[2024-11-13 07:14:23,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:23,327][root][INFO] - Training Epoch: 10/10, step 138/574 completed (loss: 0.012328160926699638, acc: 1.0)
[2024-11-13 07:14:23,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:23,732][root][INFO] - Training Epoch: 10/10, step 139/574 completed (loss: 0.09024442732334137, acc: 0.9523809552192688)
[2024-11-13 07:14:23,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:24,094][root][INFO] - Training Epoch: 10/10, step 140/574 completed (loss: 0.07328996807336807, acc: 0.9615384340286255)
[2024-11-13 07:14:24,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:24,516][root][INFO] - Training Epoch: 10/10, step 141/574 completed (loss: 0.14730322360992432, acc: 0.9677419066429138)
[2024-11-13 07:14:24,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:24,846][root][INFO] - Training Epoch: 10/10, step 142/574 completed (loss: 0.14324207603931427, acc: 0.9189189076423645)
[2024-11-13 07:14:25,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:25,620][root][INFO] - Training Epoch: 10/10, step 143/574 completed (loss: 0.23290494084358215, acc: 0.9210526347160339)
[2024-11-13 07:14:25,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:26,054][root][INFO] - Training Epoch: 10/10, step 144/574 completed (loss: 0.1966942697763443, acc: 0.9253731369972229)
[2024-11-13 07:14:26,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:26,509][root][INFO] - Training Epoch: 10/10, step 145/574 completed (loss: 0.16246362030506134, acc: 0.9693877696990967)
[2024-11-13 07:14:26,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:27,154][root][INFO] - Training Epoch: 10/10, step 146/574 completed (loss: 0.2056737095117569, acc: 0.8936170339584351)
[2024-11-13 07:14:27,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:27,540][root][INFO] - Training Epoch: 10/10, step 147/574 completed (loss: 0.17093876004219055, acc: 0.9428571462631226)
[2024-11-13 07:14:27,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:27,953][root][INFO] - Training Epoch: 10/10, step 148/574 completed (loss: 0.05225306376814842, acc: 0.9642857313156128)
[2024-11-13 07:14:28,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:28,333][root][INFO] - Training Epoch: 10/10, step 149/574 completed (loss: 0.05684304237365723, acc: 1.0)
[2024-11-13 07:14:28,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:28,724][root][INFO] - Training Epoch: 10/10, step 150/574 completed (loss: 0.20418375730514526, acc: 0.9655172228813171)
[2024-11-13 07:14:28,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:29,139][root][INFO] - Training Epoch: 10/10, step 151/574 completed (loss: 0.08184683322906494, acc: 0.97826087474823)
[2024-11-13 07:14:29,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:29,555][root][INFO] - Training Epoch: 10/10, step 152/574 completed (loss: 0.12308944016695023, acc: 0.9491525292396545)
[2024-11-13 07:14:29,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:29,974][root][INFO] - Training Epoch: 10/10, step 153/574 completed (loss: 0.2796999514102936, acc: 0.9122806787490845)
[2024-11-13 07:14:30,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:30,352][root][INFO] - Training Epoch: 10/10, step 154/574 completed (loss: 0.1785607486963272, acc: 0.9459459185600281)
[2024-11-13 07:14:30,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:30,752][root][INFO] - Training Epoch: 10/10, step 155/574 completed (loss: 0.4304232895374298, acc: 0.9642857313156128)
[2024-11-13 07:14:30,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:31,197][root][INFO] - Training Epoch: 10/10, step 156/574 completed (loss: 0.043035391718149185, acc: 0.95652174949646)
[2024-11-13 07:14:31,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:31,604][root][INFO] - Training Epoch: 10/10, step 157/574 completed (loss: 0.13456948101520538, acc: 1.0)
[2024-11-13 07:14:33,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:34,225][root][INFO] - Training Epoch: 10/10, step 158/574 completed (loss: 0.4703781008720398, acc: 0.8648648858070374)
[2024-11-13 07:14:34,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:34,657][root][INFO] - Training Epoch: 10/10, step 159/574 completed (loss: 0.35364022850990295, acc: 0.8518518805503845)
[2024-11-13 07:14:34,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:35,247][root][INFO] - Training Epoch: 10/10, step 160/574 completed (loss: 0.3119749128818512, acc: 0.8837209343910217)
[2024-11-13 07:14:35,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:36,154][root][INFO] - Training Epoch: 10/10, step 161/574 completed (loss: 0.3509155809879303, acc: 0.8588235378265381)
[2024-11-13 07:14:36,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:37,007][root][INFO] - Training Epoch: 10/10, step 162/574 completed (loss: 0.5450623035430908, acc: 0.8426966071128845)
[2024-11-13 07:14:37,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:37,465][root][INFO] - Training Epoch: 10/10, step 163/574 completed (loss: 0.03409641608595848, acc: 1.0)
[2024-11-13 07:14:37,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:37,796][root][INFO] - Training Epoch: 10/10, step 164/574 completed (loss: 0.036332547664642334, acc: 1.0)
[2024-11-13 07:14:37,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:38,197][root][INFO] - Training Epoch: 10/10, step 165/574 completed (loss: 0.1889272928237915, acc: 0.931034505367279)
[2024-11-13 07:14:38,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:38,612][root][INFO] - Training Epoch: 10/10, step 166/574 completed (loss: 0.011152319610118866, acc: 1.0)
[2024-11-13 07:14:38,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:39,049][root][INFO] - Training Epoch: 10/10, step 167/574 completed (loss: 0.0517551526427269, acc: 0.9800000190734863)
[2024-11-13 07:14:39,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:39,589][root][INFO] - Training Epoch: 10/10, step 168/574 completed (loss: 0.12080714106559753, acc: 0.9583333134651184)
[2024-11-13 07:14:39,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:39,995][root][INFO] - Training Epoch: 10/10, step 169/574 completed (loss: 0.39867472648620605, acc: 0.8725489974021912)
[2024-11-13 07:14:40,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:41,662][root][INFO] - Training Epoch: 10/10, step 170/574 completed (loss: 0.23788295686244965, acc: 0.9520547986030579)
[2024-11-13 07:14:41,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:42,057][root][INFO] - Training Epoch: 10/10, step 171/574 completed (loss: 0.005758295301347971, acc: 1.0)
[2024-11-13 07:14:42,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:42,435][root][INFO] - Training Epoch: 10/10, step 172/574 completed (loss: 0.02090507559478283, acc: 1.0)
[2024-11-13 07:14:42,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:42,766][root][INFO] - Training Epoch: 10/10, step 173/574 completed (loss: 0.04832826182246208, acc: 0.9642857313156128)
[2024-11-13 07:14:43,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:43,568][root][INFO] - Training Epoch: 10/10, step 174/574 completed (loss: 0.4894088804721832, acc: 0.8584070801734924)
[2024-11-13 07:14:43,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:43,990][root][INFO] - Training Epoch: 10/10, step 175/574 completed (loss: 0.15059806406497955, acc: 0.95652174949646)
[2024-11-13 07:14:44,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:44,362][root][INFO] - Training Epoch: 10/10, step 176/574 completed (loss: 0.08606770634651184, acc: 0.9545454382896423)
[2024-11-13 07:14:45,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:45,814][root][INFO] - Training Epoch: 10/10, step 177/574 completed (loss: 0.6067542433738708, acc: 0.8015267252922058)
[2024-11-13 07:14:46,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:46,851][root][INFO] - Training Epoch: 10/10, step 178/574 completed (loss: 0.45436394214630127, acc: 0.8518518805503845)
[2024-11-13 07:14:46,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:47,227][root][INFO] - Training Epoch: 10/10, step 179/574 completed (loss: 0.20936374366283417, acc: 0.9508196711540222)
[2024-11-13 07:14:47,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:47,588][root][INFO] - Training Epoch: 10/10, step 180/574 completed (loss: 0.002547112526372075, acc: 1.0)
[2024-11-13 07:14:47,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:48,039][root][INFO] - Training Epoch: 10/10, step 181/574 completed (loss: 0.0015677022747695446, acc: 1.0)
[2024-11-13 07:14:48,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:48,466][root][INFO] - Training Epoch: 10/10, step 182/574 completed (loss: 0.025640759617090225, acc: 1.0)
[2024-11-13 07:14:48,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:48,860][root][INFO] - Training Epoch: 10/10, step 183/574 completed (loss: 0.04294496402144432, acc: 0.9878048896789551)
[2024-11-13 07:14:49,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:49,325][root][INFO] - Training Epoch: 10/10, step 184/574 completed (loss: 0.28024283051490784, acc: 0.939577043056488)
[2024-11-13 07:14:49,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:49,776][root][INFO] - Training Epoch: 10/10, step 185/574 completed (loss: 0.3441114127635956, acc: 0.890489935874939)
[2024-11-13 07:14:50,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:50,470][root][INFO] - Training Epoch: 10/10, step 186/574 completed (loss: 0.3206577003002167, acc: 0.887499988079071)
[2024-11-13 07:14:50,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:51,208][root][INFO] - Training Epoch: 10/10, step 187/574 completed (loss: 0.4578036963939667, acc: 0.8592870831489563)
[2024-11-13 07:14:51,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:51,741][root][INFO] - Training Epoch: 10/10, step 188/574 completed (loss: 0.29581311345100403, acc: 0.9110320210456848)
[2024-11-13 07:14:51,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:52,114][root][INFO] - Training Epoch: 10/10, step 189/574 completed (loss: 0.009836659766733646, acc: 1.0)
[2024-11-13 07:14:52,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:52,944][root][INFO] - Training Epoch: 10/10, step 190/574 completed (loss: 0.15635766088962555, acc: 0.9534883499145508)
[2024-11-13 07:14:53,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:54,220][root][INFO] - Training Epoch: 10/10, step 191/574 completed (loss: 0.5246211886405945, acc: 0.817460298538208)
[2024-11-13 07:14:55,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:55,726][root][INFO] - Training Epoch: 10/10, step 192/574 completed (loss: 0.542334258556366, acc: 0.8484848737716675)
[2024-11-13 07:14:56,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:56,904][root][INFO] - Training Epoch: 10/10, step 193/574 completed (loss: 0.11593430489301682, acc: 0.9647058844566345)
[2024-11-13 07:14:57,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:14:58,660][root][INFO] - Training Epoch: 10/10, step 194/574 completed (loss: 0.46004176139831543, acc: 0.8703703880310059)
[2024-11-13 07:14:59,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:00,204][root][INFO] - Training Epoch: 10/10, step 195/574 completed (loss: 0.09965957701206207, acc: 0.9516128897666931)
[2024-11-13 07:15:00,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:00,534][root][INFO] - Training Epoch: 10/10, step 196/574 completed (loss: 0.00287686032243073, acc: 1.0)
[2024-11-13 07:15:00,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:00,876][root][INFO] - Training Epoch: 10/10, step 197/574 completed (loss: 0.07465822249650955, acc: 0.949999988079071)
[2024-11-13 07:15:01,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:01,249][root][INFO] - Training Epoch: 10/10, step 198/574 completed (loss: 0.2965643107891083, acc: 0.9264705777168274)
[2024-11-13 07:15:01,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:01,628][root][INFO] - Training Epoch: 10/10, step 199/574 completed (loss: 0.3398389518260956, acc: 0.875)
[2024-11-13 07:15:01,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:02,029][root][INFO] - Training Epoch: 10/10, step 200/574 completed (loss: 0.2409668266773224, acc: 0.8983050584793091)
[2024-11-13 07:15:02,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:02,411][root][INFO] - Training Epoch: 10/10, step 201/574 completed (loss: 0.2495216578245163, acc: 0.9179104566574097)
[2024-11-13 07:15:02,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:02,828][root][INFO] - Training Epoch: 10/10, step 202/574 completed (loss: 0.19721557199954987, acc: 0.9320388436317444)
[2024-11-13 07:15:02,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:03,183][root][INFO] - Training Epoch: 10/10, step 203/574 completed (loss: 0.35074564814567566, acc: 0.8888888955116272)
[2024-11-13 07:15:03,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:03,538][root][INFO] - Training Epoch: 10/10, step 204/574 completed (loss: 0.1086268499493599, acc: 0.9670329689979553)
[2024-11-13 07:15:03,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:03,996][root][INFO] - Training Epoch: 10/10, step 205/574 completed (loss: 0.15168115496635437, acc: 0.9461883306503296)
[2024-11-13 07:15:04,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:04,549][root][INFO] - Training Epoch: 10/10, step 206/574 completed (loss: 0.17143072187900543, acc: 0.9448819160461426)
[2024-11-13 07:15:04,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:04,991][root][INFO] - Training Epoch: 10/10, step 207/574 completed (loss: 0.1638917326927185, acc: 0.943965494632721)
[2024-11-13 07:15:05,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:05,430][root][INFO] - Training Epoch: 10/10, step 208/574 completed (loss: 0.22622691094875336, acc: 0.9239130616188049)
[2024-11-13 07:15:05,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:05,887][root][INFO] - Training Epoch: 10/10, step 209/574 completed (loss: 0.1887146234512329, acc: 0.9494163393974304)
[2024-11-13 07:15:06,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:06,270][root][INFO] - Training Epoch: 10/10, step 210/574 completed (loss: 0.04508502036333084, acc: 1.0)
[2024-11-13 07:15:06,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:06,616][root][INFO] - Training Epoch: 10/10, step 211/574 completed (loss: 0.016482705250382423, acc: 1.0)
[2024-11-13 07:15:06,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:07,022][root][INFO] - Training Epoch: 10/10, step 212/574 completed (loss: 0.05135953053832054, acc: 0.9642857313156128)
[2024-11-13 07:15:07,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:07,424][root][INFO] - Training Epoch: 10/10, step 213/574 completed (loss: 0.06477580964565277, acc: 0.978723406791687)
[2024-11-13 07:15:07,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:08,469][root][INFO] - Training Epoch: 10/10, step 214/574 completed (loss: 0.12044055759906769, acc: 0.9615384340286255)
[2024-11-13 07:15:08,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:08,897][root][INFO] - Training Epoch: 10/10, step 215/574 completed (loss: 0.05510077625513077, acc: 0.9864864945411682)
[2024-11-13 07:15:09,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:09,325][root][INFO] - Training Epoch: 10/10, step 216/574 completed (loss: 0.020616788417100906, acc: 1.0)
[2024-11-13 07:15:09,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:10,123][root][INFO] - Training Epoch: 10/10, step 217/574 completed (loss: 0.09237086772918701, acc: 0.9639639854431152)
[2024-11-13 07:15:10,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:10,664][root][INFO] - Training Epoch: 10/10, step 218/574 completed (loss: 0.0957772359251976, acc: 0.9777777791023254)
[2024-11-13 07:15:10,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:11,025][root][INFO] - Training Epoch: 10/10, step 219/574 completed (loss: 0.0073688095435500145, acc: 1.0)
[2024-11-13 07:15:11,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:11,402][root][INFO] - Training Epoch: 10/10, step 220/574 completed (loss: 0.003143772715702653, acc: 1.0)
[2024-11-13 07:15:11,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:11,745][root][INFO] - Training Epoch: 10/10, step 221/574 completed (loss: 0.007139463908970356, acc: 1.0)
[2024-11-13 07:15:11,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:12,127][root][INFO] - Training Epoch: 10/10, step 222/574 completed (loss: 0.14545883238315582, acc: 0.9615384340286255)
[2024-11-13 07:15:12,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:13,302][root][INFO] - Training Epoch: 10/10, step 223/574 completed (loss: 0.14552877843379974, acc: 0.95652174949646)
[2024-11-13 07:15:13,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:14,118][root][INFO] - Training Epoch: 10/10, step 224/574 completed (loss: 0.31548550724983215, acc: 0.8920454382896423)
[2024-11-13 07:15:14,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:14,744][root][INFO] - Training Epoch: 10/10, step 225/574 completed (loss: 0.2770799398422241, acc: 0.9255319237709045)
[2024-11-13 07:15:14,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:15,192][root][INFO] - Training Epoch: 10/10, step 226/574 completed (loss: 0.08784531056880951, acc: 0.9811320900917053)
[2024-11-13 07:15:15,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:15,611][root][INFO] - Training Epoch: 10/10, step 227/574 completed (loss: 0.11557204276323318, acc: 0.9833333492279053)
[2024-11-13 07:15:15,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:16,012][root][INFO] - Training Epoch: 10/10, step 228/574 completed (loss: 0.07857294380664825, acc: 0.9767441749572754)
[2024-11-13 07:15:16,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:16,363][root][INFO] - Training Epoch: 10/10, step 229/574 completed (loss: 0.22063986957073212, acc: 0.9666666388511658)
[2024-11-13 07:15:16,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:16,845][root][INFO] - Training Epoch: 10/10, step 230/574 completed (loss: 0.5443857908248901, acc: 0.8315789699554443)
[2024-11-13 07:15:17,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:17,270][root][INFO] - Training Epoch: 10/10, step 231/574 completed (loss: 0.6817781925201416, acc: 0.7777777910232544)
[2024-11-13 07:15:17,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:17,863][root][INFO] - Training Epoch: 10/10, step 232/574 completed (loss: 0.5624410510063171, acc: 0.8333333134651184)
[2024-11-13 07:15:18,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:18,590][root][INFO] - Training Epoch: 10/10, step 233/574 completed (loss: 0.8866714835166931, acc: 0.752293586730957)
[2024-11-13 07:15:18,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:19,339][root][INFO] - Training Epoch: 10/10, step 234/574 completed (loss: 0.5170942544937134, acc: 0.8307692408561707)
[2024-11-13 07:15:19,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:19,723][root][INFO] - Training Epoch: 10/10, step 235/574 completed (loss: 0.03724009916186333, acc: 1.0)
[2024-11-13 07:15:19,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:20,109][root][INFO] - Training Epoch: 10/10, step 236/574 completed (loss: 0.004300374072045088, acc: 1.0)
[2024-11-13 07:15:20,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:20,466][root][INFO] - Training Epoch: 10/10, step 237/574 completed (loss: 0.11761601269245148, acc: 0.9545454382896423)
[2024-11-13 07:15:20,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:20,822][root][INFO] - Training Epoch: 10/10, step 238/574 completed (loss: 0.15032395720481873, acc: 0.9629629850387573)
[2024-11-13 07:15:20,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:21,174][root][INFO] - Training Epoch: 10/10, step 239/574 completed (loss: 0.10420627146959305, acc: 0.9428571462631226)
[2024-11-13 07:15:21,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:21,557][root][INFO] - Training Epoch: 10/10, step 240/574 completed (loss: 0.10942905396223068, acc: 0.9318181872367859)
[2024-11-13 07:15:21,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:21,911][root][INFO] - Training Epoch: 10/10, step 241/574 completed (loss: 0.22016964852809906, acc: 0.9090909361839294)
[2024-11-13 07:15:22,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:22,799][root][INFO] - Training Epoch: 10/10, step 242/574 completed (loss: 0.23505514860153198, acc: 0.9032257795333862)
[2024-11-13 07:15:23,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:23,608][root][INFO] - Training Epoch: 10/10, step 243/574 completed (loss: 0.17579057812690735, acc: 0.9318181872367859)
[2024-11-13 07:15:23,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:23,953][root][INFO] - Training Epoch: 10/10, step 244/574 completed (loss: 0.000844739843159914, acc: 1.0)
[2024-11-13 07:15:24,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:24,295][root][INFO] - Training Epoch: 10/10, step 245/574 completed (loss: 0.2471027672290802, acc: 0.9615384340286255)
[2024-11-13 07:15:24,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:24,638][root][INFO] - Training Epoch: 10/10, step 246/574 completed (loss: 0.0029295131098479033, acc: 1.0)
[2024-11-13 07:15:24,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:25,028][root][INFO] - Training Epoch: 10/10, step 247/574 completed (loss: 0.007237593177706003, acc: 1.0)
[2024-11-13 07:15:25,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:25,483][root][INFO] - Training Epoch: 10/10, step 248/574 completed (loss: 0.044232167303562164, acc: 0.9729729890823364)
[2024-11-13 07:15:25,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:25,836][root][INFO] - Training Epoch: 10/10, step 249/574 completed (loss: 0.02098117582499981, acc: 1.0)
[2024-11-13 07:15:25,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:26,228][root][INFO] - Training Epoch: 10/10, step 250/574 completed (loss: 0.0048435465432703495, acc: 1.0)
[2024-11-13 07:15:26,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:26,625][root][INFO] - Training Epoch: 10/10, step 251/574 completed (loss: 0.028827974572777748, acc: 1.0)
[2024-11-13 07:15:26,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:26,999][root][INFO] - Training Epoch: 10/10, step 252/574 completed (loss: 0.027586329728364944, acc: 0.9756097793579102)
[2024-11-13 07:15:27,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:27,422][root][INFO] - Training Epoch: 10/10, step 253/574 completed (loss: 0.002474888926371932, acc: 1.0)
[2024-11-13 07:15:27,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:27,783][root][INFO] - Training Epoch: 10/10, step 254/574 completed (loss: 0.0035894052125513554, acc: 1.0)
[2024-11-13 07:15:27,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:28,128][root][INFO] - Training Epoch: 10/10, step 255/574 completed (loss: 0.004916056524962187, acc: 1.0)
[2024-11-13 07:15:28,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:28,490][root][INFO] - Training Epoch: 10/10, step 256/574 completed (loss: 0.04905155301094055, acc: 0.9824561476707458)
[2024-11-13 07:15:28,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:28,840][root][INFO] - Training Epoch: 10/10, step 257/574 completed (loss: 0.2227795273065567, acc: 0.9428571462631226)
[2024-11-13 07:15:28,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:29,213][root][INFO] - Training Epoch: 10/10, step 258/574 completed (loss: 0.048332951962947845, acc: 0.9868420958518982)
[2024-11-13 07:15:29,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:30,073][root][INFO] - Training Epoch: 10/10, step 259/574 completed (loss: 0.20485475659370422, acc: 0.9245283007621765)
[2024-11-13 07:15:30,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:30,957][root][INFO] - Training Epoch: 10/10, step 260/574 completed (loss: 0.15311899781227112, acc: 0.9750000238418579)
[2024-11-13 07:15:31,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:31,302][root][INFO] - Training Epoch: 10/10, step 261/574 completed (loss: 0.022862790152430534, acc: 1.0)
[2024-11-13 07:15:31,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:31,651][root][INFO] - Training Epoch: 10/10, step 262/574 completed (loss: 0.06637328863143921, acc: 0.9677419066429138)
[2024-11-13 07:15:31,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:32,075][root][INFO] - Training Epoch: 10/10, step 263/574 completed (loss: 0.2539338767528534, acc: 0.8933333158493042)
[2024-11-13 07:15:32,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:32,530][root][INFO] - Training Epoch: 10/10, step 264/574 completed (loss: 0.09943322092294693, acc: 0.9375)
[2024-11-13 07:15:33,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:33,863][root][INFO] - Training Epoch: 10/10, step 265/574 completed (loss: 0.4510079026222229, acc: 0.871999979019165)
[2024-11-13 07:15:34,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:34,257][root][INFO] - Training Epoch: 10/10, step 266/574 completed (loss: 0.3211795389652252, acc: 0.898876428604126)
[2024-11-13 07:15:34,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:34,698][root][INFO] - Training Epoch: 10/10, step 267/574 completed (loss: 0.21316014230251312, acc: 0.9459459185600281)
[2024-11-13 07:15:35,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:36,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:36,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:37,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:37,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:38,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:38,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:39,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:39,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:40,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:40,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:41,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:41,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:42,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:42,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:43,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:43,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:44,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:44,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:45,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:45,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:46,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:46,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:47,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:47,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:48,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:48,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:49,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:49,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:50,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:50,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:51,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:51,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:52,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:52,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:53,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:54,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:54,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:55,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:55,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:56,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:56,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:57,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:57,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:57,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:58,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:58,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:59,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:15:59,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:00,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:00,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:01,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:01,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:02,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:02,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:03,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:03,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:04,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:04,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:05,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:05,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:06,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:07,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:07,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:08,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:08,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:09,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:09,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:10,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:10,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:11,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:11,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:12,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:12,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:13,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:13,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:14,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:14,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:15,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:15,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:16,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:16,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:17,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:18,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:18,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:19,233][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5154, device='cuda:0') eval_epoch_loss=tensor(0.9224, device='cuda:0') eval_epoch_acc=tensor(0.8148, device='cuda:0')
[2024-11-13 07:16:19,234][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 07:16:19,235][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 07:16:19,678][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_268_loss_0.9224377870559692/model.pt
[2024-11-13 07:16:19,684][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 07:16:20,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:20,399][root][INFO] - Training Epoch: 10/10, step 268/574 completed (loss: 0.03253279998898506, acc: 1.0)
[2024-11-13 07:16:20,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:20,805][root][INFO] - Training Epoch: 10/10, step 269/574 completed (loss: 0.02532791532576084, acc: 1.0)
[2024-11-13 07:16:21,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:21,225][root][INFO] - Training Epoch: 10/10, step 270/574 completed (loss: 0.0764918103814125, acc: 0.9545454382896423)
[2024-11-13 07:16:21,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:21,687][root][INFO] - Training Epoch: 10/10, step 271/574 completed (loss: 0.012201746925711632, acc: 1.0)
[2024-11-13 07:16:21,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:22,094][root][INFO] - Training Epoch: 10/10, step 272/574 completed (loss: 0.007725672330707312, acc: 1.0)
[2024-11-13 07:16:22,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:22,619][root][INFO] - Training Epoch: 10/10, step 273/574 completed (loss: 0.1776071935892105, acc: 0.949999988079071)
[2024-11-13 07:16:22,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:23,038][root][INFO] - Training Epoch: 10/10, step 274/574 completed (loss: 0.2032403200864792, acc: 0.9375)
[2024-11-13 07:16:23,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:23,436][root][INFO] - Training Epoch: 10/10, step 275/574 completed (loss: 0.03395479917526245, acc: 0.9666666388511658)
[2024-11-13 07:16:23,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:23,850][root][INFO] - Training Epoch: 10/10, step 276/574 completed (loss: 0.007151975762099028, acc: 1.0)
[2024-11-13 07:16:24,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:24,232][root][INFO] - Training Epoch: 10/10, step 277/574 completed (loss: 0.003276117844507098, acc: 1.0)
[2024-11-13 07:16:24,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:24,604][root][INFO] - Training Epoch: 10/10, step 278/574 completed (loss: 0.043809179216623306, acc: 0.978723406791687)
[2024-11-13 07:16:24,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:24,996][root][INFO] - Training Epoch: 10/10, step 279/574 completed (loss: 0.05734267830848694, acc: 0.9791666865348816)
[2024-11-13 07:16:25,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:25,392][root][INFO] - Training Epoch: 10/10, step 280/574 completed (loss: 0.004011357203125954, acc: 1.0)
[2024-11-13 07:16:25,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:25,982][root][INFO] - Training Epoch: 10/10, step 281/574 completed (loss: 0.09818482398986816, acc: 0.9638554453849792)
[2024-11-13 07:16:26,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:26,450][root][INFO] - Training Epoch: 10/10, step 282/574 completed (loss: 0.38415560126304626, acc: 0.8888888955116272)
[2024-11-13 07:16:26,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:26,858][root][INFO] - Training Epoch: 10/10, step 283/574 completed (loss: 0.0340142548084259, acc: 1.0)
[2024-11-13 07:16:27,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:27,303][root][INFO] - Training Epoch: 10/10, step 284/574 completed (loss: 0.14816716313362122, acc: 0.9117646813392639)
[2024-11-13 07:16:27,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:27,762][root][INFO] - Training Epoch: 10/10, step 285/574 completed (loss: 0.10163446515798569, acc: 0.9750000238418579)
[2024-11-13 07:16:27,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:28,156][root][INFO] - Training Epoch: 10/10, step 286/574 completed (loss: 0.2108190655708313, acc: 0.90625)
[2024-11-13 07:16:28,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:28,565][root][INFO] - Training Epoch: 10/10, step 287/574 completed (loss: 0.224008709192276, acc: 0.9120000004768372)
[2024-11-13 07:16:28,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:28,916][root][INFO] - Training Epoch: 10/10, step 288/574 completed (loss: 0.16567052900791168, acc: 0.9340659379959106)
[2024-11-13 07:16:29,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:29,343][root][INFO] - Training Epoch: 10/10, step 289/574 completed (loss: 0.13388024270534515, acc: 0.9627329111099243)
[2024-11-13 07:16:29,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:29,814][root][INFO] - Training Epoch: 10/10, step 290/574 completed (loss: 0.24955515563488007, acc: 0.9020618796348572)
[2024-11-13 07:16:30,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:30,199][root][INFO] - Training Epoch: 10/10, step 291/574 completed (loss: 0.01118712779134512, acc: 1.0)
[2024-11-13 07:16:30,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:30,619][root][INFO] - Training Epoch: 10/10, step 292/574 completed (loss: 0.12386742234230042, acc: 0.9523809552192688)
[2024-11-13 07:16:30,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:31,042][root][INFO] - Training Epoch: 10/10, step 293/574 completed (loss: 0.07486974447965622, acc: 0.9655172228813171)
[2024-11-13 07:16:31,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:31,718][root][INFO] - Training Epoch: 10/10, step 294/574 completed (loss: 0.060116957873106, acc: 0.9636363387107849)
[2024-11-13 07:16:32,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:32,536][root][INFO] - Training Epoch: 10/10, step 295/574 completed (loss: 0.34785202145576477, acc: 0.8917526006698608)
[2024-11-13 07:16:32,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:32,974][root][INFO] - Training Epoch: 10/10, step 296/574 completed (loss: 0.11055252701044083, acc: 0.9482758641242981)
[2024-11-13 07:16:33,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:33,422][root][INFO] - Training Epoch: 10/10, step 297/574 completed (loss: 0.01781519316136837, acc: 1.0)
[2024-11-13 07:16:33,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:33,843][root][INFO] - Training Epoch: 10/10, step 298/574 completed (loss: 0.31393730640411377, acc: 0.9210526347160339)
[2024-11-13 07:16:34,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:34,223][root][INFO] - Training Epoch: 10/10, step 299/574 completed (loss: 0.07840972393751144, acc: 0.9821428656578064)
[2024-11-13 07:16:34,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:34,629][root][INFO] - Training Epoch: 10/10, step 300/574 completed (loss: 0.12230020761489868, acc: 0.9375)
[2024-11-13 07:16:34,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:35,029][root][INFO] - Training Epoch: 10/10, step 301/574 completed (loss: 0.027189355343580246, acc: 1.0)
[2024-11-13 07:16:35,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:35,523][root][INFO] - Training Epoch: 10/10, step 302/574 completed (loss: 0.0036931659560650587, acc: 1.0)
[2024-11-13 07:16:35,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:35,965][root][INFO] - Training Epoch: 10/10, step 303/574 completed (loss: 0.0014035181375220418, acc: 1.0)
[2024-11-13 07:16:36,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:36,330][root][INFO] - Training Epoch: 10/10, step 304/574 completed (loss: 0.02830163575708866, acc: 1.0)
[2024-11-13 07:16:36,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:36,750][root][INFO] - Training Epoch: 10/10, step 305/574 completed (loss: 0.16935360431671143, acc: 0.9508196711540222)
[2024-11-13 07:16:36,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:37,094][root][INFO] - Training Epoch: 10/10, step 306/574 completed (loss: 0.1473984569311142, acc: 0.9666666388511658)
[2024-11-13 07:16:37,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:37,441][root][INFO] - Training Epoch: 10/10, step 307/574 completed (loss: 0.0009423481533303857, acc: 1.0)
[2024-11-13 07:16:37,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:37,836][root][INFO] - Training Epoch: 10/10, step 308/574 completed (loss: 0.058637749403715134, acc: 0.9710144996643066)
[2024-11-13 07:16:38,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:38,413][root][INFO] - Training Epoch: 10/10, step 309/574 completed (loss: 0.023116877302527428, acc: 1.0)
[2024-11-13 07:16:38,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:38,804][root][INFO] - Training Epoch: 10/10, step 310/574 completed (loss: 0.014001160860061646, acc: 1.0)
[2024-11-13 07:16:38,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:39,168][root][INFO] - Training Epoch: 10/10, step 311/574 completed (loss: 0.1602994054555893, acc: 0.9487179517745972)
[2024-11-13 07:16:39,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:39,608][root][INFO] - Training Epoch: 10/10, step 312/574 completed (loss: 0.04684552922844887, acc: 0.9795918464660645)
[2024-11-13 07:16:39,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:39,990][root][INFO] - Training Epoch: 10/10, step 313/574 completed (loss: 0.0032622881699353456, acc: 1.0)
[2024-11-13 07:16:40,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:40,385][root][INFO] - Training Epoch: 10/10, step 314/574 completed (loss: 0.021727850660681725, acc: 1.0)
[2024-11-13 07:16:40,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:40,805][root][INFO] - Training Epoch: 10/10, step 315/574 completed (loss: 0.05599653348326683, acc: 0.9677419066429138)
[2024-11-13 07:16:40,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:41,183][root][INFO] - Training Epoch: 10/10, step 316/574 completed (loss: 0.19872169196605682, acc: 0.9354838728904724)
[2024-11-13 07:16:41,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:41,611][root][INFO] - Training Epoch: 10/10, step 317/574 completed (loss: 0.0921911969780922, acc: 0.9701492786407471)
[2024-11-13 07:16:41,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:42,042][root][INFO] - Training Epoch: 10/10, step 318/574 completed (loss: 0.13715213537216187, acc: 0.9615384340286255)
[2024-11-13 07:16:42,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:42,446][root][INFO] - Training Epoch: 10/10, step 319/574 completed (loss: 0.018842514604330063, acc: 1.0)
[2024-11-13 07:16:42,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:42,845][root][INFO] - Training Epoch: 10/10, step 320/574 completed (loss: 0.012706614099442959, acc: 1.0)
[2024-11-13 07:16:43,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:43,283][root][INFO] - Training Epoch: 10/10, step 321/574 completed (loss: 0.03648743778467178, acc: 0.9800000190734863)
[2024-11-13 07:16:43,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:43,692][root][INFO] - Training Epoch: 10/10, step 322/574 completed (loss: 0.34823691844940186, acc: 0.9259259104728699)
[2024-11-13 07:16:43,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:44,013][root][INFO] - Training Epoch: 10/10, step 323/574 completed (loss: 0.21847626566886902, acc: 0.9142857193946838)
[2024-11-13 07:16:44,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:44,442][root][INFO] - Training Epoch: 10/10, step 324/574 completed (loss: 0.25480684638023376, acc: 0.9230769276618958)
[2024-11-13 07:16:44,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:44,834][root][INFO] - Training Epoch: 10/10, step 325/574 completed (loss: 0.25854432582855225, acc: 0.9512194991111755)
[2024-11-13 07:16:44,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:45,179][root][INFO] - Training Epoch: 10/10, step 326/574 completed (loss: 0.18558062613010406, acc: 0.9210526347160339)
[2024-11-13 07:16:45,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:45,541][root][INFO] - Training Epoch: 10/10, step 327/574 completed (loss: 0.009816238656640053, acc: 1.0)
[2024-11-13 07:16:45,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:45,895][root][INFO] - Training Epoch: 10/10, step 328/574 completed (loss: 0.05554736405611038, acc: 0.9642857313156128)
[2024-11-13 07:16:46,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:46,296][root][INFO] - Training Epoch: 10/10, step 329/574 completed (loss: 0.00819998886436224, acc: 1.0)
[2024-11-13 07:16:46,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:46,743][root][INFO] - Training Epoch: 10/10, step 330/574 completed (loss: 0.0019600132945924997, acc: 1.0)
[2024-11-13 07:16:46,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:47,172][root][INFO] - Training Epoch: 10/10, step 331/574 completed (loss: 0.128424733877182, acc: 0.9677419066429138)
[2024-11-13 07:16:47,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:47,649][root][INFO] - Training Epoch: 10/10, step 332/574 completed (loss: 0.06863674521446228, acc: 0.9649122953414917)
[2024-11-13 07:16:47,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:48,059][root][INFO] - Training Epoch: 10/10, step 333/574 completed (loss: 0.016507744789123535, acc: 1.0)
[2024-11-13 07:16:48,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:48,444][root][INFO] - Training Epoch: 10/10, step 334/574 completed (loss: 0.03114529512822628, acc: 0.9666666388511658)
[2024-11-13 07:16:48,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:48,797][root][INFO] - Training Epoch: 10/10, step 335/574 completed (loss: 0.004672043025493622, acc: 1.0)
[2024-11-13 07:16:48,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:49,203][root][INFO] - Training Epoch: 10/10, step 336/574 completed (loss: 0.10435934364795685, acc: 0.9800000190734863)
[2024-11-13 07:16:49,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:49,680][root][INFO] - Training Epoch: 10/10, step 337/574 completed (loss: 0.24978968501091003, acc: 0.931034505367279)
[2024-11-13 07:16:49,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:50,125][root][INFO] - Training Epoch: 10/10, step 338/574 completed (loss: 0.4927504062652588, acc: 0.8085106611251831)
[2024-11-13 07:16:50,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:50,567][root][INFO] - Training Epoch: 10/10, step 339/574 completed (loss: 0.22540393471717834, acc: 0.9277108311653137)
[2024-11-13 07:16:50,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:50,970][root][INFO] - Training Epoch: 10/10, step 340/574 completed (loss: 0.004627362824976444, acc: 1.0)
[2024-11-13 07:16:51,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:51,356][root][INFO] - Training Epoch: 10/10, step 341/574 completed (loss: 0.0056955222971737385, acc: 1.0)
[2024-11-13 07:16:51,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:51,768][root][INFO] - Training Epoch: 10/10, step 342/574 completed (loss: 0.17112410068511963, acc: 0.9518072009086609)
[2024-11-13 07:16:51,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:52,191][root][INFO] - Training Epoch: 10/10, step 343/574 completed (loss: 0.07193347066640854, acc: 0.9622641801834106)
[2024-11-13 07:16:52,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:52,608][root][INFO] - Training Epoch: 10/10, step 344/574 completed (loss: 0.024442428722977638, acc: 0.9873417615890503)
[2024-11-13 07:16:52,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:52,999][root][INFO] - Training Epoch: 10/10, step 345/574 completed (loss: 0.0748564600944519, acc: 0.9803921580314636)
[2024-11-13 07:16:53,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:53,385][root][INFO] - Training Epoch: 10/10, step 346/574 completed (loss: 0.07849344611167908, acc: 0.9701492786407471)
[2024-11-13 07:16:53,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:53,763][root][INFO] - Training Epoch: 10/10, step 347/574 completed (loss: 0.0015162393683567643, acc: 1.0)
[2024-11-13 07:16:53,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:54,189][root][INFO] - Training Epoch: 10/10, step 348/574 completed (loss: 0.030807385221123695, acc: 1.0)
[2024-11-13 07:16:54,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:54,698][root][INFO] - Training Epoch: 10/10, step 349/574 completed (loss: 0.07951703667640686, acc: 0.9722222089767456)
[2024-11-13 07:16:54,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:55,113][root][INFO] - Training Epoch: 10/10, step 350/574 completed (loss: 0.12903501093387604, acc: 0.9767441749572754)
[2024-11-13 07:16:55,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:55,553][root][INFO] - Training Epoch: 10/10, step 351/574 completed (loss: 0.02650059200823307, acc: 1.0)
[2024-11-13 07:16:55,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:56,030][root][INFO] - Training Epoch: 10/10, step 352/574 completed (loss: 0.23931296169757843, acc: 0.9111111164093018)
[2024-11-13 07:16:56,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:56,438][root][INFO] - Training Epoch: 10/10, step 353/574 completed (loss: 0.0021574352867901325, acc: 1.0)
[2024-11-13 07:16:56,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:56,892][root][INFO] - Training Epoch: 10/10, step 354/574 completed (loss: 0.015235316008329391, acc: 1.0)
[2024-11-13 07:16:57,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:57,352][root][INFO] - Training Epoch: 10/10, step 355/574 completed (loss: 0.19384527206420898, acc: 0.9230769276618958)
[2024-11-13 07:16:57,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:58,079][root][INFO] - Training Epoch: 10/10, step 356/574 completed (loss: 0.07820794731378555, acc: 0.9739130139350891)
[2024-11-13 07:16:58,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:58,491][root][INFO] - Training Epoch: 10/10, step 357/574 completed (loss: 0.17383764684200287, acc: 0.967391312122345)
[2024-11-13 07:16:58,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:58,890][root][INFO] - Training Epoch: 10/10, step 358/574 completed (loss: 0.11111068725585938, acc: 0.9591836929321289)
[2024-11-13 07:16:59,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:59,285][root][INFO] - Training Epoch: 10/10, step 359/574 completed (loss: 0.006189514417201281, acc: 1.0)
[2024-11-13 07:16:59,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:16:59,703][root][INFO] - Training Epoch: 10/10, step 360/574 completed (loss: 0.01572217233479023, acc: 1.0)
[2024-11-13 07:16:59,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:00,124][root][INFO] - Training Epoch: 10/10, step 361/574 completed (loss: 0.08741104602813721, acc: 0.9756097793579102)
[2024-11-13 07:17:00,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:00,566][root][INFO] - Training Epoch: 10/10, step 362/574 completed (loss: 0.20848232507705688, acc: 0.9555555582046509)
[2024-11-13 07:17:00,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:01,014][root][INFO] - Training Epoch: 10/10, step 363/574 completed (loss: 0.0899408683180809, acc: 0.9736841917037964)
[2024-11-13 07:17:01,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:01,460][root][INFO] - Training Epoch: 10/10, step 364/574 completed (loss: 0.04523168131709099, acc: 0.9756097793579102)
[2024-11-13 07:17:01,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:01,835][root][INFO] - Training Epoch: 10/10, step 365/574 completed (loss: 0.07897167652845383, acc: 0.9696969985961914)
[2024-11-13 07:17:02,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:02,251][root][INFO] - Training Epoch: 10/10, step 366/574 completed (loss: 0.002875123405829072, acc: 1.0)
[2024-11-13 07:17:02,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:02,649][root][INFO] - Training Epoch: 10/10, step 367/574 completed (loss: 0.003608108265325427, acc: 1.0)
[2024-11-13 07:17:02,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:02,996][root][INFO] - Training Epoch: 10/10, step 368/574 completed (loss: 0.01654994487762451, acc: 1.0)
[2024-11-13 07:17:03,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:03,432][root][INFO] - Training Epoch: 10/10, step 369/574 completed (loss: 0.04954418167471886, acc: 0.96875)
[2024-11-13 07:17:03,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:04,341][root][INFO] - Training Epoch: 10/10, step 370/574 completed (loss: 0.5155596733093262, acc: 0.8545454740524292)
[2024-11-13 07:17:05,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:05,653][root][INFO] - Training Epoch: 10/10, step 371/574 completed (loss: 0.15601103007793427, acc: 0.9716981053352356)
[2024-11-13 07:17:05,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:06,072][root][INFO] - Training Epoch: 10/10, step 372/574 completed (loss: 0.0605616420507431, acc: 0.9777777791023254)
[2024-11-13 07:17:06,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:06,446][root][INFO] - Training Epoch: 10/10, step 373/574 completed (loss: 0.21522441506385803, acc: 0.9642857313156128)
[2024-11-13 07:17:06,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:06,891][root][INFO] - Training Epoch: 10/10, step 374/574 completed (loss: 0.022225378081202507, acc: 1.0)
[2024-11-13 07:17:07,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:07,305][root][INFO] - Training Epoch: 10/10, step 375/574 completed (loss: 0.001100637367926538, acc: 1.0)
[2024-11-13 07:17:07,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:07,628][root][INFO] - Training Epoch: 10/10, step 376/574 completed (loss: 0.0007131805759854615, acc: 1.0)
[2024-11-13 07:17:07,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:08,068][root][INFO] - Training Epoch: 10/10, step 377/574 completed (loss: 0.05248187482357025, acc: 0.9791666865348816)
[2024-11-13 07:17:08,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:08,515][root][INFO] - Training Epoch: 10/10, step 378/574 completed (loss: 0.013604609295725822, acc: 1.0)
[2024-11-13 07:17:08,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:09,380][root][INFO] - Training Epoch: 10/10, step 379/574 completed (loss: 0.1530718356370926, acc: 0.9580838084220886)
[2024-11-13 07:17:09,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:09,935][root][INFO] - Training Epoch: 10/10, step 380/574 completed (loss: 0.20518837869167328, acc: 0.9548872113227844)
[2024-11-13 07:17:11,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:11,828][root][INFO] - Training Epoch: 10/10, step 381/574 completed (loss: 0.4434984624385834, acc: 0.8823529481887817)
[2024-11-13 07:17:12,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:12,681][root][INFO] - Training Epoch: 10/10, step 382/574 completed (loss: 0.14414653182029724, acc: 0.954954981803894)
[2024-11-13 07:17:12,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:13,034][root][INFO] - Training Epoch: 10/10, step 383/574 completed (loss: 0.021271197125315666, acc: 1.0)
[2024-11-13 07:17:13,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:13,371][root][INFO] - Training Epoch: 10/10, step 384/574 completed (loss: 0.001365315169095993, acc: 1.0)
[2024-11-13 07:17:13,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:13,717][root][INFO] - Training Epoch: 10/10, step 385/574 completed (loss: 0.0021160515025258064, acc: 1.0)
[2024-11-13 07:17:13,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:14,099][root][INFO] - Training Epoch: 10/10, step 386/574 completed (loss: 0.019397415220737457, acc: 1.0)
[2024-11-13 07:17:14,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:14,444][root][INFO] - Training Epoch: 10/10, step 387/574 completed (loss: 0.0011786912800744176, acc: 1.0)
[2024-11-13 07:17:14,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:14,803][root][INFO] - Training Epoch: 10/10, step 388/574 completed (loss: 0.0027298987843096256, acc: 1.0)
[2024-11-13 07:17:14,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:15,157][root][INFO] - Training Epoch: 10/10, step 389/574 completed (loss: 0.0011009385343641043, acc: 1.0)
[2024-11-13 07:17:15,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:15,504][root][INFO] - Training Epoch: 10/10, step 390/574 completed (loss: 0.03307332471013069, acc: 1.0)
[2024-11-13 07:17:15,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:15,898][root][INFO] - Training Epoch: 10/10, step 391/574 completed (loss: 0.08367480337619781, acc: 0.9814814925193787)
[2024-11-13 07:17:16,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:16,316][root][INFO] - Training Epoch: 10/10, step 392/574 completed (loss: 0.29198840260505676, acc: 0.9320388436317444)
[2024-11-13 07:17:16,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:17,081][root][INFO] - Training Epoch: 10/10, step 393/574 completed (loss: 0.34224623441696167, acc: 0.9117646813392639)
[2024-11-13 07:17:17,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:17,568][root][INFO] - Training Epoch: 10/10, step 394/574 completed (loss: 0.38039302825927734, acc: 0.8933333158493042)
[2024-11-13 07:17:17,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:18,089][root][INFO] - Training Epoch: 10/10, step 395/574 completed (loss: 0.2600649297237396, acc: 0.9375)
[2024-11-13 07:17:18,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:18,433][root][INFO] - Training Epoch: 10/10, step 396/574 completed (loss: 0.014097091741859913, acc: 1.0)
[2024-11-13 07:17:18,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:18,769][root][INFO] - Training Epoch: 10/10, step 397/574 completed (loss: 0.009195569902658463, acc: 1.0)
[2024-11-13 07:17:18,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:19,165][root][INFO] - Training Epoch: 10/10, step 398/574 completed (loss: 0.014598176814615726, acc: 1.0)
[2024-11-13 07:17:19,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:19,571][root][INFO] - Training Epoch: 10/10, step 399/574 completed (loss: 0.007945047691464424, acc: 1.0)
[2024-11-13 07:17:19,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:20,364][root][INFO] - Training Epoch: 10/10, step 400/574 completed (loss: 0.12998400628566742, acc: 0.9558823704719543)
[2024-11-13 07:17:20,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:20,752][root][INFO] - Training Epoch: 10/10, step 401/574 completed (loss: 0.19352702796459198, acc: 0.9333333373069763)
[2024-11-13 07:17:20,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:21,166][root][INFO] - Training Epoch: 10/10, step 402/574 completed (loss: 0.07281800359487534, acc: 0.9696969985961914)
[2024-11-13 07:17:21,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:21,546][root][INFO] - Training Epoch: 10/10, step 403/574 completed (loss: 0.03088742308318615, acc: 1.0)
[2024-11-13 07:17:21,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:21,915][root][INFO] - Training Epoch: 10/10, step 404/574 completed (loss: 0.011611776426434517, acc: 1.0)
[2024-11-13 07:17:22,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:22,276][root][INFO] - Training Epoch: 10/10, step 405/574 completed (loss: 0.009835101664066315, acc: 1.0)
[2024-11-13 07:17:22,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:22,683][root][INFO] - Training Epoch: 10/10, step 406/574 completed (loss: 0.0005931268096901476, acc: 1.0)
[2024-11-13 07:17:22,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:23,070][root][INFO] - Training Epoch: 10/10, step 407/574 completed (loss: 0.022612709552049637, acc: 0.9722222089767456)
[2024-11-13 07:17:23,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:23,416][root][INFO] - Training Epoch: 10/10, step 408/574 completed (loss: 0.007811188697814941, acc: 1.0)
[2024-11-13 07:17:23,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:23,778][root][INFO] - Training Epoch: 10/10, step 409/574 completed (loss: 0.004501876886934042, acc: 1.0)
[2024-11-13 07:17:23,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:24,171][root][INFO] - Training Epoch: 10/10, step 410/574 completed (loss: 0.004218959715217352, acc: 1.0)
[2024-11-13 07:17:25,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:25,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:25,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:26,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:27,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:27,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:28,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:28,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:28,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:29,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:30,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:30,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:31,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:31,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:32,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:32,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:33,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:33,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:34,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:34,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:34,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:35,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:35,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:36,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:36,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:37,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:37,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:37,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:38,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:38,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:39,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:39,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:40,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:40,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:40,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:41,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:41,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:42,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:42,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:43,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:43,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:44,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:44,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:45,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:45,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:45,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:46,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:46,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:47,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:47,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:48,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:48,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:49,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:49,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:50,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:50,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:51,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:51,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:52,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:52,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:52,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:53,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:54,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:54,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:55,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:55,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:56,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:56,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:57,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:57,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:58,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:58,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:59,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:17:59,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:00,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:00,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:01,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:01,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:02,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:02,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:03,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:03,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:04,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:04,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:05,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:05,866][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4506, device='cuda:0') eval_epoch_loss=tensor(0.8963, device='cuda:0') eval_epoch_acc=tensor(0.8140, device='cuda:0')
[2024-11-13 07:18:05,867][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 07:18:05,868][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 07:18:06,255][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_411_loss_0.8963234424591064/model.pt
[2024-11-13 07:18:06,260][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 07:18:06,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:06,716][root][INFO] - Training Epoch: 10/10, step 411/574 completed (loss: 0.08218105137348175, acc: 0.9642857313156128)
[2024-11-13 07:18:06,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:07,134][root][INFO] - Training Epoch: 10/10, step 412/574 completed (loss: 0.012863130308687687, acc: 1.0)
[2024-11-13 07:18:07,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:07,582][root][INFO] - Training Epoch: 10/10, step 413/574 completed (loss: 0.006438543554395437, acc: 1.0)
[2024-11-13 07:18:07,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:08,005][root][INFO] - Training Epoch: 10/10, step 414/574 completed (loss: 0.0033921473659574986, acc: 1.0)
[2024-11-13 07:18:08,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:08,409][root][INFO] - Training Epoch: 10/10, step 415/574 completed (loss: 0.13871227204799652, acc: 0.9607843160629272)
[2024-11-13 07:18:08,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:08,803][root][INFO] - Training Epoch: 10/10, step 416/574 completed (loss: 0.010456047020852566, acc: 1.0)
[2024-11-13 07:18:09,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:09,226][root][INFO] - Training Epoch: 10/10, step 417/574 completed (loss: 0.1281099170446396, acc: 0.9444444179534912)
[2024-11-13 07:18:09,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:09,633][root][INFO] - Training Epoch: 10/10, step 418/574 completed (loss: 0.04341961070895195, acc: 0.9750000238418579)
[2024-11-13 07:18:09,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:10,025][root][INFO] - Training Epoch: 10/10, step 419/574 completed (loss: 0.00443984055891633, acc: 1.0)
[2024-11-13 07:18:10,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:10,410][root][INFO] - Training Epoch: 10/10, step 420/574 completed (loss: 0.0008950880728662014, acc: 1.0)
[2024-11-13 07:18:10,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:10,820][root][INFO] - Training Epoch: 10/10, step 421/574 completed (loss: 0.01624787040054798, acc: 1.0)
[2024-11-13 07:18:11,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:11,223][root][INFO] - Training Epoch: 10/10, step 422/574 completed (loss: 0.06813956052064896, acc: 0.96875)
[2024-11-13 07:18:11,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:11,587][root][INFO] - Training Epoch: 10/10, step 423/574 completed (loss: 0.003816751064732671, acc: 1.0)
[2024-11-13 07:18:11,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:11,991][root][INFO] - Training Epoch: 10/10, step 424/574 completed (loss: 0.0063530211336910725, acc: 1.0)
[2024-11-13 07:18:12,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:12,449][root][INFO] - Training Epoch: 10/10, step 425/574 completed (loss: 0.027843091636896133, acc: 0.9696969985961914)
[2024-11-13 07:18:12,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:12,860][root][INFO] - Training Epoch: 10/10, step 426/574 completed (loss: 0.0032088831067085266, acc: 1.0)
[2024-11-13 07:18:12,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:13,204][root][INFO] - Training Epoch: 10/10, step 427/574 completed (loss: 0.0349237322807312, acc: 0.9729729890823364)
[2024-11-13 07:18:13,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:13,601][root][INFO] - Training Epoch: 10/10, step 428/574 completed (loss: 0.04099404066801071, acc: 0.9629629850387573)
[2024-11-13 07:18:13,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:13,974][root][INFO] - Training Epoch: 10/10, step 429/574 completed (loss: 0.06123442202806473, acc: 0.95652174949646)
[2024-11-13 07:18:14,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:14,360][root][INFO] - Training Epoch: 10/10, step 430/574 completed (loss: 0.0007481969078071415, acc: 1.0)
[2024-11-13 07:18:14,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:14,704][root][INFO] - Training Epoch: 10/10, step 431/574 completed (loss: 0.010609167627990246, acc: 1.0)
[2024-11-13 07:18:14,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:15,105][root][INFO] - Training Epoch: 10/10, step 432/574 completed (loss: 0.001527043990790844, acc: 1.0)
[2024-11-13 07:18:15,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:15,587][root][INFO] - Training Epoch: 10/10, step 433/574 completed (loss: 0.254235178232193, acc: 0.9444444179534912)
[2024-11-13 07:18:15,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:15,982][root][INFO] - Training Epoch: 10/10, step 434/574 completed (loss: 0.0007912953151389956, acc: 1.0)
[2024-11-13 07:18:16,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:16,392][root][INFO] - Training Epoch: 10/10, step 435/574 completed (loss: 0.0013116250047460198, acc: 1.0)
[2024-11-13 07:18:16,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:16,810][root][INFO] - Training Epoch: 10/10, step 436/574 completed (loss: 0.12455812841653824, acc: 0.9444444179534912)
[2024-11-13 07:18:16,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:17,214][root][INFO] - Training Epoch: 10/10, step 437/574 completed (loss: 0.12664230167865753, acc: 0.9772727489471436)
[2024-11-13 07:18:17,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:17,584][root][INFO] - Training Epoch: 10/10, step 438/574 completed (loss: 0.001514305709861219, acc: 1.0)
[2024-11-13 07:18:17,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:17,914][root][INFO] - Training Epoch: 10/10, step 439/574 completed (loss: 0.011829925701022148, acc: 1.0)
[2024-11-13 07:18:18,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:18,581][root][INFO] - Training Epoch: 10/10, step 440/574 completed (loss: 0.1516820788383484, acc: 0.939393937587738)
[2024-11-13 07:18:19,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:19,636][root][INFO] - Training Epoch: 10/10, step 441/574 completed (loss: 0.2614676356315613, acc: 0.9039999842643738)
[2024-11-13 07:18:19,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:20,198][root][INFO] - Training Epoch: 10/10, step 442/574 completed (loss: 0.3891098201274872, acc: 0.8629032373428345)
[2024-11-13 07:18:20,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:21,192][root][INFO] - Training Epoch: 10/10, step 443/574 completed (loss: 0.30317914485931396, acc: 0.9203979969024658)
[2024-11-13 07:18:21,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:21,585][root][INFO] - Training Epoch: 10/10, step 444/574 completed (loss: 0.04991936683654785, acc: 0.9811320900917053)
[2024-11-13 07:18:21,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:22,176][root][INFO] - Training Epoch: 10/10, step 445/574 completed (loss: 0.012688816525042057, acc: 1.0)
[2024-11-13 07:18:22,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:22,525][root][INFO] - Training Epoch: 10/10, step 446/574 completed (loss: 0.1106194406747818, acc: 0.95652174949646)
[2024-11-13 07:18:22,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:22,858][root][INFO] - Training Epoch: 10/10, step 447/574 completed (loss: 0.05478387326002121, acc: 0.9615384340286255)
[2024-11-13 07:18:22,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:23,174][root][INFO] - Training Epoch: 10/10, step 448/574 completed (loss: 0.0033316262997686863, acc: 1.0)
[2024-11-13 07:18:23,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:23,582][root][INFO] - Training Epoch: 10/10, step 449/574 completed (loss: 0.025495093315839767, acc: 0.9850746393203735)
[2024-11-13 07:18:23,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:24,059][root][INFO] - Training Epoch: 10/10, step 450/574 completed (loss: 0.03978627547621727, acc: 0.9861111044883728)
[2024-11-13 07:18:24,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:24,491][root][INFO] - Training Epoch: 10/10, step 451/574 completed (loss: 0.037796132266521454, acc: 0.97826087474823)
[2024-11-13 07:18:24,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:24,883][root][INFO] - Training Epoch: 10/10, step 452/574 completed (loss: 0.045515742152929306, acc: 0.9871794581413269)
[2024-11-13 07:18:25,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:25,243][root][INFO] - Training Epoch: 10/10, step 453/574 completed (loss: 0.06047726422548294, acc: 0.9736841917037964)
[2024-11-13 07:18:25,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:25,610][root][INFO] - Training Epoch: 10/10, step 454/574 completed (loss: 0.022104812785983086, acc: 1.0)
[2024-11-13 07:18:25,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:26,019][root][INFO] - Training Epoch: 10/10, step 455/574 completed (loss: 0.04805714637041092, acc: 0.9696969985961914)
[2024-11-13 07:18:26,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:26,420][root][INFO] - Training Epoch: 10/10, step 456/574 completed (loss: 0.11115409433841705, acc: 0.969072163105011)
[2024-11-13 07:18:26,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:26,846][root][INFO] - Training Epoch: 10/10, step 457/574 completed (loss: 0.10642523318529129, acc: 0.9714285731315613)
[2024-11-13 07:18:27,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:27,349][root][INFO] - Training Epoch: 10/10, step 458/574 completed (loss: 0.20853310823440552, acc: 0.9418604373931885)
[2024-11-13 07:18:27,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:27,751][root][INFO] - Training Epoch: 10/10, step 459/574 completed (loss: 0.047917213290929794, acc: 0.9821428656578064)
[2024-11-13 07:18:27,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:28,130][root][INFO] - Training Epoch: 10/10, step 460/574 completed (loss: 0.16139821708202362, acc: 0.9753086566925049)
[2024-11-13 07:18:28,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:28,508][root][INFO] - Training Epoch: 10/10, step 461/574 completed (loss: 0.04831443727016449, acc: 0.9722222089767456)
[2024-11-13 07:18:28,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:28,859][root][INFO] - Training Epoch: 10/10, step 462/574 completed (loss: 0.009943722747266293, acc: 1.0)
[2024-11-13 07:18:29,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:29,273][root][INFO] - Training Epoch: 10/10, step 463/574 completed (loss: 0.0047108870930969715, acc: 1.0)
[2024-11-13 07:18:29,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:29,692][root][INFO] - Training Epoch: 10/10, step 464/574 completed (loss: 0.02855287864804268, acc: 0.97826087474823)
[2024-11-13 07:18:29,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:30,143][root][INFO] - Training Epoch: 10/10, step 465/574 completed (loss: 0.06426756829023361, acc: 0.988095223903656)
[2024-11-13 07:18:30,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:30,586][root][INFO] - Training Epoch: 10/10, step 466/574 completed (loss: 0.1970430165529251, acc: 0.9277108311653137)
[2024-11-13 07:18:30,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:31,010][root][INFO] - Training Epoch: 10/10, step 467/574 completed (loss: 0.03137785196304321, acc: 0.9909909963607788)
[2024-11-13 07:18:31,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:31,433][root][INFO] - Training Epoch: 10/10, step 468/574 completed (loss: 0.20643487572669983, acc: 0.9320388436317444)
[2024-11-13 07:18:31,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:31,809][root][INFO] - Training Epoch: 10/10, step 469/574 completed (loss: 0.11000382155179977, acc: 0.9837398529052734)
[2024-11-13 07:18:31,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:32,221][root][INFO] - Training Epoch: 10/10, step 470/574 completed (loss: 0.004762955475598574, acc: 1.0)
[2024-11-13 07:18:32,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:32,611][root][INFO] - Training Epoch: 10/10, step 471/574 completed (loss: 0.07490342855453491, acc: 0.9642857313156128)
[2024-11-13 07:18:32,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:33,165][root][INFO] - Training Epoch: 10/10, step 472/574 completed (loss: 0.05919282138347626, acc: 0.9803921580314636)
[2024-11-13 07:18:33,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:33,630][root][INFO] - Training Epoch: 10/10, step 473/574 completed (loss: 0.3545818626880646, acc: 0.8864628672599792)
[2024-11-13 07:18:33,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:34,109][root][INFO] - Training Epoch: 10/10, step 474/574 completed (loss: 0.1281401664018631, acc: 0.9479166865348816)
[2024-11-13 07:18:34,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:34,533][root][INFO] - Training Epoch: 10/10, step 475/574 completed (loss: 0.11698146909475327, acc: 0.9631901979446411)
[2024-11-13 07:18:34,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:34,913][root][INFO] - Training Epoch: 10/10, step 476/574 completed (loss: 0.0982925295829773, acc: 0.9856114983558655)
[2024-11-13 07:18:35,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:35,347][root][INFO] - Training Epoch: 10/10, step 477/574 completed (loss: 0.2137194722890854, acc: 0.929648220539093)
[2024-11-13 07:18:35,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:35,771][root][INFO] - Training Epoch: 10/10, step 478/574 completed (loss: 0.01334258634597063, acc: 1.0)
[2024-11-13 07:18:35,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:36,162][root][INFO] - Training Epoch: 10/10, step 479/574 completed (loss: 0.014455914497375488, acc: 1.0)
[2024-11-13 07:18:36,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:36,557][root][INFO] - Training Epoch: 10/10, step 480/574 completed (loss: 0.10530690848827362, acc: 0.9629629850387573)
[2024-11-13 07:18:36,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:36,935][root][INFO] - Training Epoch: 10/10, step 481/574 completed (loss: 0.02847590483725071, acc: 1.0)
[2024-11-13 07:18:37,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:37,362][root][INFO] - Training Epoch: 10/10, step 482/574 completed (loss: 0.15431298315525055, acc: 0.949999988079071)
[2024-11-13 07:18:37,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:37,893][root][INFO] - Training Epoch: 10/10, step 483/574 completed (loss: 0.04527950659394264, acc: 1.0)
[2024-11-13 07:18:38,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:38,274][root][INFO] - Training Epoch: 10/10, step 484/574 completed (loss: 0.017236018553376198, acc: 1.0)
[2024-11-13 07:18:38,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:38,624][root][INFO] - Training Epoch: 10/10, step 485/574 completed (loss: 0.004671771544963121, acc: 1.0)
[2024-11-13 07:18:38,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:38,974][root][INFO] - Training Epoch: 10/10, step 486/574 completed (loss: 0.0838264748454094, acc: 0.9629629850387573)
[2024-11-13 07:18:39,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:39,319][root][INFO] - Training Epoch: 10/10, step 487/574 completed (loss: 0.31155574321746826, acc: 0.9523809552192688)
[2024-11-13 07:18:39,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:39,666][root][INFO] - Training Epoch: 10/10, step 488/574 completed (loss: 0.2732295095920563, acc: 0.9545454382896423)
[2024-11-13 07:18:39,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:40,118][root][INFO] - Training Epoch: 10/10, step 489/574 completed (loss: 0.33226096630096436, acc: 0.8769230842590332)
[2024-11-13 07:18:40,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:40,505][root][INFO] - Training Epoch: 10/10, step 490/574 completed (loss: 0.06105621159076691, acc: 0.9666666388511658)
[2024-11-13 07:18:40,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:40,854][root][INFO] - Training Epoch: 10/10, step 491/574 completed (loss: 0.42647531628608704, acc: 0.9655172228813171)
[2024-11-13 07:18:40,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:41,208][root][INFO] - Training Epoch: 10/10, step 492/574 completed (loss: 0.04811371862888336, acc: 0.9803921580314636)
[2024-11-13 07:18:41,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:41,563][root][INFO] - Training Epoch: 10/10, step 493/574 completed (loss: 0.009334845468401909, acc: 1.0)
[2024-11-13 07:18:41,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:41,887][root][INFO] - Training Epoch: 10/10, step 494/574 completed (loss: 0.01633530855178833, acc: 1.0)
[2024-11-13 07:18:42,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:42,211][root][INFO] - Training Epoch: 10/10, step 495/574 completed (loss: 0.003631274215877056, acc: 1.0)
[2024-11-13 07:18:42,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:42,638][root][INFO] - Training Epoch: 10/10, step 496/574 completed (loss: 0.12011587619781494, acc: 0.9642857313156128)
[2024-11-13 07:18:42,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:43,137][root][INFO] - Training Epoch: 10/10, step 497/574 completed (loss: 0.10444455593824387, acc: 0.9550561904907227)
[2024-11-13 07:18:43,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:43,543][root][INFO] - Training Epoch: 10/10, step 498/574 completed (loss: 0.17982347309589386, acc: 0.9438202381134033)
[2024-11-13 07:18:43,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:43,972][root][INFO] - Training Epoch: 10/10, step 499/574 completed (loss: 0.4558570384979248, acc: 0.8652482032775879)
[2024-11-13 07:18:44,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:44,387][root][INFO] - Training Epoch: 10/10, step 500/574 completed (loss: 0.24414002895355225, acc: 0.8913043737411499)
[2024-11-13 07:18:44,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:44,762][root][INFO] - Training Epoch: 10/10, step 501/574 completed (loss: 0.0014238950097933412, acc: 1.0)
[2024-11-13 07:18:44,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:45,111][root][INFO] - Training Epoch: 10/10, step 502/574 completed (loss: 0.0004675633099395782, acc: 1.0)
[2024-11-13 07:18:45,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:45,493][root][INFO] - Training Epoch: 10/10, step 503/574 completed (loss: 0.06796228140592575, acc: 0.9629629850387573)
[2024-11-13 07:18:45,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:45,853][root][INFO] - Training Epoch: 10/10, step 504/574 completed (loss: 0.022337879985570908, acc: 1.0)
[2024-11-13 07:18:45,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:46,193][root][INFO] - Training Epoch: 10/10, step 505/574 completed (loss: 0.08966965228319168, acc: 0.9811320900917053)
[2024-11-13 07:18:46,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:46,536][root][INFO] - Training Epoch: 10/10, step 506/574 completed (loss: 0.3143908977508545, acc: 0.931034505367279)
[2024-11-13 07:18:46,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:47,427][root][INFO] - Training Epoch: 10/10, step 507/574 completed (loss: 0.431989848613739, acc: 0.8738738894462585)
[2024-11-13 07:18:47,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:48,050][root][INFO] - Training Epoch: 10/10, step 508/574 completed (loss: 0.40377652645111084, acc: 0.8732394576072693)
[2024-11-13 07:18:48,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:48,391][root][INFO] - Training Epoch: 10/10, step 509/574 completed (loss: 0.03535644710063934, acc: 1.0)
[2024-11-13 07:18:48,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:48,748][root][INFO] - Training Epoch: 10/10, step 510/574 completed (loss: 0.008448660373687744, acc: 1.0)
[2024-11-13 07:18:48,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:49,097][root][INFO] - Training Epoch: 10/10, step 511/574 completed (loss: 0.007418043911457062, acc: 1.0)
[2024-11-13 07:18:51,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:52,806][root][INFO] - Training Epoch: 10/10, step 512/574 completed (loss: 0.5101797580718994, acc: 0.8571428656578064)
[2024-11-13 07:18:53,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:54,008][root][INFO] - Training Epoch: 10/10, step 513/574 completed (loss: 0.1688023954629898, acc: 0.9603174328804016)
[2024-11-13 07:18:54,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:54,367][root][INFO] - Training Epoch: 10/10, step 514/574 completed (loss: 0.34177055954933167, acc: 0.9285714030265808)
[2024-11-13 07:18:54,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:54,763][root][INFO] - Training Epoch: 10/10, step 515/574 completed (loss: 0.04264809936285019, acc: 0.9833333492279053)
[2024-11-13 07:18:55,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:55,829][root][INFO] - Training Epoch: 10/10, step 516/574 completed (loss: 0.05387917160987854, acc: 0.9861111044883728)
[2024-11-13 07:18:55,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:56,210][root][INFO] - Training Epoch: 10/10, step 517/574 completed (loss: 0.0028738370165228844, acc: 1.0)
[2024-11-13 07:18:56,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:56,581][root][INFO] - Training Epoch: 10/10, step 518/574 completed (loss: 0.017331019043922424, acc: 1.0)
[2024-11-13 07:18:56,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:56,932][root][INFO] - Training Epoch: 10/10, step 519/574 completed (loss: 0.0059327553026378155, acc: 1.0)
[2024-11-13 07:18:57,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:57,319][root][INFO] - Training Epoch: 10/10, step 520/574 completed (loss: 0.05868019536137581, acc: 0.9629629850387573)
[2024-11-13 07:18:58,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:58,847][root][INFO] - Training Epoch: 10/10, step 521/574 completed (loss: 0.4705241322517395, acc: 0.8559321761131287)
[2024-11-13 07:18:59,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:59,295][root][INFO] - Training Epoch: 10/10, step 522/574 completed (loss: 0.1377025544643402, acc: 0.9626865386962891)
[2024-11-13 07:18:59,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:18:59,774][root][INFO] - Training Epoch: 10/10, step 523/574 completed (loss: 0.14813469350337982, acc: 0.9416058659553528)
[2024-11-13 07:19:00,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:00,616][root][INFO] - Training Epoch: 10/10, step 524/574 completed (loss: 0.346337229013443, acc: 0.9100000262260437)
[2024-11-13 07:19:00,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:01,017][root][INFO] - Training Epoch: 10/10, step 525/574 completed (loss: 0.03279811888933182, acc: 0.9814814925193787)
[2024-11-13 07:19:01,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:01,381][root][INFO] - Training Epoch: 10/10, step 526/574 completed (loss: 0.01574123650789261, acc: 1.0)
[2024-11-13 07:19:01,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:01,793][root][INFO] - Training Epoch: 10/10, step 527/574 completed (loss: 0.04988863319158554, acc: 0.9523809552192688)
[2024-11-13 07:19:01,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:02,203][root][INFO] - Training Epoch: 10/10, step 528/574 completed (loss: 0.2741118371486664, acc: 0.9016393423080444)
[2024-11-13 07:19:02,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:02,555][root][INFO] - Training Epoch: 10/10, step 529/574 completed (loss: 0.050356414169073105, acc: 0.9830508232116699)
[2024-11-13 07:19:02,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:02,906][root][INFO] - Training Epoch: 10/10, step 530/574 completed (loss: 0.31435060501098633, acc: 0.9069767594337463)
[2024-11-13 07:19:03,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:03,278][root][INFO] - Training Epoch: 10/10, step 531/574 completed (loss: 0.22186890244483948, acc: 0.9318181872367859)
[2024-11-13 07:19:03,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:03,641][root][INFO] - Training Epoch: 10/10, step 532/574 completed (loss: 0.24477402865886688, acc: 0.9056603908538818)
[2024-11-13 07:19:03,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:04,015][root][INFO] - Training Epoch: 10/10, step 533/574 completed (loss: 0.032813042402267456, acc: 1.0)
[2024-11-13 07:19:04,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:04,364][root][INFO] - Training Epoch: 10/10, step 534/574 completed (loss: 0.0446336455643177, acc: 1.0)
[2024-11-13 07:19:04,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:04,763][root][INFO] - Training Epoch: 10/10, step 535/574 completed (loss: 0.01742607168853283, acc: 1.0)
[2024-11-13 07:19:04,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:05,093][root][INFO] - Training Epoch: 10/10, step 536/574 completed (loss: 0.0644148513674736, acc: 0.9545454382896423)
[2024-11-13 07:19:05,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:05,627][root][INFO] - Training Epoch: 10/10, step 537/574 completed (loss: 0.08754216879606247, acc: 0.9692307710647583)
[2024-11-13 07:19:05,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:06,034][root][INFO] - Training Epoch: 10/10, step 538/574 completed (loss: 0.17207206785678864, acc: 0.96875)
[2024-11-13 07:19:06,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:06,559][root][INFO] - Training Epoch: 10/10, step 539/574 completed (loss: 0.06706428527832031, acc: 0.96875)
[2024-11-13 07:19:06,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:06,909][root][INFO] - Training Epoch: 10/10, step 540/574 completed (loss: 0.07195029407739639, acc: 0.9696969985961914)
[2024-11-13 07:19:07,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:07,224][root][INFO] - Training Epoch: 10/10, step 541/574 completed (loss: 0.01631399616599083, acc: 1.0)
[2024-11-13 07:19:07,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:07,562][root][INFO] - Training Epoch: 10/10, step 542/574 completed (loss: 0.003033316694200039, acc: 1.0)
[2024-11-13 07:19:07,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:07,921][root][INFO] - Training Epoch: 10/10, step 543/574 completed (loss: 0.05946705490350723, acc: 0.95652174949646)
[2024-11-13 07:19:08,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:08,239][root][INFO] - Training Epoch: 10/10, step 544/574 completed (loss: 0.016992367804050446, acc: 1.0)
[2024-11-13 07:19:08,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:08,602][root][INFO] - Training Epoch: 10/10, step 545/574 completed (loss: 0.026184000074863434, acc: 1.0)
[2024-11-13 07:19:08,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:08,988][root][INFO] - Training Epoch: 10/10, step 546/574 completed (loss: 0.12040743231773376, acc: 0.9714285731315613)
[2024-11-13 07:19:09,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:09,374][root][INFO] - Training Epoch: 10/10, step 547/574 completed (loss: 0.01686529815196991, acc: 1.0)
[2024-11-13 07:19:09,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:09,743][root][INFO] - Training Epoch: 10/10, step 548/574 completed (loss: 0.020469168201088905, acc: 1.0)
[2024-11-13 07:19:09,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:10,186][root][INFO] - Training Epoch: 10/10, step 549/574 completed (loss: 0.019889963790774345, acc: 1.0)
[2024-11-13 07:19:10,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:10,579][root][INFO] - Training Epoch: 10/10, step 550/574 completed (loss: 0.15481382608413696, acc: 0.9696969985961914)
[2024-11-13 07:19:10,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:10,968][root][INFO] - Training Epoch: 10/10, step 551/574 completed (loss: 0.00822328682988882, acc: 1.0)
[2024-11-13 07:19:11,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:11,356][root][INFO] - Training Epoch: 10/10, step 552/574 completed (loss: 0.04765329882502556, acc: 0.9714285731315613)
[2024-11-13 07:19:11,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:11,733][root][INFO] - Training Epoch: 10/10, step 553/574 completed (loss: 0.11960756033658981, acc: 0.9635036587715149)
[2024-11-13 07:19:12,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:13,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:13,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:13,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:14,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:15,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:15,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:15,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:16,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:16,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:17,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:17,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:18,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:18,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:19,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:19,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:20,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:20,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:21,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:21,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:21,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:22,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:22,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:23,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:23,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:24,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:24,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:24,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:25,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:25,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:26,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:26,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:27,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:28,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:28,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:28,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:29,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:29,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:30,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:30,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:31,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:31,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:32,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:32,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:32,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:33,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:33,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:34,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:34,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:35,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:35,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:35,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:36,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:36,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:37,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:37,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:38,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:38,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:39,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:39,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:40,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:41,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:41,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:42,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:42,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:43,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:43,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:44,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:45,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:45,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:46,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:46,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:47,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:47,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:48,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:48,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:49,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:49,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:50,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:50,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:50,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:51,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:52,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:52,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:53,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:53,877][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5604, device='cuda:0') eval_epoch_loss=tensor(0.9402, device='cuda:0') eval_epoch_acc=tensor(0.8109, device='cuda:0')
[2024-11-13 07:19:53,878][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-13 07:19:53,878][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-13 07:19:54,263][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_554_loss_0.9401524662971497/model.pt
[2024-11-13 07:19:54,268][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-13 07:19:54,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:54,761][root][INFO] - Training Epoch: 10/10, step 554/574 completed (loss: 0.0874476432800293, acc: 0.9655172228813171)
[2024-11-13 07:19:54,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:55,147][root][INFO] - Training Epoch: 10/10, step 555/574 completed (loss: 0.10800783336162567, acc: 0.9714285731315613)
[2024-11-13 07:19:55,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:55,565][root][INFO] - Training Epoch: 10/10, step 556/574 completed (loss: 0.1464163064956665, acc: 0.940397322177887)
[2024-11-13 07:19:55,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:55,926][root][INFO] - Training Epoch: 10/10, step 557/574 completed (loss: 0.03140053525567055, acc: 0.9914529919624329)
[2024-11-13 07:19:56,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:56,321][root][INFO] - Training Epoch: 10/10, step 558/574 completed (loss: 0.007633876986801624, acc: 1.0)
[2024-11-13 07:19:56,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:56,747][root][INFO] - Training Epoch: 10/10, step 559/574 completed (loss: 0.0026705218479037285, acc: 1.0)
[2024-11-13 07:19:56,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:57,135][root][INFO] - Training Epoch: 10/10, step 560/574 completed (loss: 0.019199592992663383, acc: 1.0)
[2024-11-13 07:19:57,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:57,570][root][INFO] - Training Epoch: 10/10, step 561/574 completed (loss: 0.0064375982619822025, acc: 1.0)
[2024-11-13 07:19:57,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:58,029][root][INFO] - Training Epoch: 10/10, step 562/574 completed (loss: 0.06801620125770569, acc: 0.9888888597488403)
[2024-11-13 07:19:58,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:58,469][root][INFO] - Training Epoch: 10/10, step 563/574 completed (loss: 0.09369763731956482, acc: 0.9740259647369385)
[2024-11-13 07:19:58,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:58,900][root][INFO] - Training Epoch: 10/10, step 564/574 completed (loss: 0.05540764331817627, acc: 0.9583333134651184)
[2024-11-13 07:19:59,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:59,267][root][INFO] - Training Epoch: 10/10, step 565/574 completed (loss: 0.11851821094751358, acc: 0.9482758641242981)
[2024-11-13 07:19:59,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:19:59,695][root][INFO] - Training Epoch: 10/10, step 566/574 completed (loss: 0.06839872151613235, acc: 0.988095223903656)
[2024-11-13 07:19:59,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:20:00,101][root][INFO] - Training Epoch: 10/10, step 567/574 completed (loss: 0.015935316681861877, acc: 1.0)
[2024-11-13 07:20:00,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:20:00,534][root][INFO] - Training Epoch: 10/10, step 568/574 completed (loss: 0.05628329887986183, acc: 0.9629629850387573)
[2024-11-13 07:20:00,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:20:01,046][root][INFO] - Training Epoch: 10/10, step 569/574 completed (loss: 0.057571690529584885, acc: 0.9732620120048523)
[2024-11-13 07:20:01,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:20:01,426][root][INFO] - Training Epoch: 10/10, step 570/574 completed (loss: 0.016106605529785156, acc: 1.0)
[2024-11-13 07:20:01,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:20:01,830][root][INFO] - Training Epoch: 10/10, step 571/574 completed (loss: 0.07879461348056793, acc: 0.9658119678497314)
[2024-11-13 07:20:01,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:20:02,201][root][INFO] - Training Epoch: 10/10, step 572/574 completed (loss: 0.22985196113586426, acc: 0.9336734414100647)
[2024-11-13 07:20:02,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-13 07:20:02,621][root][INFO] - Training Epoch: 10/10, step 573/574 completed (loss: 0.09896516054868698, acc: 0.9748427867889404)
[2024-11-13 07:20:03,158][slam_llm.utils.train_utils][INFO] - Epoch 10: train_perplexity=1.1388, train_epoch_loss=0.1300, epoch time 456.86697618849576s
[2024-11-13 07:20:03,158][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2024-11-13 07:20:03,158][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 16 GB
[2024-11-13 07:20:03,158][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2024-11-13 07:20:03,158][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 30
[2024-11-13 07:20:03,158][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-13 07:20:03,162][root][INFO] - Key: avg_train_prep, Value: 3.1092209815979004
[2024-11-13 07:20:03,164][root][INFO] - Key: avg_train_loss, Value: 0.6501536965370178
[2024-11-13 07:20:03,164][root][INFO] - Key: avg_train_acc, Value: 0.8318856358528137
[2024-11-13 07:20:03,164][root][INFO] - Key: avg_eval_prep, Value: 3.410100221633911
[2024-11-13 07:20:03,164][root][INFO] - Key: avg_eval_loss, Value: 1.043900728225708
[2024-11-13 07:20:03,164][root][INFO] - Key: avg_eval_acc, Value: 0.7493736147880554
[2024-11-13 07:20:03,165][root][INFO] - Key: avg_epoch_time, Value: 464.6983608486131
[2024-11-13 07:20:03,165][root][INFO] - Key: avg_checkpoint_time, Value: 0.4773418235592544
[2024-11-29 03:42:51,479][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 554, 'resume_epoch': 10, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True}
[2024-11-29 03:42:51,480][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-29 03:42:51,480][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme'}
[2024-11-29 03:42:51,480][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_dual_peft', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2024-11-29_03-42-51.txt', 'log_interval': 5}
[2024-11-29 03:43:11,128][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-29 03:43:16,633][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-29 03:43:16,635][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-29 03:43:16,637][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-29 03:43:16,638][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-29 03:43:18,724][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-29 03:43:18,727][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-29 03:43:18,729][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-29 03:43:18,730][slam_llm.utils.train_utils][INFO] - --> w2v2 has 0.0 Million params

[2024-11-29 03:43:23,963][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-29 03:43:23,965][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-29 03:43:23,965][slam_llm.models.slam_model][INFO] - setup peft...
[2024-11-29 03:43:24,089][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-29 03:43:24,091][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-29 03:43:24,268][slam_llm.utils.train_utils][INFO] - --> Module dual
[2024-11-29 03:43:24,269][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2024-11-29 03:43:24,269][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_554_loss_0.9401524662971497/model.pt
[2024-11-29 03:43:24,509][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-29 03:43:24,515][slam_llm.utils.train_utils][INFO] - --> asr has 30.806016 Million params

[2024-11-29 03:43:27,302][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-29 03:43:27,851][root][INFO] - --> Training Set Length = 2298
[2024-11-29 03:43:27,854][root][INFO] - --> Validation Set Length = 341
[2024-11-29 03:43:27,855][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-29 03:43:27,855][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-29 03:43:29,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:30,603][root][INFO] - Training Epoch: 10/10, step 554/574 completed (loss: 0.0017268694937229156, acc: 1.0)
[2024-11-29 03:43:30,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:31,242][numexpr.utils][INFO] - Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-11-29 03:43:31,751][root][INFO] - Training Epoch: 10/10, step 555/574 completed (loss: 0.0071664671413600445, acc: 1.0)
[2024-11-29 03:43:31,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:32,193][root][INFO] - Training Epoch: 10/10, step 556/574 completed (loss: 0.19244177639484406, acc: 0.9459459185600281)
[2024-11-29 03:43:32,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:32,654][root][INFO] - Training Epoch: 10/10, step 557/574 completed (loss: 0.04969446733593941, acc: 0.9736841917037964)
[2024-11-29 03:43:32,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:33,193][root][INFO] - Training Epoch: 10/10, step 558/574 completed (loss: 0.03924015909433365, acc: 0.9729729890823364)
[2024-11-29 03:43:33,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:33,612][root][INFO] - Training Epoch: 10/10, step 559/574 completed (loss: 0.015144606120884418, acc: 1.0)
[2024-11-29 03:43:33,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:34,162][root][INFO] - Training Epoch: 10/10, step 560/574 completed (loss: 0.1169687956571579, acc: 0.9387755393981934)
[2024-11-29 03:43:34,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:34,624][root][INFO] - Training Epoch: 10/10, step 561/574 completed (loss: 0.05913630500435829, acc: 1.0)
[2024-11-29 03:43:34,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:35,127][root][INFO] - Training Epoch: 10/10, step 562/574 completed (loss: 0.001186894834972918, acc: 1.0)
[2024-11-29 03:43:35,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:35,629][root][INFO] - Training Epoch: 10/10, step 563/574 completed (loss: 0.015227322466671467, acc: 1.0)
[2024-11-29 03:43:35,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:36,077][root][INFO] - Training Epoch: 10/10, step 564/574 completed (loss: 0.01772662065923214, acc: 1.0)
[2024-11-29 03:43:36,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:36,493][root][INFO] - Training Epoch: 10/10, step 565/574 completed (loss: 0.17968495190143585, acc: 0.9743589758872986)
[2024-11-29 03:43:36,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:36,941][root][INFO] - Training Epoch: 10/10, step 566/574 completed (loss: 0.02962128259241581, acc: 1.0)
[2024-11-29 03:43:37,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:37,380][root][INFO] - Training Epoch: 10/10, step 567/574 completed (loss: 0.057948268949985504, acc: 0.97826087474823)
[2024-11-29 03:43:37,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:37,840][root][INFO] - Training Epoch: 10/10, step 568/574 completed (loss: 0.016673365607857704, acc: 1.0)
[2024-11-29 03:43:38,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:38,199][root][INFO] - Training Epoch: 10/10, step 569/574 completed (loss: 0.018173187971115112, acc: 1.0)
[2024-11-29 03:43:38,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:38,595][root][INFO] - Training Epoch: 10/10, step 570/574 completed (loss: 0.04905620217323303, acc: 1.0)
[2024-11-29 03:43:38,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:38,991][root][INFO] - Training Epoch: 10/10, step 571/574 completed (loss: 0.010856720618903637, acc: 1.0)
[2024-11-29 03:43:39,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:39,425][root][INFO] - Training Epoch: 10/10, step 572/574 completed (loss: 0.11032282561063766, acc: 0.9444444179534912)
[2024-11-29 03:43:39,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:39,859][root][INFO] - Training Epoch: 10/10, step 573/574 completed (loss: 0.08055844902992249, acc: 0.9473684430122375)
[2024-11-29 03:43:40,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 03:43:40,288][root][INFO] - Training Epoch: 10/10, step 574/574 completed (loss: 0.1423349529504776, acc: 0.9615384340286255)
[2024-11-29 03:43:41,072][slam_llm.utils.train_utils][INFO] - Epoch 10: train_perplexity=1.0021, train_epoch_loss=0.0021, epoch time 13.20871376618743s
[2024-11-29 03:43:41,072][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-29 03:43:41,072][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-11-29 03:43:41,072][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-29 03:43:41,072][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-29 03:43:41,072][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-29 04:30:23,444][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 554, 'resume_epoch': 10, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True}
[2024-11-29 04:30:23,444][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-29 04:30:23,444][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme'}
[2024-11-29 04:30:23,445][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_dual_peft', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2024-11-29_04-30-23.txt', 'log_interval': 5}
[2024-11-29 04:30:45,087][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-29 04:30:52,309][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-29 04:30:52,314][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-29 04:30:52,316][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-29 04:30:52,317][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-29 04:30:54,073][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-29 04:30:54,074][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-29 04:30:54,076][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-29 04:30:54,077][slam_llm.utils.train_utils][INFO] - --> w2v2 has 0.0 Million params

[2024-11-29 04:30:58,879][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-29 04:30:58,882][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-29 04:30:58,882][slam_llm.models.slam_model][INFO] - setup peft...
[2024-11-29 04:30:59,005][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-29 04:30:59,007][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-29 04:30:59,202][slam_llm.utils.train_utils][INFO] - --> Module dual
[2024-11-29 04:30:59,203][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2024-11-29 04:30:59,203][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_554_loss_0.9401524662971497/model.pt
[2024-11-29 04:30:59,507][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-29 04:30:59,513][slam_llm.utils.train_utils][INFO] - --> asr has 30.806016 Million params

[2024-11-29 04:31:03,562][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-29 04:31:04,751][root][INFO] - --> Training Set Length = 2298
[2024-11-29 04:31:04,755][root][INFO] - --> Validation Set Length = 341
[2024-11-29 04:31:04,755][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-29 04:31:04,756][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-29 04:31:06,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:07,214][root][INFO] - Training Epoch: 10/10, step 554/574 completed (loss: 0.0017268694937229156, acc: 1.0)
[2024-11-29 04:31:07,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:07,863][numexpr.utils][INFO] - Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-11-29 04:31:08,641][root][INFO] - Training Epoch: 10/10, step 555/574 completed (loss: 0.0071664671413600445, acc: 1.0)
[2024-11-29 04:31:08,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:08,952][root][INFO] - Training Epoch: 10/10, step 556/574 completed (loss: 0.19244177639484406, acc: 0.9459459185600281)
[2024-11-29 04:31:09,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:09,283][root][INFO] - Training Epoch: 10/10, step 557/574 completed (loss: 0.04969446733593941, acc: 0.9736841917037964)
[2024-11-29 04:31:09,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:09,590][root][INFO] - Training Epoch: 10/10, step 558/574 completed (loss: 0.03924015909433365, acc: 0.9729729890823364)
[2024-11-29 04:31:09,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:09,891][root][INFO] - Training Epoch: 10/10, step 559/574 completed (loss: 0.015144606120884418, acc: 1.0)
[2024-11-29 04:31:10,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:10,220][root][INFO] - Training Epoch: 10/10, step 560/574 completed (loss: 0.1169687956571579, acc: 0.9387755393981934)
[2024-11-29 04:31:10,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:10,517][root][INFO] - Training Epoch: 10/10, step 561/574 completed (loss: 0.05913630500435829, acc: 1.0)
[2024-11-29 04:31:10,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:10,850][root][INFO] - Training Epoch: 10/10, step 562/574 completed (loss: 0.001186894834972918, acc: 1.0)
[2024-11-29 04:31:10,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:11,153][root][INFO] - Training Epoch: 10/10, step 563/574 completed (loss: 0.015227322466671467, acc: 1.0)
[2024-11-29 04:31:11,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:11,448][root][INFO] - Training Epoch: 10/10, step 564/574 completed (loss: 0.01772662065923214, acc: 1.0)
[2024-11-29 04:31:11,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:11,758][root][INFO] - Training Epoch: 10/10, step 565/574 completed (loss: 0.17968495190143585, acc: 0.9743589758872986)
[2024-11-29 04:31:11,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:12,059][root][INFO] - Training Epoch: 10/10, step 566/574 completed (loss: 0.02962128259241581, acc: 1.0)
[2024-11-29 04:31:12,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:12,366][root][INFO] - Training Epoch: 10/10, step 567/574 completed (loss: 0.057948268949985504, acc: 0.97826087474823)
[2024-11-29 04:31:12,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:12,687][root][INFO] - Training Epoch: 10/10, step 568/574 completed (loss: 0.016673365607857704, acc: 1.0)
[2024-11-29 04:31:12,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:12,986][root][INFO] - Training Epoch: 10/10, step 569/574 completed (loss: 0.018173187971115112, acc: 1.0)
[2024-11-29 04:31:13,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:13,298][root][INFO] - Training Epoch: 10/10, step 570/574 completed (loss: 0.04905620217323303, acc: 1.0)
[2024-11-29 04:31:13,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:13,591][root][INFO] - Training Epoch: 10/10, step 571/574 completed (loss: 0.010856720618903637, acc: 1.0)
[2024-11-29 04:31:13,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:13,885][root][INFO] - Training Epoch: 10/10, step 572/574 completed (loss: 0.11032282561063766, acc: 0.9444444179534912)
[2024-11-29 04:31:14,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:14,165][root][INFO] - Training Epoch: 10/10, step 573/574 completed (loss: 0.08055844902992249, acc: 0.9473684430122375)
[2024-11-29 04:31:14,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-29 04:31:14,458][root][INFO] - Training Epoch: 10/10, step 574/574 completed (loss: 0.1423349529504776, acc: 0.9615384340286255)
[2024-11-29 04:31:14,938][slam_llm.utils.train_utils][INFO] - Epoch 10: train_perplexity=1.0021, train_epoch_loss=0.0021, epoch time 10.173109665513039s
[2024-11-29 04:31:14,938][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-29 04:31:14,938][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-11-29 04:31:14,938][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-29 04:31:14,938][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-29 04:31:14,939][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-02-08 19:23:05,071][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 10, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 554, 'resume_epoch': 10, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 10, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-08 19:23:05,071][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-08 19:23:05,071][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme', 'identifier': 'psst_phoneme_wavlm_llama32_1b_dual_peft'}
[2025-02-08 19:23:05,071][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_dual_peft', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-08_19-23-04.txt', 'log_interval': 5}
[2025-02-08 19:23:26,453][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-08 19:23:32,681][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-08 19:23:32,685][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-08 19:23:32,687][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-08 19:23:32,688][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-08 19:23:35,391][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-08 19:23:35,393][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2025-02-08 19:23:35,395][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-08 19:23:35,396][slam_llm.utils.train_utils][INFO] - --> w2v2 has 0.0 Million params

[2025-02-08 19:23:42,904][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-08 19:23:42,906][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-08 19:23:42,906][slam_llm.models.slam_model][INFO] - setup peft...
[2025-02-08 19:23:43,040][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-08 19:23:43,042][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-08 19:23:43,241][slam_llm.utils.train_utils][INFO] - --> Module dual
[2025-02-08 19:23:43,242][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2025-02-08 19:23:43,242][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_554_loss_0.9401524662971497/model.pt
[2025-02-08 19:23:43,538][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-08 19:23:43,545][slam_llm.utils.train_utils][INFO] - --> asr has 30.806016 Million params

[2025-02-08 19:23:48,279][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-08 19:23:50,779][root][INFO] - --> Training Set Length = 2298
[2025-02-08 19:23:50,789][root][INFO] - --> Validation Set Length = 341
[2025-02-08 19:23:50,789][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-08 19:23:50,789][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-09 03:32:35,857][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 10, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 554, 'resume_epoch': 10, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 10, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-09 03:32:35,857][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-09 03:32:35,858][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme', 'identifier': 'psst_phoneme_wavlm_llama32_1b_dual_peft'}
[2025-02-09 03:32:35,858][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_dual_peft', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-09_03-32-35.txt', 'log_interval': 5}
[2025-02-09 03:32:57,915][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-09 03:33:03,411][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-09 03:33:03,416][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-09 03:33:03,424][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-09 03:33:03,427][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-09 03:33:05,578][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-09 03:33:05,579][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2025-02-09 03:33:05,581][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-09 03:33:05,582][slam_llm.utils.train_utils][INFO] - --> w2v2 has 0.0 Million params

[2025-02-09 03:33:11,054][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-09 03:33:11,055][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-09 03:33:11,056][slam_llm.models.slam_model][INFO] - setup peft...
[2025-02-09 03:33:11,179][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-09 03:33:11,181][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-09 03:33:11,360][slam_llm.utils.train_utils][INFO] - --> Module dual
[2025-02-09 03:33:11,360][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2025-02-09 03:33:11,360][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_554_loss_0.9401524662971497/model.pt
[2025-02-09 03:33:11,616][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-09 03:33:11,622][slam_llm.utils.train_utils][INFO] - --> asr has 30.806016 Million params

[2025-02-09 03:33:14,451][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-09 03:33:16,013][root][INFO] - --> Training Set Length = 2298
[2025-02-09 03:33:16,022][root][INFO] - --> Validation Set Length = 341
[2025-02-09 03:33:16,023][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-09 03:33:16,024][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
