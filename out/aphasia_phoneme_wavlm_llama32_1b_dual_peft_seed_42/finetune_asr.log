[2025-02-16 10:56:32,281][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_dual_peft_seed_42', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False, 'test_flag': True}
[2025-02-16 10:56:32,281][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-16 10:56:32,282][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme', 'identifier': 'aphasia_phoneme_wavlm_llama32_1b_dual_peft_seed_42'}
[2025-02-16 10:56:32,282][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'aphasia_phoneme_wavlm_llama32_1b_dual_peft_seed_42', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-16_10-56-31.txt', 'log_interval': 5}
[2025-02-16 10:56:56,756][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-16 10:57:02,256][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-16 10:57:02,258][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-16 10:57:02,260][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-16 10:57:02,261][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-16 10:57:03,978][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-16 10:57:03,979][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2025-02-16 10:57:03,982][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-16 10:57:03,983][slam_llm.utils.train_utils][INFO] - --> w2v2 has 0.0 Million params

[2025-02-16 10:57:12,134][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-16 10:57:12,135][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-16 10:57:12,136][slam_llm.models.slam_model][INFO] - setup peft...
[2025-02-16 10:57:12,260][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-16 10:57:12,262][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-16 10:57:12,442][slam_llm.utils.train_utils][INFO] - --> Module dual
[2025-02-16 10:57:12,443][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2025-02-16 10:57:12,443][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-16 10:57:12,448][slam_llm.utils.train_utils][INFO] - --> asr has 30.806016 Million params

[2025-02-16 10:57:15,153][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-16 10:57:18,124][root][INFO] - --> Training Set Length = 95353
[2025-02-16 10:57:18,180][root][INFO] - --> Validation Set Length = 13162
[2025-02-16 10:57:18,181][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-16 10:57:18,181][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-16 10:57:20,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:21,510][root][INFO] - Training Epoch: 1/2, step 0/23838 completed (loss: 6.347560405731201, acc: 0.12195122241973877)
[2025-02-16 10:57:21,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:21,920][root][INFO] - Training Epoch: 1/2, step 1/23838 completed (loss: 7.2454833984375, acc: 0.01587301678955555)
[2025-02-16 10:57:22,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:22,524][root][INFO] - Training Epoch: 1/2, step 2/23838 completed (loss: 6.826210975646973, acc: 0.0)
[2025-02-16 10:57:22,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:22,909][root][INFO] - Training Epoch: 1/2, step 3/23838 completed (loss: 7.7033233642578125, acc: 0.0)
[2025-02-16 10:57:23,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:23,772][root][INFO] - Training Epoch: 1/2, step 4/23838 completed (loss: 7.889801025390625, acc: 0.019607843831181526)
[2025-02-16 10:57:24,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:24,325][root][INFO] - Training Epoch: 1/2, step 5/23838 completed (loss: 7.134945392608643, acc: 0.0533333346247673)
[2025-02-16 10:57:24,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:24,832][root][INFO] - Training Epoch: 1/2, step 6/23838 completed (loss: 6.370185852050781, acc: 0.048076923936605453)
[2025-02-16 10:57:25,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:25,745][root][INFO] - Training Epoch: 1/2, step 7/23838 completed (loss: 6.714569091796875, acc: 0.03100775182247162)
[2025-02-16 10:57:26,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:26,309][root][INFO] - Training Epoch: 1/2, step 8/23838 completed (loss: 6.685436248779297, acc: 0.02459016442298889)
[2025-02-16 10:57:26,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:26,780][root][INFO] - Training Epoch: 1/2, step 9/23838 completed (loss: 6.345818519592285, acc: 0.09259258955717087)
[2025-02-16 10:57:27,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:27,242][root][INFO] - Training Epoch: 1/2, step 10/23838 completed (loss: 7.674261569976807, acc: 0.0)
[2025-02-16 10:57:27,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:27,689][root][INFO] - Training Epoch: 1/2, step 11/23838 completed (loss: 7.052487373352051, acc: 0.05454545468091965)
[2025-02-16 10:57:27,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:28,147][root][INFO] - Training Epoch: 1/2, step 12/23838 completed (loss: 7.879693508148193, acc: 0.0)
[2025-02-16 10:57:28,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:28,614][root][INFO] - Training Epoch: 1/2, step 13/23838 completed (loss: 6.925734519958496, acc: 0.06451612710952759)
[2025-02-16 10:57:28,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:28,993][root][INFO] - Training Epoch: 1/2, step 14/23838 completed (loss: 6.58022928237915, acc: 0.05714285746216774)
[2025-02-16 10:57:29,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:29,689][root][INFO] - Training Epoch: 1/2, step 15/23838 completed (loss: 6.190547943115234, acc: 0.02631578966975212)
[2025-02-16 10:57:29,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:30,169][root][INFO] - Training Epoch: 1/2, step 16/23838 completed (loss: 6.5765156745910645, acc: 0.040816325694322586)
[2025-02-16 10:57:30,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:30,619][root][INFO] - Training Epoch: 1/2, step 17/23838 completed (loss: 7.5006585121154785, acc: 0.0)
[2025-02-16 10:57:30,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:31,219][root][INFO] - Training Epoch: 1/2, step 18/23838 completed (loss: 5.8332905769348145, acc: 0.10447761416435242)
[2025-02-16 10:57:31,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:31,914][root][INFO] - Training Epoch: 1/2, step 19/23838 completed (loss: 5.593940734863281, acc: 0.0546875)
[2025-02-16 10:57:32,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:32,370][root][INFO] - Training Epoch: 1/2, step 20/23838 completed (loss: 6.082190036773682, acc: 0.051948051899671555)
[2025-02-16 10:57:32,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:32,817][root][INFO] - Training Epoch: 1/2, step 21/23838 completed (loss: 7.515689373016357, acc: 0.0)
[2025-02-16 10:57:32,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:33,213][root][INFO] - Training Epoch: 1/2, step 22/23838 completed (loss: 7.178473472595215, acc: 0.03999999910593033)
[2025-02-16 10:57:33,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:33,675][root][INFO] - Training Epoch: 1/2, step 23/23838 completed (loss: 6.390475749969482, acc: 0.032608695328235626)
[2025-02-16 10:57:33,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:34,189][root][INFO] - Training Epoch: 1/2, step 24/23838 completed (loss: 5.5970540046691895, acc: 0.17283950746059418)
[2025-02-16 10:57:34,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:34,639][root][INFO] - Training Epoch: 1/2, step 25/23838 completed (loss: 5.858569622039795, acc: 0.09876543283462524)
[2025-02-16 10:57:34,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:35,088][root][INFO] - Training Epoch: 1/2, step 26/23838 completed (loss: 5.93543004989624, acc: 0.02380952425301075)
[2025-02-16 10:57:35,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:35,591][root][INFO] - Training Epoch: 1/2, step 27/23838 completed (loss: 6.278151988983154, acc: 0.0)
[2025-02-16 10:57:35,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:36,041][root][INFO] - Training Epoch: 1/2, step 28/23838 completed (loss: 6.365105152130127, acc: 0.03333333507180214)
[2025-02-16 10:57:36,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:36,430][root][INFO] - Training Epoch: 1/2, step 29/23838 completed (loss: 5.870537757873535, acc: 0.0810810774564743)
[2025-02-16 10:57:36,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:36,896][root][INFO] - Training Epoch: 1/2, step 30/23838 completed (loss: 8.615180969238281, acc: 0.0)
[2025-02-16 10:57:37,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:37,310][root][INFO] - Training Epoch: 1/2, step 31/23838 completed (loss: 7.167409420013428, acc: 0.0)
[2025-02-16 10:57:37,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:37,771][root][INFO] - Training Epoch: 1/2, step 32/23838 completed (loss: 7.334896564483643, acc: 0.06666667014360428)
[2025-02-16 10:57:37,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:38,190][root][INFO] - Training Epoch: 1/2, step 33/23838 completed (loss: 7.851011753082275, acc: 0.0)
[2025-02-16 10:57:38,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:38,617][root][INFO] - Training Epoch: 1/2, step 34/23838 completed (loss: 7.689743518829346, acc: 0.03333333507180214)
[2025-02-16 10:57:38,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:39,064][root][INFO] - Training Epoch: 1/2, step 35/23838 completed (loss: 8.295707702636719, acc: 0.0)
[2025-02-16 10:57:39,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:39,537][root][INFO] - Training Epoch: 1/2, step 36/23838 completed (loss: 7.7206711769104, acc: 0.0)
[2025-02-16 10:57:39,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:40,399][root][INFO] - Training Epoch: 1/2, step 37/23838 completed (loss: 5.590123176574707, acc: 0.058139536529779434)
[2025-02-16 10:57:40,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:41,237][root][INFO] - Training Epoch: 1/2, step 38/23838 completed (loss: 6.528254508972168, acc: 0.0)
[2025-02-16 10:57:41,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:41,631][root][INFO] - Training Epoch: 1/2, step 39/23838 completed (loss: 10.143047332763672, acc: 0.0)
[2025-02-16 10:57:41,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:42,067][root][INFO] - Training Epoch: 1/2, step 40/23838 completed (loss: 10.36453914642334, acc: 0.0)
[2025-02-16 10:57:42,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:42,533][root][INFO] - Training Epoch: 1/2, step 41/23838 completed (loss: 6.171597480773926, acc: 0.0)
[2025-02-16 10:57:42,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:43,012][root][INFO] - Training Epoch: 1/2, step 42/23838 completed (loss: 7.522311210632324, acc: 0.095238097012043)
[2025-02-16 10:57:43,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:43,422][root][INFO] - Training Epoch: 1/2, step 43/23838 completed (loss: 6.991754531860352, acc: 0.0)
[2025-02-16 10:57:43,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:43,872][root][INFO] - Training Epoch: 1/2, step 44/23838 completed (loss: 6.905796051025391, acc: 0.02500000037252903)
[2025-02-16 10:57:44,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:44,304][root][INFO] - Training Epoch: 1/2, step 45/23838 completed (loss: 6.315135955810547, acc: 0.036363635212183)
[2025-02-16 10:57:44,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:44,798][root][INFO] - Training Epoch: 1/2, step 46/23838 completed (loss: 7.437958717346191, acc: 0.03846153989434242)
[2025-02-16 10:57:44,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:45,195][root][INFO] - Training Epoch: 1/2, step 47/23838 completed (loss: 7.697489261627197, acc: 0.0)
[2025-02-16 10:57:45,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:45,679][root][INFO] - Training Epoch: 1/2, step 48/23838 completed (loss: 5.320687770843506, acc: 0.056603774428367615)
[2025-02-16 10:57:45,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:46,151][root][INFO] - Training Epoch: 1/2, step 49/23838 completed (loss: 5.213812828063965, acc: 0.07920791953802109)
[2025-02-16 10:57:46,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:46,557][root][INFO] - Training Epoch: 1/2, step 50/23838 completed (loss: 5.414294242858887, acc: 0.08045977354049683)
[2025-02-16 10:57:46,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:47,055][root][INFO] - Training Epoch: 1/2, step 51/23838 completed (loss: 4.8494553565979, acc: 0.09459459781646729)
[2025-02-16 10:57:47,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:47,455][root][INFO] - Training Epoch: 1/2, step 52/23838 completed (loss: 5.31157112121582, acc: 0.03703703731298447)
[2025-02-16 10:57:47,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:47,873][root][INFO] - Training Epoch: 1/2, step 53/23838 completed (loss: 5.166654109954834, acc: 0.1111111119389534)
[2025-02-16 10:57:48,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:48,344][root][INFO] - Training Epoch: 1/2, step 54/23838 completed (loss: 5.532528877258301, acc: 0.04545454680919647)
[2025-02-16 10:57:48,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:48,824][root][INFO] - Training Epoch: 1/2, step 55/23838 completed (loss: 4.713996410369873, acc: 0.09433962404727936)
[2025-02-16 10:57:49,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:49,337][root][INFO] - Training Epoch: 1/2, step 56/23838 completed (loss: 4.846978664398193, acc: 0.0535714291036129)
[2025-02-16 10:57:49,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:49,818][root][INFO] - Training Epoch: 1/2, step 57/23838 completed (loss: 4.944425106048584, acc: 0.0796460211277008)
[2025-02-16 10:57:50,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:50,281][root][INFO] - Training Epoch: 1/2, step 58/23838 completed (loss: 5.059474945068359, acc: 0.06666667014360428)
[2025-02-16 10:57:50,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:50,815][root][INFO] - Training Epoch: 1/2, step 59/23838 completed (loss: 4.969229698181152, acc: 0.07216494530439377)
[2025-02-16 10:57:51,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:51,560][root][INFO] - Training Epoch: 1/2, step 60/23838 completed (loss: 4.593339920043945, acc: 0.09166666865348816)
[2025-02-16 10:57:51,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:51,999][root][INFO] - Training Epoch: 1/2, step 61/23838 completed (loss: 4.636696815490723, acc: 0.10619468986988068)
[2025-02-16 10:57:52,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:52,502][root][INFO] - Training Epoch: 1/2, step 62/23838 completed (loss: 4.6773223876953125, acc: 0.09890110045671463)
[2025-02-16 10:57:52,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:53,003][root][INFO] - Training Epoch: 1/2, step 63/23838 completed (loss: 4.730898380279541, acc: 0.06306306272745132)
[2025-02-16 10:57:53,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:53,406][root][INFO] - Training Epoch: 1/2, step 64/23838 completed (loss: 4.781951427459717, acc: 0.056338027119636536)
[2025-02-16 10:57:53,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:53,854][root][INFO] - Training Epoch: 1/2, step 65/23838 completed (loss: 4.427630424499512, acc: 0.13709677755832672)
[2025-02-16 10:57:54,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:54,303][root][INFO] - Training Epoch: 1/2, step 66/23838 completed (loss: 4.893350124359131, acc: 0.12727272510528564)
[2025-02-16 10:57:54,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:54,688][root][INFO] - Training Epoch: 1/2, step 67/23838 completed (loss: 4.87606954574585, acc: 0.058252427726984024)
[2025-02-16 10:57:54,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:55,104][root][INFO] - Training Epoch: 1/2, step 68/23838 completed (loss: 4.702288627624512, acc: 0.125)
[2025-02-16 10:57:55,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:55,490][root][INFO] - Training Epoch: 1/2, step 69/23838 completed (loss: 5.572062015533447, acc: 0.19230769574642181)
[2025-02-16 10:57:55,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:55,957][root][INFO] - Training Epoch: 1/2, step 70/23838 completed (loss: 4.210644245147705, acc: 0.2380952388048172)
[2025-02-16 10:57:56,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:56,381][root][INFO] - Training Epoch: 1/2, step 71/23838 completed (loss: 4.842371940612793, acc: 0.17142857611179352)
[2025-02-16 10:57:56,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:57,044][root][INFO] - Training Epoch: 1/2, step 72/23838 completed (loss: 5.307345390319824, acc: 0.15625)
[2025-02-16 10:57:57,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:57,588][root][INFO] - Training Epoch: 1/2, step 73/23838 completed (loss: 4.877127647399902, acc: 0.08695652335882187)
[2025-02-16 10:57:57,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:58,003][root][INFO] - Training Epoch: 1/2, step 74/23838 completed (loss: 5.2880473136901855, acc: 0.07692307978868484)
[2025-02-16 10:57:58,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:58,462][root][INFO] - Training Epoch: 1/2, step 75/23838 completed (loss: 5.730158805847168, acc: 0.07999999821186066)
[2025-02-16 10:57:58,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:58,877][root][INFO] - Training Epoch: 1/2, step 76/23838 completed (loss: 5.398616313934326, acc: 0.043478261679410934)
[2025-02-16 10:57:59,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:59,464][root][INFO] - Training Epoch: 1/2, step 77/23838 completed (loss: 3.876857280731201, acc: 0.1525423675775528)
[2025-02-16 10:57:59,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:57:59,855][root][INFO] - Training Epoch: 1/2, step 78/23838 completed (loss: 5.165159702301025, acc: 0.03448275849223137)
[2025-02-16 10:58:00,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:00,513][root][INFO] - Training Epoch: 1/2, step 79/23838 completed (loss: 4.8743133544921875, acc: 0.0)
[2025-02-16 10:58:00,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:00,965][root][INFO] - Training Epoch: 1/2, step 80/23838 completed (loss: 4.009058475494385, acc: 0.20000000298023224)
[2025-02-16 10:58:01,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:01,376][root][INFO] - Training Epoch: 1/2, step 81/23838 completed (loss: 4.041407108306885, acc: 0.21052631735801697)
[2025-02-16 10:58:01,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:01,824][root][INFO] - Training Epoch: 1/2, step 82/23838 completed (loss: 5.3854289054870605, acc: 0.0)
[2025-02-16 10:58:02,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:02,311][root][INFO] - Training Epoch: 1/2, step 83/23838 completed (loss: 5.0660600662231445, acc: 0.095238097012043)
[2025-02-16 10:58:02,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:02,748][root][INFO] - Training Epoch: 1/2, step 84/23838 completed (loss: 4.640025615692139, acc: 0.0634920671582222)
[2025-02-16 10:58:03,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:03,260][root][INFO] - Training Epoch: 1/2, step 85/23838 completed (loss: 4.3888349533081055, acc: 0.021276595070958138)
[2025-02-16 10:58:03,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:03,708][root][INFO] - Training Epoch: 1/2, step 86/23838 completed (loss: 4.573533058166504, acc: 0.0625)
[2025-02-16 10:58:04,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:04,344][root][INFO] - Training Epoch: 1/2, step 87/23838 completed (loss: 4.098526954650879, acc: 0.10447761416435242)
[2025-02-16 10:58:04,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:04,739][root][INFO] - Training Epoch: 1/2, step 88/23838 completed (loss: 4.616746425628662, acc: 0.06521739065647125)
[2025-02-16 10:58:04,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:05,164][root][INFO] - Training Epoch: 1/2, step 89/23838 completed (loss: 4.26881742477417, acc: 0.08791209012269974)
[2025-02-16 10:58:05,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:05,652][root][INFO] - Training Epoch: 1/2, step 90/23838 completed (loss: 4.307197093963623, acc: 0.15492957830429077)
[2025-02-16 10:58:05,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:06,175][root][INFO] - Training Epoch: 1/2, step 91/23838 completed (loss: 4.046860694885254, acc: 0.10526315867900848)
[2025-02-16 10:58:06,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:06,577][root][INFO] - Training Epoch: 1/2, step 92/23838 completed (loss: 3.907604932785034, acc: 0.11538461595773697)
[2025-02-16 10:58:06,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:06,977][root][INFO] - Training Epoch: 1/2, step 93/23838 completed (loss: 4.07783842086792, acc: 0.12790697813034058)
[2025-02-16 10:58:07,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:07,429][root][INFO] - Training Epoch: 1/2, step 94/23838 completed (loss: 3.8319263458251953, acc: 0.2073170691728592)
[2025-02-16 10:58:07,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:07,808][root][INFO] - Training Epoch: 1/2, step 95/23838 completed (loss: 4.616451263427734, acc: 0.1666666716337204)
[2025-02-16 10:58:07,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:08,197][root][INFO] - Training Epoch: 1/2, step 96/23838 completed (loss: 4.5047712326049805, acc: 0.125)
[2025-02-16 10:58:08,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:08,634][root][INFO] - Training Epoch: 1/2, step 97/23838 completed (loss: 2.3624114990234375, acc: 0.5600000023841858)
[2025-02-16 10:58:08,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:09,080][root][INFO] - Training Epoch: 1/2, step 98/23838 completed (loss: 4.480340003967285, acc: 0.09090909361839294)
[2025-02-16 10:58:09,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:09,508][root][INFO] - Training Epoch: 1/2, step 99/23838 completed (loss: 2.2645251750946045, acc: 0.42105263471603394)
[2025-02-16 10:58:09,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:09,975][root][INFO] - Training Epoch: 1/2, step 100/23838 completed (loss: 4.39900016784668, acc: 0.23529411852359772)
[2025-02-16 10:58:10,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:10,406][root][INFO] - Training Epoch: 1/2, step 101/23838 completed (loss: 3.789018392562866, acc: 0.20000000298023224)
[2025-02-16 10:58:10,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:10,829][root][INFO] - Training Epoch: 1/2, step 102/23838 completed (loss: 3.645216226577759, acc: 0.3181818127632141)
[2025-02-16 10:58:11,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:11,271][root][INFO] - Training Epoch: 1/2, step 103/23838 completed (loss: 3.1877875328063965, acc: 0.20000000298023224)
[2025-02-16 10:58:11,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:11,723][root][INFO] - Training Epoch: 1/2, step 104/23838 completed (loss: 3.537121295928955, acc: 0.3478260934352875)
[2025-02-16 10:58:11,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:12,115][root][INFO] - Training Epoch: 1/2, step 105/23838 completed (loss: 3.1583094596862793, acc: 0.1764705926179886)
[2025-02-16 10:58:12,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:12,501][root][INFO] - Training Epoch: 1/2, step 106/23838 completed (loss: 3.348281145095825, acc: 0.24242424964904785)
[2025-02-16 10:58:12,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:12,936][root][INFO] - Training Epoch: 1/2, step 107/23838 completed (loss: 2.398231267929077, acc: 0.4000000059604645)
[2025-02-16 10:58:13,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:13,413][root][INFO] - Training Epoch: 1/2, step 108/23838 completed (loss: 2.298184394836426, acc: 0.40740740299224854)
[2025-02-16 10:58:13,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:13,830][root][INFO] - Training Epoch: 1/2, step 109/23838 completed (loss: 3.635648012161255, acc: 0.1764705926179886)
[2025-02-16 10:58:14,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:14,308][root][INFO] - Training Epoch: 1/2, step 110/23838 completed (loss: 2.782921075820923, acc: 0.4193548262119293)
[2025-02-16 10:58:14,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:14,738][root][INFO] - Training Epoch: 1/2, step 111/23838 completed (loss: 1.9431923627853394, acc: 0.5416666865348816)
[2025-02-16 10:58:14,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:15,161][root][INFO] - Training Epoch: 1/2, step 112/23838 completed (loss: 2.5117740631103516, acc: 0.3103448152542114)
[2025-02-16 10:58:15,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:15,620][root][INFO] - Training Epoch: 1/2, step 113/23838 completed (loss: 2.3924834728240967, acc: 0.4166666567325592)
[2025-02-16 10:58:15,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:16,026][root][INFO] - Training Epoch: 1/2, step 114/23838 completed (loss: 2.7297189235687256, acc: 0.4444444477558136)
[2025-02-16 10:58:16,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:16,425][root][INFO] - Training Epoch: 1/2, step 115/23838 completed (loss: 4.415712833404541, acc: 0.1525423675775528)
[2025-02-16 10:58:16,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:16,920][root][INFO] - Training Epoch: 1/2, step 116/23838 completed (loss: 3.949740409851074, acc: 0.15068493783473969)
[2025-02-16 10:58:17,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:17,317][root][INFO] - Training Epoch: 1/2, step 117/23838 completed (loss: 3.5532639026641846, acc: 0.2291666716337204)
[2025-02-16 10:58:17,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:17,711][root][INFO] - Training Epoch: 1/2, step 118/23838 completed (loss: 4.344669342041016, acc: 0.11764705926179886)
[2025-02-16 10:58:17,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:18,161][root][INFO] - Training Epoch: 1/2, step 119/23838 completed (loss: 4.041775226593018, acc: 0.19148936867713928)
[2025-02-16 10:58:18,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:18,610][root][INFO] - Training Epoch: 1/2, step 120/23838 completed (loss: 3.9748284816741943, acc: 0.1489361673593521)
[2025-02-16 10:58:18,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:19,113][root][INFO] - Training Epoch: 1/2, step 121/23838 completed (loss: 3.488142728805542, acc: 0.19512194395065308)
[2025-02-16 10:58:19,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:19,528][root][INFO] - Training Epoch: 1/2, step 122/23838 completed (loss: 3.692452907562256, acc: 0.1190476194024086)
[2025-02-16 10:58:19,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:19,897][root][INFO] - Training Epoch: 1/2, step 123/23838 completed (loss: 3.4564809799194336, acc: 0.1666666716337204)
[2025-02-16 10:58:20,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:20,351][root][INFO] - Training Epoch: 1/2, step 124/23838 completed (loss: 3.9895541667938232, acc: 0.15000000596046448)
[2025-02-16 10:58:20,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:20,744][root][INFO] - Training Epoch: 1/2, step 125/23838 completed (loss: 3.3859992027282715, acc: 0.1621621549129486)
[2025-02-16 10:58:20,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:21,195][root][INFO] - Training Epoch: 1/2, step 126/23838 completed (loss: 3.5406668186187744, acc: 0.190476194024086)
[2025-02-16 10:58:21,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:21,633][root][INFO] - Training Epoch: 1/2, step 127/23838 completed (loss: 3.0934135913848877, acc: 0.30666667222976685)
[2025-02-16 10:58:21,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:22,034][root][INFO] - Training Epoch: 1/2, step 128/23838 completed (loss: 3.4323599338531494, acc: 0.2133333384990692)
[2025-02-16 10:58:22,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:22,485][root][INFO] - Training Epoch: 1/2, step 129/23838 completed (loss: 3.0783886909484863, acc: 0.239130437374115)
[2025-02-16 10:58:22,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:22,896][root][INFO] - Training Epoch: 1/2, step 130/23838 completed (loss: 3.1564762592315674, acc: 0.24137930572032928)
[2025-02-16 10:58:23,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:23,295][root][INFO] - Training Epoch: 1/2, step 131/23838 completed (loss: 3.597385883331299, acc: 0.1666666716337204)
[2025-02-16 10:58:23,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:23,758][root][INFO] - Training Epoch: 1/2, step 132/23838 completed (loss: 3.039245128631592, acc: 0.21875)
[2025-02-16 10:58:23,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:24,212][root][INFO] - Training Epoch: 1/2, step 133/23838 completed (loss: 2.9741151332855225, acc: 0.2777777910232544)
[2025-02-16 10:58:24,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:24,686][root][INFO] - Training Epoch: 1/2, step 134/23838 completed (loss: 2.8450543880462646, acc: 0.23333333432674408)
[2025-02-16 10:58:24,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:25,115][root][INFO] - Training Epoch: 1/2, step 135/23838 completed (loss: 3.0756642818450928, acc: 0.23076923191547394)
[2025-02-16 10:58:25,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:25,565][root][INFO] - Training Epoch: 1/2, step 136/23838 completed (loss: 3.285576105117798, acc: 0.25581395626068115)
[2025-02-16 10:58:25,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:26,006][root][INFO] - Training Epoch: 1/2, step 137/23838 completed (loss: 2.9761595726013184, acc: 0.3174603283405304)
[2025-02-16 10:58:26,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:26,499][root][INFO] - Training Epoch: 1/2, step 138/23838 completed (loss: 2.9538094997406006, acc: 0.24390244483947754)
[2025-02-16 10:58:26,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:26,932][root][INFO] - Training Epoch: 1/2, step 139/23838 completed (loss: 2.9568824768066406, acc: 0.3478260934352875)
[2025-02-16 10:58:27,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:27,408][root][INFO] - Training Epoch: 1/2, step 140/23838 completed (loss: 3.1153457164764404, acc: 0.23529411852359772)
[2025-02-16 10:58:27,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:27,838][root][INFO] - Training Epoch: 1/2, step 141/23838 completed (loss: 3.4899818897247314, acc: 0.23333333432674408)
[2025-02-16 10:58:28,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:28,329][root][INFO] - Training Epoch: 1/2, step 142/23838 completed (loss: 3.2287790775299072, acc: 0.2800000011920929)
[2025-02-16 10:58:28,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:28,743][root][INFO] - Training Epoch: 1/2, step 143/23838 completed (loss: 3.0577399730682373, acc: 0.21568627655506134)
[2025-02-16 10:58:28,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:29,181][root][INFO] - Training Epoch: 1/2, step 144/23838 completed (loss: 2.575228691101074, acc: 0.3055555522441864)
[2025-02-16 10:58:29,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:29,619][root][INFO] - Training Epoch: 1/2, step 145/23838 completed (loss: 3.1432785987854004, acc: 0.2199999988079071)
[2025-02-16 10:58:29,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:30,087][root][INFO] - Training Epoch: 1/2, step 146/23838 completed (loss: 2.8524467945098877, acc: 0.2631579041481018)
[2025-02-16 10:58:30,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:30,590][root][INFO] - Training Epoch: 1/2, step 147/23838 completed (loss: 2.9888482093811035, acc: 0.2839506268501282)
[2025-02-16 10:58:30,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:31,026][root][INFO] - Training Epoch: 1/2, step 148/23838 completed (loss: 2.803938150405884, acc: 0.260869562625885)
[2025-02-16 10:58:31,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:31,457][root][INFO] - Training Epoch: 1/2, step 149/23838 completed (loss: 2.4583170413970947, acc: 0.4166666567325592)
[2025-02-16 10:58:31,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:31,939][root][INFO] - Training Epoch: 1/2, step 150/23838 completed (loss: 3.1036267280578613, acc: 0.296875)
[2025-02-16 10:58:32,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:32,387][root][INFO] - Training Epoch: 1/2, step 151/23838 completed (loss: 2.7574260234832764, acc: 0.2777777910232544)
[2025-02-16 10:58:32,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:32,808][root][INFO] - Training Epoch: 1/2, step 152/23838 completed (loss: 2.9022207260131836, acc: 0.37837839126586914)
[2025-02-16 10:58:33,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:33,280][root][INFO] - Training Epoch: 1/2, step 153/23838 completed (loss: 3.016268014907837, acc: 0.2195121943950653)
[2025-02-16 10:58:33,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:33,766][root][INFO] - Training Epoch: 1/2, step 154/23838 completed (loss: 2.5638060569763184, acc: 0.38333332538604736)
[2025-02-16 10:58:34,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:34,266][root][INFO] - Training Epoch: 1/2, step 155/23838 completed (loss: 2.6599199771881104, acc: 0.3717948794364929)
[2025-02-16 10:58:34,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:34,726][root][INFO] - Training Epoch: 1/2, step 156/23838 completed (loss: 2.777381658554077, acc: 0.29629629850387573)
[2025-02-16 10:58:34,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:35,162][root][INFO] - Training Epoch: 1/2, step 157/23838 completed (loss: 3.2071657180786133, acc: 0.23529411852359772)
[2025-02-16 10:58:35,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:35,622][root][INFO] - Training Epoch: 1/2, step 158/23838 completed (loss: 2.460735559463501, acc: 0.4545454680919647)
[2025-02-16 10:58:35,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:36,102][root][INFO] - Training Epoch: 1/2, step 159/23838 completed (loss: 2.8796231746673584, acc: 0.2777777910232544)
[2025-02-16 10:58:36,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:36,521][root][INFO] - Training Epoch: 1/2, step 160/23838 completed (loss: 2.774256944656372, acc: 0.3448275923728943)
[2025-02-16 10:58:36,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:36,983][root][INFO] - Training Epoch: 1/2, step 161/23838 completed (loss: 2.4582581520080566, acc: 0.5333333611488342)
[2025-02-16 10:58:37,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:37,445][root][INFO] - Training Epoch: 1/2, step 162/23838 completed (loss: 2.451756000518799, acc: 0.38461539149284363)
[2025-02-16 10:58:37,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:37,854][root][INFO] - Training Epoch: 1/2, step 163/23838 completed (loss: 2.625201463699341, acc: 0.36231884360313416)
[2025-02-16 10:58:38,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:38,383][root][INFO] - Training Epoch: 1/2, step 164/23838 completed (loss: 2.71382737159729, acc: 0.36274510622024536)
[2025-02-16 10:58:38,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:38,836][root][INFO] - Training Epoch: 1/2, step 165/23838 completed (loss: 2.749763250350952, acc: 0.30985915660858154)
[2025-02-16 10:58:39,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:39,327][root][INFO] - Training Epoch: 1/2, step 166/23838 completed (loss: 2.8168163299560547, acc: 0.2321428507566452)
[2025-02-16 10:58:39,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:39,744][root][INFO] - Training Epoch: 1/2, step 167/23838 completed (loss: 2.4760828018188477, acc: 0.38317757844924927)
[2025-02-16 10:58:39,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:40,225][root][INFO] - Training Epoch: 1/2, step 168/23838 completed (loss: 2.5236048698425293, acc: 0.4032258093357086)
[2025-02-16 10:58:40,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:40,675][root][INFO] - Training Epoch: 1/2, step 169/23838 completed (loss: 2.4641900062561035, acc: 0.41304346919059753)
[2025-02-16 10:58:40,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:41,120][root][INFO] - Training Epoch: 1/2, step 170/23838 completed (loss: 2.7762811183929443, acc: 0.3606557250022888)
[2025-02-16 10:58:41,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:41,536][root][INFO] - Training Epoch: 1/2, step 171/23838 completed (loss: 2.5341570377349854, acc: 0.4285714328289032)
[2025-02-16 10:58:41,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:41,978][root][INFO] - Training Epoch: 1/2, step 172/23838 completed (loss: 1.9961451292037964, acc: 0.6875)
[2025-02-16 10:58:42,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:42,400][root][INFO] - Training Epoch: 1/2, step 173/23838 completed (loss: 2.7612979412078857, acc: 0.28985506296157837)
[2025-02-16 10:58:42,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:42,794][root][INFO] - Training Epoch: 1/2, step 174/23838 completed (loss: 2.9703574180603027, acc: 0.3636363744735718)
[2025-02-16 10:58:43,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:43,251][root][INFO] - Training Epoch: 1/2, step 175/23838 completed (loss: 2.6922688484191895, acc: 0.30985915660858154)
[2025-02-16 10:58:43,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:43,735][root][INFO] - Training Epoch: 1/2, step 176/23838 completed (loss: 2.6215662956237793, acc: 0.3417721390724182)
[2025-02-16 10:58:43,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:44,214][root][INFO] - Training Epoch: 1/2, step 177/23838 completed (loss: 2.4025938510894775, acc: 0.3877550959587097)
[2025-02-16 10:58:44,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:44,696][root][INFO] - Training Epoch: 1/2, step 178/23838 completed (loss: 2.6648967266082764, acc: 0.35227271914482117)
[2025-02-16 10:58:44,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:45,154][root][INFO] - Training Epoch: 1/2, step 179/23838 completed (loss: 2.6266064643859863, acc: 0.3617021143436432)
[2025-02-16 10:58:45,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:45,581][root][INFO] - Training Epoch: 1/2, step 180/23838 completed (loss: 2.3825113773345947, acc: 0.375)
[2025-02-16 10:58:45,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:46,038][root][INFO] - Training Epoch: 1/2, step 181/23838 completed (loss: 2.4308412075042725, acc: 0.4098360538482666)
[2025-02-16 10:58:46,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:46,479][root][INFO] - Training Epoch: 1/2, step 182/23838 completed (loss: 2.8700292110443115, acc: 0.359375)
[2025-02-16 10:58:46,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:46,849][root][INFO] - Training Epoch: 1/2, step 183/23838 completed (loss: 2.4917783737182617, acc: 0.38333332538604736)
[2025-02-16 10:58:47,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:47,332][root][INFO] - Training Epoch: 1/2, step 184/23838 completed (loss: 2.4566781520843506, acc: 0.395061731338501)
[2025-02-16 10:58:47,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:47,866][root][INFO] - Training Epoch: 1/2, step 185/23838 completed (loss: 2.360262155532837, acc: 0.4333333373069763)
[2025-02-16 10:58:48,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:48,332][root][INFO] - Training Epoch: 1/2, step 186/23838 completed (loss: 2.7317798137664795, acc: 0.37704917788505554)
[2025-02-16 10:58:48,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:48,744][root][INFO] - Training Epoch: 1/2, step 187/23838 completed (loss: 2.496574878692627, acc: 0.4124999940395355)
[2025-02-16 10:58:48,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:49,232][root][INFO] - Training Epoch: 1/2, step 188/23838 completed (loss: 2.443873882293701, acc: 0.2970297038555145)
[2025-02-16 10:58:49,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:49,683][root][INFO] - Training Epoch: 1/2, step 189/23838 completed (loss: 2.3845925331115723, acc: 0.420560747385025)
[2025-02-16 10:58:49,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:50,252][root][INFO] - Training Epoch: 1/2, step 190/23838 completed (loss: 2.1044838428497314, acc: 0.5189873576164246)
[2025-02-16 10:58:50,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:50,667][root][INFO] - Training Epoch: 1/2, step 191/23838 completed (loss: 2.09865403175354, acc: 0.4516128897666931)
[2025-02-16 10:58:50,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:51,080][root][INFO] - Training Epoch: 1/2, step 192/23838 completed (loss: 1.707862138748169, acc: 0.5405405163764954)
[2025-02-16 10:58:51,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:51,549][root][INFO] - Training Epoch: 1/2, step 193/23838 completed (loss: 2.330172061920166, acc: 0.3684210479259491)
[2025-02-16 10:58:51,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:51,978][root][INFO] - Training Epoch: 1/2, step 194/23838 completed (loss: 2.399829149246216, acc: 0.3142857253551483)
[2025-02-16 10:58:52,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:52,410][root][INFO] - Training Epoch: 1/2, step 195/23838 completed (loss: 2.6376116275787354, acc: 0.2897196114063263)
[2025-02-16 10:58:52,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:52,850][root][INFO] - Training Epoch: 1/2, step 196/23838 completed (loss: 2.6668941974639893, acc: 0.261904776096344)
[2025-02-16 10:58:53,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:53,285][root][INFO] - Training Epoch: 1/2, step 197/23838 completed (loss: 3.4102885723114014, acc: 0.14814814925193787)
[2025-02-16 10:58:53,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:53,705][root][INFO] - Training Epoch: 1/2, step 198/23838 completed (loss: 3.2977488040924072, acc: 0.21621622145175934)
[2025-02-16 10:58:53,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:54,131][root][INFO] - Training Epoch: 1/2, step 199/23838 completed (loss: 2.368692398071289, acc: 0.42307692766189575)
[2025-02-16 10:58:54,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:54,535][root][INFO] - Training Epoch: 1/2, step 200/23838 completed (loss: 1.9737526178359985, acc: 0.37037035822868347)
[2025-02-16 10:58:54,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:54,951][root][INFO] - Training Epoch: 1/2, step 201/23838 completed (loss: 2.0472354888916016, acc: 0.4444444477558136)
[2025-02-16 10:58:55,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:55,360][root][INFO] - Training Epoch: 1/2, step 202/23838 completed (loss: 2.923969268798828, acc: 0.28125)
[2025-02-16 10:58:55,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:55,748][root][INFO] - Training Epoch: 1/2, step 203/23838 completed (loss: 2.4112725257873535, acc: 0.4375)
[2025-02-16 10:58:55,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:56,138][root][INFO] - Training Epoch: 1/2, step 204/23838 completed (loss: 2.489703893661499, acc: 0.2954545319080353)
[2025-02-16 10:58:56,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:56,592][root][INFO] - Training Epoch: 1/2, step 205/23838 completed (loss: 2.621795892715454, acc: 0.30000001192092896)
[2025-02-16 10:58:56,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:57,031][root][INFO] - Training Epoch: 1/2, step 206/23838 completed (loss: 2.4485766887664795, acc: 0.5600000023841858)
[2025-02-16 10:58:57,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:57,510][root][INFO] - Training Epoch: 1/2, step 207/23838 completed (loss: 3.0921683311462402, acc: 0.22727273404598236)
[2025-02-16 10:58:57,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:57,919][root][INFO] - Training Epoch: 1/2, step 208/23838 completed (loss: 2.3458776473999023, acc: 0.39534884691238403)
[2025-02-16 10:58:58,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:58,360][root][INFO] - Training Epoch: 1/2, step 209/23838 completed (loss: 2.5391480922698975, acc: 0.38461539149284363)
[2025-02-16 10:58:58,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:58,838][root][INFO] - Training Epoch: 1/2, step 210/23838 completed (loss: 1.9524896144866943, acc: 0.42307692766189575)
[2025-02-16 10:58:59,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:59,251][root][INFO] - Training Epoch: 1/2, step 211/23838 completed (loss: 2.225376844406128, acc: 0.3478260934352875)
[2025-02-16 10:58:59,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:58:59,704][root][INFO] - Training Epoch: 1/2, step 212/23838 completed (loss: 2.334665060043335, acc: 0.4285714328289032)
[2025-02-16 10:58:59,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:00,107][root][INFO] - Training Epoch: 1/2, step 213/23838 completed (loss: 2.4408185482025146, acc: 0.3684210479259491)
[2025-02-16 10:59:00,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:00,499][root][INFO] - Training Epoch: 1/2, step 214/23838 completed (loss: 2.2544379234313965, acc: 0.4642857015132904)
[2025-02-16 10:59:00,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:00,874][root][INFO] - Training Epoch: 1/2, step 215/23838 completed (loss: 2.5885818004608154, acc: 0.3333333432674408)
[2025-02-16 10:59:01,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:01,312][root][INFO] - Training Epoch: 1/2, step 216/23838 completed (loss: 2.285764217376709, acc: 0.4516128897666931)
[2025-02-16 10:59:01,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:01,783][root][INFO] - Training Epoch: 1/2, step 217/23838 completed (loss: 3.223440170288086, acc: 0.25)
[2025-02-16 10:59:02,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:02,246][root][INFO] - Training Epoch: 1/2, step 218/23838 completed (loss: 2.0718469619750977, acc: 0.5333333611488342)
[2025-02-16 10:59:02,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:02,632][root][INFO] - Training Epoch: 1/2, step 219/23838 completed (loss: 2.4265429973602295, acc: 0.3499999940395355)
[2025-02-16 10:59:02,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:03,066][root][INFO] - Training Epoch: 1/2, step 220/23838 completed (loss: 2.3931424617767334, acc: 0.38461539149284363)
[2025-02-16 10:59:03,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:03,487][root][INFO] - Training Epoch: 1/2, step 221/23838 completed (loss: 2.51631498336792, acc: 0.35483869910240173)
[2025-02-16 10:59:03,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:03,964][root][INFO] - Training Epoch: 1/2, step 222/23838 completed (loss: 2.4266083240509033, acc: 0.4000000059604645)
[2025-02-16 10:59:04,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:04,366][root][INFO] - Training Epoch: 1/2, step 223/23838 completed (loss: 2.112389087677002, acc: 0.523809552192688)
[2025-02-16 10:59:04,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:04,844][root][INFO] - Training Epoch: 1/2, step 224/23838 completed (loss: 2.789734125137329, acc: 0.3488371968269348)
[2025-02-16 10:59:05,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:05,261][root][INFO] - Training Epoch: 1/2, step 225/23838 completed (loss: 1.9777846336364746, acc: 0.4324324429035187)
[2025-02-16 10:59:05,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:05,634][root][INFO] - Training Epoch: 1/2, step 226/23838 completed (loss: 2.0099096298217773, acc: 0.5652173757553101)
[2025-02-16 10:59:05,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:06,022][root][INFO] - Training Epoch: 1/2, step 227/23838 completed (loss: 2.0211687088012695, acc: 0.46875)
[2025-02-16 10:59:06,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:06,427][root][INFO] - Training Epoch: 1/2, step 228/23838 completed (loss: 2.6751983165740967, acc: 0.3333333432674408)
[2025-02-16 10:59:06,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:06,826][root][INFO] - Training Epoch: 1/2, step 229/23838 completed (loss: 2.139092445373535, acc: 0.3333333432674408)
[2025-02-16 10:59:07,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:07,238][root][INFO] - Training Epoch: 1/2, step 230/23838 completed (loss: 2.626655340194702, acc: 0.2916666567325592)
[2025-02-16 10:59:07,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:07,695][root][INFO] - Training Epoch: 1/2, step 231/23838 completed (loss: 1.4166470766067505, acc: 0.6666666865348816)
[2025-02-16 10:59:07,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:08,086][root][INFO] - Training Epoch: 1/2, step 232/23838 completed (loss: 1.832113265991211, acc: 0.5)
[2025-02-16 10:59:08,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:08,596][root][INFO] - Training Epoch: 1/2, step 233/23838 completed (loss: 2.5888500213623047, acc: 0.3684210479259491)
[2025-02-16 10:59:08,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:09,010][root][INFO] - Training Epoch: 1/2, step 234/23838 completed (loss: 0.7443223595619202, acc: 0.75)
[2025-02-16 10:59:09,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:09,394][root][INFO] - Training Epoch: 1/2, step 235/23838 completed (loss: 2.912944793701172, acc: 0.27272728085517883)
[2025-02-16 10:59:09,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:09,779][root][INFO] - Training Epoch: 1/2, step 236/23838 completed (loss: 2.8333659172058105, acc: 0.28125)
[2025-02-16 10:59:09,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:10,204][root][INFO] - Training Epoch: 1/2, step 237/23838 completed (loss: 2.259500026702881, acc: 0.36000001430511475)
[2025-02-16 10:59:10,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:10,590][root][INFO] - Training Epoch: 1/2, step 238/23838 completed (loss: 2.7723631858825684, acc: 0.30000001192092896)
[2025-02-16 10:59:10,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:11,021][root][INFO] - Training Epoch: 1/2, step 239/23838 completed (loss: 2.7781002521514893, acc: 0.2708333432674408)
[2025-02-16 10:59:11,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:11,443][root][INFO] - Training Epoch: 1/2, step 240/23838 completed (loss: 1.3658400774002075, acc: 0.6071428656578064)
[2025-02-16 10:59:11,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:11,849][root][INFO] - Training Epoch: 1/2, step 241/23838 completed (loss: 2.600761890411377, acc: 0.25531914830207825)
[2025-02-16 10:59:12,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:12,427][root][INFO] - Training Epoch: 1/2, step 242/23838 completed (loss: 3.067736864089966, acc: 0.21052631735801697)
[2025-02-16 10:59:12,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:12,879][root][INFO] - Training Epoch: 1/2, step 243/23838 completed (loss: 2.483366012573242, acc: 0.3478260934352875)
[2025-02-16 10:59:13,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:13,294][root][INFO] - Training Epoch: 1/2, step 244/23838 completed (loss: 3.088536500930786, acc: 0.3333333432674408)
[2025-02-16 10:59:13,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:13,675][root][INFO] - Training Epoch: 1/2, step 245/23838 completed (loss: 3.0839810371398926, acc: 0.25)
[2025-02-16 10:59:13,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:14,087][root][INFO] - Training Epoch: 1/2, step 246/23838 completed (loss: 2.162541627883911, acc: 0.5)
[2025-02-16 10:59:14,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:14,441][root][INFO] - Training Epoch: 1/2, step 247/23838 completed (loss: 2.536743402481079, acc: 0.3333333432674408)
[2025-02-16 10:59:14,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:14,867][root][INFO] - Training Epoch: 1/2, step 248/23838 completed (loss: 2.4199092388153076, acc: 0.3888888955116272)
[2025-02-16 10:59:15,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:15,277][root][INFO] - Training Epoch: 1/2, step 249/23838 completed (loss: 2.574218511581421, acc: 0.25581395626068115)
[2025-02-16 10:59:15,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:15,900][root][INFO] - Training Epoch: 1/2, step 250/23838 completed (loss: 2.9484951496124268, acc: 0.32258063554763794)
[2025-02-16 10:59:16,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:16,321][root][INFO] - Training Epoch: 1/2, step 251/23838 completed (loss: 2.2877449989318848, acc: 0.46341463923454285)
[2025-02-16 10:59:16,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:16,807][root][INFO] - Training Epoch: 1/2, step 252/23838 completed (loss: 2.7718687057495117, acc: 0.20512820780277252)
[2025-02-16 10:59:17,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:17,236][root][INFO] - Training Epoch: 1/2, step 253/23838 completed (loss: 2.4623076915740967, acc: 0.4000000059604645)
[2025-02-16 10:59:17,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:17,654][root][INFO] - Training Epoch: 1/2, step 254/23838 completed (loss: 2.402388095855713, acc: 0.3333333432674408)
[2025-02-16 10:59:17,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:18,032][root][INFO] - Training Epoch: 1/2, step 255/23838 completed (loss: 0.7992782592773438, acc: 0.7307692170143127)
[2025-02-16 10:59:18,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:18,477][root][INFO] - Training Epoch: 1/2, step 256/23838 completed (loss: 1.6917870044708252, acc: 0.625)
[2025-02-16 10:59:18,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:18,936][root][INFO] - Training Epoch: 1/2, step 257/23838 completed (loss: 3.2561638355255127, acc: 0.21739129722118378)
[2025-02-16 10:59:19,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:19,384][root][INFO] - Training Epoch: 1/2, step 258/23838 completed (loss: 2.75295090675354, acc: 0.3243243098258972)
[2025-02-16 10:59:19,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:19,894][root][INFO] - Training Epoch: 1/2, step 259/23838 completed (loss: 2.7551023960113525, acc: 0.30188679695129395)
[2025-02-16 10:59:20,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:20,357][root][INFO] - Training Epoch: 1/2, step 260/23838 completed (loss: 2.7291946411132812, acc: 0.31168830394744873)
[2025-02-16 10:59:20,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:20,761][root][INFO] - Training Epoch: 1/2, step 261/23838 completed (loss: 2.7093420028686523, acc: 0.3239436745643616)
[2025-02-16 10:59:20,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:21,112][root][INFO] - Training Epoch: 1/2, step 262/23838 completed (loss: 2.3961689472198486, acc: 0.44897958636283875)
[2025-02-16 10:59:21,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:21,517][root][INFO] - Training Epoch: 1/2, step 263/23838 completed (loss: 3.042243242263794, acc: 0.2222222238779068)
[2025-02-16 10:59:21,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:21,958][root][INFO] - Training Epoch: 1/2, step 264/23838 completed (loss: 3.104585886001587, acc: 0.24444444477558136)
[2025-02-16 10:59:22,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:22,375][root][INFO] - Training Epoch: 1/2, step 265/23838 completed (loss: 2.6725473403930664, acc: 0.3181818127632141)
[2025-02-16 10:59:22,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:22,839][root][INFO] - Training Epoch: 1/2, step 266/23838 completed (loss: 2.5807390213012695, acc: 0.3617021143436432)
[2025-02-16 10:59:23,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:23,272][root][INFO] - Training Epoch: 1/2, step 267/23838 completed (loss: 2.2814416885375977, acc: 0.42424243688583374)
[2025-02-16 10:59:23,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:23,743][root][INFO] - Training Epoch: 1/2, step 268/23838 completed (loss: 2.3646812438964844, acc: 0.3488371968269348)
[2025-02-16 10:59:23,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:24,211][root][INFO] - Training Epoch: 1/2, step 269/23838 completed (loss: 2.4524850845336914, acc: 0.28333333134651184)
[2025-02-16 10:59:24,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:24,680][root][INFO] - Training Epoch: 1/2, step 270/23838 completed (loss: 2.5039658546447754, acc: 0.31111112236976624)
[2025-02-16 10:59:24,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:25,172][root][INFO] - Training Epoch: 1/2, step 271/23838 completed (loss: 2.2354094982147217, acc: 0.46666666865348816)
[2025-02-16 10:59:25,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:25,621][root][INFO] - Training Epoch: 1/2, step 272/23838 completed (loss: 2.3612098693847656, acc: 0.35593220591545105)
[2025-02-16 10:59:25,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:26,097][root][INFO] - Training Epoch: 1/2, step 273/23838 completed (loss: 2.0135297775268555, acc: 0.5185185074806213)
[2025-02-16 10:59:26,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:26,562][root][INFO] - Training Epoch: 1/2, step 274/23838 completed (loss: 2.095568895339966, acc: 0.5135135054588318)
[2025-02-16 10:59:26,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:27,012][root][INFO] - Training Epoch: 1/2, step 275/23838 completed (loss: 2.3036856651306152, acc: 0.47058823704719543)
[2025-02-16 10:59:27,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:27,400][root][INFO] - Training Epoch: 1/2, step 276/23838 completed (loss: 1.8643057346343994, acc: 0.5199999809265137)
[2025-02-16 10:59:27,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:27,795][root][INFO] - Training Epoch: 1/2, step 277/23838 completed (loss: 1.567300796508789, acc: 0.5263158082962036)
[2025-02-16 10:59:28,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:28,227][root][INFO] - Training Epoch: 1/2, step 278/23838 completed (loss: 2.3984761238098145, acc: 0.4000000059604645)
[2025-02-16 10:59:28,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:28,671][root][INFO] - Training Epoch: 1/2, step 279/23838 completed (loss: 2.557948589324951, acc: 0.2857142984867096)
[2025-02-16 10:59:28,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:29,108][root][INFO] - Training Epoch: 1/2, step 280/23838 completed (loss: 2.675471305847168, acc: 0.2750000059604645)
[2025-02-16 10:59:29,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:29,575][root][INFO] - Training Epoch: 1/2, step 281/23838 completed (loss: 2.2721660137176514, acc: 0.4019607901573181)
[2025-02-16 10:59:29,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:30,033][root][INFO] - Training Epoch: 1/2, step 282/23838 completed (loss: 1.8151695728302002, acc: 0.5681818127632141)
[2025-02-16 10:59:30,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:30,435][root][INFO] - Training Epoch: 1/2, step 283/23838 completed (loss: 1.7714794874191284, acc: 0.5416666865348816)
[2025-02-16 10:59:30,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:30,887][root][INFO] - Training Epoch: 1/2, step 284/23838 completed (loss: 1.9542473554611206, acc: 0.5)
[2025-02-16 10:59:31,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:31,352][root][INFO] - Training Epoch: 1/2, step 285/23838 completed (loss: 2.365222692489624, acc: 0.34545454382896423)
[2025-02-16 10:59:31,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:31,753][root][INFO] - Training Epoch: 1/2, step 286/23838 completed (loss: 2.44254732131958, acc: 0.3448275923728943)
[2025-02-16 10:59:31,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:32,227][root][INFO] - Training Epoch: 1/2, step 287/23838 completed (loss: 2.5484213829040527, acc: 0.3510638177394867)
[2025-02-16 10:59:32,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:32,628][root][INFO] - Training Epoch: 1/2, step 288/23838 completed (loss: 2.7414021492004395, acc: 0.34567901492118835)
[2025-02-16 10:59:32,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:33,084][root][INFO] - Training Epoch: 1/2, step 289/23838 completed (loss: 2.53637957572937, acc: 0.27272728085517883)
[2025-02-16 10:59:33,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:33,515][root][INFO] - Training Epoch: 1/2, step 290/23838 completed (loss: 2.930818557739258, acc: 0.23456789553165436)
[2025-02-16 10:59:33,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:33,946][root][INFO] - Training Epoch: 1/2, step 291/23838 completed (loss: 2.479667901992798, acc: 0.302325576543808)
[2025-02-16 10:59:34,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:34,345][root][INFO] - Training Epoch: 1/2, step 292/23838 completed (loss: 2.6329874992370605, acc: 0.2876712381839752)
[2025-02-16 10:59:34,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:34,800][root][INFO] - Training Epoch: 1/2, step 293/23838 completed (loss: 2.0719170570373535, acc: 0.3928571343421936)
[2025-02-16 10:59:35,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:35,274][root][INFO] - Training Epoch: 1/2, step 294/23838 completed (loss: 2.5125882625579834, acc: 0.35922330617904663)
[2025-02-16 10:59:35,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:35,709][root][INFO] - Training Epoch: 1/2, step 295/23838 completed (loss: 2.3731987476348877, acc: 0.4845360815525055)
[2025-02-16 10:59:35,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:36,139][root][INFO] - Training Epoch: 1/2, step 296/23838 completed (loss: 3.0277130603790283, acc: 0.2641509473323822)
[2025-02-16 10:59:36,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:36,607][root][INFO] - Training Epoch: 1/2, step 297/23838 completed (loss: 2.5899243354797363, acc: 0.2747252881526947)
[2025-02-16 10:59:36,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:37,074][root][INFO] - Training Epoch: 1/2, step 298/23838 completed (loss: 2.7946090698242188, acc: 0.2954545319080353)
[2025-02-16 10:59:37,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:37,532][root][INFO] - Training Epoch: 1/2, step 299/23838 completed (loss: 2.8460779190063477, acc: 0.23021583259105682)
[2025-02-16 10:59:37,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:37,962][root][INFO] - Training Epoch: 1/2, step 300/23838 completed (loss: 2.682081699371338, acc: 0.2649572789669037)
[2025-02-16 10:59:38,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:38,414][root][INFO] - Training Epoch: 1/2, step 301/23838 completed (loss: 2.6271352767944336, acc: 0.3076923191547394)
[2025-02-16 10:59:38,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:38,805][root][INFO] - Training Epoch: 1/2, step 302/23838 completed (loss: 2.619328498840332, acc: 0.34375)
[2025-02-16 10:59:39,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:39,246][root][INFO] - Training Epoch: 1/2, step 303/23838 completed (loss: 2.739475965499878, acc: 0.3188405930995941)
[2025-02-16 10:59:39,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:39,673][root][INFO] - Training Epoch: 1/2, step 304/23838 completed (loss: 2.3165664672851562, acc: 0.39759036898612976)
[2025-02-16 10:59:39,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:40,127][root][INFO] - Training Epoch: 1/2, step 305/23838 completed (loss: 2.7715768814086914, acc: 0.29629629850387573)
[2025-02-16 10:59:40,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:40,586][root][INFO] - Training Epoch: 1/2, step 306/23838 completed (loss: 2.780928134918213, acc: 0.20000000298023224)
[2025-02-16 10:59:40,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:41,036][root][INFO] - Training Epoch: 1/2, step 307/23838 completed (loss: 2.488255023956299, acc: 0.3720930218696594)
[2025-02-16 10:59:41,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:41,484][root][INFO] - Training Epoch: 1/2, step 308/23838 completed (loss: 2.584362268447876, acc: 0.3333333432674408)
[2025-02-16 10:59:41,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:41,883][root][INFO] - Training Epoch: 1/2, step 309/23838 completed (loss: 2.447726249694824, acc: 0.3384615480899811)
[2025-02-16 10:59:42,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:42,336][root][INFO] - Training Epoch: 1/2, step 310/23838 completed (loss: 2.618631362915039, acc: 0.27272728085517883)
[2025-02-16 10:59:42,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:42,825][root][INFO] - Training Epoch: 1/2, step 311/23838 completed (loss: 2.2050092220306396, acc: 0.3777777850627899)
[2025-02-16 10:59:43,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:43,341][root][INFO] - Training Epoch: 1/2, step 312/23838 completed (loss: 2.6823885440826416, acc: 0.31683167815208435)
[2025-02-16 10:59:43,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:43,732][root][INFO] - Training Epoch: 1/2, step 313/23838 completed (loss: 2.4924092292785645, acc: 0.25)
[2025-02-16 10:59:43,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:44,196][root][INFO] - Training Epoch: 1/2, step 314/23838 completed (loss: 2.863407611846924, acc: 0.30000001192092896)
[2025-02-16 10:59:44,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:44,589][root][INFO] - Training Epoch: 1/2, step 315/23838 completed (loss: 2.440103769302368, acc: 0.3544303774833679)
[2025-02-16 10:59:44,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:45,004][root][INFO] - Training Epoch: 1/2, step 316/23838 completed (loss: 2.7682740688323975, acc: 0.2460317462682724)
[2025-02-16 10:59:45,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:45,389][root][INFO] - Training Epoch: 1/2, step 317/23838 completed (loss: 2.68017315864563, acc: 0.25)
[2025-02-16 10:59:45,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:45,835][root][INFO] - Training Epoch: 1/2, step 318/23838 completed (loss: 2.2093567848205566, acc: 0.3799999952316284)
[2025-02-16 10:59:46,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:46,244][root][INFO] - Training Epoch: 1/2, step 319/23838 completed (loss: 2.445629835128784, acc: 0.29032257199287415)
[2025-02-16 10:59:46,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:46,629][root][INFO] - Training Epoch: 1/2, step 320/23838 completed (loss: 2.0444953441619873, acc: 0.4032258093357086)
[2025-02-16 10:59:46,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:47,015][root][INFO] - Training Epoch: 1/2, step 321/23838 completed (loss: 2.6909701824188232, acc: 0.25806450843811035)
[2025-02-16 10:59:47,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:47,406][root][INFO] - Training Epoch: 1/2, step 322/23838 completed (loss: 2.18546462059021, acc: 0.41860464215278625)
[2025-02-16 10:59:47,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:47,787][root][INFO] - Training Epoch: 1/2, step 323/23838 completed (loss: 2.5071163177490234, acc: 0.25581395626068115)
[2025-02-16 10:59:47,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:48,156][root][INFO] - Training Epoch: 1/2, step 324/23838 completed (loss: 2.6068243980407715, acc: 0.2954545319080353)
[2025-02-16 10:59:48,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:48,569][root][INFO] - Training Epoch: 1/2, step 325/23838 completed (loss: 2.412353277206421, acc: 0.3928571343421936)
[2025-02-16 10:59:48,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:49,035][root][INFO] - Training Epoch: 1/2, step 326/23838 completed (loss: 2.473038911819458, acc: 0.296875)
[2025-02-16 10:59:49,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:49,475][root][INFO] - Training Epoch: 1/2, step 327/23838 completed (loss: 2.2977919578552246, acc: 0.4285714328289032)
[2025-02-16 10:59:49,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:49,917][root][INFO] - Training Epoch: 1/2, step 328/23838 completed (loss: 2.3465747833251953, acc: 0.35849055647850037)
[2025-02-16 10:59:50,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:50,369][root][INFO] - Training Epoch: 1/2, step 329/23838 completed (loss: 2.677696466445923, acc: 0.2647058963775635)
[2025-02-16 10:59:50,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:50,861][root][INFO] - Training Epoch: 1/2, step 330/23838 completed (loss: 2.4850709438323975, acc: 0.2551020383834839)
[2025-02-16 10:59:51,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:51,340][root][INFO] - Training Epoch: 1/2, step 331/23838 completed (loss: 2.0499486923217773, acc: 0.39393940567970276)
[2025-02-16 10:59:51,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:51,836][root][INFO] - Training Epoch: 1/2, step 332/23838 completed (loss: 2.7489073276519775, acc: 0.28205129504203796)
[2025-02-16 10:59:52,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:52,267][root][INFO] - Training Epoch: 1/2, step 333/23838 completed (loss: 2.554133415222168, acc: 0.22857142984867096)
[2025-02-16 10:59:52,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:52,700][root][INFO] - Training Epoch: 1/2, step 334/23838 completed (loss: 2.806950807571411, acc: 0.21212121844291687)
[2025-02-16 10:59:52,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:53,111][root][INFO] - Training Epoch: 1/2, step 335/23838 completed (loss: 1.945455551147461, acc: 0.4545454680919647)
[2025-02-16 10:59:53,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:53,602][root][INFO] - Training Epoch: 1/2, step 336/23838 completed (loss: 2.4512012004852295, acc: 0.2874999940395355)
[2025-02-16 10:59:53,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:54,021][root][INFO] - Training Epoch: 1/2, step 337/23838 completed (loss: 2.7085115909576416, acc: 0.2083333283662796)
[2025-02-16 10:59:54,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:54,450][root][INFO] - Training Epoch: 1/2, step 338/23838 completed (loss: 2.620572805404663, acc: 0.353658527135849)
[2025-02-16 10:59:54,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:54,830][root][INFO] - Training Epoch: 1/2, step 339/23838 completed (loss: 2.5290908813476562, acc: 0.3203883469104767)
[2025-02-16 10:59:55,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:55,286][root][INFO] - Training Epoch: 1/2, step 340/23838 completed (loss: 2.6129682064056396, acc: 0.3086419701576233)
[2025-02-16 10:59:55,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:55,794][root][INFO] - Training Epoch: 1/2, step 341/23838 completed (loss: 2.3387725353240967, acc: 0.44999998807907104)
[2025-02-16 10:59:56,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:56,274][root][INFO] - Training Epoch: 1/2, step 342/23838 completed (loss: 2.6418070793151855, acc: 0.2772277295589447)
[2025-02-16 10:59:56,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:56,810][root][INFO] - Training Epoch: 1/2, step 343/23838 completed (loss: 2.6714463233947754, acc: 0.2800000011920929)
[2025-02-16 10:59:57,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:57,264][root][INFO] - Training Epoch: 1/2, step 344/23838 completed (loss: 2.449537992477417, acc: 0.2916666567325592)
[2025-02-16 10:59:57,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:57,738][root][INFO] - Training Epoch: 1/2, step 345/23838 completed (loss: 2.6227927207946777, acc: 0.2295081913471222)
[2025-02-16 10:59:58,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:58,276][root][INFO] - Training Epoch: 1/2, step 346/23838 completed (loss: 2.8029401302337646, acc: 0.24418604373931885)
[2025-02-16 10:59:58,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:58,712][root][INFO] - Training Epoch: 1/2, step 347/23838 completed (loss: 2.3890621662139893, acc: 0.3199999928474426)
[2025-02-16 10:59:58,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:59,182][root][INFO] - Training Epoch: 1/2, step 348/23838 completed (loss: 2.156238317489624, acc: 0.44736841320991516)
[2025-02-16 10:59:59,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:59,592][root][INFO] - Training Epoch: 1/2, step 349/23838 completed (loss: 2.6589813232421875, acc: 0.25287356972694397)
[2025-02-16 10:59:59,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 10:59:59,998][root][INFO] - Training Epoch: 1/2, step 350/23838 completed (loss: 2.3926961421966553, acc: 0.28999999165534973)
[2025-02-16 11:00:00,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:00,452][root][INFO] - Training Epoch: 1/2, step 351/23838 completed (loss: 2.5021393299102783, acc: 0.28125)
[2025-02-16 11:00:00,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:00,905][root][INFO] - Training Epoch: 1/2, step 352/23838 completed (loss: 2.2790613174438477, acc: 0.5)
[2025-02-16 11:00:01,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:01,375][root][INFO] - Training Epoch: 1/2, step 353/23838 completed (loss: 2.3803577423095703, acc: 0.35185185074806213)
[2025-02-16 11:00:01,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:01,868][root][INFO] - Training Epoch: 1/2, step 354/23838 completed (loss: 2.5075173377990723, acc: 0.29411765933036804)
[2025-02-16 11:00:02,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:02,286][root][INFO] - Training Epoch: 1/2, step 355/23838 completed (loss: 2.4431920051574707, acc: 0.33000001311302185)
[2025-02-16 11:00:02,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:02,811][root][INFO] - Training Epoch: 1/2, step 356/23838 completed (loss: 2.5296335220336914, acc: 0.3086419701576233)
[2025-02-16 11:00:03,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:03,291][root][INFO] - Training Epoch: 1/2, step 357/23838 completed (loss: 2.411649227142334, acc: 0.3333333432674408)
[2025-02-16 11:00:03,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:03,760][root][INFO] - Training Epoch: 1/2, step 358/23838 completed (loss: 2.4228076934814453, acc: 0.24074074625968933)
[2025-02-16 11:00:03,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:04,218][root][INFO] - Training Epoch: 1/2, step 359/23838 completed (loss: 2.1341965198516846, acc: 0.3835616409778595)
[2025-02-16 11:00:04,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:04,678][root][INFO] - Training Epoch: 1/2, step 360/23838 completed (loss: 2.1814916133880615, acc: 0.3877550959587097)
[2025-02-16 11:00:04,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:05,144][root][INFO] - Training Epoch: 1/2, step 361/23838 completed (loss: 2.0256569385528564, acc: 0.42105263471603394)
[2025-02-16 11:00:05,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:05,563][root][INFO] - Training Epoch: 1/2, step 362/23838 completed (loss: 2.0237982273101807, acc: 0.4166666567325592)
[2025-02-16 11:00:05,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:06,052][root][INFO] - Training Epoch: 1/2, step 363/23838 completed (loss: 2.6458144187927246, acc: 0.24637681245803833)
[2025-02-16 11:00:06,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:06,498][root][INFO] - Training Epoch: 1/2, step 364/23838 completed (loss: 2.594850778579712, acc: 0.4117647111415863)
[2025-02-16 11:00:06,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:06,898][root][INFO] - Training Epoch: 1/2, step 365/23838 completed (loss: 2.5320839881896973, acc: 0.37837839126586914)
[2025-02-16 11:00:07,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:07,277][root][INFO] - Training Epoch: 1/2, step 366/23838 completed (loss: 2.615337371826172, acc: 0.2711864411830902)
[2025-02-16 11:00:07,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:07,665][root][INFO] - Training Epoch: 1/2, step 367/23838 completed (loss: 2.4695093631744385, acc: 0.3199999928474426)
[2025-02-16 11:00:07,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:08,034][root][INFO] - Training Epoch: 1/2, step 368/23838 completed (loss: 2.6521832942962646, acc: 0.25)
[2025-02-16 11:00:08,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:08,443][root][INFO] - Training Epoch: 1/2, step 369/23838 completed (loss: 2.8591902256011963, acc: 0.25609755516052246)
[2025-02-16 11:00:08,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:08,823][root][INFO] - Training Epoch: 1/2, step 370/23838 completed (loss: 2.7259204387664795, acc: 0.2567567527294159)
[2025-02-16 11:00:08,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:09,271][root][INFO] - Training Epoch: 1/2, step 371/23838 completed (loss: 2.5873262882232666, acc: 0.291262149810791)
[2025-02-16 11:00:09,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:09,761][root][INFO] - Training Epoch: 1/2, step 372/23838 completed (loss: 2.3798024654388428, acc: 0.3913043439388275)
[2025-02-16 11:00:09,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:10,233][root][INFO] - Training Epoch: 1/2, step 373/23838 completed (loss: 2.42649507522583, acc: 0.315315306186676)
[2025-02-16 11:00:10,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:10,692][root][INFO] - Training Epoch: 1/2, step 374/23838 completed (loss: 2.3775312900543213, acc: 0.3448275923728943)
[2025-02-16 11:00:10,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:11,138][root][INFO] - Training Epoch: 1/2, step 375/23838 completed (loss: 2.4664742946624756, acc: 0.3541666567325592)
[2025-02-16 11:00:11,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:11,576][root][INFO] - Training Epoch: 1/2, step 376/23838 completed (loss: 2.359344005584717, acc: 0.3664122223854065)
[2025-02-16 11:00:11,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:12,066][root][INFO] - Training Epoch: 1/2, step 377/23838 completed (loss: 2.4231584072113037, acc: 0.3359375)
[2025-02-16 11:00:12,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:12,691][root][INFO] - Training Epoch: 1/2, step 378/23838 completed (loss: 2.4267280101776123, acc: 0.32777777314186096)
[2025-02-16 11:00:12,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:13,057][root][INFO] - Training Epoch: 1/2, step 379/23838 completed (loss: 2.2584691047668457, acc: 0.3513513505458832)
[2025-02-16 11:00:13,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:13,557][root][INFO] - Training Epoch: 1/2, step 380/23838 completed (loss: 2.5102362632751465, acc: 0.31333333253860474)
[2025-02-16 11:00:13,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:14,005][root][INFO] - Training Epoch: 1/2, step 381/23838 completed (loss: 2.430710792541504, acc: 0.39024388790130615)
[2025-02-16 11:00:14,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:14,471][root][INFO] - Training Epoch: 1/2, step 382/23838 completed (loss: 2.5123274326324463, acc: 0.27358490228652954)
[2025-02-16 11:00:14,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:15,143][root][INFO] - Training Epoch: 1/2, step 383/23838 completed (loss: 2.633291721343994, acc: 0.23000000417232513)
[2025-02-16 11:00:15,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:15,639][root][INFO] - Training Epoch: 1/2, step 384/23838 completed (loss: 2.2369959354400635, acc: 0.4153846204280853)
[2025-02-16 11:00:15,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:16,131][root][INFO] - Training Epoch: 1/2, step 385/23838 completed (loss: 2.4275102615356445, acc: 0.3827160596847534)
[2025-02-16 11:00:16,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:16,560][root][INFO] - Training Epoch: 1/2, step 386/23838 completed (loss: 2.4522151947021484, acc: 0.32786884903907776)
[2025-02-16 11:00:16,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:17,083][root][INFO] - Training Epoch: 1/2, step 387/23838 completed (loss: 2.3519129753112793, acc: 0.3539822995662689)
[2025-02-16 11:00:17,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:17,539][root][INFO] - Training Epoch: 1/2, step 388/23838 completed (loss: 2.3993427753448486, acc: 0.343137264251709)
[2025-02-16 11:00:17,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:18,004][root][INFO] - Training Epoch: 1/2, step 389/23838 completed (loss: 2.547308921813965, acc: 0.3188405930995941)
[2025-02-16 11:00:18,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:18,462][root][INFO] - Training Epoch: 1/2, step 390/23838 completed (loss: 2.6403746604919434, acc: 0.2800000011920929)
[2025-02-16 11:00:18,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:18,926][root][INFO] - Training Epoch: 1/2, step 391/23838 completed (loss: 1.9408373832702637, acc: 0.4959999918937683)
[2025-02-16 11:00:19,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:19,331][root][INFO] - Training Epoch: 1/2, step 392/23838 completed (loss: 2.4779882431030273, acc: 0.34285715222358704)
[2025-02-16 11:00:19,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:19,718][root][INFO] - Training Epoch: 1/2, step 393/23838 completed (loss: 2.3208351135253906, acc: 0.3731343150138855)
[2025-02-16 11:00:19,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:20,189][root][INFO] - Training Epoch: 1/2, step 394/23838 completed (loss: 2.2793173789978027, acc: 0.35211268067359924)
[2025-02-16 11:00:20,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:20,644][root][INFO] - Training Epoch: 1/2, step 395/23838 completed (loss: 2.200417995452881, acc: 0.3913043439388275)
[2025-02-16 11:00:20,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:21,071][root][INFO] - Training Epoch: 1/2, step 396/23838 completed (loss: 2.293314218521118, acc: 0.4124999940395355)
[2025-02-16 11:00:21,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:21,682][root][INFO] - Training Epoch: 1/2, step 397/23838 completed (loss: 2.3237380981445312, acc: 0.37931033968925476)
[2025-02-16 11:00:21,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:22,098][root][INFO] - Training Epoch: 1/2, step 398/23838 completed (loss: 2.0988245010375977, acc: 0.4095238149166107)
[2025-02-16 11:00:22,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:22,524][root][INFO] - Training Epoch: 1/2, step 399/23838 completed (loss: 2.2802531719207764, acc: 0.3881579041481018)
[2025-02-16 11:00:22,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:23,129][root][INFO] - Training Epoch: 1/2, step 400/23838 completed (loss: 2.277971029281616, acc: 0.4202898442745209)
[2025-02-16 11:00:23,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:23,681][root][INFO] - Training Epoch: 1/2, step 401/23838 completed (loss: 2.3422839641571045, acc: 0.3604061007499695)
[2025-02-16 11:00:23,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:24,136][root][INFO] - Training Epoch: 1/2, step 402/23838 completed (loss: 2.3019049167633057, acc: 0.3804347813129425)
[2025-02-16 11:00:24,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:24,582][root][INFO] - Training Epoch: 1/2, step 403/23838 completed (loss: 2.223229169845581, acc: 0.41304346919059753)
[2025-02-16 11:00:24,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:25,074][root][INFO] - Training Epoch: 1/2, step 404/23838 completed (loss: 2.3496756553649902, acc: 0.4000000059604645)
[2025-02-16 11:00:25,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:25,515][root][INFO] - Training Epoch: 1/2, step 405/23838 completed (loss: 1.799849033355713, acc: 0.5352112650871277)
[2025-02-16 11:00:25,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:25,963][root][INFO] - Training Epoch: 1/2, step 406/23838 completed (loss: 2.686206102371216, acc: 0.31578946113586426)
[2025-02-16 11:00:26,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:26,459][root][INFO] - Training Epoch: 1/2, step 407/23838 completed (loss: 2.431199073791504, acc: 0.42222222685813904)
[2025-02-16 11:00:26,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:26,922][root][INFO] - Training Epoch: 1/2, step 408/23838 completed (loss: 2.4278934001922607, acc: 0.4313725531101227)
[2025-02-16 11:00:27,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:27,401][root][INFO] - Training Epoch: 1/2, step 409/23838 completed (loss: 1.497748851776123, acc: 0.53125)
[2025-02-16 11:00:27,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:27,845][root][INFO] - Training Epoch: 1/2, step 410/23838 completed (loss: 1.9117616415023804, acc: 0.4444444477558136)
[2025-02-16 11:00:28,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:28,200][root][INFO] - Training Epoch: 1/2, step 411/23838 completed (loss: 2.319441080093384, acc: 0.36734694242477417)
[2025-02-16 11:00:28,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:28,630][root][INFO] - Training Epoch: 1/2, step 412/23838 completed (loss: 2.53904128074646, acc: 0.31111112236976624)
[2025-02-16 11:00:28,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:29,070][root][INFO] - Training Epoch: 1/2, step 413/23838 completed (loss: 2.2192249298095703, acc: 0.4047619104385376)
[2025-02-16 11:00:29,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:29,541][root][INFO] - Training Epoch: 1/2, step 414/23838 completed (loss: 2.2987558841705322, acc: 0.3611111044883728)
[2025-02-16 11:00:29,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:29,973][root][INFO] - Training Epoch: 1/2, step 415/23838 completed (loss: 2.9327259063720703, acc: 0.2857142984867096)
[2025-02-16 11:00:30,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:30,378][root][INFO] - Training Epoch: 1/2, step 416/23838 completed (loss: 2.2376201152801514, acc: 0.4363636374473572)
[2025-02-16 11:00:30,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:30,755][root][INFO] - Training Epoch: 1/2, step 417/23838 completed (loss: 2.076676368713379, acc: 0.44999998807907104)
[2025-02-16 11:00:30,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:31,135][root][INFO] - Training Epoch: 1/2, step 418/23838 completed (loss: 2.0077855587005615, acc: 0.4444444477558136)
[2025-02-16 11:00:31,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:31,535][root][INFO] - Training Epoch: 1/2, step 419/23838 completed (loss: 2.3665966987609863, acc: 0.4000000059604645)
[2025-02-16 11:00:31,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:31,926][root][INFO] - Training Epoch: 1/2, step 420/23838 completed (loss: 2.861697196960449, acc: 0.31707316637039185)
[2025-02-16 11:00:32,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:32,414][root][INFO] - Training Epoch: 1/2, step 421/23838 completed (loss: 1.7621170282363892, acc: 0.5256410241127014)
[2025-02-16 11:00:32,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:32,822][root][INFO] - Training Epoch: 1/2, step 422/23838 completed (loss: 2.3048298358917236, acc: 0.375)
[2025-02-16 11:00:33,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:33,272][root][INFO] - Training Epoch: 1/2, step 423/23838 completed (loss: 2.5924155712127686, acc: 0.3132530152797699)
[2025-02-16 11:00:33,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:33,662][root][INFO] - Training Epoch: 1/2, step 424/23838 completed (loss: 2.3770864009857178, acc: 0.33870968222618103)
[2025-02-16 11:00:33,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:34,135][root][INFO] - Training Epoch: 1/2, step 425/23838 completed (loss: 2.226097345352173, acc: 0.46666666865348816)
[2025-02-16 11:00:34,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:34,582][root][INFO] - Training Epoch: 1/2, step 426/23838 completed (loss: 3.3463058471679688, acc: 0.0)
[2025-02-16 11:00:34,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:35,022][root][INFO] - Training Epoch: 1/2, step 427/23838 completed (loss: 2.4422852993011475, acc: 0.3333333432674408)
[2025-02-16 11:00:35,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:35,396][root][INFO] - Training Epoch: 1/2, step 428/23838 completed (loss: 2.7176263332366943, acc: 0.4166666567325592)
[2025-02-16 11:00:35,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:35,824][root][INFO] - Training Epoch: 1/2, step 429/23838 completed (loss: 0.9532833099365234, acc: 0.7272727489471436)
[2025-02-16 11:00:35,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:36,222][root][INFO] - Training Epoch: 1/2, step 430/23838 completed (loss: 3.380002975463867, acc: 0.12727272510528564)
[2025-02-16 11:00:36,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:36,724][root][INFO] - Training Epoch: 1/2, step 431/23838 completed (loss: 3.2836170196533203, acc: 0.2247191071510315)
[2025-02-16 11:00:36,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:37,230][root][INFO] - Training Epoch: 1/2, step 432/23838 completed (loss: 2.9539999961853027, acc: 0.20909090340137482)
[2025-02-16 11:00:37,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:37,734][root][INFO] - Training Epoch: 1/2, step 433/23838 completed (loss: 2.5946202278137207, acc: 0.31111112236976624)
[2025-02-16 11:00:37,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:38,132][root][INFO] - Training Epoch: 1/2, step 434/23838 completed (loss: 3.174023389816284, acc: 0.24705882370471954)
[2025-02-16 11:00:38,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:38,539][root][INFO] - Training Epoch: 1/2, step 435/23838 completed (loss: 2.78295636177063, acc: 0.3384615480899811)
[2025-02-16 11:00:38,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:38,949][root][INFO] - Training Epoch: 1/2, step 436/23838 completed (loss: 2.9148221015930176, acc: 0.25609755516052246)
[2025-02-16 11:00:39,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:39,327][root][INFO] - Training Epoch: 1/2, step 437/23838 completed (loss: 2.84318208694458, acc: 0.24347825348377228)
[2025-02-16 11:00:39,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:39,708][root][INFO] - Training Epoch: 1/2, step 438/23838 completed (loss: 2.8843557834625244, acc: 0.2844827473163605)
[2025-02-16 11:00:39,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:40,082][root][INFO] - Training Epoch: 1/2, step 439/23838 completed (loss: 2.5839216709136963, acc: 0.33673468232154846)
[2025-02-16 11:00:40,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:40,513][root][INFO] - Training Epoch: 1/2, step 440/23838 completed (loss: 2.499638080596924, acc: 0.3333333432674408)
[2025-02-16 11:00:40,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:40,939][root][INFO] - Training Epoch: 1/2, step 441/23838 completed (loss: 2.9153733253479004, acc: 0.239130437374115)
[2025-02-16 11:00:41,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:41,334][root][INFO] - Training Epoch: 1/2, step 442/23838 completed (loss: 2.581932783126831, acc: 0.34328359365463257)
[2025-02-16 11:00:41,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:41,718][root][INFO] - Training Epoch: 1/2, step 443/23838 completed (loss: 2.3738865852355957, acc: 0.4909090995788574)
[2025-02-16 11:00:41,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:42,107][root][INFO] - Training Epoch: 1/2, step 444/23838 completed (loss: 2.8814988136291504, acc: 0.2467532455921173)
[2025-02-16 11:00:42,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:42,604][root][INFO] - Training Epoch: 1/2, step 445/23838 completed (loss: 2.577761650085449, acc: 0.3181818127632141)
[2025-02-16 11:00:42,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:43,092][root][INFO] - Training Epoch: 1/2, step 446/23838 completed (loss: 2.433060646057129, acc: 0.3333333432674408)
[2025-02-16 11:00:43,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:43,501][root][INFO] - Training Epoch: 1/2, step 447/23838 completed (loss: 2.52553129196167, acc: 0.3181818127632141)
[2025-02-16 11:00:43,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:43,895][root][INFO] - Training Epoch: 1/2, step 448/23838 completed (loss: 2.488107919692993, acc: 0.3636363744735718)
[2025-02-16 11:00:44,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:44,304][root][INFO] - Training Epoch: 1/2, step 449/23838 completed (loss: 2.5564236640930176, acc: 0.23999999463558197)
[2025-02-16 11:00:44,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:44,863][root][INFO] - Training Epoch: 1/2, step 450/23838 completed (loss: 2.490349054336548, acc: 0.30000001192092896)
[2025-02-16 11:00:45,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:45,388][root][INFO] - Training Epoch: 1/2, step 451/23838 completed (loss: 2.303328514099121, acc: 0.43617022037506104)
[2025-02-16 11:00:45,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:45,751][root][INFO] - Training Epoch: 1/2, step 452/23838 completed (loss: 2.7256596088409424, acc: 0.29629629850387573)
[2025-02-16 11:00:45,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:46,184][root][INFO] - Training Epoch: 1/2, step 453/23838 completed (loss: 2.4575772285461426, acc: 0.3203125)
[2025-02-16 11:00:46,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:46,659][root][INFO] - Training Epoch: 1/2, step 454/23838 completed (loss: 2.4637932777404785, acc: 0.3191489279270172)
[2025-02-16 11:00:46,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:47,064][root][INFO] - Training Epoch: 1/2, step 455/23838 completed (loss: 2.035794734954834, acc: 0.46341463923454285)
[2025-02-16 11:00:47,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:47,544][root][INFO] - Training Epoch: 1/2, step 456/23838 completed (loss: 2.5027523040771484, acc: 0.30434781312942505)
[2025-02-16 11:00:47,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:48,169][root][INFO] - Training Epoch: 1/2, step 457/23838 completed (loss: 2.079519510269165, acc: 0.41803279519081116)
[2025-02-16 11:00:48,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:48,616][root][INFO] - Training Epoch: 1/2, step 458/23838 completed (loss: 2.5916271209716797, acc: 0.21374045312404633)
[2025-02-16 11:00:48,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:49,036][root][INFO] - Training Epoch: 1/2, step 459/23838 completed (loss: 2.4700534343719482, acc: 0.2795698940753937)
[2025-02-16 11:00:49,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:49,548][root][INFO] - Training Epoch: 1/2, step 460/23838 completed (loss: 2.157485008239746, acc: 0.36263737082481384)
[2025-02-16 11:00:49,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:50,031][root][INFO] - Training Epoch: 1/2, step 461/23838 completed (loss: 2.2901663780212402, acc: 0.40566039085388184)
[2025-02-16 11:00:50,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:50,474][root][INFO] - Training Epoch: 1/2, step 462/23838 completed (loss: 2.4067587852478027, acc: 0.3669724762439728)
[2025-02-16 11:00:50,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:50,999][root][INFO] - Training Epoch: 1/2, step 463/23838 completed (loss: 2.219085931777954, acc: 0.3529411852359772)
[2025-02-16 11:00:51,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:51,408][root][INFO] - Training Epoch: 1/2, step 464/23838 completed (loss: 2.216942071914673, acc: 0.3913043439388275)
[2025-02-16 11:00:51,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:51,893][root][INFO] - Training Epoch: 1/2, step 465/23838 completed (loss: 2.3853023052215576, acc: 0.34375)
[2025-02-16 11:00:52,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:52,324][root][INFO] - Training Epoch: 1/2, step 466/23838 completed (loss: 2.3491296768188477, acc: 0.39516130089759827)
[2025-02-16 11:00:52,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:52,806][root][INFO] - Training Epoch: 1/2, step 467/23838 completed (loss: 2.0723917484283447, acc: 0.4453781545162201)
[2025-02-16 11:00:53,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:53,279][root][INFO] - Training Epoch: 1/2, step 468/23838 completed (loss: 2.300393581390381, acc: 0.37662336230278015)
[2025-02-16 11:00:53,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:53,722][root][INFO] - Training Epoch: 1/2, step 469/23838 completed (loss: 1.4870349168777466, acc: 0.6142857074737549)
[2025-02-16 11:00:53,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:54,092][root][INFO] - Training Epoch: 1/2, step 470/23838 completed (loss: 2.6664719581604004, acc: 0.2584269642829895)
[2025-02-16 11:00:54,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:54,504][root][INFO] - Training Epoch: 1/2, step 471/23838 completed (loss: 2.058195114135742, acc: 0.37735849618911743)
[2025-02-16 11:00:54,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:54,987][root][INFO] - Training Epoch: 1/2, step 472/23838 completed (loss: 2.3168413639068604, acc: 0.3396226465702057)
[2025-02-16 11:00:55,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:55,410][root][INFO] - Training Epoch: 1/2, step 473/23838 completed (loss: 1.7517348527908325, acc: 0.5657894611358643)
[2025-02-16 11:00:55,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:55,808][root][INFO] - Training Epoch: 1/2, step 474/23838 completed (loss: 2.147252321243286, acc: 0.380952388048172)
[2025-02-16 11:00:56,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:56,262][root][INFO] - Training Epoch: 1/2, step 475/23838 completed (loss: 2.4708657264709473, acc: 0.30841121077537537)
[2025-02-16 11:00:56,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:56,769][root][INFO] - Training Epoch: 1/2, step 476/23838 completed (loss: 2.3318288326263428, acc: 0.3132530152797699)
[2025-02-16 11:00:57,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:57,595][root][INFO] - Training Epoch: 1/2, step 477/23838 completed (loss: 2.402379274368286, acc: 0.3300970792770386)
[2025-02-16 11:00:57,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:58,106][root][INFO] - Training Epoch: 1/2, step 478/23838 completed (loss: 2.2178494930267334, acc: 0.3937007784843445)
[2025-02-16 11:00:58,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:58,557][root][INFO] - Training Epoch: 1/2, step 479/23838 completed (loss: 2.224780797958374, acc: 0.40816327929496765)
[2025-02-16 11:00:58,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:59,132][root][INFO] - Training Epoch: 1/2, step 480/23838 completed (loss: 2.147160768508911, acc: 0.42635658383369446)
[2025-02-16 11:00:59,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:59,564][root][INFO] - Training Epoch: 1/2, step 481/23838 completed (loss: 2.1271064281463623, acc: 0.4054054021835327)
[2025-02-16 11:00:59,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:00:59,934][root][INFO] - Training Epoch: 1/2, step 482/23838 completed (loss: 2.4757585525512695, acc: 0.3472222089767456)
[2025-02-16 11:01:00,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:00,336][root][INFO] - Training Epoch: 1/2, step 483/23838 completed (loss: 1.9946244955062866, acc: 0.4615384638309479)
[2025-02-16 11:01:00,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:00,773][root][INFO] - Training Epoch: 1/2, step 484/23838 completed (loss: 2.4486002922058105, acc: 0.3448275923728943)
[2025-02-16 11:01:00,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:01,219][root][INFO] - Training Epoch: 1/2, step 485/23838 completed (loss: 2.715834379196167, acc: 0.26229506731033325)
[2025-02-16 11:01:01,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:01,639][root][INFO] - Training Epoch: 1/2, step 486/23838 completed (loss: 2.701213836669922, acc: 0.28358209133148193)
[2025-02-16 11:01:01,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:02,096][root][INFO] - Training Epoch: 1/2, step 487/23838 completed (loss: 2.2931690216064453, acc: 0.30263158679008484)
[2025-02-16 11:01:02,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:02,524][root][INFO] - Training Epoch: 1/2, step 488/23838 completed (loss: 2.3139114379882812, acc: 0.3661971688270569)
[2025-02-16 11:01:02,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:02,882][root][INFO] - Training Epoch: 1/2, step 489/23838 completed (loss: 2.1880722045898438, acc: 0.4025973975658417)
[2025-02-16 11:01:03,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:03,252][root][INFO] - Training Epoch: 1/2, step 490/23838 completed (loss: 2.421535015106201, acc: 0.31168830394744873)
[2025-02-16 11:01:03,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:03,638][root][INFO] - Training Epoch: 1/2, step 491/23838 completed (loss: 2.392610788345337, acc: 0.3466666638851166)
[2025-02-16 11:01:03,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:04,147][root][INFO] - Training Epoch: 1/2, step 492/23838 completed (loss: 2.2632415294647217, acc: 0.328125)
[2025-02-16 11:01:04,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:04,621][root][INFO] - Training Epoch: 1/2, step 493/23838 completed (loss: 2.058492422103882, acc: 0.4769230782985687)
[2025-02-16 11:01:04,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:05,126][root][INFO] - Training Epoch: 1/2, step 494/23838 completed (loss: 2.2399041652679443, acc: 0.41333332657814026)
[2025-02-16 11:01:05,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:05,577][root][INFO] - Training Epoch: 1/2, step 495/23838 completed (loss: 2.3451108932495117, acc: 0.375)
[2025-02-16 11:01:05,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:05,933][root][INFO] - Training Epoch: 1/2, step 496/23838 completed (loss: 2.5848987102508545, acc: 0.3611111044883728)
[2025-02-16 11:01:06,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:06,397][root][INFO] - Training Epoch: 1/2, step 497/23838 completed (loss: 2.1977880001068115, acc: 0.4444444477558136)
[2025-02-16 11:01:06,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:06,786][root][INFO] - Training Epoch: 1/2, step 498/23838 completed (loss: 2.7891416549682617, acc: 0.30000001192092896)
[2025-02-16 11:01:07,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:07,693][root][INFO] - Training Epoch: 1/2, step 499/23838 completed (loss: 1.9238272905349731, acc: 0.49593496322631836)
[2025-02-16 11:01:07,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:08,190][root][INFO] - Training Epoch: 1/2, step 500/23838 completed (loss: 1.9780151844024658, acc: 0.4732142984867096)
[2025-02-16 11:01:08,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:08,604][root][INFO] - Training Epoch: 1/2, step 501/23838 completed (loss: 2.0662436485290527, acc: 0.42500001192092896)
[2025-02-16 11:01:08,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:09,052][root][INFO] - Training Epoch: 1/2, step 502/23838 completed (loss: 2.09202241897583, acc: 0.4020618498325348)
[2025-02-16 11:01:09,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:09,489][root][INFO] - Training Epoch: 1/2, step 503/23838 completed (loss: 2.088230848312378, acc: 0.47457626461982727)
[2025-02-16 11:01:09,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:09,894][root][INFO] - Training Epoch: 1/2, step 504/23838 completed (loss: 2.3034021854400635, acc: 0.3333333432674408)
[2025-02-16 11:01:10,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:10,318][root][INFO] - Training Epoch: 1/2, step 505/23838 completed (loss: 2.1538851261138916, acc: 0.38793104887008667)
[2025-02-16 11:01:10,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:10,773][root][INFO] - Training Epoch: 1/2, step 506/23838 completed (loss: 2.0108819007873535, acc: 0.4406779706478119)
[2025-02-16 11:01:11,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:11,373][root][INFO] - Training Epoch: 1/2, step 507/23838 completed (loss: 1.921256184577942, acc: 0.43023255467414856)
[2025-02-16 11:01:11,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:11,844][root][INFO] - Training Epoch: 1/2, step 508/23838 completed (loss: 2.2014849185943604, acc: 0.3684210479259491)
[2025-02-16 11:01:12,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:12,294][root][INFO] - Training Epoch: 1/2, step 509/23838 completed (loss: 1.887390375137329, acc: 0.4406779706478119)
[2025-02-16 11:01:12,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:12,750][root][INFO] - Training Epoch: 1/2, step 510/23838 completed (loss: 1.719460129737854, acc: 0.5970149040222168)
[2025-02-16 11:01:12,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:13,202][root][INFO] - Training Epoch: 1/2, step 511/23838 completed (loss: 2.1233701705932617, acc: 0.4423076808452606)
[2025-02-16 11:01:13,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:13,626][root][INFO] - Training Epoch: 1/2, step 512/23838 completed (loss: 1.7847371101379395, acc: 0.4444444477558136)
[2025-02-16 11:01:13,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:14,125][root][INFO] - Training Epoch: 1/2, step 513/23838 completed (loss: 2.027333974838257, acc: 0.4444444477558136)
[2025-02-16 11:01:14,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:14,608][root][INFO] - Training Epoch: 1/2, step 514/23838 completed (loss: 1.793807029724121, acc: 0.5277777910232544)
[2025-02-16 11:01:14,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:15,090][root][INFO] - Training Epoch: 1/2, step 515/23838 completed (loss: 2.3409581184387207, acc: 0.3333333432674408)
[2025-02-16 11:01:15,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:15,463][root][INFO] - Training Epoch: 1/2, step 516/23838 completed (loss: 2.3184752464294434, acc: 0.42168673872947693)
[2025-02-16 11:01:15,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:15,887][root][INFO] - Training Epoch: 1/2, step 517/23838 completed (loss: 1.968359112739563, acc: 0.4821428656578064)
[2025-02-16 11:01:16,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:16,340][root][INFO] - Training Epoch: 1/2, step 518/23838 completed (loss: 1.8959810733795166, acc: 0.4645669162273407)
[2025-02-16 11:01:16,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:16,808][root][INFO] - Training Epoch: 1/2, step 519/23838 completed (loss: 1.808846116065979, acc: 0.5263158082962036)
[2025-02-16 11:01:17,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:17,319][root][INFO] - Training Epoch: 1/2, step 520/23838 completed (loss: 1.936900019645691, acc: 0.42399999499320984)
[2025-02-16 11:01:17,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:18,077][root][INFO] - Training Epoch: 1/2, step 521/23838 completed (loss: 2.2674009799957275, acc: 0.3392857015132904)
[2025-02-16 11:01:18,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:18,444][root][INFO] - Training Epoch: 1/2, step 522/23838 completed (loss: 1.8137651681900024, acc: 0.45679011940956116)
[2025-02-16 11:01:18,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:18,936][root][INFO] - Training Epoch: 1/2, step 523/23838 completed (loss: 1.6539474725723267, acc: 0.49673202633857727)
[2025-02-16 11:01:19,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:19,376][root][INFO] - Training Epoch: 1/2, step 524/23838 completed (loss: 1.8789242506027222, acc: 0.4722222089767456)
[2025-02-16 11:01:19,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:19,841][root][INFO] - Training Epoch: 1/2, step 525/23838 completed (loss: 2.0781564712524414, acc: 0.4791666567325592)
[2025-02-16 11:01:20,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:20,295][root][INFO] - Training Epoch: 1/2, step 526/23838 completed (loss: 1.7291702032089233, acc: 0.5514018535614014)
[2025-02-16 11:01:20,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:20,772][root][INFO] - Training Epoch: 1/2, step 527/23838 completed (loss: 1.8661879301071167, acc: 0.46000000834465027)
[2025-02-16 11:01:20,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:21,162][root][INFO] - Training Epoch: 1/2, step 528/23838 completed (loss: 1.419135332107544, acc: 0.6517857313156128)
[2025-02-16 11:01:21,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:21,623][root][INFO] - Training Epoch: 1/2, step 529/23838 completed (loss: 1.9860072135925293, acc: 0.4724409580230713)
[2025-02-16 11:01:21,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:22,045][root][INFO] - Training Epoch: 1/2, step 530/23838 completed (loss: 1.9110583066940308, acc: 0.4675324559211731)
[2025-02-16 11:01:22,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:22,531][root][INFO] - Training Epoch: 1/2, step 531/23838 completed (loss: 1.8735992908477783, acc: 0.46666666865348816)
[2025-02-16 11:01:22,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:22,982][root][INFO] - Training Epoch: 1/2, step 532/23838 completed (loss: 1.8723185062408447, acc: 0.4285714328289032)
[2025-02-16 11:01:23,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:23,468][root][INFO] - Training Epoch: 1/2, step 533/23838 completed (loss: 2.185934543609619, acc: 0.4406779706478119)
[2025-02-16 11:01:23,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:23,839][root][INFO] - Training Epoch: 1/2, step 534/23838 completed (loss: 2.217442274093628, acc: 0.43103447556495667)
[2025-02-16 11:01:24,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:24,284][root][INFO] - Training Epoch: 1/2, step 535/23838 completed (loss: 1.817518949508667, acc: 0.5102040767669678)
[2025-02-16 11:01:24,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:24,846][root][INFO] - Training Epoch: 1/2, step 536/23838 completed (loss: 1.9322776794433594, acc: 0.4141414165496826)
[2025-02-16 11:01:25,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:25,586][root][INFO] - Training Epoch: 1/2, step 537/23838 completed (loss: 2.0566697120666504, acc: 0.5106382966041565)
[2025-02-16 11:01:25,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:25,965][root][INFO] - Training Epoch: 1/2, step 538/23838 completed (loss: 1.9123973846435547, acc: 0.4714285731315613)
[2025-02-16 11:01:26,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:26,471][root][INFO] - Training Epoch: 1/2, step 539/23838 completed (loss: 1.949603796005249, acc: 0.4651162922382355)
[2025-02-16 11:01:26,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:26,881][root][INFO] - Training Epoch: 1/2, step 540/23838 completed (loss: 2.0330426692962646, acc: 0.4545454680919647)
[2025-02-16 11:01:27,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:27,646][root][INFO] - Training Epoch: 1/2, step 541/23838 completed (loss: 1.7776567935943604, acc: 0.5270270109176636)
[2025-02-16 11:01:27,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:28,091][root][INFO] - Training Epoch: 1/2, step 542/23838 completed (loss: 2.063199996948242, acc: 0.46268656849861145)
[2025-02-16 11:01:28,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:28,581][root][INFO] - Training Epoch: 1/2, step 543/23838 completed (loss: 1.694677710533142, acc: 0.5593220591545105)
[2025-02-16 11:01:28,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:29,086][root][INFO] - Training Epoch: 1/2, step 544/23838 completed (loss: 1.7370456457138062, acc: 0.5274725556373596)
[2025-02-16 11:01:29,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:29,621][root][INFO] - Training Epoch: 1/2, step 545/23838 completed (loss: 1.5957679748535156, acc: 0.6048387289047241)
[2025-02-16 11:01:29,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:30,084][root][INFO] - Training Epoch: 1/2, step 546/23838 completed (loss: 1.6118576526641846, acc: 0.49450549483299255)
[2025-02-16 11:01:30,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:30,537][root][INFO] - Training Epoch: 1/2, step 547/23838 completed (loss: 1.801936149597168, acc: 0.5473684072494507)
[2025-02-16 11:01:30,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:30,946][root][INFO] - Training Epoch: 1/2, step 548/23838 completed (loss: 1.3860628604888916, acc: 0.6707317233085632)
[2025-02-16 11:01:31,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:31,361][root][INFO] - Training Epoch: 1/2, step 549/23838 completed (loss: 2.1255030632019043, acc: 0.44871795177459717)
[2025-02-16 11:01:31,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:31,916][root][INFO] - Training Epoch: 1/2, step 550/23838 completed (loss: 1.6257290840148926, acc: 0.5916666388511658)
[2025-02-16 11:01:32,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:32,404][root][INFO] - Training Epoch: 1/2, step 551/23838 completed (loss: 1.8664398193359375, acc: 0.5340909361839294)
[2025-02-16 11:01:32,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:32,849][root][INFO] - Training Epoch: 1/2, step 552/23838 completed (loss: 1.6991252899169922, acc: 0.574999988079071)
[2025-02-16 11:01:33,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:33,284][root][INFO] - Training Epoch: 1/2, step 553/23838 completed (loss: 1.7174148559570312, acc: 0.5407407283782959)
[2025-02-16 11:01:33,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:33,739][root][INFO] - Training Epoch: 1/2, step 554/23838 completed (loss: 1.2138739824295044, acc: 0.6491228342056274)
[2025-02-16 11:01:34,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:34,252][root][INFO] - Training Epoch: 1/2, step 555/23838 completed (loss: 2.075955390930176, acc: 0.43220338225364685)
[2025-02-16 11:01:34,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:34,741][root][INFO] - Training Epoch: 1/2, step 556/23838 completed (loss: 1.6579973697662354, acc: 0.5270270109176636)
[2025-02-16 11:01:34,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:35,234][root][INFO] - Training Epoch: 1/2, step 557/23838 completed (loss: 1.7543261051177979, acc: 0.550632894039154)
[2025-02-16 11:01:35,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:35,771][root][INFO] - Training Epoch: 1/2, step 558/23838 completed (loss: 1.7150338888168335, acc: 0.6145833134651184)
[2025-02-16 11:01:35,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:36,173][root][INFO] - Training Epoch: 1/2, step 559/23838 completed (loss: 1.648012638092041, acc: 0.5789473652839661)
[2025-02-16 11:01:36,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:36,596][root][INFO] - Training Epoch: 1/2, step 560/23838 completed (loss: 1.5166363716125488, acc: 0.578125)
[2025-02-16 11:01:36,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:37,082][root][INFO] - Training Epoch: 1/2, step 561/23838 completed (loss: 1.635138750076294, acc: 0.5833333134651184)
[2025-02-16 11:01:37,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:37,558][root][INFO] - Training Epoch: 1/2, step 562/23838 completed (loss: 1.5661897659301758, acc: 0.6438356041908264)
[2025-02-16 11:01:37,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:37,968][root][INFO] - Training Epoch: 1/2, step 563/23838 completed (loss: 1.6220000982284546, acc: 0.5789473652839661)
[2025-02-16 11:01:38,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:38,452][root][INFO] - Training Epoch: 1/2, step 564/23838 completed (loss: 1.2208259105682373, acc: 0.7063491940498352)
[2025-02-16 11:01:38,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:38,863][root][INFO] - Training Epoch: 1/2, step 565/23838 completed (loss: 1.4379749298095703, acc: 0.5810810923576355)
[2025-02-16 11:01:38,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:39,205][root][INFO] - Training Epoch: 1/2, step 566/23838 completed (loss: 1.868971824645996, acc: 0.5306122303009033)
[2025-02-16 11:01:39,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:39,645][root][INFO] - Training Epoch: 1/2, step 567/23838 completed (loss: 2.191579818725586, acc: 0.46296295523643494)
[2025-02-16 11:01:39,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:40,141][root][INFO] - Training Epoch: 1/2, step 568/23838 completed (loss: 2.2428946495056152, acc: 0.4285714328289032)
[2025-02-16 11:01:40,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:40,550][root][INFO] - Training Epoch: 1/2, step 569/23838 completed (loss: 2.570868968963623, acc: 0.4000000059604645)
[2025-02-16 11:01:40,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:41,009][root][INFO] - Training Epoch: 1/2, step 570/23838 completed (loss: 1.798546552658081, acc: 0.5444444417953491)
[2025-02-16 11:01:41,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:41,446][root][INFO] - Training Epoch: 1/2, step 571/23838 completed (loss: 1.5906145572662354, acc: 0.5362318754196167)
[2025-02-16 11:01:41,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:41,869][root][INFO] - Training Epoch: 1/2, step 572/23838 completed (loss: 2.137202739715576, acc: 0.41489362716674805)
[2025-02-16 11:01:42,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:42,332][root][INFO] - Training Epoch: 1/2, step 573/23838 completed (loss: 2.056981325149536, acc: 0.4716981053352356)
[2025-02-16 11:01:42,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:42,770][root][INFO] - Training Epoch: 1/2, step 574/23838 completed (loss: 2.8298499584198, acc: 0.20000000298023224)
[2025-02-16 11:01:43,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:43,286][root][INFO] - Training Epoch: 1/2, step 575/23838 completed (loss: 2.5851356983184814, acc: 0.30909091234207153)
[2025-02-16 11:01:43,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:43,666][root][INFO] - Training Epoch: 1/2, step 576/23838 completed (loss: 1.787462592124939, acc: 0.5185185074806213)
[2025-02-16 11:01:43,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:44,169][root][INFO] - Training Epoch: 1/2, step 577/23838 completed (loss: 2.09700870513916, acc: 0.4516128897666931)
[2025-02-16 11:01:44,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:44,619][root][INFO] - Training Epoch: 1/2, step 578/23838 completed (loss: 2.3226382732391357, acc: 0.34375)
[2025-02-16 11:01:44,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:45,041][root][INFO] - Training Epoch: 1/2, step 579/23838 completed (loss: 2.4437942504882812, acc: 0.3333333432674408)
[2025-02-16 11:01:45,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:45,479][root][INFO] - Training Epoch: 1/2, step 580/23838 completed (loss: 2.151092290878296, acc: 0.3529411852359772)
[2025-02-16 11:01:45,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:45,854][root][INFO] - Training Epoch: 1/2, step 581/23838 completed (loss: 2.120931625366211, acc: 0.37931033968925476)
[2025-02-16 11:01:46,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:46,281][root][INFO] - Training Epoch: 1/2, step 582/23838 completed (loss: 2.5824875831604004, acc: 0.3888888955116272)
[2025-02-16 11:01:46,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:46,676][root][INFO] - Training Epoch: 1/2, step 583/23838 completed (loss: 2.7135162353515625, acc: 0.25)
[2025-02-16 11:01:46,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:47,107][root][INFO] - Training Epoch: 1/2, step 584/23838 completed (loss: 2.4010157585144043, acc: 0.2926829159259796)
[2025-02-16 11:01:47,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:47,539][root][INFO] - Training Epoch: 1/2, step 585/23838 completed (loss: 2.6421968936920166, acc: 0.25)
[2025-02-16 11:01:47,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:47,964][root][INFO] - Training Epoch: 1/2, step 586/23838 completed (loss: 2.1885275840759277, acc: 0.380952388048172)
[2025-02-16 11:01:48,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:48,346][root][INFO] - Training Epoch: 1/2, step 587/23838 completed (loss: 2.0746407508850098, acc: 0.4363636374473572)
[2025-02-16 11:01:48,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:48,747][root][INFO] - Training Epoch: 1/2, step 588/23838 completed (loss: 2.5744595527648926, acc: 0.28125)
[2025-02-16 11:01:48,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:49,156][root][INFO] - Training Epoch: 1/2, step 589/23838 completed (loss: 2.238830327987671, acc: 0.29032257199287415)
[2025-02-16 11:01:49,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:49,574][root][INFO] - Training Epoch: 1/2, step 590/23838 completed (loss: 2.749424934387207, acc: 0.30000001192092896)
[2025-02-16 11:01:49,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:50,228][root][INFO] - Training Epoch: 1/2, step 591/23838 completed (loss: 2.637071371078491, acc: 0.267123281955719)
[2025-02-16 11:01:50,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:50,716][root][INFO] - Training Epoch: 1/2, step 592/23838 completed (loss: 2.359985828399658, acc: 0.3488371968269348)
[2025-02-16 11:01:51,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:51,276][root][INFO] - Training Epoch: 1/2, step 593/23838 completed (loss: 2.5346410274505615, acc: 0.32236841320991516)
[2025-02-16 11:01:51,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:51,952][root][INFO] - Training Epoch: 1/2, step 594/23838 completed (loss: 2.315316915512085, acc: 0.3504273593425751)
[2025-02-16 11:01:52,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:52,439][root][INFO] - Training Epoch: 1/2, step 595/23838 completed (loss: 2.3790969848632812, acc: 0.375)
[2025-02-16 11:01:52,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:52,859][root][INFO] - Training Epoch: 1/2, step 596/23838 completed (loss: 2.36677885055542, acc: 0.4137931168079376)
[2025-02-16 11:01:53,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:53,256][root][INFO] - Training Epoch: 1/2, step 597/23838 completed (loss: 2.5026886463165283, acc: 0.3333333432674408)
[2025-02-16 11:01:53,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:53,699][root][INFO] - Training Epoch: 1/2, step 598/23838 completed (loss: 2.1090340614318848, acc: 0.5)
[2025-02-16 11:01:53,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:54,137][root][INFO] - Training Epoch: 1/2, step 599/23838 completed (loss: 2.6185011863708496, acc: 0.2777777910232544)
[2025-02-16 11:01:54,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:54,598][root][INFO] - Training Epoch: 1/2, step 600/23838 completed (loss: 2.3213512897491455, acc: 0.3368421196937561)
[2025-02-16 11:01:54,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:55,018][root][INFO] - Training Epoch: 1/2, step 601/23838 completed (loss: 2.2593319416046143, acc: 0.34375)
[2025-02-16 11:01:55,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:55,578][root][INFO] - Training Epoch: 1/2, step 602/23838 completed (loss: 2.244377374649048, acc: 0.3523809611797333)
[2025-02-16 11:01:55,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:55,997][root][INFO] - Training Epoch: 1/2, step 603/23838 completed (loss: 2.213731527328491, acc: 0.3488371968269348)
[2025-02-16 11:01:56,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:56,412][root][INFO] - Training Epoch: 1/2, step 604/23838 completed (loss: 2.304935932159424, acc: 0.3291139304637909)
[2025-02-16 11:01:56,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:57,150][root][INFO] - Training Epoch: 1/2, step 605/23838 completed (loss: 2.174165725708008, acc: 0.40909090638160706)
[2025-02-16 11:01:57,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:57,641][root][INFO] - Training Epoch: 1/2, step 606/23838 completed (loss: 2.132154703140259, acc: 0.4270833432674408)
[2025-02-16 11:01:57,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:58,140][root][INFO] - Training Epoch: 1/2, step 607/23838 completed (loss: 2.1458442211151123, acc: 0.4000000059604645)
[2025-02-16 11:01:58,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:58,616][root][INFO] - Training Epoch: 1/2, step 608/23838 completed (loss: 2.3367347717285156, acc: 0.375)
[2025-02-16 11:01:58,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:59,104][root][INFO] - Training Epoch: 1/2, step 609/23838 completed (loss: 2.5371596813201904, acc: 0.28735631704330444)
[2025-02-16 11:01:59,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:59,551][root][INFO] - Training Epoch: 1/2, step 610/23838 completed (loss: 2.1949145793914795, acc: 0.3488371968269348)
[2025-02-16 11:01:59,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:01:59,970][root][INFO] - Training Epoch: 1/2, step 611/23838 completed (loss: 2.254715919494629, acc: 0.4285714328289032)
[2025-02-16 11:02:00,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:00,340][root][INFO] - Training Epoch: 1/2, step 612/23838 completed (loss: 1.9863338470458984, acc: 0.4150943458080292)
[2025-02-16 11:02:00,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:00,781][root][INFO] - Training Epoch: 1/2, step 613/23838 completed (loss: 2.382862091064453, acc: 0.30588236451148987)
[2025-02-16 11:02:00,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:01,185][root][INFO] - Training Epoch: 1/2, step 614/23838 completed (loss: 2.5019898414611816, acc: 0.3176470696926117)
[2025-02-16 11:02:01,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:01,639][root][INFO] - Training Epoch: 1/2, step 615/23838 completed (loss: 2.3737053871154785, acc: 0.3162393271923065)
[2025-02-16 11:02:01,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:02,110][root][INFO] - Training Epoch: 1/2, step 616/23838 completed (loss: 2.3420207500457764, acc: 0.3333333432674408)
[2025-02-16 11:02:02,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:02,591][root][INFO] - Training Epoch: 1/2, step 617/23838 completed (loss: 2.0341053009033203, acc: 0.43421053886413574)
[2025-02-16 11:02:02,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:03,000][root][INFO] - Training Epoch: 1/2, step 618/23838 completed (loss: 2.6683578491210938, acc: 0.29230770468711853)
[2025-02-16 11:02:03,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:03,369][root][INFO] - Training Epoch: 1/2, step 619/23838 completed (loss: 2.3358991146087646, acc: 0.35333332419395447)
[2025-02-16 11:02:03,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:03,799][root][INFO] - Training Epoch: 1/2, step 620/23838 completed (loss: 2.195084810256958, acc: 0.375)
[2025-02-16 11:02:03,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:04,211][root][INFO] - Training Epoch: 1/2, step 621/23838 completed (loss: 2.398751974105835, acc: 0.38297873735427856)
[2025-02-16 11:02:04,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:04,647][root][INFO] - Training Epoch: 1/2, step 622/23838 completed (loss: 2.4873321056365967, acc: 0.34545454382896423)
[2025-02-16 11:02:04,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:05,063][root][INFO] - Training Epoch: 1/2, step 623/23838 completed (loss: 2.1874752044677734, acc: 0.3918918967247009)
[2025-02-16 11:02:05,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:05,513][root][INFO] - Training Epoch: 1/2, step 624/23838 completed (loss: 2.1555240154266357, acc: 0.36734694242477417)
[2025-02-16 11:02:05,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:05,933][root][INFO] - Training Epoch: 1/2, step 625/23838 completed (loss: 2.340049982070923, acc: 0.2830188572406769)
[2025-02-16 11:02:06,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:06,347][root][INFO] - Training Epoch: 1/2, step 626/23838 completed (loss: 1.930137276649475, acc: 0.45348837971687317)
[2025-02-16 11:02:06,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:06,776][root][INFO] - Training Epoch: 1/2, step 627/23838 completed (loss: 2.259502410888672, acc: 0.3478260934352875)
[2025-02-16 11:02:06,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:07,177][root][INFO] - Training Epoch: 1/2, step 628/23838 completed (loss: 2.2225773334503174, acc: 0.3617021143436432)
[2025-02-16 11:02:07,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:07,601][root][INFO] - Training Epoch: 1/2, step 629/23838 completed (loss: 2.20086932182312, acc: 0.31506848335266113)
[2025-02-16 11:02:07,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:08,064][root][INFO] - Training Epoch: 1/2, step 630/23838 completed (loss: 2.576636791229248, acc: 0.28070175647735596)
[2025-02-16 11:02:08,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:08,495][root][INFO] - Training Epoch: 1/2, step 631/23838 completed (loss: 1.7143172025680542, acc: 0.529411792755127)
[2025-02-16 11:02:08,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:08,988][root][INFO] - Training Epoch: 1/2, step 632/23838 completed (loss: 2.7038254737854004, acc: 0.2199999988079071)
[2025-02-16 11:02:09,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:09,402][root][INFO] - Training Epoch: 1/2, step 633/23838 completed (loss: 2.2812576293945312, acc: 0.3786407709121704)
[2025-02-16 11:02:09,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:09,900][root][INFO] - Training Epoch: 1/2, step 634/23838 completed (loss: 2.0888378620147705, acc: 0.4649122953414917)
[2025-02-16 11:02:10,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:10,338][root][INFO] - Training Epoch: 1/2, step 635/23838 completed (loss: 2.445516347885132, acc: 0.3181818127632141)
[2025-02-16 11:02:10,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:10,738][root][INFO] - Training Epoch: 1/2, step 636/23838 completed (loss: 1.9097044467926025, acc: 0.4761904776096344)
[2025-02-16 11:02:10,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:11,172][root][INFO] - Training Epoch: 1/2, step 637/23838 completed (loss: 2.0118753910064697, acc: 0.5303030014038086)
[2025-02-16 11:02:11,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:11,596][root][INFO] - Training Epoch: 1/2, step 638/23838 completed (loss: 1.9313219785690308, acc: 0.41304346919059753)
[2025-02-16 11:02:11,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:11,996][root][INFO] - Training Epoch: 1/2, step 639/23838 completed (loss: 2.251924991607666, acc: 0.42168673872947693)
[2025-02-16 11:02:12,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:12,376][root][INFO] - Training Epoch: 1/2, step 640/23838 completed (loss: 2.0822343826293945, acc: 0.3835616409778595)
[2025-02-16 11:02:12,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:12,771][root][INFO] - Training Epoch: 1/2, step 641/23838 completed (loss: 1.9554375410079956, acc: 0.4571428596973419)
[2025-02-16 11:02:12,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:13,180][root][INFO] - Training Epoch: 1/2, step 642/23838 completed (loss: 2.5254595279693604, acc: 0.3380281627178192)
[2025-02-16 11:02:13,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:13,665][root][INFO] - Training Epoch: 1/2, step 643/23838 completed (loss: 2.4137964248657227, acc: 0.32786884903907776)
[2025-02-16 11:02:13,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:14,151][root][INFO] - Training Epoch: 1/2, step 644/23838 completed (loss: 2.494196891784668, acc: 0.3589743673801422)
[2025-02-16 11:02:14,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:14,597][root][INFO] - Training Epoch: 1/2, step 645/23838 completed (loss: 2.2392754554748535, acc: 0.4137931168079376)
[2025-02-16 11:02:14,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:14,999][root][INFO] - Training Epoch: 1/2, step 646/23838 completed (loss: 1.9354979991912842, acc: 0.4591836631298065)
[2025-02-16 11:02:15,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:15,413][root][INFO] - Training Epoch: 1/2, step 647/23838 completed (loss: 2.2111220359802246, acc: 0.41044774651527405)
[2025-02-16 11:02:15,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:15,790][root][INFO] - Training Epoch: 1/2, step 648/23838 completed (loss: 2.0612635612487793, acc: 0.4933333396911621)
[2025-02-16 11:02:15,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:16,222][root][INFO] - Training Epoch: 1/2, step 649/23838 completed (loss: 2.2183172702789307, acc: 0.37931033968925476)
[2025-02-16 11:02:16,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:16,657][root][INFO] - Training Epoch: 1/2, step 650/23838 completed (loss: 2.4272303581237793, acc: 0.34415584802627563)
[2025-02-16 11:02:16,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:17,064][root][INFO] - Training Epoch: 1/2, step 651/23838 completed (loss: 2.143216848373413, acc: 0.4017857015132904)
[2025-02-16 11:02:17,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:17,515][root][INFO] - Training Epoch: 1/2, step 652/23838 completed (loss: 2.46079158782959, acc: 0.35338345170021057)
[2025-02-16 11:02:17,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:17,938][root][INFO] - Training Epoch: 1/2, step 653/23838 completed (loss: 2.243622064590454, acc: 0.3828125)
[2025-02-16 11:02:18,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:18,393][root][INFO] - Training Epoch: 1/2, step 654/23838 completed (loss: 2.0728983879089355, acc: 0.5116279125213623)
[2025-02-16 11:02:18,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:18,877][root][INFO] - Training Epoch: 1/2, step 655/23838 completed (loss: 1.7719069719314575, acc: 0.5206611752510071)
[2025-02-16 11:02:19,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:19,291][root][INFO] - Training Epoch: 1/2, step 656/23838 completed (loss: 1.6352238655090332, acc: 0.5087719559669495)
[2025-02-16 11:02:19,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:19,754][root][INFO] - Training Epoch: 1/2, step 657/23838 completed (loss: 2.061849355697632, acc: 0.4204545319080353)
[2025-02-16 11:02:19,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:20,131][root][INFO] - Training Epoch: 1/2, step 658/23838 completed (loss: 1.8756834268569946, acc: 0.4716981053352356)
[2025-02-16 11:02:20,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:20,522][root][INFO] - Training Epoch: 1/2, step 659/23838 completed (loss: 2.1372063159942627, acc: 0.41025641560554504)
[2025-02-16 11:02:20,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:20,931][root][INFO] - Training Epoch: 1/2, step 660/23838 completed (loss: 2.08166766166687, acc: 0.3499999940395355)
[2025-02-16 11:02:21,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:21,346][root][INFO] - Training Epoch: 1/2, step 661/23838 completed (loss: 2.20352840423584, acc: 0.37735849618911743)
[2025-02-16 11:02:21,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:21,791][root][INFO] - Training Epoch: 1/2, step 662/23838 completed (loss: 2.4372024536132812, acc: 0.32967033982276917)
[2025-02-16 11:02:22,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:22,254][root][INFO] - Training Epoch: 1/2, step 663/23838 completed (loss: 2.1511244773864746, acc: 0.39080458879470825)
[2025-02-16 11:02:22,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:22,679][root][INFO] - Training Epoch: 1/2, step 664/23838 completed (loss: 2.293370008468628, acc: 0.4652777910232544)
[2025-02-16 11:02:22,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:23,119][root][INFO] - Training Epoch: 1/2, step 665/23838 completed (loss: 2.153299331665039, acc: 0.4110429584980011)
[2025-02-16 11:02:23,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:23,525][root][INFO] - Training Epoch: 1/2, step 666/23838 completed (loss: 1.7489676475524902, acc: 0.45045045018196106)
[2025-02-16 11:02:23,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:23,978][root][INFO] - Training Epoch: 1/2, step 667/23838 completed (loss: 2.1093931198120117, acc: 0.4363636374473572)
[2025-02-16 11:02:24,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:24,361][root][INFO] - Training Epoch: 1/2, step 668/23838 completed (loss: 1.7983992099761963, acc: 0.4852941036224365)
[2025-02-16 11:02:24,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:24,819][root][INFO] - Training Epoch: 1/2, step 669/23838 completed (loss: 2.1940808296203613, acc: 0.3933333456516266)
[2025-02-16 11:02:25,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:25,291][root][INFO] - Training Epoch: 1/2, step 670/23838 completed (loss: 1.5498887300491333, acc: 0.5882353186607361)
[2025-02-16 11:02:25,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:25,706][root][INFO] - Training Epoch: 1/2, step 671/23838 completed (loss: 1.9604629278182983, acc: 0.46000000834465027)
[2025-02-16 11:02:25,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:26,152][root][INFO] - Training Epoch: 1/2, step 672/23838 completed (loss: 2.120218276977539, acc: 0.3888888955116272)
[2025-02-16 11:02:26,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:26,589][root][INFO] - Training Epoch: 1/2, step 673/23838 completed (loss: 2.1014554500579834, acc: 0.40776699781417847)
[2025-02-16 11:02:26,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:26,981][root][INFO] - Training Epoch: 1/2, step 674/23838 completed (loss: 2.116598129272461, acc: 0.41304346919059753)
[2025-02-16 11:02:27,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:27,438][root][INFO] - Training Epoch: 1/2, step 675/23838 completed (loss: 2.0189366340637207, acc: 0.36283186078071594)
[2025-02-16 11:02:27,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:27,893][root][INFO] - Training Epoch: 1/2, step 676/23838 completed (loss: 1.7832139730453491, acc: 0.5)
[2025-02-16 11:02:28,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:28,361][root][INFO] - Training Epoch: 1/2, step 677/23838 completed (loss: 2.168449640274048, acc: 0.4356435537338257)
[2025-02-16 11:02:28,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:28,776][root][INFO] - Training Epoch: 1/2, step 678/23838 completed (loss: 1.9537816047668457, acc: 0.5)
[2025-02-16 11:02:28,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:29,146][root][INFO] - Training Epoch: 1/2, step 679/23838 completed (loss: 1.9303455352783203, acc: 0.4769230782985687)
[2025-02-16 11:02:29,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:29,597][root][INFO] - Training Epoch: 1/2, step 680/23838 completed (loss: 1.7894384860992432, acc: 0.5600000023841858)
[2025-02-16 11:02:29,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:30,015][root][INFO] - Training Epoch: 1/2, step 681/23838 completed (loss: 2.090519666671753, acc: 0.4128440320491791)
[2025-02-16 11:02:30,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:30,463][root][INFO] - Training Epoch: 1/2, step 682/23838 completed (loss: 2.6894049644470215, acc: 0.25)
[2025-02-16 11:02:30,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:30,836][root][INFO] - Training Epoch: 1/2, step 683/23838 completed (loss: 2.4752895832061768, acc: 0.28057554364204407)
[2025-02-16 11:02:31,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:31,267][root][INFO] - Training Epoch: 1/2, step 684/23838 completed (loss: 2.2191476821899414, acc: 0.36571428179740906)
[2025-02-16 11:02:31,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:31,811][root][INFO] - Training Epoch: 1/2, step 685/23838 completed (loss: 2.0533735752105713, acc: 0.42134830355644226)
[2025-02-16 11:02:32,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:32,323][root][INFO] - Training Epoch: 1/2, step 686/23838 completed (loss: 2.0496604442596436, acc: 0.446601927280426)
[2025-02-16 11:02:32,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:32,793][root][INFO] - Training Epoch: 1/2, step 687/23838 completed (loss: 2.052816867828369, acc: 0.4330708682537079)
[2025-02-16 11:02:33,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:33,228][root][INFO] - Training Epoch: 1/2, step 688/23838 completed (loss: 2.0543336868286133, acc: 0.4482758641242981)
[2025-02-16 11:02:33,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:33,668][root][INFO] - Training Epoch: 1/2, step 689/23838 completed (loss: 1.7913134098052979, acc: 0.5)
[2025-02-16 11:02:33,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:34,176][root][INFO] - Training Epoch: 1/2, step 690/23838 completed (loss: 2.1954331398010254, acc: 0.3909091055393219)
[2025-02-16 11:02:34,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:34,594][root][INFO] - Training Epoch: 1/2, step 691/23838 completed (loss: 1.4738761186599731, acc: 0.5757575631141663)
[2025-02-16 11:02:34,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:34,991][root][INFO] - Training Epoch: 1/2, step 692/23838 completed (loss: 1.9047892093658447, acc: 0.457446813583374)
[2025-02-16 11:02:35,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:35,377][root][INFO] - Training Epoch: 1/2, step 693/23838 completed (loss: 1.518645167350769, acc: 0.6000000238418579)
[2025-02-16 11:02:35,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:35,822][root][INFO] - Training Epoch: 1/2, step 694/23838 completed (loss: 2.016355276107788, acc: 0.4166666567325592)
[2025-02-16 11:02:35,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:36,198][root][INFO] - Training Epoch: 1/2, step 695/23838 completed (loss: 2.396216630935669, acc: 0.3762376308441162)
[2025-02-16 11:02:36,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:36,692][root][INFO] - Training Epoch: 1/2, step 696/23838 completed (loss: 2.1834754943847656, acc: 0.4533333480358124)
[2025-02-16 11:02:36,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:37,161][root][INFO] - Training Epoch: 1/2, step 697/23838 completed (loss: 1.799676775932312, acc: 0.5)
[2025-02-16 11:02:37,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:37,596][root][INFO] - Training Epoch: 1/2, step 698/23838 completed (loss: 1.9994934797286987, acc: 0.5074626803398132)
[2025-02-16 11:02:37,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:38,016][root][INFO] - Training Epoch: 1/2, step 699/23838 completed (loss: 1.85067617893219, acc: 0.47413793206214905)
[2025-02-16 11:02:38,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:38,450][root][INFO] - Training Epoch: 1/2, step 700/23838 completed (loss: 2.2424204349517822, acc: 0.34736841917037964)
[2025-02-16 11:02:38,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:38,842][root][INFO] - Training Epoch: 1/2, step 701/23838 completed (loss: 1.8183790445327759, acc: 0.49494948983192444)
[2025-02-16 11:02:39,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:39,318][root][INFO] - Training Epoch: 1/2, step 702/23838 completed (loss: 1.5452839136123657, acc: 0.5797101259231567)
[2025-02-16 11:02:39,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:39,847][root][INFO] - Training Epoch: 1/2, step 703/23838 completed (loss: 2.0693392753601074, acc: 0.40776699781417847)
[2025-02-16 11:02:40,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:40,292][root][INFO] - Training Epoch: 1/2, step 704/23838 completed (loss: 1.578856110572815, acc: 0.5625)
[2025-02-16 11:02:40,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:40,742][root][INFO] - Training Epoch: 1/2, step 705/23838 completed (loss: 1.830407977104187, acc: 0.4912280738353729)
[2025-02-16 11:02:40,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:41,175][root][INFO] - Training Epoch: 1/2, step 706/23838 completed (loss: 2.0995023250579834, acc: 0.39534884691238403)
[2025-02-16 11:02:41,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:41,604][root][INFO] - Training Epoch: 1/2, step 707/23838 completed (loss: 2.102762460708618, acc: 0.4444444477558136)
[2025-02-16 11:02:41,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:42,028][root][INFO] - Training Epoch: 1/2, step 708/23838 completed (loss: 2.041818618774414, acc: 0.4305555522441864)
[2025-02-16 11:02:42,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:42,493][root][INFO] - Training Epoch: 1/2, step 709/23838 completed (loss: 2.2012112140655518, acc: 0.40707963705062866)
[2025-02-16 11:02:42,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:42,991][root][INFO] - Training Epoch: 1/2, step 710/23838 completed (loss: 2.105395555496216, acc: 0.41304346919059753)
[2025-02-16 11:02:43,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:43,448][root][INFO] - Training Epoch: 1/2, step 711/23838 completed (loss: 2.042659044265747, acc: 0.447761207818985)
[2025-02-16 11:02:43,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:43,907][root][INFO] - Training Epoch: 1/2, step 712/23838 completed (loss: 1.8036435842514038, acc: 0.53125)
[2025-02-16 11:02:44,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:44,286][root][INFO] - Training Epoch: 1/2, step 713/23838 completed (loss: 2.1682164669036865, acc: 0.4444444477558136)
[2025-02-16 11:02:44,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:44,751][root][INFO] - Training Epoch: 1/2, step 714/23838 completed (loss: 1.9123256206512451, acc: 0.5567010045051575)
[2025-02-16 11:02:44,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:45,133][root][INFO] - Training Epoch: 1/2, step 715/23838 completed (loss: 1.6832466125488281, acc: 0.5945945978164673)
[2025-02-16 11:02:45,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:45,654][root][INFO] - Training Epoch: 1/2, step 716/23838 completed (loss: 2.272270441055298, acc: 0.42990654706954956)
[2025-02-16 11:02:45,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:46,350][root][INFO] - Training Epoch: 1/2, step 717/23838 completed (loss: 1.9172303676605225, acc: 0.4699999988079071)
[2025-02-16 11:02:46,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:47,036][root][INFO] - Training Epoch: 1/2, step 718/23838 completed (loss: 1.8871829509735107, acc: 0.5333333611488342)
[2025-02-16 11:02:47,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:47,497][root][INFO] - Training Epoch: 1/2, step 719/23838 completed (loss: 1.0954241752624512, acc: 0.7096773982048035)
[2025-02-16 11:02:47,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:48,000][root][INFO] - Training Epoch: 1/2, step 720/23838 completed (loss: 1.582992672920227, acc: 0.5454545617103577)
[2025-02-16 11:02:48,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:48,708][root][INFO] - Training Epoch: 1/2, step 721/23838 completed (loss: 1.9231345653533936, acc: 0.453125)
[2025-02-16 11:02:48,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:49,082][root][INFO] - Training Epoch: 1/2, step 722/23838 completed (loss: 1.8151077032089233, acc: 0.5113636255264282)
[2025-02-16 11:02:49,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:49,515][root][INFO] - Training Epoch: 1/2, step 723/23838 completed (loss: 2.1770341396331787, acc: 0.37974682450294495)
[2025-02-16 11:02:49,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:49,966][root][INFO] - Training Epoch: 1/2, step 724/23838 completed (loss: 1.9079158306121826, acc: 0.4615384638309479)
[2025-02-16 11:02:50,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:50,887][root][INFO] - Training Epoch: 1/2, step 725/23838 completed (loss: 2.127521514892578, acc: 0.3918918967247009)
[2025-02-16 11:02:51,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:51,349][root][INFO] - Training Epoch: 1/2, step 726/23838 completed (loss: 2.160637855529785, acc: 0.38372093439102173)
[2025-02-16 11:02:51,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:51,848][root][INFO] - Training Epoch: 1/2, step 727/23838 completed (loss: 2.096651315689087, acc: 0.4202127754688263)
[2025-02-16 11:02:52,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:52,404][root][INFO] - Training Epoch: 1/2, step 728/23838 completed (loss: 2.1889142990112305, acc: 0.3708609342575073)
[2025-02-16 11:02:52,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:52,854][root][INFO] - Training Epoch: 1/2, step 729/23838 completed (loss: 2.549673318862915, acc: 0.27173912525177)
[2025-02-16 11:02:53,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:53,340][root][INFO] - Training Epoch: 1/2, step 730/23838 completed (loss: 2.6530323028564453, acc: 0.25925925374031067)
[2025-02-16 11:02:53,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:53,921][root][INFO] - Training Epoch: 1/2, step 731/23838 completed (loss: 2.0145983695983887, acc: 0.4642857015132904)
[2025-02-16 11:02:54,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:54,698][root][INFO] - Training Epoch: 1/2, step 732/23838 completed (loss: 2.408238649368286, acc: 0.38842976093292236)
[2025-02-16 11:02:54,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:55,175][root][INFO] - Training Epoch: 1/2, step 733/23838 completed (loss: 2.352348804473877, acc: 0.37755101919174194)
[2025-02-16 11:02:55,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:55,950][root][INFO] - Training Epoch: 1/2, step 734/23838 completed (loss: 2.1401636600494385, acc: 0.43661972880363464)
[2025-02-16 11:02:56,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:56,461][root][INFO] - Training Epoch: 1/2, step 735/23838 completed (loss: 2.050027847290039, acc: 0.4743589758872986)
[2025-02-16 11:02:56,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:56,887][root][INFO] - Training Epoch: 1/2, step 736/23838 completed (loss: 2.1218128204345703, acc: 0.4693877696990967)
[2025-02-16 11:02:57,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:57,710][root][INFO] - Training Epoch: 1/2, step 737/23838 completed (loss: 2.7901763916015625, acc: 0.27272728085517883)
[2025-02-16 11:02:57,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:58,216][root][INFO] - Training Epoch: 1/2, step 738/23838 completed (loss: 2.205422878265381, acc: 0.4040403962135315)
[2025-02-16 11:02:58,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:58,907][root][INFO] - Training Epoch: 1/2, step 739/23838 completed (loss: 2.578364849090576, acc: 0.2844036817550659)
[2025-02-16 11:02:59,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:59,331][root][INFO] - Training Epoch: 1/2, step 740/23838 completed (loss: 2.4192349910736084, acc: 0.2604166567325592)
[2025-02-16 11:02:59,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:02:59,847][root][INFO] - Training Epoch: 1/2, step 741/23838 completed (loss: 2.0741403102874756, acc: 0.4545454680919647)
[2025-02-16 11:02:59,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:00,203][root][INFO] - Training Epoch: 1/2, step 742/23838 completed (loss: 2.2952680587768555, acc: 0.3529411852359772)
[2025-02-16 11:03:00,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:00,652][root][INFO] - Training Epoch: 1/2, step 743/23838 completed (loss: 2.194882392883301, acc: 0.3888888955116272)
[2025-02-16 11:03:00,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:01,241][root][INFO] - Training Epoch: 1/2, step 744/23838 completed (loss: 2.1375691890716553, acc: 0.4177215099334717)
[2025-02-16 11:03:01,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:01,736][root][INFO] - Training Epoch: 1/2, step 745/23838 completed (loss: 2.330726385116577, acc: 0.4247787594795227)
[2025-02-16 11:03:02,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:02,486][root][INFO] - Training Epoch: 1/2, step 746/23838 completed (loss: 2.427879810333252, acc: 0.3643410801887512)
[2025-02-16 11:03:02,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:03,131][root][INFO] - Training Epoch: 1/2, step 747/23838 completed (loss: 2.1352086067199707, acc: 0.3925233781337738)
[2025-02-16 11:03:03,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:04,014][root][INFO] - Training Epoch: 1/2, step 748/23838 completed (loss: 2.0526740550994873, acc: 0.3695652186870575)
[2025-02-16 11:03:04,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:04,550][root][INFO] - Training Epoch: 1/2, step 749/23838 completed (loss: 2.16628360748291, acc: 0.41860464215278625)
[2025-02-16 11:03:04,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:05,315][root][INFO] - Training Epoch: 1/2, step 750/23838 completed (loss: 2.1499221324920654, acc: 0.42307692766189575)
[2025-02-16 11:03:05,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:05,880][root][INFO] - Training Epoch: 1/2, step 751/23838 completed (loss: 2.076780080795288, acc: 0.40229883790016174)
[2025-02-16 11:03:06,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:07,210][root][INFO] - Training Epoch: 1/2, step 752/23838 completed (loss: 2.235697031021118, acc: 0.39855071902275085)
[2025-02-16 11:03:07,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:08,106][root][INFO] - Training Epoch: 1/2, step 753/23838 completed (loss: 2.2364444732666016, acc: 0.34188035130500793)
[2025-02-16 11:03:08,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:08,560][root][INFO] - Training Epoch: 1/2, step 754/23838 completed (loss: 2.1524569988250732, acc: 0.4354838728904724)
[2025-02-16 11:03:08,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:09,290][root][INFO] - Training Epoch: 1/2, step 755/23838 completed (loss: 2.0731565952301025, acc: 0.4571428596973419)
[2025-02-16 11:03:09,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:09,843][root][INFO] - Training Epoch: 1/2, step 756/23838 completed (loss: 2.147104501724243, acc: 0.3711340129375458)
[2025-02-16 11:03:10,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:10,505][root][INFO] - Training Epoch: 1/2, step 757/23838 completed (loss: 1.8380653858184814, acc: 0.5070422291755676)
[2025-02-16 11:03:10,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:10,961][root][INFO] - Training Epoch: 1/2, step 758/23838 completed (loss: 1.803539514541626, acc: 0.5)
[2025-02-16 11:03:11,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:11,766][root][INFO] - Training Epoch: 1/2, step 759/23838 completed (loss: 2.232231616973877, acc: 0.36082473397254944)
[2025-02-16 11:03:12,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:12,408][root][INFO] - Training Epoch: 1/2, step 760/23838 completed (loss: 2.1456289291381836, acc: 0.4000000059604645)
[2025-02-16 11:03:12,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:12,928][root][INFO] - Training Epoch: 1/2, step 761/23838 completed (loss: 2.2188217639923096, acc: 0.39024388790130615)
[2025-02-16 11:03:13,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:13,419][root][INFO] - Training Epoch: 1/2, step 762/23838 completed (loss: 1.7289748191833496, acc: 0.5324675440788269)
[2025-02-16 11:03:13,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:14,002][root][INFO] - Training Epoch: 1/2, step 763/23838 completed (loss: 1.88881254196167, acc: 0.49450549483299255)
[2025-02-16 11:03:14,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:14,499][root][INFO] - Training Epoch: 1/2, step 764/23838 completed (loss: 1.5270203351974487, acc: 0.5909090638160706)
[2025-02-16 11:03:14,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:14,987][root][INFO] - Training Epoch: 1/2, step 765/23838 completed (loss: 2.18701434135437, acc: 0.4177215099334717)
[2025-02-16 11:03:15,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:15,722][root][INFO] - Training Epoch: 1/2, step 766/23838 completed (loss: 2.2199478149414062, acc: 0.375)
[2025-02-16 11:03:15,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:16,253][root][INFO] - Training Epoch: 1/2, step 767/23838 completed (loss: 1.863929033279419, acc: 0.42424243688583374)
[2025-02-16 11:03:16,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:17,024][root][INFO] - Training Epoch: 1/2, step 768/23838 completed (loss: 2.1416656970977783, acc: 0.3888888955116272)
[2025-02-16 11:03:17,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:17,736][root][INFO] - Training Epoch: 1/2, step 769/23838 completed (loss: 2.417874574661255, acc: 0.3333333432674408)
[2025-02-16 11:03:18,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:18,397][root][INFO] - Training Epoch: 1/2, step 770/23838 completed (loss: 1.527993083000183, acc: 0.5862069129943848)
[2025-02-16 11:03:18,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:18,808][root][INFO] - Training Epoch: 1/2, step 771/23838 completed (loss: 1.652488350868225, acc: 0.5753424763679504)
[2025-02-16 11:03:18,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:19,210][root][INFO] - Training Epoch: 1/2, step 772/23838 completed (loss: 1.6953164339065552, acc: 0.5)
[2025-02-16 11:03:19,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:19,697][root][INFO] - Training Epoch: 1/2, step 773/23838 completed (loss: 2.2504615783691406, acc: 0.4444444477558136)
[2025-02-16 11:03:19,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:20,214][root][INFO] - Training Epoch: 1/2, step 774/23838 completed (loss: 1.7809666395187378, acc: 0.5161290168762207)
[2025-02-16 11:03:20,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:20,729][root][INFO] - Training Epoch: 1/2, step 775/23838 completed (loss: 2.277543783187866, acc: 0.36764705181121826)
[2025-02-16 11:03:20,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:21,098][root][INFO] - Training Epoch: 1/2, step 776/23838 completed (loss: 2.5910146236419678, acc: 0.3392857015132904)
[2025-02-16 11:03:21,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:21,598][root][INFO] - Training Epoch: 1/2, step 777/23838 completed (loss: 2.60923433303833, acc: 0.2857142984867096)
[2025-02-16 11:03:21,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:22,057][root][INFO] - Training Epoch: 1/2, step 778/23838 completed (loss: 1.9306288957595825, acc: 0.4354838728904724)
[2025-02-16 11:03:22,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:22,516][root][INFO] - Training Epoch: 1/2, step 779/23838 completed (loss: 1.9196422100067139, acc: 0.5769230723381042)
[2025-02-16 11:03:22,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:22,964][root][INFO] - Training Epoch: 1/2, step 780/23838 completed (loss: 1.3286164999008179, acc: 0.6666666865348816)
[2025-02-16 11:03:23,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:23,716][root][INFO] - Training Epoch: 1/2, step 781/23838 completed (loss: 1.3466265201568604, acc: 0.5957446694374084)
[2025-02-16 11:03:23,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:24,178][root][INFO] - Training Epoch: 1/2, step 782/23838 completed (loss: 1.5372837781906128, acc: 0.5925925970077515)
[2025-02-16 11:03:24,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:24,607][root][INFO] - Training Epoch: 1/2, step 783/23838 completed (loss: 1.4911996126174927, acc: 0.6296296119689941)
[2025-02-16 11:03:24,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:25,097][root][INFO] - Training Epoch: 1/2, step 784/23838 completed (loss: 1.6546342372894287, acc: 0.529411792755127)
[2025-02-16 11:03:25,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:25,513][root][INFO] - Training Epoch: 1/2, step 785/23838 completed (loss: 1.6946004629135132, acc: 0.6428571343421936)
[2025-02-16 11:03:25,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:25,935][root][INFO] - Training Epoch: 1/2, step 786/23838 completed (loss: 1.6084519624710083, acc: 0.5681818127632141)
[2025-02-16 11:03:26,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:26,350][root][INFO] - Training Epoch: 1/2, step 787/23838 completed (loss: 1.703819751739502, acc: 0.4722222089767456)
[2025-02-16 11:03:26,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:26,779][root][INFO] - Training Epoch: 1/2, step 788/23838 completed (loss: 1.6386899948120117, acc: 0.5272727012634277)
[2025-02-16 11:03:26,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:27,156][root][INFO] - Training Epoch: 1/2, step 789/23838 completed (loss: 1.2386959791183472, acc: 0.6666666865348816)
[2025-02-16 11:03:27,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:27,602][root][INFO] - Training Epoch: 1/2, step 790/23838 completed (loss: 0.6444560885429382, acc: 0.7727272510528564)
[2025-02-16 11:03:27,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:28,094][root][INFO] - Training Epoch: 1/2, step 791/23838 completed (loss: 0.8781270384788513, acc: 0.7916666865348816)
[2025-02-16 11:03:28,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:28,483][root][INFO] - Training Epoch: 1/2, step 792/23838 completed (loss: 0.6547336578369141, acc: 0.8666666746139526)
[2025-02-16 11:03:28,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:28,921][root][INFO] - Training Epoch: 1/2, step 793/23838 completed (loss: 2.040611505508423, acc: 0.5666666626930237)
[2025-02-16 11:03:29,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:29,320][root][INFO] - Training Epoch: 1/2, step 794/23838 completed (loss: 1.5282299518585205, acc: 0.6129032373428345)
[2025-02-16 11:03:29,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:29,766][root][INFO] - Training Epoch: 1/2, step 795/23838 completed (loss: 1.1646928787231445, acc: 0.8421052694320679)
[2025-02-16 11:03:30,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:30,251][root][INFO] - Training Epoch: 1/2, step 796/23838 completed (loss: 1.5333657264709473, acc: 0.5909090638160706)
[2025-02-16 11:03:30,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:30,693][root][INFO] - Training Epoch: 1/2, step 797/23838 completed (loss: 3.3727049827575684, acc: 0.25)
[2025-02-16 11:03:30,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:31,106][root][INFO] - Training Epoch: 1/2, step 798/23838 completed (loss: 1.9931362867355347, acc: 0.4444444477558136)
[2025-02-16 11:03:31,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:31,528][root][INFO] - Training Epoch: 1/2, step 799/23838 completed (loss: 2.6808459758758545, acc: 0.28205129504203796)
[2025-02-16 11:03:31,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:31,929][root][INFO] - Training Epoch: 1/2, step 800/23838 completed (loss: 1.6794517040252686, acc: 0.6315789222717285)
[2025-02-16 11:03:32,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:32,369][root][INFO] - Training Epoch: 1/2, step 801/23838 completed (loss: 2.545836925506592, acc: 0.27586206793785095)
[2025-02-16 11:03:32,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:32,817][root][INFO] - Training Epoch: 1/2, step 802/23838 completed (loss: 2.220221996307373, acc: 0.260869562625885)
[2025-02-16 11:03:32,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:33,209][root][INFO] - Training Epoch: 1/2, step 803/23838 completed (loss: 2.241288661956787, acc: 0.4615384638309479)
[2025-02-16 11:03:33,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:33,578][root][INFO] - Training Epoch: 1/2, step 804/23838 completed (loss: 2.3714427947998047, acc: 0.44117647409439087)
[2025-02-16 11:03:33,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:33,948][root][INFO] - Training Epoch: 1/2, step 805/23838 completed (loss: 1.1974773406982422, acc: 0.625)
[2025-02-16 11:03:34,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:34,344][root][INFO] - Training Epoch: 1/2, step 806/23838 completed (loss: 2.8016343116760254, acc: 0.17391304671764374)
[2025-02-16 11:03:34,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:34,769][root][INFO] - Training Epoch: 1/2, step 807/23838 completed (loss: 1.9341188669204712, acc: 0.5)
[2025-02-16 11:03:34,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:35,231][root][INFO] - Training Epoch: 1/2, step 808/23838 completed (loss: 2.4563798904418945, acc: 0.34545454382896423)
[2025-02-16 11:03:35,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:35,677][root][INFO] - Training Epoch: 1/2, step 809/23838 completed (loss: 2.525125503540039, acc: 0.30985915660858154)
[2025-02-16 11:03:35,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:36,161][root][INFO] - Training Epoch: 1/2, step 810/23838 completed (loss: 2.5970370769500732, acc: 0.33766233921051025)
[2025-02-16 11:03:36,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:36,567][root][INFO] - Training Epoch: 1/2, step 811/23838 completed (loss: 2.5783164501190186, acc: 0.3166666626930237)
[2025-02-16 11:03:36,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:37,016][root][INFO] - Training Epoch: 1/2, step 812/23838 completed (loss: 2.5970067977905273, acc: 0.2666666805744171)
[2025-02-16 11:03:37,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:37,516][root][INFO] - Training Epoch: 1/2, step 813/23838 completed (loss: 2.5798590183258057, acc: 0.31147539615631104)
[2025-02-16 11:03:37,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:37,970][root][INFO] - Training Epoch: 1/2, step 814/23838 completed (loss: 2.687697410583496, acc: 0.3378378450870514)
[2025-02-16 11:03:38,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:38,395][root][INFO] - Training Epoch: 1/2, step 815/23838 completed (loss: 2.687786102294922, acc: 0.24712643027305603)
[2025-02-16 11:03:38,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:38,762][root][INFO] - Training Epoch: 1/2, step 816/23838 completed (loss: 2.595595598220825, acc: 0.2839506268501282)
[2025-02-16 11:03:38,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:39,183][root][INFO] - Training Epoch: 1/2, step 817/23838 completed (loss: 2.4850120544433594, acc: 0.3515625)
[2025-02-16 11:03:39,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:39,612][root][INFO] - Training Epoch: 1/2, step 818/23838 completed (loss: 2.262268543243408, acc: 0.37362638115882874)
[2025-02-16 11:03:39,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:40,007][root][INFO] - Training Epoch: 1/2, step 819/23838 completed (loss: 2.33092999458313, acc: 0.3185185194015503)
[2025-02-16 11:03:40,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:40,466][root][INFO] - Training Epoch: 1/2, step 820/23838 completed (loss: 2.105785846710205, acc: 0.4560000002384186)
[2025-02-16 11:03:40,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:40,945][root][INFO] - Training Epoch: 1/2, step 821/23838 completed (loss: 2.3628265857696533, acc: 0.28037384152412415)
[2025-02-16 11:03:41,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:41,385][root][INFO] - Training Epoch: 1/2, step 822/23838 completed (loss: 2.2379846572875977, acc: 0.341085284948349)
[2025-02-16 11:03:41,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:41,846][root][INFO] - Training Epoch: 1/2, step 823/23838 completed (loss: 2.441343307495117, acc: 0.22772277891635895)
[2025-02-16 11:03:42,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:42,281][root][INFO] - Training Epoch: 1/2, step 824/23838 completed (loss: 2.2923011779785156, acc: 0.3717948794364929)
[2025-02-16 11:03:42,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:42,685][root][INFO] - Training Epoch: 1/2, step 825/23838 completed (loss: 2.313873767852783, acc: 0.32926830649375916)
[2025-02-16 11:03:42,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:43,041][root][INFO] - Training Epoch: 1/2, step 826/23838 completed (loss: 2.3309388160705566, acc: 0.3734939694404602)
[2025-02-16 11:03:43,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:43,444][root][INFO] - Training Epoch: 1/2, step 827/23838 completed (loss: 2.387864589691162, acc: 0.30337077379226685)
[2025-02-16 11:03:43,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:44,231][root][INFO] - Training Epoch: 1/2, step 828/23838 completed (loss: 2.3477706909179688, acc: 0.3361344635486603)
[2025-02-16 11:03:44,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:44,681][root][INFO] - Training Epoch: 1/2, step 829/23838 completed (loss: 2.263967752456665, acc: 0.38805970549583435)
[2025-02-16 11:03:44,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:45,082][root][INFO] - Training Epoch: 1/2, step 830/23838 completed (loss: 2.2574570178985596, acc: 0.4025973975658417)
[2025-02-16 11:03:45,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:45,569][root][INFO] - Training Epoch: 1/2, step 831/23838 completed (loss: 2.0424821376800537, acc: 0.42553192377090454)
[2025-02-16 11:03:45,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:46,093][root][INFO] - Training Epoch: 1/2, step 832/23838 completed (loss: 2.364778995513916, acc: 0.36538460850715637)
[2025-02-16 11:03:46,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:46,543][root][INFO] - Training Epoch: 1/2, step 833/23838 completed (loss: 2.0661323070526123, acc: 0.4318181872367859)
[2025-02-16 11:03:46,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:47,063][root][INFO] - Training Epoch: 1/2, step 834/23838 completed (loss: 2.448275327682495, acc: 0.3670886158943176)
[2025-02-16 11:03:47,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:47,465][root][INFO] - Training Epoch: 1/2, step 835/23838 completed (loss: 1.6730455160140991, acc: 0.5396825671195984)
[2025-02-16 11:03:47,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:47,918][root][INFO] - Training Epoch: 1/2, step 836/23838 completed (loss: 2.16858172416687, acc: 0.4285714328289032)
[2025-02-16 11:03:48,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:48,355][root][INFO] - Training Epoch: 1/2, step 837/23838 completed (loss: 1.9764680862426758, acc: 0.4609375)
[2025-02-16 11:03:48,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:48,767][root][INFO] - Training Epoch: 1/2, step 838/23838 completed (loss: 2.2025022506713867, acc: 0.3958333432674408)
[2025-02-16 11:03:49,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:49,229][root][INFO] - Training Epoch: 1/2, step 839/23838 completed (loss: 2.0584089756011963, acc: 0.4197530746459961)
[2025-02-16 11:03:49,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:49,717][root][INFO] - Training Epoch: 1/2, step 840/23838 completed (loss: 2.0746591091156006, acc: 0.4444444477558136)
[2025-02-16 11:03:49,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:50,123][root][INFO] - Training Epoch: 1/2, step 841/23838 completed (loss: 2.2128472328186035, acc: 0.3781512677669525)
[2025-02-16 11:03:50,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:50,846][root][INFO] - Training Epoch: 1/2, step 842/23838 completed (loss: 2.1391396522521973, acc: 0.43023255467414856)
[2025-02-16 11:03:51,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:51,341][root][INFO] - Training Epoch: 1/2, step 843/23838 completed (loss: 1.885820746421814, acc: 0.4822694957256317)
[2025-02-16 11:03:51,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:51,807][root][INFO] - Training Epoch: 1/2, step 844/23838 completed (loss: 2.1324312686920166, acc: 0.4204545319080353)
[2025-02-16 11:03:52,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:52,268][root][INFO] - Training Epoch: 1/2, step 845/23838 completed (loss: 2.2158966064453125, acc: 0.39603960514068604)
[2025-02-16 11:03:52,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:52,718][root][INFO] - Training Epoch: 1/2, step 846/23838 completed (loss: 2.031419038772583, acc: 0.5102040767669678)
[2025-02-16 11:03:52,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:53,205][root][INFO] - Training Epoch: 1/2, step 847/23838 completed (loss: 2.0507652759552, acc: 0.4054054021835327)
[2025-02-16 11:03:53,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:53,582][root][INFO] - Training Epoch: 1/2, step 848/23838 completed (loss: 2.1245861053466797, acc: 0.40909090638160706)
[2025-02-16 11:03:53,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:53,944][root][INFO] - Training Epoch: 1/2, step 849/23838 completed (loss: 1.531675100326538, acc: 0.5882353186607361)
[2025-02-16 11:03:54,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:54,368][root][INFO] - Training Epoch: 1/2, step 850/23838 completed (loss: 2.1625545024871826, acc: 0.4285714328289032)
[2025-02-16 11:03:54,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:54,787][root][INFO] - Training Epoch: 1/2, step 851/23838 completed (loss: 2.0963337421417236, acc: 0.42424243688583374)
[2025-02-16 11:03:54,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:55,210][root][INFO] - Training Epoch: 1/2, step 852/23838 completed (loss: 1.8570265769958496, acc: 0.4444444477558136)
[2025-02-16 11:03:55,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:55,687][root][INFO] - Training Epoch: 1/2, step 853/23838 completed (loss: 2.0233004093170166, acc: 0.4444444477558136)
[2025-02-16 11:03:55,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:56,120][root][INFO] - Training Epoch: 1/2, step 854/23838 completed (loss: 0.7562183141708374, acc: 0.8571428656578064)
[2025-02-16 11:03:56,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:56,577][root][INFO] - Training Epoch: 1/2, step 855/23838 completed (loss: 1.8903727531433105, acc: 0.4000000059604645)
[2025-02-16 11:03:56,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:57,027][root][INFO] - Training Epoch: 1/2, step 856/23838 completed (loss: 1.1980254650115967, acc: 0.6060606241226196)
[2025-02-16 11:03:57,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:57,478][root][INFO] - Training Epoch: 1/2, step 857/23838 completed (loss: 1.2077357769012451, acc: 0.6086956262588501)
[2025-02-16 11:03:57,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:57,907][root][INFO] - Training Epoch: 1/2, step 858/23838 completed (loss: 2.687999963760376, acc: 0.2631579041481018)
[2025-02-16 11:03:58,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:58,348][root][INFO] - Training Epoch: 1/2, step 859/23838 completed (loss: 1.3215728998184204, acc: 0.5799999833106995)
[2025-02-16 11:03:58,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:58,841][root][INFO] - Training Epoch: 1/2, step 860/23838 completed (loss: 1.8080872297286987, acc: 0.6041666865348816)
[2025-02-16 11:03:59,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:59,301][root][INFO] - Training Epoch: 1/2, step 861/23838 completed (loss: 1.578691840171814, acc: 0.6000000238418579)
[2025-02-16 11:03:59,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:03:59,782][root][INFO] - Training Epoch: 1/2, step 862/23838 completed (loss: 1.3254222869873047, acc: 0.644444465637207)
[2025-02-16 11:04:00,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:00,267][root][INFO] - Training Epoch: 1/2, step 863/23838 completed (loss: 1.248118281364441, acc: 0.6111111044883728)
[2025-02-16 11:04:00,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:00,846][root][INFO] - Training Epoch: 1/2, step 864/23838 completed (loss: 1.1461944580078125, acc: 0.6571428775787354)
[2025-02-16 11:04:01,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:01,256][root][INFO] - Training Epoch: 1/2, step 865/23838 completed (loss: 0.851959228515625, acc: 0.7777777910232544)
[2025-02-16 11:04:01,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:01,743][root][INFO] - Training Epoch: 1/2, step 866/23838 completed (loss: 1.3735504150390625, acc: 0.6086956262588501)
[2025-02-16 11:04:01,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:02,230][root][INFO] - Training Epoch: 1/2, step 867/23838 completed (loss: 1.4486393928527832, acc: 0.6521739363670349)
[2025-02-16 11:04:02,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:02,701][root][INFO] - Training Epoch: 1/2, step 868/23838 completed (loss: 1.7711598873138428, acc: 0.529411792755127)
[2025-02-16 11:04:02,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:03,148][root][INFO] - Training Epoch: 1/2, step 869/23838 completed (loss: 1.53860604763031, acc: 0.5806451439857483)
[2025-02-16 11:04:03,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:03,524][root][INFO] - Training Epoch: 1/2, step 870/23838 completed (loss: 0.943946897983551, acc: 0.75)
[2025-02-16 11:04:03,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:04,003][root][INFO] - Training Epoch: 1/2, step 871/23838 completed (loss: 0.7747995853424072, acc: 0.7777777910232544)
[2025-02-16 11:04:04,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:04,416][root][INFO] - Training Epoch: 1/2, step 872/23838 completed (loss: 0.5393182039260864, acc: 0.761904776096344)
[2025-02-16 11:04:04,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:04,881][root][INFO] - Training Epoch: 1/2, step 873/23838 completed (loss: 1.6855419874191284, acc: 0.4444444477558136)
[2025-02-16 11:04:05,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:05,271][root][INFO] - Training Epoch: 1/2, step 874/23838 completed (loss: 0.7967791557312012, acc: 0.7647058963775635)
[2025-02-16 11:04:05,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:05,710][root][INFO] - Training Epoch: 1/2, step 875/23838 completed (loss: 0.33430808782577515, acc: 1.0)
[2025-02-16 11:04:05,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:06,165][root][INFO] - Training Epoch: 1/2, step 876/23838 completed (loss: 0.5007259845733643, acc: 0.8888888955116272)
[2025-02-16 11:04:06,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:06,617][root][INFO] - Training Epoch: 1/2, step 877/23838 completed (loss: 1.056801199913025, acc: 0.7200000286102295)
[2025-02-16 11:04:06,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:07,027][root][INFO] - Training Epoch: 1/2, step 878/23838 completed (loss: 1.183121919631958, acc: 0.6153846383094788)
[2025-02-16 11:04:07,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:07,440][root][INFO] - Training Epoch: 1/2, step 879/23838 completed (loss: 1.1764850616455078, acc: 0.7200000286102295)
[2025-02-16 11:04:07,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:07,868][root][INFO] - Training Epoch: 1/2, step 880/23838 completed (loss: 0.4921266734600067, acc: 0.800000011920929)
[2025-02-16 11:04:08,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:08,259][root][INFO] - Training Epoch: 1/2, step 881/23838 completed (loss: 0.9048035740852356, acc: 0.6875)
[2025-02-16 11:04:08,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:08,678][root][INFO] - Training Epoch: 1/2, step 882/23838 completed (loss: 0.9149059653282166, acc: 0.7272727489471436)
[2025-02-16 11:04:08,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:09,122][root][INFO] - Training Epoch: 1/2, step 883/23838 completed (loss: 0.4862094223499298, acc: 0.95652174949646)
[2025-02-16 11:04:09,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:09,584][root][INFO] - Training Epoch: 1/2, step 884/23838 completed (loss: 1.1830713748931885, acc: 0.699999988079071)
[2025-02-16 11:04:09,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:10,035][root][INFO] - Training Epoch: 1/2, step 885/23838 completed (loss: 1.3536478281021118, acc: 0.8333333134651184)
[2025-02-16 11:04:10,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:10,487][root][INFO] - Training Epoch: 1/2, step 886/23838 completed (loss: 1.7456374168395996, acc: 0.5614035129547119)
[2025-02-16 11:04:10,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:10,874][root][INFO] - Training Epoch: 1/2, step 887/23838 completed (loss: 1.3103160858154297, acc: 0.6060606241226196)
[2025-02-16 11:04:11,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:11,237][root][INFO] - Training Epoch: 1/2, step 888/23838 completed (loss: 2.0120863914489746, acc: 0.5454545617103577)
[2025-02-16 11:04:11,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:11,664][root][INFO] - Training Epoch: 1/2, step 889/23838 completed (loss: 0.6121624708175659, acc: 0.75)
[2025-02-16 11:04:11,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:12,176][root][INFO] - Training Epoch: 1/2, step 890/23838 completed (loss: 1.162588119506836, acc: 0.6666666865348816)
[2025-02-16 11:04:12,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:12,564][root][INFO] - Training Epoch: 1/2, step 891/23838 completed (loss: 1.361617088317871, acc: 0.5588235259056091)
[2025-02-16 11:04:12,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:13,009][root][INFO] - Training Epoch: 1/2, step 892/23838 completed (loss: 1.8307054042816162, acc: 0.4647887349128723)
[2025-02-16 11:04:13,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:13,471][root][INFO] - Training Epoch: 1/2, step 893/23838 completed (loss: 0.6439273953437805, acc: 0.8333333134651184)
[2025-02-16 11:04:13,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:13,868][root][INFO] - Training Epoch: 1/2, step 894/23838 completed (loss: 1.1236048936843872, acc: 0.625)
[2025-02-16 11:04:14,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:14,297][root][INFO] - Training Epoch: 1/2, step 895/23838 completed (loss: 0.8590870499610901, acc: 0.7599999904632568)
[2025-02-16 11:04:14,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:14,675][root][INFO] - Training Epoch: 1/2, step 896/23838 completed (loss: 1.1459732055664062, acc: 0.699999988079071)
[2025-02-16 11:04:14,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:15,078][root][INFO] - Training Epoch: 1/2, step 897/23838 completed (loss: 0.2892594337463379, acc: 0.9259259104728699)
[2025-02-16 11:04:15,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:15,476][root][INFO] - Training Epoch: 1/2, step 898/23838 completed (loss: 0.44206809997558594, acc: 0.8260869383811951)
[2025-02-16 11:04:15,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:15,843][root][INFO] - Training Epoch: 1/2, step 899/23838 completed (loss: 1.7393015623092651, acc: 0.6176470518112183)
[2025-02-16 11:04:16,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:16,263][root][INFO] - Training Epoch: 1/2, step 900/23838 completed (loss: 1.0729765892028809, acc: 0.6896551847457886)
[2025-02-16 11:04:16,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:16,674][root][INFO] - Training Epoch: 1/2, step 901/23838 completed (loss: 0.7901127338409424, acc: 0.7241379022598267)
[2025-02-16 11:04:16,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:17,072][root][INFO] - Training Epoch: 1/2, step 902/23838 completed (loss: 1.2873667478561401, acc: 0.6363636255264282)
[2025-02-16 11:04:17,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:17,483][root][INFO] - Training Epoch: 1/2, step 903/23838 completed (loss: 0.9340591430664062, acc: 0.7368420958518982)
[2025-02-16 11:04:17,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:17,921][root][INFO] - Training Epoch: 1/2, step 904/23838 completed (loss: 0.7278454899787903, acc: 0.8108108043670654)
[2025-02-16 11:04:18,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:18,406][root][INFO] - Training Epoch: 1/2, step 905/23838 completed (loss: 0.7925370931625366, acc: 0.8372092843055725)
[2025-02-16 11:04:18,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:18,856][root][INFO] - Training Epoch: 1/2, step 906/23838 completed (loss: 1.0559873580932617, acc: 0.6944444179534912)
[2025-02-16 11:04:19,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:19,280][root][INFO] - Training Epoch: 1/2, step 907/23838 completed (loss: 1.6486451625823975, acc: 0.5531914830207825)
[2025-02-16 11:04:19,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:19,734][root][INFO] - Training Epoch: 1/2, step 908/23838 completed (loss: 0.46404093503952026, acc: 0.8799999952316284)
[2025-02-16 11:04:19,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:20,183][root][INFO] - Training Epoch: 1/2, step 909/23838 completed (loss: 1.364367961883545, acc: 0.6538461446762085)
[2025-02-16 11:04:20,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:20,650][root][INFO] - Training Epoch: 1/2, step 910/23838 completed (loss: 0.9271628260612488, acc: 0.699999988079071)
[2025-02-16 11:04:20,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:21,059][root][INFO] - Training Epoch: 1/2, step 911/23838 completed (loss: 1.4993256330490112, acc: 0.5925925970077515)
[2025-02-16 11:04:21,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:21,499][root][INFO] - Training Epoch: 1/2, step 912/23838 completed (loss: 1.0809824466705322, acc: 0.6666666865348816)
[2025-02-16 11:04:21,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:21,887][root][INFO] - Training Epoch: 1/2, step 913/23838 completed (loss: 0.7907937169075012, acc: 0.738095223903656)
[2025-02-16 11:04:22,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:22,366][root][INFO] - Training Epoch: 1/2, step 914/23838 completed (loss: 0.7433557510375977, acc: 0.8461538553237915)
[2025-02-16 11:04:22,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:22,831][root][INFO] - Training Epoch: 1/2, step 915/23838 completed (loss: 1.0595207214355469, acc: 0.7777777910232544)
[2025-02-16 11:04:23,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:23,234][root][INFO] - Training Epoch: 1/2, step 916/23838 completed (loss: 1.5411109924316406, acc: 0.75)
[2025-02-16 11:04:23,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:23,669][root][INFO] - Training Epoch: 1/2, step 917/23838 completed (loss: 1.2065495252609253, acc: 0.6315789222717285)
[2025-02-16 11:04:23,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:24,081][root][INFO] - Training Epoch: 1/2, step 918/23838 completed (loss: 1.242678165435791, acc: 0.7200000286102295)
[2025-02-16 11:04:24,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:24,492][root][INFO] - Training Epoch: 1/2, step 919/23838 completed (loss: 0.4982791543006897, acc: 0.8809523582458496)
[2025-02-16 11:04:24,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:24,920][root][INFO] - Training Epoch: 1/2, step 920/23838 completed (loss: 1.4931797981262207, acc: 0.7777777910232544)
[2025-02-16 11:04:25,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:25,330][root][INFO] - Training Epoch: 1/2, step 921/23838 completed (loss: 0.6047374606132507, acc: 0.8823529481887817)
[2025-02-16 11:04:25,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:25,733][root][INFO] - Training Epoch: 1/2, step 922/23838 completed (loss: 1.4192941188812256, acc: 0.6363636255264282)
[2025-02-16 11:04:25,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:26,162][root][INFO] - Training Epoch: 1/2, step 923/23838 completed (loss: 1.352494239807129, acc: 0.6764705777168274)
[2025-02-16 11:04:26,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:26,608][root][INFO] - Training Epoch: 1/2, step 924/23838 completed (loss: 0.6571754813194275, acc: 0.8695651888847351)
[2025-02-16 11:04:26,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:27,024][root][INFO] - Training Epoch: 1/2, step 925/23838 completed (loss: 0.8572087287902832, acc: 0.75)
[2025-02-16 11:04:27,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:27,480][root][INFO] - Training Epoch: 1/2, step 926/23838 completed (loss: 0.35015010833740234, acc: 0.8999999761581421)
[2025-02-16 11:04:27,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:27,942][root][INFO] - Training Epoch: 1/2, step 927/23838 completed (loss: 2.1450142860412598, acc: 0.2631579041481018)
[2025-02-16 11:04:28,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:28,351][root][INFO] - Training Epoch: 1/2, step 928/23838 completed (loss: 1.2032444477081299, acc: 0.6875)
[2025-02-16 11:04:28,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:28,809][root][INFO] - Training Epoch: 1/2, step 929/23838 completed (loss: 1.3933016061782837, acc: 0.6666666865348816)
[2025-02-16 11:04:29,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:29,234][root][INFO] - Training Epoch: 1/2, step 930/23838 completed (loss: 0.6900658011436462, acc: 0.8148148059844971)
[2025-02-16 11:04:29,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:29,670][root][INFO] - Training Epoch: 1/2, step 931/23838 completed (loss: 1.0413694381713867, acc: 0.7200000286102295)
[2025-02-16 11:04:29,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:30,078][root][INFO] - Training Epoch: 1/2, step 932/23838 completed (loss: 0.6387950778007507, acc: 0.7241379022598267)
[2025-02-16 11:04:30,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:30,524][root][INFO] - Training Epoch: 1/2, step 933/23838 completed (loss: 0.7585628032684326, acc: 0.7599999904632568)
[2025-02-16 11:04:30,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:30,943][root][INFO] - Training Epoch: 1/2, step 934/23838 completed (loss: 0.3178374171257019, acc: 0.8333333134651184)
[2025-02-16 11:04:31,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:31,356][root][INFO] - Training Epoch: 1/2, step 935/23838 completed (loss: 2.032975196838379, acc: 0.5581395626068115)
[2025-02-16 11:04:31,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:31,732][root][INFO] - Training Epoch: 1/2, step 936/23838 completed (loss: 1.684301495552063, acc: 0.5)
[2025-02-16 11:04:31,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:32,161][root][INFO] - Training Epoch: 1/2, step 937/23838 completed (loss: 1.8233455419540405, acc: 0.5681818127632141)
[2025-02-16 11:04:32,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:32,608][root][INFO] - Training Epoch: 1/2, step 938/23838 completed (loss: 0.7280532717704773, acc: 0.7272727489471436)
[2025-02-16 11:04:32,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:33,030][root][INFO] - Training Epoch: 1/2, step 939/23838 completed (loss: 2.2526133060455322, acc: 0.3913043439388275)
[2025-02-16 11:04:33,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:33,439][root][INFO] - Training Epoch: 1/2, step 940/23838 completed (loss: 2.6565299034118652, acc: 0.328125)
[2025-02-16 11:04:33,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:33,918][root][INFO] - Training Epoch: 1/2, step 941/23838 completed (loss: 2.6341519355773926, acc: 0.3564356565475464)
[2025-02-16 11:04:34,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:34,335][root][INFO] - Training Epoch: 1/2, step 942/23838 completed (loss: 2.767277240753174, acc: 0.31313130259513855)
[2025-02-16 11:04:34,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:34,730][root][INFO] - Training Epoch: 1/2, step 943/23838 completed (loss: 2.461573362350464, acc: 0.3709677457809448)
[2025-02-16 11:04:34,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:35,103][root][INFO] - Training Epoch: 1/2, step 944/23838 completed (loss: 2.04240345954895, acc: 0.4399999976158142)
[2025-02-16 11:04:35,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:35,524][root][INFO] - Training Epoch: 1/2, step 945/23838 completed (loss: 2.3055193424224854, acc: 0.3695652186870575)
[2025-02-16 11:04:35,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:35,973][root][INFO] - Training Epoch: 1/2, step 946/23838 completed (loss: 2.482274293899536, acc: 0.3333333432674408)
[2025-02-16 11:04:36,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:36,361][root][INFO] - Training Epoch: 1/2, step 947/23838 completed (loss: 2.5529699325561523, acc: 0.2857142984867096)
[2025-02-16 11:04:36,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:36,764][root][INFO] - Training Epoch: 1/2, step 948/23838 completed (loss: 2.5784270763397217, acc: 0.25)
[2025-02-16 11:04:36,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:37,216][root][INFO] - Training Epoch: 1/2, step 949/23838 completed (loss: 2.6257593631744385, acc: 0.3199999928474426)
[2025-02-16 11:04:37,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:37,645][root][INFO] - Training Epoch: 1/2, step 950/23838 completed (loss: 2.471710205078125, acc: 0.29347825050354004)
[2025-02-16 11:04:37,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:38,047][root][INFO] - Training Epoch: 1/2, step 951/23838 completed (loss: 2.37978458404541, acc: 0.3472222089767456)
[2025-02-16 11:04:38,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:38,452][root][INFO] - Training Epoch: 1/2, step 952/23838 completed (loss: 1.9805567264556885, acc: 0.478723406791687)
[2025-02-16 11:04:38,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:38,910][root][INFO] - Training Epoch: 1/2, step 953/23838 completed (loss: 2.3154282569885254, acc: 0.39393940567970276)
[2025-02-16 11:04:39,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:39,420][root][INFO] - Training Epoch: 1/2, step 954/23838 completed (loss: 2.0454933643341064, acc: 0.4262295067310333)
[2025-02-16 11:04:39,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:39,873][root][INFO] - Training Epoch: 1/2, step 955/23838 completed (loss: 2.1590118408203125, acc: 0.469696968793869)
[2025-02-16 11:04:40,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:40,423][root][INFO] - Training Epoch: 1/2, step 956/23838 completed (loss: 2.259319543838501, acc: 0.37593984603881836)
[2025-02-16 11:04:40,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:40,859][root][INFO] - Training Epoch: 1/2, step 957/23838 completed (loss: 2.2444136142730713, acc: 0.37931033968925476)
[2025-02-16 11:04:41,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:41,237][root][INFO] - Training Epoch: 1/2, step 958/23838 completed (loss: 1.9807827472686768, acc: 0.469696968793869)
[2025-02-16 11:04:41,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:41,689][root][INFO] - Training Epoch: 1/2, step 959/23838 completed (loss: 1.5994664430618286, acc: 0.5901639461517334)
[2025-02-16 11:04:41,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:42,149][root][INFO] - Training Epoch: 1/2, step 960/23838 completed (loss: 2.1151773929595947, acc: 0.5192307829856873)
[2025-02-16 11:04:42,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:42,587][root][INFO] - Training Epoch: 1/2, step 961/23838 completed (loss: 2.140336275100708, acc: 0.3604651093482971)
[2025-02-16 11:04:42,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:42,998][root][INFO] - Training Epoch: 1/2, step 962/23838 completed (loss: 1.6731507778167725, acc: 0.5675675868988037)
[2025-02-16 11:04:43,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:43,468][root][INFO] - Training Epoch: 1/2, step 963/23838 completed (loss: 2.579556703567505, acc: 0.4057970941066742)
[2025-02-16 11:04:43,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:43,895][root][INFO] - Training Epoch: 1/2, step 964/23838 completed (loss: 1.9970715045928955, acc: 0.4038461446762085)
[2025-02-16 11:04:44,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:44,360][root][INFO] - Training Epoch: 1/2, step 965/23838 completed (loss: 2.429471015930176, acc: 0.3055555522441864)
[2025-02-16 11:04:44,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:44,852][root][INFO] - Training Epoch: 1/2, step 966/23838 completed (loss: 2.159250259399414, acc: 0.40860214829444885)
[2025-02-16 11:04:45,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:45,320][root][INFO] - Training Epoch: 1/2, step 967/23838 completed (loss: 2.463728427886963, acc: 0.3466666638851166)
[2025-02-16 11:04:45,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:45,688][root][INFO] - Training Epoch: 1/2, step 968/23838 completed (loss: 1.962091326713562, acc: 0.4642857015132904)
[2025-02-16 11:04:45,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:46,145][root][INFO] - Training Epoch: 1/2, step 969/23838 completed (loss: 2.4408063888549805, acc: 0.3636363744735718)
[2025-02-16 11:04:46,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:46,592][root][INFO] - Training Epoch: 1/2, step 970/23838 completed (loss: 2.3790783882141113, acc: 0.3439490497112274)
[2025-02-16 11:04:46,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:47,024][root][INFO] - Training Epoch: 1/2, step 971/23838 completed (loss: 2.468865394592285, acc: 0.3285714387893677)
[2025-02-16 11:04:47,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:47,427][root][INFO] - Training Epoch: 1/2, step 972/23838 completed (loss: 1.9581753015518188, acc: 0.46715328097343445)
[2025-02-16 11:04:47,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:47,809][root][INFO] - Training Epoch: 1/2, step 973/23838 completed (loss: 1.7587140798568726, acc: 0.5185185074806213)
[2025-02-16 11:04:48,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:48,223][root][INFO] - Training Epoch: 1/2, step 974/23838 completed (loss: 2.1662213802337646, acc: 0.4301075339317322)
[2025-02-16 11:04:48,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:48,680][root][INFO] - Training Epoch: 1/2, step 975/23838 completed (loss: 1.7770663499832153, acc: 0.49056604504585266)
[2025-02-16 11:04:48,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:49,132][root][INFO] - Training Epoch: 1/2, step 976/23838 completed (loss: 2.191525459289551, acc: 0.3888888955116272)
[2025-02-16 11:04:49,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:49,570][root][INFO] - Training Epoch: 1/2, step 977/23838 completed (loss: 2.2303712368011475, acc: 0.43518519401550293)
[2025-02-16 11:04:49,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:50,003][root][INFO] - Training Epoch: 1/2, step 978/23838 completed (loss: 1.9595335721969604, acc: 0.46315789222717285)
[2025-02-16 11:04:50,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:50,414][root][INFO] - Training Epoch: 1/2, step 979/23838 completed (loss: 2.3262670040130615, acc: 0.3786407709121704)
[2025-02-16 11:04:50,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:50,844][root][INFO] - Training Epoch: 1/2, step 980/23838 completed (loss: 2.3425211906433105, acc: 0.3214285671710968)
[2025-02-16 11:04:51,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:51,239][root][INFO] - Training Epoch: 1/2, step 981/23838 completed (loss: 2.285111427307129, acc: 0.36231884360313416)
[2025-02-16 11:04:51,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:51,627][root][INFO] - Training Epoch: 1/2, step 982/23838 completed (loss: 1.540906548500061, acc: 0.5600000023841858)
[2025-02-16 11:04:51,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:52,093][root][INFO] - Training Epoch: 1/2, step 983/23838 completed (loss: 1.9150495529174805, acc: 0.4215686321258545)
[2025-02-16 11:04:52,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:52,474][root][INFO] - Training Epoch: 1/2, step 984/23838 completed (loss: 2.2081589698791504, acc: 0.3636363744735718)
[2025-02-16 11:04:52,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:52,891][root][INFO] - Training Epoch: 1/2, step 985/23838 completed (loss: 1.5786197185516357, acc: 0.6000000238418579)
[2025-02-16 11:04:53,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:53,375][root][INFO] - Training Epoch: 1/2, step 986/23838 completed (loss: 1.9616094827651978, acc: 0.48051947355270386)
[2025-02-16 11:04:53,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:53,815][root][INFO] - Training Epoch: 1/2, step 987/23838 completed (loss: 2.063267230987549, acc: 0.37142857909202576)
[2025-02-16 11:04:54,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:54,239][root][INFO] - Training Epoch: 1/2, step 988/23838 completed (loss: 2.1245250701904297, acc: 0.3580246865749359)
[2025-02-16 11:04:54,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:54,606][root][INFO] - Training Epoch: 1/2, step 989/23838 completed (loss: 1.8249653577804565, acc: 0.484375)
[2025-02-16 11:04:54,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:55,107][root][INFO] - Training Epoch: 1/2, step 990/23838 completed (loss: 2.12471866607666, acc: 0.4095238149166107)
[2025-02-16 11:04:55,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:55,566][root][INFO] - Training Epoch: 1/2, step 991/23838 completed (loss: 2.631831645965576, acc: 0.3139534890651703)
[2025-02-16 11:04:55,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:56,013][root][INFO] - Training Epoch: 1/2, step 992/23838 completed (loss: 2.3364696502685547, acc: 0.3333333432674408)
[2025-02-16 11:04:56,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:56,387][root][INFO] - Training Epoch: 1/2, step 993/23838 completed (loss: 2.172083616256714, acc: 0.4367816150188446)
[2025-02-16 11:04:56,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:56,809][root][INFO] - Training Epoch: 1/2, step 994/23838 completed (loss: 1.5062706470489502, acc: 0.6363636255264282)
[2025-02-16 11:04:56,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:57,176][root][INFO] - Training Epoch: 1/2, step 995/23838 completed (loss: 1.7459275722503662, acc: 0.5094339847564697)
[2025-02-16 11:04:57,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:57,644][root][INFO] - Training Epoch: 1/2, step 996/23838 completed (loss: 1.923461675643921, acc: 0.47663551568984985)
[2025-02-16 11:04:57,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:58,041][root][INFO] - Training Epoch: 1/2, step 997/23838 completed (loss: 1.6278914213180542, acc: 0.5545454621315002)
[2025-02-16 11:04:58,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:58,553][root][INFO] - Training Epoch: 1/2, step 998/23838 completed (loss: 1.7405585050582886, acc: 0.5948275923728943)
[2025-02-16 11:04:58,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:59,002][root][INFO] - Training Epoch: 1/2, step 999/23838 completed (loss: 2.134495258331299, acc: 0.36206895112991333)
[2025-02-16 11:04:59,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:59,418][root][INFO] - Training Epoch: 1/2, step 1000/23838 completed (loss: 2.101722002029419, acc: 0.5)
[2025-02-16 11:04:59,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:04:59,776][root][INFO] - Training Epoch: 1/2, step 1001/23838 completed (loss: 2.075005531311035, acc: 0.40229883790016174)
[2025-02-16 11:05:00,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:00,241][root][INFO] - Training Epoch: 1/2, step 1002/23838 completed (loss: 2.2207553386688232, acc: 0.40506330132484436)
[2025-02-16 11:05:00,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:00,684][root][INFO] - Training Epoch: 1/2, step 1003/23838 completed (loss: 2.4673876762390137, acc: 0.3505154550075531)
[2025-02-16 11:05:00,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:01,165][root][INFO] - Training Epoch: 1/2, step 1004/23838 completed (loss: 1.9643564224243164, acc: 0.4457831382751465)
[2025-02-16 11:05:01,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:01,609][root][INFO] - Training Epoch: 1/2, step 1005/23838 completed (loss: 1.9431865215301514, acc: 0.5789473652839661)
[2025-02-16 11:05:01,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:02,085][root][INFO] - Training Epoch: 1/2, step 1006/23838 completed (loss: 2.1458959579467773, acc: 0.42276424169540405)
[2025-02-16 11:05:02,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:02,456][root][INFO] - Training Epoch: 1/2, step 1007/23838 completed (loss: 2.1560633182525635, acc: 0.3863636255264282)
[2025-02-16 11:05:02,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:02,905][root][INFO] - Training Epoch: 1/2, step 1008/23838 completed (loss: 1.9747589826583862, acc: 0.4375)
[2025-02-16 11:05:03,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:03,286][root][INFO] - Training Epoch: 1/2, step 1009/23838 completed (loss: 1.8915759325027466, acc: 0.4893617033958435)
[2025-02-16 11:05:03,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:03,728][root][INFO] - Training Epoch: 1/2, step 1010/23838 completed (loss: 1.573982834815979, acc: 0.5961538553237915)
[2025-02-16 11:05:03,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:04,135][root][INFO] - Training Epoch: 1/2, step 1011/23838 completed (loss: 1.9623103141784668, acc: 0.449438214302063)
[2025-02-16 11:05:04,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:04,613][root][INFO] - Training Epoch: 1/2, step 1012/23838 completed (loss: 1.9674581289291382, acc: 0.45588234066963196)
[2025-02-16 11:05:04,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:05,011][root][INFO] - Training Epoch: 1/2, step 1013/23838 completed (loss: 1.7879765033721924, acc: 0.46875)
[2025-02-16 11:05:05,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:05,392][root][INFO] - Training Epoch: 1/2, step 1014/23838 completed (loss: 1.8648607730865479, acc: 0.5405405163764954)
[2025-02-16 11:05:05,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:05,822][root][INFO] - Training Epoch: 1/2, step 1015/23838 completed (loss: 2.2205185890197754, acc: 0.39534884691238403)
[2025-02-16 11:05:06,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:06,327][root][INFO] - Training Epoch: 1/2, step 1016/23838 completed (loss: 1.8903940916061401, acc: 0.46590909361839294)
[2025-02-16 11:05:06,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:06,788][root][INFO] - Training Epoch: 1/2, step 1017/23838 completed (loss: 1.7781951427459717, acc: 0.5227272510528564)
[2025-02-16 11:05:06,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:07,182][root][INFO] - Training Epoch: 1/2, step 1018/23838 completed (loss: 1.8189727067947388, acc: 0.47887325286865234)
[2025-02-16 11:05:07,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:07,616][root][INFO] - Training Epoch: 1/2, step 1019/23838 completed (loss: 2.105212688446045, acc: 0.44029849767684937)
[2025-02-16 11:05:07,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:08,020][root][INFO] - Training Epoch: 1/2, step 1020/23838 completed (loss: 1.6893364191055298, acc: 0.4838709533214569)
[2025-02-16 11:05:08,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:08,407][root][INFO] - Training Epoch: 1/2, step 1021/23838 completed (loss: 1.8626781702041626, acc: 0.4699999988079071)
[2025-02-16 11:05:08,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:08,780][root][INFO] - Training Epoch: 1/2, step 1022/23838 completed (loss: 2.070518732070923, acc: 0.4385964870452881)
[2025-02-16 11:05:08,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:09,111][root][INFO] - Training Epoch: 1/2, step 1023/23838 completed (loss: 1.7855874300003052, acc: 0.5675675868988037)
[2025-02-16 11:05:09,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:09,531][root][INFO] - Training Epoch: 1/2, step 1024/23838 completed (loss: 1.8948945999145508, acc: 0.44961240887641907)
[2025-02-16 11:05:09,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:09,905][root][INFO] - Training Epoch: 1/2, step 1025/23838 completed (loss: 2.100445032119751, acc: 0.43809524178504944)
[2025-02-16 11:05:10,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:10,326][root][INFO] - Training Epoch: 1/2, step 1026/23838 completed (loss: 1.8834258317947388, acc: 0.4577464759349823)
[2025-02-16 11:05:10,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:10,738][root][INFO] - Training Epoch: 1/2, step 1027/23838 completed (loss: 1.825716257095337, acc: 0.4965035021305084)
[2025-02-16 11:05:10,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:11,157][root][INFO] - Training Epoch: 1/2, step 1028/23838 completed (loss: 2.26330304145813, acc: 0.3469387888908386)
[2025-02-16 11:05:11,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:11,616][root][INFO] - Training Epoch: 1/2, step 1029/23838 completed (loss: 1.7530033588409424, acc: 0.5232558250427246)
[2025-02-16 11:05:11,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:12,116][root][INFO] - Training Epoch: 1/2, step 1030/23838 completed (loss: 2.150590419769287, acc: 0.420560747385025)
[2025-02-16 11:05:12,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:12,563][root][INFO] - Training Epoch: 1/2, step 1031/23838 completed (loss: 2.0088677406311035, acc: 0.4545454680919647)
[2025-02-16 11:05:12,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:13,024][root][INFO] - Training Epoch: 1/2, step 1032/23838 completed (loss: 1.746578574180603, acc: 0.4792899489402771)
[2025-02-16 11:05:13,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:13,464][root][INFO] - Training Epoch: 1/2, step 1033/23838 completed (loss: 1.975765347480774, acc: 0.49367088079452515)
[2025-02-16 11:05:13,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:13,844][root][INFO] - Training Epoch: 1/2, step 1034/23838 completed (loss: 1.9323393106460571, acc: 0.45121949911117554)
[2025-02-16 11:05:13,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:14,217][root][INFO] - Training Epoch: 1/2, step 1035/23838 completed (loss: 1.9327688217163086, acc: 0.44999998807907104)
[2025-02-16 11:05:14,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:14,649][root][INFO] - Training Epoch: 1/2, step 1036/23838 completed (loss: 2.3733322620391846, acc: 0.3762376308441162)
[2025-02-16 11:05:14,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:15,122][root][INFO] - Training Epoch: 1/2, step 1037/23838 completed (loss: 1.973056674003601, acc: 0.44565218687057495)
[2025-02-16 11:05:15,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:15,606][root][INFO] - Training Epoch: 1/2, step 1038/23838 completed (loss: 1.673382043838501, acc: 0.5128205418586731)
[2025-02-16 11:05:15,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:16,054][root][INFO] - Training Epoch: 1/2, step 1039/23838 completed (loss: 1.7065833806991577, acc: 0.5319148898124695)
[2025-02-16 11:05:16,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:16,545][root][INFO] - Training Epoch: 1/2, step 1040/23838 completed (loss: 2.060202121734619, acc: 0.4202898442745209)
[2025-02-16 11:05:16,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:16,977][root][INFO] - Training Epoch: 1/2, step 1041/23838 completed (loss: 1.821115493774414, acc: 0.47999998927116394)
[2025-02-16 11:05:17,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:17,575][root][INFO] - Training Epoch: 1/2, step 1042/23838 completed (loss: 1.9292646646499634, acc: 0.4316546618938446)
[2025-02-16 11:05:17,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:18,035][root][INFO] - Training Epoch: 1/2, step 1043/23838 completed (loss: 1.8684914112091064, acc: 0.48255813121795654)
[2025-02-16 11:05:18,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:18,486][root][INFO] - Training Epoch: 1/2, step 1044/23838 completed (loss: 1.4396586418151855, acc: 0.5820895433425903)
[2025-02-16 11:05:18,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:18,909][root][INFO] - Training Epoch: 1/2, step 1045/23838 completed (loss: 2.1830239295959473, acc: 0.421875)
[2025-02-16 11:05:19,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:19,356][root][INFO] - Training Epoch: 1/2, step 1046/23838 completed (loss: 1.7517741918563843, acc: 0.508474588394165)
[2025-02-16 11:05:19,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:19,750][root][INFO] - Training Epoch: 1/2, step 1047/23838 completed (loss: 1.843080997467041, acc: 0.4166666567325592)
[2025-02-16 11:05:19,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:20,145][root][INFO] - Training Epoch: 1/2, step 1048/23838 completed (loss: 1.706153154373169, acc: 0.5)
[2025-02-16 11:05:20,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:20,547][root][INFO] - Training Epoch: 1/2, step 1049/23838 completed (loss: 1.590603232383728, acc: 0.5454545617103577)
[2025-02-16 11:05:20,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:20,929][root][INFO] - Training Epoch: 1/2, step 1050/23838 completed (loss: 1.3082001209259033, acc: 0.5882353186607361)
[2025-02-16 11:05:21,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:21,364][root][INFO] - Training Epoch: 1/2, step 1051/23838 completed (loss: 1.6120972633361816, acc: 0.5319148898124695)
[2025-02-16 11:05:21,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:21,854][root][INFO] - Training Epoch: 1/2, step 1052/23838 completed (loss: 1.8612545728683472, acc: 0.49295774102211)
[2025-02-16 11:05:22,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:22,234][root][INFO] - Training Epoch: 1/2, step 1053/23838 completed (loss: 1.7560228109359741, acc: 0.5416666865348816)
[2025-02-16 11:05:22,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:22,624][root][INFO] - Training Epoch: 1/2, step 1054/23838 completed (loss: 1.5816537141799927, acc: 0.5094339847564697)
[2025-02-16 11:05:22,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:23,057][root][INFO] - Training Epoch: 1/2, step 1055/23838 completed (loss: 1.6420124769210815, acc: 0.4722222089767456)
[2025-02-16 11:05:23,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:23,506][root][INFO] - Training Epoch: 1/2, step 1056/23838 completed (loss: 1.3805419206619263, acc: 0.6037735939025879)
[2025-02-16 11:05:23,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:23,955][root][INFO] - Training Epoch: 1/2, step 1057/23838 completed (loss: 1.3790972232818604, acc: 0.6166666746139526)
[2025-02-16 11:05:24,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:24,364][root][INFO] - Training Epoch: 1/2, step 1058/23838 completed (loss: 1.6111011505126953, acc: 0.5256410241127014)
[2025-02-16 11:05:24,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:24,885][root][INFO] - Training Epoch: 1/2, step 1059/23838 completed (loss: 1.3118467330932617, acc: 0.6859503984451294)
[2025-02-16 11:05:25,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:25,294][root][INFO] - Training Epoch: 1/2, step 1060/23838 completed (loss: 2.042210578918457, acc: 0.4523809552192688)
[2025-02-16 11:05:25,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:25,709][root][INFO] - Training Epoch: 1/2, step 1061/23838 completed (loss: 1.8638325929641724, acc: 0.42424243688583374)
[2025-02-16 11:05:25,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:26,113][root][INFO] - Training Epoch: 1/2, step 1062/23838 completed (loss: 1.8329591751098633, acc: 0.4285714328289032)
[2025-02-16 11:05:26,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:26,483][root][INFO] - Training Epoch: 1/2, step 1063/23838 completed (loss: 1.5931020975112915, acc: 0.5857142806053162)
[2025-02-16 11:05:26,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:26,905][root][INFO] - Training Epoch: 1/2, step 1064/23838 completed (loss: 1.1400787830352783, acc: 0.6619718074798584)
[2025-02-16 11:05:27,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:27,318][root][INFO] - Training Epoch: 1/2, step 1065/23838 completed (loss: 1.438953161239624, acc: 0.6120689511299133)
[2025-02-16 11:05:27,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:27,742][root][INFO] - Training Epoch: 1/2, step 1066/23838 completed (loss: 1.7261794805526733, acc: 0.4566929042339325)
[2025-02-16 11:05:27,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:28,155][root][INFO] - Training Epoch: 1/2, step 1067/23838 completed (loss: 1.4088093042373657, acc: 0.6161616444587708)
[2025-02-16 11:05:28,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:28,561][root][INFO] - Training Epoch: 1/2, step 1068/23838 completed (loss: 2.0500385761260986, acc: 0.39344263076782227)
[2025-02-16 11:05:28,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:29,119][root][INFO] - Training Epoch: 1/2, step 1069/23838 completed (loss: 1.864711880683899, acc: 0.5209580659866333)
[2025-02-16 11:05:29,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:29,581][root][INFO] - Training Epoch: 1/2, step 1070/23838 completed (loss: 2.0597634315490723, acc: 0.4303797483444214)
[2025-02-16 11:05:29,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:30,038][root][INFO] - Training Epoch: 1/2, step 1071/23838 completed (loss: 1.7080399990081787, acc: 0.5655737519264221)
[2025-02-16 11:05:30,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:30,524][root][INFO] - Training Epoch: 1/2, step 1072/23838 completed (loss: 1.7201428413391113, acc: 0.5573770403862)
[2025-02-16 11:05:30,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:30,935][root][INFO] - Training Epoch: 1/2, step 1073/23838 completed (loss: 1.8129409551620483, acc: 0.4712643623352051)
[2025-02-16 11:05:31,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:31,361][root][INFO] - Training Epoch: 1/2, step 1074/23838 completed (loss: 1.5626362562179565, acc: 0.5703703761100769)
[2025-02-16 11:05:31,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:31,779][root][INFO] - Training Epoch: 1/2, step 1075/23838 completed (loss: 1.025889277458191, acc: 0.6851851940155029)
[2025-02-16 11:05:32,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:32,256][root][INFO] - Training Epoch: 1/2, step 1076/23838 completed (loss: 1.8235983848571777, acc: 0.49132949113845825)
[2025-02-16 11:05:32,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:32,740][root][INFO] - Training Epoch: 1/2, step 1077/23838 completed (loss: 1.4273934364318848, acc: 0.6281406879425049)
[2025-02-16 11:05:32,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:33,209][root][INFO] - Training Epoch: 1/2, step 1078/23838 completed (loss: 1.531578779220581, acc: 0.5473684072494507)
[2025-02-16 11:05:33,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:33,729][root][INFO] - Training Epoch: 1/2, step 1079/23838 completed (loss: 1.3866095542907715, acc: 0.5876288414001465)
[2025-02-16 11:05:33,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:34,162][root][INFO] - Training Epoch: 1/2, step 1080/23838 completed (loss: 1.2898989915847778, acc: 0.6935483813285828)
[2025-02-16 11:05:34,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:34,726][root][INFO] - Training Epoch: 1/2, step 1081/23838 completed (loss: 1.1819204092025757, acc: 0.6935483813285828)
[2025-02-16 11:05:34,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:35,083][root][INFO] - Training Epoch: 1/2, step 1082/23838 completed (loss: 1.6366506814956665, acc: 0.6619718074798584)
[2025-02-16 11:05:35,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:35,531][root][INFO] - Training Epoch: 1/2, step 1083/23838 completed (loss: 1.9641321897506714, acc: 0.4431818127632141)
[2025-02-16 11:05:35,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:35,944][root][INFO] - Training Epoch: 1/2, step 1084/23838 completed (loss: 1.5418697595596313, acc: 0.5892857313156128)
[2025-02-16 11:05:36,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:36,333][root][INFO] - Training Epoch: 1/2, step 1085/23838 completed (loss: 1.8042887449264526, acc: 0.5714285969734192)
[2025-02-16 11:05:36,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:36,804][root][INFO] - Training Epoch: 1/2, step 1086/23838 completed (loss: 1.5559788942337036, acc: 0.5882353186607361)
[2025-02-16 11:05:37,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:37,253][root][INFO] - Training Epoch: 1/2, step 1087/23838 completed (loss: 1.1815217733383179, acc: 0.6393442749977112)
[2025-02-16 11:05:37,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:37,703][root][INFO] - Training Epoch: 1/2, step 1088/23838 completed (loss: 1.6651424169540405, acc: 0.6117647290229797)
[2025-02-16 11:05:37,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:38,165][root][INFO] - Training Epoch: 1/2, step 1089/23838 completed (loss: 2.1224145889282227, acc: 0.4038461446762085)
[2025-02-16 11:05:38,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:38,701][root][INFO] - Training Epoch: 1/2, step 1090/23838 completed (loss: 1.795700192451477, acc: 0.54347825050354)
[2025-02-16 11:05:38,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:39,152][root][INFO] - Training Epoch: 1/2, step 1091/23838 completed (loss: 1.5375587940216064, acc: 0.5822784900665283)
[2025-02-16 11:05:39,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:39,538][root][INFO] - Training Epoch: 1/2, step 1092/23838 completed (loss: 2.2722795009613037, acc: 0.37037035822868347)
[2025-02-16 11:05:39,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:39,983][root][INFO] - Training Epoch: 1/2, step 1093/23838 completed (loss: 1.7587522268295288, acc: 0.529411792755127)
[2025-02-16 11:05:40,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:40,410][root][INFO] - Training Epoch: 1/2, step 1094/23838 completed (loss: 2.1296422481536865, acc: 0.44736841320991516)
[2025-02-16 11:05:40,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:40,860][root][INFO] - Training Epoch: 1/2, step 1095/23838 completed (loss: 1.6620945930480957, acc: 0.5438596606254578)
[2025-02-16 11:05:41,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:41,317][root][INFO] - Training Epoch: 1/2, step 1096/23838 completed (loss: 1.2937073707580566, acc: 0.6410256624221802)
[2025-02-16 11:05:41,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:41,841][root][INFO] - Training Epoch: 1/2, step 1097/23838 completed (loss: 1.4287718534469604, acc: 0.6290322542190552)
[2025-02-16 11:05:42,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:42,242][root][INFO] - Training Epoch: 1/2, step 1098/23838 completed (loss: 1.697891116142273, acc: 0.5510203838348389)
[2025-02-16 11:05:42,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:42,672][root][INFO] - Training Epoch: 1/2, step 1099/23838 completed (loss: 1.2357773780822754, acc: 0.6666666865348816)
[2025-02-16 11:05:42,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:43,095][root][INFO] - Training Epoch: 1/2, step 1100/23838 completed (loss: 1.866479754447937, acc: 0.4399999976158142)
[2025-02-16 11:05:43,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:43,489][root][INFO] - Training Epoch: 1/2, step 1101/23838 completed (loss: 1.833249807357788, acc: 0.5178571343421936)
[2025-02-16 11:05:43,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:43,924][root][INFO] - Training Epoch: 1/2, step 1102/23838 completed (loss: 1.9297869205474854, acc: 0.4523809552192688)
[2025-02-16 11:05:44,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:44,340][root][INFO] - Training Epoch: 1/2, step 1103/23838 completed (loss: 2.0276637077331543, acc: 0.46341463923454285)
[2025-02-16 11:05:44,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:44,786][root][INFO] - Training Epoch: 1/2, step 1104/23838 completed (loss: 1.3838070631027222, acc: 0.6590909361839294)
[2025-02-16 11:05:44,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:45,136][root][INFO] - Training Epoch: 1/2, step 1105/23838 completed (loss: 1.6670233011245728, acc: 0.4285714328289032)
[2025-02-16 11:05:45,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:45,492][root][INFO] - Training Epoch: 1/2, step 1106/23838 completed (loss: 1.73843252658844, acc: 0.4791666567325592)
[2025-02-16 11:05:45,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:45,911][root][INFO] - Training Epoch: 1/2, step 1107/23838 completed (loss: 2.2342236042022705, acc: 0.42105263471603394)
[2025-02-16 11:05:46,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:46,332][root][INFO] - Training Epoch: 1/2, step 1108/23838 completed (loss: 1.798203945159912, acc: 0.4864864945411682)
[2025-02-16 11:05:46,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:46,709][root][INFO] - Training Epoch: 1/2, step 1109/23838 completed (loss: 1.47488534450531, acc: 0.6222222447395325)
[2025-02-16 11:05:46,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:47,114][root][INFO] - Training Epoch: 1/2, step 1110/23838 completed (loss: 1.3562637567520142, acc: 0.7058823704719543)
[2025-02-16 11:05:47,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:47,473][root][INFO] - Training Epoch: 1/2, step 1111/23838 completed (loss: 2.668482542037964, acc: 0.3478260934352875)
[2025-02-16 11:05:47,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:47,921][root][INFO] - Training Epoch: 1/2, step 1112/23838 completed (loss: 2.5803885459899902, acc: 0.35593220591545105)
[2025-02-16 11:05:48,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:48,346][root][INFO] - Training Epoch: 1/2, step 1113/23838 completed (loss: 3.3150179386138916, acc: 0.0)
[2025-02-16 11:05:48,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:48,828][root][INFO] - Training Epoch: 1/2, step 1114/23838 completed (loss: 1.7683242559432983, acc: 0.5454545617103577)
[2025-02-16 11:05:49,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:49,237][root][INFO] - Training Epoch: 1/2, step 1115/23838 completed (loss: 2.4707863330841064, acc: 0.46666666865348816)
[2025-02-16 11:05:49,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:49,647][root][INFO] - Training Epoch: 1/2, step 1116/23838 completed (loss: 2.6973044872283936, acc: 0.35483869910240173)
[2025-02-16 11:05:49,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:50,047][root][INFO] - Training Epoch: 1/2, step 1117/23838 completed (loss: 2.205374240875244, acc: 0.44285714626312256)
[2025-02-16 11:05:50,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:50,456][root][INFO] - Training Epoch: 1/2, step 1118/23838 completed (loss: 2.132319450378418, acc: 0.37142857909202576)
[2025-02-16 11:05:50,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:50,943][root][INFO] - Training Epoch: 1/2, step 1119/23838 completed (loss: 1.9966446161270142, acc: 0.4193548262119293)
[2025-02-16 11:05:51,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:51,406][root][INFO] - Training Epoch: 1/2, step 1120/23838 completed (loss: 2.2681264877319336, acc: 0.37662336230278015)
[2025-02-16 11:05:51,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:51,826][root][INFO] - Training Epoch: 1/2, step 1121/23838 completed (loss: 2.1439461708068848, acc: 0.6315789222717285)
[2025-02-16 11:05:52,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:52,316][root][INFO] - Training Epoch: 1/2, step 1122/23838 completed (loss: 1.8942375183105469, acc: 0.5074626803398132)
[2025-02-16 11:05:52,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:52,741][root][INFO] - Training Epoch: 1/2, step 1123/23838 completed (loss: 2.0429575443267822, acc: 0.4556961953639984)
[2025-02-16 11:05:52,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:53,152][root][INFO] - Training Epoch: 1/2, step 1124/23838 completed (loss: 2.154090166091919, acc: 0.5384615659713745)
[2025-02-16 11:05:53,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:53,551][root][INFO] - Training Epoch: 1/2, step 1125/23838 completed (loss: 1.5754473209381104, acc: 0.5824176073074341)
[2025-02-16 11:05:53,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:54,004][root][INFO] - Training Epoch: 1/2, step 1126/23838 completed (loss: 1.5878108739852905, acc: 0.5675675868988037)
[2025-02-16 11:05:54,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:54,411][root][INFO] - Training Epoch: 1/2, step 1127/23838 completed (loss: 1.6689515113830566, acc: 0.4675324559211731)
[2025-02-16 11:05:54,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:54,900][root][INFO] - Training Epoch: 1/2, step 1128/23838 completed (loss: 1.633487582206726, acc: 0.510869562625885)
[2025-02-16 11:05:55,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:55,329][root][INFO] - Training Epoch: 1/2, step 1129/23838 completed (loss: 0.8907025456428528, acc: 0.7307692170143127)
[2025-02-16 11:05:55,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:55,873][root][INFO] - Training Epoch: 1/2, step 1130/23838 completed (loss: 1.7715282440185547, acc: 0.45614033937454224)
[2025-02-16 11:05:56,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:56,331][root][INFO] - Training Epoch: 1/2, step 1131/23838 completed (loss: 1.4392828941345215, acc: 0.5585585832595825)
[2025-02-16 11:05:56,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:56,774][root][INFO] - Training Epoch: 1/2, step 1132/23838 completed (loss: 1.2256739139556885, acc: 0.6666666865348816)
[2025-02-16 11:05:56,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:57,135][root][INFO] - Training Epoch: 1/2, step 1133/23838 completed (loss: 1.2241863012313843, acc: 0.6746987700462341)
[2025-02-16 11:05:57,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:57,588][root][INFO] - Training Epoch: 1/2, step 1134/23838 completed (loss: 1.5999938249588013, acc: 0.5)
[2025-02-16 11:05:57,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:57,957][root][INFO] - Training Epoch: 1/2, step 1135/23838 completed (loss: 1.4728082418441772, acc: 0.6111111044883728)
[2025-02-16 11:05:58,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:58,399][root][INFO] - Training Epoch: 1/2, step 1136/23838 completed (loss: 1.5938438177108765, acc: 0.6206896305084229)
[2025-02-16 11:05:58,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:58,808][root][INFO] - Training Epoch: 1/2, step 1137/23838 completed (loss: 0.9833127856254578, acc: 0.7346938848495483)
[2025-02-16 11:05:59,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:59,229][root][INFO] - Training Epoch: 1/2, step 1138/23838 completed (loss: 1.3898476362228394, acc: 0.6000000238418579)
[2025-02-16 11:05:59,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:05:59,672][root][INFO] - Training Epoch: 1/2, step 1139/23838 completed (loss: 1.517486333847046, acc: 0.5853658318519592)
[2025-02-16 11:05:59,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:00,113][root][INFO] - Training Epoch: 1/2, step 1140/23838 completed (loss: 1.2724865674972534, acc: 0.6436781883239746)
[2025-02-16 11:06:00,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:00,517][root][INFO] - Training Epoch: 1/2, step 1141/23838 completed (loss: 1.3000109195709229, acc: 0.6666666865348816)
[2025-02-16 11:06:00,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:00,990][root][INFO] - Training Epoch: 1/2, step 1142/23838 completed (loss: 0.9437233209609985, acc: 0.75)
[2025-02-16 11:06:01,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:01,400][root][INFO] - Training Epoch: 1/2, step 1143/23838 completed (loss: 1.5762608051300049, acc: 0.5701754093170166)
[2025-02-16 11:06:01,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:01,927][root][INFO] - Training Epoch: 1/2, step 1144/23838 completed (loss: 1.1653742790222168, acc: 0.686274528503418)
[2025-02-16 11:06:02,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:02,382][root][INFO] - Training Epoch: 1/2, step 1145/23838 completed (loss: 1.6141797304153442, acc: 0.5737704634666443)
[2025-02-16 11:06:02,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:02,821][root][INFO] - Training Epoch: 1/2, step 1146/23838 completed (loss: 1.783871054649353, acc: 0.5483871102333069)
[2025-02-16 11:06:02,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:03,215][root][INFO] - Training Epoch: 1/2, step 1147/23838 completed (loss: 1.473680853843689, acc: 0.5903614163398743)
[2025-02-16 11:06:03,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:04,134][root][INFO] - Training Epoch: 1/2, step 1148/23838 completed (loss: 1.9520831108093262, acc: 0.4390243887901306)
[2025-02-16 11:06:04,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:04,578][root][INFO] - Training Epoch: 1/2, step 1149/23838 completed (loss: 1.5918691158294678, acc: 0.5189873576164246)
[2025-02-16 11:06:04,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:04,976][root][INFO] - Training Epoch: 1/2, step 1150/23838 completed (loss: 1.777490258216858, acc: 0.4952380955219269)
[2025-02-16 11:06:05,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:05,438][root][INFO] - Training Epoch: 1/2, step 1151/23838 completed (loss: 1.6219041347503662, acc: 0.581632673740387)
[2025-02-16 11:06:05,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:05,887][root][INFO] - Training Epoch: 1/2, step 1152/23838 completed (loss: 1.0465795993804932, acc: 0.6913580298423767)
[2025-02-16 11:06:06,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:06,362][root][INFO] - Training Epoch: 1/2, step 1153/23838 completed (loss: 1.404435634613037, acc: 0.6136363744735718)
[2025-02-16 11:06:06,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:06,829][root][INFO] - Training Epoch: 1/2, step 1154/23838 completed (loss: 1.0095247030258179, acc: 0.729411780834198)
[2025-02-16 11:06:07,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:07,266][root][INFO] - Training Epoch: 1/2, step 1155/23838 completed (loss: 1.0510374307632446, acc: 0.7402597665786743)
[2025-02-16 11:06:07,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:07,717][root][INFO] - Training Epoch: 1/2, step 1156/23838 completed (loss: 1.139806866645813, acc: 0.6557376980781555)
[2025-02-16 11:06:07,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:08,124][root][INFO] - Training Epoch: 1/2, step 1157/23838 completed (loss: 0.9835666418075562, acc: 0.7184466123580933)
[2025-02-16 11:06:08,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:08,723][root][INFO] - Training Epoch: 1/2, step 1158/23838 completed (loss: 1.5281316041946411, acc: 0.5593220591545105)
[2025-02-16 11:06:09,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:09,477][root][INFO] - Training Epoch: 1/2, step 1159/23838 completed (loss: 1.1378672122955322, acc: 0.7021276354789734)
[2025-02-16 11:06:09,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:09,899][root][INFO] - Training Epoch: 1/2, step 1160/23838 completed (loss: 1.0752822160720825, acc: 0.6296296119689941)
[2025-02-16 11:06:10,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:10,312][root][INFO] - Training Epoch: 1/2, step 1161/23838 completed (loss: 1.3361575603485107, acc: 0.6530612111091614)
[2025-02-16 11:06:10,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:10,693][root][INFO] - Training Epoch: 1/2, step 1162/23838 completed (loss: 1.0381354093551636, acc: 0.7058823704719543)
[2025-02-16 11:06:10,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:11,144][root][INFO] - Training Epoch: 1/2, step 1163/23838 completed (loss: 1.1084152460098267, acc: 0.6966292262077332)
[2025-02-16 11:06:11,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:11,516][root][INFO] - Training Epoch: 1/2, step 1164/23838 completed (loss: 1.502462387084961, acc: 0.6271186470985413)
[2025-02-16 11:06:11,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:12,062][root][INFO] - Training Epoch: 1/2, step 1165/23838 completed (loss: 0.8757933974266052, acc: 0.6888889074325562)
[2025-02-16 11:06:12,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:12,469][root][INFO] - Training Epoch: 1/2, step 1166/23838 completed (loss: 1.5983437299728394, acc: 0.5789473652839661)
[2025-02-16 11:06:13,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:13,600][root][INFO] - Training Epoch: 1/2, step 1167/23838 completed (loss: 1.1246846914291382, acc: 0.6495726704597473)
[2025-02-16 11:06:13,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:14,189][root][INFO] - Training Epoch: 1/2, step 1168/23838 completed (loss: 0.9206333756446838, acc: 0.7307692170143127)
[2025-02-16 11:06:14,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:14,621][root][INFO] - Training Epoch: 1/2, step 1169/23838 completed (loss: 1.057729721069336, acc: 0.6756756901741028)
[2025-02-16 11:06:14,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:15,094][root][INFO] - Training Epoch: 1/2, step 1170/23838 completed (loss: 0.9349187016487122, acc: 0.7272727489471436)
[2025-02-16 11:06:15,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:16,088][root][INFO] - Training Epoch: 1/2, step 1171/23838 completed (loss: 1.360419750213623, acc: 0.7045454382896423)
[2025-02-16 11:06:16,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:16,573][root][INFO] - Training Epoch: 1/2, step 1172/23838 completed (loss: 1.0780954360961914, acc: 0.6176470518112183)
[2025-02-16 11:06:17,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:17,604][root][INFO] - Training Epoch: 1/2, step 1173/23838 completed (loss: 1.457085132598877, acc: 0.6287878751754761)
[2025-02-16 11:06:17,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:17,991][root][INFO] - Training Epoch: 1/2, step 1174/23838 completed (loss: 0.8892669677734375, acc: 0.7906976938247681)
[2025-02-16 11:06:18,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:18,534][root][INFO] - Training Epoch: 1/2, step 1175/23838 completed (loss: 1.5487728118896484, acc: 0.6095238327980042)
[2025-02-16 11:06:18,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:18,961][root][INFO] - Training Epoch: 1/2, step 1176/23838 completed (loss: 1.1179178953170776, acc: 0.6898733973503113)
[2025-02-16 11:06:19,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:19,420][root][INFO] - Training Epoch: 1/2, step 1177/23838 completed (loss: 0.824221670627594, acc: 0.7674418687820435)
[2025-02-16 11:06:19,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:19,970][root][INFO] - Training Epoch: 1/2, step 1178/23838 completed (loss: 0.969149649143219, acc: 0.7523809671401978)
[2025-02-16 11:06:20,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:20,418][root][INFO] - Training Epoch: 1/2, step 1179/23838 completed (loss: 0.9783792495727539, acc: 0.7571428418159485)
[2025-02-16 11:06:20,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:20,825][root][INFO] - Training Epoch: 1/2, step 1180/23838 completed (loss: 0.7528093457221985, acc: 0.8181818127632141)
[2025-02-16 11:06:21,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:21,611][root][INFO] - Training Epoch: 1/2, step 1181/23838 completed (loss: 1.0222313404083252, acc: 0.7692307829856873)
[2025-02-16 11:06:21,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:22,380][root][INFO] - Training Epoch: 1/2, step 1182/23838 completed (loss: 0.9654305577278137, acc: 0.7260273694992065)
[2025-02-16 11:06:22,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:22,861][root][INFO] - Training Epoch: 1/2, step 1183/23838 completed (loss: 0.6778474450111389, acc: 0.8285714387893677)
[2025-02-16 11:06:23,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:23,276][root][INFO] - Training Epoch: 1/2, step 1184/23838 completed (loss: 1.1567758321762085, acc: 0.6880733966827393)
[2025-02-16 11:06:23,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:23,748][root][INFO] - Training Epoch: 1/2, step 1185/23838 completed (loss: 1.2182127237319946, acc: 0.6752136945724487)
[2025-02-16 11:06:23,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:24,248][root][INFO] - Training Epoch: 1/2, step 1186/23838 completed (loss: 0.9736636281013489, acc: 0.75)
[2025-02-16 11:06:24,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:24,693][root][INFO] - Training Epoch: 1/2, step 1187/23838 completed (loss: 0.9259443283081055, acc: 0.7142857313156128)
[2025-02-16 11:06:24,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:25,267][root][INFO] - Training Epoch: 1/2, step 1188/23838 completed (loss: 1.4050660133361816, acc: 0.5428571701049805)
[2025-02-16 11:06:25,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:25,756][root][INFO] - Training Epoch: 1/2, step 1189/23838 completed (loss: 0.8520625829696655, acc: 0.7402597665786743)
[2025-02-16 11:06:26,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:26,722][root][INFO] - Training Epoch: 1/2, step 1190/23838 completed (loss: 0.8848708868026733, acc: 0.7311828136444092)
[2025-02-16 11:06:26,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:27,163][root][INFO] - Training Epoch: 1/2, step 1191/23838 completed (loss: 1.3575513362884521, acc: 0.7142857313156128)
[2025-02-16 11:06:27,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:27,711][root][INFO] - Training Epoch: 1/2, step 1192/23838 completed (loss: 1.0105116367340088, acc: 0.6829268336296082)
[2025-02-16 11:06:28,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:28,704][root][INFO] - Training Epoch: 1/2, step 1193/23838 completed (loss: 1.4502915143966675, acc: 0.6060606241226196)
[2025-02-16 11:06:28,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:29,215][root][INFO] - Training Epoch: 1/2, step 1194/23838 completed (loss: 1.127878189086914, acc: 0.6818181872367859)
[2025-02-16 11:06:29,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:29,685][root][INFO] - Training Epoch: 1/2, step 1195/23838 completed (loss: 0.9033783674240112, acc: 0.760869562625885)
[2025-02-16 11:06:29,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:30,277][root][INFO] - Training Epoch: 1/2, step 1196/23838 completed (loss: 0.8648452758789062, acc: 0.7575757503509521)
[2025-02-16 11:06:30,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:30,751][root][INFO] - Training Epoch: 1/2, step 1197/23838 completed (loss: 1.0825917720794678, acc: 0.7307692170143127)
[2025-02-16 11:06:30,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:31,169][root][INFO] - Training Epoch: 1/2, step 1198/23838 completed (loss: 1.2573291063308716, acc: 0.6944444179534912)
[2025-02-16 11:06:31,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:31,543][root][INFO] - Training Epoch: 1/2, step 1199/23838 completed (loss: 1.0690330266952515, acc: 0.7413793206214905)
[2025-02-16 11:06:31,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:32,118][root][INFO] - Training Epoch: 1/2, step 1200/23838 completed (loss: 1.3313947916030884, acc: 0.6399999856948853)
[2025-02-16 11:06:32,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:32,842][root][INFO] - Training Epoch: 1/2, step 1201/23838 completed (loss: 1.271826982498169, acc: 0.6875)
[2025-02-16 11:06:33,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:33,496][root][INFO] - Training Epoch: 1/2, step 1202/23838 completed (loss: 1.1326537132263184, acc: 0.7200000286102295)
[2025-02-16 11:06:33,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:34,076][root][INFO] - Training Epoch: 1/2, step 1203/23838 completed (loss: 0.8952420353889465, acc: 0.7216494679450989)
[2025-02-16 11:06:34,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:34,534][root][INFO] - Training Epoch: 1/2, step 1204/23838 completed (loss: 1.4463719129562378, acc: 0.5975610017776489)
[2025-02-16 11:06:34,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:35,012][root][INFO] - Training Epoch: 1/2, step 1205/23838 completed (loss: 0.8279426693916321, acc: 0.761904776096344)
[2025-02-16 11:06:35,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:35,486][root][INFO] - Training Epoch: 1/2, step 1206/23838 completed (loss: 1.1116989850997925, acc: 0.6916666626930237)
[2025-02-16 11:06:35,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:35,990][root][INFO] - Training Epoch: 1/2, step 1207/23838 completed (loss: 0.9091091752052307, acc: 0.6904761791229248)
[2025-02-16 11:06:36,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:36,405][root][INFO] - Training Epoch: 1/2, step 1208/23838 completed (loss: 0.8601829409599304, acc: 0.7674418687820435)
[2025-02-16 11:06:36,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:36,886][root][INFO] - Training Epoch: 1/2, step 1209/23838 completed (loss: 1.672461748123169, acc: 0.6052631735801697)
[2025-02-16 11:06:37,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:37,320][root][INFO] - Training Epoch: 1/2, step 1210/23838 completed (loss: 0.9404615163803101, acc: 0.7234042286872864)
[2025-02-16 11:06:37,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:37,789][root][INFO] - Training Epoch: 1/2, step 1211/23838 completed (loss: 0.7493430972099304, acc: 0.8299999833106995)
[2025-02-16 11:06:37,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:38,220][root][INFO] - Training Epoch: 1/2, step 1212/23838 completed (loss: 0.8487109541893005, acc: 0.7614678740501404)
[2025-02-16 11:06:38,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:38,753][root][INFO] - Training Epoch: 1/2, step 1213/23838 completed (loss: 1.266179084777832, acc: 0.6847826242446899)
[2025-02-16 11:06:39,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:39,290][root][INFO] - Training Epoch: 1/2, step 1214/23838 completed (loss: 1.1474953889846802, acc: 0.7398374080657959)
[2025-02-16 11:06:39,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:39,731][root][INFO] - Training Epoch: 1/2, step 1215/23838 completed (loss: 1.1366111040115356, acc: 0.7272727489471436)
[2025-02-16 11:06:40,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:40,389][root][INFO] - Training Epoch: 1/2, step 1216/23838 completed (loss: 0.8951463103294373, acc: 0.7426470518112183)
[2025-02-16 11:06:40,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:40,834][root][INFO] - Training Epoch: 1/2, step 1217/23838 completed (loss: 1.0725910663604736, acc: 0.7346938848495483)
[2025-02-16 11:06:41,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:41,309][root][INFO] - Training Epoch: 1/2, step 1218/23838 completed (loss: 0.7845072746276855, acc: 0.7647058963775635)
[2025-02-16 11:06:41,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:41,793][root][INFO] - Training Epoch: 1/2, step 1219/23838 completed (loss: 1.202511191368103, acc: 0.7323943376541138)
[2025-02-16 11:06:41,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:42,190][root][INFO] - Training Epoch: 1/2, step 1220/23838 completed (loss: 1.0864253044128418, acc: 0.7123287916183472)
[2025-02-16 11:06:42,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:42,598][root][INFO] - Training Epoch: 1/2, step 1221/23838 completed (loss: 0.8323268294334412, acc: 0.84375)
[2025-02-16 11:06:42,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:42,980][root][INFO] - Training Epoch: 1/2, step 1222/23838 completed (loss: 1.0719571113586426, acc: 0.6451612710952759)
[2025-02-16 11:06:43,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:43,404][root][INFO] - Training Epoch: 1/2, step 1223/23838 completed (loss: 0.8878931403160095, acc: 0.7543859481811523)
[2025-02-16 11:06:43,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:44,074][root][INFO] - Training Epoch: 1/2, step 1224/23838 completed (loss: 0.8126980066299438, acc: 0.7904762029647827)
[2025-02-16 11:06:44,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:44,538][root][INFO] - Training Epoch: 1/2, step 1225/23838 completed (loss: 0.8357749581336975, acc: 0.7916666865348816)
[2025-02-16 11:06:44,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:44,901][root][INFO] - Training Epoch: 1/2, step 1226/23838 completed (loss: 1.0205800533294678, acc: 0.7037037014961243)
[2025-02-16 11:06:45,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:45,331][root][INFO] - Training Epoch: 1/2, step 1227/23838 completed (loss: 0.9783429503440857, acc: 0.8387096524238586)
[2025-02-16 11:06:45,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:45,720][root][INFO] - Training Epoch: 1/2, step 1228/23838 completed (loss: 0.7548097372055054, acc: 0.7924528121948242)
[2025-02-16 11:06:45,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:46,184][root][INFO] - Training Epoch: 1/2, step 1229/23838 completed (loss: 0.9230785369873047, acc: 0.7777777910232544)
[2025-02-16 11:06:46,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:46,640][root][INFO] - Training Epoch: 1/2, step 1230/23838 completed (loss: 0.4815097451210022, acc: 0.8833333253860474)
[2025-02-16 11:06:46,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:47,085][root][INFO] - Training Epoch: 1/2, step 1231/23838 completed (loss: 1.5035558938980103, acc: 0.6341463327407837)
[2025-02-16 11:06:47,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:47,542][root][INFO] - Training Epoch: 1/2, step 1232/23838 completed (loss: 0.7825261354446411, acc: 0.747474730014801)
[2025-02-16 11:06:47,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:47,923][root][INFO] - Training Epoch: 1/2, step 1233/23838 completed (loss: 0.3879322111606598, acc: 0.8648648858070374)
[2025-02-16 11:06:48,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:48,350][root][INFO] - Training Epoch: 1/2, step 1234/23838 completed (loss: 0.8279613256454468, acc: 0.8115941882133484)
[2025-02-16 11:06:48,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:48,741][root][INFO] - Training Epoch: 1/2, step 1235/23838 completed (loss: 0.9527095556259155, acc: 0.7840909361839294)
[2025-02-16 11:06:48,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:49,146][root][INFO] - Training Epoch: 1/2, step 1236/23838 completed (loss: 1.1414228677749634, acc: 0.6734693646430969)
[2025-02-16 11:06:49,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:49,625][root][INFO] - Training Epoch: 1/2, step 1237/23838 completed (loss: 0.5960356593132019, acc: 0.835616409778595)
[2025-02-16 11:06:50,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:50,451][root][INFO] - Training Epoch: 1/2, step 1238/23838 completed (loss: 0.9598395824432373, acc: 0.7454545497894287)
[2025-02-16 11:06:50,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:50,870][root][INFO] - Training Epoch: 1/2, step 1239/23838 completed (loss: 0.8193538784980774, acc: 0.804347813129425)
[2025-02-16 11:06:51,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:51,434][root][INFO] - Training Epoch: 1/2, step 1240/23838 completed (loss: 0.921867311000824, acc: 0.7699999809265137)
[2025-02-16 11:06:51,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:51,817][root][INFO] - Training Epoch: 1/2, step 1241/23838 completed (loss: 0.7099225521087646, acc: 0.7692307829856873)
[2025-02-16 11:06:51,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:52,195][root][INFO] - Training Epoch: 1/2, step 1242/23838 completed (loss: 3.4552664756774902, acc: 0.4117647111415863)
[2025-02-16 11:06:52,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:52,661][root][INFO] - Training Epoch: 1/2, step 1243/23838 completed (loss: 1.2824398279190063, acc: 0.6585366129875183)
[2025-02-16 11:06:52,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:53,105][root][INFO] - Training Epoch: 1/2, step 1244/23838 completed (loss: 1.385197401046753, acc: 0.7058823704719543)
[2025-02-16 11:06:53,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:53,526][root][INFO] - Training Epoch: 1/2, step 1245/23838 completed (loss: 1.747929334640503, acc: 0.4444444477558136)
[2025-02-16 11:06:53,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:53,914][root][INFO] - Training Epoch: 1/2, step 1246/23838 completed (loss: 1.6563442945480347, acc: 0.5)
[2025-02-16 11:06:54,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:54,336][root][INFO] - Training Epoch: 1/2, step 1247/23838 completed (loss: 1.4602493047714233, acc: 0.5)
[2025-02-16 11:06:54,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:54,728][root][INFO] - Training Epoch: 1/2, step 1248/23838 completed (loss: 1.2588722705841064, acc: 0.5)
[2025-02-16 11:06:54,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:55,116][root][INFO] - Training Epoch: 1/2, step 1249/23838 completed (loss: 1.032823920249939, acc: 0.6153846383094788)
[2025-02-16 11:06:55,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:55,549][root][INFO] - Training Epoch: 1/2, step 1250/23838 completed (loss: 1.1858572959899902, acc: 0.6538461446762085)
[2025-02-16 11:06:55,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:55,994][root][INFO] - Training Epoch: 1/2, step 1251/23838 completed (loss: 1.0415171384811401, acc: 0.7575757503509521)
[2025-02-16 11:06:56,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:56,469][root][INFO] - Training Epoch: 1/2, step 1252/23838 completed (loss: 2.4890432357788086, acc: 0.34193548560142517)
[2025-02-16 11:06:56,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:56,935][root][INFO] - Training Epoch: 1/2, step 1253/23838 completed (loss: 1.7616037130355835, acc: 0.4722222089767456)
[2025-02-16 11:06:57,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:57,424][root][INFO] - Training Epoch: 1/2, step 1254/23838 completed (loss: 1.79048752784729, acc: 0.49367088079452515)
[2025-02-16 11:06:57,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:57,858][root][INFO] - Training Epoch: 1/2, step 1255/23838 completed (loss: 1.887696385383606, acc: 0.4838709533214569)
[2025-02-16 11:06:58,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:58,243][root][INFO] - Training Epoch: 1/2, step 1256/23838 completed (loss: 1.9183224439620972, acc: 0.4615384638309479)
[2025-02-16 11:06:58,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:58,643][root][INFO] - Training Epoch: 1/2, step 1257/23838 completed (loss: 1.4300537109375, acc: 0.5670102834701538)
[2025-02-16 11:06:58,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:59,054][root][INFO] - Training Epoch: 1/2, step 1258/23838 completed (loss: 1.6186201572418213, acc: 0.5654761791229248)
[2025-02-16 11:06:59,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:59,453][root][INFO] - Training Epoch: 1/2, step 1259/23838 completed (loss: 1.5763548612594604, acc: 0.5144927501678467)
[2025-02-16 11:06:59,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:06:59,927][root][INFO] - Training Epoch: 1/2, step 1260/23838 completed (loss: 1.7496001720428467, acc: 0.54347825050354)
[2025-02-16 11:07:00,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:00,355][root][INFO] - Training Epoch: 1/2, step 1261/23838 completed (loss: 1.7380809783935547, acc: 0.516853928565979)
[2025-02-16 11:07:00,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:00,798][root][INFO] - Training Epoch: 1/2, step 1262/23838 completed (loss: 1.3132058382034302, acc: 0.6074766516685486)
[2025-02-16 11:07:01,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:01,276][root][INFO] - Training Epoch: 1/2, step 1263/23838 completed (loss: 1.4902684688568115, acc: 0.6379310488700867)
[2025-02-16 11:07:01,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:01,673][root][INFO] - Training Epoch: 1/2, step 1264/23838 completed (loss: 1.515167474746704, acc: 0.6153846383094788)
[2025-02-16 11:07:01,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:02,129][root][INFO] - Training Epoch: 1/2, step 1265/23838 completed (loss: 1.7250089645385742, acc: 0.5754716992378235)
[2025-02-16 11:07:02,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:02,505][root][INFO] - Training Epoch: 1/2, step 1266/23838 completed (loss: 1.5498881340026855, acc: 0.5413534045219421)
[2025-02-16 11:07:02,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:02,938][root][INFO] - Training Epoch: 1/2, step 1267/23838 completed (loss: 1.1304782629013062, acc: 0.698924720287323)
[2025-02-16 11:07:03,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:03,417][root][INFO] - Training Epoch: 1/2, step 1268/23838 completed (loss: 1.2209711074829102, acc: 0.6857143044471741)
[2025-02-16 11:07:03,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:03,913][root][INFO] - Training Epoch: 1/2, step 1269/23838 completed (loss: 1.686402440071106, acc: 0.49438202381134033)
[2025-02-16 11:07:04,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:04,356][root][INFO] - Training Epoch: 1/2, step 1270/23838 completed (loss: 1.2716717720031738, acc: 0.6578947305679321)
[2025-02-16 11:07:04,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:04,814][root][INFO] - Training Epoch: 1/2, step 1271/23838 completed (loss: 1.6506614685058594, acc: 0.5344827771186829)
[2025-02-16 11:07:05,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:05,256][root][INFO] - Training Epoch: 1/2, step 1272/23838 completed (loss: 1.5442932844161987, acc: 0.5714285969734192)
[2025-02-16 11:07:05,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:05,701][root][INFO] - Training Epoch: 1/2, step 1273/23838 completed (loss: 1.4327386617660522, acc: 0.653333306312561)
[2025-02-16 11:07:05,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:06,163][root][INFO] - Training Epoch: 1/2, step 1274/23838 completed (loss: 1.178041696548462, acc: 0.7021276354789734)
[2025-02-16 11:07:06,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:06,594][root][INFO] - Training Epoch: 1/2, step 1275/23838 completed (loss: 1.1923043727874756, acc: 0.6792452931404114)
[2025-02-16 11:07:06,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:07,035][root][INFO] - Training Epoch: 1/2, step 1276/23838 completed (loss: 0.9908429980278015, acc: 0.7241379022598267)
[2025-02-16 11:07:07,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:07,480][root][INFO] - Training Epoch: 1/2, step 1277/23838 completed (loss: 1.6399363279342651, acc: 0.511904776096344)
[2025-02-16 11:07:07,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:07,884][root][INFO] - Training Epoch: 1/2, step 1278/23838 completed (loss: 1.1906182765960693, acc: 0.7108433842658997)
[2025-02-16 11:07:08,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:08,259][root][INFO] - Training Epoch: 1/2, step 1279/23838 completed (loss: 0.9918452501296997, acc: 0.6904761791229248)
[2025-02-16 11:07:08,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:08,616][root][INFO] - Training Epoch: 1/2, step 1280/23838 completed (loss: 1.4266222715377808, acc: 0.6181818246841431)
[2025-02-16 11:07:08,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:09,048][root][INFO] - Training Epoch: 1/2, step 1281/23838 completed (loss: 1.417000412940979, acc: 0.6025640964508057)
[2025-02-16 11:07:09,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:09,462][root][INFO] - Training Epoch: 1/2, step 1282/23838 completed (loss: 1.644629716873169, acc: 0.5)
[2025-02-16 11:07:09,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:09,940][root][INFO] - Training Epoch: 1/2, step 1283/23838 completed (loss: 1.4566442966461182, acc: 0.5942028760910034)
[2025-02-16 11:07:10,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:10,435][root][INFO] - Training Epoch: 1/2, step 1284/23838 completed (loss: 1.1141129732131958, acc: 0.6693548560142517)
[2025-02-16 11:07:10,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:10,887][root][INFO] - Training Epoch: 1/2, step 1285/23838 completed (loss: 1.6367113590240479, acc: 0.5049505233764648)
[2025-02-16 11:07:11,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:11,368][root][INFO] - Training Epoch: 1/2, step 1286/23838 completed (loss: 1.6009336709976196, acc: 0.5541401505470276)
[2025-02-16 11:07:11,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:11,805][root][INFO] - Training Epoch: 1/2, step 1287/23838 completed (loss: 1.366505742073059, acc: 0.6529680490493774)
[2025-02-16 11:07:12,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:12,247][root][INFO] - Training Epoch: 1/2, step 1288/23838 completed (loss: 1.1817303895950317, acc: 0.6960784196853638)
[2025-02-16 11:07:12,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:12,638][root][INFO] - Training Epoch: 1/2, step 1289/23838 completed (loss: 1.411655306816101, acc: 0.5978260636329651)
[2025-02-16 11:07:12,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:13,143][root][INFO] - Training Epoch: 1/2, step 1290/23838 completed (loss: 1.4493699073791504, acc: 0.5752212405204773)
[2025-02-16 11:07:13,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:13,596][root][INFO] - Training Epoch: 1/2, step 1291/23838 completed (loss: 1.269863486289978, acc: 0.717391312122345)
[2025-02-16 11:07:13,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:14,071][root][INFO] - Training Epoch: 1/2, step 1292/23838 completed (loss: 1.0071018934249878, acc: 0.7007299065589905)
[2025-02-16 11:07:14,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:14,466][root][INFO] - Training Epoch: 1/2, step 1293/23838 completed (loss: 2.2520415782928467, acc: 0.38181817531585693)
[2025-02-16 11:07:14,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:14,902][root][INFO] - Training Epoch: 1/2, step 1294/23838 completed (loss: 1.6388267278671265, acc: 0.5909090638160706)
[2025-02-16 11:07:15,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:15,291][root][INFO] - Training Epoch: 1/2, step 1295/23838 completed (loss: 1.1538468599319458, acc: 0.6666666865348816)
[2025-02-16 11:07:15,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:15,724][root][INFO] - Training Epoch: 1/2, step 1296/23838 completed (loss: 1.0285578966140747, acc: 0.7407407164573669)
[2025-02-16 11:07:15,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:16,144][root][INFO] - Training Epoch: 1/2, step 1297/23838 completed (loss: 1.538747787475586, acc: 0.5409836173057556)
[2025-02-16 11:07:16,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:16,548][root][INFO] - Training Epoch: 1/2, step 1298/23838 completed (loss: 1.170910120010376, acc: 0.6266666650772095)
[2025-02-16 11:07:16,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:16,949][root][INFO] - Training Epoch: 1/2, step 1299/23838 completed (loss: 1.4143575429916382, acc: 0.6086956262588501)
[2025-02-16 11:07:17,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:17,481][root][INFO] - Training Epoch: 1/2, step 1300/23838 completed (loss: 1.2618365287780762, acc: 0.7022900581359863)
[2025-02-16 11:07:17,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:17,916][root][INFO] - Training Epoch: 1/2, step 1301/23838 completed (loss: 1.2329328060150146, acc: 0.64673912525177)
[2025-02-16 11:07:18,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:18,407][root][INFO] - Training Epoch: 1/2, step 1302/23838 completed (loss: 1.3286149501800537, acc: 0.6580645442008972)
[2025-02-16 11:07:18,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:18,897][root][INFO] - Training Epoch: 1/2, step 1303/23838 completed (loss: 1.1511136293411255, acc: 0.6617646813392639)
[2025-02-16 11:07:19,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:19,435][root][INFO] - Training Epoch: 1/2, step 1304/23838 completed (loss: 1.4545303583145142, acc: 0.5854700803756714)
[2025-02-16 11:07:19,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:19,900][root][INFO] - Training Epoch: 1/2, step 1305/23838 completed (loss: 0.8218923211097717, acc: 0.7987421154975891)
[2025-02-16 11:07:20,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:20,296][root][INFO] - Training Epoch: 1/2, step 1306/23838 completed (loss: 1.167861819267273, acc: 0.6936936974525452)
[2025-02-16 11:07:20,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:20,709][root][INFO] - Training Epoch: 1/2, step 1307/23838 completed (loss: 1.0216717720031738, acc: 0.7333333492279053)
[2025-02-16 11:07:20,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:21,108][root][INFO] - Training Epoch: 1/2, step 1308/23838 completed (loss: 1.300389051437378, acc: 0.658823549747467)
[2025-02-16 11:07:21,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:21,503][root][INFO] - Training Epoch: 1/2, step 1309/23838 completed (loss: 1.3711998462677002, acc: 0.5669291615486145)
[2025-02-16 11:07:21,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:21,943][root][INFO] - Training Epoch: 1/2, step 1310/23838 completed (loss: 1.3589895963668823, acc: 0.6380952596664429)
[2025-02-16 11:07:22,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:22,374][root][INFO] - Training Epoch: 1/2, step 1311/23838 completed (loss: 1.1599321365356445, acc: 0.6962025165557861)
[2025-02-16 11:07:22,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:22,852][root][INFO] - Training Epoch: 1/2, step 1312/23838 completed (loss: 1.1612951755523682, acc: 0.6410256624221802)
[2025-02-16 11:07:23,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:23,333][root][INFO] - Training Epoch: 1/2, step 1313/23838 completed (loss: 1.1839362382888794, acc: 0.6666666865348816)
[2025-02-16 11:07:23,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:23,801][root][INFO] - Training Epoch: 1/2, step 1314/23838 completed (loss: 0.8714849948883057, acc: 0.8227847814559937)
[2025-02-16 11:07:24,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:24,241][root][INFO] - Training Epoch: 1/2, step 1315/23838 completed (loss: 0.919615626335144, acc: 0.762499988079071)
[2025-02-16 11:07:24,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:24,689][root][INFO] - Training Epoch: 1/2, step 1316/23838 completed (loss: 1.1822363138198853, acc: 0.7176470756530762)
[2025-02-16 11:07:24,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:25,117][root][INFO] - Training Epoch: 1/2, step 1317/23838 completed (loss: 1.082680583000183, acc: 0.7307692170143127)
[2025-02-16 11:07:25,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:25,567][root][INFO] - Training Epoch: 1/2, step 1318/23838 completed (loss: 1.036794900894165, acc: 0.7079645991325378)
[2025-02-16 11:07:25,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:25,966][root][INFO] - Training Epoch: 1/2, step 1319/23838 completed (loss: 0.884855329990387, acc: 0.7593985199928284)
[2025-02-16 11:07:26,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:26,351][root][INFO] - Training Epoch: 1/2, step 1320/23838 completed (loss: 0.7644633650779724, acc: 0.7714285850524902)
[2025-02-16 11:07:26,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:26,777][root][INFO] - Training Epoch: 1/2, step 1321/23838 completed (loss: 0.89284747838974, acc: 0.7564102411270142)
[2025-02-16 11:07:26,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:27,218][root][INFO] - Training Epoch: 1/2, step 1322/23838 completed (loss: 1.2075275182724, acc: 0.6666666865348816)
[2025-02-16 11:07:27,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:27,644][root][INFO] - Training Epoch: 1/2, step 1323/23838 completed (loss: 1.5370371341705322, acc: 0.6382978558540344)
[2025-02-16 11:07:27,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:28,117][root][INFO] - Training Epoch: 1/2, step 1324/23838 completed (loss: 1.2770888805389404, acc: 0.675000011920929)
[2025-02-16 11:07:28,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:28,497][root][INFO] - Training Epoch: 1/2, step 1325/23838 completed (loss: 1.016298532485962, acc: 0.7319587469100952)
[2025-02-16 11:07:28,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:28,892][root][INFO] - Training Epoch: 1/2, step 1326/23838 completed (loss: 1.2813223600387573, acc: 0.625)
[2025-02-16 11:07:29,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:29,331][root][INFO] - Training Epoch: 1/2, step 1327/23838 completed (loss: 0.8890557885169983, acc: 0.7088607549667358)
[2025-02-16 11:07:29,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:29,714][root][INFO] - Training Epoch: 1/2, step 1328/23838 completed (loss: 1.1468135118484497, acc: 0.6987951993942261)
[2025-02-16 11:07:29,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:30,143][root][INFO] - Training Epoch: 1/2, step 1329/23838 completed (loss: 1.7845728397369385, acc: 0.44736841320991516)
[2025-02-16 11:07:30,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:30,549][root][INFO] - Training Epoch: 1/2, step 1330/23838 completed (loss: 1.4093046188354492, acc: 0.6582278609275818)
[2025-02-16 11:07:30,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:30,978][root][INFO] - Training Epoch: 1/2, step 1331/23838 completed (loss: 0.9404663443565369, acc: 0.7386363744735718)
[2025-02-16 11:07:31,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:31,372][root][INFO] - Training Epoch: 1/2, step 1332/23838 completed (loss: 1.1850640773773193, acc: 0.6781609058380127)
[2025-02-16 11:07:31,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:31,792][root][INFO] - Training Epoch: 1/2, step 1333/23838 completed (loss: 1.082440733909607, acc: 0.7254902124404907)
[2025-02-16 11:07:31,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:32,209][root][INFO] - Training Epoch: 1/2, step 1334/23838 completed (loss: 1.6546605825424194, acc: 0.4883720874786377)
[2025-02-16 11:07:32,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:32,621][root][INFO] - Training Epoch: 1/2, step 1335/23838 completed (loss: 0.6915934681892395, acc: 0.8188976645469666)
[2025-02-16 11:07:32,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:33,016][root][INFO] - Training Epoch: 1/2, step 1336/23838 completed (loss: 0.7184014916419983, acc: 0.807692289352417)
[2025-02-16 11:07:33,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:33,412][root][INFO] - Training Epoch: 1/2, step 1337/23838 completed (loss: 0.8266986608505249, acc: 0.7894737124443054)
[2025-02-16 11:07:33,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:33,892][root][INFO] - Training Epoch: 1/2, step 1338/23838 completed (loss: 1.4807502031326294, acc: 0.5625)
[2025-02-16 11:07:34,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:34,311][root][INFO] - Training Epoch: 1/2, step 1339/23838 completed (loss: 1.895443081855774, acc: 0.447761207818985)
[2025-02-16 11:07:34,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:34,738][root][INFO] - Training Epoch: 1/2, step 1340/23838 completed (loss: 1.7750028371810913, acc: 0.5263158082962036)
[2025-02-16 11:07:34,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:35,159][root][INFO] - Training Epoch: 1/2, step 1341/23838 completed (loss: 1.3700101375579834, acc: 0.5769230723381042)
[2025-02-16 11:07:35,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:35,569][root][INFO] - Training Epoch: 1/2, step 1342/23838 completed (loss: 1.0701552629470825, acc: 0.6481481194496155)
[2025-02-16 11:07:35,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:35,982][root][INFO] - Training Epoch: 1/2, step 1343/23838 completed (loss: 1.1847903728485107, acc: 0.7115384340286255)
[2025-02-16 11:07:36,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:36,363][root][INFO] - Training Epoch: 1/2, step 1344/23838 completed (loss: 1.301403284072876, acc: 0.699999988079071)
[2025-02-16 11:07:36,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:36,762][root][INFO] - Training Epoch: 1/2, step 1345/23838 completed (loss: 1.4528050422668457, acc: 0.6176470518112183)
[2025-02-16 11:07:36,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:37,205][root][INFO] - Training Epoch: 1/2, step 1346/23838 completed (loss: 1.0441040992736816, acc: 0.7093023061752319)
[2025-02-16 11:07:37,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:37,632][root][INFO] - Training Epoch: 1/2, step 1347/23838 completed (loss: 1.2270090579986572, acc: 0.7051281929016113)
[2025-02-16 11:07:37,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:38,097][root][INFO] - Training Epoch: 1/2, step 1348/23838 completed (loss: 1.4573092460632324, acc: 0.5925925970077515)
[2025-02-16 11:07:38,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:38,538][root][INFO] - Training Epoch: 1/2, step 1349/23838 completed (loss: 1.2389034032821655, acc: 0.671875)
[2025-02-16 11:07:38,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:38,958][root][INFO] - Training Epoch: 1/2, step 1350/23838 completed (loss: 1.4186573028564453, acc: 0.6142857074737549)
[2025-02-16 11:07:39,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:39,396][root][INFO] - Training Epoch: 1/2, step 1351/23838 completed (loss: 1.7009845972061157, acc: 0.540229856967926)
[2025-02-16 11:07:39,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:39,811][root][INFO] - Training Epoch: 1/2, step 1352/23838 completed (loss: 1.9006706476211548, acc: 0.5270270109176636)
[2025-02-16 11:07:40,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:40,243][root][INFO] - Training Epoch: 1/2, step 1353/23838 completed (loss: 1.0763193368911743, acc: 0.6964285969734192)
[2025-02-16 11:07:40,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:40,691][root][INFO] - Training Epoch: 1/2, step 1354/23838 completed (loss: 1.134381651878357, acc: 0.6870229244232178)
[2025-02-16 11:07:40,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:41,111][root][INFO] - Training Epoch: 1/2, step 1355/23838 completed (loss: 1.272681713104248, acc: 0.6477272510528564)
[2025-02-16 11:07:41,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:41,564][root][INFO] - Training Epoch: 1/2, step 1356/23838 completed (loss: 1.1320041418075562, acc: 0.7066666483879089)
[2025-02-16 11:07:41,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:41,967][root][INFO] - Training Epoch: 1/2, step 1357/23838 completed (loss: 1.6230844259262085, acc: 0.5400000214576721)
[2025-02-16 11:07:42,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:42,360][root][INFO] - Training Epoch: 1/2, step 1358/23838 completed (loss: 0.9491198062896729, acc: 0.7534246444702148)
[2025-02-16 11:07:42,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:42,788][root][INFO] - Training Epoch: 1/2, step 1359/23838 completed (loss: 1.0113812685012817, acc: 0.7285714149475098)
[2025-02-16 11:07:42,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:43,164][root][INFO] - Training Epoch: 1/2, step 1360/23838 completed (loss: 1.3777986764907837, acc: 0.591549277305603)
[2025-02-16 11:07:43,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:43,573][root][INFO] - Training Epoch: 1/2, step 1361/23838 completed (loss: 1.1228140592575073, acc: 0.6727272868156433)
[2025-02-16 11:07:43,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:43,982][root][INFO] - Training Epoch: 1/2, step 1362/23838 completed (loss: 0.9614064693450928, acc: 0.8070175647735596)
[2025-02-16 11:07:44,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:44,374][root][INFO] - Training Epoch: 1/2, step 1363/23838 completed (loss: 0.7993509769439697, acc: 0.75)
[2025-02-16 11:07:44,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:44,871][root][INFO] - Training Epoch: 1/2, step 1364/23838 completed (loss: 1.1969307661056519, acc: 0.6666666865348816)
[2025-02-16 11:07:45,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:45,347][root][INFO] - Training Epoch: 1/2, step 1365/23838 completed (loss: 0.9961611032485962, acc: 0.7154471278190613)
[2025-02-16 11:07:45,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:45,746][root][INFO] - Training Epoch: 1/2, step 1366/23838 completed (loss: 0.6377570629119873, acc: 0.8489208817481995)
[2025-02-16 11:07:45,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:46,158][root][INFO] - Training Epoch: 1/2, step 1367/23838 completed (loss: 0.9341989755630493, acc: 0.756302535533905)
[2025-02-16 11:07:46,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:46,613][root][INFO] - Training Epoch: 1/2, step 1368/23838 completed (loss: 0.7312962412834167, acc: 0.7752808928489685)
[2025-02-16 11:07:46,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:47,046][root][INFO] - Training Epoch: 1/2, step 1369/23838 completed (loss: 1.1272298097610474, acc: 0.6499999761581421)
[2025-02-16 11:07:47,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:47,536][root][INFO] - Training Epoch: 1/2, step 1370/23838 completed (loss: 1.0287034511566162, acc: 0.7244898080825806)
[2025-02-16 11:07:47,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:47,954][root][INFO] - Training Epoch: 1/2, step 1371/23838 completed (loss: 1.3233476877212524, acc: 0.6233766078948975)
[2025-02-16 11:07:48,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:48,402][root][INFO] - Training Epoch: 1/2, step 1372/23838 completed (loss: 0.7907969355583191, acc: 0.7943925261497498)
[2025-02-16 11:07:48,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:48,823][root][INFO] - Training Epoch: 1/2, step 1373/23838 completed (loss: 0.5513636469841003, acc: 0.8260869383811951)
[2025-02-16 11:07:49,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:49,295][root][INFO] - Training Epoch: 1/2, step 1374/23838 completed (loss: 0.8277742862701416, acc: 0.761904776096344)
[2025-02-16 11:07:49,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:49,693][root][INFO] - Training Epoch: 1/2, step 1375/23838 completed (loss: 1.0536314249038696, acc: 0.7164179086685181)
[2025-02-16 11:07:49,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:50,070][root][INFO] - Training Epoch: 1/2, step 1376/23838 completed (loss: 0.9653710126876831, acc: 0.7333333492279053)
[2025-02-16 11:07:50,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:50,490][root][INFO] - Training Epoch: 1/2, step 1377/23838 completed (loss: 0.7299903631210327, acc: 0.800000011920929)
[2025-02-16 11:07:50,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:50,875][root][INFO] - Training Epoch: 1/2, step 1378/23838 completed (loss: 1.3170276880264282, acc: 0.6071428656578064)
[2025-02-16 11:07:51,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:51,309][root][INFO] - Training Epoch: 1/2, step 1379/23838 completed (loss: 1.1399297714233398, acc: 0.695652186870575)
[2025-02-16 11:07:51,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:51,793][root][INFO] - Training Epoch: 1/2, step 1380/23838 completed (loss: 0.8679337501525879, acc: 0.7142857313156128)
[2025-02-16 11:07:51,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:52,189][root][INFO] - Training Epoch: 1/2, step 1381/23838 completed (loss: 0.8624461889266968, acc: 0.7603305578231812)
[2025-02-16 11:07:52,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:52,577][root][INFO] - Training Epoch: 1/2, step 1382/23838 completed (loss: 1.0250862836837769, acc: 0.7214285731315613)
[2025-02-16 11:07:52,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:52,907][root][INFO] - Training Epoch: 1/2, step 1383/23838 completed (loss: 0.946439802646637, acc: 0.699999988079071)
[2025-02-16 11:07:53,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:53,310][root][INFO] - Training Epoch: 1/2, step 1384/23838 completed (loss: 0.5589467287063599, acc: 0.8780487775802612)
[2025-02-16 11:07:53,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:53,768][root][INFO] - Training Epoch: 1/2, step 1385/23838 completed (loss: 1.518906831741333, acc: 0.5593220591545105)
[2025-02-16 11:07:53,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:54,168][root][INFO] - Training Epoch: 1/2, step 1386/23838 completed (loss: 1.3800263404846191, acc: 0.625)
[2025-02-16 11:07:54,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:54,565][root][INFO] - Training Epoch: 1/2, step 1387/23838 completed (loss: 1.7602615356445312, acc: 0.550000011920929)
[2025-02-16 11:07:54,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:54,947][root][INFO] - Training Epoch: 1/2, step 1388/23838 completed (loss: 0.7784997224807739, acc: 0.7414966225624084)
[2025-02-16 11:07:55,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:55,381][root][INFO] - Training Epoch: 1/2, step 1389/23838 completed (loss: 0.5740076899528503, acc: 0.8103448152542114)
[2025-02-16 11:07:55,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:55,879][root][INFO] - Training Epoch: 1/2, step 1390/23838 completed (loss: 1.088079571723938, acc: 0.671875)
[2025-02-16 11:07:56,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:56,298][root][INFO] - Training Epoch: 1/2, step 1391/23838 completed (loss: 1.0024064779281616, acc: 0.6781609058380127)
[2025-02-16 11:07:56,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:56,785][root][INFO] - Training Epoch: 1/2, step 1392/23838 completed (loss: 0.7083767652511597, acc: 0.8144329786300659)
[2025-02-16 11:07:56,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:57,218][root][INFO] - Training Epoch: 1/2, step 1393/23838 completed (loss: 0.7039644122123718, acc: 0.8152173757553101)
[2025-02-16 11:07:57,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:57,721][root][INFO] - Training Epoch: 1/2, step 1394/23838 completed (loss: 0.7711380124092102, acc: 0.7948718070983887)
[2025-02-16 11:07:57,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:58,181][root][INFO] - Training Epoch: 1/2, step 1395/23838 completed (loss: 0.8615906834602356, acc: 0.77173912525177)
[2025-02-16 11:07:58,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:58,629][root][INFO] - Training Epoch: 1/2, step 1396/23838 completed (loss: 0.9505773782730103, acc: 0.699999988079071)
[2025-02-16 11:07:58,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:59,039][root][INFO] - Training Epoch: 1/2, step 1397/23838 completed (loss: 0.7030261158943176, acc: 0.8372092843055725)
[2025-02-16 11:07:59,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:59,401][root][INFO] - Training Epoch: 1/2, step 1398/23838 completed (loss: 0.7690467238426208, acc: 0.7954545617103577)
[2025-02-16 11:07:59,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:07:59,837][root][INFO] - Training Epoch: 1/2, step 1399/23838 completed (loss: 1.1370600461959839, acc: 0.7160493731498718)
[2025-02-16 11:08:00,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:00,309][root][INFO] - Training Epoch: 1/2, step 1400/23838 completed (loss: 0.516933798789978, acc: 0.8315789699554443)
[2025-02-16 11:08:00,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:00,764][root][INFO] - Training Epoch: 1/2, step 1401/23838 completed (loss: 1.0911506414413452, acc: 0.6842105388641357)
[2025-02-16 11:08:01,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:01,339][root][INFO] - Training Epoch: 1/2, step 1402/23838 completed (loss: 0.8427038788795471, acc: 0.7362637519836426)
[2025-02-16 11:08:01,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:01,728][root][INFO] - Training Epoch: 1/2, step 1403/23838 completed (loss: 0.9464402198791504, acc: 0.7166666388511658)
[2025-02-16 11:08:01,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:02,257][root][INFO] - Training Epoch: 1/2, step 1404/23838 completed (loss: 0.7323160767555237, acc: 0.8086419701576233)
[2025-02-16 11:08:02,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:02,730][root][INFO] - Training Epoch: 1/2, step 1405/23838 completed (loss: 0.6913221478462219, acc: 0.7843137383460999)
[2025-02-16 11:08:02,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:03,230][root][INFO] - Training Epoch: 1/2, step 1406/23838 completed (loss: 1.0050193071365356, acc: 0.7661290168762207)
[2025-02-16 11:08:03,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:03,665][root][INFO] - Training Epoch: 1/2, step 1407/23838 completed (loss: 0.48973536491394043, acc: 0.8455284833908081)
[2025-02-16 11:08:03,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:04,256][root][INFO] - Training Epoch: 1/2, step 1408/23838 completed (loss: 0.5074496865272522, acc: 0.8947368264198303)
[2025-02-16 11:08:04,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:04,698][root][INFO] - Training Epoch: 1/2, step 1409/23838 completed (loss: 0.6198567152023315, acc: 0.824999988079071)
[2025-02-16 11:08:04,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:05,084][root][INFO] - Training Epoch: 1/2, step 1410/23838 completed (loss: 0.5473393797874451, acc: 0.8421052694320679)
[2025-02-16 11:08:05,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:05,449][root][INFO] - Training Epoch: 1/2, step 1411/23838 completed (loss: 0.6009055972099304, acc: 0.8260869383811951)
[2025-02-16 11:08:05,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:05,915][root][INFO] - Training Epoch: 1/2, step 1412/23838 completed (loss: 0.4132080674171448, acc: 0.9047619104385376)
[2025-02-16 11:08:06,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:06,355][root][INFO] - Training Epoch: 1/2, step 1413/23838 completed (loss: 0.476248174905777, acc: 0.8970588445663452)
[2025-02-16 11:08:06,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:06,819][root][INFO] - Training Epoch: 1/2, step 1414/23838 completed (loss: 0.6200467348098755, acc: 0.8474576473236084)
[2025-02-16 11:08:07,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:07,285][root][INFO] - Training Epoch: 1/2, step 1415/23838 completed (loss: 0.5954456329345703, acc: 0.8556700944900513)
[2025-02-16 11:08:07,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:07,743][root][INFO] - Training Epoch: 1/2, step 1416/23838 completed (loss: 1.131913423538208, acc: 0.6891891956329346)
[2025-02-16 11:08:07,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:08,231][root][INFO] - Training Epoch: 1/2, step 1417/23838 completed (loss: 0.4924692213535309, acc: 0.875)
[2025-02-16 11:08:08,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:08,709][root][INFO] - Training Epoch: 1/2, step 1418/23838 completed (loss: 0.6185314059257507, acc: 0.8631578683853149)
[2025-02-16 11:08:08,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:09,209][root][INFO] - Training Epoch: 1/2, step 1419/23838 completed (loss: 0.964919924736023, acc: 0.8014184236526489)
[2025-02-16 11:08:09,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:09,650][root][INFO] - Training Epoch: 1/2, step 1420/23838 completed (loss: 0.6367303729057312, acc: 0.8040540814399719)
[2025-02-16 11:08:09,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:10,031][root][INFO] - Training Epoch: 1/2, step 1421/23838 completed (loss: 0.39848339557647705, acc: 0.8857142925262451)
[2025-02-16 11:08:10,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:10,472][root][INFO] - Training Epoch: 1/2, step 1422/23838 completed (loss: 0.6126456260681152, acc: 0.824999988079071)
[2025-02-16 11:08:10,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:10,924][root][INFO] - Training Epoch: 1/2, step 1423/23838 completed (loss: 0.6489126086235046, acc: 0.8235294222831726)
[2025-02-16 11:08:11,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:11,361][root][INFO] - Training Epoch: 1/2, step 1424/23838 completed (loss: 0.2041347771883011, acc: 0.932584285736084)
[2025-02-16 11:08:11,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:11,831][root][INFO] - Training Epoch: 1/2, step 1425/23838 completed (loss: 0.4083896577358246, acc: 0.8598130941390991)
[2025-02-16 11:08:11,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:12,206][root][INFO] - Training Epoch: 1/2, step 1426/23838 completed (loss: 0.7335308194160461, acc: 0.8142856955528259)
[2025-02-16 11:08:12,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:12,629][root][INFO] - Training Epoch: 1/2, step 1427/23838 completed (loss: 0.9484593272209167, acc: 0.7708333134651184)
[2025-02-16 11:08:12,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:13,008][root][INFO] - Training Epoch: 1/2, step 1428/23838 completed (loss: 0.35044848918914795, acc: 0.9021739363670349)
[2025-02-16 11:08:13,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:13,377][root][INFO] - Training Epoch: 1/2, step 1429/23838 completed (loss: 0.8398727178573608, acc: 0.8064516186714172)
[2025-02-16 11:08:13,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:13,845][root][INFO] - Training Epoch: 1/2, step 1430/23838 completed (loss: 0.33121466636657715, acc: 0.9047619104385376)
[2025-02-16 11:08:14,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:14,262][root][INFO] - Training Epoch: 1/2, step 1431/23838 completed (loss: 0.6209652423858643, acc: 0.844660222530365)
[2025-02-16 11:08:14,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:14,670][root][INFO] - Training Epoch: 1/2, step 1432/23838 completed (loss: 0.2387857288122177, acc: 0.9387755393981934)
[2025-02-16 11:08:14,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:15,089][root][INFO] - Training Epoch: 1/2, step 1433/23838 completed (loss: 0.3881570100784302, acc: 0.9032257795333862)
[2025-02-16 11:08:15,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:15,501][root][INFO] - Training Epoch: 1/2, step 1434/23838 completed (loss: 0.6525990962982178, acc: 0.7916666865348816)
[2025-02-16 11:08:15,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:15,931][root][INFO] - Training Epoch: 1/2, step 1435/23838 completed (loss: 0.3937997817993164, acc: 0.9230769276618958)
[2025-02-16 11:08:16,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:16,336][root][INFO] - Training Epoch: 1/2, step 1436/23838 completed (loss: 0.22913895547389984, acc: 0.9354838728904724)
[2025-02-16 11:08:16,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:16,775][root][INFO] - Training Epoch: 1/2, step 1437/23838 completed (loss: 0.5793052911758423, acc: 0.8513513803482056)
[2025-02-16 11:08:17,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:17,615][root][INFO] - Training Epoch: 1/2, step 1438/23838 completed (loss: 0.5801839232444763, acc: 0.8373205661773682)
[2025-02-16 11:08:17,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:18,087][root][INFO] - Training Epoch: 1/2, step 1439/23838 completed (loss: 0.36875876784324646, acc: 0.8947368264198303)
[2025-02-16 11:08:18,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:18,548][root][INFO] - Training Epoch: 1/2, step 1440/23838 completed (loss: 0.545633852481842, acc: 0.8476821184158325)
[2025-02-16 11:08:18,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:18,953][root][INFO] - Training Epoch: 1/2, step 1441/23838 completed (loss: 0.5233164429664612, acc: 0.8805969953536987)
[2025-02-16 11:08:19,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:19,362][root][INFO] - Training Epoch: 1/2, step 1442/23838 completed (loss: 0.43808022141456604, acc: 0.8775510191917419)
[2025-02-16 11:08:19,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:19,797][root][INFO] - Training Epoch: 1/2, step 1443/23838 completed (loss: 0.23800364136695862, acc: 0.9484536051750183)
[2025-02-16 11:08:20,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:20,260][root][INFO] - Training Epoch: 1/2, step 1444/23838 completed (loss: 0.3970280885696411, acc: 0.8899999856948853)
[2025-02-16 11:08:20,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:20,720][root][INFO] - Training Epoch: 1/2, step 1445/23838 completed (loss: 0.26900938153266907, acc: 0.9370629191398621)
[2025-02-16 11:08:20,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:21,109][root][INFO] - Training Epoch: 1/2, step 1446/23838 completed (loss: 0.38225239515304565, acc: 0.8813559412956238)
[2025-02-16 11:08:21,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:21,556][root][INFO] - Training Epoch: 1/2, step 1447/23838 completed (loss: 0.34267401695251465, acc: 0.9090909361839294)
[2025-02-16 11:08:21,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:22,010][root][INFO] - Training Epoch: 1/2, step 1448/23838 completed (loss: 0.31627196073532104, acc: 0.9454545378684998)
[2025-02-16 11:08:22,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:22,426][root][INFO] - Training Epoch: 1/2, step 1449/23838 completed (loss: 0.8551190495491028, acc: 0.8676470518112183)
[2025-02-16 11:08:22,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:22,889][root][INFO] - Training Epoch: 1/2, step 1450/23838 completed (loss: 0.6182082295417786, acc: 0.8125)
[2025-02-16 11:08:23,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:23,299][root][INFO] - Training Epoch: 1/2, step 1451/23838 completed (loss: 0.4757052958011627, acc: 0.875)
[2025-02-16 11:08:23,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:23,718][root][INFO] - Training Epoch: 1/2, step 1452/23838 completed (loss: 1.0439859628677368, acc: 0.6734693646430969)
[2025-02-16 11:08:23,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:24,161][root][INFO] - Training Epoch: 1/2, step 1453/23838 completed (loss: 1.0306061506271362, acc: 0.75)
[2025-02-16 11:08:24,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:24,596][root][INFO] - Training Epoch: 1/2, step 1454/23838 completed (loss: 1.4293456077575684, acc: 0.5952380895614624)
[2025-02-16 11:08:24,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:25,055][root][INFO] - Training Epoch: 1/2, step 1455/23838 completed (loss: 0.8982589244842529, acc: 0.7719298005104065)
[2025-02-16 11:08:25,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:25,450][root][INFO] - Training Epoch: 1/2, step 1456/23838 completed (loss: 1.9951388835906982, acc: 0.5769230723381042)
[2025-02-16 11:08:25,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:25,889][root][INFO] - Training Epoch: 1/2, step 1457/23838 completed (loss: 1.221688151359558, acc: 0.6111111044883728)
[2025-02-16 11:08:26,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:26,326][root][INFO] - Training Epoch: 1/2, step 1458/23838 completed (loss: 0.9283419251441956, acc: 0.7727272510528564)
[2025-02-16 11:08:26,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:26,693][root][INFO] - Training Epoch: 1/2, step 1459/23838 completed (loss: 1.626605749130249, acc: 0.4615384638309479)
[2025-02-16 11:08:26,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:27,127][root][INFO] - Training Epoch: 1/2, step 1460/23838 completed (loss: 1.6075947284698486, acc: 0.6086956262588501)
[2025-02-16 11:08:27,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:27,521][root][INFO] - Training Epoch: 1/2, step 1461/23838 completed (loss: 1.3334050178527832, acc: 0.6451612710952759)
[2025-02-16 11:08:27,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:27,952][root][INFO] - Training Epoch: 1/2, step 1462/23838 completed (loss: 0.9294795393943787, acc: 0.800000011920929)
[2025-02-16 11:08:28,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:28,417][root][INFO] - Training Epoch: 1/2, step 1463/23838 completed (loss: 0.9986780881881714, acc: 0.7142857313156128)
[2025-02-16 11:08:28,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:28,905][root][INFO] - Training Epoch: 1/2, step 1464/23838 completed (loss: 1.4895122051239014, acc: 0.5882353186607361)
[2025-02-16 11:08:29,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:29,314][root][INFO] - Training Epoch: 1/2, step 1465/23838 completed (loss: 2.2471156120300293, acc: 0.4615384638309479)
[2025-02-16 11:08:29,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:29,720][root][INFO] - Training Epoch: 1/2, step 1466/23838 completed (loss: 1.1033509969711304, acc: 0.6875)
[2025-02-16 11:08:29,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:30,111][root][INFO] - Training Epoch: 1/2, step 1467/23838 completed (loss: 2.2259280681610107, acc: 0.4523809552192688)
[2025-02-16 11:08:30,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:30,526][root][INFO] - Training Epoch: 1/2, step 1468/23838 completed (loss: 1.4286061525344849, acc: 0.692307710647583)
[2025-02-16 11:08:30,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:30,974][root][INFO] - Training Epoch: 1/2, step 1469/23838 completed (loss: 1.3480244874954224, acc: 0.6578947305679321)
[2025-02-16 11:08:31,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:31,415][root][INFO] - Training Epoch: 1/2, step 1470/23838 completed (loss: 0.7100749015808105, acc: 0.824999988079071)
[2025-02-16 11:08:31,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:31,865][root][INFO] - Training Epoch: 1/2, step 1471/23838 completed (loss: 1.4797508716583252, acc: 0.617977499961853)
[2025-02-16 11:08:32,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:32,312][root][INFO] - Training Epoch: 1/2, step 1472/23838 completed (loss: 1.3356324434280396, acc: 0.6296296119689941)
[2025-02-16 11:08:32,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:32,759][root][INFO] - Training Epoch: 1/2, step 1473/23838 completed (loss: 0.7090916633605957, acc: 0.800000011920929)
[2025-02-16 11:08:32,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:33,166][root][INFO] - Training Epoch: 1/2, step 1474/23838 completed (loss: 0.4728420078754425, acc: 0.9230769276618958)
[2025-02-16 11:08:33,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:33,550][root][INFO] - Training Epoch: 1/2, step 1475/23838 completed (loss: 0.6877149939537048, acc: 0.8297872543334961)
[2025-02-16 11:08:33,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:33,996][root][INFO] - Training Epoch: 1/2, step 1476/23838 completed (loss: 1.0772945880889893, acc: 0.7142857313156128)
[2025-02-16 11:08:34,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:34,384][root][INFO] - Training Epoch: 1/2, step 1477/23838 completed (loss: 0.6233319044113159, acc: 0.843137264251709)
[2025-02-16 11:08:34,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:34,816][root][INFO] - Training Epoch: 1/2, step 1478/23838 completed (loss: 0.5546439290046692, acc: 0.9047619104385376)
[2025-02-16 11:08:35,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:35,301][root][INFO] - Training Epoch: 1/2, step 1479/23838 completed (loss: 0.963579535484314, acc: 0.699999988079071)
[2025-02-16 11:08:35,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:35,762][root][INFO] - Training Epoch: 1/2, step 1480/23838 completed (loss: 1.3034155368804932, acc: 0.7272727489471436)
[2025-02-16 11:08:35,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:36,187][root][INFO] - Training Epoch: 1/2, step 1481/23838 completed (loss: 2.2992007732391357, acc: 0.4399999976158142)
[2025-02-16 11:08:36,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:36,540][root][INFO] - Training Epoch: 1/2, step 1482/23838 completed (loss: 0.5750845074653625, acc: 0.774193525314331)
[2025-02-16 11:08:36,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:36,968][root][INFO] - Training Epoch: 1/2, step 1483/23838 completed (loss: 0.7732532620429993, acc: 0.6521739363670349)
[2025-02-16 11:08:37,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:37,339][root][INFO] - Training Epoch: 1/2, step 1484/23838 completed (loss: 0.40602973103523254, acc: 0.7894737124443054)
[2025-02-16 11:08:37,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:37,689][root][INFO] - Training Epoch: 1/2, step 1485/23838 completed (loss: 0.8006801605224609, acc: 0.7948718070983887)
[2025-02-16 11:08:37,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:38,140][root][INFO] - Training Epoch: 1/2, step 1486/23838 completed (loss: 0.6551258563995361, acc: 0.8163265585899353)
[2025-02-16 11:08:38,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:38,536][root][INFO] - Training Epoch: 1/2, step 1487/23838 completed (loss: 0.75202876329422, acc: 0.8275862336158752)
[2025-02-16 11:08:38,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:38,952][root][INFO] - Training Epoch: 1/2, step 1488/23838 completed (loss: 1.9540587663650513, acc: 0.5483871102333069)
[2025-02-16 11:08:39,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:39,321][root][INFO] - Training Epoch: 1/2, step 1489/23838 completed (loss: 1.7436094284057617, acc: 0.5555555820465088)
[2025-02-16 11:08:39,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:39,681][root][INFO] - Training Epoch: 1/2, step 1490/23838 completed (loss: 0.45538362860679626, acc: 0.8478260636329651)
[2025-02-16 11:08:39,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:40,117][root][INFO] - Training Epoch: 1/2, step 1491/23838 completed (loss: 0.15863879024982452, acc: 0.9523809552192688)
[2025-02-16 11:08:40,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:40,523][root][INFO] - Training Epoch: 1/2, step 1492/23838 completed (loss: 0.5766395330429077, acc: 0.8333333134651184)
[2025-02-16 11:08:40,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:40,951][root][INFO] - Training Epoch: 1/2, step 1493/23838 completed (loss: 0.6947196125984192, acc: 0.8636363744735718)
[2025-02-16 11:08:41,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:41,358][root][INFO] - Training Epoch: 1/2, step 1494/23838 completed (loss: 0.3060515224933624, acc: 0.9210526347160339)
[2025-02-16 11:08:41,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:41,759][root][INFO] - Training Epoch: 1/2, step 1495/23838 completed (loss: 1.1514787673950195, acc: 0.7419354915618896)
[2025-02-16 11:08:41,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:42,170][root][INFO] - Training Epoch: 1/2, step 1496/23838 completed (loss: 0.41978755593299866, acc: 0.8484848737716675)
[2025-02-16 11:08:42,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:42,566][root][INFO] - Training Epoch: 1/2, step 1497/23838 completed (loss: 1.1289364099502563, acc: 0.8214285969734192)
[2025-02-16 11:08:42,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:42,990][root][INFO] - Training Epoch: 1/2, step 1498/23838 completed (loss: 0.8461132645606995, acc: 0.7674418687820435)
[2025-02-16 11:08:43,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:43,389][root][INFO] - Training Epoch: 1/2, step 1499/23838 completed (loss: 1.5220057964324951, acc: 0.604651153087616)
[2025-02-16 11:08:43,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:43,814][root][INFO] - Training Epoch: 1/2, step 1500/23838 completed (loss: 1.1528931856155396, acc: 0.6969696879386902)
[2025-02-16 11:08:44,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:44,246][root][INFO] - Training Epoch: 1/2, step 1501/23838 completed (loss: 0.7810240983963013, acc: 0.782608687877655)
[2025-02-16 11:08:44,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:44,682][root][INFO] - Training Epoch: 1/2, step 1502/23838 completed (loss: 0.8062630295753479, acc: 0.7894737124443054)
[2025-02-16 11:08:44,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:45,090][root][INFO] - Training Epoch: 1/2, step 1503/23838 completed (loss: 0.8135321140289307, acc: 0.7647058963775635)
[2025-02-16 11:08:45,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:45,462][root][INFO] - Training Epoch: 1/2, step 1504/23838 completed (loss: 0.9674805998802185, acc: 0.800000011920929)
[2025-02-16 11:08:45,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:45,876][root][INFO] - Training Epoch: 1/2, step 1505/23838 completed (loss: 1.1121289730072021, acc: 0.6571428775787354)
[2025-02-16 11:08:46,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:46,299][root][INFO] - Training Epoch: 1/2, step 1506/23838 completed (loss: 0.735954999923706, acc: 0.7837837934494019)
[2025-02-16 11:08:46,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:46,686][root][INFO] - Training Epoch: 1/2, step 1507/23838 completed (loss: 1.0532819032669067, acc: 0.7200000286102295)
[2025-02-16 11:08:46,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:47,142][root][INFO] - Training Epoch: 1/2, step 1508/23838 completed (loss: 0.425025075674057, acc: 0.9032257795333862)
[2025-02-16 11:08:47,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:47,486][root][INFO] - Training Epoch: 1/2, step 1509/23838 completed (loss: 1.1008137464523315, acc: 0.6607142686843872)
[2025-02-16 11:08:47,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:47,832][root][INFO] - Training Epoch: 1/2, step 1510/23838 completed (loss: 0.25594958662986755, acc: 0.9555555582046509)
[2025-02-16 11:08:47,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:48,199][root][INFO] - Training Epoch: 1/2, step 1511/23838 completed (loss: 0.3599734604358673, acc: 0.8863636255264282)
[2025-02-16 11:08:48,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:48,617][root][INFO] - Training Epoch: 1/2, step 1512/23838 completed (loss: 0.5578275918960571, acc: 0.8809523582458496)
[2025-02-16 11:08:48,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:48,989][root][INFO] - Training Epoch: 1/2, step 1513/23838 completed (loss: 0.584952175617218, acc: 0.8500000238418579)
[2025-02-16 11:08:49,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:49,341][root][INFO] - Training Epoch: 1/2, step 1514/23838 completed (loss: 0.4934433698654175, acc: 0.9090909361839294)
[2025-02-16 11:08:49,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:49,722][root][INFO] - Training Epoch: 1/2, step 1515/23838 completed (loss: 1.0002497434616089, acc: 0.7083333134651184)
[2025-02-16 11:08:49,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:50,167][root][INFO] - Training Epoch: 1/2, step 1516/23838 completed (loss: 0.7171783447265625, acc: 0.7941176295280457)
[2025-02-16 11:08:50,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:50,649][root][INFO] - Training Epoch: 1/2, step 1517/23838 completed (loss: 0.9730952382087708, acc: 0.6774193644523621)
[2025-02-16 11:08:50,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:51,055][root][INFO] - Training Epoch: 1/2, step 1518/23838 completed (loss: 0.979981005191803, acc: 0.75)
[2025-02-16 11:08:51,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:51,426][root][INFO] - Training Epoch: 1/2, step 1519/23838 completed (loss: 1.2777256965637207, acc: 0.6944444179534912)
[2025-02-16 11:08:51,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:51,855][root][INFO] - Training Epoch: 1/2, step 1520/23838 completed (loss: 1.0344526767730713, acc: 0.7127659320831299)
[2025-02-16 11:08:52,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:52,288][root][INFO] - Training Epoch: 1/2, step 1521/23838 completed (loss: 0.594798743724823, acc: 0.8399999737739563)
[2025-02-16 11:08:52,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:52,721][root][INFO] - Training Epoch: 1/2, step 1522/23838 completed (loss: 1.7057394981384277, acc: 0.4285714328289032)
[2025-02-16 11:08:52,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:53,092][root][INFO] - Training Epoch: 1/2, step 1523/23838 completed (loss: 1.5291017293930054, acc: 0.6842105388641357)
[2025-02-16 11:08:53,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:53,450][root][INFO] - Training Epoch: 1/2, step 1524/23838 completed (loss: 0.7954422831535339, acc: 0.7317073345184326)
[2025-02-16 11:08:53,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:53,904][root][INFO] - Training Epoch: 1/2, step 1525/23838 completed (loss: 0.6940873861312866, acc: 0.8064516186714172)
[2025-02-16 11:08:54,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:54,309][root][INFO] - Training Epoch: 1/2, step 1526/23838 completed (loss: 1.6288671493530273, acc: 0.59375)
[2025-02-16 11:08:54,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:54,716][root][INFO] - Training Epoch: 1/2, step 1527/23838 completed (loss: 1.7822272777557373, acc: 0.5952380895614624)
[2025-02-16 11:08:54,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:55,216][root][INFO] - Training Epoch: 1/2, step 1528/23838 completed (loss: 1.3845908641815186, acc: 0.6329113841056824)
[2025-02-16 11:08:55,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:55,604][root][INFO] - Training Epoch: 1/2, step 1529/23838 completed (loss: 1.1394803524017334, acc: 0.739130437374115)
[2025-02-16 11:08:55,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:56,027][root][INFO] - Training Epoch: 1/2, step 1530/23838 completed (loss: 1.5737789869308472, acc: 0.5199999809265137)
[2025-02-16 11:08:56,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:56,394][root][INFO] - Training Epoch: 1/2, step 1531/23838 completed (loss: 0.8996089100837708, acc: 0.6842105388641357)
[2025-02-16 11:08:56,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:56,826][root][INFO] - Training Epoch: 1/2, step 1532/23838 completed (loss: 0.5275402665138245, acc: 0.9024389982223511)
[2025-02-16 11:08:57,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:57,240][root][INFO] - Training Epoch: 1/2, step 1533/23838 completed (loss: 1.012042760848999, acc: 0.7241379022598267)
[2025-02-16 11:08:57,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:57,677][root][INFO] - Training Epoch: 1/2, step 1534/23838 completed (loss: 0.5795958638191223, acc: 0.8387096524238586)
[2025-02-16 11:08:57,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:58,108][root][INFO] - Training Epoch: 1/2, step 1535/23838 completed (loss: 1.3016380071640015, acc: 0.7142857313156128)
[2025-02-16 11:08:58,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:58,503][root][INFO] - Training Epoch: 1/2, step 1536/23838 completed (loss: 1.5702496767044067, acc: 0.5454545617103577)
[2025-02-16 11:08:58,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:58,891][root][INFO] - Training Epoch: 1/2, step 1537/23838 completed (loss: 0.5477011799812317, acc: 0.8684210777282715)
[2025-02-16 11:08:59,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:59,352][root][INFO] - Training Epoch: 1/2, step 1538/23838 completed (loss: 0.761871337890625, acc: 0.7407407164573669)
[2025-02-16 11:08:59,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:08:59,809][root][INFO] - Training Epoch: 1/2, step 1539/23838 completed (loss: 0.9064273238182068, acc: 0.7878788113594055)
[2025-02-16 11:09:00,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:00,281][root][INFO] - Training Epoch: 1/2, step 1540/23838 completed (loss: 0.9426406621932983, acc: 0.7567567825317383)
[2025-02-16 11:09:00,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:00,690][root][INFO] - Training Epoch: 1/2, step 1541/23838 completed (loss: 0.6669931411743164, acc: 0.7941176295280457)
[2025-02-16 11:09:00,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:01,131][root][INFO] - Training Epoch: 1/2, step 1542/23838 completed (loss: 0.7824551463127136, acc: 0.7321428656578064)
[2025-02-16 11:09:01,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:01,506][root][INFO] - Training Epoch: 1/2, step 1543/23838 completed (loss: 1.1089986562728882, acc: 0.738095223903656)
[2025-02-16 11:09:01,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:01,933][root][INFO] - Training Epoch: 1/2, step 1544/23838 completed (loss: 0.9070591330528259, acc: 0.7297297120094299)
[2025-02-16 11:09:02,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:02,743][root][INFO] - Training Epoch: 1/2, step 1545/23838 completed (loss: 1.3759242296218872, acc: 0.5918367505073547)
[2025-02-16 11:09:02,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:03,193][root][INFO] - Training Epoch: 1/2, step 1546/23838 completed (loss: 1.0101677179336548, acc: 0.7250000238418579)
[2025-02-16 11:09:03,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:03,591][root][INFO] - Training Epoch: 1/2, step 1547/23838 completed (loss: 1.2336452007293701, acc: 0.6213592290878296)
[2025-02-16 11:09:03,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:04,026][root][INFO] - Training Epoch: 1/2, step 1548/23838 completed (loss: 1.0959643125534058, acc: 0.7184466123580933)
[2025-02-16 11:09:04,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:04,601][root][INFO] - Training Epoch: 1/2, step 1549/23838 completed (loss: 1.3045576810836792, acc: 0.71875)
[2025-02-16 11:09:04,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:05,055][root][INFO] - Training Epoch: 1/2, step 1550/23838 completed (loss: 1.1584335565567017, acc: 0.7395833134651184)
[2025-02-16 11:09:05,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:05,480][root][INFO] - Training Epoch: 1/2, step 1551/23838 completed (loss: 1.4535272121429443, acc: 0.594936728477478)
[2025-02-16 11:09:05,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:05,922][root][INFO] - Training Epoch: 1/2, step 1552/23838 completed (loss: 1.4871212244033813, acc: 0.6607142686843872)
[2025-02-16 11:09:06,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:06,567][root][INFO] - Training Epoch: 1/2, step 1553/23838 completed (loss: 1.2262834310531616, acc: 0.6486486196517944)
[2025-02-16 11:09:06,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:07,146][root][INFO] - Training Epoch: 1/2, step 1554/23838 completed (loss: 0.9169762134552002, acc: 0.7315436005592346)
[2025-02-16 11:09:07,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:08,000][root][INFO] - Training Epoch: 1/2, step 1555/23838 completed (loss: 1.1831477880477905, acc: 0.6764705777168274)
[2025-02-16 11:09:08,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:08,392][root][INFO] - Training Epoch: 1/2, step 1556/23838 completed (loss: 1.052670955657959, acc: 0.7580645084381104)
[2025-02-16 11:09:08,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:08,838][root][INFO] - Training Epoch: 1/2, step 1557/23838 completed (loss: 1.270272970199585, acc: 0.6521739363670349)
[2025-02-16 11:09:09,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:09,236][root][INFO] - Training Epoch: 1/2, step 1558/23838 completed (loss: 1.6538186073303223, acc: 0.5)
[2025-02-16 11:09:09,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:09,615][root][INFO] - Training Epoch: 1/2, step 1559/23838 completed (loss: 1.5741872787475586, acc: 0.5769230723381042)
[2025-02-16 11:09:09,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:10,032][root][INFO] - Training Epoch: 1/2, step 1560/23838 completed (loss: 1.2110103368759155, acc: 0.6307692527770996)
[2025-02-16 11:09:10,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:10,447][root][INFO] - Training Epoch: 1/2, step 1561/23838 completed (loss: 0.9229696989059448, acc: 0.6984127163887024)
[2025-02-16 11:09:10,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:10,836][root][INFO] - Training Epoch: 1/2, step 1562/23838 completed (loss: 1.429581880569458, acc: 0.6153846383094788)
[2025-02-16 11:09:11,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:11,313][root][INFO] - Training Epoch: 1/2, step 1563/23838 completed (loss: 1.1181740760803223, acc: 0.6823529601097107)
[2025-02-16 11:09:11,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:11,763][root][INFO] - Training Epoch: 1/2, step 1564/23838 completed (loss: 1.0438618659973145, acc: 0.7142857313156128)
[2025-02-16 11:09:11,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:12,152][root][INFO] - Training Epoch: 1/2, step 1565/23838 completed (loss: 0.8052238821983337, acc: 0.7857142686843872)
[2025-02-16 11:09:12,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:12,514][root][INFO] - Training Epoch: 1/2, step 1566/23838 completed (loss: 1.246267318725586, acc: 0.6153846383094788)
[2025-02-16 11:09:12,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:13,101][root][INFO] - Training Epoch: 1/2, step 1567/23838 completed (loss: 1.098241925239563, acc: 0.678787887096405)
[2025-02-16 11:09:13,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:13,689][root][INFO] - Training Epoch: 1/2, step 1568/23838 completed (loss: 1.4289579391479492, acc: 0.6212121248245239)
[2025-02-16 11:09:13,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:14,106][root][INFO] - Training Epoch: 1/2, step 1569/23838 completed (loss: 0.9134137630462646, acc: 0.7586206793785095)
[2025-02-16 11:09:14,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:14,931][root][INFO] - Training Epoch: 1/2, step 1570/23838 completed (loss: 1.400619387626648, acc: 0.6115702390670776)
[2025-02-16 11:09:15,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:15,356][root][INFO] - Training Epoch: 1/2, step 1571/23838 completed (loss: 0.737835705280304, acc: 0.8032786846160889)
[2025-02-16 11:09:15,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:15,960][root][INFO] - Training Epoch: 1/2, step 1572/23838 completed (loss: 1.0206003189086914, acc: 0.7043478488922119)
[2025-02-16 11:09:16,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:16,524][root][INFO] - Training Epoch: 1/2, step 1573/23838 completed (loss: 0.9084205627441406, acc: 0.7372262477874756)
[2025-02-16 11:09:16,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:16,980][root][INFO] - Training Epoch: 1/2, step 1574/23838 completed (loss: 1.3561975955963135, acc: 0.6233766078948975)
[2025-02-16 11:09:17,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:17,498][root][INFO] - Training Epoch: 1/2, step 1575/23838 completed (loss: 1.2943916320800781, acc: 0.6835442781448364)
[2025-02-16 11:09:17,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:18,100][root][INFO] - Training Epoch: 1/2, step 1576/23838 completed (loss: 1.3705089092254639, acc: 0.6592592597007751)
[2025-02-16 11:09:18,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:18,983][root][INFO] - Training Epoch: 1/2, step 1577/23838 completed (loss: 1.1556801795959473, acc: 0.6645161509513855)
[2025-02-16 11:09:19,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:19,424][root][INFO] - Training Epoch: 1/2, step 1578/23838 completed (loss: 0.9230589270591736, acc: 0.7941176295280457)
[2025-02-16 11:09:19,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:19,876][root][INFO] - Training Epoch: 1/2, step 1579/23838 completed (loss: 1.0320130586624146, acc: 0.6976743936538696)
[2025-02-16 11:09:20,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:20,243][root][INFO] - Training Epoch: 1/2, step 1580/23838 completed (loss: 1.6147935390472412, acc: 0.5454545617103577)
[2025-02-16 11:09:20,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:20,814][root][INFO] - Training Epoch: 1/2, step 1581/23838 completed (loss: 0.8841668367385864, acc: 0.7226890921592712)
[2025-02-16 11:09:21,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:21,220][root][INFO] - Training Epoch: 1/2, step 1582/23838 completed (loss: 0.5972403883934021, acc: 0.8529411554336548)
[2025-02-16 11:09:21,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:21,589][root][INFO] - Training Epoch: 1/2, step 1583/23838 completed (loss: 1.3065897226333618, acc: 0.65625)
[2025-02-16 11:09:21,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:21,969][root][INFO] - Training Epoch: 1/2, step 1584/23838 completed (loss: 1.2351378202438354, acc: 0.6857143044471741)
[2025-02-16 11:09:22,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:22,382][root][INFO] - Training Epoch: 1/2, step 1585/23838 completed (loss: 1.0066925287246704, acc: 0.7142857313156128)
[2025-02-16 11:09:22,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:22,882][root][INFO] - Training Epoch: 1/2, step 1586/23838 completed (loss: 0.8337618112564087, acc: 0.7578125)
[2025-02-16 11:09:23,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:23,247][root][INFO] - Training Epoch: 1/2, step 1587/23838 completed (loss: 0.7988801598548889, acc: 0.7123287916183472)
[2025-02-16 11:09:23,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:23,651][root][INFO] - Training Epoch: 1/2, step 1588/23838 completed (loss: 1.3222424983978271, acc: 0.5970149040222168)
[2025-02-16 11:09:23,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:24,112][root][INFO] - Training Epoch: 1/2, step 1589/23838 completed (loss: 0.7045478820800781, acc: 0.7721518874168396)
[2025-02-16 11:09:24,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:24,594][root][INFO] - Training Epoch: 1/2, step 1590/23838 completed (loss: 1.1278741359710693, acc: 0.6791045069694519)
[2025-02-16 11:09:24,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:25,055][root][INFO] - Training Epoch: 1/2, step 1591/23838 completed (loss: 0.8218410611152649, acc: 0.7916666865348816)
[2025-02-16 11:09:25,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:25,561][root][INFO] - Training Epoch: 1/2, step 1592/23838 completed (loss: 0.6006627082824707, acc: 0.8493150472640991)
[2025-02-16 11:09:25,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:26,011][root][INFO] - Training Epoch: 1/2, step 1593/23838 completed (loss: 0.8176261186599731, acc: 0.7575757503509521)
[2025-02-16 11:09:26,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:26,476][root][INFO] - Training Epoch: 1/2, step 1594/23838 completed (loss: 0.5502967834472656, acc: 0.8169013857841492)
[2025-02-16 11:09:26,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:26,987][root][INFO] - Training Epoch: 1/2, step 1595/23838 completed (loss: 0.6998308300971985, acc: 0.8288288116455078)
[2025-02-16 11:09:27,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:27,447][root][INFO] - Training Epoch: 1/2, step 1596/23838 completed (loss: 0.6299352645874023, acc: 0.8292682766914368)
[2025-02-16 11:09:27,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:27,840][root][INFO] - Training Epoch: 1/2, step 1597/23838 completed (loss: 0.8433486223220825, acc: 0.7652173638343811)
[2025-02-16 11:09:28,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:28,228][root][INFO] - Training Epoch: 1/2, step 1598/23838 completed (loss: 1.0157102346420288, acc: 0.6451612710952759)
[2025-02-16 11:09:28,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:28,611][root][INFO] - Training Epoch: 1/2, step 1599/23838 completed (loss: 1.0671050548553467, acc: 0.7142857313156128)
[2025-02-16 11:09:28,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:29,092][root][INFO] - Training Epoch: 1/2, step 1600/23838 completed (loss: 0.7751178741455078, acc: 0.7272727489471436)
[2025-02-16 11:09:29,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:29,531][root][INFO] - Training Epoch: 1/2, step 1601/23838 completed (loss: 0.9496014714241028, acc: 0.7096773982048035)
[2025-02-16 11:09:29,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:29,969][root][INFO] - Training Epoch: 1/2, step 1602/23838 completed (loss: 1.0102171897888184, acc: 0.7254902124404907)
[2025-02-16 11:09:30,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:30,413][root][INFO] - Training Epoch: 1/2, step 1603/23838 completed (loss: 0.47856709361076355, acc: 0.8494623899459839)
[2025-02-16 11:09:30,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:30,884][root][INFO] - Training Epoch: 1/2, step 1604/23838 completed (loss: 0.970983624458313, acc: 0.7413793206214905)
[2025-02-16 11:09:31,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:31,377][root][INFO] - Training Epoch: 1/2, step 1605/23838 completed (loss: 0.7055416107177734, acc: 0.8260869383811951)
[2025-02-16 11:09:31,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:31,852][root][INFO] - Training Epoch: 1/2, step 1606/23838 completed (loss: 1.0321046113967896, acc: 0.7300000190734863)
[2025-02-16 11:09:32,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:32,242][root][INFO] - Training Epoch: 1/2, step 1607/23838 completed (loss: 1.257502555847168, acc: 0.6764705777168274)
[2025-02-16 11:09:32,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:32,706][root][INFO] - Training Epoch: 1/2, step 1608/23838 completed (loss: 1.0349726676940918, acc: 0.6833333373069763)
[2025-02-16 11:09:32,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:33,177][root][INFO] - Training Epoch: 1/2, step 1609/23838 completed (loss: 0.544359028339386, acc: 0.8547008633613586)
[2025-02-16 11:09:33,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:33,648][root][INFO] - Training Epoch: 1/2, step 1610/23838 completed (loss: 0.6661802530288696, acc: 0.8125)
[2025-02-16 11:09:33,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:34,160][root][INFO] - Training Epoch: 1/2, step 1611/23838 completed (loss: 0.7015953063964844, acc: 0.8370370268821716)
[2025-02-16 11:09:34,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:34,713][root][INFO] - Training Epoch: 1/2, step 1612/23838 completed (loss: 1.0980031490325928, acc: 0.6901408433914185)
[2025-02-16 11:09:34,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:35,190][root][INFO] - Training Epoch: 1/2, step 1613/23838 completed (loss: 0.9036208391189575, acc: 0.6964285969734192)
[2025-02-16 11:09:35,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:35,620][root][INFO] - Training Epoch: 1/2, step 1614/23838 completed (loss: 1.3270795345306396, acc: 0.5967742204666138)
[2025-02-16 11:09:35,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:36,037][root][INFO] - Training Epoch: 1/2, step 1615/23838 completed (loss: 0.8966299891471863, acc: 0.732876718044281)
[2025-02-16 11:09:36,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:36,386][root][INFO] - Training Epoch: 1/2, step 1616/23838 completed (loss: 0.9604329466819763, acc: 0.782608687877655)
[2025-02-16 11:09:36,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:36,768][root][INFO] - Training Epoch: 1/2, step 1617/23838 completed (loss: 1.281955361366272, acc: 0.6486486196517944)
[2025-02-16 11:09:36,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:37,182][root][INFO] - Training Epoch: 1/2, step 1618/23838 completed (loss: 1.2028204202651978, acc: 0.6376811861991882)
[2025-02-16 11:09:37,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:37,748][root][INFO] - Training Epoch: 1/2, step 1619/23838 completed (loss: 0.9622560739517212, acc: 0.725806474685669)
[2025-02-16 11:09:37,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:38,228][root][INFO] - Training Epoch: 1/2, step 1620/23838 completed (loss: 0.7214826941490173, acc: 0.7785235047340393)
[2025-02-16 11:09:38,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:38,769][root][INFO] - Training Epoch: 1/2, step 1621/23838 completed (loss: 0.7049141526222229, acc: 0.7777777910232544)
[2025-02-16 11:09:38,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:39,158][root][INFO] - Training Epoch: 1/2, step 1622/23838 completed (loss: 0.9265789985656738, acc: 0.761904776096344)
[2025-02-16 11:09:39,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:39,603][root][INFO] - Training Epoch: 1/2, step 1623/23838 completed (loss: 0.6328892111778259, acc: 0.8131868243217468)
[2025-02-16 11:09:39,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:40,199][root][INFO] - Training Epoch: 1/2, step 1624/23838 completed (loss: 0.8361119627952576, acc: 0.7297297120094299)
[2025-02-16 11:09:40,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:40,695][root][INFO] - Training Epoch: 1/2, step 1625/23838 completed (loss: 0.4774381220340729, acc: 0.8709677457809448)
[2025-02-16 11:09:40,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:41,137][root][INFO] - Training Epoch: 1/2, step 1626/23838 completed (loss: 0.44062408804893494, acc: 0.8867924809455872)
[2025-02-16 11:09:41,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:41,665][root][INFO] - Training Epoch: 1/2, step 1627/23838 completed (loss: 0.5562684535980225, acc: 0.8399999737739563)
[2025-02-16 11:09:41,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:42,154][root][INFO] - Training Epoch: 1/2, step 1628/23838 completed (loss: 0.6997280716896057, acc: 0.756302535533905)
[2025-02-16 11:09:42,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:42,596][root][INFO] - Training Epoch: 1/2, step 1629/23838 completed (loss: 0.35782545804977417, acc: 0.9047619104385376)
[2025-02-16 11:09:42,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:43,018][root][INFO] - Training Epoch: 1/2, step 1630/23838 completed (loss: 0.5083139538764954, acc: 0.8548387289047241)
[2025-02-16 11:09:43,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:43,401][root][INFO] - Training Epoch: 1/2, step 1631/23838 completed (loss: 0.5367061495780945, acc: 0.8783783912658691)
[2025-02-16 11:09:43,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:43,880][root][INFO] - Training Epoch: 1/2, step 1632/23838 completed (loss: 0.5216512084007263, acc: 0.8108108043670654)
[2025-02-16 11:09:44,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:44,317][root][INFO] - Training Epoch: 1/2, step 1633/23838 completed (loss: 0.6906470656394958, acc: 0.8085106611251831)
[2025-02-16 11:09:44,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:44,749][root][INFO] - Training Epoch: 1/2, step 1634/23838 completed (loss: 0.5736651420593262, acc: 0.8787878751754761)
[2025-02-16 11:09:44,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:45,159][root][INFO] - Training Epoch: 1/2, step 1635/23838 completed (loss: 0.7048050165176392, acc: 0.8359375)
[2025-02-16 11:09:45,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:45,747][root][INFO] - Training Epoch: 1/2, step 1636/23838 completed (loss: 0.4603777229785919, acc: 0.8709677457809448)
[2025-02-16 11:09:46,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:46,347][root][INFO] - Training Epoch: 1/2, step 1637/23838 completed (loss: 0.59609055519104, acc: 0.8421052694320679)
[2025-02-16 11:09:46,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:46,799][root][INFO] - Training Epoch: 1/2, step 1638/23838 completed (loss: 0.3927347660064697, acc: 0.9102563858032227)
[2025-02-16 11:09:47,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:47,626][root][INFO] - Training Epoch: 1/2, step 1639/23838 completed (loss: 0.9326854348182678, acc: 0.7615384459495544)
[2025-02-16 11:09:47,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:48,019][root][INFO] - Training Epoch: 1/2, step 1640/23838 completed (loss: 0.20707572996616364, acc: 0.9454545378684998)
[2025-02-16 11:09:48,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:48,426][root][INFO] - Training Epoch: 1/2, step 1641/23838 completed (loss: 1.3116095066070557, acc: 0.5714285969734192)
[2025-02-16 11:09:48,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:49,002][root][INFO] - Training Epoch: 1/2, step 1642/23838 completed (loss: 1.0739774703979492, acc: 0.7454545497894287)
[2025-02-16 11:09:49,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:49,445][root][INFO] - Training Epoch: 1/2, step 1643/23838 completed (loss: 0.7675938606262207, acc: 0.7719298005104065)
[2025-02-16 11:09:49,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:49,915][root][INFO] - Training Epoch: 1/2, step 1644/23838 completed (loss: 1.0674762725830078, acc: 0.7297297120094299)
[2025-02-16 11:09:50,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:50,374][root][INFO] - Training Epoch: 1/2, step 1645/23838 completed (loss: 0.650980532169342, acc: 0.8117647171020508)
[2025-02-16 11:09:50,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:50,801][root][INFO] - Training Epoch: 1/2, step 1646/23838 completed (loss: 0.6889697313308716, acc: 0.8039215803146362)
[2025-02-16 11:09:51,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:51,345][root][INFO] - Training Epoch: 1/2, step 1647/23838 completed (loss: 0.6058478355407715, acc: 0.804347813129425)
[2025-02-16 11:09:51,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:51,696][root][INFO] - Training Epoch: 1/2, step 1648/23838 completed (loss: 0.7758103609085083, acc: 0.8301886916160583)
[2025-02-16 11:09:51,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:52,115][root][INFO] - Training Epoch: 1/2, step 1649/23838 completed (loss: 0.3023394048213959, acc: 0.9215686321258545)
[2025-02-16 11:09:52,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:52,512][root][INFO] - Training Epoch: 1/2, step 1650/23838 completed (loss: 0.49796193838119507, acc: 0.8817204236984253)
[2025-02-16 11:09:52,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:52,955][root][INFO] - Training Epoch: 1/2, step 1651/23838 completed (loss: 1.1360851526260376, acc: 0.7402597665786743)
[2025-02-16 11:09:53,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:53,345][root][INFO] - Training Epoch: 1/2, step 1652/23838 completed (loss: 0.9631376266479492, acc: 0.7142857313156128)
[2025-02-16 11:09:53,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:53,774][root][INFO] - Training Epoch: 1/2, step 1653/23838 completed (loss: 0.6218353509902954, acc: 0.875)
[2025-02-16 11:09:54,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:54,253][root][INFO] - Training Epoch: 1/2, step 1654/23838 completed (loss: 0.13535170257091522, acc: 0.949999988079071)
[2025-02-16 11:09:54,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:54,704][root][INFO] - Training Epoch: 1/2, step 1655/23838 completed (loss: 0.3458269238471985, acc: 0.875)
[2025-02-16 11:09:54,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:55,167][root][INFO] - Training Epoch: 1/2, step 1656/23838 completed (loss: 0.9403542876243591, acc: 0.7355371713638306)
[2025-02-16 11:09:55,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:55,580][root][INFO] - Training Epoch: 1/2, step 1657/23838 completed (loss: 0.6416254639625549, acc: 0.800000011920929)
[2025-02-16 11:09:55,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:55,969][root][INFO] - Training Epoch: 1/2, step 1658/23838 completed (loss: 0.5710251927375793, acc: 0.8269230723381042)
[2025-02-16 11:09:56,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:56,326][root][INFO] - Training Epoch: 1/2, step 1659/23838 completed (loss: 0.7476069331169128, acc: 0.6976743936538696)
[2025-02-16 11:09:56,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:56,804][root][INFO] - Training Epoch: 1/2, step 1660/23838 completed (loss: 0.744549572467804, acc: 0.7536231875419617)
[2025-02-16 11:09:57,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:57,248][root][INFO] - Training Epoch: 1/2, step 1661/23838 completed (loss: 0.7679045796394348, acc: 0.792682945728302)
[2025-02-16 11:09:57,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:57,629][root][INFO] - Training Epoch: 1/2, step 1662/23838 completed (loss: 0.5983425378799438, acc: 0.824999988079071)
[2025-02-16 11:09:57,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:58,037][root][INFO] - Training Epoch: 1/2, step 1663/23838 completed (loss: 0.5957773923873901, acc: 0.8799999952316284)
[2025-02-16 11:09:58,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:58,446][root][INFO] - Training Epoch: 1/2, step 1664/23838 completed (loss: 0.4183754026889801, acc: 0.8888888955116272)
[2025-02-16 11:09:58,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:58,898][root][INFO] - Training Epoch: 1/2, step 1665/23838 completed (loss: 0.5917708277702332, acc: 0.8125)
[2025-02-16 11:09:59,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:59,262][root][INFO] - Training Epoch: 1/2, step 1666/23838 completed (loss: 0.6909616589546204, acc: 0.8842105269432068)
[2025-02-16 11:09:59,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:09:59,707][root][INFO] - Training Epoch: 1/2, step 1667/23838 completed (loss: 0.8756906390190125, acc: 0.7978723645210266)
[2025-02-16 11:09:59,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:00,077][root][INFO] - Training Epoch: 1/2, step 1668/23838 completed (loss: 0.7458011507987976, acc: 0.7857142686843872)
[2025-02-16 11:10:00,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:00,509][root][INFO] - Training Epoch: 1/2, step 1669/23838 completed (loss: 1.023926019668579, acc: 0.649350643157959)
[2025-02-16 11:10:00,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:00,980][root][INFO] - Training Epoch: 1/2, step 1670/23838 completed (loss: 0.3831529915332794, acc: 0.9473684430122375)
[2025-02-16 11:10:01,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:01,346][root][INFO] - Training Epoch: 1/2, step 1671/23838 completed (loss: 0.8826557993888855, acc: 0.7857142686843872)
[2025-02-16 11:10:01,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:01,826][root][INFO] - Training Epoch: 1/2, step 1672/23838 completed (loss: 0.7348819375038147, acc: 0.8030303120613098)
[2025-02-16 11:10:01,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:02,211][root][INFO] - Training Epoch: 1/2, step 1673/23838 completed (loss: 0.7956234216690063, acc: 0.8045976758003235)
[2025-02-16 11:10:02,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:02,703][root][INFO] - Training Epoch: 1/2, step 1674/23838 completed (loss: 0.5967843532562256, acc: 0.8376068472862244)
[2025-02-16 11:10:02,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:03,146][root][INFO] - Training Epoch: 1/2, step 1675/23838 completed (loss: 0.6815685033798218, acc: 0.800000011920929)
[2025-02-16 11:10:03,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:04,237][root][INFO] - Training Epoch: 1/2, step 1676/23838 completed (loss: 0.5563201308250427, acc: 0.8121547102928162)
[2025-02-16 11:10:04,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:04,619][root][INFO] - Training Epoch: 1/2, step 1677/23838 completed (loss: 0.48978161811828613, acc: 0.8547008633613586)
[2025-02-16 11:10:04,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:05,124][root][INFO] - Training Epoch: 1/2, step 1678/23838 completed (loss: 0.6814086437225342, acc: 0.7692307829856873)
[2025-02-16 11:10:05,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:05,973][root][INFO] - Training Epoch: 1/2, step 1679/23838 completed (loss: 0.6475241780281067, acc: 0.8054053783416748)
[2025-02-16 11:10:06,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:06,578][root][INFO] - Training Epoch: 1/2, step 1680/23838 completed (loss: 0.6256003379821777, acc: 0.847328245639801)
[2025-02-16 11:10:06,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:07,036][root][INFO] - Training Epoch: 1/2, step 1681/23838 completed (loss: 1.2347650527954102, acc: 0.6666666865348816)
[2025-02-16 11:10:07,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:07,569][root][INFO] - Training Epoch: 1/2, step 1682/23838 completed (loss: 0.5333360433578491, acc: 0.8028169274330139)
[2025-02-16 11:10:07,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:08,032][root][INFO] - Training Epoch: 1/2, step 1683/23838 completed (loss: 0.8431278467178345, acc: 0.7916666865348816)
[2025-02-16 11:10:08,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:08,396][root][INFO] - Training Epoch: 1/2, step 1684/23838 completed (loss: 0.6205381155014038, acc: 0.8448275923728943)
[2025-02-16 11:10:08,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:08,929][root][INFO] - Training Epoch: 1/2, step 1685/23838 completed (loss: 0.435017853975296, acc: 0.8842975497245789)
[2025-02-16 11:10:09,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:09,653][root][INFO] - Training Epoch: 1/2, step 1686/23838 completed (loss: 0.5304068922996521, acc: 0.875)
[2025-02-16 11:10:09,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:10,100][root][INFO] - Training Epoch: 1/2, step 1687/23838 completed (loss: 1.1494615077972412, acc: 0.720588207244873)
[2025-02-16 11:10:10,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:10,557][root][INFO] - Training Epoch: 1/2, step 1688/23838 completed (loss: 0.7445440292358398, acc: 0.8161764740943909)
[2025-02-16 11:10:10,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:10,994][root][INFO] - Training Epoch: 1/2, step 1689/23838 completed (loss: 0.56584632396698, acc: 0.843478262424469)
[2025-02-16 11:10:11,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:11,465][root][INFO] - Training Epoch: 1/2, step 1690/23838 completed (loss: 0.8068032264709473, acc: 0.7947019934654236)
[2025-02-16 11:10:11,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:12,091][root][INFO] - Training Epoch: 1/2, step 1691/23838 completed (loss: 1.6890041828155518, acc: 0.5875706076622009)
[2025-02-16 11:10:12,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:12,799][root][INFO] - Training Epoch: 1/2, step 1692/23838 completed (loss: 1.8362590074539185, acc: 0.5409836173057556)
[2025-02-16 11:10:13,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:13,272][root][INFO] - Training Epoch: 1/2, step 1693/23838 completed (loss: 1.273939847946167, acc: 0.6399999856948853)
[2025-02-16 11:10:13,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:13,938][root][INFO] - Training Epoch: 1/2, step 1694/23838 completed (loss: 1.6716797351837158, acc: 0.5563910007476807)
[2025-02-16 11:10:14,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:14,391][root][INFO] - Training Epoch: 1/2, step 1695/23838 completed (loss: 0.7040151953697205, acc: 0.7830188870429993)
[2025-02-16 11:10:14,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:14,823][root][INFO] - Training Epoch: 1/2, step 1696/23838 completed (loss: 0.7703204154968262, acc: 0.7777777910232544)
[2025-02-16 11:10:15,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:15,252][root][INFO] - Training Epoch: 1/2, step 1697/23838 completed (loss: 1.0163956880569458, acc: 0.6907216310501099)
[2025-02-16 11:10:15,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:15,647][root][INFO] - Training Epoch: 1/2, step 1698/23838 completed (loss: 1.1526947021484375, acc: 0.6836734414100647)
[2025-02-16 11:10:15,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:16,073][root][INFO] - Training Epoch: 1/2, step 1699/23838 completed (loss: 0.7886027097702026, acc: 0.7802197933197021)
[2025-02-16 11:10:16,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:16,442][root][INFO] - Training Epoch: 1/2, step 1700/23838 completed (loss: 0.8701328039169312, acc: 0.7547169923782349)
[2025-02-16 11:10:16,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:16,862][root][INFO] - Training Epoch: 1/2, step 1701/23838 completed (loss: 1.1405010223388672, acc: 0.6527777910232544)
[2025-02-16 11:10:17,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:17,297][root][INFO] - Training Epoch: 1/2, step 1702/23838 completed (loss: 0.6542176008224487, acc: 0.8586956262588501)
[2025-02-16 11:10:17,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:17,659][root][INFO] - Training Epoch: 1/2, step 1703/23838 completed (loss: 0.6783616542816162, acc: 0.8636363744735718)
[2025-02-16 11:10:17,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:18,027][root][INFO] - Training Epoch: 1/2, step 1704/23838 completed (loss: 1.205358624458313, acc: 0.6133333444595337)
[2025-02-16 11:10:18,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:18,450][root][INFO] - Training Epoch: 1/2, step 1705/23838 completed (loss: 0.7942778468132019, acc: 0.7594936490058899)
[2025-02-16 11:10:18,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:18,866][root][INFO] - Training Epoch: 1/2, step 1706/23838 completed (loss: 0.6371722221374512, acc: 0.8970588445663452)
[2025-02-16 11:10:19,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:19,335][root][INFO] - Training Epoch: 1/2, step 1707/23838 completed (loss: 0.7776439189910889, acc: 0.8021978139877319)
[2025-02-16 11:10:19,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:19,763][root][INFO] - Training Epoch: 1/2, step 1708/23838 completed (loss: 0.8277373909950256, acc: 0.7310924530029297)
[2025-02-16 11:10:19,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:20,186][root][INFO] - Training Epoch: 1/2, step 1709/23838 completed (loss: 0.6376051306724548, acc: 0.7916666865348816)
[2025-02-16 11:10:20,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:20,666][root][INFO] - Training Epoch: 1/2, step 1710/23838 completed (loss: 0.919634997844696, acc: 0.730434775352478)
[2025-02-16 11:10:20,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:21,046][root][INFO] - Training Epoch: 1/2, step 1711/23838 completed (loss: 0.5496001839637756, acc: 0.8641975522041321)
[2025-02-16 11:10:21,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:21,424][root][INFO] - Training Epoch: 1/2, step 1712/23838 completed (loss: 1.1673504114151, acc: 0.6000000238418579)
[2025-02-16 11:10:21,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:21,792][root][INFO] - Training Epoch: 1/2, step 1713/23838 completed (loss: 1.1187984943389893, acc: 0.7204301357269287)
[2025-02-16 11:10:21,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:22,193][root][INFO] - Training Epoch: 1/2, step 1714/23838 completed (loss: 0.9480833411216736, acc: 0.7362637519836426)
[2025-02-16 11:10:22,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:22,615][root][INFO] - Training Epoch: 1/2, step 1715/23838 completed (loss: 1.0933253765106201, acc: 0.7746478915214539)
[2025-02-16 11:10:22,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:23,015][root][INFO] - Training Epoch: 1/2, step 1716/23838 completed (loss: 0.7626608610153198, acc: 0.800000011920929)
[2025-02-16 11:10:23,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:23,420][root][INFO] - Training Epoch: 1/2, step 1717/23838 completed (loss: 0.6731793284416199, acc: 0.78125)
[2025-02-16 11:10:23,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:23,851][root][INFO] - Training Epoch: 1/2, step 1718/23838 completed (loss: 0.7113456726074219, acc: 0.7966101765632629)
[2025-02-16 11:10:24,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:24,262][root][INFO] - Training Epoch: 1/2, step 1719/23838 completed (loss: 1.0118027925491333, acc: 0.6792452931404114)
[2025-02-16 11:10:24,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:24,720][root][INFO] - Training Epoch: 1/2, step 1720/23838 completed (loss: 1.388782024383545, acc: 0.7068965435028076)
[2025-02-16 11:10:24,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:25,114][root][INFO] - Training Epoch: 1/2, step 1721/23838 completed (loss: 1.0372207164764404, acc: 0.6878612637519836)
[2025-02-16 11:10:25,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:25,485][root][INFO] - Training Epoch: 1/2, step 1722/23838 completed (loss: 0.9643286466598511, acc: 0.7676767706871033)
[2025-02-16 11:10:25,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:25,934][root][INFO] - Training Epoch: 1/2, step 1723/23838 completed (loss: 0.8911309838294983, acc: 0.7575757503509521)
[2025-02-16 11:10:26,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:26,309][root][INFO] - Training Epoch: 1/2, step 1724/23838 completed (loss: 1.1401039361953735, acc: 0.75)
[2025-02-16 11:10:26,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:26,680][root][INFO] - Training Epoch: 1/2, step 1725/23838 completed (loss: 0.7446923851966858, acc: 0.8387096524238586)
[2025-02-16 11:10:26,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:27,121][root][INFO] - Training Epoch: 1/2, step 1726/23838 completed (loss: 0.9257864356040955, acc: 0.7288135886192322)
[2025-02-16 11:10:27,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:27,528][root][INFO] - Training Epoch: 1/2, step 1727/23838 completed (loss: 0.6436643004417419, acc: 0.7910447716712952)
[2025-02-16 11:10:27,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:27,999][root][INFO] - Training Epoch: 1/2, step 1728/23838 completed (loss: 1.1932193040847778, acc: 0.6346153616905212)
[2025-02-16 11:10:28,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:28,391][root][INFO] - Training Epoch: 1/2, step 1729/23838 completed (loss: 0.6073601245880127, acc: 0.8396946787834167)
[2025-02-16 11:10:28,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:28,837][root][INFO] - Training Epoch: 1/2, step 1730/23838 completed (loss: 0.595589280128479, acc: 0.8139534592628479)
[2025-02-16 11:10:29,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:29,266][root][INFO] - Training Epoch: 1/2, step 1731/23838 completed (loss: 0.9763798713684082, acc: 0.7142857313156128)
[2025-02-16 11:10:29,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:29,640][root][INFO] - Training Epoch: 1/2, step 1732/23838 completed (loss: 0.8283267021179199, acc: 0.774193525314331)
[2025-02-16 11:10:29,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:30,084][root][INFO] - Training Epoch: 1/2, step 1733/23838 completed (loss: 0.49712926149368286, acc: 0.8314606547355652)
[2025-02-16 11:10:30,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:30,495][root][INFO] - Training Epoch: 1/2, step 1734/23838 completed (loss: 0.8722380995750427, acc: 0.772357702255249)
[2025-02-16 11:10:30,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:30,882][root][INFO] - Training Epoch: 1/2, step 1735/23838 completed (loss: 1.197235345840454, acc: 0.671875)
[2025-02-16 11:10:31,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:31,353][root][INFO] - Training Epoch: 1/2, step 1736/23838 completed (loss: 0.4115371108055115, acc: 0.8909090757369995)
[2025-02-16 11:10:31,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:31,797][root][INFO] - Training Epoch: 1/2, step 1737/23838 completed (loss: 0.8993854522705078, acc: 0.7878788113594055)
[2025-02-16 11:10:31,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:32,203][root][INFO] - Training Epoch: 1/2, step 1738/23838 completed (loss: 0.475639671087265, acc: 0.8623188138008118)
[2025-02-16 11:10:32,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:32,625][root][INFO] - Training Epoch: 1/2, step 1739/23838 completed (loss: 1.2958667278289795, acc: 0.625)
[2025-02-16 11:10:32,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:33,204][root][INFO] - Training Epoch: 1/2, step 1740/23838 completed (loss: 0.6128548979759216, acc: 0.824701189994812)
[2025-02-16 11:10:33,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:33,650][root][INFO] - Training Epoch: 1/2, step 1741/23838 completed (loss: 0.4585930109024048, acc: 0.8899999856948853)
[2025-02-16 11:10:33,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:34,363][root][INFO] - Training Epoch: 1/2, step 1742/23838 completed (loss: 0.7798286080360413, acc: 0.7809917330741882)
[2025-02-16 11:10:34,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:34,800][root][INFO] - Training Epoch: 1/2, step 1743/23838 completed (loss: 0.7773951888084412, acc: 0.7708333134651184)
[2025-02-16 11:10:34,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:35,183][root][INFO] - Training Epoch: 1/2, step 1744/23838 completed (loss: 0.8862740993499756, acc: 0.7529411911964417)
[2025-02-16 11:10:35,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:35,633][root][INFO] - Training Epoch: 1/2, step 1745/23838 completed (loss: 0.6723973751068115, acc: 0.8068181872367859)
[2025-02-16 11:10:35,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:36,086][root][INFO] - Training Epoch: 1/2, step 1746/23838 completed (loss: 0.6606273055076599, acc: 0.8260869383811951)
[2025-02-16 11:10:36,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:36,491][root][INFO] - Training Epoch: 1/2, step 1747/23838 completed (loss: 0.5981547236442566, acc: 0.8583333492279053)
[2025-02-16 11:10:36,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:36,913][root][INFO] - Training Epoch: 1/2, step 1748/23838 completed (loss: 0.37168678641319275, acc: 0.8888888955116272)
[2025-02-16 11:10:37,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:37,393][root][INFO] - Training Epoch: 1/2, step 1749/23838 completed (loss: 0.6577632427215576, acc: 0.75)
[2025-02-16 11:10:37,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:37,821][root][INFO] - Training Epoch: 1/2, step 1750/23838 completed (loss: 0.4884456396102905, acc: 0.8648648858070374)
[2025-02-16 11:10:38,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:38,242][root][INFO] - Training Epoch: 1/2, step 1751/23838 completed (loss: 0.4178256392478943, acc: 0.8910890817642212)
[2025-02-16 11:10:38,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:38,665][root][INFO] - Training Epoch: 1/2, step 1752/23838 completed (loss: 0.4543645977973938, acc: 0.8977272510528564)
[2025-02-16 11:10:38,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:39,085][root][INFO] - Training Epoch: 1/2, step 1753/23838 completed (loss: 0.6591794490814209, acc: 0.7638888955116272)
[2025-02-16 11:10:39,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:39,470][root][INFO] - Training Epoch: 1/2, step 1754/23838 completed (loss: 0.9768586158752441, acc: 0.6354166865348816)
[2025-02-16 11:10:39,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:39,936][root][INFO] - Training Epoch: 1/2, step 1755/23838 completed (loss: 0.7590861320495605, acc: 0.7815126180648804)
[2025-02-16 11:10:40,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:40,327][root][INFO] - Training Epoch: 1/2, step 1756/23838 completed (loss: 1.0206114053726196, acc: 0.7551020383834839)
[2025-02-16 11:10:40,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:40,695][root][INFO] - Training Epoch: 1/2, step 1757/23838 completed (loss: 0.5634374618530273, acc: 0.8691588640213013)
[2025-02-16 11:10:40,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:41,114][root][INFO] - Training Epoch: 1/2, step 1758/23838 completed (loss: 0.3718148171901703, acc: 0.8873239159584045)
[2025-02-16 11:10:41,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:41,484][root][INFO] - Training Epoch: 1/2, step 1759/23838 completed (loss: 0.5389657020568848, acc: 0.8725489974021912)
[2025-02-16 11:10:41,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:41,965][root][INFO] - Training Epoch: 1/2, step 1760/23838 completed (loss: 0.4552420675754547, acc: 0.8414633870124817)
[2025-02-16 11:10:42,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:42,417][root][INFO] - Training Epoch: 1/2, step 1761/23838 completed (loss: 0.5580284595489502, acc: 0.8571428656578064)
[2025-02-16 11:10:42,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:42,938][root][INFO] - Training Epoch: 1/2, step 1762/23838 completed (loss: 0.7280982732772827, acc: 0.8211920261383057)
[2025-02-16 11:10:43,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:43,401][root][INFO] - Training Epoch: 1/2, step 1763/23838 completed (loss: 0.7051418423652649, acc: 0.8190476298332214)
[2025-02-16 11:10:43,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:43,782][root][INFO] - Training Epoch: 1/2, step 1764/23838 completed (loss: 1.1635884046554565, acc: 0.682539701461792)
[2025-02-16 11:10:43,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:44,147][root][INFO] - Training Epoch: 1/2, step 1765/23838 completed (loss: 0.8759071826934814, acc: 0.7714285850524902)
[2025-02-16 11:10:44,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:44,521][root][INFO] - Training Epoch: 1/2, step 1766/23838 completed (loss: 0.6981133222579956, acc: 0.8235294222831726)
[2025-02-16 11:10:44,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:44,886][root][INFO] - Training Epoch: 1/2, step 1767/23838 completed (loss: 0.8837746381759644, acc: 0.7400000095367432)
[2025-02-16 11:10:45,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:45,335][root][INFO] - Training Epoch: 1/2, step 1768/23838 completed (loss: 0.6778101921081543, acc: 0.807692289352417)
[2025-02-16 11:10:45,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:45,711][root][INFO] - Training Epoch: 1/2, step 1769/23838 completed (loss: 0.6729275584220886, acc: 0.8541666865348816)
[2025-02-16 11:10:45,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:46,157][root][INFO] - Training Epoch: 1/2, step 1770/23838 completed (loss: 0.9895933866500854, acc: 0.6842105388641357)
[2025-02-16 11:10:46,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:46,575][root][INFO] - Training Epoch: 1/2, step 1771/23838 completed (loss: 0.814073920249939, acc: 0.7681159377098083)
[2025-02-16 11:10:46,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:46,963][root][INFO] - Training Epoch: 1/2, step 1772/23838 completed (loss: 0.8168827891349792, acc: 0.7799999713897705)
[2025-02-16 11:10:47,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:47,312][root][INFO] - Training Epoch: 1/2, step 1773/23838 completed (loss: 0.7832961082458496, acc: 0.792792797088623)
[2025-02-16 11:10:47,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:47,721][root][INFO] - Training Epoch: 1/2, step 1774/23838 completed (loss: 0.8379241228103638, acc: 0.746835470199585)
[2025-02-16 11:10:47,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:48,146][root][INFO] - Training Epoch: 1/2, step 1775/23838 completed (loss: 1.2191599607467651, acc: 0.6913580298423767)
[2025-02-16 11:10:48,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:48,594][root][INFO] - Training Epoch: 1/2, step 1776/23838 completed (loss: 0.9341204166412354, acc: 0.7903226017951965)
[2025-02-16 11:10:48,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:49,049][root][INFO] - Training Epoch: 1/2, step 1777/23838 completed (loss: 0.9260850548744202, acc: 0.7692307829856873)
[2025-02-16 11:10:49,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:49,493][root][INFO] - Training Epoch: 1/2, step 1778/23838 completed (loss: 0.9296661615371704, acc: 0.7599999904632568)
[2025-02-16 11:10:49,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:49,914][root][INFO] - Training Epoch: 1/2, step 1779/23838 completed (loss: 0.44824814796447754, acc: 0.8904109597206116)
[2025-02-16 11:10:50,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:50,326][root][INFO] - Training Epoch: 1/2, step 1780/23838 completed (loss: 0.6382158398628235, acc: 0.8354430198669434)
[2025-02-16 11:10:50,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:50,748][root][INFO] - Training Epoch: 1/2, step 1781/23838 completed (loss: 0.6677916049957275, acc: 0.7786259651184082)
[2025-02-16 11:10:50,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:51,119][root][INFO] - Training Epoch: 1/2, step 1782/23838 completed (loss: 0.8897979259490967, acc: 0.7752808928489685)
[2025-02-16 11:10:51,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:51,487][root][INFO] - Training Epoch: 1/2, step 1783/23838 completed (loss: 0.9001861214637756, acc: 0.7435897588729858)
[2025-02-16 11:10:51,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:51,914][root][INFO] - Training Epoch: 1/2, step 1784/23838 completed (loss: 0.827483057975769, acc: 0.7142857313156128)
[2025-02-16 11:10:52,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:52,343][root][INFO] - Training Epoch: 1/2, step 1785/23838 completed (loss: 1.3002477884292603, acc: 0.6428571343421936)
[2025-02-16 11:10:52,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:52,710][root][INFO] - Training Epoch: 1/2, step 1786/23838 completed (loss: 0.7622625827789307, acc: 0.7727272510528564)
[2025-02-16 11:10:52,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:53,132][root][INFO] - Training Epoch: 1/2, step 1787/23838 completed (loss: 1.1569526195526123, acc: 0.6666666865348816)
[2025-02-16 11:10:53,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:53,577][root][INFO] - Training Epoch: 1/2, step 1788/23838 completed (loss: 0.7088213562965393, acc: 0.8409090638160706)
[2025-02-16 11:10:53,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:54,007][root][INFO] - Training Epoch: 1/2, step 1789/23838 completed (loss: 0.6432965397834778, acc: 0.8181818127632141)
[2025-02-16 11:10:54,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:54,526][root][INFO] - Training Epoch: 1/2, step 1790/23838 completed (loss: 0.5231009721755981, acc: 0.8674699068069458)
[2025-02-16 11:10:54,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:54,970][root][INFO] - Training Epoch: 1/2, step 1791/23838 completed (loss: 0.8155994415283203, acc: 0.7833333611488342)
[2025-02-16 11:10:55,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:55,349][root][INFO] - Training Epoch: 1/2, step 1792/23838 completed (loss: 0.4509355127811432, acc: 0.8888888955116272)
[2025-02-16 11:10:55,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:55,824][root][INFO] - Training Epoch: 1/2, step 1793/23838 completed (loss: 0.7815914750099182, acc: 0.7419354915618896)
[2025-02-16 11:10:56,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:56,245][root][INFO] - Training Epoch: 1/2, step 1794/23838 completed (loss: 1.0383002758026123, acc: 0.7179487347602844)
[2025-02-16 11:10:56,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:56,616][root][INFO] - Training Epoch: 1/2, step 1795/23838 completed (loss: 0.6499913334846497, acc: 0.8181818127632141)
[2025-02-16 11:10:56,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:56,979][root][INFO] - Training Epoch: 1/2, step 1796/23838 completed (loss: 0.3916061818599701, acc: 0.892307698726654)
[2025-02-16 11:10:57,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:57,444][root][INFO] - Training Epoch: 1/2, step 1797/23838 completed (loss: 0.5513404607772827, acc: 0.8260869383811951)
[2025-02-16 11:10:57,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:57,871][root][INFO] - Training Epoch: 1/2, step 1798/23838 completed (loss: 0.2903622090816498, acc: 0.9146341681480408)
[2025-02-16 11:10:58,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:58,453][root][INFO] - Training Epoch: 1/2, step 1799/23838 completed (loss: 0.5758388638496399, acc: 0.8285714387893677)
[2025-02-16 11:10:58,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:58,846][root][INFO] - Training Epoch: 1/2, step 1800/23838 completed (loss: 0.6908000707626343, acc: 0.8164557218551636)
[2025-02-16 11:10:59,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:59,251][root][INFO] - Training Epoch: 1/2, step 1801/23838 completed (loss: 1.3279170989990234, acc: 0.6499999761581421)
[2025-02-16 11:10:59,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:10:59,663][root][INFO] - Training Epoch: 1/2, step 1802/23838 completed (loss: 0.1882684975862503, acc: 0.9710144996643066)
[2025-02-16 11:10:59,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:00,169][root][INFO] - Training Epoch: 1/2, step 1803/23838 completed (loss: 0.28568270802497864, acc: 0.9259259104728699)
[2025-02-16 11:11:00,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:00,667][root][INFO] - Training Epoch: 1/2, step 1804/23838 completed (loss: 0.8165010213851929, acc: 0.7727272510528564)
[2025-02-16 11:11:00,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:01,138][root][INFO] - Training Epoch: 1/2, step 1805/23838 completed (loss: 0.12177655845880508, acc: 0.9838709831237793)
[2025-02-16 11:11:01,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:01,577][root][INFO] - Training Epoch: 1/2, step 1806/23838 completed (loss: 0.3240220844745636, acc: 0.9230769276618958)
[2025-02-16 11:11:01,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:01,972][root][INFO] - Training Epoch: 1/2, step 1807/23838 completed (loss: 0.46695995330810547, acc: 0.8716577291488647)
[2025-02-16 11:11:02,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:02,620][root][INFO] - Training Epoch: 1/2, step 1808/23838 completed (loss: 0.43628257513046265, acc: 0.8723404407501221)
[2025-02-16 11:11:02,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:03,049][root][INFO] - Training Epoch: 1/2, step 1809/23838 completed (loss: 0.29151952266693115, acc: 0.8909090757369995)
[2025-02-16 11:11:03,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:03,493][root][INFO] - Training Epoch: 1/2, step 1810/23838 completed (loss: 0.5052026510238647, acc: 0.875)
[2025-02-16 11:11:03,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:03,874][root][INFO] - Training Epoch: 1/2, step 1811/23838 completed (loss: 0.441679984331131, acc: 0.9324324131011963)
[2025-02-16 11:11:04,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:04,337][root][INFO] - Training Epoch: 1/2, step 1812/23838 completed (loss: 0.4152272045612335, acc: 0.9069767594337463)
[2025-02-16 11:11:04,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:04,903][root][INFO] - Training Epoch: 1/2, step 1813/23838 completed (loss: 0.3627832531929016, acc: 0.8969072103500366)
[2025-02-16 11:11:05,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:05,338][root][INFO] - Training Epoch: 1/2, step 1814/23838 completed (loss: 0.42364323139190674, acc: 0.8740741014480591)
[2025-02-16 11:11:05,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:05,775][root][INFO] - Training Epoch: 1/2, step 1815/23838 completed (loss: 0.4289313554763794, acc: 0.875)
[2025-02-16 11:11:05,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:06,221][root][INFO] - Training Epoch: 1/2, step 1816/23838 completed (loss: 0.4504203498363495, acc: 0.8728813529014587)
[2025-02-16 11:11:06,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:06,718][root][INFO] - Training Epoch: 1/2, step 1817/23838 completed (loss: 0.4093317985534668, acc: 0.8679245114326477)
[2025-02-16 11:11:06,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:07,128][root][INFO] - Training Epoch: 1/2, step 1818/23838 completed (loss: 0.3927442729473114, acc: 0.931506872177124)
[2025-02-16 11:11:07,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:07,530][root][INFO] - Training Epoch: 1/2, step 1819/23838 completed (loss: 0.693536102771759, acc: 0.8169013857841492)
[2025-02-16 11:11:07,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:07,911][root][INFO] - Training Epoch: 1/2, step 1820/23838 completed (loss: 0.5333359837532043, acc: 0.8299999833106995)
[2025-02-16 11:11:08,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:08,342][root][INFO] - Training Epoch: 1/2, step 1821/23838 completed (loss: 0.41249924898147583, acc: 0.8838709592819214)
[2025-02-16 11:11:08,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:08,718][root][INFO] - Training Epoch: 1/2, step 1822/23838 completed (loss: 1.0643559694290161, acc: 0.7157894968986511)
[2025-02-16 11:11:08,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:09,132][root][INFO] - Training Epoch: 1/2, step 1823/23838 completed (loss: 0.5591582655906677, acc: 0.8229166865348816)
[2025-02-16 11:11:09,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:09,557][root][INFO] - Training Epoch: 1/2, step 1824/23838 completed (loss: 0.7038116455078125, acc: 0.8028169274330139)
[2025-02-16 11:11:09,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:09,992][root][INFO] - Training Epoch: 1/2, step 1825/23838 completed (loss: 1.141406536102295, acc: 0.730434775352478)
[2025-02-16 11:11:10,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:10,465][root][INFO] - Training Epoch: 1/2, step 1826/23838 completed (loss: 0.6389120817184448, acc: 0.8207547068595886)
[2025-02-16 11:11:10,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:10,858][root][INFO] - Training Epoch: 1/2, step 1827/23838 completed (loss: 0.9415886402130127, acc: 0.7551020383834839)
[2025-02-16 11:11:11,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:11,356][root][INFO] - Training Epoch: 1/2, step 1828/23838 completed (loss: 1.0978052616119385, acc: 0.6499999761581421)
[2025-02-16 11:11:11,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:11,806][root][INFO] - Training Epoch: 1/2, step 1829/23838 completed (loss: 0.6986939907073975, acc: 0.7803030014038086)
[2025-02-16 11:11:12,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:12,248][root][INFO] - Training Epoch: 1/2, step 1830/23838 completed (loss: 0.9724782109260559, acc: 0.7212121486663818)
[2025-02-16 11:11:12,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:12,663][root][INFO] - Training Epoch: 1/2, step 1831/23838 completed (loss: 1.9363943338394165, acc: 0.6000000238418579)
[2025-02-16 11:11:12,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:13,065][root][INFO] - Training Epoch: 1/2, step 1832/23838 completed (loss: 1.1970698833465576, acc: 0.6194690465927124)
[2025-02-16 11:11:13,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:13,627][root][INFO] - Training Epoch: 1/2, step 1833/23838 completed (loss: 1.321138620376587, acc: 0.6153846383094788)
[2025-02-16 11:11:13,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:14,020][root][INFO] - Training Epoch: 1/2, step 1834/23838 completed (loss: 1.1170697212219238, acc: 0.6399999856948853)
[2025-02-16 11:11:14,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:14,490][root][INFO] - Training Epoch: 1/2, step 1835/23838 completed (loss: 0.6054136157035828, acc: 0.8380952477455139)
[2025-02-16 11:11:14,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:14,941][root][INFO] - Training Epoch: 1/2, step 1836/23838 completed (loss: 0.7046101093292236, acc: 0.8105263113975525)
[2025-02-16 11:11:15,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:15,365][root][INFO] - Training Epoch: 1/2, step 1837/23838 completed (loss: 0.8608468174934387, acc: 0.7395833134651184)
[2025-02-16 11:11:15,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:15,731][root][INFO] - Training Epoch: 1/2, step 1838/23838 completed (loss: 0.7574958801269531, acc: 0.7589285969734192)
[2025-02-16 11:11:15,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:16,223][root][INFO] - Training Epoch: 1/2, step 1839/23838 completed (loss: 1.0914857387542725, acc: 0.6796116232872009)
[2025-02-16 11:11:16,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:16,598][root][INFO] - Training Epoch: 1/2, step 1840/23838 completed (loss: 1.2191414833068848, acc: 0.6600000262260437)
[2025-02-16 11:11:16,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:17,155][root][INFO] - Training Epoch: 1/2, step 1841/23838 completed (loss: 0.800786554813385, acc: 0.7692307829856873)
[2025-02-16 11:11:17,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:17,611][root][INFO] - Training Epoch: 1/2, step 1842/23838 completed (loss: 0.6193230748176575, acc: 0.8170731663703918)
[2025-02-16 11:11:17,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:18,097][root][INFO] - Training Epoch: 1/2, step 1843/23838 completed (loss: 0.9735909104347229, acc: 0.6938775777816772)
[2025-02-16 11:11:18,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:18,505][root][INFO] - Training Epoch: 1/2, step 1844/23838 completed (loss: 0.936062216758728, acc: 0.7272727489471436)
[2025-02-16 11:11:18,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:18,890][root][INFO] - Training Epoch: 1/2, step 1845/23838 completed (loss: 0.6299660205841064, acc: 0.8571428656578064)
[2025-02-16 11:11:19,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:19,251][root][INFO] - Training Epoch: 1/2, step 1846/23838 completed (loss: 1.41213059425354, acc: 0.6428571343421936)
[2025-02-16 11:11:19,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:19,650][root][INFO] - Training Epoch: 1/2, step 1847/23838 completed (loss: 0.7365692853927612, acc: 0.7594936490058899)
[2025-02-16 11:11:19,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:20,060][root][INFO] - Training Epoch: 1/2, step 1848/23838 completed (loss: 1.2438064813613892, acc: 0.6315789222717285)
[2025-02-16 11:11:20,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:20,420][root][INFO] - Training Epoch: 1/2, step 1849/23838 completed (loss: 0.7962396144866943, acc: 0.782608687877655)
[2025-02-16 11:11:20,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:20,874][root][INFO] - Training Epoch: 1/2, step 1850/23838 completed (loss: 0.9121431112289429, acc: 0.7156862616539001)
[2025-02-16 11:11:21,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:21,323][root][INFO] - Training Epoch: 1/2, step 1851/23838 completed (loss: 0.9482194185256958, acc: 0.6969696879386902)
[2025-02-16 11:11:21,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:21,772][root][INFO] - Training Epoch: 1/2, step 1852/23838 completed (loss: 0.6915446519851685, acc: 0.8198198080062866)
[2025-02-16 11:11:21,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:22,230][root][INFO] - Training Epoch: 1/2, step 1853/23838 completed (loss: 1.0077807903289795, acc: 0.704081654548645)
[2025-02-16 11:11:22,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:23,079][root][INFO] - Training Epoch: 1/2, step 1854/23838 completed (loss: 0.9486359357833862, acc: 0.7322834730148315)
[2025-02-16 11:11:23,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:23,545][root][INFO] - Training Epoch: 1/2, step 1855/23838 completed (loss: 0.9855688214302063, acc: 0.7297297120094299)
[2025-02-16 11:11:23,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:24,008][root][INFO] - Training Epoch: 1/2, step 1856/23838 completed (loss: 0.8743563294410706, acc: 0.75)
[2025-02-16 11:11:24,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:24,420][root][INFO] - Training Epoch: 1/2, step 1857/23838 completed (loss: 0.9084197878837585, acc: 0.7297297120094299)
[2025-02-16 11:11:24,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:24,806][root][INFO] - Training Epoch: 1/2, step 1858/23838 completed (loss: 0.8561018109321594, acc: 0.7434210777282715)
[2025-02-16 11:11:24,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:25,248][root][INFO] - Training Epoch: 1/2, step 1859/23838 completed (loss: 1.388296127319336, acc: 0.6352941393852234)
[2025-02-16 11:11:25,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:25,839][root][INFO] - Training Epoch: 1/2, step 1860/23838 completed (loss: 0.7404992580413818, acc: 0.7916666865348816)
[2025-02-16 11:11:26,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:26,287][root][INFO] - Training Epoch: 1/2, step 1861/23838 completed (loss: 0.8114364743232727, acc: 0.7831325531005859)
[2025-02-16 11:11:26,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:26,657][root][INFO] - Training Epoch: 1/2, step 1862/23838 completed (loss: 1.0181571245193481, acc: 0.7291666865348816)
[2025-02-16 11:11:26,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:27,101][root][INFO] - Training Epoch: 1/2, step 1863/23838 completed (loss: 0.9526271820068359, acc: 0.6811594367027283)
[2025-02-16 11:11:27,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:27,565][root][INFO] - Training Epoch: 1/2, step 1864/23838 completed (loss: 0.5559160113334656, acc: 0.8540145754814148)
[2025-02-16 11:11:27,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:27,975][root][INFO] - Training Epoch: 1/2, step 1865/23838 completed (loss: 0.5890763401985168, acc: 0.8260869383811951)
[2025-02-16 11:11:28,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:28,451][root][INFO] - Training Epoch: 1/2, step 1866/23838 completed (loss: 0.7003525495529175, acc: 0.782608687877655)
[2025-02-16 11:11:28,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:28,894][root][INFO] - Training Epoch: 1/2, step 1867/23838 completed (loss: 0.9675791263580322, acc: 0.70652174949646)
[2025-02-16 11:11:29,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:29,334][root][INFO] - Training Epoch: 1/2, step 1868/23838 completed (loss: 0.31977057456970215, acc: 0.8730158805847168)
[2025-02-16 11:11:29,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:29,958][root][INFO] - Training Epoch: 1/2, step 1869/23838 completed (loss: 0.9092429280281067, acc: 0.7368420958518982)
[2025-02-16 11:11:30,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:30,401][root][INFO] - Training Epoch: 1/2, step 1870/23838 completed (loss: 0.6645837426185608, acc: 0.8360655903816223)
[2025-02-16 11:11:30,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:30,869][root][INFO] - Training Epoch: 1/2, step 1871/23838 completed (loss: 0.59628826379776, acc: 0.8493150472640991)
[2025-02-16 11:11:31,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:31,371][root][INFO] - Training Epoch: 1/2, step 1872/23838 completed (loss: 0.644427478313446, acc: 0.8219178318977356)
[2025-02-16 11:11:31,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:31,840][root][INFO] - Training Epoch: 1/2, step 1873/23838 completed (loss: 0.654468297958374, acc: 0.824999988079071)
[2025-02-16 11:11:32,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:32,250][root][INFO] - Training Epoch: 1/2, step 1874/23838 completed (loss: 0.9054449200630188, acc: 0.7542372941970825)
[2025-02-16 11:11:32,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:32,637][root][INFO] - Training Epoch: 1/2, step 1875/23838 completed (loss: 0.7755800485610962, acc: 0.7971014380455017)
[2025-02-16 11:11:32,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:33,092][root][INFO] - Training Epoch: 1/2, step 1876/23838 completed (loss: 0.552370011806488, acc: 0.7924528121948242)
[2025-02-16 11:11:33,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:33,570][root][INFO] - Training Epoch: 1/2, step 1877/23838 completed (loss: 1.0402039289474487, acc: 0.7272727489471436)
[2025-02-16 11:11:33,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:34,021][root][INFO] - Training Epoch: 1/2, step 1878/23838 completed (loss: 1.1132444143295288, acc: 0.7407407164573669)
[2025-02-16 11:11:34,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:34,525][root][INFO] - Training Epoch: 1/2, step 1879/23838 completed (loss: 0.7013304233551025, acc: 0.7674418687820435)
[2025-02-16 11:11:34,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:34,946][root][INFO] - Training Epoch: 1/2, step 1880/23838 completed (loss: 0.762432873249054, acc: 0.8500000238418579)
[2025-02-16 11:11:35,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:35,343][root][INFO] - Training Epoch: 1/2, step 1881/23838 completed (loss: 1.083053469657898, acc: 0.7142857313156128)
[2025-02-16 11:11:35,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:35,734][root][INFO] - Training Epoch: 1/2, step 1882/23838 completed (loss: 0.8152307868003845, acc: 0.7692307829856873)
[2025-02-16 11:11:35,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:36,205][root][INFO] - Training Epoch: 1/2, step 1883/23838 completed (loss: 1.054294228553772, acc: 0.6779661178588867)
[2025-02-16 11:11:36,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:36,619][root][INFO] - Training Epoch: 1/2, step 1884/23838 completed (loss: 1.211004376411438, acc: 0.6176470518112183)
[2025-02-16 11:11:36,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:37,078][root][INFO] - Training Epoch: 1/2, step 1885/23838 completed (loss: 1.8832730054855347, acc: 0.5777778029441833)
[2025-02-16 11:11:37,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:37,541][root][INFO] - Training Epoch: 1/2, step 1886/23838 completed (loss: 1.1011079549789429, acc: 0.7083333134651184)
[2025-02-16 11:11:37,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:37,913][root][INFO] - Training Epoch: 1/2, step 1887/23838 completed (loss: 0.994156539440155, acc: 0.7021276354789734)
[2025-02-16 11:11:38,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:38,333][root][INFO] - Training Epoch: 1/2, step 1888/23838 completed (loss: 0.919879674911499, acc: 0.7346938848495483)
[2025-02-16 11:11:38,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:38,731][root][INFO] - Training Epoch: 1/2, step 1889/23838 completed (loss: 1.546250581741333, acc: 0.6551724076271057)
[2025-02-16 11:11:38,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:39,229][root][INFO] - Training Epoch: 1/2, step 1890/23838 completed (loss: 1.4306665658950806, acc: 0.5492957830429077)
[2025-02-16 11:11:39,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:39,671][root][INFO] - Training Epoch: 1/2, step 1891/23838 completed (loss: 0.7354342341423035, acc: 0.7435897588729858)
[2025-02-16 11:11:39,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:40,146][root][INFO] - Training Epoch: 1/2, step 1892/23838 completed (loss: 1.047379732131958, acc: 0.6000000238418579)
[2025-02-16 11:11:40,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:40,572][root][INFO] - Training Epoch: 1/2, step 1893/23838 completed (loss: 1.0574426651000977, acc: 0.7857142686843872)
[2025-02-16 11:11:40,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:41,019][root][INFO] - Training Epoch: 1/2, step 1894/23838 completed (loss: 0.7017523646354675, acc: 0.8571428656578064)
[2025-02-16 11:11:41,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:41,381][root][INFO] - Training Epoch: 1/2, step 1895/23838 completed (loss: 1.0225369930267334, acc: 0.6976743936538696)
[2025-02-16 11:11:41,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:41,797][root][INFO] - Training Epoch: 1/2, step 1896/23838 completed (loss: 1.0025341510772705, acc: 0.6451612710952759)
[2025-02-16 11:11:41,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:42,153][root][INFO] - Training Epoch: 1/2, step 1897/23838 completed (loss: 1.1564087867736816, acc: 0.6875)
[2025-02-16 11:11:42,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:42,615][root][INFO] - Training Epoch: 1/2, step 1898/23838 completed (loss: 1.0125203132629395, acc: 0.686274528503418)
[2025-02-16 11:11:42,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:43,014][root][INFO] - Training Epoch: 1/2, step 1899/23838 completed (loss: 0.5025027394294739, acc: 0.7714285850524902)
[2025-02-16 11:11:43,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:43,438][root][INFO] - Training Epoch: 1/2, step 1900/23838 completed (loss: 0.662799060344696, acc: 0.824999988079071)
[2025-02-16 11:11:43,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:43,858][root][INFO] - Training Epoch: 1/2, step 1901/23838 completed (loss: 1.0599452257156372, acc: 0.7894737124443054)
[2025-02-16 11:11:44,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:44,315][root][INFO] - Training Epoch: 1/2, step 1902/23838 completed (loss: 1.9295191764831543, acc: 0.6111111044883728)
[2025-02-16 11:11:44,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:44,713][root][INFO] - Training Epoch: 1/2, step 1903/23838 completed (loss: 0.5775580406188965, acc: 0.78125)
[2025-02-16 11:11:44,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:45,117][root][INFO] - Training Epoch: 1/2, step 1904/23838 completed (loss: 0.6113633513450623, acc: 0.8421052694320679)
[2025-02-16 11:11:45,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:45,548][root][INFO] - Training Epoch: 1/2, step 1905/23838 completed (loss: 1.0814573764801025, acc: 0.6911764740943909)
[2025-02-16 11:11:45,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:46,018][root][INFO] - Training Epoch: 1/2, step 1906/23838 completed (loss: 1.1482608318328857, acc: 0.7142857313156128)
[2025-02-16 11:11:46,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:46,528][root][INFO] - Training Epoch: 1/2, step 1907/23838 completed (loss: 1.422370433807373, acc: 0.642201840877533)
[2025-02-16 11:11:46,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:47,152][root][INFO] - Training Epoch: 1/2, step 1908/23838 completed (loss: 0.6570742726325989, acc: 0.8285714387893677)
[2025-02-16 11:11:47,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:47,648][root][INFO] - Training Epoch: 1/2, step 1909/23838 completed (loss: 0.5079705715179443, acc: 0.8421052694320679)
[2025-02-16 11:11:47,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:48,096][root][INFO] - Training Epoch: 1/2, step 1910/23838 completed (loss: 0.5635957717895508, acc: 0.8241758346557617)
[2025-02-16 11:11:48,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:48,525][root][INFO] - Training Epoch: 1/2, step 1911/23838 completed (loss: 0.8367793560028076, acc: 0.800000011920929)
[2025-02-16 11:11:48,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:48,951][root][INFO] - Training Epoch: 1/2, step 1912/23838 completed (loss: 0.8989822864532471, acc: 0.7017543911933899)
[2025-02-16 11:11:49,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:49,588][root][INFO] - Training Epoch: 1/2, step 1913/23838 completed (loss: 1.2470920085906982, acc: 0.6521739363670349)
[2025-02-16 11:11:49,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:50,002][root][INFO] - Training Epoch: 1/2, step 1914/23838 completed (loss: 1.2245291471481323, acc: 0.7419354915618896)
[2025-02-16 11:11:50,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:50,436][root][INFO] - Training Epoch: 1/2, step 1915/23838 completed (loss: 0.7959765195846558, acc: 0.7285714149475098)
[2025-02-16 11:11:50,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:50,856][root][INFO] - Training Epoch: 1/2, step 1916/23838 completed (loss: 0.8797414302825928, acc: 0.7183098793029785)
[2025-02-16 11:11:51,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:51,181][root][INFO] - Training Epoch: 1/2, step 1917/23838 completed (loss: 0.7363619804382324, acc: 0.8163265585899353)
[2025-02-16 11:11:51,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:51,917][root][INFO] - Training Epoch: 1/2, step 1918/23838 completed (loss: 1.1305593252182007, acc: 0.6867470145225525)
[2025-02-16 11:11:52,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:52,293][root][INFO] - Training Epoch: 1/2, step 1919/23838 completed (loss: 0.571488618850708, acc: 0.875)
[2025-02-16 11:11:52,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:52,752][root][INFO] - Training Epoch: 1/2, step 1920/23838 completed (loss: 0.5582756996154785, acc: 0.8372092843055725)
[2025-02-16 11:11:52,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:53,198][root][INFO] - Training Epoch: 1/2, step 1921/23838 completed (loss: 0.5547578930854797, acc: 0.8933333158493042)
[2025-02-16 11:11:53,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:53,920][root][INFO] - Training Epoch: 1/2, step 1922/23838 completed (loss: 1.0105082988739014, acc: 0.7464788556098938)
[2025-02-16 11:11:54,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:54,342][root][INFO] - Training Epoch: 1/2, step 1923/23838 completed (loss: 0.689276397228241, acc: 0.7708333134651184)
[2025-02-16 11:11:54,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:55,024][root][INFO] - Training Epoch: 1/2, step 1924/23838 completed (loss: 0.890859842300415, acc: 0.698113203048706)
[2025-02-16 11:11:55,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:55,416][root][INFO] - Training Epoch: 1/2, step 1925/23838 completed (loss: 0.38582390546798706, acc: 0.925000011920929)
[2025-02-16 11:11:55,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:55,818][root][INFO] - Training Epoch: 1/2, step 1926/23838 completed (loss: 0.8590602278709412, acc: 0.8148148059844971)
[2025-02-16 11:11:56,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:56,305][root][INFO] - Training Epoch: 1/2, step 1927/23838 completed (loss: 0.8764634132385254, acc: 0.7460317611694336)
[2025-02-16 11:11:56,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:56,726][root][INFO] - Training Epoch: 1/2, step 1928/23838 completed (loss: 0.6027795076370239, acc: 0.84375)
[2025-02-16 11:11:56,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:57,237][root][INFO] - Training Epoch: 1/2, step 1929/23838 completed (loss: 0.5492844581604004, acc: 0.8939393758773804)
[2025-02-16 11:11:57,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:57,661][root][INFO] - Training Epoch: 1/2, step 1930/23838 completed (loss: 1.1481412649154663, acc: 0.692307710647583)
[2025-02-16 11:11:57,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:58,040][root][INFO] - Training Epoch: 1/2, step 1931/23838 completed (loss: 0.5543473958969116, acc: 0.8444444537162781)
[2025-02-16 11:11:58,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:58,512][root][INFO] - Training Epoch: 1/2, step 1932/23838 completed (loss: 0.9593018293380737, acc: 0.8095238208770752)
[2025-02-16 11:11:58,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:58,886][root][INFO] - Training Epoch: 1/2, step 1933/23838 completed (loss: 0.5926239490509033, acc: 0.8510638475418091)
[2025-02-16 11:11:59,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:59,313][root][INFO] - Training Epoch: 1/2, step 1934/23838 completed (loss: 1.3686658143997192, acc: 0.6285714507102966)
[2025-02-16 11:11:59,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:11:59,700][root][INFO] - Training Epoch: 1/2, step 1935/23838 completed (loss: 0.8125374913215637, acc: 0.8478260636329651)
[2025-02-16 11:11:59,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:00,138][root][INFO] - Training Epoch: 1/2, step 1936/23838 completed (loss: 1.0073710680007935, acc: 0.7555555701255798)
[2025-02-16 11:12:00,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:00,567][root][INFO] - Training Epoch: 1/2, step 1937/23838 completed (loss: 0.770908772945404, acc: 0.7678571343421936)
[2025-02-16 11:12:00,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:00,954][root][INFO] - Training Epoch: 1/2, step 1938/23838 completed (loss: 1.7536941766738892, acc: 0.59375)
[2025-02-16 11:12:01,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:01,411][root][INFO] - Training Epoch: 1/2, step 1939/23838 completed (loss: 0.9793874025344849, acc: 0.7948718070983887)
[2025-02-16 11:12:01,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:01,887][root][INFO] - Training Epoch: 1/2, step 1940/23838 completed (loss: 1.8740915060043335, acc: 0.529411792755127)
[2025-02-16 11:12:02,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:02,367][root][INFO] - Training Epoch: 1/2, step 1941/23838 completed (loss: 0.8595372438430786, acc: 0.7978723645210266)
[2025-02-16 11:12:02,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:03,421][root][INFO] - Training Epoch: 1/2, step 1942/23838 completed (loss: 0.5661715865135193, acc: 0.8090909123420715)
[2025-02-16 11:12:03,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:03,848][root][INFO] - Training Epoch: 1/2, step 1943/23838 completed (loss: 0.6552883982658386, acc: 0.9047619104385376)
[2025-02-16 11:12:04,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:04,265][root][INFO] - Training Epoch: 1/2, step 1944/23838 completed (loss: 0.9507772326469421, acc: 0.800000011920929)
[2025-02-16 11:12:04,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:04,649][root][INFO] - Training Epoch: 1/2, step 1945/23838 completed (loss: 1.0109732151031494, acc: 0.8095238208770752)
[2025-02-16 11:12:04,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:05,077][root][INFO] - Training Epoch: 1/2, step 1946/23838 completed (loss: 1.3478597402572632, acc: 0.5714285969734192)
[2025-02-16 11:12:05,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:05,518][root][INFO] - Training Epoch: 1/2, step 1947/23838 completed (loss: 1.4812779426574707, acc: 0.5909090638160706)
[2025-02-16 11:12:05,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:05,991][root][INFO] - Training Epoch: 1/2, step 1948/23838 completed (loss: 0.7074549198150635, acc: 0.7727272510528564)
[2025-02-16 11:12:06,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:06,392][root][INFO] - Training Epoch: 1/2, step 1949/23838 completed (loss: 1.0976184606552124, acc: 0.7368420958518982)
[2025-02-16 11:12:06,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:06,758][root][INFO] - Training Epoch: 1/2, step 1950/23838 completed (loss: 0.7926338315010071, acc: 0.6666666865348816)
[2025-02-16 11:12:06,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:07,185][root][INFO] - Training Epoch: 1/2, step 1951/23838 completed (loss: 0.4126978814601898, acc: 0.8260869383811951)
[2025-02-16 11:12:07,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:07,606][root][INFO] - Training Epoch: 1/2, step 1952/23838 completed (loss: 0.4368356466293335, acc: 0.875)
[2025-02-16 11:12:07,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:08,027][root][INFO] - Training Epoch: 1/2, step 1953/23838 completed (loss: 0.8440933227539062, acc: 0.800000011920929)
[2025-02-16 11:12:08,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:08,378][root][INFO] - Training Epoch: 1/2, step 1954/23838 completed (loss: 0.7125681042671204, acc: 0.739130437374115)
[2025-02-16 11:12:08,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:08,772][root][INFO] - Training Epoch: 1/2, step 1955/23838 completed (loss: 0.8902900815010071, acc: 0.800000011920929)
[2025-02-16 11:12:08,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:09,170][root][INFO] - Training Epoch: 1/2, step 1956/23838 completed (loss: 1.181816816329956, acc: 0.6842105388641357)
[2025-02-16 11:12:09,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:09,641][root][INFO] - Training Epoch: 1/2, step 1957/23838 completed (loss: 1.0137239694595337, acc: 0.5789473652839661)
[2025-02-16 11:12:09,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:09,999][root][INFO] - Training Epoch: 1/2, step 1958/23838 completed (loss: 1.2628082036972046, acc: 0.761904776096344)
[2025-02-16 11:12:10,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:10,398][root][INFO] - Training Epoch: 1/2, step 1959/23838 completed (loss: 0.4479072093963623, acc: 0.8358209133148193)
[2025-02-16 11:12:10,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:10,763][root][INFO] - Training Epoch: 1/2, step 1960/23838 completed (loss: 2.3926491737365723, acc: 0.5945945978164673)
[2025-02-16 11:12:10,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:11,196][root][INFO] - Training Epoch: 1/2, step 1961/23838 completed (loss: 0.5288138389587402, acc: 0.8478260636329651)
[2025-02-16 11:12:11,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:11,787][root][INFO] - Training Epoch: 1/2, step 1962/23838 completed (loss: 1.5248854160308838, acc: 0.5714285969734192)
[2025-02-16 11:12:11,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:12,164][root][INFO] - Training Epoch: 1/2, step 1963/23838 completed (loss: 0.7816959023475647, acc: 0.7361111044883728)
[2025-02-16 11:12:12,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:12,650][root][INFO] - Training Epoch: 1/2, step 1964/23838 completed (loss: 0.8087665438652039, acc: 0.7252747416496277)
[2025-02-16 11:12:12,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:13,273][root][INFO] - Training Epoch: 1/2, step 1965/23838 completed (loss: 0.9153571724891663, acc: 0.7633587718009949)
[2025-02-16 11:12:13,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:13,780][root][INFO] - Training Epoch: 1/2, step 1966/23838 completed (loss: 0.6961039900779724, acc: 0.8333333134651184)
[2025-02-16 11:12:14,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:14,566][root][INFO] - Training Epoch: 1/2, step 1967/23838 completed (loss: 0.8539615273475647, acc: 0.7235772609710693)
[2025-02-16 11:12:14,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:15,347][root][INFO] - Training Epoch: 1/2, step 1968/23838 completed (loss: 1.3818405866622925, acc: 0.6712328791618347)
[2025-02-16 11:12:15,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:15,786][root][INFO] - Training Epoch: 1/2, step 1969/23838 completed (loss: 0.47381478548049927, acc: 0.8676470518112183)
[2025-02-16 11:12:15,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:16,216][root][INFO] - Training Epoch: 1/2, step 1970/23838 completed (loss: 0.6715489029884338, acc: 0.868852436542511)
[2025-02-16 11:12:16,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:16,737][root][INFO] - Training Epoch: 1/2, step 1971/23838 completed (loss: 0.6539998650550842, acc: 0.8350515365600586)
[2025-02-16 11:12:16,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:17,165][root][INFO] - Training Epoch: 1/2, step 1972/23838 completed (loss: 0.5347135066986084, acc: 0.8666666746139526)
[2025-02-16 11:12:17,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:17,626][root][INFO] - Training Epoch: 1/2, step 1973/23838 completed (loss: 0.623664915561676, acc: 0.8461538553237915)
[2025-02-16 11:12:17,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:18,068][root][INFO] - Training Epoch: 1/2, step 1974/23838 completed (loss: 0.9738982915878296, acc: 0.7808219194412231)
[2025-02-16 11:12:18,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:18,501][root][INFO] - Training Epoch: 1/2, step 1975/23838 completed (loss: 0.30569106340408325, acc: 0.9324324131011963)
[2025-02-16 11:12:18,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:18,967][root][INFO] - Training Epoch: 1/2, step 1976/23838 completed (loss: 0.9931793808937073, acc: 0.7636363506317139)
[2025-02-16 11:12:19,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:19,396][root][INFO] - Training Epoch: 1/2, step 1977/23838 completed (loss: 0.8763672113418579, acc: 0.734375)
[2025-02-16 11:12:19,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:19,815][root][INFO] - Training Epoch: 1/2, step 1978/23838 completed (loss: 1.8473215103149414, acc: 0.4642857015132904)
[2025-02-16 11:12:19,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:20,164][root][INFO] - Training Epoch: 1/2, step 1979/23838 completed (loss: 2.721930503845215, acc: 0.5)
[2025-02-16 11:12:20,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:20,583][root][INFO] - Training Epoch: 1/2, step 1980/23838 completed (loss: 0.7588411569595337, acc: 0.761904776096344)
[2025-02-16 11:12:20,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:20,981][root][INFO] - Training Epoch: 1/2, step 1981/23838 completed (loss: 1.4170271158218384, acc: 0.5454545617103577)
[2025-02-16 11:12:21,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:21,402][root][INFO] - Training Epoch: 1/2, step 1982/23838 completed (loss: 1.4597808122634888, acc: 0.5897436141967773)
[2025-02-16 11:12:21,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:21,813][root][INFO] - Training Epoch: 1/2, step 1983/23838 completed (loss: 0.8681370615959167, acc: 0.7647058963775635)
[2025-02-16 11:12:21,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:22,232][root][INFO] - Training Epoch: 1/2, step 1984/23838 completed (loss: 0.8310622572898865, acc: 0.75)
[2025-02-16 11:12:22,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:22,687][root][INFO] - Training Epoch: 1/2, step 1985/23838 completed (loss: 1.0823978185653687, acc: 0.6078431606292725)
[2025-02-16 11:12:22,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:23,101][root][INFO] - Training Epoch: 1/2, step 1986/23838 completed (loss: 1.4387637376785278, acc: 0.5555555820465088)
[2025-02-16 11:12:23,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:23,566][root][INFO] - Training Epoch: 1/2, step 1987/23838 completed (loss: 0.9984028935432434, acc: 0.6521739363670349)
[2025-02-16 11:12:23,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:23,989][root][INFO] - Training Epoch: 1/2, step 1988/23838 completed (loss: 0.7223461270332336, acc: 0.8095238208770752)
[2025-02-16 11:12:24,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:24,354][root][INFO] - Training Epoch: 1/2, step 1989/23838 completed (loss: 1.3062776327133179, acc: 0.6060606241226196)
[2025-02-16 11:12:24,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:24,802][root][INFO] - Training Epoch: 1/2, step 1990/23838 completed (loss: 1.223296880722046, acc: 0.6833333373069763)
[2025-02-16 11:12:24,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:25,210][root][INFO] - Training Epoch: 1/2, step 1991/23838 completed (loss: 0.591560423374176, acc: 0.8023256063461304)
[2025-02-16 11:12:25,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:25,668][root][INFO] - Training Epoch: 1/2, step 1992/23838 completed (loss: 0.7604717016220093, acc: 0.7777777910232544)
[2025-02-16 11:12:25,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:26,105][root][INFO] - Training Epoch: 1/2, step 1993/23838 completed (loss: 0.9093248248100281, acc: 0.7755101919174194)
[2025-02-16 11:12:26,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:26,875][root][INFO] - Training Epoch: 1/2, step 1994/23838 completed (loss: 0.7693291902542114, acc: 0.800000011920929)
[2025-02-16 11:12:27,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:27,320][root][INFO] - Training Epoch: 1/2, step 1995/23838 completed (loss: 1.0238808393478394, acc: 0.6969696879386902)
[2025-02-16 11:12:27,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:27,697][root][INFO] - Training Epoch: 1/2, step 1996/23838 completed (loss: 1.5389995574951172, acc: 0.5822784900665283)
[2025-02-16 11:12:27,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:28,123][root][INFO] - Training Epoch: 1/2, step 1997/23838 completed (loss: 1.0914340019226074, acc: 0.692307710647583)
[2025-02-16 11:12:28,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:28,574][root][INFO] - Training Epoch: 1/2, step 1998/23838 completed (loss: 1.5350755453109741, acc: 0.6176470518112183)
[2025-02-16 11:12:28,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:28,965][root][INFO] - Training Epoch: 1/2, step 1999/23838 completed (loss: 1.010815978050232, acc: 0.7096773982048035)
[2025-02-16 11:12:29,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:29,345][root][INFO] - Training Epoch: 1/2, step 2000/23838 completed (loss: 1.4808523654937744, acc: 0.5833333134651184)
[2025-02-16 11:12:29,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:29,813][root][INFO] - Training Epoch: 1/2, step 2001/23838 completed (loss: 1.4151248931884766, acc: 0.6282051205635071)
[2025-02-16 11:12:29,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:30,229][root][INFO] - Training Epoch: 1/2, step 2002/23838 completed (loss: 0.8140235543251038, acc: 0.7260273694992065)
[2025-02-16 11:12:30,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:30,699][root][INFO] - Training Epoch: 1/2, step 2003/23838 completed (loss: 0.7435786128044128, acc: 0.7770270109176636)
[2025-02-16 11:12:30,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:31,077][root][INFO] - Training Epoch: 1/2, step 2004/23838 completed (loss: 0.7503334879875183, acc: 0.7803030014038086)
[2025-02-16 11:12:31,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:31,450][root][INFO] - Training Epoch: 1/2, step 2005/23838 completed (loss: 0.9171866774559021, acc: 0.7835051417350769)
[2025-02-16 11:12:31,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:31,882][root][INFO] - Training Epoch: 1/2, step 2006/23838 completed (loss: 0.6660298109054565, acc: 0.817307710647583)
[2025-02-16 11:12:32,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:32,367][root][INFO] - Training Epoch: 1/2, step 2007/23838 completed (loss: 1.0833461284637451, acc: 0.6703296899795532)
[2025-02-16 11:12:32,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:32,835][root][INFO] - Training Epoch: 1/2, step 2008/23838 completed (loss: 0.3329789936542511, acc: 0.9175257682800293)
[2025-02-16 11:12:33,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:33,310][root][INFO] - Training Epoch: 1/2, step 2009/23838 completed (loss: 0.8101038336753845, acc: 0.75)
[2025-02-16 11:12:33,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:33,784][root][INFO] - Training Epoch: 1/2, step 2010/23838 completed (loss: 0.7182257771492004, acc: 0.7731092572212219)
[2025-02-16 11:12:33,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:34,226][root][INFO] - Training Epoch: 1/2, step 2011/23838 completed (loss: 0.6337791681289673, acc: 0.77173912525177)
[2025-02-16 11:12:34,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:34,727][root][INFO] - Training Epoch: 1/2, step 2012/23838 completed (loss: 0.742839515209198, acc: 0.7628205418586731)
[2025-02-16 11:12:34,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:35,132][root][INFO] - Training Epoch: 1/2, step 2013/23838 completed (loss: 0.6407949924468994, acc: 0.8299999833106995)
[2025-02-16 11:12:35,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:35,580][root][INFO] - Training Epoch: 1/2, step 2014/23838 completed (loss: 0.7242130637168884, acc: 0.7720588445663452)
[2025-02-16 11:12:35,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:36,019][root][INFO] - Training Epoch: 1/2, step 2015/23838 completed (loss: 0.944811224937439, acc: 0.6851851940155029)
[2025-02-16 11:12:36,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:36,509][root][INFO] - Training Epoch: 1/2, step 2016/23838 completed (loss: 0.7157860398292542, acc: 0.7692307829856873)
[2025-02-16 11:12:36,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:36,924][root][INFO] - Training Epoch: 1/2, step 2017/23838 completed (loss: 0.49544885754585266, acc: 0.8362069129943848)
[2025-02-16 11:12:37,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:37,405][root][INFO] - Training Epoch: 1/2, step 2018/23838 completed (loss: 0.5733325481414795, acc: 0.8407079577445984)
[2025-02-16 11:12:37,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:37,804][root][INFO] - Training Epoch: 1/2, step 2019/23838 completed (loss: 0.6485836505889893, acc: 0.8169934749603271)
[2025-02-16 11:12:38,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:38,247][root][INFO] - Training Epoch: 1/2, step 2020/23838 completed (loss: 0.9031437039375305, acc: 0.7631579041481018)
[2025-02-16 11:12:38,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:38,698][root][INFO] - Training Epoch: 1/2, step 2021/23838 completed (loss: 0.68390291929245, acc: 0.7473683953285217)
[2025-02-16 11:12:38,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:39,132][root][INFO] - Training Epoch: 1/2, step 2022/23838 completed (loss: 0.5534620881080627, acc: 0.8536585569381714)
[2025-02-16 11:12:39,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:39,534][root][INFO] - Training Epoch: 1/2, step 2023/23838 completed (loss: 0.5688617825508118, acc: 0.8541666865348816)
[2025-02-16 11:12:39,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:39,988][root][INFO] - Training Epoch: 1/2, step 2024/23838 completed (loss: 0.41922950744628906, acc: 0.9021739363670349)
[2025-02-16 11:12:40,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:40,432][root][INFO] - Training Epoch: 1/2, step 2025/23838 completed (loss: 0.480375736951828, acc: 0.8787878751754761)
[2025-02-16 11:12:40,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:40,860][root][INFO] - Training Epoch: 1/2, step 2026/23838 completed (loss: 0.7388198375701904, acc: 0.7446808218955994)
[2025-02-16 11:12:41,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:41,287][root][INFO] - Training Epoch: 1/2, step 2027/23838 completed (loss: 0.6269233226776123, acc: 0.78125)
[2025-02-16 11:12:41,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:41,668][root][INFO] - Training Epoch: 1/2, step 2028/23838 completed (loss: 0.433315247297287, acc: 0.9411764740943909)
[2025-02-16 11:12:41,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:42,079][root][INFO] - Training Epoch: 1/2, step 2029/23838 completed (loss: 0.6217449307441711, acc: 0.796875)
[2025-02-16 11:12:42,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:42,526][root][INFO] - Training Epoch: 1/2, step 2030/23838 completed (loss: 0.38225361704826355, acc: 0.8942307829856873)
[2025-02-16 11:12:42,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:42,986][root][INFO] - Training Epoch: 1/2, step 2031/23838 completed (loss: 0.5603975653648376, acc: 0.8623853325843811)
[2025-02-16 11:12:43,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:43,423][root][INFO] - Training Epoch: 1/2, step 2032/23838 completed (loss: 0.528123140335083, acc: 0.8444444537162781)
[2025-02-16 11:12:43,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:43,901][root][INFO] - Training Epoch: 1/2, step 2033/23838 completed (loss: 0.6748288869857788, acc: 0.8629032373428345)
[2025-02-16 11:12:44,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:44,356][root][INFO] - Training Epoch: 1/2, step 2034/23838 completed (loss: 0.33419567346572876, acc: 0.9397590160369873)
[2025-02-16 11:12:44,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:44,814][root][INFO] - Training Epoch: 1/2, step 2035/23838 completed (loss: 0.30363503098487854, acc: 0.9322034120559692)
[2025-02-16 11:12:45,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:45,258][root][INFO] - Training Epoch: 1/2, step 2036/23838 completed (loss: 0.5889497399330139, acc: 0.8279569745063782)
[2025-02-16 11:12:45,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:45,651][root][INFO] - Training Epoch: 1/2, step 2037/23838 completed (loss: 0.5442768931388855, acc: 0.8024691343307495)
[2025-02-16 11:12:45,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:46,131][root][INFO] - Training Epoch: 1/2, step 2038/23838 completed (loss: 0.7803955078125, acc: 0.7821782231330872)
[2025-02-16 11:12:46,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:46,600][root][INFO] - Training Epoch: 1/2, step 2039/23838 completed (loss: 0.5430329442024231, acc: 0.859649121761322)
[2025-02-16 11:12:46,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:46,985][root][INFO] - Training Epoch: 1/2, step 2040/23838 completed (loss: 0.642425537109375, acc: 0.8518518805503845)
[2025-02-16 11:12:47,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:47,428][root][INFO] - Training Epoch: 1/2, step 2041/23838 completed (loss: 0.4448547959327698, acc: 0.8731343150138855)
[2025-02-16 11:12:47,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:47,818][root][INFO] - Training Epoch: 1/2, step 2042/23838 completed (loss: 0.5688340067863464, acc: 0.8333333134651184)
[2025-02-16 11:12:48,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:48,276][root][INFO] - Training Epoch: 1/2, step 2043/23838 completed (loss: 0.40432125329971313, acc: 0.9111111164093018)
[2025-02-16 11:12:48,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:48,731][root][INFO] - Training Epoch: 1/2, step 2044/23838 completed (loss: 0.6121291518211365, acc: 0.8139534592628479)
[2025-02-16 11:12:48,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:49,195][root][INFO] - Training Epoch: 1/2, step 2045/23838 completed (loss: 0.8745236992835999, acc: 0.7567567825317383)
[2025-02-16 11:12:49,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:49,592][root][INFO] - Training Epoch: 1/2, step 2046/23838 completed (loss: 0.3354727327823639, acc: 0.9024389982223511)
[2025-02-16 11:12:49,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:49,966][root][INFO] - Training Epoch: 1/2, step 2047/23838 completed (loss: 0.587410569190979, acc: 0.834782600402832)
[2025-02-16 11:12:50,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:50,378][root][INFO] - Training Epoch: 1/2, step 2048/23838 completed (loss: 0.5305175185203552, acc: 0.8379888534545898)
[2025-02-16 11:12:50,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:50,818][root][INFO] - Training Epoch: 1/2, step 2049/23838 completed (loss: 0.531554102897644, acc: 0.8479999899864197)
[2025-02-16 11:12:50,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:51,188][root][INFO] - Training Epoch: 1/2, step 2050/23838 completed (loss: 0.7960429191589355, acc: 0.7851851582527161)
[2025-02-16 11:12:51,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:51,581][root][INFO] - Training Epoch: 1/2, step 2051/23838 completed (loss: 0.6409148573875427, acc: 0.875)
[2025-02-16 11:12:51,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:52,020][root][INFO] - Training Epoch: 1/2, step 2052/23838 completed (loss: 0.7004157304763794, acc: 0.8032786846160889)
[2025-02-16 11:12:52,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:52,469][root][INFO] - Training Epoch: 1/2, step 2053/23838 completed (loss: 0.276716023683548, acc: 0.8999999761581421)
[2025-02-16 11:12:52,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:52,862][root][INFO] - Training Epoch: 1/2, step 2054/23838 completed (loss: 0.34447699785232544, acc: 0.875)
[2025-02-16 11:12:53,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:53,238][root][INFO] - Training Epoch: 1/2, step 2055/23838 completed (loss: 0.2059943974018097, acc: 0.95652174949646)
[2025-02-16 11:12:53,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:53,613][root][INFO] - Training Epoch: 1/2, step 2056/23838 completed (loss: 0.3747539818286896, acc: 0.9200000166893005)
[2025-02-16 11:12:53,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:54,060][root][INFO] - Training Epoch: 1/2, step 2057/23838 completed (loss: 0.29376867413520813, acc: 0.9178082346916199)
[2025-02-16 11:12:54,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:54,398][root][INFO] - Training Epoch: 1/2, step 2058/23838 completed (loss: 0.43938037753105164, acc: 0.8974359035491943)
[2025-02-16 11:12:54,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:54,805][root][INFO] - Training Epoch: 1/2, step 2059/23838 completed (loss: 0.25911492109298706, acc: 0.9545454382896423)
[2025-02-16 11:12:55,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:55,264][root][INFO] - Training Epoch: 1/2, step 2060/23838 completed (loss: 0.28930041193962097, acc: 0.9237288236618042)
[2025-02-16 11:12:55,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:55,685][root][INFO] - Training Epoch: 1/2, step 2061/23838 completed (loss: 0.25197383761405945, acc: 0.9111111164093018)
[2025-02-16 11:12:55,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:56,105][root][INFO] - Training Epoch: 1/2, step 2062/23838 completed (loss: 0.1682734489440918, acc: 0.9530201554298401)
[2025-02-16 11:12:56,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:56,522][root][INFO] - Training Epoch: 1/2, step 2063/23838 completed (loss: 0.19218730926513672, acc: 0.9375)
[2025-02-16 11:12:56,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:57,001][root][INFO] - Training Epoch: 1/2, step 2064/23838 completed (loss: 0.5094074010848999, acc: 0.8588235378265381)
[2025-02-16 11:12:57,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:57,426][root][INFO] - Training Epoch: 1/2, step 2065/23838 completed (loss: 0.14478139579296112, acc: 0.9620253443717957)
[2025-02-16 11:12:57,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:57,814][root][INFO] - Training Epoch: 1/2, step 2066/23838 completed (loss: 0.2462688386440277, acc: 0.9191918969154358)
[2025-02-16 11:12:57,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:58,229][root][INFO] - Training Epoch: 1/2, step 2067/23838 completed (loss: 0.4127528965473175, acc: 0.8796992301940918)
[2025-02-16 11:12:58,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:58,658][root][INFO] - Training Epoch: 1/2, step 2068/23838 completed (loss: 0.32715556025505066, acc: 0.949999988079071)
[2025-02-16 11:12:58,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:59,015][root][INFO] - Training Epoch: 1/2, step 2069/23838 completed (loss: 0.5193875432014465, acc: 0.875)
[2025-02-16 11:12:59,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:59,447][root][INFO] - Training Epoch: 1/2, step 2070/23838 completed (loss: 0.6327440738677979, acc: 0.8630136847496033)
[2025-02-16 11:12:59,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:12:59,861][root][INFO] - Training Epoch: 1/2, step 2071/23838 completed (loss: 0.37552911043167114, acc: 0.9090909361839294)
[2025-02-16 11:13:00,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:00,319][root][INFO] - Training Epoch: 1/2, step 2072/23838 completed (loss: 0.5056858658790588, acc: 0.8571428656578064)
[2025-02-16 11:13:00,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:00,769][root][INFO] - Training Epoch: 1/2, step 2073/23838 completed (loss: 0.48514315485954285, acc: 0.9029126167297363)
[2025-02-16 11:13:00,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:01,245][root][INFO] - Training Epoch: 1/2, step 2074/23838 completed (loss: 0.3783627152442932, acc: 0.8823529481887817)
[2025-02-16 11:13:01,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:01,687][root][INFO] - Training Epoch: 1/2, step 2075/23838 completed (loss: 0.6052157878875732, acc: 0.8636363744735718)
[2025-02-16 11:13:01,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:02,074][root][INFO] - Training Epoch: 1/2, step 2076/23838 completed (loss: 0.3459276556968689, acc: 0.8924731016159058)
[2025-02-16 11:13:02,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:02,458][root][INFO] - Training Epoch: 1/2, step 2077/23838 completed (loss: 0.6614188551902771, acc: 0.8421052694320679)
[2025-02-16 11:13:02,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:03,245][root][INFO] - Training Epoch: 1/2, step 2078/23838 completed (loss: 0.34914323687553406, acc: 0.895061731338501)
[2025-02-16 11:13:03,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:03,666][root][INFO] - Training Epoch: 1/2, step 2079/23838 completed (loss: 0.4182153642177582, acc: 0.8909090757369995)
[2025-02-16 11:13:03,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:04,087][root][INFO] - Training Epoch: 1/2, step 2080/23838 completed (loss: 0.4202178418636322, acc: 0.8829787373542786)
[2025-02-16 11:13:04,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:04,497][root][INFO] - Training Epoch: 1/2, step 2081/23838 completed (loss: 0.5881444215774536, acc: 0.8374999761581421)
[2025-02-16 11:13:04,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:05,003][root][INFO] - Training Epoch: 1/2, step 2082/23838 completed (loss: 1.498786211013794, acc: 0.560606062412262)
[2025-02-16 11:13:05,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:05,469][root][INFO] - Training Epoch: 1/2, step 2083/23838 completed (loss: 0.5302236676216125, acc: 0.8085106611251831)
[2025-02-16 11:13:05,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:05,930][root][INFO] - Training Epoch: 1/2, step 2084/23838 completed (loss: 0.6710410118103027, acc: 0.7435897588729858)
[2025-02-16 11:13:06,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:06,429][root][INFO] - Training Epoch: 1/2, step 2085/23838 completed (loss: 0.597194254398346, acc: 0.8636363744735718)
[2025-02-16 11:13:06,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:06,977][root][INFO] - Training Epoch: 1/2, step 2086/23838 completed (loss: 1.1405831575393677, acc: 0.738095223903656)
[2025-02-16 11:13:07,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:07,419][root][INFO] - Training Epoch: 1/2, step 2087/23838 completed (loss: 1.0306235551834106, acc: 0.7166666388511658)
[2025-02-16 11:13:07,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:07,810][root][INFO] - Training Epoch: 1/2, step 2088/23838 completed (loss: 0.5534319877624512, acc: 0.8214285969734192)
[2025-02-16 11:13:07,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:08,239][root][INFO] - Training Epoch: 1/2, step 2089/23838 completed (loss: 0.6862233281135559, acc: 0.8305084705352783)
[2025-02-16 11:13:08,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:08,633][root][INFO] - Training Epoch: 1/2, step 2090/23838 completed (loss: 1.1743593215942383, acc: 0.6896551847457886)
[2025-02-16 11:13:08,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:09,037][root][INFO] - Training Epoch: 1/2, step 2091/23838 completed (loss: 1.257349967956543, acc: 0.6571428775787354)
[2025-02-16 11:13:09,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:09,408][root][INFO] - Training Epoch: 1/2, step 2092/23838 completed (loss: 1.3681539297103882, acc: 0.6282051205635071)
[2025-02-16 11:13:09,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:09,816][root][INFO] - Training Epoch: 1/2, step 2093/23838 completed (loss: 1.3115438222885132, acc: 0.6354166865348816)
[2025-02-16 11:13:09,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:10,215][root][INFO] - Training Epoch: 1/2, step 2094/23838 completed (loss: 0.9492563605308533, acc: 0.7083333134651184)
[2025-02-16 11:13:10,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:10,711][root][INFO] - Training Epoch: 1/2, step 2095/23838 completed (loss: 1.1871798038482666, acc: 0.7250000238418579)
[2025-02-16 11:13:10,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:11,154][root][INFO] - Training Epoch: 1/2, step 2096/23838 completed (loss: 1.097381353378296, acc: 0.7042253613471985)
[2025-02-16 11:13:11,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:11,600][root][INFO] - Training Epoch: 1/2, step 2097/23838 completed (loss: 0.5854001641273499, acc: 0.8105263113975525)
[2025-02-16 11:13:11,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:12,124][root][INFO] - Training Epoch: 1/2, step 2098/23838 completed (loss: 1.3056786060333252, acc: 0.6800000071525574)
[2025-02-16 11:13:12,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:12,630][root][INFO] - Training Epoch: 1/2, step 2099/23838 completed (loss: 0.8269171118736267, acc: 0.7542372941970825)
[2025-02-16 11:13:12,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:13,115][root][INFO] - Training Epoch: 1/2, step 2100/23838 completed (loss: 0.6210253834724426, acc: 0.8730158805847168)
[2025-02-16 11:13:13,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:13,625][root][INFO] - Training Epoch: 1/2, step 2101/23838 completed (loss: 0.616740882396698, acc: 0.841269850730896)
[2025-02-16 11:13:13,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:14,105][root][INFO] - Training Epoch: 1/2, step 2102/23838 completed (loss: 0.8105118274688721, acc: 0.76106196641922)
[2025-02-16 11:13:14,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:14,769][root][INFO] - Training Epoch: 1/2, step 2103/23838 completed (loss: 0.7029140591621399, acc: 0.7739130258560181)
[2025-02-16 11:13:14,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:15,204][root][INFO] - Training Epoch: 1/2, step 2104/23838 completed (loss: 0.6055803894996643, acc: 0.8392857313156128)
[2025-02-16 11:13:15,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:15,611][root][INFO] - Training Epoch: 1/2, step 2105/23838 completed (loss: 1.097808837890625, acc: 0.7216494679450989)
[2025-02-16 11:13:15,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:16,000][root][INFO] - Training Epoch: 1/2, step 2106/23838 completed (loss: 1.1725999116897583, acc: 0.7254902124404907)
[2025-02-16 11:13:16,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:16,434][root][INFO] - Training Epoch: 1/2, step 2107/23838 completed (loss: 1.0059207677841187, acc: 0.75)
[2025-02-16 11:13:16,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:16,899][root][INFO] - Training Epoch: 1/2, step 2108/23838 completed (loss: 1.005142331123352, acc: 0.7364341020584106)
[2025-02-16 11:13:17,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:17,298][root][INFO] - Training Epoch: 1/2, step 2109/23838 completed (loss: 0.9217689037322998, acc: 0.7476635575294495)
[2025-02-16 11:13:17,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:17,900][root][INFO] - Training Epoch: 1/2, step 2110/23838 completed (loss: 1.0695290565490723, acc: 0.7314814925193787)
[2025-02-16 11:13:18,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:18,394][root][INFO] - Training Epoch: 1/2, step 2111/23838 completed (loss: 0.921064019203186, acc: 0.7127659320831299)
[2025-02-16 11:13:18,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:18,842][root][INFO] - Training Epoch: 1/2, step 2112/23838 completed (loss: 0.8264517784118652, acc: 0.8305084705352783)
[2025-02-16 11:13:19,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:19,305][root][INFO] - Training Epoch: 1/2, step 2113/23838 completed (loss: 0.7633813619613647, acc: 0.7638888955116272)
[2025-02-16 11:13:19,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:20,274][root][INFO] - Training Epoch: 1/2, step 2114/23838 completed (loss: 0.9882938861846924, acc: 0.748031497001648)
[2025-02-16 11:13:20,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:20,859][root][INFO] - Training Epoch: 1/2, step 2115/23838 completed (loss: 0.7079687714576721, acc: 0.7981651425361633)
[2025-02-16 11:13:21,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:21,365][root][INFO] - Training Epoch: 1/2, step 2116/23838 completed (loss: 0.9095112085342407, acc: 0.748344361782074)
[2025-02-16 11:13:21,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:21,753][root][INFO] - Training Epoch: 1/2, step 2117/23838 completed (loss: 0.841832160949707, acc: 0.7777777910232544)
[2025-02-16 11:13:21,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:22,230][root][INFO] - Training Epoch: 1/2, step 2118/23838 completed (loss: 0.834903359413147, acc: 0.8028169274330139)
[2025-02-16 11:13:22,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:22,676][root][INFO] - Training Epoch: 1/2, step 2119/23838 completed (loss: 0.948993444442749, acc: 0.7142857313156128)
[2025-02-16 11:13:22,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:23,109][root][INFO] - Training Epoch: 1/2, step 2120/23838 completed (loss: 0.8973531126976013, acc: 0.804347813129425)
[2025-02-16 11:13:23,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:23,518][root][INFO] - Training Epoch: 1/2, step 2121/23838 completed (loss: 1.8198858499526978, acc: 0.5135135054588318)
[2025-02-16 11:13:23,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:23,902][root][INFO] - Training Epoch: 1/2, step 2122/23838 completed (loss: 1.0276470184326172, acc: 0.7358490824699402)
[2025-02-16 11:13:24,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:24,269][root][INFO] - Training Epoch: 1/2, step 2123/23838 completed (loss: 0.9684269428253174, acc: 0.6800000071525574)
[2025-02-16 11:13:24,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:24,640][root][INFO] - Training Epoch: 1/2, step 2124/23838 completed (loss: 0.8945558667182922, acc: 0.7234042286872864)
[2025-02-16 11:13:24,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:25,074][root][INFO] - Training Epoch: 1/2, step 2125/23838 completed (loss: 1.14443838596344, acc: 0.6274510025978088)
[2025-02-16 11:13:25,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:25,523][root][INFO] - Training Epoch: 1/2, step 2126/23838 completed (loss: 1.1319315433502197, acc: 0.625)
[2025-02-16 11:13:25,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:25,914][root][INFO] - Training Epoch: 1/2, step 2127/23838 completed (loss: 1.0273699760437012, acc: 0.6666666865348816)
[2025-02-16 11:13:26,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:26,403][root][INFO] - Training Epoch: 1/2, step 2128/23838 completed (loss: 0.7796220183372498, acc: 0.7580645084381104)
[2025-02-16 11:13:26,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:26,797][root][INFO] - Training Epoch: 1/2, step 2129/23838 completed (loss: 0.5197475552558899, acc: 0.8947368264198303)
[2025-02-16 11:13:27,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:27,401][root][INFO] - Training Epoch: 1/2, step 2130/23838 completed (loss: 0.8963208794593811, acc: 0.7428571581840515)
[2025-02-16 11:13:27,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:27,870][root][INFO] - Training Epoch: 1/2, step 2131/23838 completed (loss: 0.5480332374572754, acc: 0.843137264251709)
[2025-02-16 11:13:28,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:28,379][root][INFO] - Training Epoch: 1/2, step 2132/23838 completed (loss: 0.5713818073272705, acc: 0.8037382960319519)
[2025-02-16 11:13:28,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:28,886][root][INFO] - Training Epoch: 1/2, step 2133/23838 completed (loss: 1.1071525812149048, acc: 0.682170569896698)
[2025-02-16 11:13:29,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:29,664][root][INFO] - Training Epoch: 1/2, step 2134/23838 completed (loss: 1.009099006652832, acc: 0.7355371713638306)
[2025-02-16 11:13:29,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:30,057][root][INFO] - Training Epoch: 1/2, step 2135/23838 completed (loss: 0.9824497699737549, acc: 0.7580645084381104)
[2025-02-16 11:13:30,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:30,469][root][INFO] - Training Epoch: 1/2, step 2136/23838 completed (loss: 0.5091059803962708, acc: 0.8476190567016602)
[2025-02-16 11:13:30,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:30,849][root][INFO] - Training Epoch: 1/2, step 2137/23838 completed (loss: 1.1103951930999756, acc: 0.6732673048973083)
[2025-02-16 11:13:30,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:31,220][root][INFO] - Training Epoch: 1/2, step 2138/23838 completed (loss: 0.4661303460597992, acc: 0.8493150472640991)
[2025-02-16 11:13:31,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:31,620][root][INFO] - Training Epoch: 1/2, step 2139/23838 completed (loss: 0.43467220664024353, acc: 0.8769230842590332)
[2025-02-16 11:13:31,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:32,012][root][INFO] - Training Epoch: 1/2, step 2140/23838 completed (loss: 0.33722156286239624, acc: 0.9090909361839294)
[2025-02-16 11:13:32,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:32,392][root][INFO] - Training Epoch: 1/2, step 2141/23838 completed (loss: 0.5270062685012817, acc: 0.8518518805503845)
[2025-02-16 11:13:32,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:32,789][root][INFO] - Training Epoch: 1/2, step 2142/23838 completed (loss: 0.7002202868461609, acc: 0.8090909123420715)
[2025-02-16 11:13:33,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:33,286][root][INFO] - Training Epoch: 1/2, step 2143/23838 completed (loss: 0.6684032678604126, acc: 0.8534482717514038)
[2025-02-16 11:13:33,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:33,648][root][INFO] - Training Epoch: 1/2, step 2144/23838 completed (loss: 0.7787109613418579, acc: 0.8191489577293396)
[2025-02-16 11:13:33,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:34,123][root][INFO] - Training Epoch: 1/2, step 2145/23838 completed (loss: 0.6183689832687378, acc: 0.8417266011238098)
[2025-02-16 11:13:34,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:34,499][root][INFO] - Training Epoch: 1/2, step 2146/23838 completed (loss: 1.2424503564834595, acc: 0.70652174949646)
[2025-02-16 11:13:34,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:34,907][root][INFO] - Training Epoch: 1/2, step 2147/23838 completed (loss: 1.0061750411987305, acc: 0.7155963182449341)
[2025-02-16 11:13:35,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:35,319][root][INFO] - Training Epoch: 1/2, step 2148/23838 completed (loss: 1.4718592166900635, acc: 0.625)
[2025-02-16 11:13:35,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:35,692][root][INFO] - Training Epoch: 1/2, step 2149/23838 completed (loss: 1.6668274402618408, acc: 0.5925925970077515)
[2025-02-16 11:13:35,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:36,130][root][INFO] - Training Epoch: 1/2, step 2150/23838 completed (loss: 2.3444173336029053, acc: 0.4736842215061188)
[2025-02-16 11:13:36,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:36,511][root][INFO] - Training Epoch: 1/2, step 2151/23838 completed (loss: 1.4955247640609741, acc: 0.5)
[2025-02-16 11:13:36,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:36,918][root][INFO] - Training Epoch: 1/2, step 2152/23838 completed (loss: 1.4651596546173096, acc: 0.6363636255264282)
[2025-02-16 11:13:37,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:37,296][root][INFO] - Training Epoch: 1/2, step 2153/23838 completed (loss: 1.8686634302139282, acc: 0.5)
[2025-02-16 11:13:37,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:37,701][root][INFO] - Training Epoch: 1/2, step 2154/23838 completed (loss: 0.6997489333152771, acc: 0.8695651888847351)
[2025-02-16 11:13:37,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:38,093][root][INFO] - Training Epoch: 1/2, step 2155/23838 completed (loss: 1.2177183628082275, acc: 0.7083333134651184)
[2025-02-16 11:13:38,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:38,518][root][INFO] - Training Epoch: 1/2, step 2156/23838 completed (loss: 1.1858261823654175, acc: 0.7368420958518982)
[2025-02-16 11:13:38,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:38,932][root][INFO] - Training Epoch: 1/2, step 2157/23838 completed (loss: 1.6757287979125977, acc: 0.625)
[2025-02-16 11:13:39,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:39,346][root][INFO] - Training Epoch: 1/2, step 2158/23838 completed (loss: 1.065656065940857, acc: 0.7142857313156128)
[2025-02-16 11:13:39,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:39,712][root][INFO] - Training Epoch: 1/2, step 2159/23838 completed (loss: 1.232686161994934, acc: 0.5833333134651184)
[2025-02-16 11:13:39,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:40,131][root][INFO] - Training Epoch: 1/2, step 2160/23838 completed (loss: 2.1009140014648438, acc: 0.3870967626571655)
[2025-02-16 11:13:40,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:40,523][root][INFO] - Training Epoch: 1/2, step 2161/23838 completed (loss: 1.6249510049819946, acc: 0.5199999809265137)
[2025-02-16 11:13:40,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:40,924][root][INFO] - Training Epoch: 1/2, step 2162/23838 completed (loss: 1.571323037147522, acc: 0.6521739363670349)
[2025-02-16 11:13:41,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:41,329][root][INFO] - Training Epoch: 1/2, step 2163/23838 completed (loss: 1.723846673965454, acc: 0.4761904776096344)
[2025-02-16 11:13:41,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:41,746][root][INFO] - Training Epoch: 1/2, step 2164/23838 completed (loss: 1.2292616367340088, acc: 0.6499999761581421)
[2025-02-16 11:13:41,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:42,181][root][INFO] - Training Epoch: 1/2, step 2165/23838 completed (loss: 1.2449672222137451, acc: 0.6521739363670349)
[2025-02-16 11:13:42,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:42,611][root][INFO] - Training Epoch: 1/2, step 2166/23838 completed (loss: 1.4239883422851562, acc: 0.6027397513389587)
[2025-02-16 11:13:42,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:43,000][root][INFO] - Training Epoch: 1/2, step 2167/23838 completed (loss: 0.8347299098968506, acc: 0.7758620977401733)
[2025-02-16 11:13:43,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:43,450][root][INFO] - Training Epoch: 1/2, step 2168/23838 completed (loss: 1.5368119478225708, acc: 0.6126760840415955)
[2025-02-16 11:13:43,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:43,934][root][INFO] - Training Epoch: 1/2, step 2169/23838 completed (loss: 1.3208011388778687, acc: 0.5545454621315002)
[2025-02-16 11:13:44,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:44,301][root][INFO] - Training Epoch: 1/2, step 2170/23838 completed (loss: 1.6590676307678223, acc: 0.5208333134651184)
[2025-02-16 11:13:44,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:44,728][root][INFO] - Training Epoch: 1/2, step 2171/23838 completed (loss: 1.0874688625335693, acc: 0.699999988079071)
[2025-02-16 11:13:44,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:45,192][root][INFO] - Training Epoch: 1/2, step 2172/23838 completed (loss: 1.0356523990631104, acc: 0.6979166865348816)
[2025-02-16 11:13:45,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:45,687][root][INFO] - Training Epoch: 1/2, step 2173/23838 completed (loss: 0.8841264247894287, acc: 0.7473683953285217)
[2025-02-16 11:13:45,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:46,095][root][INFO] - Training Epoch: 1/2, step 2174/23838 completed (loss: 0.796116292476654, acc: 0.7857142686843872)
[2025-02-16 11:13:46,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:46,485][root][INFO] - Training Epoch: 1/2, step 2175/23838 completed (loss: 0.7556353211402893, acc: 0.761904776096344)
[2025-02-16 11:13:46,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:46,896][root][INFO] - Training Epoch: 1/2, step 2176/23838 completed (loss: 0.7880527973175049, acc: 0.8054053783416748)
[2025-02-16 11:13:47,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:47,331][root][INFO] - Training Epoch: 1/2, step 2177/23838 completed (loss: 1.0607815980911255, acc: 0.6833333373069763)
[2025-02-16 11:13:47,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:47,784][root][INFO] - Training Epoch: 1/2, step 2178/23838 completed (loss: 1.1382943391799927, acc: 0.7027027010917664)
[2025-02-16 11:13:47,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:48,168][root][INFO] - Training Epoch: 1/2, step 2179/23838 completed (loss: 0.8367078304290771, acc: 0.7651006579399109)
[2025-02-16 11:13:48,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:48,656][root][INFO] - Training Epoch: 1/2, step 2180/23838 completed (loss: 1.1140321493148804, acc: 0.6959459185600281)
[2025-02-16 11:13:48,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:49,110][root][INFO] - Training Epoch: 1/2, step 2181/23838 completed (loss: 0.7054252624511719, acc: 0.8231292366981506)
[2025-02-16 11:13:49,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:49,533][root][INFO] - Training Epoch: 1/2, step 2182/23838 completed (loss: 1.2066285610198975, acc: 0.6116504669189453)
[2025-02-16 11:13:49,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:49,935][root][INFO] - Training Epoch: 1/2, step 2183/23838 completed (loss: 0.7978754639625549, acc: 0.7662337422370911)
[2025-02-16 11:13:50,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:50,371][root][INFO] - Training Epoch: 1/2, step 2184/23838 completed (loss: 1.5213114023208618, acc: 0.5742574334144592)
[2025-02-16 11:13:50,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:50,782][root][INFO] - Training Epoch: 1/2, step 2185/23838 completed (loss: 0.8716433048248291, acc: 0.7460317611694336)
[2025-02-16 11:13:50,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:51,185][root][INFO] - Training Epoch: 1/2, step 2186/23838 completed (loss: 0.9566912651062012, acc: 0.7338709831237793)
[2025-02-16 11:13:51,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:51,646][root][INFO] - Training Epoch: 1/2, step 2187/23838 completed (loss: 0.9199508428573608, acc: 0.7414966225624084)
[2025-02-16 11:13:51,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:52,001][root][INFO] - Training Epoch: 1/2, step 2188/23838 completed (loss: 0.8732661604881287, acc: 0.7666666507720947)
[2025-02-16 11:13:52,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:52,445][root][INFO] - Training Epoch: 1/2, step 2189/23838 completed (loss: 0.6235471963882446, acc: 0.8367347121238708)
[2025-02-16 11:13:52,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:52,838][root][INFO] - Training Epoch: 1/2, step 2190/23838 completed (loss: 1.5084823369979858, acc: 0.6292135119438171)
[2025-02-16 11:13:53,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:53,303][root][INFO] - Training Epoch: 1/2, step 2191/23838 completed (loss: 0.6264713406562805, acc: 0.842424213886261)
[2025-02-16 11:13:53,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:53,685][root][INFO] - Training Epoch: 1/2, step 2192/23838 completed (loss: 0.6584081053733826, acc: 0.800000011920929)
[2025-02-16 11:13:53,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:54,133][root][INFO] - Training Epoch: 1/2, step 2193/23838 completed (loss: 0.4222988188266754, acc: 0.8831169009208679)
[2025-02-16 11:13:54,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:54,529][root][INFO] - Training Epoch: 1/2, step 2194/23838 completed (loss: 1.115695595741272, acc: 0.6530612111091614)
[2025-02-16 11:13:54,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:54,925][root][INFO] - Training Epoch: 1/2, step 2195/23838 completed (loss: 0.6161028146743774, acc: 0.8387096524238586)
[2025-02-16 11:13:55,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:55,313][root][INFO] - Training Epoch: 1/2, step 2196/23838 completed (loss: 0.5624433159828186, acc: 0.792682945728302)
[2025-02-16 11:13:55,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:55,709][root][INFO] - Training Epoch: 1/2, step 2197/23838 completed (loss: 0.5784392952919006, acc: 0.843137264251709)
[2025-02-16 11:13:55,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:56,092][root][INFO] - Training Epoch: 1/2, step 2198/23838 completed (loss: 1.204172968864441, acc: 0.6136363744735718)
[2025-02-16 11:13:56,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:56,535][root][INFO] - Training Epoch: 1/2, step 2199/23838 completed (loss: 1.1332839727401733, acc: 0.6938775777816772)
[2025-02-16 11:13:56,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:56,888][root][INFO] - Training Epoch: 1/2, step 2200/23838 completed (loss: 1.0071405172348022, acc: 0.7090908885002136)
[2025-02-16 11:13:57,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:57,338][root][INFO] - Training Epoch: 1/2, step 2201/23838 completed (loss: 0.9769745469093323, acc: 0.7622950673103333)
[2025-02-16 11:13:57,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:57,823][root][INFO] - Training Epoch: 1/2, step 2202/23838 completed (loss: 0.9765694737434387, acc: 0.6666666865348816)
[2025-02-16 11:13:58,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:58,291][root][INFO] - Training Epoch: 1/2, step 2203/23838 completed (loss: 0.7683321833610535, acc: 0.805031418800354)
[2025-02-16 11:13:58,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:58,733][root][INFO] - Training Epoch: 1/2, step 2204/23838 completed (loss: 1.0792735815048218, acc: 0.7014925479888916)
[2025-02-16 11:13:58,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:59,216][root][INFO] - Training Epoch: 1/2, step 2205/23838 completed (loss: 0.8613676428794861, acc: 0.7621951103210449)
[2025-02-16 11:13:59,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:13:59,689][root][INFO] - Training Epoch: 1/2, step 2206/23838 completed (loss: 0.7235750555992126, acc: 0.797468364238739)
[2025-02-16 11:13:59,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:00,156][root][INFO] - Training Epoch: 1/2, step 2207/23838 completed (loss: 0.5883246064186096, acc: 0.8353658318519592)
[2025-02-16 11:14:00,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:00,569][root][INFO] - Training Epoch: 1/2, step 2208/23838 completed (loss: 1.1264759302139282, acc: 0.6694915294647217)
[2025-02-16 11:14:00,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:01,009][root][INFO] - Training Epoch: 1/2, step 2209/23838 completed (loss: 0.9044657349586487, acc: 0.773809552192688)
[2025-02-16 11:14:01,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:01,382][root][INFO] - Training Epoch: 1/2, step 2210/23838 completed (loss: 0.9300596117973328, acc: 0.7627118825912476)
[2025-02-16 11:14:01,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:01,821][root][INFO] - Training Epoch: 1/2, step 2211/23838 completed (loss: 0.6374046206474304, acc: 0.8260869383811951)
[2025-02-16 11:14:02,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:02,267][root][INFO] - Training Epoch: 1/2, step 2212/23838 completed (loss: 0.8087290525436401, acc: 0.75)
[2025-02-16 11:14:02,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:02,718][root][INFO] - Training Epoch: 1/2, step 2213/23838 completed (loss: 0.669098973274231, acc: 0.8057143092155457)
[2025-02-16 11:14:02,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:03,177][root][INFO] - Training Epoch: 1/2, step 2214/23838 completed (loss: 0.8467884063720703, acc: 0.7762237787246704)
[2025-02-16 11:14:03,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:03,596][root][INFO] - Training Epoch: 1/2, step 2215/23838 completed (loss: 0.5000201463699341, acc: 0.8670886158943176)
[2025-02-16 11:14:03,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:04,053][root][INFO] - Training Epoch: 1/2, step 2216/23838 completed (loss: 1.1012901067733765, acc: 0.6916666626930237)
[2025-02-16 11:14:04,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:04,464][root][INFO] - Training Epoch: 1/2, step 2217/23838 completed (loss: 1.1305835247039795, acc: 0.6722689270973206)
[2025-02-16 11:14:04,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:04,943][root][INFO] - Training Epoch: 1/2, step 2218/23838 completed (loss: 0.858768880367279, acc: 0.7696969509124756)
[2025-02-16 11:14:05,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:05,352][root][INFO] - Training Epoch: 1/2, step 2219/23838 completed (loss: 1.1315323114395142, acc: 0.7155963182449341)
[2025-02-16 11:14:05,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:05,817][root][INFO] - Training Epoch: 1/2, step 2220/23838 completed (loss: 0.5624820590019226, acc: 0.8533333539962769)
[2025-02-16 11:14:06,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:06,280][root][INFO] - Training Epoch: 1/2, step 2221/23838 completed (loss: 0.9112547039985657, acc: 0.78125)
[2025-02-16 11:14:06,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:06,754][root][INFO] - Training Epoch: 1/2, step 2222/23838 completed (loss: 0.6378310918807983, acc: 0.8240000009536743)
[2025-02-16 11:14:06,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:07,172][root][INFO] - Training Epoch: 1/2, step 2223/23838 completed (loss: 0.7515029311180115, acc: 0.7810218930244446)
[2025-02-16 11:14:07,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:07,624][root][INFO] - Training Epoch: 1/2, step 2224/23838 completed (loss: 0.9729865789413452, acc: 0.7413793206214905)
[2025-02-16 11:14:07,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:08,029][root][INFO] - Training Epoch: 1/2, step 2225/23838 completed (loss: 0.8197036981582642, acc: 0.7476635575294495)
[2025-02-16 11:14:08,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:08,436][root][INFO] - Training Epoch: 1/2, step 2226/23838 completed (loss: 0.7154524326324463, acc: 0.78899085521698)
[2025-02-16 11:14:08,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:08,896][root][INFO] - Training Epoch: 1/2, step 2227/23838 completed (loss: 0.24719080328941345, acc: 0.95652174949646)
[2025-02-16 11:14:09,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:09,361][root][INFO] - Training Epoch: 1/2, step 2228/23838 completed (loss: 0.5613539814949036, acc: 0.8684210777282715)
[2025-02-16 11:14:09,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:09,776][root][INFO] - Training Epoch: 1/2, step 2229/23838 completed (loss: 0.6518529057502747, acc: 0.8510638475418091)
[2025-02-16 11:14:10,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:10,274][root][INFO] - Training Epoch: 1/2, step 2230/23838 completed (loss: 0.6126363277435303, acc: 0.8089887499809265)
[2025-02-16 11:14:10,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:10,746][root][INFO] - Training Epoch: 1/2, step 2231/23838 completed (loss: 0.7345812320709229, acc: 0.8258706331253052)
[2025-02-16 11:14:10,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:11,128][root][INFO] - Training Epoch: 1/2, step 2232/23838 completed (loss: 0.9156726002693176, acc: 0.753731369972229)
[2025-02-16 11:14:11,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:11,564][root][INFO] - Training Epoch: 1/2, step 2233/23838 completed (loss: 0.8012643456459045, acc: 0.779411792755127)
[2025-02-16 11:14:11,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:12,004][root][INFO] - Training Epoch: 1/2, step 2234/23838 completed (loss: 0.7339408993721008, acc: 0.7641509175300598)
[2025-02-16 11:14:12,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:12,457][root][INFO] - Training Epoch: 1/2, step 2235/23838 completed (loss: 1.3381774425506592, acc: 0.646616518497467)
[2025-02-16 11:14:12,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:12,886][root][INFO] - Training Epoch: 1/2, step 2236/23838 completed (loss: 1.2853881120681763, acc: 0.6315789222717285)
[2025-02-16 11:14:13,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:13,313][root][INFO] - Training Epoch: 1/2, step 2237/23838 completed (loss: 0.6766326427459717, acc: 0.7831325531005859)
[2025-02-16 11:14:13,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:13,778][root][INFO] - Training Epoch: 1/2, step 2238/23838 completed (loss: 0.7062846422195435, acc: 0.7749999761581421)
[2025-02-16 11:14:13,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:14,206][root][INFO] - Training Epoch: 1/2, step 2239/23838 completed (loss: 0.7527115941047668, acc: 0.75)
[2025-02-16 11:14:14,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:14,676][root][INFO] - Training Epoch: 1/2, step 2240/23838 completed (loss: 0.640098512172699, acc: 0.8167939186096191)
[2025-02-16 11:14:14,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:15,111][root][INFO] - Training Epoch: 1/2, step 2241/23838 completed (loss: 0.7290567755699158, acc: 0.8090909123420715)
[2025-02-16 11:14:15,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:15,511][root][INFO] - Training Epoch: 1/2, step 2242/23838 completed (loss: 1.3273051977157593, acc: 0.6800000071525574)
[2025-02-16 11:14:15,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:15,915][root][INFO] - Training Epoch: 1/2, step 2243/23838 completed (loss: 0.7380117774009705, acc: 0.7727272510528564)
[2025-02-16 11:14:16,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:16,283][root][INFO] - Training Epoch: 1/2, step 2244/23838 completed (loss: 1.101925253868103, acc: 0.682692289352417)
[2025-02-16 11:14:16,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:16,674][root][INFO] - Training Epoch: 1/2, step 2245/23838 completed (loss: 1.2999030351638794, acc: 0.6268656849861145)
[2025-02-16 11:14:16,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:17,105][root][INFO] - Training Epoch: 1/2, step 2246/23838 completed (loss: 1.3465021848678589, acc: 0.6060606241226196)
[2025-02-16 11:14:17,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:17,542][root][INFO] - Training Epoch: 1/2, step 2247/23838 completed (loss: 1.0450952053070068, acc: 0.7124999761581421)
[2025-02-16 11:14:17,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:18,156][root][INFO] - Training Epoch: 1/2, step 2248/23838 completed (loss: 0.7843948602676392, acc: 0.8274509906768799)
[2025-02-16 11:14:18,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:18,613][root][INFO] - Training Epoch: 1/2, step 2249/23838 completed (loss: 1.0455970764160156, acc: 0.701298713684082)
[2025-02-16 11:14:18,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:19,028][root][INFO] - Training Epoch: 1/2, step 2250/23838 completed (loss: 1.1275144815444946, acc: 0.7168141603469849)
[2025-02-16 11:14:19,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:19,457][root][INFO] - Training Epoch: 1/2, step 2251/23838 completed (loss: 0.8594422340393066, acc: 0.807692289352417)
[2025-02-16 11:14:19,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:19,839][root][INFO] - Training Epoch: 1/2, step 2252/23838 completed (loss: 0.4726638197898865, acc: 0.8562091588973999)
[2025-02-16 11:14:20,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:20,515][root][INFO] - Training Epoch: 1/2, step 2253/23838 completed (loss: 1.1670589447021484, acc: 0.6930232644081116)
[2025-02-16 11:14:20,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:20,983][root][INFO] - Training Epoch: 1/2, step 2254/23838 completed (loss: 0.8510538339614868, acc: 0.7744361162185669)
[2025-02-16 11:14:21,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:21,483][root][INFO] - Training Epoch: 1/2, step 2255/23838 completed (loss: 1.3045289516448975, acc: 0.6333333253860474)
[2025-02-16 11:14:21,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:21,874][root][INFO] - Training Epoch: 1/2, step 2256/23838 completed (loss: 0.7088379263877869, acc: 0.8080000281333923)
[2025-02-16 11:14:22,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:22,296][root][INFO] - Training Epoch: 1/2, step 2257/23838 completed (loss: 0.7956512570381165, acc: 0.8039215803146362)
[2025-02-16 11:14:22,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:22,783][root][INFO] - Training Epoch: 1/2, step 2258/23838 completed (loss: 0.8925589919090271, acc: 0.7232142686843872)
[2025-02-16 11:14:23,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:23,415][root][INFO] - Training Epoch: 1/2, step 2259/23838 completed (loss: 0.7281383872032166, acc: 0.7786561250686646)
[2025-02-16 11:14:23,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:23,869][root][INFO] - Training Epoch: 1/2, step 2260/23838 completed (loss: 1.095034122467041, acc: 0.7446808218955994)
[2025-02-16 11:14:24,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:24,278][root][INFO] - Training Epoch: 1/2, step 2261/23838 completed (loss: 0.9210124015808105, acc: 0.752293586730957)
[2025-02-16 11:14:24,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:24,682][root][INFO] - Training Epoch: 1/2, step 2262/23838 completed (loss: 0.8148561716079712, acc: 0.7954545617103577)
[2025-02-16 11:14:24,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:25,124][root][INFO] - Training Epoch: 1/2, step 2263/23838 completed (loss: 0.9771425127983093, acc: 0.7542372941970825)
[2025-02-16 11:14:25,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:25,567][root][INFO] - Training Epoch: 1/2, step 2264/23838 completed (loss: 0.5181406736373901, acc: 0.8518518805503845)
[2025-02-16 11:14:25,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:26,062][root][INFO] - Training Epoch: 1/2, step 2265/23838 completed (loss: 0.9683253169059753, acc: 0.7681159377098083)
[2025-02-16 11:14:26,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:26,471][root][INFO] - Training Epoch: 1/2, step 2266/23838 completed (loss: 0.7223091125488281, acc: 0.8222222328186035)
[2025-02-16 11:14:26,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:26,946][root][INFO] - Training Epoch: 1/2, step 2267/23838 completed (loss: 0.7057422399520874, acc: 0.8181818127632141)
[2025-02-16 11:14:27,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:27,371][root][INFO] - Training Epoch: 1/2, step 2268/23838 completed (loss: 0.8524701595306396, acc: 0.7777777910232544)
[2025-02-16 11:14:27,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:27,948][root][INFO] - Training Epoch: 1/2, step 2269/23838 completed (loss: 1.1912564039230347, acc: 0.6212121248245239)
[2025-02-16 11:14:28,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:28,338][root][INFO] - Training Epoch: 1/2, step 2270/23838 completed (loss: 0.7593789100646973, acc: 0.7530864477157593)
[2025-02-16 11:14:28,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:28,764][root][INFO] - Training Epoch: 1/2, step 2271/23838 completed (loss: 0.9749252200126648, acc: 0.7472527623176575)
[2025-02-16 11:14:29,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:29,244][root][INFO] - Training Epoch: 1/2, step 2272/23838 completed (loss: 1.6100918054580688, acc: 0.5675675868988037)
[2025-02-16 11:14:29,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:29,698][root][INFO] - Training Epoch: 1/2, step 2273/23838 completed (loss: 0.3382361829280853, acc: 0.930232584476471)
[2025-02-16 11:14:29,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:30,123][root][INFO] - Training Epoch: 1/2, step 2274/23838 completed (loss: 0.5358988046646118, acc: 0.8152173757553101)
[2025-02-16 11:14:30,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:30,585][root][INFO] - Training Epoch: 1/2, step 2275/23838 completed (loss: 0.8735853433609009, acc: 0.7692307829856873)
[2025-02-16 11:14:30,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:30,983][root][INFO] - Training Epoch: 1/2, step 2276/23838 completed (loss: 0.27907833456993103, acc: 0.9428571462631226)
[2025-02-16 11:14:31,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:31,357][root][INFO] - Training Epoch: 1/2, step 2277/23838 completed (loss: 0.5082085132598877, acc: 0.8367347121238708)
[2025-02-16 11:14:31,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:31,784][root][INFO] - Training Epoch: 1/2, step 2278/23838 completed (loss: 0.6194073557853699, acc: 0.7857142686843872)
[2025-02-16 11:14:31,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:32,196][root][INFO] - Training Epoch: 1/2, step 2279/23838 completed (loss: 0.8566948771476746, acc: 0.7714285850524902)
[2025-02-16 11:14:32,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:32,729][root][INFO] - Training Epoch: 1/2, step 2280/23838 completed (loss: 0.9449715614318848, acc: 0.7710843086242676)
[2025-02-16 11:14:32,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:33,200][root][INFO] - Training Epoch: 1/2, step 2281/23838 completed (loss: 0.9013251662254333, acc: 0.7160493731498718)
[2025-02-16 11:14:33,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:33,598][root][INFO] - Training Epoch: 1/2, step 2282/23838 completed (loss: 1.0809872150421143, acc: 0.6888889074325562)
[2025-02-16 11:14:33,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:34,124][root][INFO] - Training Epoch: 1/2, step 2283/23838 completed (loss: 0.7590029835700989, acc: 0.7961165308952332)
[2025-02-16 11:14:34,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:34,550][root][INFO] - Training Epoch: 1/2, step 2284/23838 completed (loss: 0.7472963929176331, acc: 0.8045976758003235)
[2025-02-16 11:14:34,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:35,154][root][INFO] - Training Epoch: 1/2, step 2285/23838 completed (loss: 0.5964847803115845, acc: 0.8070175647735596)
[2025-02-16 11:14:35,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:35,588][root][INFO] - Training Epoch: 1/2, step 2286/23838 completed (loss: 1.1321637630462646, acc: 0.7142857313156128)
[2025-02-16 11:14:35,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:36,087][root][INFO] - Training Epoch: 1/2, step 2287/23838 completed (loss: 0.7872514724731445, acc: 0.7872340679168701)
[2025-02-16 11:14:36,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:36,498][root][INFO] - Training Epoch: 1/2, step 2288/23838 completed (loss: 0.46124887466430664, acc: 0.8374999761581421)
[2025-02-16 11:14:36,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:36,888][root][INFO] - Training Epoch: 1/2, step 2289/23838 completed (loss: 0.5415627956390381, acc: 0.7894737124443054)
[2025-02-16 11:14:37,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:37,334][root][INFO] - Training Epoch: 1/2, step 2290/23838 completed (loss: 0.642059326171875, acc: 0.7727272510528564)
[2025-02-16 11:14:37,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:37,885][root][INFO] - Training Epoch: 1/2, step 2291/23838 completed (loss: 0.6104270815849304, acc: 0.7971014380455017)
[2025-02-16 11:14:38,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:38,286][root][INFO] - Training Epoch: 1/2, step 2292/23838 completed (loss: 0.598395586013794, acc: 0.8297872543334961)
[2025-02-16 11:14:38,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:38,701][root][INFO] - Training Epoch: 1/2, step 2293/23838 completed (loss: 0.5289252996444702, acc: 0.8598130941390991)
[2025-02-16 11:14:38,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:39,098][root][INFO] - Training Epoch: 1/2, step 2294/23838 completed (loss: 0.655092179775238, acc: 0.8169013857841492)
[2025-02-16 11:14:39,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:39,583][root][INFO] - Training Epoch: 1/2, step 2295/23838 completed (loss: 0.8825523853302002, acc: 0.8064516186714172)
[2025-02-16 11:14:39,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:39,990][root][INFO] - Training Epoch: 1/2, step 2296/23838 completed (loss: 1.1131876707077026, acc: 0.7222222089767456)
[2025-02-16 11:14:40,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:40,452][root][INFO] - Training Epoch: 1/2, step 2297/23838 completed (loss: 0.8855689167976379, acc: 0.75)
[2025-02-16 11:14:40,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:40,933][root][INFO] - Training Epoch: 1/2, step 2298/23838 completed (loss: 0.5901942253112793, acc: 0.8472222089767456)
[2025-02-16 11:14:41,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:41,446][root][INFO] - Training Epoch: 1/2, step 2299/23838 completed (loss: 0.5650429725646973, acc: 0.8474576473236084)
[2025-02-16 11:14:41,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:41,915][root][INFO] - Training Epoch: 1/2, step 2300/23838 completed (loss: 0.6638466715812683, acc: 0.8142856955528259)
[2025-02-16 11:14:42,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:42,374][root][INFO] - Training Epoch: 1/2, step 2301/23838 completed (loss: 0.9829173684120178, acc: 0.7384615540504456)
[2025-02-16 11:14:42,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:42,869][root][INFO] - Training Epoch: 1/2, step 2302/23838 completed (loss: 0.8106237649917603, acc: 0.7777777910232544)
[2025-02-16 11:14:43,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:43,299][root][INFO] - Training Epoch: 1/2, step 2303/23838 completed (loss: 1.3040920495986938, acc: 0.6129032373428345)
[2025-02-16 11:14:43,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:43,653][root][INFO] - Training Epoch: 1/2, step 2304/23838 completed (loss: 0.46137481927871704, acc: 0.8571428656578064)
[2025-02-16 11:14:43,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:44,087][root][INFO] - Training Epoch: 1/2, step 2305/23838 completed (loss: 0.9044599533081055, acc: 0.7547169923782349)
[2025-02-16 11:14:44,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:44,470][root][INFO] - Training Epoch: 1/2, step 2306/23838 completed (loss: 0.8321117758750916, acc: 0.734375)
[2025-02-16 11:14:44,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:44,849][root][INFO] - Training Epoch: 1/2, step 2307/23838 completed (loss: 0.6019394397735596, acc: 0.7692307829856873)
[2025-02-16 11:14:45,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:45,262][root][INFO] - Training Epoch: 1/2, step 2308/23838 completed (loss: 0.49025776982307434, acc: 0.8539325594902039)
[2025-02-16 11:14:45,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:45,704][root][INFO] - Training Epoch: 1/2, step 2309/23838 completed (loss: 0.5754482746124268, acc: 0.824999988079071)
[2025-02-16 11:14:45,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:46,128][root][INFO] - Training Epoch: 1/2, step 2310/23838 completed (loss: 0.9349626302719116, acc: 0.7573529481887817)
[2025-02-16 11:14:46,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:46,537][root][INFO] - Training Epoch: 1/2, step 2311/23838 completed (loss: 1.3072123527526855, acc: 0.6111111044883728)
[2025-02-16 11:14:47,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:47,560][root][INFO] - Training Epoch: 1/2, step 2312/23838 completed (loss: 1.2477139234542847, acc: 0.6875)
[2025-02-16 11:14:47,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:47,943][root][INFO] - Training Epoch: 1/2, step 2313/23838 completed (loss: 0.8440262079238892, acc: 0.7422680258750916)
[2025-02-16 11:14:48,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:48,326][root][INFO] - Training Epoch: 1/2, step 2314/23838 completed (loss: 0.6291351914405823, acc: 0.8230088353157043)
[2025-02-16 11:14:48,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:49,292][root][INFO] - Training Epoch: 1/2, step 2315/23838 completed (loss: 1.3861027956008911, acc: 0.6521739363670349)
[2025-02-16 11:14:49,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:49,697][root][INFO] - Training Epoch: 1/2, step 2316/23838 completed (loss: 0.74339759349823, acc: 0.7666666507720947)
[2025-02-16 11:14:49,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:50,121][root][INFO] - Training Epoch: 1/2, step 2317/23838 completed (loss: 1.0832319259643555, acc: 0.739130437374115)
[2025-02-16 11:14:50,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:50,873][root][INFO] - Training Epoch: 1/2, step 2318/23838 completed (loss: 0.7801756858825684, acc: 0.769784152507782)
[2025-02-16 11:14:51,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:51,307][root][INFO] - Training Epoch: 1/2, step 2319/23838 completed (loss: 0.9442886114120483, acc: 0.7313432693481445)
[2025-02-16 11:14:51,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:51,759][root][INFO] - Training Epoch: 1/2, step 2320/23838 completed (loss: 0.7545605301856995, acc: 0.807692289352417)
[2025-02-16 11:14:52,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:52,478][root][INFO] - Training Epoch: 1/2, step 2321/23838 completed (loss: 0.5929949879646301, acc: 0.8235294222831726)
[2025-02-16 11:14:53,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:53,508][root][INFO] - Training Epoch: 1/2, step 2322/23838 completed (loss: 1.209052324295044, acc: 0.701298713684082)
[2025-02-16 11:14:53,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:54,117][root][INFO] - Training Epoch: 1/2, step 2323/23838 completed (loss: 1.074535608291626, acc: 0.6875)
[2025-02-16 11:14:54,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:55,049][root][INFO] - Training Epoch: 1/2, step 2324/23838 completed (loss: 0.5474342107772827, acc: 0.8793103694915771)
[2025-02-16 11:14:55,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:55,547][root][INFO] - Training Epoch: 1/2, step 2325/23838 completed (loss: 0.8906530141830444, acc: 0.7950819730758667)
[2025-02-16 11:14:55,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:56,157][root][INFO] - Training Epoch: 1/2, step 2326/23838 completed (loss: 0.559153139591217, acc: 0.8861788511276245)
[2025-02-16 11:14:56,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:56,679][root][INFO] - Training Epoch: 1/2, step 2327/23838 completed (loss: 0.5655926465988159, acc: 0.8219178318977356)
[2025-02-16 11:14:56,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:57,117][root][INFO] - Training Epoch: 1/2, step 2328/23838 completed (loss: 1.5313291549682617, acc: 0.5686274766921997)
[2025-02-16 11:14:57,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:57,509][root][INFO] - Training Epoch: 1/2, step 2329/23838 completed (loss: 1.6096699237823486, acc: 0.6176470518112183)
[2025-02-16 11:14:57,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:57,911][root][INFO] - Training Epoch: 1/2, step 2330/23838 completed (loss: 1.5305801630020142, acc: 0.6081081032752991)
[2025-02-16 11:14:58,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:58,364][root][INFO] - Training Epoch: 1/2, step 2331/23838 completed (loss: 1.6362485885620117, acc: 0.5625)
[2025-02-16 11:14:58,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:58,806][root][INFO] - Training Epoch: 1/2, step 2332/23838 completed (loss: 0.9985209703445435, acc: 0.782608687877655)
[2025-02-16 11:14:59,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:59,410][root][INFO] - Training Epoch: 1/2, step 2333/23838 completed (loss: 0.7947243452072144, acc: 0.7872340679168701)
[2025-02-16 11:14:59,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:14:59,876][root][INFO] - Training Epoch: 1/2, step 2334/23838 completed (loss: 1.499125361442566, acc: 0.5588235259056091)
[2025-02-16 11:15:00,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:00,328][root][INFO] - Training Epoch: 1/2, step 2335/23838 completed (loss: 0.89215087890625, acc: 0.7415730357170105)
[2025-02-16 11:15:00,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:00,927][root][INFO] - Training Epoch: 1/2, step 2336/23838 completed (loss: 0.7008389234542847, acc: 0.8224852085113525)
[2025-02-16 11:15:01,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:01,399][root][INFO] - Training Epoch: 1/2, step 2337/23838 completed (loss: 1.2750723361968994, acc: 0.688524603843689)
[2025-02-16 11:15:01,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:02,107][root][INFO] - Training Epoch: 1/2, step 2338/23838 completed (loss: 0.7260555028915405, acc: 0.738095223903656)
[2025-02-16 11:15:02,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:02,632][root][INFO] - Training Epoch: 1/2, step 2339/23838 completed (loss: 0.9292892813682556, acc: 0.7471264600753784)
[2025-02-16 11:15:02,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:03,074][root][INFO] - Training Epoch: 1/2, step 2340/23838 completed (loss: 0.8291869759559631, acc: 0.7538461685180664)
[2025-02-16 11:15:03,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:03,527][root][INFO] - Training Epoch: 1/2, step 2341/23838 completed (loss: 0.7008787989616394, acc: 0.8235294222831726)
[2025-02-16 11:15:03,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:03,953][root][INFO] - Training Epoch: 1/2, step 2342/23838 completed (loss: 0.8488561511039734, acc: 0.767123281955719)
[2025-02-16 11:15:04,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:04,372][root][INFO] - Training Epoch: 1/2, step 2343/23838 completed (loss: 0.6029587388038635, acc: 0.8552631735801697)
[2025-02-16 11:15:04,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:04,803][root][INFO] - Training Epoch: 1/2, step 2344/23838 completed (loss: 1.1305795907974243, acc: 0.694915235042572)
[2025-02-16 11:15:05,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:05,275][root][INFO] - Training Epoch: 1/2, step 2345/23838 completed (loss: 1.2464516162872314, acc: 0.6530612111091614)
[2025-02-16 11:15:05,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:05,700][root][INFO] - Training Epoch: 1/2, step 2346/23838 completed (loss: 0.7747827172279358, acc: 0.8205128312110901)
[2025-02-16 11:15:05,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:06,213][root][INFO] - Training Epoch: 1/2, step 2347/23838 completed (loss: 0.5696898698806763, acc: 0.843137264251709)
[2025-02-16 11:15:06,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:06,815][root][INFO] - Training Epoch: 1/2, step 2348/23838 completed (loss: 0.4753074645996094, acc: 0.8902438879013062)
[2025-02-16 11:15:07,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:07,298][root][INFO] - Training Epoch: 1/2, step 2349/23838 completed (loss: 0.7838305830955505, acc: 0.7947019934654236)
[2025-02-16 11:15:07,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:07,680][root][INFO] - Training Epoch: 1/2, step 2350/23838 completed (loss: 0.41861987113952637, acc: 0.8809523582458496)
[2025-02-16 11:15:07,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:08,145][root][INFO] - Training Epoch: 1/2, step 2351/23838 completed (loss: 0.477043479681015, acc: 0.8721804618835449)
[2025-02-16 11:15:08,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:08,515][root][INFO] - Training Epoch: 1/2, step 2352/23838 completed (loss: 0.5502591729164124, acc: 0.8440366983413696)
[2025-02-16 11:15:08,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:08,961][root][INFO] - Training Epoch: 1/2, step 2353/23838 completed (loss: 0.6770479083061218, acc: 0.7814207673072815)
[2025-02-16 11:15:09,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:09,360][root][INFO] - Training Epoch: 1/2, step 2354/23838 completed (loss: 1.106107234954834, acc: 0.7083333134651184)
[2025-02-16 11:15:09,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:09,813][root][INFO] - Training Epoch: 1/2, step 2355/23838 completed (loss: 0.993893563747406, acc: 0.739130437374115)
[2025-02-16 11:15:10,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:10,386][root][INFO] - Training Epoch: 1/2, step 2356/23838 completed (loss: 0.7521361708641052, acc: 0.7573529481887817)
[2025-02-16 11:15:10,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:10,917][root][INFO] - Training Epoch: 1/2, step 2357/23838 completed (loss: 0.9917089939117432, acc: 0.6499999761581421)
[2025-02-16 11:15:11,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:11,336][root][INFO] - Training Epoch: 1/2, step 2358/23838 completed (loss: 1.9733848571777344, acc: 0.48148149251937866)
[2025-02-16 11:15:11,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:11,775][root][INFO] - Training Epoch: 1/2, step 2359/23838 completed (loss: 1.0838639736175537, acc: 0.7184466123580933)
[2025-02-16 11:15:11,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:12,215][root][INFO] - Training Epoch: 1/2, step 2360/23838 completed (loss: 0.9831399917602539, acc: 0.7692307829856873)
[2025-02-16 11:15:12,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:12,638][root][INFO] - Training Epoch: 1/2, step 2361/23838 completed (loss: 0.6536716818809509, acc: 0.8055555820465088)
[2025-02-16 11:15:12,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:13,104][root][INFO] - Training Epoch: 1/2, step 2362/23838 completed (loss: 0.4753892421722412, acc: 0.9032257795333862)
[2025-02-16 11:15:13,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:13,566][root][INFO] - Training Epoch: 1/2, step 2363/23838 completed (loss: 0.5957530736923218, acc: 0.8295454382896423)
[2025-02-16 11:15:13,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:14,037][root][INFO] - Training Epoch: 1/2, step 2364/23838 completed (loss: 0.5119950771331787, acc: 0.8133971095085144)
[2025-02-16 11:15:14,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:14,464][root][INFO] - Training Epoch: 1/2, step 2365/23838 completed (loss: 0.7154762744903564, acc: 0.8552631735801697)
[2025-02-16 11:15:14,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:14,948][root][INFO] - Training Epoch: 1/2, step 2366/23838 completed (loss: 0.5387009382247925, acc: 0.8225806355476379)
[2025-02-16 11:15:15,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:15,500][root][INFO] - Training Epoch: 1/2, step 2367/23838 completed (loss: 0.6724023818969727, acc: 0.810606062412262)
[2025-02-16 11:15:15,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:15,964][root][INFO] - Training Epoch: 1/2, step 2368/23838 completed (loss: 0.48478400707244873, acc: 0.8301886916160583)
[2025-02-16 11:15:16,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:16,425][root][INFO] - Training Epoch: 1/2, step 2369/23838 completed (loss: 1.194742202758789, acc: 0.6458333134651184)
[2025-02-16 11:15:16,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:16,891][root][INFO] - Training Epoch: 1/2, step 2370/23838 completed (loss: 1.053863763809204, acc: 0.7088607549667358)
[2025-02-16 11:15:17,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:17,292][root][INFO] - Training Epoch: 1/2, step 2371/23838 completed (loss: 0.6095498204231262, acc: 0.8333333134651184)
[2025-02-16 11:15:17,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:17,698][root][INFO] - Training Epoch: 1/2, step 2372/23838 completed (loss: 0.3540617525577545, acc: 0.8794326186180115)
[2025-02-16 11:15:17,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:18,227][root][INFO] - Training Epoch: 1/2, step 2373/23838 completed (loss: 0.393462598323822, acc: 0.8849557638168335)
[2025-02-16 11:15:18,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:18,690][root][INFO] - Training Epoch: 1/2, step 2374/23838 completed (loss: 0.4115428924560547, acc: 0.8910256624221802)
[2025-02-16 11:15:18,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:19,046][root][INFO] - Training Epoch: 1/2, step 2375/23838 completed (loss: 0.6335799098014832, acc: 0.8059701323509216)
[2025-02-16 11:15:19,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:19,458][root][INFO] - Training Epoch: 1/2, step 2376/23838 completed (loss: 1.0726653337478638, acc: 0.7407407164573669)
[2025-02-16 11:15:19,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:19,925][root][INFO] - Training Epoch: 1/2, step 2377/23838 completed (loss: 0.38368120789527893, acc: 0.8965517282485962)
[2025-02-16 11:15:20,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:20,635][root][INFO] - Training Epoch: 1/2, step 2378/23838 completed (loss: 0.6747309565544128, acc: 0.8251366019248962)
[2025-02-16 11:15:20,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:21,041][root][INFO] - Training Epoch: 1/2, step 2379/23838 completed (loss: 0.8262222409248352, acc: 0.7857142686843872)
[2025-02-16 11:15:21,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:21,455][root][INFO] - Training Epoch: 1/2, step 2380/23838 completed (loss: 2.0479226112365723, acc: 0.3461538553237915)
[2025-02-16 11:15:21,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:21,921][root][INFO] - Training Epoch: 1/2, step 2381/23838 completed (loss: 1.905128002166748, acc: 0.4571428596973419)
[2025-02-16 11:15:22,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:22,337][root][INFO] - Training Epoch: 1/2, step 2382/23838 completed (loss: 1.3906201124191284, acc: 0.644444465637207)
[2025-02-16 11:15:22,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:22,776][root][INFO] - Training Epoch: 1/2, step 2383/23838 completed (loss: 0.958370566368103, acc: 0.720588207244873)
[2025-02-16 11:15:22,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:23,216][root][INFO] - Training Epoch: 1/2, step 2384/23838 completed (loss: 0.8672052621841431, acc: 0.7837837934494019)
[2025-02-16 11:15:23,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:23,649][root][INFO] - Training Epoch: 1/2, step 2385/23838 completed (loss: 1.1196025609970093, acc: 0.6800000071525574)
[2025-02-16 11:15:23,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:24,036][root][INFO] - Training Epoch: 1/2, step 2386/23838 completed (loss: 1.253126859664917, acc: 0.6511628031730652)
[2025-02-16 11:15:24,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:24,507][root][INFO] - Training Epoch: 1/2, step 2387/23838 completed (loss: 1.0285754203796387, acc: 0.6753246784210205)
[2025-02-16 11:15:24,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:24,968][root][INFO] - Training Epoch: 1/2, step 2388/23838 completed (loss: 0.972713828086853, acc: 0.7599999904632568)
[2025-02-16 11:15:25,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:25,377][root][INFO] - Training Epoch: 1/2, step 2389/23838 completed (loss: 0.8505719900131226, acc: 0.7833333611488342)
[2025-02-16 11:15:25,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:25,838][root][INFO] - Training Epoch: 1/2, step 2390/23838 completed (loss: 0.7379188537597656, acc: 0.7636363506317139)
[2025-02-16 11:15:26,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:26,232][root][INFO] - Training Epoch: 1/2, step 2391/23838 completed (loss: 1.1033614873886108, acc: 0.7058823704719543)
[2025-02-16 11:15:26,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:26,631][root][INFO] - Training Epoch: 1/2, step 2392/23838 completed (loss: 1.3531209230422974, acc: 0.6590909361839294)
[2025-02-16 11:15:26,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:27,082][root][INFO] - Training Epoch: 1/2, step 2393/23838 completed (loss: 0.6033919453620911, acc: 0.8026315569877625)
[2025-02-16 11:15:27,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:27,461][root][INFO] - Training Epoch: 1/2, step 2394/23838 completed (loss: 1.2648528814315796, acc: 0.5882353186607361)
[2025-02-16 11:15:27,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:27,936][root][INFO] - Training Epoch: 1/2, step 2395/23838 completed (loss: 1.2159185409545898, acc: 0.695652186870575)
[2025-02-16 11:15:28,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:28,352][root][INFO] - Training Epoch: 1/2, step 2396/23838 completed (loss: 1.335330605506897, acc: 0.6399999856948853)
[2025-02-16 11:15:28,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:28,781][root][INFO] - Training Epoch: 1/2, step 2397/23838 completed (loss: 1.4742239713668823, acc: 0.6851851940155029)
[2025-02-16 11:15:28,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:29,218][root][INFO] - Training Epoch: 1/2, step 2398/23838 completed (loss: 1.634952425956726, acc: 0.6166666746139526)
[2025-02-16 11:15:29,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:29,689][root][INFO] - Training Epoch: 1/2, step 2399/23838 completed (loss: 1.1999690532684326, acc: 0.6388888955116272)
[2025-02-16 11:15:29,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:30,102][root][INFO] - Training Epoch: 1/2, step 2400/23838 completed (loss: 1.093882441520691, acc: 0.6593406796455383)
[2025-02-16 11:15:30,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:30,514][root][INFO] - Training Epoch: 1/2, step 2401/23838 completed (loss: 0.9635152220726013, acc: 0.7027027010917664)
[2025-02-16 11:15:30,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:30,911][root][INFO] - Training Epoch: 1/2, step 2402/23838 completed (loss: 0.7136939167976379, acc: 0.7407407164573669)
[2025-02-16 11:15:31,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:31,365][root][INFO] - Training Epoch: 1/2, step 2403/23838 completed (loss: 1.6363799571990967, acc: 0.5)
[2025-02-16 11:15:31,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:31,784][root][INFO] - Training Epoch: 1/2, step 2404/23838 completed (loss: 1.3532487154006958, acc: 0.5555555820465088)
[2025-02-16 11:15:31,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:32,229][root][INFO] - Training Epoch: 1/2, step 2405/23838 completed (loss: 1.4422563314437866, acc: 0.6538461446762085)
[2025-02-16 11:15:32,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:32,681][root][INFO] - Training Epoch: 1/2, step 2406/23838 completed (loss: 0.8713217973709106, acc: 0.7428571581840515)
[2025-02-16 11:15:32,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:33,072][root][INFO] - Training Epoch: 1/2, step 2407/23838 completed (loss: 1.7911624908447266, acc: 0.5)
[2025-02-16 11:15:33,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:33,501][root][INFO] - Training Epoch: 1/2, step 2408/23838 completed (loss: 0.20851100981235504, acc: 1.0)
[2025-02-16 11:15:33,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:33,898][root][INFO] - Training Epoch: 1/2, step 2409/23838 completed (loss: 2.2589614391326904, acc: 0.4722222089767456)
[2025-02-16 11:15:34,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:34,358][root][INFO] - Training Epoch: 1/2, step 2410/23838 completed (loss: 1.7699636220932007, acc: 0.6206896305084229)
[2025-02-16 11:15:34,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:34,836][root][INFO] - Training Epoch: 1/2, step 2411/23838 completed (loss: 1.022547960281372, acc: 0.800000011920929)
[2025-02-16 11:15:35,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:35,273][root][INFO] - Training Epoch: 1/2, step 2412/23838 completed (loss: 0.7029063701629639, acc: 0.8414633870124817)
[2025-02-16 11:15:35,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:35,711][root][INFO] - Training Epoch: 1/2, step 2413/23838 completed (loss: 0.9762589931488037, acc: 0.8139534592628479)
[2025-02-16 11:15:35,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:36,160][root][INFO] - Training Epoch: 1/2, step 2414/23838 completed (loss: 1.9014958143234253, acc: 0.6666666865348816)
[2025-02-16 11:15:36,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:36,574][root][INFO] - Training Epoch: 1/2, step 2415/23838 completed (loss: 1.2414162158966064, acc: 0.7386363744735718)
[2025-02-16 11:15:36,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:36,959][root][INFO] - Training Epoch: 1/2, step 2416/23838 completed (loss: 1.3481014966964722, acc: 0.695652186870575)
[2025-02-16 11:15:37,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:37,348][root][INFO] - Training Epoch: 1/2, step 2417/23838 completed (loss: 1.2474043369293213, acc: 0.7142857313156128)
[2025-02-16 11:15:37,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:37,771][root][INFO] - Training Epoch: 1/2, step 2418/23838 completed (loss: 1.4467847347259521, acc: 0.6060606241226196)
[2025-02-16 11:15:37,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:38,223][root][INFO] - Training Epoch: 1/2, step 2419/23838 completed (loss: 1.251137614250183, acc: 0.6415094137191772)
[2025-02-16 11:15:38,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:38,679][root][INFO] - Training Epoch: 1/2, step 2420/23838 completed (loss: 1.168736219406128, acc: 0.5820895433425903)
[2025-02-16 11:15:38,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:39,140][root][INFO] - Training Epoch: 1/2, step 2421/23838 completed (loss: 0.7177330851554871, acc: 0.8115941882133484)
[2025-02-16 11:15:39,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:39,565][root][INFO] - Training Epoch: 1/2, step 2422/23838 completed (loss: 0.8012781143188477, acc: 0.75)
[2025-02-16 11:15:39,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:39,945][root][INFO] - Training Epoch: 1/2, step 2423/23838 completed (loss: 0.8260472416877747, acc: 0.7647058963775635)
[2025-02-16 11:15:40,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:40,380][root][INFO] - Training Epoch: 1/2, step 2424/23838 completed (loss: 1.4652061462402344, acc: 0.6666666865348816)
[2025-02-16 11:15:40,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:40,775][root][INFO] - Training Epoch: 1/2, step 2425/23838 completed (loss: 1.0246906280517578, acc: 0.7352941036224365)
[2025-02-16 11:15:40,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:41,209][root][INFO] - Training Epoch: 1/2, step 2426/23838 completed (loss: 1.5068868398666382, acc: 0.5882353186607361)
[2025-02-16 11:15:41,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:41,621][root][INFO] - Training Epoch: 1/2, step 2427/23838 completed (loss: 1.5300586223602295, acc: 0.5319148898124695)
[2025-02-16 11:15:41,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:42,003][root][INFO] - Training Epoch: 1/2, step 2428/23838 completed (loss: 1.5667896270751953, acc: 0.4901960790157318)
[2025-02-16 11:15:42,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:42,520][root][INFO] - Training Epoch: 1/2, step 2429/23838 completed (loss: 1.6387202739715576, acc: 0.5675675868988037)
[2025-02-16 11:15:42,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:42,983][root][INFO] - Training Epoch: 1/2, step 2430/23838 completed (loss: 1.8782932758331299, acc: 0.5365853905677795)
[2025-02-16 11:15:43,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:43,437][root][INFO] - Training Epoch: 1/2, step 2431/23838 completed (loss: 0.922875702381134, acc: 0.75)
[2025-02-16 11:15:43,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:43,957][root][INFO] - Training Epoch: 1/2, step 2432/23838 completed (loss: 1.2321691513061523, acc: 0.6666666865348816)
[2025-02-16 11:15:44,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:44,432][root][INFO] - Training Epoch: 1/2, step 2433/23838 completed (loss: 0.9337884783744812, acc: 0.7250000238418579)
[2025-02-16 11:15:44,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:44,874][root][INFO] - Training Epoch: 1/2, step 2434/23838 completed (loss: 1.3398929834365845, acc: 0.6399999856948853)
[2025-02-16 11:15:45,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:45,268][root][INFO] - Training Epoch: 1/2, step 2435/23838 completed (loss: 1.2417914867401123, acc: 0.6666666865348816)
[2025-02-16 11:15:45,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:45,735][root][INFO] - Training Epoch: 1/2, step 2436/23838 completed (loss: 0.7170842885971069, acc: 0.84375)
[2025-02-16 11:15:45,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:46,179][root][INFO] - Training Epoch: 1/2, step 2437/23838 completed (loss: 0.7071523666381836, acc: 0.8163265585899353)
[2025-02-16 11:15:46,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:46,649][root][INFO] - Training Epoch: 1/2, step 2438/23838 completed (loss: 1.3941773176193237, acc: 0.5862069129943848)
[2025-02-16 11:15:46,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:47,085][root][INFO] - Training Epoch: 1/2, step 2439/23838 completed (loss: 0.7966678738594055, acc: 0.7045454382896423)
[2025-02-16 11:15:47,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:47,545][root][INFO] - Training Epoch: 1/2, step 2440/23838 completed (loss: 1.8333256244659424, acc: 0.5666666626930237)
[2025-02-16 11:15:47,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:47,941][root][INFO] - Training Epoch: 1/2, step 2441/23838 completed (loss: 1.7660484313964844, acc: 0.6296296119689941)
[2025-02-16 11:15:48,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:48,319][root][INFO] - Training Epoch: 1/2, step 2442/23838 completed (loss: 1.3613147735595703, acc: 0.6285714507102966)
[2025-02-16 11:15:48,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:48,728][root][INFO] - Training Epoch: 1/2, step 2443/23838 completed (loss: 0.9989719986915588, acc: 0.7446808218955994)
[2025-02-16 11:15:48,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:49,197][root][INFO] - Training Epoch: 1/2, step 2444/23838 completed (loss: 1.0551316738128662, acc: 0.7066666483879089)
[2025-02-16 11:15:49,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:49,614][root][INFO] - Training Epoch: 1/2, step 2445/23838 completed (loss: 1.0130892992019653, acc: 0.7142857313156128)
[2025-02-16 11:15:49,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:50,125][root][INFO] - Training Epoch: 1/2, step 2446/23838 completed (loss: 0.668986439704895, acc: 0.761904776096344)
[2025-02-16 11:15:50,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:50,502][root][INFO] - Training Epoch: 1/2, step 2447/23838 completed (loss: 0.9243221282958984, acc: 0.7291666865348816)
[2025-02-16 11:15:50,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:50,915][root][INFO] - Training Epoch: 1/2, step 2448/23838 completed (loss: 1.2888479232788086, acc: 0.5897436141967773)
[2025-02-16 11:15:51,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:51,306][root][INFO] - Training Epoch: 1/2, step 2449/23838 completed (loss: 1.3330881595611572, acc: 0.6346153616905212)
[2025-02-16 11:15:51,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:51,746][root][INFO] - Training Epoch: 1/2, step 2450/23838 completed (loss: 1.4117714166641235, acc: 0.6265060305595398)
[2025-02-16 11:15:51,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:52,235][root][INFO] - Training Epoch: 1/2, step 2451/23838 completed (loss: 1.0270057916641235, acc: 0.6428571343421936)
[2025-02-16 11:15:52,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:52,678][root][INFO] - Training Epoch: 1/2, step 2452/23838 completed (loss: 1.569974660873413, acc: 0.6060606241226196)
[2025-02-16 11:15:52,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:53,102][root][INFO] - Training Epoch: 1/2, step 2453/23838 completed (loss: 1.0324997901916504, acc: 0.6315789222717285)
[2025-02-16 11:15:53,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:53,609][root][INFO] - Training Epoch: 1/2, step 2454/23838 completed (loss: 1.157486915588379, acc: 0.6382978558540344)
[2025-02-16 11:15:53,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:54,040][root][INFO] - Training Epoch: 1/2, step 2455/23838 completed (loss: 1.3186404705047607, acc: 0.6200000047683716)
[2025-02-16 11:15:54,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:54,418][root][INFO] - Training Epoch: 1/2, step 2456/23838 completed (loss: 0.8620219826698303, acc: 0.7641509175300598)
[2025-02-16 11:15:54,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:54,887][root][INFO] - Training Epoch: 1/2, step 2457/23838 completed (loss: 0.7632735967636108, acc: 0.7792207598686218)
[2025-02-16 11:15:55,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:55,366][root][INFO] - Training Epoch: 1/2, step 2458/23838 completed (loss: 0.794420599937439, acc: 0.8130841255187988)
[2025-02-16 11:15:55,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:55,762][root][INFO] - Training Epoch: 1/2, step 2459/23838 completed (loss: 0.774711549282074, acc: 0.8141592741012573)
[2025-02-16 11:15:55,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:56,200][root][INFO] - Training Epoch: 1/2, step 2460/23838 completed (loss: 0.8396127820014954, acc: 0.8333333134651184)
[2025-02-16 11:15:56,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:56,616][root][INFO] - Training Epoch: 1/2, step 2461/23838 completed (loss: 0.718193769454956, acc: 0.8068181872367859)
[2025-02-16 11:15:56,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:56,987][root][INFO] - Training Epoch: 1/2, step 2462/23838 completed (loss: 1.365543246269226, acc: 0.5675675868988037)
[2025-02-16 11:15:57,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:57,362][root][INFO] - Training Epoch: 1/2, step 2463/23838 completed (loss: 0.6203939318656921, acc: 0.7586206793785095)
[2025-02-16 11:15:57,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:57,995][root][INFO] - Training Epoch: 1/2, step 2464/23838 completed (loss: 0.5960214734077454, acc: 0.8144329786300659)
[2025-02-16 11:15:58,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:58,363][root][INFO] - Training Epoch: 1/2, step 2465/23838 completed (loss: 1.0844072103500366, acc: 0.7777777910232544)
[2025-02-16 11:15:58,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:58,830][root][INFO] - Training Epoch: 1/2, step 2466/23838 completed (loss: 0.9416454434394836, acc: 0.760869562625885)
[2025-02-16 11:15:59,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:59,361][root][INFO] - Training Epoch: 1/2, step 2467/23838 completed (loss: 0.8729040622711182, acc: 0.7674418687820435)
[2025-02-16 11:15:59,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:15:59,799][root][INFO] - Training Epoch: 1/2, step 2468/23838 completed (loss: 0.9657865166664124, acc: 0.707317054271698)
[2025-02-16 11:16:00,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:00,222][root][INFO] - Training Epoch: 1/2, step 2469/23838 completed (loss: 0.45823097229003906, acc: 0.8571428656578064)
[2025-02-16 11:16:00,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:00,707][root][INFO] - Training Epoch: 1/2, step 2470/23838 completed (loss: 0.5943806767463684, acc: 0.8488371968269348)
[2025-02-16 11:16:00,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:01,118][root][INFO] - Training Epoch: 1/2, step 2471/23838 completed (loss: 0.7778093218803406, acc: 0.8285714387893677)
[2025-02-16 11:16:01,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:01,551][root][INFO] - Training Epoch: 1/2, step 2472/23838 completed (loss: 0.8026039600372314, acc: 0.8139534592628479)
[2025-02-16 11:16:01,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:02,107][root][INFO] - Training Epoch: 1/2, step 2473/23838 completed (loss: 0.7358411550521851, acc: 0.8395061492919922)
[2025-02-16 11:16:02,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:02,609][root][INFO] - Training Epoch: 1/2, step 2474/23838 completed (loss: 0.4624921679496765, acc: 0.8870967626571655)
[2025-02-16 11:16:02,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:03,187][root][INFO] - Training Epoch: 1/2, step 2475/23838 completed (loss: 0.5359053611755371, acc: 0.8793103694915771)
[2025-02-16 11:16:03,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:03,633][root][INFO] - Training Epoch: 1/2, step 2476/23838 completed (loss: 0.788140594959259, acc: 0.7558139562606812)
[2025-02-16 11:16:03,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:04,036][root][INFO] - Training Epoch: 1/2, step 2477/23838 completed (loss: 0.5224432349205017, acc: 0.8958333134651184)
[2025-02-16 11:16:04,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:04,463][root][INFO] - Training Epoch: 1/2, step 2478/23838 completed (loss: 1.2842159271240234, acc: 0.7333333492279053)
[2025-02-16 11:16:04,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:04,932][root][INFO] - Training Epoch: 1/2, step 2479/23838 completed (loss: 0.9431887269020081, acc: 0.7702702879905701)
[2025-02-16 11:16:05,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:05,389][root][INFO] - Training Epoch: 1/2, step 2480/23838 completed (loss: 1.3232678174972534, acc: 0.6190476417541504)
[2025-02-16 11:16:05,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:05,849][root][INFO] - Training Epoch: 1/2, step 2481/23838 completed (loss: 0.4911436438560486, acc: 0.8714285492897034)
[2025-02-16 11:16:06,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:06,307][root][INFO] - Training Epoch: 1/2, step 2482/23838 completed (loss: 0.6841241121292114, acc: 0.862500011920929)
[2025-02-16 11:16:06,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:06,762][root][INFO] - Training Epoch: 1/2, step 2483/23838 completed (loss: 1.04794180393219, acc: 0.761904776096344)
[2025-02-16 11:16:07,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:07,269][root][INFO] - Training Epoch: 1/2, step 2484/23838 completed (loss: 0.7937341928482056, acc: 0.7684210538864136)
[2025-02-16 11:16:07,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:07,759][root][INFO] - Training Epoch: 1/2, step 2485/23838 completed (loss: 0.8023871183395386, acc: 0.761904776096344)
[2025-02-16 11:16:07,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:08,213][root][INFO] - Training Epoch: 1/2, step 2486/23838 completed (loss: 0.6993187069892883, acc: 0.8888888955116272)
[2025-02-16 11:16:08,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:08,715][root][INFO] - Training Epoch: 1/2, step 2487/23838 completed (loss: 1.1312371492385864, acc: 0.7284768223762512)
[2025-02-16 11:16:08,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:09,215][root][INFO] - Training Epoch: 1/2, step 2488/23838 completed (loss: 0.8859558701515198, acc: 0.8217821717262268)
[2025-02-16 11:16:09,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:09,788][root][INFO] - Training Epoch: 1/2, step 2489/23838 completed (loss: 0.6557167172431946, acc: 0.8380952477455139)
[2025-02-16 11:16:09,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:10,208][root][INFO] - Training Epoch: 1/2, step 2490/23838 completed (loss: 0.3728887736797333, acc: 0.924369752407074)
[2025-02-16 11:16:10,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:10,565][root][INFO] - Training Epoch: 1/2, step 2491/23838 completed (loss: 0.4095793664455414, acc: 0.9285714030265808)
[2025-02-16 11:16:10,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:11,077][root][INFO] - Training Epoch: 1/2, step 2492/23838 completed (loss: 0.40931037068367004, acc: 0.9117646813392639)
[2025-02-16 11:16:11,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:11,512][root][INFO] - Training Epoch: 1/2, step 2493/23838 completed (loss: 0.6489450335502625, acc: 0.8163265585899353)
[2025-02-16 11:16:11,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:12,025][root][INFO] - Training Epoch: 1/2, step 2494/23838 completed (loss: 0.7307850122451782, acc: 0.8026315569877625)
[2025-02-16 11:16:12,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:12,481][root][INFO] - Training Epoch: 1/2, step 2495/23838 completed (loss: 1.2037763595581055, acc: 0.6511628031730652)
[2025-02-16 11:16:12,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:13,129][root][INFO] - Training Epoch: 1/2, step 2496/23838 completed (loss: 0.4965888261795044, acc: 0.8382353186607361)
[2025-02-16 11:16:13,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:13,545][root][INFO] - Training Epoch: 1/2, step 2497/23838 completed (loss: 0.37713906168937683, acc: 0.9156626462936401)
[2025-02-16 11:16:13,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:13,994][root][INFO] - Training Epoch: 1/2, step 2498/23838 completed (loss: 0.5304802656173706, acc: 0.8199999928474426)
[2025-02-16 11:16:14,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:14,560][root][INFO] - Training Epoch: 1/2, step 2499/23838 completed (loss: 0.4754101634025574, acc: 0.8720930218696594)
[2025-02-16 11:16:14,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:14,962][root][INFO] - Training Epoch: 1/2, step 2500/23838 completed (loss: 0.5931630730628967, acc: 0.8426966071128845)
[2025-02-16 11:16:15,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:15,394][root][INFO] - Training Epoch: 1/2, step 2501/23838 completed (loss: 0.7825005650520325, acc: 0.8166666626930237)
[2025-02-16 11:16:15,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:15,837][root][INFO] - Training Epoch: 1/2, step 2502/23838 completed (loss: 0.33904680609703064, acc: 0.9266055226325989)
[2025-02-16 11:16:16,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:16,289][root][INFO] - Training Epoch: 1/2, step 2503/23838 completed (loss: 1.010991096496582, acc: 0.6206896305084229)
[2025-02-16 11:16:16,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:16,722][root][INFO] - Training Epoch: 1/2, step 2504/23838 completed (loss: 0.4894771873950958, acc: 0.7971014380455017)
[2025-02-16 11:16:17,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:17,450][root][INFO] - Training Epoch: 1/2, step 2505/23838 completed (loss: 0.6106172204017639, acc: 0.8472222089767456)
[2025-02-16 11:16:17,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:17,822][root][INFO] - Training Epoch: 1/2, step 2506/23838 completed (loss: 0.21114236116409302, acc: 0.9322034120559692)
[2025-02-16 11:16:18,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:18,346][root][INFO] - Training Epoch: 1/2, step 2507/23838 completed (loss: 0.39060768485069275, acc: 0.868686854839325)
[2025-02-16 11:16:18,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:18,760][root][INFO] - Training Epoch: 1/2, step 2508/23838 completed (loss: 0.5696905255317688, acc: 0.8602150678634644)
[2025-02-16 11:16:18,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:19,191][root][INFO] - Training Epoch: 1/2, step 2509/23838 completed (loss: 0.8468083143234253, acc: 0.7903226017951965)
[2025-02-16 11:16:19,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:19,895][root][INFO] - Training Epoch: 1/2, step 2510/23838 completed (loss: 1.4941236972808838, acc: 0.6103286147117615)
[2025-02-16 11:16:20,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:20,693][root][INFO] - Training Epoch: 1/2, step 2511/23838 completed (loss: 1.5176745653152466, acc: 0.6051282286643982)
[2025-02-16 11:16:20,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:21,309][root][INFO] - Training Epoch: 1/2, step 2512/23838 completed (loss: 1.0387803316116333, acc: 0.6910112500190735)
[2025-02-16 11:16:21,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:21,761][root][INFO] - Training Epoch: 1/2, step 2513/23838 completed (loss: 1.971917986869812, acc: 0.5170068144798279)
[2025-02-16 11:16:21,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:22,165][root][INFO] - Training Epoch: 1/2, step 2514/23838 completed (loss: 0.9737268090248108, acc: 0.7113401889801025)
[2025-02-16 11:16:22,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:22,638][root][INFO] - Training Epoch: 1/2, step 2515/23838 completed (loss: 1.221199870109558, acc: 0.6689655184745789)
[2025-02-16 11:16:22,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:23,068][root][INFO] - Training Epoch: 1/2, step 2516/23838 completed (loss: 1.452634572982788, acc: 0.6565656661987305)
[2025-02-16 11:16:23,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:23,452][root][INFO] - Training Epoch: 1/2, step 2517/23838 completed (loss: 1.08646821975708, acc: 0.7259259223937988)
[2025-02-16 11:16:23,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:23,896][root][INFO] - Training Epoch: 1/2, step 2518/23838 completed (loss: 1.1219003200531006, acc: 0.6907216310501099)
[2025-02-16 11:16:24,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:24,309][root][INFO] - Training Epoch: 1/2, step 2519/23838 completed (loss: 1.348713755607605, acc: 0.6336633563041687)
[2025-02-16 11:16:24,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:24,754][root][INFO] - Training Epoch: 1/2, step 2520/23838 completed (loss: 1.1326143741607666, acc: 0.6363636255264282)
[2025-02-16 11:16:24,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:25,222][root][INFO] - Training Epoch: 1/2, step 2521/23838 completed (loss: 0.9213642477989197, acc: 0.7385892271995544)
[2025-02-16 11:16:25,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:25,982][root][INFO] - Training Epoch: 1/2, step 2522/23838 completed (loss: 1.7287967205047607, acc: 0.5743243098258972)
[2025-02-16 11:16:26,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:26,583][root][INFO] - Training Epoch: 1/2, step 2523/23838 completed (loss: 0.7044223546981812, acc: 0.7961783409118652)
[2025-02-16 11:16:26,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:26,992][root][INFO] - Training Epoch: 1/2, step 2524/23838 completed (loss: 1.1334843635559082, acc: 0.7155172228813171)
[2025-02-16 11:16:27,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:27,845][root][INFO] - Training Epoch: 1/2, step 2525/23838 completed (loss: 1.097193717956543, acc: 0.7254902124404907)
[2025-02-16 11:16:27,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:28,228][root][INFO] - Training Epoch: 1/2, step 2526/23838 completed (loss: 1.0246081352233887, acc: 0.7456140518188477)
[2025-02-16 11:16:28,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:28,687][root][INFO] - Training Epoch: 1/2, step 2527/23838 completed (loss: 0.863547146320343, acc: 0.738095223903656)
[2025-02-16 11:16:28,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:29,092][root][INFO] - Training Epoch: 1/2, step 2528/23838 completed (loss: 0.6929041147232056, acc: 0.8426966071128845)
[2025-02-16 11:16:29,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:29,582][root][INFO] - Training Epoch: 1/2, step 2529/23838 completed (loss: 0.4071832001209259, acc: 0.9111111164093018)
[2025-02-16 11:16:29,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:29,994][root][INFO] - Training Epoch: 1/2, step 2530/23838 completed (loss: 1.2874420881271362, acc: 0.5365853905677795)
[2025-02-16 11:16:30,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:30,415][root][INFO] - Training Epoch: 1/2, step 2531/23838 completed (loss: 0.7212437987327576, acc: 0.7916666865348816)
[2025-02-16 11:16:30,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:30,826][root][INFO] - Training Epoch: 1/2, step 2532/23838 completed (loss: 0.5541103482246399, acc: 0.8333333134651184)
[2025-02-16 11:16:31,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:31,230][root][INFO] - Training Epoch: 1/2, step 2533/23838 completed (loss: 0.33230656385421753, acc: 0.9152542352676392)
[2025-02-16 11:16:31,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:31,677][root][INFO] - Training Epoch: 1/2, step 2534/23838 completed (loss: 0.7669623494148254, acc: 0.8181818127632141)
[2025-02-16 11:16:31,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:32,086][root][INFO] - Training Epoch: 1/2, step 2535/23838 completed (loss: 0.20591485500335693, acc: 0.9482758641242981)
[2025-02-16 11:16:32,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:32,562][root][INFO] - Training Epoch: 1/2, step 2536/23838 completed (loss: 0.11110354959964752, acc: 0.9754098653793335)
[2025-02-16 11:16:32,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:32,998][root][INFO] - Training Epoch: 1/2, step 2537/23838 completed (loss: 1.8444108963012695, acc: 0.5)
[2025-02-16 11:16:33,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:33,436][root][INFO] - Training Epoch: 1/2, step 2538/23838 completed (loss: 0.7231082916259766, acc: 0.7922077775001526)
[2025-02-16 11:16:33,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:34,174][root][INFO] - Training Epoch: 1/2, step 2539/23838 completed (loss: 1.0567176342010498, acc: 0.7096773982048035)
[2025-02-16 11:16:34,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:34,624][root][INFO] - Training Epoch: 1/2, step 2540/23838 completed (loss: 0.7649946808815002, acc: 0.7906976938247681)
[2025-02-16 11:16:34,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:35,090][root][INFO] - Training Epoch: 1/2, step 2541/23838 completed (loss: 0.3761522173881531, acc: 0.8604651093482971)
[2025-02-16 11:16:35,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:35,494][root][INFO] - Training Epoch: 1/2, step 2542/23838 completed (loss: 0.8263170123100281, acc: 0.7567567825317383)
[2025-02-16 11:16:35,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:35,944][root][INFO] - Training Epoch: 1/2, step 2543/23838 completed (loss: 0.7108244895935059, acc: 0.7941176295280457)
[2025-02-16 11:16:36,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:36,370][root][INFO] - Training Epoch: 1/2, step 2544/23838 completed (loss: 0.3710515797138214, acc: 0.9230769276618958)
[2025-02-16 11:16:36,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:36,798][root][INFO] - Training Epoch: 1/2, step 2545/23838 completed (loss: 0.07093368470668793, acc: 1.0)
[2025-02-16 11:16:37,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:37,287][root][INFO] - Training Epoch: 1/2, step 2546/23838 completed (loss: 1.029396653175354, acc: 0.7323943376541138)
[2025-02-16 11:16:37,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:37,671][root][INFO] - Training Epoch: 1/2, step 2547/23838 completed (loss: 0.8886855244636536, acc: 0.698113203048706)
[2025-02-16 11:16:37,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:38,026][root][INFO] - Training Epoch: 1/2, step 2548/23838 completed (loss: 0.6665486693382263, acc: 0.8461538553237915)
[2025-02-16 11:16:38,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:38,456][root][INFO] - Training Epoch: 1/2, step 2549/23838 completed (loss: 1.043989658355713, acc: 0.7592592835426331)
[2025-02-16 11:16:38,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:38,921][root][INFO] - Training Epoch: 1/2, step 2550/23838 completed (loss: 0.7810962200164795, acc: 0.7317073345184326)
[2025-02-16 11:16:39,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:39,338][root][INFO] - Training Epoch: 1/2, step 2551/23838 completed (loss: 0.9789944291114807, acc: 0.692307710647583)
[2025-02-16 11:16:39,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:39,763][root][INFO] - Training Epoch: 1/2, step 2552/23838 completed (loss: 0.745112419128418, acc: 0.8636363744735718)
[2025-02-16 11:16:39,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:40,225][root][INFO] - Training Epoch: 1/2, step 2553/23838 completed (loss: 0.64931720495224, acc: 0.8148148059844971)
[2025-02-16 11:16:40,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:40,682][root][INFO] - Training Epoch: 1/2, step 2554/23838 completed (loss: 1.4448567628860474, acc: 0.6515151262283325)
[2025-02-16 11:16:40,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:41,119][root][INFO] - Training Epoch: 1/2, step 2555/23838 completed (loss: 1.1252275705337524, acc: 0.7415730357170105)
[2025-02-16 11:16:41,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:41,520][root][INFO] - Training Epoch: 1/2, step 2556/23838 completed (loss: 0.4852215349674225, acc: 0.862500011920929)
[2025-02-16 11:16:41,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:41,908][root][INFO] - Training Epoch: 1/2, step 2557/23838 completed (loss: 0.6526122689247131, acc: 0.8837209343910217)
[2025-02-16 11:16:42,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:42,280][root][INFO] - Training Epoch: 1/2, step 2558/23838 completed (loss: 1.7194502353668213, acc: 0.5555555820465088)
[2025-02-16 11:16:42,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:42,753][root][INFO] - Training Epoch: 1/2, step 2559/23838 completed (loss: 1.25038743019104, acc: 0.7252747416496277)
[2025-02-16 11:16:42,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:43,203][root][INFO] - Training Epoch: 1/2, step 2560/23838 completed (loss: 0.4040369689464569, acc: 0.9193548560142517)
[2025-02-16 11:16:43,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:43,743][root][INFO] - Training Epoch: 1/2, step 2561/23838 completed (loss: 0.7772720456123352, acc: 0.7654321193695068)
[2025-02-16 11:16:43,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:44,207][root][INFO] - Training Epoch: 1/2, step 2562/23838 completed (loss: 0.2915774881839752, acc: 0.9270833134651184)
[2025-02-16 11:16:44,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:44,680][root][INFO] - Training Epoch: 1/2, step 2563/23838 completed (loss: 0.21899405121803284, acc: 0.9677419066429138)
[2025-02-16 11:16:45,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:45,346][root][INFO] - Training Epoch: 1/2, step 2564/23838 completed (loss: 0.5274569988250732, acc: 0.8390804529190063)
[2025-02-16 11:16:45,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:45,756][root][INFO] - Training Epoch: 1/2, step 2565/23838 completed (loss: 0.7794177532196045, acc: 0.761904776096344)
[2025-02-16 11:16:45,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:46,192][root][INFO] - Training Epoch: 1/2, step 2566/23838 completed (loss: 0.7243379354476929, acc: 0.8181818127632141)
[2025-02-16 11:16:46,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:46,620][root][INFO] - Training Epoch: 1/2, step 2567/23838 completed (loss: 0.36358675360679626, acc: 0.8947368264198303)
[2025-02-16 11:16:46,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:47,053][root][INFO] - Training Epoch: 1/2, step 2568/23838 completed (loss: 0.3680526614189148, acc: 0.8970588445663452)
[2025-02-16 11:16:47,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:47,497][root][INFO] - Training Epoch: 1/2, step 2569/23838 completed (loss: 1.3613115549087524, acc: 0.7045454382896423)
[2025-02-16 11:16:47,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:47,897][root][INFO] - Training Epoch: 1/2, step 2570/23838 completed (loss: 0.4855344891548157, acc: 0.8709677457809448)
[2025-02-16 11:16:48,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:48,401][root][INFO] - Training Epoch: 1/2, step 2571/23838 completed (loss: 0.4246724247932434, acc: 0.9041095972061157)
[2025-02-16 11:16:48,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:48,846][root][INFO] - Training Epoch: 1/2, step 2572/23838 completed (loss: 1.1942434310913086, acc: 0.7096773982048035)
[2025-02-16 11:16:49,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:49,425][root][INFO] - Training Epoch: 1/2, step 2573/23838 completed (loss: 1.8178060054779053, acc: 0.5490196347236633)
[2025-02-16 11:16:49,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:49,827][root][INFO] - Training Epoch: 1/2, step 2574/23838 completed (loss: 1.7487975358963013, acc: 0.692307710647583)
[2025-02-16 11:16:49,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:50,262][root][INFO] - Training Epoch: 1/2, step 2575/23838 completed (loss: 1.2572211027145386, acc: 0.6470588445663452)
[2025-02-16 11:16:50,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:50,763][root][INFO] - Training Epoch: 1/2, step 2576/23838 completed (loss: 1.796294093132019, acc: 0.5675675868988037)
[2025-02-16 11:16:50,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:51,213][root][INFO] - Training Epoch: 1/2, step 2577/23838 completed (loss: 1.488666296005249, acc: 0.5612244606018066)
[2025-02-16 11:16:51,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:51,702][root][INFO] - Training Epoch: 1/2, step 2578/23838 completed (loss: 1.1468398571014404, acc: 0.6708860993385315)
[2025-02-16 11:16:51,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:52,081][root][INFO] - Training Epoch: 1/2, step 2579/23838 completed (loss: 1.2732728719711304, acc: 0.5873016119003296)
[2025-02-16 11:16:52,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:52,570][root][INFO] - Training Epoch: 1/2, step 2580/23838 completed (loss: 1.1934961080551147, acc: 0.6714285612106323)
[2025-02-16 11:16:52,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:52,991][root][INFO] - Training Epoch: 1/2, step 2581/23838 completed (loss: 1.531981110572815, acc: 0.6222222447395325)
[2025-02-16 11:16:53,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:53,414][root][INFO] - Training Epoch: 1/2, step 2582/23838 completed (loss: 1.6963733434677124, acc: 0.5249999761581421)
[2025-02-16 11:16:53,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:54,097][root][INFO] - Training Epoch: 1/2, step 2583/23838 completed (loss: 1.4721356630325317, acc: 0.6590909361839294)
[2025-02-16 11:16:54,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:54,533][root][INFO] - Training Epoch: 1/2, step 2584/23838 completed (loss: 1.6920346021652222, acc: 0.523809552192688)
[2025-02-16 11:16:54,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:54,986][root][INFO] - Training Epoch: 1/2, step 2585/23838 completed (loss: 0.9963534474372864, acc: 0.6571428775787354)
[2025-02-16 11:16:55,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:55,466][root][INFO] - Training Epoch: 1/2, step 2586/23838 completed (loss: 1.274427056312561, acc: 0.641791045665741)
[2025-02-16 11:16:55,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:55,892][root][INFO] - Training Epoch: 1/2, step 2587/23838 completed (loss: 1.2745013236999512, acc: 0.6000000238418579)
[2025-02-16 11:16:56,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:56,348][root][INFO] - Training Epoch: 1/2, step 2588/23838 completed (loss: 1.1638901233673096, acc: 0.7307692170143127)
[2025-02-16 11:16:56,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:56,970][root][INFO] - Training Epoch: 1/2, step 2589/23838 completed (loss: 0.9599644541740417, acc: 0.7234042286872864)
[2025-02-16 11:16:57,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:57,455][root][INFO] - Training Epoch: 1/2, step 2590/23838 completed (loss: 0.9212973713874817, acc: 0.7592592835426331)
[2025-02-16 11:16:57,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:57,885][root][INFO] - Training Epoch: 1/2, step 2591/23838 completed (loss: 1.3235533237457275, acc: 0.6029411554336548)
[2025-02-16 11:16:58,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:58,253][root][INFO] - Training Epoch: 1/2, step 2592/23838 completed (loss: 1.359449028968811, acc: 0.5882353186607361)
[2025-02-16 11:16:58,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:58,643][root][INFO] - Training Epoch: 1/2, step 2593/23838 completed (loss: 1.1462664604187012, acc: 0.7045454382896423)
[2025-02-16 11:16:58,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:59,086][root][INFO] - Training Epoch: 1/2, step 2594/23838 completed (loss: 2.1066291332244873, acc: 0.44999998807907104)
[2025-02-16 11:16:59,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:59,539][root][INFO] - Training Epoch: 1/2, step 2595/23838 completed (loss: 1.284125804901123, acc: 0.625)
[2025-02-16 11:16:59,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:16:59,999][root][INFO] - Training Epoch: 1/2, step 2596/23838 completed (loss: 1.0927326679229736, acc: 0.6739130616188049)
[2025-02-16 11:17:00,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:00,367][root][INFO] - Training Epoch: 1/2, step 2597/23838 completed (loss: 0.9895330667495728, acc: 0.6399999856948853)
[2025-02-16 11:17:00,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:00,811][root][INFO] - Training Epoch: 1/2, step 2598/23838 completed (loss: 1.127278208732605, acc: 0.6764705777168274)
[2025-02-16 11:17:01,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:01,259][root][INFO] - Training Epoch: 1/2, step 2599/23838 completed (loss: 1.092501163482666, acc: 0.5652173757553101)
[2025-02-16 11:17:01,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:01,683][root][INFO] - Training Epoch: 1/2, step 2600/23838 completed (loss: 1.613552451133728, acc: 0.5909090638160706)
[2025-02-16 11:17:01,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:02,118][root][INFO] - Training Epoch: 1/2, step 2601/23838 completed (loss: 0.7630912661552429, acc: 0.8222222328186035)
[2025-02-16 11:17:02,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:02,715][root][INFO] - Training Epoch: 1/2, step 2602/23838 completed (loss: 0.7488279938697815, acc: 0.7647058963775635)
[2025-02-16 11:17:02,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:03,157][root][INFO] - Training Epoch: 1/2, step 2603/23838 completed (loss: 1.64090096950531, acc: 0.568965494632721)
[2025-02-16 11:17:03,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:03,569][root][INFO] - Training Epoch: 1/2, step 2604/23838 completed (loss: 0.7264959216117859, acc: 0.7567567825317383)
[2025-02-16 11:17:03,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:03,953][root][INFO] - Training Epoch: 1/2, step 2605/23838 completed (loss: 0.9579910039901733, acc: 0.6756756901741028)
[2025-02-16 11:17:04,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:04,398][root][INFO] - Training Epoch: 1/2, step 2606/23838 completed (loss: 0.9686943292617798, acc: 0.7428571581840515)
[2025-02-16 11:17:04,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:04,752][root][INFO] - Training Epoch: 1/2, step 2607/23838 completed (loss: 1.0881651639938354, acc: 0.699999988079071)
[2025-02-16 11:17:04,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:05,217][root][INFO] - Training Epoch: 1/2, step 2608/23838 completed (loss: 0.6954944133758545, acc: 0.8367347121238708)
[2025-02-16 11:17:05,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:05,750][root][INFO] - Training Epoch: 1/2, step 2609/23838 completed (loss: 1.2384710311889648, acc: 0.6704545617103577)
[2025-02-16 11:17:05,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:06,177][root][INFO] - Training Epoch: 1/2, step 2610/23838 completed (loss: 1.2611137628555298, acc: 0.782608687877655)
[2025-02-16 11:17:06,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:06,607][root][INFO] - Training Epoch: 1/2, step 2611/23838 completed (loss: 1.0351308584213257, acc: 0.7142857313156128)
[2025-02-16 11:17:06,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:07,085][root][INFO] - Training Epoch: 1/2, step 2612/23838 completed (loss: 1.1575267314910889, acc: 0.6774193644523621)
[2025-02-16 11:17:07,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:07,448][root][INFO] - Training Epoch: 1/2, step 2613/23838 completed (loss: 1.3527222871780396, acc: 0.649350643157959)
[2025-02-16 11:17:07,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:07,804][root][INFO] - Training Epoch: 1/2, step 2614/23838 completed (loss: 1.7430360317230225, acc: 0.5)
[2025-02-16 11:17:08,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:08,228][root][INFO] - Training Epoch: 1/2, step 2615/23838 completed (loss: 1.2626659870147705, acc: 0.6610169410705566)
[2025-02-16 11:17:08,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:08,658][root][INFO] - Training Epoch: 1/2, step 2616/23838 completed (loss: 1.7440699338912964, acc: 0.5600000023841858)
[2025-02-16 11:17:08,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:09,192][root][INFO] - Training Epoch: 1/2, step 2617/23838 completed (loss: 1.1354670524597168, acc: 0.6585366129875183)
[2025-02-16 11:17:09,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:09,629][root][INFO] - Training Epoch: 1/2, step 2618/23838 completed (loss: 1.692222237586975, acc: 0.5641025900840759)
[2025-02-16 11:17:09,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:10,048][root][INFO] - Training Epoch: 1/2, step 2619/23838 completed (loss: 1.2098842859268188, acc: 0.6938775777816772)
[2025-02-16 11:17:10,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:10,436][root][INFO] - Training Epoch: 1/2, step 2620/23838 completed (loss: 0.9785149097442627, acc: 0.7083333134651184)
[2025-02-16 11:17:10,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:10,811][root][INFO] - Training Epoch: 1/2, step 2621/23838 completed (loss: 0.8675681948661804, acc: 0.7657142877578735)
[2025-02-16 11:17:11,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:11,267][root][INFO] - Training Epoch: 1/2, step 2622/23838 completed (loss: 0.6254408359527588, acc: 0.8032786846160889)
[2025-02-16 11:17:11,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:11,746][root][INFO] - Training Epoch: 1/2, step 2623/23838 completed (loss: 0.8915901780128479, acc: 0.7623762488365173)
[2025-02-16 11:17:11,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:12,189][root][INFO] - Training Epoch: 1/2, step 2624/23838 completed (loss: 1.1308473348617554, acc: 0.704081654548645)
[2025-02-16 11:17:12,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:12,577][root][INFO] - Training Epoch: 1/2, step 2625/23838 completed (loss: 0.6106411218643188, acc: 0.8091602921485901)
[2025-02-16 11:17:12,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:12,999][root][INFO] - Training Epoch: 1/2, step 2626/23838 completed (loss: 0.8181496262550354, acc: 0.7606837749481201)
[2025-02-16 11:17:13,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:13,385][root][INFO] - Training Epoch: 1/2, step 2627/23838 completed (loss: 1.028327465057373, acc: 0.6944444179534912)
[2025-02-16 11:17:13,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:13,822][root][INFO] - Training Epoch: 1/2, step 2628/23838 completed (loss: 1.1739146709442139, acc: 0.6637930870056152)
[2025-02-16 11:17:13,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:14,213][root][INFO] - Training Epoch: 1/2, step 2629/23838 completed (loss: 0.7605228424072266, acc: 0.7761194109916687)
[2025-02-16 11:17:14,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:14,662][root][INFO] - Training Epoch: 1/2, step 2630/23838 completed (loss: 0.6870765686035156, acc: 0.7880794405937195)
[2025-02-16 11:17:14,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:15,102][root][INFO] - Training Epoch: 1/2, step 2631/23838 completed (loss: 0.7649754881858826, acc: 0.758865237236023)
[2025-02-16 11:17:15,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:15,481][root][INFO] - Training Epoch: 1/2, step 2632/23838 completed (loss: 0.9067363739013672, acc: 0.730434775352478)
[2025-02-16 11:17:15,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:15,875][root][INFO] - Training Epoch: 1/2, step 2633/23838 completed (loss: 0.8225094676017761, acc: 0.7710843086242676)
[2025-02-16 11:17:16,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:16,275][root][INFO] - Training Epoch: 1/2, step 2634/23838 completed (loss: 0.8746501803398132, acc: 0.7560975551605225)
[2025-02-16 11:17:16,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:16,678][root][INFO] - Training Epoch: 1/2, step 2635/23838 completed (loss: 1.2310792207717896, acc: 0.6762589812278748)
[2025-02-16 11:17:16,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:17,149][root][INFO] - Training Epoch: 1/2, step 2636/23838 completed (loss: 0.7869734764099121, acc: 0.8387096524238586)
[2025-02-16 11:17:17,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:17,589][root][INFO] - Training Epoch: 1/2, step 2637/23838 completed (loss: 0.9247204661369324, acc: 0.7560975551605225)
[2025-02-16 11:17:17,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:17,970][root][INFO] - Training Epoch: 1/2, step 2638/23838 completed (loss: 0.8649806976318359, acc: 0.7865168452262878)
[2025-02-16 11:17:18,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:18,340][root][INFO] - Training Epoch: 1/2, step 2639/23838 completed (loss: 0.7833036184310913, acc: 0.8157894611358643)
[2025-02-16 11:17:18,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:18,768][root][INFO] - Training Epoch: 1/2, step 2640/23838 completed (loss: 0.7644606828689575, acc: 0.7861271500587463)
[2025-02-16 11:17:18,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:19,183][root][INFO] - Training Epoch: 1/2, step 2641/23838 completed (loss: 0.7350467443466187, acc: 0.8283582329750061)
[2025-02-16 11:17:19,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:19,627][root][INFO] - Training Epoch: 1/2, step 2642/23838 completed (loss: 1.6607141494750977, acc: 0.5476190447807312)
[2025-02-16 11:17:19,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:20,040][root][INFO] - Training Epoch: 1/2, step 2643/23838 completed (loss: 0.9966768026351929, acc: 0.7121211886405945)
[2025-02-16 11:17:20,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:20,503][root][INFO] - Training Epoch: 1/2, step 2644/23838 completed (loss: 0.7074214816093445, acc: 0.7680000066757202)
[2025-02-16 11:17:20,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:20,870][root][INFO] - Training Epoch: 1/2, step 2645/23838 completed (loss: 1.223454236984253, acc: 0.6842105388641357)
[2025-02-16 11:17:21,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:21,254][root][INFO] - Training Epoch: 1/2, step 2646/23838 completed (loss: 0.8092230558395386, acc: 0.737500011920929)
[2025-02-16 11:17:21,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:21,638][root][INFO] - Training Epoch: 1/2, step 2647/23838 completed (loss: 0.638941764831543, acc: 0.8344370722770691)
[2025-02-16 11:17:21,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:22,055][root][INFO] - Training Epoch: 1/2, step 2648/23838 completed (loss: 1.0441759824752808, acc: 0.6777777671813965)
[2025-02-16 11:17:22,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:22,494][root][INFO] - Training Epoch: 1/2, step 2649/23838 completed (loss: 0.7658345103263855, acc: 0.7676767706871033)
[2025-02-16 11:17:22,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:22,934][root][INFO] - Training Epoch: 1/2, step 2650/23838 completed (loss: 1.013270616531372, acc: 0.7130434513092041)
[2025-02-16 11:17:23,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:23,398][root][INFO] - Training Epoch: 1/2, step 2651/23838 completed (loss: 0.8516761064529419, acc: 0.7647058963775635)
[2025-02-16 11:17:23,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:23,851][root][INFO] - Training Epoch: 1/2, step 2652/23838 completed (loss: 0.7644063830375671, acc: 0.7830188870429993)
[2025-02-16 11:17:24,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:24,277][root][INFO] - Training Epoch: 1/2, step 2653/23838 completed (loss: 1.2148176431655884, acc: 0.6428571343421936)
[2025-02-16 11:17:24,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:24,707][root][INFO] - Training Epoch: 1/2, step 2654/23838 completed (loss: 1.2001134157180786, acc: 0.6517857313156128)
[2025-02-16 11:17:24,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:25,130][root][INFO] - Training Epoch: 1/2, step 2655/23838 completed (loss: 1.1605662107467651, acc: 0.6842105388641357)
[2025-02-16 11:17:25,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:25,535][root][INFO] - Training Epoch: 1/2, step 2656/23838 completed (loss: 0.7879309058189392, acc: 0.7647058963775635)
[2025-02-16 11:17:25,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:26,011][root][INFO] - Training Epoch: 1/2, step 2657/23838 completed (loss: 1.0478640794754028, acc: 0.7329192757606506)
[2025-02-16 11:17:26,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:26,393][root][INFO] - Training Epoch: 1/2, step 2658/23838 completed (loss: 0.6421930193901062, acc: 0.8484848737716675)
[2025-02-16 11:17:26,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:26,825][root][INFO] - Training Epoch: 1/2, step 2659/23838 completed (loss: 1.1361500024795532, acc: 0.7460317611694336)
[2025-02-16 11:17:26,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:27,224][root][INFO] - Training Epoch: 1/2, step 2660/23838 completed (loss: 0.5668071508407593, acc: 0.8421052694320679)
[2025-02-16 11:17:27,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:27,610][root][INFO] - Training Epoch: 1/2, step 2661/23838 completed (loss: 0.9681274890899658, acc: 0.7763158082962036)
[2025-02-16 11:17:27,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:28,015][root][INFO] - Training Epoch: 1/2, step 2662/23838 completed (loss: 1.147273302078247, acc: 0.6578947305679321)
[2025-02-16 11:17:28,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:28,429][root][INFO] - Training Epoch: 1/2, step 2663/23838 completed (loss: 1.2033023834228516, acc: 0.7244898080825806)
[2025-02-16 11:17:28,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:28,813][root][INFO] - Training Epoch: 1/2, step 2664/23838 completed (loss: 0.8384031653404236, acc: 0.7615384459495544)
[2025-02-16 11:17:29,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:29,225][root][INFO] - Training Epoch: 1/2, step 2665/23838 completed (loss: 1.0593501329421997, acc: 0.75)
[2025-02-16 11:17:29,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:29,643][root][INFO] - Training Epoch: 1/2, step 2666/23838 completed (loss: 1.061675786972046, acc: 0.6666666865348816)
[2025-02-16 11:17:29,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:30,072][root][INFO] - Training Epoch: 1/2, step 2667/23838 completed (loss: 0.8961626291275024, acc: 0.734375)
[2025-02-16 11:17:30,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:30,483][root][INFO] - Training Epoch: 1/2, step 2668/23838 completed (loss: 0.8050373792648315, acc: 0.8010203838348389)
[2025-02-16 11:17:30,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:30,852][root][INFO] - Training Epoch: 1/2, step 2669/23838 completed (loss: 1.0204663276672363, acc: 0.7049180269241333)
[2025-02-16 11:17:30,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:31,215][root][INFO] - Training Epoch: 1/2, step 2670/23838 completed (loss: 0.9428159594535828, acc: 0.767241358757019)
[2025-02-16 11:17:31,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:31,575][root][INFO] - Training Epoch: 1/2, step 2671/23838 completed (loss: 0.9927195906639099, acc: 0.7115384340286255)
[2025-02-16 11:17:31,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:32,018][root][INFO] - Training Epoch: 1/2, step 2672/23838 completed (loss: 0.7278875112533569, acc: 0.8103448152542114)
[2025-02-16 11:17:32,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:32,461][root][INFO] - Training Epoch: 1/2, step 2673/23838 completed (loss: 0.9711808562278748, acc: 0.7373737096786499)
[2025-02-16 11:17:32,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:32,884][root][INFO] - Training Epoch: 1/2, step 2674/23838 completed (loss: 1.0934531688690186, acc: 0.7264957427978516)
[2025-02-16 11:17:33,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:33,284][root][INFO] - Training Epoch: 1/2, step 2675/23838 completed (loss: 1.0556995868682861, acc: 0.6915887594223022)
[2025-02-16 11:17:33,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:33,663][root][INFO] - Training Epoch: 1/2, step 2676/23838 completed (loss: 0.7997251152992249, acc: 0.7727272510528564)
[2025-02-16 11:17:33,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:34,183][root][INFO] - Training Epoch: 1/2, step 2677/23838 completed (loss: 1.0750821828842163, acc: 0.7384615540504456)
[2025-02-16 11:17:34,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:34,607][root][INFO] - Training Epoch: 1/2, step 2678/23838 completed (loss: 0.8434518575668335, acc: 0.739130437374115)
[2025-02-16 11:17:34,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:34,978][root][INFO] - Training Epoch: 1/2, step 2679/23838 completed (loss: 1.273513913154602, acc: 0.6052631735801697)
[2025-02-16 11:17:35,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:35,383][root][INFO] - Training Epoch: 1/2, step 2680/23838 completed (loss: 1.2927254438400269, acc: 0.6627907156944275)
[2025-02-16 11:17:35,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:35,763][root][INFO] - Training Epoch: 1/2, step 2681/23838 completed (loss: 0.9192384481430054, acc: 0.7403846383094788)
[2025-02-16 11:17:35,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:36,159][root][INFO] - Training Epoch: 1/2, step 2682/23838 completed (loss: 0.8407673835754395, acc: 0.8130841255187988)
[2025-02-16 11:17:36,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:36,518][root][INFO] - Training Epoch: 1/2, step 2683/23838 completed (loss: 0.7355019450187683, acc: 0.8524590134620667)
[2025-02-16 11:17:36,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:36,944][root][INFO] - Training Epoch: 1/2, step 2684/23838 completed (loss: 0.6006371974945068, acc: 0.8359375)
[2025-02-16 11:17:37,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:37,344][root][INFO] - Training Epoch: 1/2, step 2685/23838 completed (loss: 0.6738341450691223, acc: 0.8205128312110901)
[2025-02-16 11:17:37,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:37,741][root][INFO] - Training Epoch: 1/2, step 2686/23838 completed (loss: 0.8162358999252319, acc: 0.7641509175300598)
[2025-02-16 11:17:37,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:38,181][root][INFO] - Training Epoch: 1/2, step 2687/23838 completed (loss: 0.8265604376792908, acc: 0.761904776096344)
[2025-02-16 11:17:38,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:38,642][root][INFO] - Training Epoch: 1/2, step 2688/23838 completed (loss: 0.9187119007110596, acc: 0.75)
[2025-02-16 11:17:38,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:39,045][root][INFO] - Training Epoch: 1/2, step 2689/23838 completed (loss: 0.646865725517273, acc: 0.8207547068595886)
[2025-02-16 11:17:39,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:39,494][root][INFO] - Training Epoch: 1/2, step 2690/23838 completed (loss: 0.67453533411026, acc: 0.8611111044883728)
[2025-02-16 11:17:39,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:39,862][root][INFO] - Training Epoch: 1/2, step 2691/23838 completed (loss: 0.5708721876144409, acc: 0.875)
[2025-02-16 11:17:40,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:40,296][root][INFO] - Training Epoch: 1/2, step 2692/23838 completed (loss: 0.7884724140167236, acc: 0.8059701323509216)
[2025-02-16 11:17:40,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:40,712][root][INFO] - Training Epoch: 1/2, step 2693/23838 completed (loss: 0.6241428256034851, acc: 0.8547008633613586)
[2025-02-16 11:17:40,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:41,170][root][INFO] - Training Epoch: 1/2, step 2694/23838 completed (loss: 0.8546163439750671, acc: 0.7755101919174194)
[2025-02-16 11:17:41,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:41,588][root][INFO] - Training Epoch: 1/2, step 2695/23838 completed (loss: 0.6068177819252014, acc: 0.8244274854660034)
[2025-02-16 11:17:41,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:42,000][root][INFO] - Training Epoch: 1/2, step 2696/23838 completed (loss: 0.864138662815094, acc: 0.7586206793785095)
[2025-02-16 11:17:42,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:42,377][root][INFO] - Training Epoch: 1/2, step 2697/23838 completed (loss: 2.1794509887695312, acc: 0.4166666567325592)
[2025-02-16 11:17:42,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:42,740][root][INFO] - Training Epoch: 1/2, step 2698/23838 completed (loss: 0.8482085466384888, acc: 0.7586206793785095)
[2025-02-16 11:17:42,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:43,124][root][INFO] - Training Epoch: 1/2, step 2699/23838 completed (loss: 0.937979519367218, acc: 0.7246376872062683)
[2025-02-16 11:17:43,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:43,518][root][INFO] - Training Epoch: 1/2, step 2700/23838 completed (loss: 0.8712021708488464, acc: 0.7523809671401978)
[2025-02-16 11:17:43,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:43,920][root][INFO] - Training Epoch: 1/2, step 2701/23838 completed (loss: 0.6890248656272888, acc: 0.7848101258277893)
[2025-02-16 11:17:44,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:44,377][root][INFO] - Training Epoch: 1/2, step 2702/23838 completed (loss: 1.1527725458145142, acc: 0.7216494679450989)
[2025-02-16 11:17:44,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:44,802][root][INFO] - Training Epoch: 1/2, step 2703/23838 completed (loss: 1.0494203567504883, acc: 0.7142857313156128)
[2025-02-16 11:17:44,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:45,224][root][INFO] - Training Epoch: 1/2, step 2704/23838 completed (loss: 1.118518352508545, acc: 0.699999988079071)
[2025-02-16 11:17:45,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:45,637][root][INFO] - Training Epoch: 1/2, step 2705/23838 completed (loss: 1.5372451543807983, acc: 0.5679012537002563)
[2025-02-16 11:17:45,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:46,109][root][INFO] - Training Epoch: 1/2, step 2706/23838 completed (loss: 0.7453106045722961, acc: 0.7802197933197021)
[2025-02-16 11:17:46,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:46,537][root][INFO] - Training Epoch: 1/2, step 2707/23838 completed (loss: 0.7469978928565979, acc: 0.7415730357170105)
[2025-02-16 11:17:46,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:46,954][root][INFO] - Training Epoch: 1/2, step 2708/23838 completed (loss: 0.7462822794914246, acc: 0.8064516186714172)
[2025-02-16 11:17:47,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:47,395][root][INFO] - Training Epoch: 1/2, step 2709/23838 completed (loss: 0.4477902352809906, acc: 0.8695651888847351)
[2025-02-16 11:17:47,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:47,821][root][INFO] - Training Epoch: 1/2, step 2710/23838 completed (loss: 0.6018174886703491, acc: 0.84112149477005)
[2025-02-16 11:17:48,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:48,263][root][INFO] - Training Epoch: 1/2, step 2711/23838 completed (loss: 0.8855206370353699, acc: 0.7801418304443359)
[2025-02-16 11:17:48,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:48,681][root][INFO] - Training Epoch: 1/2, step 2712/23838 completed (loss: 1.7091907262802124, acc: 0.5333333611488342)
[2025-02-16 11:17:48,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:49,120][root][INFO] - Training Epoch: 1/2, step 2713/23838 completed (loss: 1.295596957206726, acc: 0.6285714507102966)
[2025-02-16 11:17:49,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:49,589][root][INFO] - Training Epoch: 1/2, step 2714/23838 completed (loss: 0.8969098925590515, acc: 0.7866666913032532)
[2025-02-16 11:17:49,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:50,025][root][INFO] - Training Epoch: 1/2, step 2715/23838 completed (loss: 0.7761861681938171, acc: 0.7931034564971924)
[2025-02-16 11:17:50,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:50,469][root][INFO] - Training Epoch: 1/2, step 2716/23838 completed (loss: 0.7308914065361023, acc: 0.8137931227684021)
[2025-02-16 11:17:50,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:50,934][root][INFO] - Training Epoch: 1/2, step 2717/23838 completed (loss: 0.46765077114105225, acc: 0.8841463327407837)
[2025-02-16 11:17:51,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:51,363][root][INFO] - Training Epoch: 1/2, step 2718/23838 completed (loss: 0.5129199624061584, acc: 0.8811880946159363)
[2025-02-16 11:17:51,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:51,757][root][INFO] - Training Epoch: 1/2, step 2719/23838 completed (loss: 0.5751528739929199, acc: 0.8421052694320679)
[2025-02-16 11:17:51,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:52,149][root][INFO] - Training Epoch: 1/2, step 2720/23838 completed (loss: 0.6330127120018005, acc: 0.8172042965888977)
[2025-02-16 11:17:52,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:52,546][root][INFO] - Training Epoch: 1/2, step 2721/23838 completed (loss: 0.992579996585846, acc: 0.719298243522644)
[2025-02-16 11:17:52,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:52,988][root][INFO] - Training Epoch: 1/2, step 2722/23838 completed (loss: 1.0889978408813477, acc: 0.7244898080825806)
[2025-02-16 11:17:53,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:53,440][root][INFO] - Training Epoch: 1/2, step 2723/23838 completed (loss: 1.235622525215149, acc: 0.642201840877533)
[2025-02-16 11:17:53,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:53,832][root][INFO] - Training Epoch: 1/2, step 2724/23838 completed (loss: 0.5323387980461121, acc: 0.8399999737739563)
[2025-02-16 11:17:54,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:54,235][root][INFO] - Training Epoch: 1/2, step 2725/23838 completed (loss: 0.6821233034133911, acc: 0.8091602921485901)
[2025-02-16 11:17:54,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:54,670][root][INFO] - Training Epoch: 1/2, step 2726/23838 completed (loss: 0.5567072033882141, acc: 0.8351648449897766)
[2025-02-16 11:17:54,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:55,130][root][INFO] - Training Epoch: 1/2, step 2727/23838 completed (loss: 0.6568267941474915, acc: 0.8053097128868103)
[2025-02-16 11:17:55,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:55,505][root][INFO] - Training Epoch: 1/2, step 2728/23838 completed (loss: 0.9513354897499084, acc: 0.75)
[2025-02-16 11:17:55,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:55,905][root][INFO] - Training Epoch: 1/2, step 2729/23838 completed (loss: 1.1719690561294556, acc: 0.7037037014961243)
[2025-02-16 11:17:56,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:56,274][root][INFO] - Training Epoch: 1/2, step 2730/23838 completed (loss: 0.6516884565353394, acc: 0.8125)
[2025-02-16 11:17:56,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:56,671][root][INFO] - Training Epoch: 1/2, step 2731/23838 completed (loss: 0.8988334536552429, acc: 0.7272727489471436)
[2025-02-16 11:17:56,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:57,121][root][INFO] - Training Epoch: 1/2, step 2732/23838 completed (loss: 1.3805134296417236, acc: 0.6333333253860474)
[2025-02-16 11:17:57,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:57,527][root][INFO] - Training Epoch: 1/2, step 2733/23838 completed (loss: 1.2835825681686401, acc: 0.6608695387840271)
[2025-02-16 11:17:57,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:57,990][root][INFO] - Training Epoch: 1/2, step 2734/23838 completed (loss: 0.597732424736023, acc: 0.8205128312110901)
[2025-02-16 11:17:58,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:58,377][root][INFO] - Training Epoch: 1/2, step 2735/23838 completed (loss: 1.2213636636734009, acc: 0.671875)
[2025-02-16 11:17:58,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:58,851][root][INFO] - Training Epoch: 1/2, step 2736/23838 completed (loss: 0.8273903131484985, acc: 0.7450980544090271)
[2025-02-16 11:17:59,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:59,334][root][INFO] - Training Epoch: 1/2, step 2737/23838 completed (loss: 1.2890040874481201, acc: 0.6595744490623474)
[2025-02-16 11:17:59,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:17:59,816][root][INFO] - Training Epoch: 1/2, step 2738/23838 completed (loss: 0.9173545241355896, acc: 0.7857142686843872)
[2025-02-16 11:18:00,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:00,469][root][INFO] - Training Epoch: 1/2, step 2739/23838 completed (loss: 1.4807019233703613, acc: 0.5344827771186829)
[2025-02-16 11:18:00,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:00,853][root][INFO] - Training Epoch: 1/2, step 2740/23838 completed (loss: 1.2971973419189453, acc: 0.6521739363670349)
[2025-02-16 11:18:01,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:01,366][root][INFO] - Training Epoch: 1/2, step 2741/23838 completed (loss: 1.1537774801254272, acc: 0.6515151262283325)
[2025-02-16 11:18:01,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:01,880][root][INFO] - Training Epoch: 1/2, step 2742/23838 completed (loss: 1.196897029876709, acc: 0.6721311211585999)
[2025-02-16 11:18:02,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:02,266][root][INFO] - Training Epoch: 1/2, step 2743/23838 completed (loss: 1.5984666347503662, acc: 0.5909090638160706)
[2025-02-16 11:18:02,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:02,731][root][INFO] - Training Epoch: 1/2, step 2744/23838 completed (loss: 1.7698496580123901, acc: 0.5731707215309143)
[2025-02-16 11:18:02,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:03,178][root][INFO] - Training Epoch: 1/2, step 2745/23838 completed (loss: 1.580389380455017, acc: 0.5432098507881165)
[2025-02-16 11:18:03,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:03,644][root][INFO] - Training Epoch: 1/2, step 2746/23838 completed (loss: 1.0689613819122314, acc: 0.7460317611694336)
[2025-02-16 11:18:04,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:04,429][root][INFO] - Training Epoch: 1/2, step 2747/23838 completed (loss: 1.3575254678726196, acc: 0.6200000047683716)
[2025-02-16 11:18:04,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:04,891][root][INFO] - Training Epoch: 1/2, step 2748/23838 completed (loss: 1.5800869464874268, acc: 0.5405405163764954)
[2025-02-16 11:18:05,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:05,338][root][INFO] - Training Epoch: 1/2, step 2749/23838 completed (loss: 0.7362221479415894, acc: 0.8452380895614624)
[2025-02-16 11:18:05,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:05,776][root][INFO] - Training Epoch: 1/2, step 2750/23838 completed (loss: 1.2808514833450317, acc: 0.6842105388641357)
[2025-02-16 11:18:05,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:06,237][root][INFO] - Training Epoch: 1/2, step 2751/23838 completed (loss: 1.2635927200317383, acc: 0.6395348906517029)
[2025-02-16 11:18:06,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:06,648][root][INFO] - Training Epoch: 1/2, step 2752/23838 completed (loss: 0.9412870407104492, acc: 0.7291666865348816)
[2025-02-16 11:18:06,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:07,123][root][INFO] - Training Epoch: 1/2, step 2753/23838 completed (loss: 1.6769462823867798, acc: 0.5657894611358643)
[2025-02-16 11:18:07,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:07,569][root][INFO] - Training Epoch: 1/2, step 2754/23838 completed (loss: 0.7907757759094238, acc: 0.7710843086242676)
[2025-02-16 11:18:07,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:08,085][root][INFO] - Training Epoch: 1/2, step 2755/23838 completed (loss: 0.8249537944793701, acc: 0.7849462628364563)
[2025-02-16 11:18:08,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:08,450][root][INFO] - Training Epoch: 1/2, step 2756/23838 completed (loss: 1.2359328269958496, acc: 0.6326530575752258)
[2025-02-16 11:18:08,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:08,883][root][INFO] - Training Epoch: 1/2, step 2757/23838 completed (loss: 1.283972978591919, acc: 0.6415094137191772)
[2025-02-16 11:18:09,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:09,358][root][INFO] - Training Epoch: 1/2, step 2758/23838 completed (loss: 1.0614663362503052, acc: 0.7195122241973877)
[2025-02-16 11:18:09,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:09,776][root][INFO] - Training Epoch: 1/2, step 2759/23838 completed (loss: 0.7938992977142334, acc: 0.7564102411270142)
[2025-02-16 11:18:09,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:10,177][root][INFO] - Training Epoch: 1/2, step 2760/23838 completed (loss: 1.330886960029602, acc: 0.6666666865348816)
[2025-02-16 11:18:10,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:10,534][root][INFO] - Training Epoch: 1/2, step 2761/23838 completed (loss: 1.2513972520828247, acc: 0.6172839403152466)
[2025-02-16 11:18:10,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:11,107][root][INFO] - Training Epoch: 1/2, step 2762/23838 completed (loss: 0.9653655290603638, acc: 0.76106196641922)
[2025-02-16 11:18:11,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:11,625][root][INFO] - Training Epoch: 1/2, step 2763/23838 completed (loss: 1.3245975971221924, acc: 0.6491228342056274)
[2025-02-16 11:18:11,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:12,029][root][INFO] - Training Epoch: 1/2, step 2764/23838 completed (loss: 1.0944968461990356, acc: 0.6666666865348816)
[2025-02-16 11:18:12,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:12,415][root][INFO] - Training Epoch: 1/2, step 2765/23838 completed (loss: 1.071107029914856, acc: 0.7096773982048035)
[2025-02-16 11:18:12,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:12,843][root][INFO] - Training Epoch: 1/2, step 2766/23838 completed (loss: 1.0026785135269165, acc: 0.7037037014961243)
[2025-02-16 11:18:13,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:13,353][root][INFO] - Training Epoch: 1/2, step 2767/23838 completed (loss: 1.4404963254928589, acc: 0.6333333253860474)
[2025-02-16 11:18:13,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:13,836][root][INFO] - Training Epoch: 1/2, step 2768/23838 completed (loss: 0.7717717885971069, acc: 0.7808219194412231)
[2025-02-16 11:18:14,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:14,248][root][INFO] - Training Epoch: 1/2, step 2769/23838 completed (loss: 1.058511734008789, acc: 0.6744186282157898)
[2025-02-16 11:18:14,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:14,790][root][INFO] - Training Epoch: 1/2, step 2770/23838 completed (loss: 0.8884007930755615, acc: 0.8117647171020508)
[2025-02-16 11:18:14,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:15,187][root][INFO] - Training Epoch: 1/2, step 2771/23838 completed (loss: 1.2527215480804443, acc: 0.6935483813285828)
[2025-02-16 11:18:15,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:15,615][root][INFO] - Training Epoch: 1/2, step 2772/23838 completed (loss: 0.7971159219741821, acc: 0.7419354915618896)
[2025-02-16 11:18:15,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:16,115][root][INFO] - Training Epoch: 1/2, step 2773/23838 completed (loss: 0.9906983971595764, acc: 0.7333333492279053)
[2025-02-16 11:18:16,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:16,619][root][INFO] - Training Epoch: 1/2, step 2774/23838 completed (loss: 1.0497303009033203, acc: 0.6730769276618958)
[2025-02-16 11:18:16,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:17,288][root][INFO] - Training Epoch: 1/2, step 2775/23838 completed (loss: 1.3119423389434814, acc: 0.5820895433425903)
[2025-02-16 11:18:17,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:17,793][root][INFO] - Training Epoch: 1/2, step 2776/23838 completed (loss: 0.8119986653327942, acc: 0.8131868243217468)
[2025-02-16 11:18:18,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:18,287][root][INFO] - Training Epoch: 1/2, step 2777/23838 completed (loss: 1.4982178211212158, acc: 0.5522388219833374)
[2025-02-16 11:18:18,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:19,054][root][INFO] - Training Epoch: 1/2, step 2778/23838 completed (loss: 1.7137680053710938, acc: 0.5760869383811951)
[2025-02-16 11:18:19,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:19,668][root][INFO] - Training Epoch: 1/2, step 2779/23838 completed (loss: 0.8344050645828247, acc: 0.762499988079071)
[2025-02-16 11:18:19,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:20,148][root][INFO] - Training Epoch: 1/2, step 2780/23838 completed (loss: 1.20189368724823, acc: 0.7045454382896423)
[2025-02-16 11:18:20,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:20,594][root][INFO] - Training Epoch: 1/2, step 2781/23838 completed (loss: 0.8812040686607361, acc: 0.7450980544090271)
[2025-02-16 11:18:20,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:21,033][root][INFO] - Training Epoch: 1/2, step 2782/23838 completed (loss: 1.1881375312805176, acc: 0.6896551847457886)
[2025-02-16 11:18:21,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:21,547][root][INFO] - Training Epoch: 1/2, step 2783/23838 completed (loss: 1.249752402305603, acc: 0.6666666865348816)
[2025-02-16 11:18:21,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:21,977][root][INFO] - Training Epoch: 1/2, step 2784/23838 completed (loss: 1.486932635307312, acc: 0.6307692527770996)
[2025-02-16 11:18:22,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:22,456][root][INFO] - Training Epoch: 1/2, step 2785/23838 completed (loss: 1.3713529109954834, acc: 0.6724137663841248)
[2025-02-16 11:18:22,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:23,175][root][INFO] - Training Epoch: 1/2, step 2786/23838 completed (loss: 1.779223918914795, acc: 0.5135135054588318)
[2025-02-16 11:18:23,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:23,644][root][INFO] - Training Epoch: 1/2, step 2787/23838 completed (loss: 0.9735442996025085, acc: 0.7666666507720947)
[2025-02-16 11:18:23,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:24,133][root][INFO] - Training Epoch: 1/2, step 2788/23838 completed (loss: 1.106606364250183, acc: 0.7142857313156128)
[2025-02-16 11:18:24,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:24,607][root][INFO] - Training Epoch: 1/2, step 2789/23838 completed (loss: 1.1770873069763184, acc: 0.6823529601097107)
[2025-02-16 11:18:24,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:25,055][root][INFO] - Training Epoch: 1/2, step 2790/23838 completed (loss: 1.1733280420303345, acc: 0.6585366129875183)
[2025-02-16 11:18:25,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:25,747][root][INFO] - Training Epoch: 1/2, step 2791/23838 completed (loss: 1.4227577447891235, acc: 0.6219512224197388)
[2025-02-16 11:18:25,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:26,191][root][INFO] - Training Epoch: 1/2, step 2792/23838 completed (loss: 1.3184744119644165, acc: 0.6170212626457214)
[2025-02-16 11:18:26,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:26,662][root][INFO] - Training Epoch: 1/2, step 2793/23838 completed (loss: 1.2302920818328857, acc: 0.6774193644523621)
[2025-02-16 11:18:26,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:27,119][root][INFO] - Training Epoch: 1/2, step 2794/23838 completed (loss: 1.1396695375442505, acc: 0.7200000286102295)
[2025-02-16 11:18:27,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:27,929][root][INFO] - Training Epoch: 1/2, step 2795/23838 completed (loss: 0.9901363253593445, acc: 0.7162162065505981)
[2025-02-16 11:18:28,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:28,360][root][INFO] - Training Epoch: 1/2, step 2796/23838 completed (loss: 1.4853699207305908, acc: 0.6557376980781555)
[2025-02-16 11:18:28,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:28,879][root][INFO] - Training Epoch: 1/2, step 2797/23838 completed (loss: 0.8623210787773132, acc: 0.7463768124580383)
[2025-02-16 11:18:29,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:29,392][root][INFO] - Training Epoch: 1/2, step 2798/23838 completed (loss: 0.38337111473083496, acc: 0.8707864880561829)
[2025-02-16 11:18:29,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:29,779][root][INFO] - Training Epoch: 1/2, step 2799/23838 completed (loss: 0.5628679394721985, acc: 0.8399999737739563)
[2025-02-16 11:18:29,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:30,217][root][INFO] - Training Epoch: 1/2, step 2800/23838 completed (loss: 0.41523826122283936, acc: 0.8936170339584351)
[2025-02-16 11:18:30,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:30,661][root][INFO] - Training Epoch: 1/2, step 2801/23838 completed (loss: 0.5289214849472046, acc: 0.875)
[2025-02-16 11:18:30,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:31,070][root][INFO] - Training Epoch: 1/2, step 2802/23838 completed (loss: 0.726172149181366, acc: 0.8018018007278442)
[2025-02-16 11:18:31,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:31,506][root][INFO] - Training Epoch: 1/2, step 2803/23838 completed (loss: 0.35683974623680115, acc: 0.8947368264198303)
[2025-02-16 11:18:31,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:31,920][root][INFO] - Training Epoch: 1/2, step 2804/23838 completed (loss: 0.4566952586174011, acc: 0.875)
[2025-02-16 11:18:32,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:32,331][root][INFO] - Training Epoch: 1/2, step 2805/23838 completed (loss: 0.6943485736846924, acc: 0.8152173757553101)
[2025-02-16 11:18:32,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:32,708][root][INFO] - Training Epoch: 1/2, step 2806/23838 completed (loss: 0.7833057641983032, acc: 0.8271604776382446)
[2025-02-16 11:18:32,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:33,112][root][INFO] - Training Epoch: 1/2, step 2807/23838 completed (loss: 0.33816349506378174, acc: 0.9176470637321472)
[2025-02-16 11:18:33,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:33,565][root][INFO] - Training Epoch: 1/2, step 2808/23838 completed (loss: 0.4272043704986572, acc: 0.8571428656578064)
[2025-02-16 11:18:33,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:33,991][root][INFO] - Training Epoch: 1/2, step 2809/23838 completed (loss: 0.22893476486206055, acc: 0.9082568883895874)
[2025-02-16 11:18:34,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:34,422][root][INFO] - Training Epoch: 1/2, step 2810/23838 completed (loss: 0.19132305681705475, acc: 0.9279999732971191)
[2025-02-16 11:18:34,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:34,820][root][INFO] - Training Epoch: 1/2, step 2811/23838 completed (loss: 0.6712396144866943, acc: 0.8548387289047241)
[2025-02-16 11:18:35,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:35,323][root][INFO] - Training Epoch: 1/2, step 2812/23838 completed (loss: 0.5557246804237366, acc: 0.8475610017776489)
[2025-02-16 11:18:35,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:35,774][root][INFO] - Training Epoch: 1/2, step 2813/23838 completed (loss: 0.5074373483657837, acc: 0.8799999952316284)
[2025-02-16 11:18:35,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:36,182][root][INFO] - Training Epoch: 1/2, step 2814/23838 completed (loss: 0.8335464596748352, acc: 0.8118811845779419)
[2025-02-16 11:18:36,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:36,758][root][INFO] - Training Epoch: 1/2, step 2815/23838 completed (loss: 0.5469131469726562, acc: 0.847328245639801)
[2025-02-16 11:18:37,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:37,269][root][INFO] - Training Epoch: 1/2, step 2816/23838 completed (loss: 0.4802624583244324, acc: 0.8853503465652466)
[2025-02-16 11:18:37,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:37,764][root][INFO] - Training Epoch: 1/2, step 2817/23838 completed (loss: 0.4022710919380188, acc: 0.8857142925262451)
[2025-02-16 11:18:38,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:38,338][root][INFO] - Training Epoch: 1/2, step 2818/23838 completed (loss: 0.46462178230285645, acc: 0.8159999847412109)
[2025-02-16 11:18:38,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:38,844][root][INFO] - Training Epoch: 1/2, step 2819/23838 completed (loss: 0.7223803400993347, acc: 0.8248175382614136)
[2025-02-16 11:18:39,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:39,558][root][INFO] - Training Epoch: 1/2, step 2820/23838 completed (loss: 0.3943922221660614, acc: 0.8769230842590332)
[2025-02-16 11:18:39,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:40,013][root][INFO] - Training Epoch: 1/2, step 2821/23838 completed (loss: 0.45930948853492737, acc: 0.874316930770874)
[2025-02-16 11:18:40,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:40,489][root][INFO] - Training Epoch: 1/2, step 2822/23838 completed (loss: 0.5255141258239746, acc: 0.8618420958518982)
[2025-02-16 11:18:40,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:40,929][root][INFO] - Training Epoch: 1/2, step 2823/23838 completed (loss: 0.45433616638183594, acc: 0.8602941036224365)
[2025-02-16 11:18:41,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:41,321][root][INFO] - Training Epoch: 1/2, step 2824/23838 completed (loss: 0.5046700239181519, acc: 0.8623188138008118)
[2025-02-16 11:18:41,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:41,763][root][INFO] - Training Epoch: 1/2, step 2825/23838 completed (loss: 0.6854846477508545, acc: 0.7876105904579163)
[2025-02-16 11:18:41,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:42,218][root][INFO] - Training Epoch: 1/2, step 2826/23838 completed (loss: 0.6266070008277893, acc: 0.8125)
[2025-02-16 11:18:42,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:42,621][root][INFO] - Training Epoch: 1/2, step 2827/23838 completed (loss: 0.345010370016098, acc: 0.8914728760719299)
[2025-02-16 11:18:42,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:43,040][root][INFO] - Training Epoch: 1/2, step 2828/23838 completed (loss: 0.8709011673927307, acc: 0.7765957713127136)
[2025-02-16 11:18:43,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:43,555][root][INFO] - Training Epoch: 1/2, step 2829/23838 completed (loss: 0.7001615762710571, acc: 0.8414633870124817)
[2025-02-16 11:18:43,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:43,978][root][INFO] - Training Epoch: 1/2, step 2830/23838 completed (loss: 0.3691224455833435, acc: 0.9152542352676392)
[2025-02-16 11:18:44,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:44,400][root][INFO] - Training Epoch: 1/2, step 2831/23838 completed (loss: 0.5062075257301331, acc: 0.8787878751754761)
[2025-02-16 11:18:44,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:44,823][root][INFO] - Training Epoch: 1/2, step 2832/23838 completed (loss: 0.667576253414154, acc: 0.779411792755127)
[2025-02-16 11:18:45,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:45,246][root][INFO] - Training Epoch: 1/2, step 2833/23838 completed (loss: 0.4405834972858429, acc: 0.8681318759918213)
[2025-02-16 11:18:45,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:45,688][root][INFO] - Training Epoch: 1/2, step 2834/23838 completed (loss: 0.27958136796951294, acc: 0.9538461565971375)
[2025-02-16 11:18:45,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:46,111][root][INFO] - Training Epoch: 1/2, step 2835/23838 completed (loss: 0.46903207898139954, acc: 0.8604651093482971)
[2025-02-16 11:18:46,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:46,537][root][INFO] - Training Epoch: 1/2, step 2836/23838 completed (loss: 0.554400622844696, acc: 0.8535031676292419)
[2025-02-16 11:18:46,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:46,977][root][INFO] - Training Epoch: 1/2, step 2837/23838 completed (loss: 0.7043849229812622, acc: 0.8571428656578064)
[2025-02-16 11:18:47,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:47,445][root][INFO] - Training Epoch: 1/2, step 2838/23838 completed (loss: 0.44717976450920105, acc: 0.8999999761581421)
[2025-02-16 11:18:47,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:47,918][root][INFO] - Training Epoch: 1/2, step 2839/23838 completed (loss: 0.4223477840423584, acc: 0.8888888955116272)
[2025-02-16 11:18:48,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:48,401][root][INFO] - Training Epoch: 1/2, step 2840/23838 completed (loss: 0.7376747727394104, acc: 0.7627118825912476)
[2025-02-16 11:18:48,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:48,774][root][INFO] - Training Epoch: 1/2, step 2841/23838 completed (loss: 0.6265157461166382, acc: 0.7796609997749329)
[2025-02-16 11:18:48,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:49,209][root][INFO] - Training Epoch: 1/2, step 2842/23838 completed (loss: 0.49773746728897095, acc: 0.9156626462936401)
[2025-02-16 11:18:49,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:49,618][root][INFO] - Training Epoch: 1/2, step 2843/23838 completed (loss: 0.6526858806610107, acc: 0.8173912763595581)
[2025-02-16 11:18:49,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:50,034][root][INFO] - Training Epoch: 1/2, step 2844/23838 completed (loss: 0.31192946434020996, acc: 0.8947368264198303)
[2025-02-16 11:18:50,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:50,624][root][INFO] - Training Epoch: 1/2, step 2845/23838 completed (loss: 0.5094240307807922, acc: 0.893081784248352)
[2025-02-16 11:18:50,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:51,032][root][INFO] - Training Epoch: 1/2, step 2846/23838 completed (loss: 0.3902145028114319, acc: 0.8666666746139526)
[2025-02-16 11:18:51,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:51,504][root][INFO] - Training Epoch: 1/2, step 2847/23838 completed (loss: 0.6157788038253784, acc: 0.8135592937469482)
[2025-02-16 11:18:51,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:51,925][root][INFO] - Training Epoch: 1/2, step 2848/23838 completed (loss: 0.4341164529323578, acc: 0.8684210777282715)
[2025-02-16 11:18:52,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:52,373][root][INFO] - Training Epoch: 1/2, step 2849/23838 completed (loss: 0.33949506282806396, acc: 0.9239130616188049)
[2025-02-16 11:18:52,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:52,806][root][INFO] - Training Epoch: 1/2, step 2850/23838 completed (loss: 0.4858207702636719, acc: 0.8898305296897888)
[2025-02-16 11:18:52,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:53,183][root][INFO] - Training Epoch: 1/2, step 2851/23838 completed (loss: 0.23293843865394592, acc: 0.9333333373069763)
[2025-02-16 11:18:53,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:53,594][root][INFO] - Training Epoch: 1/2, step 2852/23838 completed (loss: 0.3710685074329376, acc: 0.890625)
[2025-02-16 11:18:53,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:54,003][root][INFO] - Training Epoch: 1/2, step 2853/23838 completed (loss: 0.5236309766769409, acc: 0.8225806355476379)
[2025-02-16 11:18:54,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:54,407][root][INFO] - Training Epoch: 1/2, step 2854/23838 completed (loss: 0.8042150139808655, acc: 0.7831325531005859)
[2025-02-16 11:18:54,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:54,831][root][INFO] - Training Epoch: 1/2, step 2855/23838 completed (loss: 0.433389276266098, acc: 0.8952381014823914)
[2025-02-16 11:18:55,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:55,275][root][INFO] - Training Epoch: 1/2, step 2856/23838 completed (loss: 0.388679176568985, acc: 0.8833333253860474)
[2025-02-16 11:18:55,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:55,722][root][INFO] - Training Epoch: 1/2, step 2857/23838 completed (loss: 0.305756151676178, acc: 0.8936170339584351)
[2025-02-16 11:18:55,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:56,198][root][INFO] - Training Epoch: 1/2, step 2858/23838 completed (loss: 0.3741984963417053, acc: 0.8941176533699036)
[2025-02-16 11:18:56,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:56,652][root][INFO] - Training Epoch: 1/2, step 2859/23838 completed (loss: 0.1514204889535904, acc: 0.9647058844566345)
[2025-02-16 11:18:56,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:57,088][root][INFO] - Training Epoch: 1/2, step 2860/23838 completed (loss: 0.3477824330329895, acc: 0.9245283007621765)
[2025-02-16 11:18:57,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:57,545][root][INFO] - Training Epoch: 1/2, step 2861/23838 completed (loss: 0.234907329082489, acc: 0.9552238583564758)
[2025-02-16 11:18:57,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:58,004][root][INFO] - Training Epoch: 1/2, step 2862/23838 completed (loss: 0.43560153245925903, acc: 0.8852459192276001)
[2025-02-16 11:18:58,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:58,508][root][INFO] - Training Epoch: 1/2, step 2863/23838 completed (loss: 0.2799835801124573, acc: 0.9280575513839722)
[2025-02-16 11:18:58,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:58,968][root][INFO] - Training Epoch: 1/2, step 2864/23838 completed (loss: 0.4181666672229767, acc: 0.9052631855010986)
[2025-02-16 11:18:59,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:59,528][root][INFO] - Training Epoch: 1/2, step 2865/23838 completed (loss: 0.2565200626850128, acc: 0.954023003578186)
[2025-02-16 11:18:59,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:18:59,963][root][INFO] - Training Epoch: 1/2, step 2866/23838 completed (loss: 0.4515625536441803, acc: 0.8896104097366333)
[2025-02-16 11:19:00,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:00,396][root][INFO] - Training Epoch: 1/2, step 2867/23838 completed (loss: 0.3342091739177704, acc: 0.9247311949729919)
[2025-02-16 11:19:00,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:00,892][root][INFO] - Training Epoch: 1/2, step 2868/23838 completed (loss: 0.2043517380952835, acc: 0.9379844665527344)
[2025-02-16 11:19:01,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:01,519][root][INFO] - Training Epoch: 1/2, step 2869/23838 completed (loss: 0.3744156062602997, acc: 0.9009009003639221)
[2025-02-16 11:19:01,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:01,956][root][INFO] - Training Epoch: 1/2, step 2870/23838 completed (loss: 0.3699166178703308, acc: 0.9298245906829834)
[2025-02-16 11:19:02,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:02,402][root][INFO] - Training Epoch: 1/2, step 2871/23838 completed (loss: 0.22070655226707458, acc: 0.9217391014099121)
[2025-02-16 11:19:02,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:02,918][root][INFO] - Training Epoch: 1/2, step 2872/23838 completed (loss: 0.217994824051857, acc: 0.9353448152542114)
[2025-02-16 11:19:03,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:03,313][root][INFO] - Training Epoch: 1/2, step 2873/23838 completed (loss: 0.5808881521224976, acc: 0.8543689250946045)
[2025-02-16 11:19:03,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:03,709][root][INFO] - Training Epoch: 1/2, step 2874/23838 completed (loss: 0.3811039328575134, acc: 0.8984375)
[2025-02-16 11:19:03,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:04,165][root][INFO] - Training Epoch: 1/2, step 2875/23838 completed (loss: 0.21576693654060364, acc: 0.9602649211883545)
[2025-02-16 11:19:04,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:04,625][root][INFO] - Training Epoch: 1/2, step 2876/23838 completed (loss: 0.3676736354827881, acc: 0.8770492076873779)
[2025-02-16 11:19:04,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:05,092][root][INFO] - Training Epoch: 1/2, step 2877/23838 completed (loss: 0.31711453199386597, acc: 0.9264705777168274)
[2025-02-16 11:19:05,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:05,506][root][INFO] - Training Epoch: 1/2, step 2878/23838 completed (loss: 0.30781179666519165, acc: 0.9179104566574097)
[2025-02-16 11:19:05,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:05,955][root][INFO] - Training Epoch: 1/2, step 2879/23838 completed (loss: 0.3081485629081726, acc: 0.9375)
[2025-02-16 11:19:06,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:06,432][root][INFO] - Training Epoch: 1/2, step 2880/23838 completed (loss: 0.3790469765663147, acc: 0.9051724076271057)
[2025-02-16 11:19:06,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:06,913][root][INFO] - Training Epoch: 1/2, step 2881/23838 completed (loss: 0.4363577961921692, acc: 0.9013158082962036)
[2025-02-16 11:19:07,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:07,394][root][INFO] - Training Epoch: 1/2, step 2882/23838 completed (loss: 0.22636578977108002, acc: 0.9328358173370361)
[2025-02-16 11:19:07,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:07,804][root][INFO] - Training Epoch: 1/2, step 2883/23838 completed (loss: 0.9955780506134033, acc: 0.7710843086242676)
[2025-02-16 11:19:08,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:08,358][root][INFO] - Training Epoch: 1/2, step 2884/23838 completed (loss: 0.26556962728500366, acc: 0.90625)
[2025-02-16 11:19:08,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:08,759][root][INFO] - Training Epoch: 1/2, step 2885/23838 completed (loss: 0.2362101525068283, acc: 0.9180327653884888)
[2025-02-16 11:19:08,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:09,224][root][INFO] - Training Epoch: 1/2, step 2886/23838 completed (loss: 0.500438392162323, acc: 0.8670886158943176)
[2025-02-16 11:19:09,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:09,815][root][INFO] - Training Epoch: 1/2, step 2887/23838 completed (loss: 0.28233450651168823, acc: 0.9111111164093018)
[2025-02-16 11:19:10,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:10,282][root][INFO] - Training Epoch: 1/2, step 2888/23838 completed (loss: 0.17367644608020782, acc: 0.967391312122345)
[2025-02-16 11:19:10,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:10,692][root][INFO] - Training Epoch: 1/2, step 2889/23838 completed (loss: 0.2904660403728485, acc: 0.9217391014099121)
[2025-02-16 11:19:10,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:11,046][root][INFO] - Training Epoch: 1/2, step 2890/23838 completed (loss: 0.585430383682251, acc: 0.7875000238418579)
[2025-02-16 11:19:11,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:11,542][root][INFO] - Training Epoch: 1/2, step 2891/23838 completed (loss: 0.5670009851455688, acc: 0.859375)
[2025-02-16 11:19:11,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:12,054][root][INFO] - Training Epoch: 1/2, step 2892/23838 completed (loss: 0.5074557065963745, acc: 0.8633093237876892)
[2025-02-16 11:19:12,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:12,510][root][INFO] - Training Epoch: 1/2, step 2893/23838 completed (loss: 0.879944384098053, acc: 0.7686567306518555)
[2025-02-16 11:19:12,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:13,036][root][INFO] - Training Epoch: 1/2, step 2894/23838 completed (loss: 0.6555685997009277, acc: 0.8369565010070801)
[2025-02-16 11:19:13,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:13,453][root][INFO] - Training Epoch: 1/2, step 2895/23838 completed (loss: 0.8613935112953186, acc: 0.7362637519836426)
[2025-02-16 11:19:13,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:14,000][root][INFO] - Training Epoch: 1/2, step 2896/23838 completed (loss: 0.6789422035217285, acc: 0.767123281955719)
[2025-02-16 11:19:14,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:14,593][root][INFO] - Training Epoch: 1/2, step 2897/23838 completed (loss: 0.5082634687423706, acc: 0.8628571629524231)
[2025-02-16 11:19:14,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:15,017][root][INFO] - Training Epoch: 1/2, step 2898/23838 completed (loss: 0.6523300409317017, acc: 0.8089887499809265)
[2025-02-16 11:19:15,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:15,433][root][INFO] - Training Epoch: 1/2, step 2899/23838 completed (loss: 0.860592782497406, acc: 0.7788461446762085)
[2025-02-16 11:19:15,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:15,896][root][INFO] - Training Epoch: 1/2, step 2900/23838 completed (loss: 0.6555949449539185, acc: 0.8623188138008118)
[2025-02-16 11:19:16,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:16,284][root][INFO] - Training Epoch: 1/2, step 2901/23838 completed (loss: 0.36152422428131104, acc: 0.8867924809455872)
[2025-02-16 11:19:16,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:16,767][root][INFO] - Training Epoch: 1/2, step 2902/23838 completed (loss: 0.464694082736969, acc: 0.8571428656578064)
[2025-02-16 11:19:16,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:17,188][root][INFO] - Training Epoch: 1/2, step 2903/23838 completed (loss: 0.6469990015029907, acc: 0.8089887499809265)
[2025-02-16 11:19:17,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:17,659][root][INFO] - Training Epoch: 1/2, step 2904/23838 completed (loss: 0.37614706158638, acc: 0.8684210777282715)
[2025-02-16 11:19:17,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:18,097][root][INFO] - Training Epoch: 1/2, step 2905/23838 completed (loss: 0.5522937774658203, acc: 0.8928571343421936)
[2025-02-16 11:19:18,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:18,522][root][INFO] - Training Epoch: 1/2, step 2906/23838 completed (loss: 0.6529296040534973, acc: 0.8409090638160706)
[2025-02-16 11:19:18,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:18,935][root][INFO] - Training Epoch: 1/2, step 2907/23838 completed (loss: 0.4764425456523895, acc: 0.9090909361839294)
[2025-02-16 11:19:19,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:19,379][root][INFO] - Training Epoch: 1/2, step 2908/23838 completed (loss: 0.9236972332000732, acc: 0.7361111044883728)
[2025-02-16 11:19:19,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:19,752][root][INFO] - Training Epoch: 1/2, step 2909/23838 completed (loss: 1.1126303672790527, acc: 0.6800000071525574)
[2025-02-16 11:19:19,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:20,166][root][INFO] - Training Epoch: 1/2, step 2910/23838 completed (loss: 0.5908734202384949, acc: 0.8030303120613098)
[2025-02-16 11:19:20,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:20,604][root][INFO] - Training Epoch: 1/2, step 2911/23838 completed (loss: 0.5878183245658875, acc: 0.8333333134651184)
[2025-02-16 11:19:20,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:21,137][root][INFO] - Training Epoch: 1/2, step 2912/23838 completed (loss: 0.5854697823524475, acc: 0.8404255509376526)
[2025-02-16 11:19:21,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:21,588][root][INFO] - Training Epoch: 1/2, step 2913/23838 completed (loss: 0.6409273147583008, acc: 0.8348624110221863)
[2025-02-16 11:19:21,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:21,977][root][INFO] - Training Epoch: 1/2, step 2914/23838 completed (loss: 0.3667047619819641, acc: 0.8907563090324402)
[2025-02-16 11:19:22,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:22,369][root][INFO] - Training Epoch: 1/2, step 2915/23838 completed (loss: 0.546370804309845, acc: 0.8227847814559937)
[2025-02-16 11:19:22,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:22,796][root][INFO] - Training Epoch: 1/2, step 2916/23838 completed (loss: 0.7426555752754211, acc: 0.7790697813034058)
[2025-02-16 11:19:22,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:23,238][root][INFO] - Training Epoch: 1/2, step 2917/23838 completed (loss: 0.36187732219696045, acc: 0.9270833134651184)
[2025-02-16 11:19:23,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:23,714][root][INFO] - Training Epoch: 1/2, step 2918/23838 completed (loss: 0.4964742362499237, acc: 0.8658536672592163)
[2025-02-16 11:19:23,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:24,159][root][INFO] - Training Epoch: 1/2, step 2919/23838 completed (loss: 0.4656830132007599, acc: 0.859375)
[2025-02-16 11:19:24,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:24,521][root][INFO] - Training Epoch: 1/2, step 2920/23838 completed (loss: 1.1053982973098755, acc: 0.6585366129875183)
[2025-02-16 11:19:24,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:25,058][root][INFO] - Training Epoch: 1/2, step 2921/23838 completed (loss: 0.5329984426498413, acc: 0.8717948794364929)
[2025-02-16 11:19:25,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:25,693][root][INFO] - Training Epoch: 1/2, step 2922/23838 completed (loss: 0.2175322026014328, acc: 0.939393937587738)
[2025-02-16 11:19:25,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:26,116][root][INFO] - Training Epoch: 1/2, step 2923/23838 completed (loss: 0.46877482533454895, acc: 0.8586956262588501)
[2025-02-16 11:19:26,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:26,542][root][INFO] - Training Epoch: 1/2, step 2924/23838 completed (loss: 0.4722864031791687, acc: 0.8391608595848083)
[2025-02-16 11:19:26,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:27,027][root][INFO] - Training Epoch: 1/2, step 2925/23838 completed (loss: 0.4598514437675476, acc: 0.8428571224212646)
[2025-02-16 11:19:27,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:27,399][root][INFO] - Training Epoch: 1/2, step 2926/23838 completed (loss: 0.7814812660217285, acc: 0.7931034564971924)
[2025-02-16 11:19:27,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:27,993][root][INFO] - Training Epoch: 1/2, step 2927/23838 completed (loss: 0.2975895404815674, acc: 0.9426229596138)
[2025-02-16 11:19:28,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:28,436][root][INFO] - Training Epoch: 1/2, step 2928/23838 completed (loss: 0.2373119443655014, acc: 0.9256198406219482)
[2025-02-16 11:19:28,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:28,873][root][INFO] - Training Epoch: 1/2, step 2929/23838 completed (loss: 0.8185206055641174, acc: 0.7682926654815674)
[2025-02-16 11:19:29,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:29,341][root][INFO] - Training Epoch: 1/2, step 2930/23838 completed (loss: 0.632442831993103, acc: 0.800000011920929)
[2025-02-16 11:19:29,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:29,742][root][INFO] - Training Epoch: 1/2, step 2931/23838 completed (loss: 0.42463696002960205, acc: 0.8615384697914124)
[2025-02-16 11:19:29,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:30,197][root][INFO] - Training Epoch: 1/2, step 2932/23838 completed (loss: 0.8843662142753601, acc: 0.7428571581840515)
[2025-02-16 11:19:30,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:30,654][root][INFO] - Training Epoch: 1/2, step 2933/23838 completed (loss: 1.0503090620040894, acc: 0.7692307829856873)
[2025-02-16 11:19:30,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:31,113][root][INFO] - Training Epoch: 1/2, step 2934/23838 completed (loss: 0.5160502195358276, acc: 0.8791208863258362)
[2025-02-16 11:19:31,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:31,606][root][INFO] - Training Epoch: 1/2, step 2935/23838 completed (loss: 0.3782723546028137, acc: 0.8846153616905212)
[2025-02-16 11:19:31,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:32,148][root][INFO] - Training Epoch: 1/2, step 2936/23838 completed (loss: 0.5511747598648071, acc: 0.8584905862808228)
[2025-02-16 11:19:32,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:32,711][root][INFO] - Training Epoch: 1/2, step 2937/23838 completed (loss: 0.4741966426372528, acc: 0.8581560254096985)
[2025-02-16 11:19:32,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:33,171][root][INFO] - Training Epoch: 1/2, step 2938/23838 completed (loss: 0.39744535088539124, acc: 0.8867924809455872)
[2025-02-16 11:19:33,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:33,636][root][INFO] - Training Epoch: 1/2, step 2939/23838 completed (loss: 0.34538260102272034, acc: 0.9185185432434082)
[2025-02-16 11:19:33,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:34,027][root][INFO] - Training Epoch: 1/2, step 2940/23838 completed (loss: 0.7071430683135986, acc: 0.792682945728302)
[2025-02-16 11:19:34,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:34,442][root][INFO] - Training Epoch: 1/2, step 2941/23838 completed (loss: 0.41946926712989807, acc: 0.8870967626571655)
[2025-02-16 11:19:34,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:34,878][root][INFO] - Training Epoch: 1/2, step 2942/23838 completed (loss: 0.3573969006538391, acc: 0.8653846383094788)
[2025-02-16 11:19:35,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:35,297][root][INFO] - Training Epoch: 1/2, step 2943/23838 completed (loss: 0.44950735569000244, acc: 0.8728813529014587)
[2025-02-16 11:19:35,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:35,743][root][INFO] - Training Epoch: 1/2, step 2944/23838 completed (loss: 0.3597553074359894, acc: 0.8589743375778198)
[2025-02-16 11:19:35,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:36,122][root][INFO] - Training Epoch: 1/2, step 2945/23838 completed (loss: 1.9744468927383423, acc: 0.5121951103210449)
[2025-02-16 11:19:36,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:36,542][root][INFO] - Training Epoch: 1/2, step 2946/23838 completed (loss: 1.360491394996643, acc: 0.6363636255264282)
[2025-02-16 11:19:36,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:36,954][root][INFO] - Training Epoch: 1/2, step 2947/23838 completed (loss: 0.35697489976882935, acc: 0.90625)
[2025-02-16 11:19:37,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:37,311][root][INFO] - Training Epoch: 1/2, step 2948/23838 completed (loss: 0.4598592519760132, acc: 0.8888888955116272)
[2025-02-16 11:19:37,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:37,677][root][INFO] - Training Epoch: 1/2, step 2949/23838 completed (loss: 0.7522806525230408, acc: 0.8160919547080994)
[2025-02-16 11:19:37,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:38,068][root][INFO] - Training Epoch: 1/2, step 2950/23838 completed (loss: 1.066339373588562, acc: 0.6800000071525574)
[2025-02-16 11:19:38,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:38,467][root][INFO] - Training Epoch: 1/2, step 2951/23838 completed (loss: 1.0861648321151733, acc: 0.6710526347160339)
[2025-02-16 11:19:38,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:38,881][root][INFO] - Training Epoch: 1/2, step 2952/23838 completed (loss: 0.7169730067253113, acc: 0.824999988079071)
[2025-02-16 11:19:39,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:39,342][root][INFO] - Training Epoch: 1/2, step 2953/23838 completed (loss: 0.8096775412559509, acc: 0.7682119011878967)
[2025-02-16 11:19:39,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:39,754][root][INFO] - Training Epoch: 1/2, step 2954/23838 completed (loss: 0.7614524364471436, acc: 0.7560975551605225)
[2025-02-16 11:19:39,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:40,182][root][INFO] - Training Epoch: 1/2, step 2955/23838 completed (loss: 0.7670872211456299, acc: 0.7799999713897705)
[2025-02-16 11:19:40,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:40,535][root][INFO] - Training Epoch: 1/2, step 2956/23838 completed (loss: 0.6772707104682922, acc: 0.8199999928474426)
[2025-02-16 11:19:40,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:40,986][root][INFO] - Training Epoch: 1/2, step 2957/23838 completed (loss: 0.49927112460136414, acc: 0.8493150472640991)
[2025-02-16 11:19:41,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:41,425][root][INFO] - Training Epoch: 1/2, step 2958/23838 completed (loss: 1.3087983131408691, acc: 0.6538461446762085)
[2025-02-16 11:19:41,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:41,922][root][INFO] - Training Epoch: 1/2, step 2959/23838 completed (loss: 1.3306069374084473, acc: 0.6594203114509583)
[2025-02-16 11:19:42,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:42,375][root][INFO] - Training Epoch: 1/2, step 2960/23838 completed (loss: 0.9280325770378113, acc: 0.7916666865348816)
[2025-02-16 11:19:42,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:42,807][root][INFO] - Training Epoch: 1/2, step 2961/23838 completed (loss: 0.7233573794364929, acc: 0.8333333134651184)
[2025-02-16 11:19:42,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:43,227][root][INFO] - Training Epoch: 1/2, step 2962/23838 completed (loss: 1.2473912239074707, acc: 0.65625)
[2025-02-16 11:19:43,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:43,679][root][INFO] - Training Epoch: 1/2, step 2963/23838 completed (loss: 1.0489689111709595, acc: 0.65625)
[2025-02-16 11:19:43,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:44,112][root][INFO] - Training Epoch: 1/2, step 2964/23838 completed (loss: 0.7366577386856079, acc: 0.800000011920929)
[2025-02-16 11:19:44,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:44,521][root][INFO] - Training Epoch: 1/2, step 2965/23838 completed (loss: 0.9195843935012817, acc: 0.699999988079071)
[2025-02-16 11:19:44,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:44,958][root][INFO] - Training Epoch: 1/2, step 2966/23838 completed (loss: 0.8202144503593445, acc: 0.7547169923782349)
[2025-02-16 11:19:45,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:45,376][root][INFO] - Training Epoch: 1/2, step 2967/23838 completed (loss: 0.7481768131256104, acc: 0.8260869383811951)
[2025-02-16 11:19:45,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:45,838][root][INFO] - Training Epoch: 1/2, step 2968/23838 completed (loss: 1.4907145500183105, acc: 0.6000000238418579)
[2025-02-16 11:19:46,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:46,243][root][INFO] - Training Epoch: 1/2, step 2969/23838 completed (loss: 0.9906644821166992, acc: 0.688524603843689)
[2025-02-16 11:19:46,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:46,616][root][INFO] - Training Epoch: 1/2, step 2970/23838 completed (loss: 0.8001839518547058, acc: 0.7945205569267273)
[2025-02-16 11:19:46,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:47,045][root][INFO] - Training Epoch: 1/2, step 2971/23838 completed (loss: 1.1379799842834473, acc: 0.6666666865348816)
[2025-02-16 11:19:47,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:47,494][root][INFO] - Training Epoch: 1/2, step 2972/23838 completed (loss: 1.0295964479446411, acc: 0.7272727489471436)
[2025-02-16 11:19:47,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:47,937][root][INFO] - Training Epoch: 1/2, step 2973/23838 completed (loss: 1.0483349561691284, acc: 0.774193525314331)
[2025-02-16 11:19:48,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:48,368][root][INFO] - Training Epoch: 1/2, step 2974/23838 completed (loss: 0.5156555771827698, acc: 0.8833333253860474)
[2025-02-16 11:19:48,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:48,879][root][INFO] - Training Epoch: 1/2, step 2975/23838 completed (loss: 1.0938209295272827, acc: 0.6808510422706604)
[2025-02-16 11:19:49,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:49,301][root][INFO] - Training Epoch: 1/2, step 2976/23838 completed (loss: 0.9584929347038269, acc: 0.6888889074325562)
[2025-02-16 11:19:49,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:50,162][root][INFO] - Training Epoch: 1/2, step 2977/23838 completed (loss: 0.6024577617645264, acc: 0.8666666746139526)
[2025-02-16 11:19:50,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:50,516][root][INFO] - Training Epoch: 1/2, step 2978/23838 completed (loss: 0.6629140377044678, acc: 0.8235294222831726)
[2025-02-16 11:19:50,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:50,873][root][INFO] - Training Epoch: 1/2, step 2979/23838 completed (loss: 0.47609084844589233, acc: 0.8399999737739563)
[2025-02-16 11:19:51,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:51,261][root][INFO] - Training Epoch: 1/2, step 2980/23838 completed (loss: 1.0815075635910034, acc: 0.7083333134651184)
[2025-02-16 11:19:51,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:51,744][root][INFO] - Training Epoch: 1/2, step 2981/23838 completed (loss: 0.7073076963424683, acc: 0.8292682766914368)
[2025-02-16 11:19:51,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:52,185][root][INFO] - Training Epoch: 1/2, step 2982/23838 completed (loss: 0.5160577297210693, acc: 0.8666666746139526)
[2025-02-16 11:19:52,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:52,633][root][INFO] - Training Epoch: 1/2, step 2983/23838 completed (loss: 1.186373233795166, acc: 0.6867470145225525)
[2025-02-16 11:19:52,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:53,073][root][INFO] - Training Epoch: 1/2, step 2984/23838 completed (loss: 1.020129919052124, acc: 0.7422680258750916)
[2025-02-16 11:19:53,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:53,505][root][INFO] - Training Epoch: 1/2, step 2985/23838 completed (loss: 0.7571104764938354, acc: 0.7317073345184326)
[2025-02-16 11:19:53,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:54,021][root][INFO] - Training Epoch: 1/2, step 2986/23838 completed (loss: 0.9732072353363037, acc: 0.6666666865348816)
[2025-02-16 11:19:54,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:54,420][root][INFO] - Training Epoch: 1/2, step 2987/23838 completed (loss: 1.2081352472305298, acc: 0.6727272868156433)
[2025-02-16 11:19:54,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:54,853][root][INFO] - Training Epoch: 1/2, step 2988/23838 completed (loss: 1.0566421747207642, acc: 0.7283950448036194)
[2025-02-16 11:19:55,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:55,301][root][INFO] - Training Epoch: 1/2, step 2989/23838 completed (loss: 0.9321659803390503, acc: 0.7941176295280457)
[2025-02-16 11:19:55,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:55,704][root][INFO] - Training Epoch: 1/2, step 2990/23838 completed (loss: 0.7672044634819031, acc: 0.824999988079071)
[2025-02-16 11:19:55,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:56,123][root][INFO] - Training Epoch: 1/2, step 2991/23838 completed (loss: 1.056587815284729, acc: 0.7042253613471985)
[2025-02-16 11:19:56,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:56,564][root][INFO] - Training Epoch: 1/2, step 2992/23838 completed (loss: 0.7313857674598694, acc: 0.804347813129425)
[2025-02-16 11:19:56,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:56,983][root][INFO] - Training Epoch: 1/2, step 2993/23838 completed (loss: 0.802970290184021, acc: 0.7954545617103577)
[2025-02-16 11:19:57,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:57,441][root][INFO] - Training Epoch: 1/2, step 2994/23838 completed (loss: 0.495634526014328, acc: 0.8064516186714172)
[2025-02-16 11:19:57,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:57,924][root][INFO] - Training Epoch: 1/2, step 2995/23838 completed (loss: 1.181095004081726, acc: 0.7307692170143127)
[2025-02-16 11:19:58,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:58,357][root][INFO] - Training Epoch: 1/2, step 2996/23838 completed (loss: 0.3588116765022278, acc: 0.9047619104385376)
[2025-02-16 11:19:58,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:58,772][root][INFO] - Training Epoch: 1/2, step 2997/23838 completed (loss: 0.2768629789352417, acc: 0.8999999761581421)
[2025-02-16 11:19:58,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:59,203][root][INFO] - Training Epoch: 1/2, step 2998/23838 completed (loss: 0.6668322682380676, acc: 0.8399999737739563)
[2025-02-16 11:19:59,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:19:59,643][root][INFO] - Training Epoch: 1/2, step 2999/23838 completed (loss: 0.3550877869129181, acc: 0.9130434989929199)
[2025-02-16 11:19:59,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:00,057][root][INFO] - Training Epoch: 1/2, step 3000/23838 completed (loss: 1.2538663148880005, acc: 0.6666666865348816)
[2025-02-16 11:20:00,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:00,435][root][INFO] - Training Epoch: 1/2, step 3001/23838 completed (loss: 1.083919644355774, acc: 0.6571428775787354)
[2025-02-16 11:20:00,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:00,854][root][INFO] - Training Epoch: 1/2, step 3002/23838 completed (loss: 1.9028635025024414, acc: 0.44999998807907104)
[2025-02-16 11:20:01,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:01,236][root][INFO] - Training Epoch: 1/2, step 3003/23838 completed (loss: 0.6283515095710754, acc: 0.7096773982048035)
[2025-02-16 11:20:01,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:01,610][root][INFO] - Training Epoch: 1/2, step 3004/23838 completed (loss: 0.8563374280929565, acc: 0.7857142686843872)
[2025-02-16 11:20:01,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:01,979][root][INFO] - Training Epoch: 1/2, step 3005/23838 completed (loss: 0.6201531291007996, acc: 0.8333333134651184)
[2025-02-16 11:20:02,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:02,333][root][INFO] - Training Epoch: 1/2, step 3006/23838 completed (loss: 0.9700700044631958, acc: 0.7142857313156128)
[2025-02-16 11:20:02,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:02,756][root][INFO] - Training Epoch: 1/2, step 3007/23838 completed (loss: 0.9924007654190063, acc: 0.7419354915618896)
[2025-02-16 11:20:02,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:03,182][root][INFO] - Training Epoch: 1/2, step 3008/23838 completed (loss: 0.4355113208293915, acc: 0.875)
[2025-02-16 11:20:03,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:03,586][root][INFO] - Training Epoch: 1/2, step 3009/23838 completed (loss: 0.7952018976211548, acc: 0.7272727489471436)
[2025-02-16 11:20:03,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:04,045][root][INFO] - Training Epoch: 1/2, step 3010/23838 completed (loss: 0.9090339541435242, acc: 0.7083333134651184)
[2025-02-16 11:20:04,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:04,449][root][INFO] - Training Epoch: 1/2, step 3011/23838 completed (loss: 1.4936078786849976, acc: 0.5833333134651184)
[2025-02-16 11:20:04,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:04,885][root][INFO] - Training Epoch: 1/2, step 3012/23838 completed (loss: 0.6342216730117798, acc: 0.8620689511299133)
[2025-02-16 11:20:05,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:05,305][root][INFO] - Training Epoch: 1/2, step 3013/23838 completed (loss: 1.192219853401184, acc: 0.6785714030265808)
[2025-02-16 11:20:05,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:05,681][root][INFO] - Training Epoch: 1/2, step 3014/23838 completed (loss: 1.43806791305542, acc: 0.5384615659713745)
[2025-02-16 11:20:05,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:06,073][root][INFO] - Training Epoch: 1/2, step 3015/23838 completed (loss: 0.4355233311653137, acc: 0.9411764740943909)
[2025-02-16 11:20:06,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:06,493][root][INFO] - Training Epoch: 1/2, step 3016/23838 completed (loss: 1.8254890441894531, acc: 0.5)
[2025-02-16 11:20:06,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:06,913][root][INFO] - Training Epoch: 1/2, step 3017/23838 completed (loss: 2.667576551437378, acc: 0.3529411852359772)
[2025-02-16 11:20:07,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:07,352][root][INFO] - Training Epoch: 1/2, step 3018/23838 completed (loss: 1.3886052370071411, acc: 0.7058823704719543)
[2025-02-16 11:20:07,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:07,774][root][INFO] - Training Epoch: 1/2, step 3019/23838 completed (loss: 0.8841729760169983, acc: 0.75)
[2025-02-16 11:20:07,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:08,157][root][INFO] - Training Epoch: 1/2, step 3020/23838 completed (loss: 0.6290478706359863, acc: 0.8421052694320679)
[2025-02-16 11:20:08,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:08,543][root][INFO] - Training Epoch: 1/2, step 3021/23838 completed (loss: 0.49261924624443054, acc: 0.8636363744735718)
[2025-02-16 11:20:08,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:08,938][root][INFO] - Training Epoch: 1/2, step 3022/23838 completed (loss: 0.3189144730567932, acc: 0.9166666865348816)
[2025-02-16 11:20:09,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:09,344][root][INFO] - Training Epoch: 1/2, step 3023/23838 completed (loss: 1.069770336151123, acc: 0.7407407164573669)
[2025-02-16 11:20:09,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:09,766][root][INFO] - Training Epoch: 1/2, step 3024/23838 completed (loss: 0.5752840638160706, acc: 0.8611111044883728)
[2025-02-16 11:20:09,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:10,146][root][INFO] - Training Epoch: 1/2, step 3025/23838 completed (loss: 0.7918776273727417, acc: 0.800000011920929)
[2025-02-16 11:20:10,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:10,518][root][INFO] - Training Epoch: 1/2, step 3026/23838 completed (loss: 1.2512203454971313, acc: 0.6818181872367859)
[2025-02-16 11:20:10,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:10,953][root][INFO] - Training Epoch: 1/2, step 3027/23838 completed (loss: 1.2277071475982666, acc: 0.6785714030265808)
[2025-02-16 11:20:11,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:11,456][root][INFO] - Training Epoch: 1/2, step 3028/23838 completed (loss: 1.9634414911270142, acc: 0.5428571701049805)
[2025-02-16 11:20:11,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:11,885][root][INFO] - Training Epoch: 1/2, step 3029/23838 completed (loss: 1.3482543230056763, acc: 0.75)
[2025-02-16 11:20:12,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:12,382][root][INFO] - Training Epoch: 1/2, step 3030/23838 completed (loss: 1.5781913995742798, acc: 0.6315789222717285)
[2025-02-16 11:20:12,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:12,860][root][INFO] - Training Epoch: 1/2, step 3031/23838 completed (loss: 2.1563174724578857, acc: 0.3913043439388275)
[2025-02-16 11:20:13,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:13,251][root][INFO] - Training Epoch: 1/2, step 3032/23838 completed (loss: 2.161627769470215, acc: 0.4038461446762085)
[2025-02-16 11:20:13,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:13,672][root][INFO] - Training Epoch: 1/2, step 3033/23838 completed (loss: 1.37794029712677, acc: 0.7037037014961243)
[2025-02-16 11:20:13,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:14,074][root][INFO] - Training Epoch: 1/2, step 3034/23838 completed (loss: 1.5400669574737549, acc: 0.6499999761581421)
[2025-02-16 11:20:14,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:14,469][root][INFO] - Training Epoch: 1/2, step 3035/23838 completed (loss: 1.033975601196289, acc: 0.71875)
[2025-02-16 11:20:14,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:14,882][root][INFO] - Training Epoch: 1/2, step 3036/23838 completed (loss: 1.3419573307037354, acc: 0.5185185074806213)
[2025-02-16 11:20:15,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:15,298][root][INFO] - Training Epoch: 1/2, step 3037/23838 completed (loss: 1.2049812078475952, acc: 0.6857143044471741)
[2025-02-16 11:20:15,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:15,704][root][INFO] - Training Epoch: 1/2, step 3038/23838 completed (loss: 1.1824756860733032, acc: 0.7272727489471436)
[2025-02-16 11:20:15,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:16,059][root][INFO] - Training Epoch: 1/2, step 3039/23838 completed (loss: 1.0877946615219116, acc: 0.739130437374115)
[2025-02-16 11:20:16,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:16,440][root][INFO] - Training Epoch: 1/2, step 3040/23838 completed (loss: 0.9247382283210754, acc: 0.75)
[2025-02-16 11:20:16,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:16,855][root][INFO] - Training Epoch: 1/2, step 3041/23838 completed (loss: 0.8793485760688782, acc: 0.761904776096344)
[2025-02-16 11:20:17,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:17,239][root][INFO] - Training Epoch: 1/2, step 3042/23838 completed (loss: 0.960055410861969, acc: 0.8125)
[2025-02-16 11:20:17,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:17,617][root][INFO] - Training Epoch: 1/2, step 3043/23838 completed (loss: 1.6338292360305786, acc: 0.5714285969734192)
[2025-02-16 11:20:17,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:18,008][root][INFO] - Training Epoch: 1/2, step 3044/23838 completed (loss: 0.7076990604400635, acc: 0.7837837934494019)
[2025-02-16 11:20:18,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:18,401][root][INFO] - Training Epoch: 1/2, step 3045/23838 completed (loss: 0.5938027501106262, acc: 0.8484848737716675)
[2025-02-16 11:20:18,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:18,881][root][INFO] - Training Epoch: 1/2, step 3046/23838 completed (loss: 1.1898391246795654, acc: 0.6666666865348816)
[2025-02-16 11:20:19,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:19,337][root][INFO] - Training Epoch: 1/2, step 3047/23838 completed (loss: 0.7687548398971558, acc: 0.75)
[2025-02-16 11:20:19,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:19,761][root][INFO] - Training Epoch: 1/2, step 3048/23838 completed (loss: 1.4670568704605103, acc: 0.5365853905677795)
[2025-02-16 11:20:19,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:20,180][root][INFO] - Training Epoch: 1/2, step 3049/23838 completed (loss: 1.4313887357711792, acc: 0.604651153087616)
[2025-02-16 11:20:20,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:20,637][root][INFO] - Training Epoch: 1/2, step 3050/23838 completed (loss: 1.492800235748291, acc: 0.5319148898124695)
[2025-02-16 11:20:20,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:21,081][root][INFO] - Training Epoch: 1/2, step 3051/23838 completed (loss: 1.2243695259094238, acc: 0.6976743936538696)
[2025-02-16 11:20:21,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:21,478][root][INFO] - Training Epoch: 1/2, step 3052/23838 completed (loss: 1.296939730644226, acc: 0.7209302186965942)
[2025-02-16 11:20:21,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:21,852][root][INFO] - Training Epoch: 1/2, step 3053/23838 completed (loss: 0.8779771327972412, acc: 0.7916666865348816)
[2025-02-16 11:20:22,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:22,300][root][INFO] - Training Epoch: 1/2, step 3054/23838 completed (loss: 0.9517655968666077, acc: 0.774193525314331)
[2025-02-16 11:20:22,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:22,743][root][INFO] - Training Epoch: 1/2, step 3055/23838 completed (loss: 1.3491748571395874, acc: 0.6279069781303406)
[2025-02-16 11:20:22,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:23,182][root][INFO] - Training Epoch: 1/2, step 3056/23838 completed (loss: 0.8020303845405579, acc: 0.7924528121948242)
[2025-02-16 11:20:23,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:23,597][root][INFO] - Training Epoch: 1/2, step 3057/23838 completed (loss: 1.0189744234085083, acc: 0.75)
[2025-02-16 11:20:23,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:23,968][root][INFO] - Training Epoch: 1/2, step 3058/23838 completed (loss: 1.6716089248657227, acc: 0.6097561120986938)
[2025-02-16 11:20:24,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:24,393][root][INFO] - Training Epoch: 1/2, step 3059/23838 completed (loss: 1.2483476400375366, acc: 0.7291666865348816)
[2025-02-16 11:20:24,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:24,823][root][INFO] - Training Epoch: 1/2, step 3060/23838 completed (loss: 0.8363572359085083, acc: 0.7037037014961243)
[2025-02-16 11:20:24,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:25,245][root][INFO] - Training Epoch: 1/2, step 3061/23838 completed (loss: 0.5255056023597717, acc: 0.8888888955116272)
[2025-02-16 11:20:25,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:25,674][root][INFO] - Training Epoch: 1/2, step 3062/23838 completed (loss: 0.811673104763031, acc: 0.7599999904632568)
[2025-02-16 11:20:25,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:26,043][root][INFO] - Training Epoch: 1/2, step 3063/23838 completed (loss: 0.6080164909362793, acc: 0.7906976938247681)
[2025-02-16 11:20:26,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:26,482][root][INFO] - Training Epoch: 1/2, step 3064/23838 completed (loss: 0.7599387764930725, acc: 0.8214285969734192)
[2025-02-16 11:20:26,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:26,956][root][INFO] - Training Epoch: 1/2, step 3065/23838 completed (loss: 1.6442686319351196, acc: 0.6129032373428345)
[2025-02-16 11:20:27,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:27,376][root][INFO] - Training Epoch: 1/2, step 3066/23838 completed (loss: 1.8972874879837036, acc: 0.5400000214576721)
[2025-02-16 11:20:27,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:27,812][root][INFO] - Training Epoch: 1/2, step 3067/23838 completed (loss: 1.9407460689544678, acc: 0.5263158082962036)
[2025-02-16 11:20:28,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:28,263][root][INFO] - Training Epoch: 1/2, step 3068/23838 completed (loss: 0.40396279096603394, acc: 0.8947368264198303)
[2025-02-16 11:20:28,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:28,714][root][INFO] - Training Epoch: 1/2, step 3069/23838 completed (loss: 1.1886996030807495, acc: 0.760869562625885)
[2025-02-16 11:20:28,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:29,164][root][INFO] - Training Epoch: 1/2, step 3070/23838 completed (loss: 1.4267820119857788, acc: 0.6129032373428345)
[2025-02-16 11:20:29,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:29,531][root][INFO] - Training Epoch: 1/2, step 3071/23838 completed (loss: 0.6442526578903198, acc: 0.75)
[2025-02-16 11:20:29,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:29,950][root][INFO] - Training Epoch: 1/2, step 3072/23838 completed (loss: 0.7083777189254761, acc: 0.8500000238418579)
[2025-02-16 11:20:30,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:30,331][root][INFO] - Training Epoch: 1/2, step 3073/23838 completed (loss: 0.8574960827827454, acc: 0.7599999904632568)
[2025-02-16 11:20:30,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:30,677][root][INFO] - Training Epoch: 1/2, step 3074/23838 completed (loss: 0.8470374345779419, acc: 0.7857142686843872)
[2025-02-16 11:20:30,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:31,063][root][INFO] - Training Epoch: 1/2, step 3075/23838 completed (loss: 2.265005588531494, acc: 0.4285714328289032)
[2025-02-16 11:20:31,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:31,510][root][INFO] - Training Epoch: 1/2, step 3076/23838 completed (loss: 1.6408942937850952, acc: 0.4838709533214569)
[2025-02-16 11:20:31,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:31,893][root][INFO] - Training Epoch: 1/2, step 3077/23838 completed (loss: 1.1255513429641724, acc: 0.6363636255264282)
[2025-02-16 11:20:32,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:32,286][root][INFO] - Training Epoch: 1/2, step 3078/23838 completed (loss: 1.4576083421707153, acc: 0.5882353186607361)
[2025-02-16 11:20:32,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:32,693][root][INFO] - Training Epoch: 1/2, step 3079/23838 completed (loss: 0.5306170582771301, acc: 0.8387096524238586)
[2025-02-16 11:20:32,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:33,075][root][INFO] - Training Epoch: 1/2, step 3080/23838 completed (loss: 0.3194468319416046, acc: 0.8571428656578064)
[2025-02-16 11:20:33,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:33,517][root][INFO] - Training Epoch: 1/2, step 3081/23838 completed (loss: 0.6683853268623352, acc: 0.7777777910232544)
[2025-02-16 11:20:33,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:33,985][root][INFO] - Training Epoch: 1/2, step 3082/23838 completed (loss: 0.4930320382118225, acc: 0.8333333134651184)
[2025-02-16 11:20:34,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:34,458][root][INFO] - Training Epoch: 1/2, step 3083/23838 completed (loss: 0.30828091502189636, acc: 0.9512194991111755)
[2025-02-16 11:20:34,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:34,874][root][INFO] - Training Epoch: 1/2, step 3084/23838 completed (loss: 1.7146081924438477, acc: 0.53125)
[2025-02-16 11:20:35,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:35,254][root][INFO] - Training Epoch: 1/2, step 3085/23838 completed (loss: 1.1803263425827026, acc: 0.6607142686843872)
[2025-02-16 11:20:35,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:35,627][root][INFO] - Training Epoch: 1/2, step 3086/23838 completed (loss: 1.1505045890808105, acc: 0.7428571581840515)
[2025-02-16 11:20:35,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:36,072][root][INFO] - Training Epoch: 1/2, step 3087/23838 completed (loss: 1.250648856163025, acc: 0.5833333134651184)
[2025-02-16 11:20:36,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:36,540][root][INFO] - Training Epoch: 1/2, step 3088/23838 completed (loss: 0.4874389171600342, acc: 0.90625)
[2025-02-16 11:20:36,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:36,945][root][INFO] - Training Epoch: 1/2, step 3089/23838 completed (loss: 0.8456794619560242, acc: 0.8305084705352783)
[2025-02-16 11:20:37,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:37,438][root][INFO] - Training Epoch: 1/2, step 3090/23838 completed (loss: 1.2042148113250732, acc: 0.703125)
[2025-02-16 11:20:37,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:37,884][root][INFO] - Training Epoch: 1/2, step 3091/23838 completed (loss: 0.4259195923805237, acc: 0.8571428656578064)
[2025-02-16 11:20:38,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:38,332][root][INFO] - Training Epoch: 1/2, step 3092/23838 completed (loss: 0.985717236995697, acc: 0.7321428656578064)
[2025-02-16 11:20:38,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:38,833][root][INFO] - Training Epoch: 1/2, step 3093/23838 completed (loss: 1.2817955017089844, acc: 0.6853932738304138)
[2025-02-16 11:20:39,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:39,324][root][INFO] - Training Epoch: 1/2, step 3094/23838 completed (loss: 1.1066652536392212, acc: 0.7234042286872864)
[2025-02-16 11:20:39,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:39,737][root][INFO] - Training Epoch: 1/2, step 3095/23838 completed (loss: 0.5316005349159241, acc: 0.8299999833106995)
[2025-02-16 11:20:39,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:40,236][root][INFO] - Training Epoch: 1/2, step 3096/23838 completed (loss: 0.4444829523563385, acc: 0.8461538553237915)
[2025-02-16 11:20:40,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:40,682][root][INFO] - Training Epoch: 1/2, step 3097/23838 completed (loss: 0.46043717861175537, acc: 0.8305084705352783)
[2025-02-16 11:20:40,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:41,213][root][INFO] - Training Epoch: 1/2, step 3098/23838 completed (loss: 0.6843656897544861, acc: 0.849056601524353)
[2025-02-16 11:20:41,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:41,675][root][INFO] - Training Epoch: 1/2, step 3099/23838 completed (loss: 0.48328450322151184, acc: 0.8709677457809448)
[2025-02-16 11:20:41,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:42,094][root][INFO] - Training Epoch: 1/2, step 3100/23838 completed (loss: 0.34908804297447205, acc: 0.9117646813392639)
[2025-02-16 11:20:42,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:42,515][root][INFO] - Training Epoch: 1/2, step 3101/23838 completed (loss: 0.34208428859710693, acc: 0.89552241563797)
[2025-02-16 11:20:42,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:42,925][root][INFO] - Training Epoch: 1/2, step 3102/23838 completed (loss: 0.5339578986167908, acc: 0.8470588326454163)
[2025-02-16 11:20:43,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:43,407][root][INFO] - Training Epoch: 1/2, step 3103/23838 completed (loss: 0.770109236240387, acc: 0.7951807379722595)
[2025-02-16 11:20:43,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:44,108][root][INFO] - Training Epoch: 1/2, step 3104/23838 completed (loss: 0.7936494946479797, acc: 0.8103448152542114)
[2025-02-16 11:20:44,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:44,487][root][INFO] - Training Epoch: 1/2, step 3105/23838 completed (loss: 0.4536522924900055, acc: 0.8199999928474426)
[2025-02-16 11:20:44,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:45,010][root][INFO] - Training Epoch: 1/2, step 3106/23838 completed (loss: 1.2546734809875488, acc: 0.7313432693481445)
[2025-02-16 11:20:45,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:45,520][root][INFO] - Training Epoch: 1/2, step 3107/23838 completed (loss: 2.0766100883483887, acc: 0.5384615659713745)
[2025-02-16 11:20:45,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:46,075][root][INFO] - Training Epoch: 1/2, step 3108/23838 completed (loss: 0.9042600989341736, acc: 0.7606837749481201)
[2025-02-16 11:20:46,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:46,559][root][INFO] - Training Epoch: 1/2, step 3109/23838 completed (loss: 0.579998254776001, acc: 0.8113207817077637)
[2025-02-16 11:20:46,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:47,195][root][INFO] - Training Epoch: 1/2, step 3110/23838 completed (loss: 0.6344923973083496, acc: 0.8235294222831726)
[2025-02-16 11:20:47,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:47,739][root][INFO] - Training Epoch: 1/2, step 3111/23838 completed (loss: 0.4807564914226532, acc: 0.8269230723381042)
[2025-02-16 11:20:47,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:48,184][root][INFO] - Training Epoch: 1/2, step 3112/23838 completed (loss: 0.6467409133911133, acc: 0.8252426981925964)
[2025-02-16 11:20:48,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:48,600][root][INFO] - Training Epoch: 1/2, step 3113/23838 completed (loss: 0.5591752529144287, acc: 0.8850574493408203)
[2025-02-16 11:20:48,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:49,049][root][INFO] - Training Epoch: 1/2, step 3114/23838 completed (loss: 0.8800491094589233, acc: 0.7752808928489685)
[2025-02-16 11:20:49,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:49,443][root][INFO] - Training Epoch: 1/2, step 3115/23838 completed (loss: 0.7954365015029907, acc: 0.84375)
[2025-02-16 11:20:49,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:49,842][root][INFO] - Training Epoch: 1/2, step 3116/23838 completed (loss: 0.8647379875183105, acc: 0.7400000095367432)
[2025-02-16 11:20:50,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:50,246][root][INFO] - Training Epoch: 1/2, step 3117/23838 completed (loss: 0.5118091106414795, acc: 0.8571428656578064)
[2025-02-16 11:20:50,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:50,643][root][INFO] - Training Epoch: 1/2, step 3118/23838 completed (loss: 0.7405245900154114, acc: 0.7272727489471436)
[2025-02-16 11:20:50,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:51,094][root][INFO] - Training Epoch: 1/2, step 3119/23838 completed (loss: 0.6742578744888306, acc: 0.7906976938247681)
[2025-02-16 11:20:51,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:51,493][root][INFO] - Training Epoch: 1/2, step 3120/23838 completed (loss: 0.5448094010353088, acc: 0.7948718070983887)
[2025-02-16 11:20:51,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:51,967][root][INFO] - Training Epoch: 1/2, step 3121/23838 completed (loss: 0.42715129256248474, acc: 0.8873239159584045)
[2025-02-16 11:20:52,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:52,386][root][INFO] - Training Epoch: 1/2, step 3122/23838 completed (loss: 1.1456456184387207, acc: 0.6551724076271057)
[2025-02-16 11:20:52,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:52,821][root][INFO] - Training Epoch: 1/2, step 3123/23838 completed (loss: 0.8663367033004761, acc: 0.7090908885002136)
[2025-02-16 11:20:53,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:53,239][root][INFO] - Training Epoch: 1/2, step 3124/23838 completed (loss: 1.5902519226074219, acc: 0.4761904776096344)
[2025-02-16 11:20:53,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:53,657][root][INFO] - Training Epoch: 1/2, step 3125/23838 completed (loss: 0.6809666156768799, acc: 0.807692289352417)
[2025-02-16 11:20:53,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:54,070][root][INFO] - Training Epoch: 1/2, step 3126/23838 completed (loss: 0.3403737246990204, acc: 0.8837209343910217)
[2025-02-16 11:20:54,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:54,501][root][INFO] - Training Epoch: 1/2, step 3127/23838 completed (loss: 0.4078613221645355, acc: 0.9120879173278809)
[2025-02-16 11:20:54,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:54,944][root][INFO] - Training Epoch: 1/2, step 3128/23838 completed (loss: 0.5959852337837219, acc: 0.8380952477455139)
[2025-02-16 11:20:55,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:55,349][root][INFO] - Training Epoch: 1/2, step 3129/23838 completed (loss: 0.807300329208374, acc: 0.7580645084381104)
[2025-02-16 11:20:55,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:55,741][root][INFO] - Training Epoch: 1/2, step 3130/23838 completed (loss: 0.6062608957290649, acc: 0.8214285969734192)
[2025-02-16 11:20:55,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:56,194][root][INFO] - Training Epoch: 1/2, step 3131/23838 completed (loss: 0.4285426735877991, acc: 0.9076923131942749)
[2025-02-16 11:20:56,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:56,594][root][INFO] - Training Epoch: 1/2, step 3132/23838 completed (loss: 0.7334867119789124, acc: 0.7798165082931519)
[2025-02-16 11:20:56,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:57,022][root][INFO] - Training Epoch: 1/2, step 3133/23838 completed (loss: 1.0982998609542847, acc: 0.699999988079071)
[2025-02-16 11:20:57,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:57,433][root][INFO] - Training Epoch: 1/2, step 3134/23838 completed (loss: 0.2631729245185852, acc: 0.9117646813392639)
[2025-02-16 11:20:57,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:57,862][root][INFO] - Training Epoch: 1/2, step 3135/23838 completed (loss: 0.5507280230522156, acc: 0.8275862336158752)
[2025-02-16 11:20:58,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:58,195][root][INFO] - Training Epoch: 1/2, step 3136/23838 completed (loss: 0.39268526434898376, acc: 0.8867924809455872)
[2025-02-16 11:20:58,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:58,640][root][INFO] - Training Epoch: 1/2, step 3137/23838 completed (loss: 0.6366990208625793, acc: 0.78125)
[2025-02-16 11:20:58,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:59,042][root][INFO] - Training Epoch: 1/2, step 3138/23838 completed (loss: 0.2575295567512512, acc: 0.8958333134651184)
[2025-02-16 11:20:59,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:59,412][root][INFO] - Training Epoch: 1/2, step 3139/23838 completed (loss: 0.3832038938999176, acc: 0.8409090638160706)
[2025-02-16 11:20:59,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:20:59,797][root][INFO] - Training Epoch: 1/2, step 3140/23838 completed (loss: 0.682157576084137, acc: 0.8219178318977356)
[2025-02-16 11:20:59,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:00,224][root][INFO] - Training Epoch: 1/2, step 3141/23838 completed (loss: 0.20494015514850616, acc: 0.9629629850387573)
[2025-02-16 11:21:00,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:00,687][root][INFO] - Training Epoch: 1/2, step 3142/23838 completed (loss: 0.3901965320110321, acc: 0.8805969953536987)
[2025-02-16 11:21:00,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:01,127][root][INFO] - Training Epoch: 1/2, step 3143/23838 completed (loss: 0.5459157824516296, acc: 0.837837815284729)
[2025-02-16 11:21:01,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:01,535][root][INFO] - Training Epoch: 1/2, step 3144/23838 completed (loss: 0.21000899374485016, acc: 0.9324324131011963)
[2025-02-16 11:21:01,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:02,002][root][INFO] - Training Epoch: 1/2, step 3145/23838 completed (loss: 0.4808606207370758, acc: 0.8961039185523987)
[2025-02-16 11:21:02,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:02,500][root][INFO] - Training Epoch: 1/2, step 3146/23838 completed (loss: 0.9115995168685913, acc: 0.7786259651184082)
[2025-02-16 11:21:02,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:02,978][root][INFO] - Training Epoch: 1/2, step 3147/23838 completed (loss: 0.4022587835788727, acc: 0.8600000143051147)
[2025-02-16 11:21:03,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:03,365][root][INFO] - Training Epoch: 1/2, step 3148/23838 completed (loss: 1.4011427164077759, acc: 0.6666666865348816)
[2025-02-16 11:21:03,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:03,808][root][INFO] - Training Epoch: 1/2, step 3149/23838 completed (loss: 0.9786378145217896, acc: 0.778761088848114)
[2025-02-16 11:21:04,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:04,268][root][INFO] - Training Epoch: 1/2, step 3150/23838 completed (loss: 0.3367289900779724, acc: 0.8947368264198303)
[2025-02-16 11:21:04,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:04,730][root][INFO] - Training Epoch: 1/2, step 3151/23838 completed (loss: 0.19378820061683655, acc: 0.9733333587646484)
[2025-02-16 11:21:04,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:05,181][root][INFO] - Training Epoch: 1/2, step 3152/23838 completed (loss: 0.5723956227302551, acc: 0.8780487775802612)
[2025-02-16 11:21:05,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:05,701][root][INFO] - Training Epoch: 1/2, step 3153/23838 completed (loss: 0.47414612770080566, acc: 0.8695651888847351)
[2025-02-16 11:21:05,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:06,148][root][INFO] - Training Epoch: 1/2, step 3154/23838 completed (loss: 0.6029811501502991, acc: 0.8518518805503845)
[2025-02-16 11:21:06,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:06,598][root][INFO] - Training Epoch: 1/2, step 3155/23838 completed (loss: 0.6471894979476929, acc: 0.8269230723381042)
[2025-02-16 11:21:06,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:07,031][root][INFO] - Training Epoch: 1/2, step 3156/23838 completed (loss: 0.25783440470695496, acc: 0.9589040875434875)
[2025-02-16 11:21:07,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:07,476][root][INFO] - Training Epoch: 1/2, step 3157/23838 completed (loss: 0.9224362969398499, acc: 0.7230769395828247)
[2025-02-16 11:21:07,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:07,921][root][INFO] - Training Epoch: 1/2, step 3158/23838 completed (loss: 0.48350828886032104, acc: 0.8764045238494873)
[2025-02-16 11:21:08,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:08,358][root][INFO] - Training Epoch: 1/2, step 3159/23838 completed (loss: 0.28030332922935486, acc: 0.9506173133850098)
[2025-02-16 11:21:08,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:08,776][root][INFO] - Training Epoch: 1/2, step 3160/23838 completed (loss: 0.14221696555614471, acc: 0.9459459185600281)
[2025-02-16 11:21:08,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:09,177][root][INFO] - Training Epoch: 1/2, step 3161/23838 completed (loss: 0.6000688672065735, acc: 0.8484848737716675)
[2025-02-16 11:21:09,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:09,595][root][INFO] - Training Epoch: 1/2, step 3162/23838 completed (loss: 0.3488420844078064, acc: 0.855555534362793)
[2025-02-16 11:21:09,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:10,059][root][INFO] - Training Epoch: 1/2, step 3163/23838 completed (loss: 0.4745032489299774, acc: 0.8888888955116272)
[2025-02-16 11:21:10,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:10,457][root][INFO] - Training Epoch: 1/2, step 3164/23838 completed (loss: 0.7578783631324768, acc: 0.8275862336158752)
[2025-02-16 11:21:10,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:10,930][root][INFO] - Training Epoch: 1/2, step 3165/23838 completed (loss: 0.8384336233139038, acc: 0.800000011920929)
[2025-02-16 11:21:11,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:11,365][root][INFO] - Training Epoch: 1/2, step 3166/23838 completed (loss: 1.3592307567596436, acc: 0.7272727489471436)
[2025-02-16 11:21:11,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:11,738][root][INFO] - Training Epoch: 1/2, step 3167/23838 completed (loss: 1.301419734954834, acc: 0.6875)
[2025-02-16 11:21:11,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:12,182][root][INFO] - Training Epoch: 1/2, step 3168/23838 completed (loss: 2.196436643600464, acc: 0.4516128897666931)
[2025-02-16 11:21:12,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:12,620][root][INFO] - Training Epoch: 1/2, step 3169/23838 completed (loss: 1.2623332738876343, acc: 0.6470588445663452)
[2025-02-16 11:21:12,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:13,072][root][INFO] - Training Epoch: 1/2, step 3170/23838 completed (loss: 1.1854937076568604, acc: 0.6976743936538696)
[2025-02-16 11:21:13,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:13,429][root][INFO] - Training Epoch: 1/2, step 3171/23838 completed (loss: 0.35062676668167114, acc: 0.9047619104385376)
[2025-02-16 11:21:13,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:13,828][root][INFO] - Training Epoch: 1/2, step 3172/23838 completed (loss: 0.8740476369857788, acc: 0.7424242496490479)
[2025-02-16 11:21:13,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:14,194][root][INFO] - Training Epoch: 1/2, step 3173/23838 completed (loss: 0.9245752692222595, acc: 0.7297297120094299)
[2025-02-16 11:21:14,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:14,581][root][INFO] - Training Epoch: 1/2, step 3174/23838 completed (loss: 0.5832083225250244, acc: 0.7692307829856873)
[2025-02-16 11:21:14,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:15,017][root][INFO] - Training Epoch: 1/2, step 3175/23838 completed (loss: 0.9834045171737671, acc: 0.75)
[2025-02-16 11:21:15,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:15,497][root][INFO] - Training Epoch: 1/2, step 3176/23838 completed (loss: 1.0337907075881958, acc: 0.6730769276618958)
[2025-02-16 11:21:15,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:15,962][root][INFO] - Training Epoch: 1/2, step 3177/23838 completed (loss: 1.0172022581100464, acc: 0.7692307829856873)
[2025-02-16 11:21:16,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:16,380][root][INFO] - Training Epoch: 1/2, step 3178/23838 completed (loss: 0.4184724986553192, acc: 0.8659793734550476)
[2025-02-16 11:21:16,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:16,886][root][INFO] - Training Epoch: 1/2, step 3179/23838 completed (loss: 0.7514591217041016, acc: 0.8160919547080994)
[2025-02-16 11:21:17,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:17,322][root][INFO] - Training Epoch: 1/2, step 3180/23838 completed (loss: 0.7002898454666138, acc: 0.8030303120613098)
[2025-02-16 11:21:17,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:17,788][root][INFO] - Training Epoch: 1/2, step 3181/23838 completed (loss: 0.6697640419006348, acc: 0.8809523582458496)
[2025-02-16 11:21:17,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:18,218][root][INFO] - Training Epoch: 1/2, step 3182/23838 completed (loss: 0.5997077822685242, acc: 0.8170731663703918)
[2025-02-16 11:21:18,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:18,678][root][INFO] - Training Epoch: 1/2, step 3183/23838 completed (loss: 0.9728822708129883, acc: 0.7311828136444092)
[2025-02-16 11:21:18,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:19,075][root][INFO] - Training Epoch: 1/2, step 3184/23838 completed (loss: 0.5441431999206543, acc: 0.8571428656578064)
[2025-02-16 11:21:19,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:19,471][root][INFO] - Training Epoch: 1/2, step 3185/23838 completed (loss: 0.3625624179840088, acc: 0.9090909361839294)
[2025-02-16 11:21:19,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:19,873][root][INFO] - Training Epoch: 1/2, step 3186/23838 completed (loss: 0.6057448387145996, acc: 0.8365384340286255)
[2025-02-16 11:21:20,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:20,316][root][INFO] - Training Epoch: 1/2, step 3187/23838 completed (loss: 1.0051442384719849, acc: 0.7681159377098083)
[2025-02-16 11:21:20,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:20,746][root][INFO] - Training Epoch: 1/2, step 3188/23838 completed (loss: 1.4464046955108643, acc: 0.6170212626457214)
[2025-02-16 11:21:20,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:21,195][root][INFO] - Training Epoch: 1/2, step 3189/23838 completed (loss: 0.5298873782157898, acc: 0.8390804529190063)
[2025-02-16 11:21:21,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:21,627][root][INFO] - Training Epoch: 1/2, step 3190/23838 completed (loss: 0.9411745667457581, acc: 0.7090908885002136)
[2025-02-16 11:21:21,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:22,034][root][INFO] - Training Epoch: 1/2, step 3191/23838 completed (loss: 0.6784709692001343, acc: 0.8181818127632141)
[2025-02-16 11:21:22,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:22,435][root][INFO] - Training Epoch: 1/2, step 3192/23838 completed (loss: 0.7078842520713806, acc: 0.7555555701255798)
[2025-02-16 11:21:22,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:22,880][root][INFO] - Training Epoch: 1/2, step 3193/23838 completed (loss: 0.5834228992462158, acc: 0.8333333134651184)
[2025-02-16 11:21:23,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:23,326][root][INFO] - Training Epoch: 1/2, step 3194/23838 completed (loss: 0.2985175549983978, acc: 0.8999999761581421)
[2025-02-16 11:21:23,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:23,776][root][INFO] - Training Epoch: 1/2, step 3195/23838 completed (loss: 0.3767293691635132, acc: 0.8857142925262451)
[2025-02-16 11:21:24,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:24,315][root][INFO] - Training Epoch: 1/2, step 3196/23838 completed (loss: 0.343061238527298, acc: 0.924369752407074)
[2025-02-16 11:21:24,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:24,700][root][INFO] - Training Epoch: 1/2, step 3197/23838 completed (loss: 1.2108012437820435, acc: 0.671875)
[2025-02-16 11:21:24,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:25,065][root][INFO] - Training Epoch: 1/2, step 3198/23838 completed (loss: 0.4173726737499237, acc: 0.8799999952316284)
[2025-02-16 11:21:25,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:25,538][root][INFO] - Training Epoch: 1/2, step 3199/23838 completed (loss: 0.6574087142944336, acc: 0.7894737124443054)
[2025-02-16 11:21:25,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:26,033][root][INFO] - Training Epoch: 1/2, step 3200/23838 completed (loss: 0.6172659397125244, acc: 0.8571428656578064)
[2025-02-16 11:21:26,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:26,530][root][INFO] - Training Epoch: 1/2, step 3201/23838 completed (loss: 0.7297707796096802, acc: 0.8271604776382446)
[2025-02-16 11:21:26,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:26,957][root][INFO] - Training Epoch: 1/2, step 3202/23838 completed (loss: 0.598064661026001, acc: 0.8235294222831726)
[2025-02-16 11:21:27,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:27,344][root][INFO] - Training Epoch: 1/2, step 3203/23838 completed (loss: 0.9658152461051941, acc: 0.7283950448036194)
[2025-02-16 11:21:27,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:27,798][root][INFO] - Training Epoch: 1/2, step 3204/23838 completed (loss: 0.4464932680130005, acc: 0.9101123809814453)
[2025-02-16 11:21:28,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:28,250][root][INFO] - Training Epoch: 1/2, step 3205/23838 completed (loss: 0.592341959476471, acc: 0.8484848737716675)
[2025-02-16 11:21:28,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:28,696][root][INFO] - Training Epoch: 1/2, step 3206/23838 completed (loss: 0.3993006944656372, acc: 0.8765432238578796)
[2025-02-16 11:21:28,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:29,117][root][INFO] - Training Epoch: 1/2, step 3207/23838 completed (loss: 1.1558623313903809, acc: 0.725806474685669)
[2025-02-16 11:21:29,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:29,596][root][INFO] - Training Epoch: 1/2, step 3208/23838 completed (loss: 0.6982691287994385, acc: 0.8023256063461304)
[2025-02-16 11:21:29,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:30,035][root][INFO] - Training Epoch: 1/2, step 3209/23838 completed (loss: 0.7212592959403992, acc: 0.7941176295280457)
[2025-02-16 11:21:30,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:30,498][root][INFO] - Training Epoch: 1/2, step 3210/23838 completed (loss: 0.9491226673126221, acc: 0.7469879388809204)
[2025-02-16 11:21:30,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:30,957][root][INFO] - Training Epoch: 1/2, step 3211/23838 completed (loss: 1.0932170152664185, acc: 0.703125)
[2025-02-16 11:21:31,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:31,398][root][INFO] - Training Epoch: 1/2, step 3212/23838 completed (loss: 0.7033752202987671, acc: 0.7777777910232544)
[2025-02-16 11:21:31,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:31,790][root][INFO] - Training Epoch: 1/2, step 3213/23838 completed (loss: 0.7061247229576111, acc: 0.800000011920929)
[2025-02-16 11:21:32,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:32,322][root][INFO] - Training Epoch: 1/2, step 3214/23838 completed (loss: 0.7829266786575317, acc: 0.779411792755127)
[2025-02-16 11:21:32,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:32,778][root][INFO] - Training Epoch: 1/2, step 3215/23838 completed (loss: 0.6142876744270325, acc: 0.8229166865348816)
[2025-02-16 11:21:32,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:33,158][root][INFO] - Training Epoch: 1/2, step 3216/23838 completed (loss: 0.21889789402484894, acc: 0.9714285731315613)
[2025-02-16 11:21:33,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:33,599][root][INFO] - Training Epoch: 1/2, step 3217/23838 completed (loss: 0.6805675625801086, acc: 0.7887324094772339)
[2025-02-16 11:21:33,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:34,029][root][INFO] - Training Epoch: 1/2, step 3218/23838 completed (loss: 0.5642511248588562, acc: 0.8064516186714172)
[2025-02-16 11:21:34,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:34,490][root][INFO] - Training Epoch: 1/2, step 3219/23838 completed (loss: 1.1378839015960693, acc: 0.699999988079071)
[2025-02-16 11:21:34,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:34,957][root][INFO] - Training Epoch: 1/2, step 3220/23838 completed (loss: 1.441839337348938, acc: 0.6333333253860474)
[2025-02-16 11:21:35,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:35,361][root][INFO] - Training Epoch: 1/2, step 3221/23838 completed (loss: 1.5189342498779297, acc: 0.5897436141967773)
[2025-02-16 11:21:35,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:35,781][root][INFO] - Training Epoch: 1/2, step 3222/23838 completed (loss: 0.976267397403717, acc: 0.75)
[2025-02-16 11:21:35,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:36,218][root][INFO] - Training Epoch: 1/2, step 3223/23838 completed (loss: 1.1793787479400635, acc: 0.6153846383094788)
[2025-02-16 11:21:36,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:36,685][root][INFO] - Training Epoch: 1/2, step 3224/23838 completed (loss: 2.2291922569274902, acc: 0.4722222089767456)
[2025-02-16 11:21:36,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:37,211][root][INFO] - Training Epoch: 1/2, step 3225/23838 completed (loss: 1.000962257385254, acc: 0.7878788113594055)
[2025-02-16 11:21:37,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:37,630][root][INFO] - Training Epoch: 1/2, step 3226/23838 completed (loss: 1.3555042743682861, acc: 0.6600000262260437)
[2025-02-16 11:21:37,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:38,045][root][INFO] - Training Epoch: 1/2, step 3227/23838 completed (loss: 1.333438754081726, acc: 0.6875)
[2025-02-16 11:21:38,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:38,430][root][INFO] - Training Epoch: 1/2, step 3228/23838 completed (loss: 1.3908836841583252, acc: 0.5714285969734192)
[2025-02-16 11:21:38,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:38,828][root][INFO] - Training Epoch: 1/2, step 3229/23838 completed (loss: 1.3291926383972168, acc: 0.5909090638160706)
[2025-02-16 11:21:39,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:39,261][root][INFO] - Training Epoch: 1/2, step 3230/23838 completed (loss: 1.2615047693252563, acc: 0.644444465637207)
[2025-02-16 11:21:39,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:39,928][root][INFO] - Training Epoch: 1/2, step 3231/23838 completed (loss: 0.5550073385238647, acc: 0.8461538553237915)
[2025-02-16 11:21:40,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:40,312][root][INFO] - Training Epoch: 1/2, step 3232/23838 completed (loss: 0.819857656955719, acc: 0.7333333492279053)
[2025-02-16 11:21:40,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:40,739][root][INFO] - Training Epoch: 1/2, step 3233/23838 completed (loss: 2.242155075073242, acc: 0.3684210479259491)
[2025-02-16 11:21:40,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:41,169][root][INFO] - Training Epoch: 1/2, step 3234/23838 completed (loss: 1.2767661809921265, acc: 0.6666666865348816)
[2025-02-16 11:21:41,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:41,618][root][INFO] - Training Epoch: 1/2, step 3235/23838 completed (loss: 0.8768651485443115, acc: 0.6857143044471741)
[2025-02-16 11:21:41,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:42,005][root][INFO] - Training Epoch: 1/2, step 3236/23838 completed (loss: 1.183652639389038, acc: 0.688524603843689)
[2025-02-16 11:21:42,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:42,392][root][INFO] - Training Epoch: 1/2, step 3237/23838 completed (loss: 0.8672230839729309, acc: 0.75)
[2025-02-16 11:21:42,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:42,815][root][INFO] - Training Epoch: 1/2, step 3238/23838 completed (loss: 0.9578440189361572, acc: 0.7368420958518982)
[2025-02-16 11:21:43,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:43,271][root][INFO] - Training Epoch: 1/2, step 3239/23838 completed (loss: 1.325528860092163, acc: 0.6666666865348816)
[2025-02-16 11:21:43,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:43,695][root][INFO] - Training Epoch: 1/2, step 3240/23838 completed (loss: 1.1961365938186646, acc: 0.7142857313156128)
[2025-02-16 11:21:43,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:44,089][root][INFO] - Training Epoch: 1/2, step 3241/23838 completed (loss: 1.14473557472229, acc: 0.6296296119689941)
[2025-02-16 11:21:44,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:44,510][root][INFO] - Training Epoch: 1/2, step 3242/23838 completed (loss: 0.9469059109687805, acc: 0.7424242496490479)
[2025-02-16 11:21:44,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:44,900][root][INFO] - Training Epoch: 1/2, step 3243/23838 completed (loss: 0.7971483469009399, acc: 0.7777777910232544)
[2025-02-16 11:21:45,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:45,290][root][INFO] - Training Epoch: 1/2, step 3244/23838 completed (loss: 0.9999856948852539, acc: 0.7272727489471436)
[2025-02-16 11:21:45,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:45,743][root][INFO] - Training Epoch: 1/2, step 3245/23838 completed (loss: 0.6852464079856873, acc: 0.760869562625885)
[2025-02-16 11:21:46,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:46,237][root][INFO] - Training Epoch: 1/2, step 3246/23838 completed (loss: 1.2478177547454834, acc: 0.6415094137191772)
[2025-02-16 11:21:46,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:46,706][root][INFO] - Training Epoch: 1/2, step 3247/23838 completed (loss: 0.9966406226158142, acc: 0.7708333134651184)
[2025-02-16 11:21:46,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:47,144][root][INFO] - Training Epoch: 1/2, step 3248/23838 completed (loss: 0.9147806167602539, acc: 0.75)
[2025-02-16 11:21:47,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:47,586][root][INFO] - Training Epoch: 1/2, step 3249/23838 completed (loss: 0.9212558269500732, acc: 0.7642276287078857)
[2025-02-16 11:21:47,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:48,213][root][INFO] - Training Epoch: 1/2, step 3250/23838 completed (loss: 1.4268120527267456, acc: 0.6530612111091614)
[2025-02-16 11:21:48,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:48,653][root][INFO] - Training Epoch: 1/2, step 3251/23838 completed (loss: 0.6506956219673157, acc: 0.8666666746139526)
[2025-02-16 11:21:48,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:49,106][root][INFO] - Training Epoch: 1/2, step 3252/23838 completed (loss: 1.725480318069458, acc: 0.5319148898124695)
[2025-02-16 11:21:49,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:49,569][root][INFO] - Training Epoch: 1/2, step 3253/23838 completed (loss: 1.0637609958648682, acc: 0.7446808218955994)
[2025-02-16 11:21:49,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:50,009][root][INFO] - Training Epoch: 1/2, step 3254/23838 completed (loss: 0.9661761522293091, acc: 0.7435897588729858)
[2025-02-16 11:21:50,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:50,518][root][INFO] - Training Epoch: 1/2, step 3255/23838 completed (loss: 0.8033921718597412, acc: 0.7666666507720947)
[2025-02-16 11:21:50,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:50,945][root][INFO] - Training Epoch: 1/2, step 3256/23838 completed (loss: 0.7088419198989868, acc: 0.7749999761581421)
[2025-02-16 11:21:51,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:51,362][root][INFO] - Training Epoch: 1/2, step 3257/23838 completed (loss: 0.4841299057006836, acc: 0.8738738894462585)
[2025-02-16 11:21:51,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:51,778][root][INFO] - Training Epoch: 1/2, step 3258/23838 completed (loss: 0.520401120185852, acc: 0.9107142686843872)
[2025-02-16 11:21:51,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:52,193][root][INFO] - Training Epoch: 1/2, step 3259/23838 completed (loss: 0.7567692995071411, acc: 0.8172042965888977)
[2025-02-16 11:21:52,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:52,644][root][INFO] - Training Epoch: 1/2, step 3260/23838 completed (loss: 1.036462426185608, acc: 0.7115384340286255)
[2025-02-16 11:21:52,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:53,103][root][INFO] - Training Epoch: 1/2, step 3261/23838 completed (loss: 1.539679765701294, acc: 0.54666668176651)
[2025-02-16 11:21:53,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:53,549][root][INFO] - Training Epoch: 1/2, step 3262/23838 completed (loss: 0.9422280192375183, acc: 0.7592592835426331)
[2025-02-16 11:21:53,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:53,999][root][INFO] - Training Epoch: 1/2, step 3263/23838 completed (loss: 1.0634658336639404, acc: 0.75)
[2025-02-16 11:21:54,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:54,433][root][INFO] - Training Epoch: 1/2, step 3264/23838 completed (loss: 1.0092699527740479, acc: 0.7285714149475098)
[2025-02-16 11:21:54,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:54,885][root][INFO] - Training Epoch: 1/2, step 3265/23838 completed (loss: 0.9547528624534607, acc: 0.6883116960525513)
[2025-02-16 11:21:55,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:55,281][root][INFO] - Training Epoch: 1/2, step 3266/23838 completed (loss: 1.1751819849014282, acc: 0.6557376980781555)
[2025-02-16 11:21:55,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:55,808][root][INFO] - Training Epoch: 1/2, step 3267/23838 completed (loss: 0.941734254360199, acc: 0.7777777910232544)
[2025-02-16 11:21:55,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:56,234][root][INFO] - Training Epoch: 1/2, step 3268/23838 completed (loss: 0.4776562452316284, acc: 0.8703703880310059)
[2025-02-16 11:21:56,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:56,853][root][INFO] - Training Epoch: 1/2, step 3269/23838 completed (loss: 0.9015680551528931, acc: 0.72826087474823)
[2025-02-16 11:21:57,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:57,340][root][INFO] - Training Epoch: 1/2, step 3270/23838 completed (loss: 1.4822924137115479, acc: 0.5882353186607361)
[2025-02-16 11:21:57,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:57,919][root][INFO] - Training Epoch: 1/2, step 3271/23838 completed (loss: 1.0576120615005493, acc: 0.658823549747467)
[2025-02-16 11:21:58,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:58,358][root][INFO] - Training Epoch: 1/2, step 3272/23838 completed (loss: 0.9424031972885132, acc: 0.75)
[2025-02-16 11:21:58,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:58,903][root][INFO] - Training Epoch: 1/2, step 3273/23838 completed (loss: 0.5829187035560608, acc: 0.8380952477455139)
[2025-02-16 11:21:59,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:59,348][root][INFO] - Training Epoch: 1/2, step 3274/23838 completed (loss: 0.8141672015190125, acc: 0.8275862336158752)
[2025-02-16 11:21:59,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:21:59,787][root][INFO] - Training Epoch: 1/2, step 3275/23838 completed (loss: 1.070159673690796, acc: 0.7540983557701111)
[2025-02-16 11:21:59,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:00,172][root][INFO] - Training Epoch: 1/2, step 3276/23838 completed (loss: 0.8435594439506531, acc: 0.7551020383834839)
[2025-02-16 11:22:00,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:00,618][root][INFO] - Training Epoch: 1/2, step 3277/23838 completed (loss: 0.8960559368133545, acc: 0.7708333134651184)
[2025-02-16 11:22:00,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:01,146][root][INFO] - Training Epoch: 1/2, step 3278/23838 completed (loss: 1.1241843700408936, acc: 0.732876718044281)
[2025-02-16 11:22:01,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:01,586][root][INFO] - Training Epoch: 1/2, step 3279/23838 completed (loss: 0.8201826214790344, acc: 0.7976190447807312)
[2025-02-16 11:22:01,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:02,025][root][INFO] - Training Epoch: 1/2, step 3280/23838 completed (loss: 1.1521666049957275, acc: 0.6666666865348816)
[2025-02-16 11:22:02,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:02,454][root][INFO] - Training Epoch: 1/2, step 3281/23838 completed (loss: 0.701826274394989, acc: 0.8113207817077637)
[2025-02-16 11:22:02,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:02,839][root][INFO] - Training Epoch: 1/2, step 3282/23838 completed (loss: 0.9643149971961975, acc: 0.7288135886192322)
[2025-02-16 11:22:03,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:03,295][root][INFO] - Training Epoch: 1/2, step 3283/23838 completed (loss: 0.7424812316894531, acc: 0.7954545617103577)
[2025-02-16 11:22:03,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:03,695][root][INFO] - Training Epoch: 1/2, step 3284/23838 completed (loss: 1.3366405963897705, acc: 0.6575342416763306)
[2025-02-16 11:22:03,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:04,069][root][INFO] - Training Epoch: 1/2, step 3285/23838 completed (loss: 1.1325712203979492, acc: 0.6129032373428345)
[2025-02-16 11:22:04,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:04,465][root][INFO] - Training Epoch: 1/2, step 3286/23838 completed (loss: 1.4755882024765015, acc: 0.609375)
[2025-02-16 11:22:04,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:04,970][root][INFO] - Training Epoch: 1/2, step 3287/23838 completed (loss: 0.5017520785331726, acc: 0.895348846912384)
[2025-02-16 11:22:05,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:05,396][root][INFO] - Training Epoch: 1/2, step 3288/23838 completed (loss: 1.1894476413726807, acc: 0.650602400302887)
[2025-02-16 11:22:05,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:05,892][root][INFO] - Training Epoch: 1/2, step 3289/23838 completed (loss: 1.4861719608306885, acc: 0.6224489808082581)
[2025-02-16 11:22:06,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:06,329][root][INFO] - Training Epoch: 1/2, step 3290/23838 completed (loss: 0.6237757205963135, acc: 0.849056601524353)
[2025-02-16 11:22:06,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:06,849][root][INFO] - Training Epoch: 1/2, step 3291/23838 completed (loss: 0.33954697847366333, acc: 0.9452054500579834)
[2025-02-16 11:22:07,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:07,285][root][INFO] - Training Epoch: 1/2, step 3292/23838 completed (loss: 0.47150781750679016, acc: 0.8666666746139526)
[2025-02-16 11:22:07,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:07,728][root][INFO] - Training Epoch: 1/2, step 3293/23838 completed (loss: 0.6864410042762756, acc: 0.8571428656578064)
[2025-02-16 11:22:07,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:08,108][root][INFO] - Training Epoch: 1/2, step 3294/23838 completed (loss: 1.3594967126846313, acc: 0.6600000262260437)
[2025-02-16 11:22:08,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:08,492][root][INFO] - Training Epoch: 1/2, step 3295/23838 completed (loss: 0.6793532371520996, acc: 0.8478260636329651)
[2025-02-16 11:22:08,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:09,247][root][INFO] - Training Epoch: 1/2, step 3296/23838 completed (loss: 0.6061733961105347, acc: 0.8656716346740723)
[2025-02-16 11:22:09,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:09,661][root][INFO] - Training Epoch: 1/2, step 3297/23838 completed (loss: 0.33186981081962585, acc: 0.8823529481887817)
[2025-02-16 11:22:09,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:10,062][root][INFO] - Training Epoch: 1/2, step 3298/23838 completed (loss: 1.137452483177185, acc: 0.6551724076271057)
[2025-02-16 11:22:10,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:10,501][root][INFO] - Training Epoch: 1/2, step 3299/23838 completed (loss: 0.2923585772514343, acc: 0.8999999761581421)
[2025-02-16 11:22:10,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:10,923][root][INFO] - Training Epoch: 1/2, step 3300/23838 completed (loss: 0.14752082526683807, acc: 1.0)
[2025-02-16 11:22:11,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:11,322][root][INFO] - Training Epoch: 1/2, step 3301/23838 completed (loss: 0.529535174369812, acc: 0.84375)
[2025-02-16 11:22:11,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:11,815][root][INFO] - Training Epoch: 1/2, step 3302/23838 completed (loss: 1.1993191242218018, acc: 0.71875)
[2025-02-16 11:22:12,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:12,224][root][INFO] - Training Epoch: 1/2, step 3303/23838 completed (loss: 0.17972813546657562, acc: 0.949999988079071)
[2025-02-16 11:22:12,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:12,691][root][INFO] - Training Epoch: 1/2, step 3304/23838 completed (loss: 1.7150287628173828, acc: 0.5573770403862)
[2025-02-16 11:22:12,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:13,208][root][INFO] - Training Epoch: 1/2, step 3305/23838 completed (loss: 0.46374019980430603, acc: 0.8653846383094788)
[2025-02-16 11:22:13,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:13,663][root][INFO] - Training Epoch: 1/2, step 3306/23838 completed (loss: 0.5186894536018372, acc: 0.8571428656578064)
[2025-02-16 11:22:13,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:14,123][root][INFO] - Training Epoch: 1/2, step 3307/23838 completed (loss: 0.9151980876922607, acc: 0.6666666865348816)
[2025-02-16 11:22:14,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:14,686][root][INFO] - Training Epoch: 1/2, step 3308/23838 completed (loss: 0.8856624364852905, acc: 0.7530864477157593)
[2025-02-16 11:22:14,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:15,087][root][INFO] - Training Epoch: 1/2, step 3309/23838 completed (loss: 0.9681593775749207, acc: 0.7123287916183472)
[2025-02-16 11:22:15,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:15,460][root][INFO] - Training Epoch: 1/2, step 3310/23838 completed (loss: 0.5008352994918823, acc: 0.8157894611358643)
[2025-02-16 11:22:15,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:15,857][root][INFO] - Training Epoch: 1/2, step 3311/23838 completed (loss: 0.22501859068870544, acc: 0.931034505367279)
[2025-02-16 11:22:16,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:16,231][root][INFO] - Training Epoch: 1/2, step 3312/23838 completed (loss: 0.7341820597648621, acc: 0.8461538553237915)
[2025-02-16 11:22:16,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:16,639][root][INFO] - Training Epoch: 1/2, step 3313/23838 completed (loss: 0.29882538318634033, acc: 0.9090909361839294)
[2025-02-16 11:22:16,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:17,087][root][INFO] - Training Epoch: 1/2, step 3314/23838 completed (loss: 0.6123908162117004, acc: 0.8421052694320679)
[2025-02-16 11:22:17,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:17,557][root][INFO] - Training Epoch: 1/2, step 3315/23838 completed (loss: 0.5330165028572083, acc: 0.8636363744735718)
[2025-02-16 11:22:17,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:18,000][root][INFO] - Training Epoch: 1/2, step 3316/23838 completed (loss: 1.4433568716049194, acc: 0.6730769276618958)
[2025-02-16 11:22:18,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:18,427][root][INFO] - Training Epoch: 1/2, step 3317/23838 completed (loss: 0.559663712978363, acc: 0.862500011920929)
[2025-02-16 11:22:18,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:18,861][root][INFO] - Training Epoch: 1/2, step 3318/23838 completed (loss: 0.586898922920227, acc: 0.8181818127632141)
[2025-02-16 11:22:19,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:19,283][root][INFO] - Training Epoch: 1/2, step 3319/23838 completed (loss: 0.30961039662361145, acc: 0.9411764740943909)
[2025-02-16 11:22:19,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:19,770][root][INFO] - Training Epoch: 1/2, step 3320/23838 completed (loss: 1.218592882156372, acc: 0.6899999976158142)
[2025-02-16 11:22:19,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:20,149][root][INFO] - Training Epoch: 1/2, step 3321/23838 completed (loss: 0.44932880997657776, acc: 0.8863636255264282)
[2025-02-16 11:22:20,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:20,734][root][INFO] - Training Epoch: 1/2, step 3322/23838 completed (loss: 0.7941633462905884, acc: 0.8260869383811951)
[2025-02-16 11:22:20,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:21,222][root][INFO] - Training Epoch: 1/2, step 3323/23838 completed (loss: 0.398212730884552, acc: 0.8648648858070374)
[2025-02-16 11:22:21,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:21,653][root][INFO] - Training Epoch: 1/2, step 3324/23838 completed (loss: 0.28722083568573, acc: 0.9791666865348816)
[2025-02-16 11:22:21,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:22,023][root][INFO] - Training Epoch: 1/2, step 3325/23838 completed (loss: 0.4523705840110779, acc: 0.9259259104728699)
[2025-02-16 11:22:22,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:22,414][root][INFO] - Training Epoch: 1/2, step 3326/23838 completed (loss: 0.2712245583534241, acc: 0.8965517282485962)
[2025-02-16 11:22:22,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:22,851][root][INFO] - Training Epoch: 1/2, step 3327/23838 completed (loss: 0.2804592549800873, acc: 0.8888888955116272)
[2025-02-16 11:22:23,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:23,513][root][INFO] - Training Epoch: 1/2, step 3328/23838 completed (loss: 1.0421743392944336, acc: 0.7106918096542358)
[2025-02-16 11:22:23,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:23,963][root][INFO] - Training Epoch: 1/2, step 3329/23838 completed (loss: 0.1675184965133667, acc: 0.9387755393981934)
[2025-02-16 11:22:24,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:24,565][root][INFO] - Training Epoch: 1/2, step 3330/23838 completed (loss: 1.078830361366272, acc: 0.6842105388641357)
[2025-02-16 11:22:24,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:25,007][root][INFO] - Training Epoch: 1/2, step 3331/23838 completed (loss: 0.5873461961746216, acc: 0.8064516186714172)
[2025-02-16 11:22:25,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:25,469][root][INFO] - Training Epoch: 1/2, step 3332/23838 completed (loss: 0.5206132531166077, acc: 0.8627451062202454)
[2025-02-16 11:22:25,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:25,847][root][INFO] - Training Epoch: 1/2, step 3333/23838 completed (loss: 0.07663357257843018, acc: 1.0)
[2025-02-16 11:22:26,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:26,633][root][INFO] - Training Epoch: 1/2, step 3334/23838 completed (loss: 0.9600542187690735, acc: 0.7966101765632629)
[2025-02-16 11:22:26,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:26,996][root][INFO] - Training Epoch: 1/2, step 3335/23838 completed (loss: 0.8567206859588623, acc: 0.8536585569381714)
[2025-02-16 11:22:27,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:27,455][root][INFO] - Training Epoch: 1/2, step 3336/23838 completed (loss: 0.8913435339927673, acc: 0.7804877758026123)
[2025-02-16 11:22:27,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:27,862][root][INFO] - Training Epoch: 1/2, step 3337/23838 completed (loss: 0.43167006969451904, acc: 0.8974359035491943)
[2025-02-16 11:22:28,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:28,254][root][INFO] - Training Epoch: 1/2, step 3338/23838 completed (loss: 0.5419740676879883, acc: 0.9024389982223511)
[2025-02-16 11:22:28,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:28,642][root][INFO] - Training Epoch: 1/2, step 3339/23838 completed (loss: 0.7541796565055847, acc: 0.8571428656578064)
[2025-02-16 11:22:28,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:29,121][root][INFO] - Training Epoch: 1/2, step 3340/23838 completed (loss: 0.204129159450531, acc: 0.90625)
[2025-02-16 11:22:29,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:29,555][root][INFO] - Training Epoch: 1/2, step 3341/23838 completed (loss: 0.3462967872619629, acc: 0.8936170339584351)
[2025-02-16 11:22:29,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:30,007][root][INFO] - Training Epoch: 1/2, step 3342/23838 completed (loss: 1.0679190158843994, acc: 0.7151898741722107)
[2025-02-16 11:22:30,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:30,444][root][INFO] - Training Epoch: 1/2, step 3343/23838 completed (loss: 0.942226231098175, acc: 0.723809540271759)
[2025-02-16 11:22:30,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:30,841][root][INFO] - Training Epoch: 1/2, step 3344/23838 completed (loss: 0.4035491645336151, acc: 0.9047619104385376)
[2025-02-16 11:22:31,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:31,292][root][INFO] - Training Epoch: 1/2, step 3345/23838 completed (loss: 0.9562475681304932, acc: 0.7398374080657959)
[2025-02-16 11:22:31,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:31,718][root][INFO] - Training Epoch: 1/2, step 3346/23838 completed (loss: 0.6166749596595764, acc: 0.8125)
[2025-02-16 11:22:31,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:32,143][root][INFO] - Training Epoch: 1/2, step 3347/23838 completed (loss: 0.9965503811836243, acc: 0.774193525314331)
[2025-02-16 11:22:32,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:32,574][root][INFO] - Training Epoch: 1/2, step 3348/23838 completed (loss: 0.4110979437828064, acc: 0.8936170339584351)
[2025-02-16 11:22:32,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:33,018][root][INFO] - Training Epoch: 1/2, step 3349/23838 completed (loss: 0.9327970743179321, acc: 0.7247706651687622)
[2025-02-16 11:22:33,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:33,429][root][INFO] - Training Epoch: 1/2, step 3350/23838 completed (loss: 1.293341040611267, acc: 0.6959999799728394)
[2025-02-16 11:22:33,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:33,845][root][INFO] - Training Epoch: 1/2, step 3351/23838 completed (loss: 1.114781141281128, acc: 0.6724137663841248)
[2025-02-16 11:22:34,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:34,340][root][INFO] - Training Epoch: 1/2, step 3352/23838 completed (loss: 0.5220935940742493, acc: 0.8653846383094788)
[2025-02-16 11:22:34,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:34,729][root][INFO] - Training Epoch: 1/2, step 3353/23838 completed (loss: 0.6947302222251892, acc: 0.853210985660553)
[2025-02-16 11:22:34,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:35,160][root][INFO] - Training Epoch: 1/2, step 3354/23838 completed (loss: 1.1178377866744995, acc: 0.7162162065505981)
[2025-02-16 11:22:35,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:35,543][root][INFO] - Training Epoch: 1/2, step 3355/23838 completed (loss: 0.6772054433822632, acc: 0.8275862336158752)
[2025-02-16 11:22:35,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:35,912][root][INFO] - Training Epoch: 1/2, step 3356/23838 completed (loss: 0.9279114603996277, acc: 0.779411792755127)
[2025-02-16 11:22:36,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:36,332][root][INFO] - Training Epoch: 1/2, step 3357/23838 completed (loss: 1.0060595273971558, acc: 0.7142857313156128)
[2025-02-16 11:22:36,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:36,713][root][INFO] - Training Epoch: 1/2, step 3358/23838 completed (loss: 0.5496882796287537, acc: 0.8272727131843567)
[2025-02-16 11:22:36,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:37,123][root][INFO] - Training Epoch: 1/2, step 3359/23838 completed (loss: 0.3714022934436798, acc: 0.8731343150138855)
[2025-02-16 11:22:37,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:37,484][root][INFO] - Training Epoch: 1/2, step 3360/23838 completed (loss: 0.4034295678138733, acc: 0.8791208863258362)
[2025-02-16 11:22:37,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:37,968][root][INFO] - Training Epoch: 1/2, step 3361/23838 completed (loss: 0.418913871049881, acc: 0.8958333134651184)
[2025-02-16 11:22:38,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:38,481][root][INFO] - Training Epoch: 1/2, step 3362/23838 completed (loss: 0.4935150146484375, acc: 0.8787878751754761)
[2025-02-16 11:22:38,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:38,896][root][INFO] - Training Epoch: 1/2, step 3363/23838 completed (loss: 0.5293464064598083, acc: 0.8407079577445984)
[2025-02-16 11:22:39,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:39,329][root][INFO] - Training Epoch: 1/2, step 3364/23838 completed (loss: 0.6583380103111267, acc: 0.8275862336158752)
[2025-02-16 11:22:39,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:39,788][root][INFO] - Training Epoch: 1/2, step 3365/23838 completed (loss: 0.5961176156997681, acc: 0.8256880640983582)
[2025-02-16 11:22:40,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:40,281][root][INFO] - Training Epoch: 1/2, step 3366/23838 completed (loss: 0.5280572175979614, acc: 0.8589743375778198)
[2025-02-16 11:22:40,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:40,716][root][INFO] - Training Epoch: 1/2, step 3367/23838 completed (loss: 0.40378817915916443, acc: 0.8320610523223877)
[2025-02-16 11:22:40,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:41,172][root][INFO] - Training Epoch: 1/2, step 3368/23838 completed (loss: 0.4866292178630829, acc: 0.8703703880310059)
[2025-02-16 11:22:41,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:41,580][root][INFO] - Training Epoch: 1/2, step 3369/23838 completed (loss: 0.6432626843452454, acc: 0.7865168452262878)
[2025-02-16 11:22:41,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:42,060][root][INFO] - Training Epoch: 1/2, step 3370/23838 completed (loss: 0.4768195152282715, acc: 0.828125)
[2025-02-16 11:22:42,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:42,529][root][INFO] - Training Epoch: 1/2, step 3371/23838 completed (loss: 0.3285282254219055, acc: 0.892307698726654)
[2025-02-16 11:22:42,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:43,107][root][INFO] - Training Epoch: 1/2, step 3372/23838 completed (loss: 0.2849322259426117, acc: 0.9099099040031433)
[2025-02-16 11:22:43,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:43,487][root][INFO] - Training Epoch: 1/2, step 3373/23838 completed (loss: 0.6838531494140625, acc: 0.8113207817077637)
[2025-02-16 11:22:43,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:43,955][root][INFO] - Training Epoch: 1/2, step 3374/23838 completed (loss: 0.4042724668979645, acc: 0.8684210777282715)
[2025-02-16 11:22:44,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:44,399][root][INFO] - Training Epoch: 1/2, step 3375/23838 completed (loss: 0.6986379027366638, acc: 0.8395061492919922)
[2025-02-16 11:22:44,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:44,780][root][INFO] - Training Epoch: 1/2, step 3376/23838 completed (loss: 0.8397811651229858, acc: 0.717391312122345)
[2025-02-16 11:22:44,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:45,172][root][INFO] - Training Epoch: 1/2, step 3377/23838 completed (loss: 1.0993070602416992, acc: 0.6779661178588867)
[2025-02-16 11:22:45,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:45,619][root][INFO] - Training Epoch: 1/2, step 3378/23838 completed (loss: 0.8082221150398254, acc: 0.7605633735656738)
[2025-02-16 11:22:45,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:46,059][root][INFO] - Training Epoch: 1/2, step 3379/23838 completed (loss: 0.3470023572444916, acc: 0.9359999895095825)
[2025-02-16 11:22:46,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:46,599][root][INFO] - Training Epoch: 1/2, step 3380/23838 completed (loss: 0.11441154032945633, acc: 0.9753086566925049)
[2025-02-16 11:22:46,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:47,311][root][INFO] - Training Epoch: 1/2, step 3381/23838 completed (loss: 0.6176097989082336, acc: 0.8482142686843872)
[2025-02-16 11:22:47,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:47,828][root][INFO] - Training Epoch: 1/2, step 3382/23838 completed (loss: 0.1690744310617447, acc: 0.9670329689979553)
[2025-02-16 11:22:48,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:48,316][root][INFO] - Training Epoch: 1/2, step 3383/23838 completed (loss: 0.5857942700386047, acc: 0.8700000047683716)
[2025-02-16 11:22:48,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:48,708][root][INFO] - Training Epoch: 1/2, step 3384/23838 completed (loss: 0.4586864113807678, acc: 0.8805969953536987)
[2025-02-16 11:22:48,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:49,199][root][INFO] - Training Epoch: 1/2, step 3385/23838 completed (loss: 0.25802311301231384, acc: 0.9210526347160339)
[2025-02-16 11:22:49,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:49,960][root][INFO] - Training Epoch: 1/2, step 3386/23838 completed (loss: 0.3699796199798584, acc: 0.8999999761581421)
[2025-02-16 11:22:50,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:50,453][root][INFO] - Training Epoch: 1/2, step 3387/23838 completed (loss: 0.13124927878379822, acc: 0.9680851101875305)
[2025-02-16 11:22:50,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:51,053][root][INFO] - Training Epoch: 1/2, step 3388/23838 completed (loss: 0.2554217576980591, acc: 0.9343065619468689)
[2025-02-16 11:22:51,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:51,905][root][INFO] - Training Epoch: 1/2, step 3389/23838 completed (loss: 0.25210490822792053, acc: 0.9477611780166626)
[2025-02-16 11:22:52,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:52,364][root][INFO] - Training Epoch: 1/2, step 3390/23838 completed (loss: 0.07854745537042618, acc: 0.9898989796638489)
[2025-02-16 11:22:52,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:52,864][root][INFO] - Training Epoch: 1/2, step 3391/23838 completed (loss: 0.39950793981552124, acc: 0.8913043737411499)
[2025-02-16 11:22:53,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:53,299][root][INFO] - Training Epoch: 1/2, step 3392/23838 completed (loss: 1.310317873954773, acc: 0.6162790656089783)
[2025-02-16 11:22:53,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:54,386][root][INFO] - Training Epoch: 1/2, step 3393/23838 completed (loss: 0.7791804671287537, acc: 0.7777777910232544)
[2025-02-16 11:22:54,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:55,409][root][INFO] - Training Epoch: 1/2, step 3394/23838 completed (loss: 0.4901706576347351, acc: 0.9005848169326782)
[2025-02-16 11:22:55,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:56,098][root][INFO] - Training Epoch: 1/2, step 3395/23838 completed (loss: 0.4322386384010315, acc: 0.8928571343421936)
[2025-02-16 11:22:56,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:57,108][root][INFO] - Training Epoch: 1/2, step 3396/23838 completed (loss: 1.8117334842681885, acc: 0.6034482717514038)
[2025-02-16 11:22:57,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:57,506][root][INFO] - Training Epoch: 1/2, step 3397/23838 completed (loss: 0.5579469203948975, acc: 0.8444444537162781)
[2025-02-16 11:22:57,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:57,913][root][INFO] - Training Epoch: 1/2, step 3398/23838 completed (loss: 0.9466080665588379, acc: 0.717391312122345)
[2025-02-16 11:22:58,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:58,331][root][INFO] - Training Epoch: 1/2, step 3399/23838 completed (loss: 0.878872275352478, acc: 0.7083333134651184)
[2025-02-16 11:22:58,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:58,748][root][INFO] - Training Epoch: 1/2, step 3400/23838 completed (loss: 0.4340956509113312, acc: 0.8734177350997925)
[2025-02-16 11:22:58,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:59,151][root][INFO] - Training Epoch: 1/2, step 3401/23838 completed (loss: 0.25962868332862854, acc: 0.9074074029922485)
[2025-02-16 11:22:59,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:22:59,628][root][INFO] - Training Epoch: 1/2, step 3402/23838 completed (loss: 0.513211190700531, acc: 0.8909090757369995)
[2025-02-16 11:22:59,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:00,083][root][INFO] - Training Epoch: 1/2, step 3403/23838 completed (loss: 0.9312231540679932, acc: 0.625)
[2025-02-16 11:23:00,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:00,541][root][INFO] - Training Epoch: 1/2, step 3404/23838 completed (loss: 1.7398585081100464, acc: 0.5)
[2025-02-16 11:23:00,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:01,012][root][INFO] - Training Epoch: 1/2, step 3405/23838 completed (loss: 0.37926656007766724, acc: 0.8888888955116272)
[2025-02-16 11:23:01,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:01,399][root][INFO] - Training Epoch: 1/2, step 3406/23838 completed (loss: 0.186187744140625, acc: 1.0)
[2025-02-16 11:23:01,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:01,819][root][INFO] - Training Epoch: 1/2, step 3407/23838 completed (loss: 0.23451505601406097, acc: 0.9444444179534912)
[2025-02-16 11:23:02,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:02,248][root][INFO] - Training Epoch: 1/2, step 3408/23838 completed (loss: 1.059546947479248, acc: 0.7037037014961243)
[2025-02-16 11:23:02,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:02,688][root][INFO] - Training Epoch: 1/2, step 3409/23838 completed (loss: 0.549796462059021, acc: 0.8470588326454163)
[2025-02-16 11:23:02,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:03,096][root][INFO] - Training Epoch: 1/2, step 3410/23838 completed (loss: 0.6543627381324768, acc: 0.8333333134651184)
[2025-02-16 11:23:03,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:03,500][root][INFO] - Training Epoch: 1/2, step 3411/23838 completed (loss: 0.5790925025939941, acc: 0.9135802388191223)
[2025-02-16 11:23:03,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:03,928][root][INFO] - Training Epoch: 1/2, step 3412/23838 completed (loss: 0.7685785889625549, acc: 0.8468468189239502)
[2025-02-16 11:23:04,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:04,314][root][INFO] - Training Epoch: 1/2, step 3413/23838 completed (loss: 0.5032808780670166, acc: 0.8804348111152649)
[2025-02-16 11:23:04,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:04,726][root][INFO] - Training Epoch: 1/2, step 3414/23838 completed (loss: 0.3486924171447754, acc: 0.9253731369972229)
[2025-02-16 11:23:04,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:05,090][root][INFO] - Training Epoch: 1/2, step 3415/23838 completed (loss: 0.5614829659461975, acc: 0.8769230842590332)
[2025-02-16 11:23:05,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:05,522][root][INFO] - Training Epoch: 1/2, step 3416/23838 completed (loss: 0.291946679353714, acc: 0.9350649118423462)
[2025-02-16 11:23:05,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:05,895][root][INFO] - Training Epoch: 1/2, step 3417/23838 completed (loss: 0.4213132858276367, acc: 0.8831169009208679)
[2025-02-16 11:23:06,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:06,435][root][INFO] - Training Epoch: 1/2, step 3418/23838 completed (loss: 0.37423208355903625, acc: 0.8933333158493042)
[2025-02-16 11:23:06,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:06,802][root][INFO] - Training Epoch: 1/2, step 3419/23838 completed (loss: 0.5627902150154114, acc: 0.7714285850524902)
[2025-02-16 11:23:06,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:07,224][root][INFO] - Training Epoch: 1/2, step 3420/23838 completed (loss: 0.5159552693367004, acc: 0.8769230842590332)
[2025-02-16 11:23:07,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:07,694][root][INFO] - Training Epoch: 1/2, step 3421/23838 completed (loss: 0.9913090467453003, acc: 0.8070175647735596)
[2025-02-16 11:23:07,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:08,119][root][INFO] - Training Epoch: 1/2, step 3422/23838 completed (loss: 0.165489062666893, acc: 0.9387755393981934)
[2025-02-16 11:23:08,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:08,495][root][INFO] - Training Epoch: 1/2, step 3423/23838 completed (loss: 1.2790915966033936, acc: 0.7857142686843872)
[2025-02-16 11:23:08,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:08,878][root][INFO] - Training Epoch: 1/2, step 3424/23838 completed (loss: 0.14696019887924194, acc: 0.9767441749572754)
[2025-02-16 11:23:09,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:09,262][root][INFO] - Training Epoch: 1/2, step 3425/23838 completed (loss: 0.8594391345977783, acc: 0.7333333492279053)
[2025-02-16 11:23:09,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:09,685][root][INFO] - Training Epoch: 1/2, step 3426/23838 completed (loss: 0.28775978088378906, acc: 0.970588207244873)
[2025-02-16 11:23:09,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:10,112][root][INFO] - Training Epoch: 1/2, step 3427/23838 completed (loss: 0.5771016478538513, acc: 0.8684210777282715)
[2025-02-16 11:23:10,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:10,528][root][INFO] - Training Epoch: 1/2, step 3428/23838 completed (loss: 0.30372124910354614, acc: 0.9324324131011963)
[2025-02-16 11:23:10,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:10,897][root][INFO] - Training Epoch: 1/2, step 3429/23838 completed (loss: 0.14812733232975006, acc: 0.9610389471054077)
[2025-02-16 11:23:11,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:11,271][root][INFO] - Training Epoch: 1/2, step 3430/23838 completed (loss: 0.17392782866954803, acc: 0.9538461565971375)
[2025-02-16 11:23:11,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:11,702][root][INFO] - Training Epoch: 1/2, step 3431/23838 completed (loss: 0.24943150579929352, acc: 0.8947368264198303)
[2025-02-16 11:23:11,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:12,155][root][INFO] - Training Epoch: 1/2, step 3432/23838 completed (loss: 0.40788599848747253, acc: 0.8961039185523987)
[2025-02-16 11:23:12,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:12,600][root][INFO] - Training Epoch: 1/2, step 3433/23838 completed (loss: 0.3523751497268677, acc: 0.9150943160057068)
[2025-02-16 11:23:12,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:13,035][root][INFO] - Training Epoch: 1/2, step 3434/23838 completed (loss: 0.7908598780632019, acc: 0.7333333492279053)
[2025-02-16 11:23:13,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:13,528][root][INFO] - Training Epoch: 1/2, step 3435/23838 completed (loss: 0.8283303380012512, acc: 0.8271604776382446)
[2025-02-16 11:23:13,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:13,955][root][INFO] - Training Epoch: 1/2, step 3436/23838 completed (loss: 0.6774551868438721, acc: 0.84375)
[2025-02-16 11:23:14,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:14,385][root][INFO] - Training Epoch: 1/2, step 3437/23838 completed (loss: 0.3074287176132202, acc: 0.8823529481887817)
[2025-02-16 11:23:14,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:14,822][root][INFO] - Training Epoch: 1/2, step 3438/23838 completed (loss: 0.5743282437324524, acc: 0.8333333134651184)
[2025-02-16 11:23:14,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:15,206][root][INFO] - Training Epoch: 1/2, step 3439/23838 completed (loss: 0.22917945683002472, acc: 0.9210526347160339)
[2025-02-16 11:23:15,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:15,633][root][INFO] - Training Epoch: 1/2, step 3440/23838 completed (loss: 0.33811965584754944, acc: 0.9111111164093018)
[2025-02-16 11:23:15,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:16,067][root][INFO] - Training Epoch: 1/2, step 3441/23838 completed (loss: 0.6371380090713501, acc: 0.8727272748947144)
[2025-02-16 11:23:16,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:16,499][root][INFO] - Training Epoch: 1/2, step 3442/23838 completed (loss: 0.6023300290107727, acc: 0.8333333134651184)
[2025-02-16 11:23:16,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:16,910][root][INFO] - Training Epoch: 1/2, step 3443/23838 completed (loss: 0.3436124622821808, acc: 0.8947368264198303)
[2025-02-16 11:23:17,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:17,343][root][INFO] - Training Epoch: 1/2, step 3444/23838 completed (loss: 0.4110262095928192, acc: 0.9016393423080444)
[2025-02-16 11:23:17,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:17,742][root][INFO] - Training Epoch: 1/2, step 3445/23838 completed (loss: 0.6581475734710693, acc: 0.8500000238418579)
[2025-02-16 11:23:17,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:18,175][root][INFO] - Training Epoch: 1/2, step 3446/23838 completed (loss: 1.2672398090362549, acc: 0.7560975551605225)
[2025-02-16 11:23:18,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:18,582][root][INFO] - Training Epoch: 1/2, step 3447/23838 completed (loss: 0.7593146562576294, acc: 0.8484848737716675)
[2025-02-16 11:23:18,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:19,011][root][INFO] - Training Epoch: 1/2, step 3448/23838 completed (loss: 0.6140307188034058, acc: 0.8085106611251831)
[2025-02-16 11:23:19,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:19,401][root][INFO] - Training Epoch: 1/2, step 3449/23838 completed (loss: 0.8739925026893616, acc: 0.800000011920929)
[2025-02-16 11:23:19,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:19,769][root][INFO] - Training Epoch: 1/2, step 3450/23838 completed (loss: 1.128035545349121, acc: 0.6000000238418579)
[2025-02-16 11:23:19,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:20,202][root][INFO] - Training Epoch: 1/2, step 3451/23838 completed (loss: 0.3877592980861664, acc: 0.8709677457809448)
[2025-02-16 11:23:20,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:20,667][root][INFO] - Training Epoch: 1/2, step 3452/23838 completed (loss: 0.7280282378196716, acc: 0.7936508059501648)
[2025-02-16 11:23:20,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:21,098][root][INFO] - Training Epoch: 1/2, step 3453/23838 completed (loss: 0.9232581853866577, acc: 0.761904776096344)
[2025-02-16 11:23:21,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:21,596][root][INFO] - Training Epoch: 1/2, step 3454/23838 completed (loss: 0.8535327315330505, acc: 0.8297872543334961)
[2025-02-16 11:23:21,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:22,087][root][INFO] - Training Epoch: 1/2, step 3455/23838 completed (loss: 0.502592921257019, acc: 0.8600000143051147)
[2025-02-16 11:23:22,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:22,550][root][INFO] - Training Epoch: 1/2, step 3456/23838 completed (loss: 0.2548433244228363, acc: 0.9538461565971375)
[2025-02-16 11:23:22,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:23,013][root][INFO] - Training Epoch: 1/2, step 3457/23838 completed (loss: 0.4765807092189789, acc: 0.8461538553237915)
[2025-02-16 11:23:23,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:23,464][root][INFO] - Training Epoch: 1/2, step 3458/23838 completed (loss: 0.6511312127113342, acc: 0.8139534592628479)
[2025-02-16 11:23:23,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:23,899][root][INFO] - Training Epoch: 1/2, step 3459/23838 completed (loss: 0.43679437041282654, acc: 0.9111111164093018)
[2025-02-16 11:23:24,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:24,429][root][INFO] - Training Epoch: 1/2, step 3460/23838 completed (loss: 0.39700591564178467, acc: 0.8913043737411499)
[2025-02-16 11:23:24,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:24,809][root][INFO] - Training Epoch: 1/2, step 3461/23838 completed (loss: 0.8790188431739807, acc: 0.7777777910232544)
[2025-02-16 11:23:25,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:25,250][root][INFO] - Training Epoch: 1/2, step 3462/23838 completed (loss: 0.706107497215271, acc: 0.8214285969734192)
[2025-02-16 11:23:25,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:25,702][root][INFO] - Training Epoch: 1/2, step 3463/23838 completed (loss: 0.5060504078865051, acc: 0.8787878751754761)
[2025-02-16 11:23:25,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:26,096][root][INFO] - Training Epoch: 1/2, step 3464/23838 completed (loss: 0.33272042870521545, acc: 0.9347826242446899)
[2025-02-16 11:23:26,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:26,571][root][INFO] - Training Epoch: 1/2, step 3465/23838 completed (loss: 1.3096492290496826, acc: 0.6756756901741028)
[2025-02-16 11:23:26,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:26,998][root][INFO] - Training Epoch: 1/2, step 3466/23838 completed (loss: 0.7239000201225281, acc: 0.75)
[2025-02-16 11:23:27,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:27,400][root][INFO] - Training Epoch: 1/2, step 3467/23838 completed (loss: 0.7260732650756836, acc: 0.807692289352417)
[2025-02-16 11:23:27,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:27,824][root][INFO] - Training Epoch: 1/2, step 3468/23838 completed (loss: 0.5924369096755981, acc: 0.800000011920929)
[2025-02-16 11:23:28,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:28,562][root][INFO] - Training Epoch: 1/2, step 3469/23838 completed (loss: 0.6062811017036438, acc: 0.828125)
[2025-02-16 11:23:28,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:28,998][root][INFO] - Training Epoch: 1/2, step 3470/23838 completed (loss: 0.9315251111984253, acc: 0.7599999904632568)
[2025-02-16 11:23:29,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:29,484][root][INFO] - Training Epoch: 1/2, step 3471/23838 completed (loss: 0.6255744695663452, acc: 0.8625954389572144)
[2025-02-16 11:23:29,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:29,929][root][INFO] - Training Epoch: 1/2, step 3472/23838 completed (loss: 0.7127343416213989, acc: 0.7971014380455017)
[2025-02-16 11:23:30,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:30,385][root][INFO] - Training Epoch: 1/2, step 3473/23838 completed (loss: 0.4658050537109375, acc: 0.8550724387168884)
[2025-02-16 11:23:30,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:30,828][root][INFO] - Training Epoch: 1/2, step 3474/23838 completed (loss: 0.4058043956756592, acc: 0.9117646813392639)
[2025-02-16 11:23:31,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:31,269][root][INFO] - Training Epoch: 1/2, step 3475/23838 completed (loss: 0.808352530002594, acc: 0.8148148059844971)
[2025-02-16 11:23:31,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:31,707][root][INFO] - Training Epoch: 1/2, step 3476/23838 completed (loss: 0.4568481147289276, acc: 0.8585858345031738)
[2025-02-16 11:23:31,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:32,133][root][INFO] - Training Epoch: 1/2, step 3477/23838 completed (loss: 1.2496216297149658, acc: 0.6666666865348816)
[2025-02-16 11:23:32,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:32,592][root][INFO] - Training Epoch: 1/2, step 3478/23838 completed (loss: 0.9600479006767273, acc: 0.7903226017951965)
[2025-02-16 11:23:32,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:32,998][root][INFO] - Training Epoch: 1/2, step 3479/23838 completed (loss: 0.8847158551216125, acc: 0.78125)
[2025-02-16 11:23:33,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:33,427][root][INFO] - Training Epoch: 1/2, step 3480/23838 completed (loss: 0.10021072626113892, acc: 0.9740259647369385)
[2025-02-16 11:23:33,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:33,900][root][INFO] - Training Epoch: 1/2, step 3481/23838 completed (loss: 0.49761590361595154, acc: 0.8735632300376892)
[2025-02-16 11:23:34,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:34,317][root][INFO] - Training Epoch: 1/2, step 3482/23838 completed (loss: 0.15442131459712982, acc: 0.9642857313156128)
[2025-02-16 11:23:34,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:34,734][root][INFO] - Training Epoch: 1/2, step 3483/23838 completed (loss: 0.15507861971855164, acc: 0.9259259104728699)
[2025-02-16 11:23:34,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:35,116][root][INFO] - Training Epoch: 1/2, step 3484/23838 completed (loss: 0.7039193511009216, acc: 0.807692289352417)
[2025-02-16 11:23:35,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:35,497][root][INFO] - Training Epoch: 1/2, step 3485/23838 completed (loss: 0.10598336160182953, acc: 0.9729729890823364)
[2025-02-16 11:23:35,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:35,864][root][INFO] - Training Epoch: 1/2, step 3486/23838 completed (loss: 0.13634136319160461, acc: 0.942307710647583)
[2025-02-16 11:23:36,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:36,282][root][INFO] - Training Epoch: 1/2, step 3487/23838 completed (loss: 0.5412355065345764, acc: 0.8421052694320679)
[2025-02-16 11:23:36,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:36,642][root][INFO] - Training Epoch: 1/2, step 3488/23838 completed (loss: 0.6782781481742859, acc: 0.8125)
[2025-02-16 11:23:36,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:37,071][root][INFO] - Training Epoch: 1/2, step 3489/23838 completed (loss: 0.6335324048995972, acc: 0.807692289352417)
[2025-02-16 11:23:37,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:37,461][root][INFO] - Training Epoch: 1/2, step 3490/23838 completed (loss: 0.24988152086734772, acc: 0.9130434989929199)
[2025-02-16 11:23:37,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:37,888][root][INFO] - Training Epoch: 1/2, step 3491/23838 completed (loss: 0.2487061768770218, acc: 0.925000011920929)
[2025-02-16 11:23:38,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:38,348][root][INFO] - Training Epoch: 1/2, step 3492/23838 completed (loss: 0.2019309550523758, acc: 0.9516128897666931)
[2025-02-16 11:23:38,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:38,748][root][INFO] - Training Epoch: 1/2, step 3493/23838 completed (loss: 0.26447194814682007, acc: 0.9318181872367859)
[2025-02-16 11:23:38,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:39,130][root][INFO] - Training Epoch: 1/2, step 3494/23838 completed (loss: 0.22643622756004333, acc: 0.9318181872367859)
[2025-02-16 11:23:39,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:39,645][root][INFO] - Training Epoch: 1/2, step 3495/23838 completed (loss: 0.6166543960571289, acc: 0.8636363744735718)
[2025-02-16 11:23:39,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:40,051][root][INFO] - Training Epoch: 1/2, step 3496/23838 completed (loss: 0.25887182354927063, acc: 0.9210526347160339)
[2025-02-16 11:23:40,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:40,466][root][INFO] - Training Epoch: 1/2, step 3497/23838 completed (loss: 0.4926776885986328, acc: 0.8666666746139526)
[2025-02-16 11:23:40,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:40,843][root][INFO] - Training Epoch: 1/2, step 3498/23838 completed (loss: 0.14004044234752655, acc: 0.936170220375061)
[2025-02-16 11:23:41,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:41,272][root][INFO] - Training Epoch: 1/2, step 3499/23838 completed (loss: 1.2767997980117798, acc: 0.6904761791229248)
[2025-02-16 11:23:41,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:41,752][root][INFO] - Training Epoch: 1/2, step 3500/23838 completed (loss: 0.8195626735687256, acc: 0.875)
[2025-02-16 11:23:41,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:42,199][root][INFO] - Training Epoch: 1/2, step 3501/23838 completed (loss: 1.0232678651809692, acc: 0.8205128312110901)
[2025-02-16 11:23:42,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:42,605][root][INFO] - Training Epoch: 1/2, step 3502/23838 completed (loss: 0.373177707195282, acc: 0.8936170339584351)
[2025-02-16 11:23:42,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:43,020][root][INFO] - Training Epoch: 1/2, step 3503/23838 completed (loss: 0.5544992685317993, acc: 0.8736842274665833)
[2025-02-16 11:23:43,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:43,404][root][INFO] - Training Epoch: 1/2, step 3504/23838 completed (loss: 0.24080753326416016, acc: 0.8918918967247009)
[2025-02-16 11:23:43,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:43,778][root][INFO] - Training Epoch: 1/2, step 3505/23838 completed (loss: 0.24420881271362305, acc: 0.9047619104385376)
[2025-02-16 11:23:43,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:44,138][root][INFO] - Training Epoch: 1/2, step 3506/23838 completed (loss: 0.1450396329164505, acc: 0.9846153855323792)
[2025-02-16 11:23:44,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:44,515][root][INFO] - Training Epoch: 1/2, step 3507/23838 completed (loss: 0.6806237697601318, acc: 0.8409090638160706)
[2025-02-16 11:23:44,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:44,997][root][INFO] - Training Epoch: 1/2, step 3508/23838 completed (loss: 0.360013872385025, acc: 0.918367326259613)
[2025-02-16 11:23:45,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:45,460][root][INFO] - Training Epoch: 1/2, step 3509/23838 completed (loss: 0.27871787548065186, acc: 0.925000011920929)
[2025-02-16 11:23:45,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:45,875][root][INFO] - Training Epoch: 1/2, step 3510/23838 completed (loss: 0.30286771059036255, acc: 0.9354838728904724)
[2025-02-16 11:23:46,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:46,263][root][INFO] - Training Epoch: 1/2, step 3511/23838 completed (loss: 0.23006729781627655, acc: 0.95652174949646)
[2025-02-16 11:23:46,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:46,658][root][INFO] - Training Epoch: 1/2, step 3512/23838 completed (loss: 0.3293241262435913, acc: 0.8913043737411499)
[2025-02-16 11:23:46,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:47,075][root][INFO] - Training Epoch: 1/2, step 3513/23838 completed (loss: 0.08721747249364853, acc: 1.0)
[2025-02-16 11:23:47,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:47,509][root][INFO] - Training Epoch: 1/2, step 3514/23838 completed (loss: 0.30964595079421997, acc: 0.8888888955116272)
[2025-02-16 11:23:47,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:47,926][root][INFO] - Training Epoch: 1/2, step 3515/23838 completed (loss: 0.12442420423030853, acc: 0.9444444179534912)
[2025-02-16 11:23:48,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:48,383][root][INFO] - Training Epoch: 1/2, step 3516/23838 completed (loss: 0.701858639717102, acc: 0.8421052694320679)
[2025-02-16 11:23:48,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:48,772][root][INFO] - Training Epoch: 1/2, step 3517/23838 completed (loss: 0.213778555393219, acc: 0.920634925365448)
[2025-02-16 11:23:48,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:49,216][root][INFO] - Training Epoch: 1/2, step 3518/23838 completed (loss: 0.4491250514984131, acc: 0.8723404407501221)
[2025-02-16 11:23:49,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:49,662][root][INFO] - Training Epoch: 1/2, step 3519/23838 completed (loss: 0.03233359009027481, acc: 0.9655172228813171)
[2025-02-16 11:23:49,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:50,031][root][INFO] - Training Epoch: 1/2, step 3520/23838 completed (loss: 0.5248576998710632, acc: 0.8823529481887817)
[2025-02-16 11:23:50,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:50,488][root][INFO] - Training Epoch: 1/2, step 3521/23838 completed (loss: 0.49020540714263916, acc: 0.8793103694915771)
[2025-02-16 11:23:50,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:50,889][root][INFO] - Training Epoch: 1/2, step 3522/23838 completed (loss: 1.4168773889541626, acc: 0.6470588445663452)
[2025-02-16 11:23:51,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:51,372][root][INFO] - Training Epoch: 1/2, step 3523/23838 completed (loss: 1.2978363037109375, acc: 0.6153846383094788)
[2025-02-16 11:23:51,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:51,752][root][INFO] - Training Epoch: 1/2, step 3524/23838 completed (loss: 0.7129787802696228, acc: 0.7441860437393188)
[2025-02-16 11:23:51,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:52,151][root][INFO] - Training Epoch: 1/2, step 3525/23838 completed (loss: 0.5382053852081299, acc: 0.8225806355476379)
[2025-02-16 11:23:52,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:52,580][root][INFO] - Training Epoch: 1/2, step 3526/23838 completed (loss: 1.4016993045806885, acc: 0.5510203838348389)
[2025-02-16 11:23:52,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:53,053][root][INFO] - Training Epoch: 1/2, step 3527/23838 completed (loss: 0.7142007946968079, acc: 0.8125)
[2025-02-16 11:23:53,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:53,449][root][INFO] - Training Epoch: 1/2, step 3528/23838 completed (loss: 1.134658932685852, acc: 0.6610169410705566)
[2025-02-16 11:23:53,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:53,875][root][INFO] - Training Epoch: 1/2, step 3529/23838 completed (loss: 1.7884093523025513, acc: 0.45098039507865906)
[2025-02-16 11:23:54,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:54,327][root][INFO] - Training Epoch: 1/2, step 3530/23838 completed (loss: 1.2712433338165283, acc: 0.6625000238418579)
[2025-02-16 11:23:54,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:54,737][root][INFO] - Training Epoch: 1/2, step 3531/23838 completed (loss: 0.705754280090332, acc: 0.7307692170143127)
[2025-02-16 11:23:54,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:55,216][root][INFO] - Training Epoch: 1/2, step 3532/23838 completed (loss: 1.3200993537902832, acc: 0.6632652878761292)
[2025-02-16 11:23:55,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:55,617][root][INFO] - Training Epoch: 1/2, step 3533/23838 completed (loss: 1.4159611463546753, acc: 0.5833333134651184)
[2025-02-16 11:23:55,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:56,016][root][INFO] - Training Epoch: 1/2, step 3534/23838 completed (loss: 0.7952094674110413, acc: 0.84375)
[2025-02-16 11:23:56,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:56,465][root][INFO] - Training Epoch: 1/2, step 3535/23838 completed (loss: 1.3046478033065796, acc: 0.686274528503418)
[2025-02-16 11:23:56,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:56,835][root][INFO] - Training Epoch: 1/2, step 3536/23838 completed (loss: 1.3161699771881104, acc: 0.6176470518112183)
[2025-02-16 11:23:57,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:57,260][root][INFO] - Training Epoch: 1/2, step 3537/23838 completed (loss: 1.1719980239868164, acc: 0.7142857313156128)
[2025-02-16 11:23:57,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:57,667][root][INFO] - Training Epoch: 1/2, step 3538/23838 completed (loss: 1.455973505973816, acc: 0.5625)
[2025-02-16 11:23:57,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:58,051][root][INFO] - Training Epoch: 1/2, step 3539/23838 completed (loss: 1.210822343826294, acc: 0.625)
[2025-02-16 11:23:58,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:58,426][root][INFO] - Training Epoch: 1/2, step 3540/23838 completed (loss: 0.8850070238113403, acc: 0.7209302186965942)
[2025-02-16 11:23:58,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:58,827][root][INFO] - Training Epoch: 1/2, step 3541/23838 completed (loss: 1.2287218570709229, acc: 0.6612903475761414)
[2025-02-16 11:23:59,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:59,344][root][INFO] - Training Epoch: 1/2, step 3542/23838 completed (loss: 1.0384621620178223, acc: 0.6716417670249939)
[2025-02-16 11:23:59,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:23:59,820][root][INFO] - Training Epoch: 1/2, step 3543/23838 completed (loss: 1.2114179134368896, acc: 0.6363636255264282)
[2025-02-16 11:24:00,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:00,293][root][INFO] - Training Epoch: 1/2, step 3544/23838 completed (loss: 1.112291693687439, acc: 0.7368420958518982)
[2025-02-16 11:24:00,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:00,712][root][INFO] - Training Epoch: 1/2, step 3545/23838 completed (loss: 1.1111286878585815, acc: 0.7142857313156128)
[2025-02-16 11:24:00,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:01,144][root][INFO] - Training Epoch: 1/2, step 3546/23838 completed (loss: 0.8015966415405273, acc: 0.8055555820465088)
[2025-02-16 11:24:01,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:01,620][root][INFO] - Training Epoch: 1/2, step 3547/23838 completed (loss: 1.364566445350647, acc: 0.6470588445663452)
[2025-02-16 11:24:01,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:02,048][root][INFO] - Training Epoch: 1/2, step 3548/23838 completed (loss: 0.9985154271125793, acc: 0.6458333134651184)
[2025-02-16 11:24:02,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:02,501][root][INFO] - Training Epoch: 1/2, step 3549/23838 completed (loss: 0.46773046255111694, acc: 0.8611111044883728)
[2025-02-16 11:24:02,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:02,926][root][INFO] - Training Epoch: 1/2, step 3550/23838 completed (loss: 1.438827395439148, acc: 0.6060606241226196)
[2025-02-16 11:24:03,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:03,425][root][INFO] - Training Epoch: 1/2, step 3551/23838 completed (loss: 0.9743058681488037, acc: 0.7126436829566956)
[2025-02-16 11:24:03,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:03,774][root][INFO] - Training Epoch: 1/2, step 3552/23838 completed (loss: 1.5463417768478394, acc: 0.49056604504585266)
[2025-02-16 11:24:03,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:04,206][root][INFO] - Training Epoch: 1/2, step 3553/23838 completed (loss: 0.7536965608596802, acc: 0.7321428656578064)
[2025-02-16 11:24:04,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:04,663][root][INFO] - Training Epoch: 1/2, step 3554/23838 completed (loss: 0.8344239592552185, acc: 0.7631579041481018)
[2025-02-16 11:24:04,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:05,062][root][INFO] - Training Epoch: 1/2, step 3555/23838 completed (loss: 1.0198413133621216, acc: 0.7179487347602844)
[2025-02-16 11:24:05,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:05,497][root][INFO] - Training Epoch: 1/2, step 3556/23838 completed (loss: 0.7629063129425049, acc: 0.7837837934494019)
[2025-02-16 11:24:05,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:05,874][root][INFO] - Training Epoch: 1/2, step 3557/23838 completed (loss: 0.8383402824401855, acc: 0.7941176295280457)
[2025-02-16 11:24:06,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:06,267][root][INFO] - Training Epoch: 1/2, step 3558/23838 completed (loss: 0.4631054401397705, acc: 0.8736842274665833)
[2025-02-16 11:24:06,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:06,692][root][INFO] - Training Epoch: 1/2, step 3559/23838 completed (loss: 1.1199299097061157, acc: 0.7254902124404907)
[2025-02-16 11:24:06,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:07,092][root][INFO] - Training Epoch: 1/2, step 3560/23838 completed (loss: 1.1018459796905518, acc: 0.6600000262260437)
[2025-02-16 11:24:07,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:07,521][root][INFO] - Training Epoch: 1/2, step 3561/23838 completed (loss: 1.338257074356079, acc: 0.6341463327407837)
[2025-02-16 11:24:07,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:08,011][root][INFO] - Training Epoch: 1/2, step 3562/23838 completed (loss: 1.0414307117462158, acc: 0.7083333134651184)
[2025-02-16 11:24:08,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:08,462][root][INFO] - Training Epoch: 1/2, step 3563/23838 completed (loss: 0.9283872246742249, acc: 0.6818181872367859)
[2025-02-16 11:24:08,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:08,900][root][INFO] - Training Epoch: 1/2, step 3564/23838 completed (loss: 1.7541158199310303, acc: 0.5555555820465088)
[2025-02-16 11:24:09,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:09,321][root][INFO] - Training Epoch: 1/2, step 3565/23838 completed (loss: 0.48203566670417786, acc: 0.800000011920929)
[2025-02-16 11:24:09,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:09,812][root][INFO] - Training Epoch: 1/2, step 3566/23838 completed (loss: 1.2723536491394043, acc: 0.6935483813285828)
[2025-02-16 11:24:10,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:10,289][root][INFO] - Training Epoch: 1/2, step 3567/23838 completed (loss: 0.49322810769081116, acc: 0.8999999761581421)
[2025-02-16 11:24:10,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:10,729][root][INFO] - Training Epoch: 1/2, step 3568/23838 completed (loss: 0.6136406064033508, acc: 0.84375)
[2025-02-16 11:24:10,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:11,207][root][INFO] - Training Epoch: 1/2, step 3569/23838 completed (loss: 1.3377301692962646, acc: 0.738095223903656)
[2025-02-16 11:24:11,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:11,787][root][INFO] - Training Epoch: 1/2, step 3570/23838 completed (loss: 0.34021106362342834, acc: 0.8999999761581421)
[2025-02-16 11:24:11,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:12,176][root][INFO] - Training Epoch: 1/2, step 3571/23838 completed (loss: 0.5813738703727722, acc: 0.8448275923728943)
[2025-02-16 11:24:12,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:12,669][root][INFO] - Training Epoch: 1/2, step 3572/23838 completed (loss: 1.3724489212036133, acc: 0.6578947305679321)
[2025-02-16 11:24:12,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:13,189][root][INFO] - Training Epoch: 1/2, step 3573/23838 completed (loss: 0.7780760526657104, acc: 0.807692289352417)
[2025-02-16 11:24:13,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:13,591][root][INFO] - Training Epoch: 1/2, step 3574/23838 completed (loss: 0.6793957352638245, acc: 0.8266666531562805)
[2025-02-16 11:24:13,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:13,984][root][INFO] - Training Epoch: 1/2, step 3575/23838 completed (loss: 0.8619051575660706, acc: 0.7560975551605225)
[2025-02-16 11:24:14,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:14,364][root][INFO] - Training Epoch: 1/2, step 3576/23838 completed (loss: 0.5385249853134155, acc: 0.859375)
[2025-02-16 11:24:14,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:14,846][root][INFO] - Training Epoch: 1/2, step 3577/23838 completed (loss: 0.6272212862968445, acc: 0.8260869383811951)
[2025-02-16 11:24:15,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:15,292][root][INFO] - Training Epoch: 1/2, step 3578/23838 completed (loss: 0.7694288492202759, acc: 0.8118811845779419)
[2025-02-16 11:24:15,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:15,720][root][INFO] - Training Epoch: 1/2, step 3579/23838 completed (loss: 1.0563820600509644, acc: 0.7777777910232544)
[2025-02-16 11:24:15,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:16,160][root][INFO] - Training Epoch: 1/2, step 3580/23838 completed (loss: 0.5310083627700806, acc: 0.837837815284729)
[2025-02-16 11:24:16,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:16,598][root][INFO] - Training Epoch: 1/2, step 3581/23838 completed (loss: 0.6012241244316101, acc: 0.8125)
[2025-02-16 11:24:16,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:17,035][root][INFO] - Training Epoch: 1/2, step 3582/23838 completed (loss: 0.6062108874320984, acc: 0.800000011920929)
[2025-02-16 11:24:17,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:17,463][root][INFO] - Training Epoch: 1/2, step 3583/23838 completed (loss: 0.7689120769500732, acc: 0.7241379022598267)
[2025-02-16 11:24:17,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:17,935][root][INFO] - Training Epoch: 1/2, step 3584/23838 completed (loss: 0.5226507782936096, acc: 0.8677685856819153)
[2025-02-16 11:24:18,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:18,460][root][INFO] - Training Epoch: 1/2, step 3585/23838 completed (loss: 0.42104414105415344, acc: 0.8799999952316284)
[2025-02-16 11:24:18,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:18,922][root][INFO] - Training Epoch: 1/2, step 3586/23838 completed (loss: 0.29209262132644653, acc: 0.9193548560142517)
[2025-02-16 11:24:19,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:19,530][root][INFO] - Training Epoch: 1/2, step 3587/23838 completed (loss: 0.7212129831314087, acc: 0.8128654956817627)
[2025-02-16 11:24:19,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:20,009][root][INFO] - Training Epoch: 1/2, step 3588/23838 completed (loss: 0.5328029990196228, acc: 0.8620689511299133)
[2025-02-16 11:24:20,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:20,477][root][INFO] - Training Epoch: 1/2, step 3589/23838 completed (loss: 0.4774029850959778, acc: 0.8493150472640991)
[2025-02-16 11:24:20,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:20,926][root][INFO] - Training Epoch: 1/2, step 3590/23838 completed (loss: 0.5520655512809753, acc: 0.837837815284729)
[2025-02-16 11:24:21,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:21,355][root][INFO] - Training Epoch: 1/2, step 3591/23838 completed (loss: 0.5453616380691528, acc: 0.8144329786300659)
[2025-02-16 11:24:21,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:21,836][root][INFO] - Training Epoch: 1/2, step 3592/23838 completed (loss: 0.49840259552001953, acc: 0.8732394576072693)
[2025-02-16 11:24:22,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:22,274][root][INFO] - Training Epoch: 1/2, step 3593/23838 completed (loss: 0.7582999467849731, acc: 0.8363636136054993)
[2025-02-16 11:24:22,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:22,658][root][INFO] - Training Epoch: 1/2, step 3594/23838 completed (loss: 0.665353000164032, acc: 0.7941176295280457)
[2025-02-16 11:24:22,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:23,102][root][INFO] - Training Epoch: 1/2, step 3595/23838 completed (loss: 0.6042888164520264, acc: 0.8282828330993652)
[2025-02-16 11:24:23,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:23,535][root][INFO] - Training Epoch: 1/2, step 3596/23838 completed (loss: 0.4980371296405792, acc: 0.9230769276618958)
[2025-02-16 11:24:23,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:23,952][root][INFO] - Training Epoch: 1/2, step 3597/23838 completed (loss: 0.6424780488014221, acc: 0.8522727489471436)
[2025-02-16 11:24:24,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:24,395][root][INFO] - Training Epoch: 1/2, step 3598/23838 completed (loss: 0.4954039752483368, acc: 0.8571428656578064)
[2025-02-16 11:24:24,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:24,841][root][INFO] - Training Epoch: 1/2, step 3599/23838 completed (loss: 0.7373683452606201, acc: 0.8455284833908081)
[2025-02-16 11:24:24,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:25,233][root][INFO] - Training Epoch: 1/2, step 3600/23838 completed (loss: 0.20665761828422546, acc: 0.9154929518699646)
[2025-02-16 11:24:25,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:25,650][root][INFO] - Training Epoch: 1/2, step 3601/23838 completed (loss: 0.22292079031467438, acc: 0.9275362491607666)
[2025-02-16 11:24:25,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:26,264][root][INFO] - Training Epoch: 1/2, step 3602/23838 completed (loss: 0.3058450222015381, acc: 0.9268292784690857)
[2025-02-16 11:24:26,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:26,786][root][INFO] - Training Epoch: 1/2, step 3603/23838 completed (loss: 0.36694636940956116, acc: 0.8724831938743591)
[2025-02-16 11:24:27,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:27,323][root][INFO] - Training Epoch: 1/2, step 3604/23838 completed (loss: 0.41009044647216797, acc: 0.8914728760719299)
[2025-02-16 11:24:27,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:27,716][root][INFO] - Training Epoch: 1/2, step 3605/23838 completed (loss: 1.2340556383132935, acc: 0.6814159154891968)
[2025-02-16 11:24:27,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:28,151][root][INFO] - Training Epoch: 1/2, step 3606/23838 completed (loss: 0.3574020266532898, acc: 0.8899999856948853)
[2025-02-16 11:24:28,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:28,637][root][INFO] - Training Epoch: 1/2, step 3607/23838 completed (loss: 0.35214969515800476, acc: 0.8846153616905212)
[2025-02-16 11:24:28,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:29,097][root][INFO] - Training Epoch: 1/2, step 3608/23838 completed (loss: 0.32444870471954346, acc: 0.9195402264595032)
[2025-02-16 11:24:29,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:29,573][root][INFO] - Training Epoch: 1/2, step 3609/23838 completed (loss: 0.24150517582893372, acc: 0.949367105960846)
[2025-02-16 11:24:29,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:29,976][root][INFO] - Training Epoch: 1/2, step 3610/23838 completed (loss: 0.3053448796272278, acc: 0.9242424368858337)
[2025-02-16 11:24:30,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:30,404][root][INFO] - Training Epoch: 1/2, step 3611/23838 completed (loss: 0.07775533199310303, acc: 1.0)
[2025-02-16 11:24:30,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:30,859][root][INFO] - Training Epoch: 1/2, step 3612/23838 completed (loss: 0.763081431388855, acc: 0.7837837934494019)
[2025-02-16 11:24:31,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:31,292][root][INFO] - Training Epoch: 1/2, step 3613/23838 completed (loss: 0.5481145977973938, acc: 0.859649121761322)
[2025-02-16 11:24:31,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:31,741][root][INFO] - Training Epoch: 1/2, step 3614/23838 completed (loss: 0.8259970545768738, acc: 0.7702702879905701)
[2025-02-16 11:24:31,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:32,147][root][INFO] - Training Epoch: 1/2, step 3615/23838 completed (loss: 0.7617589831352234, acc: 0.7954545617103577)
[2025-02-16 11:24:32,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:32,537][root][INFO] - Training Epoch: 1/2, step 3616/23838 completed (loss: 0.6974613070487976, acc: 0.7906976938247681)
[2025-02-16 11:24:32,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:32,979][root][INFO] - Training Epoch: 1/2, step 3617/23838 completed (loss: 0.08715079724788666, acc: 1.0)
[2025-02-16 11:24:33,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:33,427][root][INFO] - Training Epoch: 1/2, step 3618/23838 completed (loss: 0.6517561674118042, acc: 0.800000011920929)
[2025-02-16 11:24:33,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:34,308][root][INFO] - Training Epoch: 1/2, step 3619/23838 completed (loss: 0.6510159373283386, acc: 0.8315789699554443)
[2025-02-16 11:24:34,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:34,681][root][INFO] - Training Epoch: 1/2, step 3620/23838 completed (loss: 0.08009743690490723, acc: 0.9696969985961914)
[2025-02-16 11:24:34,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:35,100][root][INFO] - Training Epoch: 1/2, step 3621/23838 completed (loss: 0.3449939489364624, acc: 0.8852459192276001)
[2025-02-16 11:24:35,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:35,497][root][INFO] - Training Epoch: 1/2, step 3622/23838 completed (loss: 0.178613543510437, acc: 0.9599999785423279)
[2025-02-16 11:24:35,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:35,915][root][INFO] - Training Epoch: 1/2, step 3623/23838 completed (loss: 1.5598787069320679, acc: 0.5428571701049805)
[2025-02-16 11:24:36,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:36,332][root][INFO] - Training Epoch: 1/2, step 3624/23838 completed (loss: 1.099960207939148, acc: 0.707317054271698)
[2025-02-16 11:24:36,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:36,754][root][INFO] - Training Epoch: 1/2, step 3625/23838 completed (loss: 0.1752886027097702, acc: 0.9545454382896423)
[2025-02-16 11:24:36,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:37,230][root][INFO] - Training Epoch: 1/2, step 3626/23838 completed (loss: 0.43848636746406555, acc: 0.8703703880310059)
[2025-02-16 11:24:37,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:37,678][root][INFO] - Training Epoch: 1/2, step 3627/23838 completed (loss: 0.2825709581375122, acc: 0.8604651093482971)
[2025-02-16 11:24:37,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:38,129][root][INFO] - Training Epoch: 1/2, step 3628/23838 completed (loss: 0.7833839058876038, acc: 0.7872340679168701)
[2025-02-16 11:24:38,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:38,517][root][INFO] - Training Epoch: 1/2, step 3629/23838 completed (loss: 1.01101553440094, acc: 0.800000011920929)
[2025-02-16 11:24:38,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:38,958][root][INFO] - Training Epoch: 1/2, step 3630/23838 completed (loss: 0.174485445022583, acc: 1.0)
[2025-02-16 11:24:39,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:39,435][root][INFO] - Training Epoch: 1/2, step 3631/23838 completed (loss: 0.4583683907985687, acc: 0.7894737124443054)
[2025-02-16 11:24:39,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:39,885][root][INFO] - Training Epoch: 1/2, step 3632/23838 completed (loss: 0.08244805037975311, acc: 1.0)
[2025-02-16 11:24:40,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:40,298][root][INFO] - Training Epoch: 1/2, step 3633/23838 completed (loss: 0.8013768792152405, acc: 0.8181818127632141)
[2025-02-16 11:24:40,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:40,735][root][INFO] - Training Epoch: 1/2, step 3634/23838 completed (loss: 0.17876297235488892, acc: 0.9444444179534912)
[2025-02-16 11:24:40,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:41,164][root][INFO] - Training Epoch: 1/2, step 3635/23838 completed (loss: 0.294690877199173, acc: 0.9245283007621765)
[2025-02-16 11:24:41,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:41,546][root][INFO] - Training Epoch: 1/2, step 3636/23838 completed (loss: 1.1295005083084106, acc: 0.7333333492279053)
[2025-02-16 11:24:41,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:42,077][root][INFO] - Training Epoch: 1/2, step 3637/23838 completed (loss: 0.12949247658252716, acc: 0.9811320900917053)
[2025-02-16 11:24:42,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:42,549][root][INFO] - Training Epoch: 1/2, step 3638/23838 completed (loss: 0.9911275506019592, acc: 0.75)
[2025-02-16 11:24:42,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:42,955][root][INFO] - Training Epoch: 1/2, step 3639/23838 completed (loss: 0.38176000118255615, acc: 0.8823529481887817)
[2025-02-16 11:24:43,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:43,724][root][INFO] - Training Epoch: 1/2, step 3640/23838 completed (loss: 0.5197271704673767, acc: 0.8873239159584045)
[2025-02-16 11:24:43,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:44,133][root][INFO] - Training Epoch: 1/2, step 3641/23838 completed (loss: 1.4086612462997437, acc: 0.5833333134651184)
[2025-02-16 11:24:44,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:44,738][root][INFO] - Training Epoch: 1/2, step 3642/23838 completed (loss: 1.3003180027008057, acc: 0.65625)
[2025-02-16 11:24:44,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:45,238][root][INFO] - Training Epoch: 1/2, step 3643/23838 completed (loss: 0.557176947593689, acc: 0.849056601524353)
[2025-02-16 11:24:45,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:45,711][root][INFO] - Training Epoch: 1/2, step 3644/23838 completed (loss: 0.8988476991653442, acc: 0.7777777910232544)
[2025-02-16 11:24:45,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:46,186][root][INFO] - Training Epoch: 1/2, step 3645/23838 completed (loss: 0.6519149541854858, acc: 0.8245614171028137)
[2025-02-16 11:24:46,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:46,688][root][INFO] - Training Epoch: 1/2, step 3646/23838 completed (loss: 0.40339526534080505, acc: 0.9032257795333862)
[2025-02-16 11:24:46,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:47,291][root][INFO] - Training Epoch: 1/2, step 3647/23838 completed (loss: 0.4984842538833618, acc: 0.8666666746139526)
[2025-02-16 11:24:47,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:47,999][root][INFO] - Training Epoch: 1/2, step 3648/23838 completed (loss: 0.5381274819374084, acc: 0.8873239159584045)
[2025-02-16 11:24:48,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:48,444][root][INFO] - Training Epoch: 1/2, step 3649/23838 completed (loss: 0.08617479354143143, acc: 1.0)
[2025-02-16 11:24:48,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:48,911][root][INFO] - Training Epoch: 1/2, step 3650/23838 completed (loss: 0.4693259298801422, acc: 0.8928571343421936)
[2025-02-16 11:24:49,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:49,353][root][INFO] - Training Epoch: 1/2, step 3651/23838 completed (loss: 1.1603147983551025, acc: 0.7222222089767456)
[2025-02-16 11:24:49,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:49,856][root][INFO] - Training Epoch: 1/2, step 3652/23838 completed (loss: 0.2611321806907654, acc: 0.8888888955116272)
[2025-02-16 11:24:50,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:50,277][root][INFO] - Training Epoch: 1/2, step 3653/23838 completed (loss: 1.3982607126235962, acc: 0.6521739363670349)
[2025-02-16 11:24:50,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:50,738][root][INFO] - Training Epoch: 1/2, step 3654/23838 completed (loss: 0.7849113941192627, acc: 0.7435897588729858)
[2025-02-16 11:24:50,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:51,130][root][INFO] - Training Epoch: 1/2, step 3655/23838 completed (loss: 0.9865886569023132, acc: 0.7368420958518982)
[2025-02-16 11:24:51,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:51,589][root][INFO] - Training Epoch: 1/2, step 3656/23838 completed (loss: 0.807830810546875, acc: 0.7674418687820435)
[2025-02-16 11:24:51,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:52,007][root][INFO] - Training Epoch: 1/2, step 3657/23838 completed (loss: 0.6914505362510681, acc: 0.7843137383460999)
[2025-02-16 11:24:52,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:52,370][root][INFO] - Training Epoch: 1/2, step 3658/23838 completed (loss: 1.180344820022583, acc: 0.6551724076271057)
[2025-02-16 11:24:52,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:52,801][root][INFO] - Training Epoch: 1/2, step 3659/23838 completed (loss: 0.4808286726474762, acc: 0.8571428656578064)
[2025-02-16 11:24:52,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:53,227][root][INFO] - Training Epoch: 1/2, step 3660/23838 completed (loss: 0.38399219512939453, acc: 0.9268292784690857)
[2025-02-16 11:24:53,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:53,600][root][INFO] - Training Epoch: 1/2, step 3661/23838 completed (loss: 0.9091644883155823, acc: 0.7916666865348816)
[2025-02-16 11:24:53,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:53,956][root][INFO] - Training Epoch: 1/2, step 3662/23838 completed (loss: 0.299121230840683, acc: 0.9555555582046509)
[2025-02-16 11:24:54,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:54,369][root][INFO] - Training Epoch: 1/2, step 3663/23838 completed (loss: 0.7134064435958862, acc: 0.8095238208770752)
[2025-02-16 11:24:54,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:54,802][root][INFO] - Training Epoch: 1/2, step 3664/23838 completed (loss: 0.4555748403072357, acc: 0.75)
[2025-02-16 11:24:55,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:55,240][root][INFO] - Training Epoch: 1/2, step 3665/23838 completed (loss: 0.8437042236328125, acc: 0.707317054271698)
[2025-02-16 11:24:55,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:55,720][root][INFO] - Training Epoch: 1/2, step 3666/23838 completed (loss: 0.424240380525589, acc: 0.8999999761581421)
[2025-02-16 11:24:55,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:56,205][root][INFO] - Training Epoch: 1/2, step 3667/23838 completed (loss: 1.2663414478302002, acc: 0.6744186282157898)
[2025-02-16 11:24:56,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:56,578][root][INFO] - Training Epoch: 1/2, step 3668/23838 completed (loss: 0.5704347491264343, acc: 0.8275862336158752)
[2025-02-16 11:24:56,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:57,052][root][INFO] - Training Epoch: 1/2, step 3669/23838 completed (loss: 1.8810524940490723, acc: 0.5714285969734192)
[2025-02-16 11:24:57,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:57,465][root][INFO] - Training Epoch: 1/2, step 3670/23838 completed (loss: 0.6551049947738647, acc: 0.7941176295280457)
[2025-02-16 11:24:57,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:57,871][root][INFO] - Training Epoch: 1/2, step 3671/23838 completed (loss: 0.3906494081020355, acc: 0.9113923907279968)
[2025-02-16 11:24:58,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:58,326][root][INFO] - Training Epoch: 1/2, step 3672/23838 completed (loss: 0.6189563870429993, acc: 0.8333333134651184)
[2025-02-16 11:24:58,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:59,003][root][INFO] - Training Epoch: 1/2, step 3673/23838 completed (loss: 0.8231709003448486, acc: 0.7846153974533081)
[2025-02-16 11:24:59,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:24:59,440][root][INFO] - Training Epoch: 1/2, step 3674/23838 completed (loss: 1.6325953006744385, acc: 0.604651153087616)
[2025-02-16 11:24:59,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:00,208][root][INFO] - Training Epoch: 1/2, step 3675/23838 completed (loss: 0.5333207249641418, acc: 0.7848101258277893)
[2025-02-16 11:25:00,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:00,669][root][INFO] - Training Epoch: 1/2, step 3676/23838 completed (loss: 0.6144879460334778, acc: 0.8444444537162781)
[2025-02-16 11:25:00,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:01,149][root][INFO] - Training Epoch: 1/2, step 3677/23838 completed (loss: 0.1725848764181137, acc: 0.9555555582046509)
[2025-02-16 11:25:01,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:01,757][root][INFO] - Training Epoch: 1/2, step 3678/23838 completed (loss: 0.532532274723053, acc: 0.8421052694320679)
[2025-02-16 11:25:02,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:02,315][root][INFO] - Training Epoch: 1/2, step 3679/23838 completed (loss: 0.5921157002449036, acc: 0.8269230723381042)
[2025-02-16 11:25:02,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:02,796][root][INFO] - Training Epoch: 1/2, step 3680/23838 completed (loss: 0.41690078377723694, acc: 0.8727272748947144)
[2025-02-16 11:25:03,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:03,381][root][INFO] - Training Epoch: 1/2, step 3681/23838 completed (loss: 0.7279763221740723, acc: 0.8265306353569031)
[2025-02-16 11:25:03,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:03,769][root][INFO] - Training Epoch: 1/2, step 3682/23838 completed (loss: 0.3147565722465515, acc: 0.921875)
[2025-02-16 11:25:03,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:04,158][root][INFO] - Training Epoch: 1/2, step 3683/23838 completed (loss: 0.1603812277317047, acc: 0.9615384340286255)
[2025-02-16 11:25:04,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:04,631][root][INFO] - Training Epoch: 1/2, step 3684/23838 completed (loss: 0.46475648880004883, acc: 0.8717948794364929)
[2025-02-16 11:25:04,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:05,146][root][INFO] - Training Epoch: 1/2, step 3685/23838 completed (loss: 0.7227897047996521, acc: 0.774193525314331)
[2025-02-16 11:25:05,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:05,818][root][INFO] - Training Epoch: 1/2, step 3686/23838 completed (loss: 0.7766244411468506, acc: 0.7900000214576721)
[2025-02-16 11:25:06,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:06,345][root][INFO] - Training Epoch: 1/2, step 3687/23838 completed (loss: 0.44149789214134216, acc: 0.8545454740524292)
[2025-02-16 11:25:06,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:06,785][root][INFO] - Training Epoch: 1/2, step 3688/23838 completed (loss: 0.6931446194648743, acc: 0.7848101258277893)
[2025-02-16 11:25:06,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:07,217][root][INFO] - Training Epoch: 1/2, step 3689/23838 completed (loss: 0.49835872650146484, acc: 0.8653846383094788)
[2025-02-16 11:25:07,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:07,700][root][INFO] - Training Epoch: 1/2, step 3690/23838 completed (loss: 0.6935754418373108, acc: 0.7837837934494019)
[2025-02-16 11:25:07,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:08,235][root][INFO] - Training Epoch: 1/2, step 3691/23838 completed (loss: 0.8784788250923157, acc: 0.7599999904632568)
[2025-02-16 11:25:08,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:08,712][root][INFO] - Training Epoch: 1/2, step 3692/23838 completed (loss: 0.4157825708389282, acc: 0.9069767594337463)
[2025-02-16 11:25:08,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:09,264][root][INFO] - Training Epoch: 1/2, step 3693/23838 completed (loss: 0.48729079961776733, acc: 0.8761904835700989)
[2025-02-16 11:25:09,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:09,852][root][INFO] - Training Epoch: 1/2, step 3694/23838 completed (loss: 0.29475724697113037, acc: 0.9268292784690857)
[2025-02-16 11:25:10,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:10,340][root][INFO] - Training Epoch: 1/2, step 3695/23838 completed (loss: 0.2929512560367584, acc: 0.9178082346916199)
[2025-02-16 11:25:10,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:10,881][root][INFO] - Training Epoch: 1/2, step 3696/23838 completed (loss: 0.31510597467422485, acc: 0.8813559412956238)
[2025-02-16 11:25:11,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:11,365][root][INFO] - Training Epoch: 1/2, step 3697/23838 completed (loss: 0.6567385792732239, acc: 0.8285714387893677)
[2025-02-16 11:25:11,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:12,333][root][INFO] - Training Epoch: 1/2, step 3698/23838 completed (loss: 0.41665199398994446, acc: 0.8947368264198303)
[2025-02-16 11:25:12,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:12,843][root][INFO] - Training Epoch: 1/2, step 3699/23838 completed (loss: 0.2300836592912674, acc: 0.9264705777168274)
[2025-02-16 11:25:13,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:13,555][root][INFO] - Training Epoch: 1/2, step 3700/23838 completed (loss: 0.2592693567276001, acc: 0.9146341681480408)
[2025-02-16 11:25:13,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:14,306][root][INFO] - Training Epoch: 1/2, step 3701/23838 completed (loss: 0.5130576491355896, acc: 0.837837815284729)
[2025-02-16 11:25:14,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:15,383][root][INFO] - Training Epoch: 1/2, step 3702/23838 completed (loss: 0.28843188285827637, acc: 0.9142857193946838)
[2025-02-16 11:25:15,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:15,863][root][INFO] - Training Epoch: 1/2, step 3703/23838 completed (loss: 0.3549776077270508, acc: 0.8958333134651184)
[2025-02-16 11:25:16,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:16,329][root][INFO] - Training Epoch: 1/2, step 3704/23838 completed (loss: 0.9688568115234375, acc: 0.7843137383460999)
[2025-02-16 11:25:16,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:16,753][root][INFO] - Training Epoch: 1/2, step 3705/23838 completed (loss: 0.23707252740859985, acc: 0.9305555820465088)
[2025-02-16 11:25:16,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:17,220][root][INFO] - Training Epoch: 1/2, step 3706/23838 completed (loss: 1.2044695615768433, acc: 0.7291666865348816)
[2025-02-16 11:25:17,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:17,700][root][INFO] - Training Epoch: 1/2, step 3707/23838 completed (loss: 0.5420029759407043, acc: 0.8961039185523987)
[2025-02-16 11:25:17,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:18,064][root][INFO] - Training Epoch: 1/2, step 3708/23838 completed (loss: 0.7060595750808716, acc: 0.8316831588745117)
[2025-02-16 11:25:18,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:18,471][root][INFO] - Training Epoch: 1/2, step 3709/23838 completed (loss: 0.9832577109336853, acc: 0.7480915784835815)
[2025-02-16 11:25:18,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:18,947][root][INFO] - Training Epoch: 1/2, step 3710/23838 completed (loss: 0.5391950011253357, acc: 0.8548387289047241)
[2025-02-16 11:25:19,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:19,325][root][INFO] - Training Epoch: 1/2, step 3711/23838 completed (loss: 1.1263396739959717, acc: 0.6800000071525574)
[2025-02-16 11:25:19,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:19,793][root][INFO] - Training Epoch: 1/2, step 3712/23838 completed (loss: 0.9594815969467163, acc: 0.7422680258750916)
[2025-02-16 11:25:19,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:20,167][root][INFO] - Training Epoch: 1/2, step 3713/23838 completed (loss: 1.1940542459487915, acc: 0.6212121248245239)
[2025-02-16 11:25:20,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:20,559][root][INFO] - Training Epoch: 1/2, step 3714/23838 completed (loss: 1.0532379150390625, acc: 0.7310344576835632)
[2025-02-16 11:25:20,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:21,005][root][INFO] - Training Epoch: 1/2, step 3715/23838 completed (loss: 0.6288838386535645, acc: 0.8333333134651184)
[2025-02-16 11:25:21,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:21,458][root][INFO] - Training Epoch: 1/2, step 3716/23838 completed (loss: 0.7650810480117798, acc: 0.8192771077156067)
[2025-02-16 11:25:21,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:21,876][root][INFO] - Training Epoch: 1/2, step 3717/23838 completed (loss: 0.49455222487449646, acc: 0.8999999761581421)
[2025-02-16 11:25:22,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:22,312][root][INFO] - Training Epoch: 1/2, step 3718/23838 completed (loss: 0.728522002696991, acc: 0.75)
[2025-02-16 11:25:22,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:22,737][root][INFO] - Training Epoch: 1/2, step 3719/23838 completed (loss: 0.3610757887363434, acc: 0.8799999952316284)
[2025-02-16 11:25:22,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:23,174][root][INFO] - Training Epoch: 1/2, step 3720/23838 completed (loss: 0.23787568509578705, acc: 0.9322034120559692)
[2025-02-16 11:25:23,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:23,604][root][INFO] - Training Epoch: 1/2, step 3721/23838 completed (loss: 0.4203353226184845, acc: 0.891566276550293)
[2025-02-16 11:25:23,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:24,010][root][INFO] - Training Epoch: 1/2, step 3722/23838 completed (loss: 0.6708972454071045, acc: 0.8216560482978821)
[2025-02-16 11:25:24,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:24,433][root][INFO] - Training Epoch: 1/2, step 3723/23838 completed (loss: 0.38063034415245056, acc: 0.8823529481887817)
[2025-02-16 11:25:24,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:24,862][root][INFO] - Training Epoch: 1/2, step 3724/23838 completed (loss: 0.8662738800048828, acc: 0.7407407164573669)
[2025-02-16 11:25:25,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:25,376][root][INFO] - Training Epoch: 1/2, step 3725/23838 completed (loss: 0.7156962156295776, acc: 0.8064516186714172)
[2025-02-16 11:25:25,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:25,815][root][INFO] - Training Epoch: 1/2, step 3726/23838 completed (loss: 0.7619204521179199, acc: 0.8367347121238708)
[2025-02-16 11:25:25,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:26,235][root][INFO] - Training Epoch: 1/2, step 3727/23838 completed (loss: 0.48307567834854126, acc: 0.875)
[2025-02-16 11:25:26,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:26,643][root][INFO] - Training Epoch: 1/2, step 3728/23838 completed (loss: 0.7400014400482178, acc: 0.7528089880943298)
[2025-02-16 11:25:26,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:27,043][root][INFO] - Training Epoch: 1/2, step 3729/23838 completed (loss: 0.7609338164329529, acc: 0.7972972989082336)
[2025-02-16 11:25:27,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:27,509][root][INFO] - Training Epoch: 1/2, step 3730/23838 completed (loss: 0.4463525712490082, acc: 0.8602150678634644)
[2025-02-16 11:25:27,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:27,900][root][INFO] - Training Epoch: 1/2, step 3731/23838 completed (loss: 0.5251051187515259, acc: 0.8271604776382446)
[2025-02-16 11:25:28,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:28,295][root][INFO] - Training Epoch: 1/2, step 3732/23838 completed (loss: 0.6739075183868408, acc: 0.7605633735656738)
[2025-02-16 11:25:28,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:28,739][root][INFO] - Training Epoch: 1/2, step 3733/23838 completed (loss: 0.4190603196620941, acc: 0.9027777910232544)
[2025-02-16 11:25:28,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:29,138][root][INFO] - Training Epoch: 1/2, step 3734/23838 completed (loss: 0.7912048697471619, acc: 0.737864077091217)
[2025-02-16 11:25:29,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:29,509][root][INFO] - Training Epoch: 1/2, step 3735/23838 completed (loss: 1.3502862453460693, acc: 0.6557376980781555)
[2025-02-16 11:25:29,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:29,929][root][INFO] - Training Epoch: 1/2, step 3736/23838 completed (loss: 0.7232431173324585, acc: 0.8656716346740723)
[2025-02-16 11:25:30,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:30,335][root][INFO] - Training Epoch: 1/2, step 3737/23838 completed (loss: 0.9255132675170898, acc: 0.6966292262077332)
[2025-02-16 11:25:30,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:30,768][root][INFO] - Training Epoch: 1/2, step 3738/23838 completed (loss: 0.9364386796951294, acc: 0.800000011920929)
[2025-02-16 11:25:30,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:31,231][root][INFO] - Training Epoch: 1/2, step 3739/23838 completed (loss: 0.30735793709754944, acc: 0.875)
[2025-02-16 11:25:31,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:31,678][root][INFO] - Training Epoch: 1/2, step 3740/23838 completed (loss: 0.4178818166255951, acc: 0.9107142686843872)
[2025-02-16 11:25:31,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:32,089][root][INFO] - Training Epoch: 1/2, step 3741/23838 completed (loss: 0.45370444655418396, acc: 0.8730158805847168)
[2025-02-16 11:25:32,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:32,525][root][INFO] - Training Epoch: 1/2, step 3742/23838 completed (loss: 0.633015513420105, acc: 0.8461538553237915)
[2025-02-16 11:25:32,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:32,927][root][INFO] - Training Epoch: 1/2, step 3743/23838 completed (loss: 0.5056053996086121, acc: 0.8636363744735718)
[2025-02-16 11:25:33,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:33,322][root][INFO] - Training Epoch: 1/2, step 3744/23838 completed (loss: 1.273872971534729, acc: 0.6779661178588867)
[2025-02-16 11:25:33,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:33,763][root][INFO] - Training Epoch: 1/2, step 3745/23838 completed (loss: 0.6302565336227417, acc: 0.7857142686843872)
[2025-02-16 11:25:33,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:34,173][root][INFO] - Training Epoch: 1/2, step 3746/23838 completed (loss: 0.3414325416088104, acc: 0.9399999976158142)
[2025-02-16 11:25:34,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:34,610][root][INFO] - Training Epoch: 1/2, step 3747/23838 completed (loss: 0.6382396221160889, acc: 0.8514851331710815)
[2025-02-16 11:25:34,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:35,064][root][INFO] - Training Epoch: 1/2, step 3748/23838 completed (loss: 1.1087857484817505, acc: 0.7042253613471985)
[2025-02-16 11:25:35,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:35,491][root][INFO] - Training Epoch: 1/2, step 3749/23838 completed (loss: 0.5587418079376221, acc: 0.8382353186607361)
[2025-02-16 11:25:35,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:35,919][root][INFO] - Training Epoch: 1/2, step 3750/23838 completed (loss: 1.127187728881836, acc: 0.7303370833396912)
[2025-02-16 11:25:36,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:36,303][root][INFO] - Training Epoch: 1/2, step 3751/23838 completed (loss: 0.5366143584251404, acc: 0.84375)
[2025-02-16 11:25:36,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:36,684][root][INFO] - Training Epoch: 1/2, step 3752/23838 completed (loss: 0.5347887873649597, acc: 0.8421052694320679)
[2025-02-16 11:25:36,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:37,138][root][INFO] - Training Epoch: 1/2, step 3753/23838 completed (loss: 0.4321479797363281, acc: 0.8297872543334961)
[2025-02-16 11:25:37,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:37,557][root][INFO] - Training Epoch: 1/2, step 3754/23838 completed (loss: 0.4330173134803772, acc: 0.8672566413879395)
[2025-02-16 11:25:37,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:38,023][root][INFO] - Training Epoch: 1/2, step 3755/23838 completed (loss: 0.18947234749794006, acc: 0.9319728016853333)
[2025-02-16 11:25:38,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:38,495][root][INFO] - Training Epoch: 1/2, step 3756/23838 completed (loss: 0.7471282482147217, acc: 0.8062015771865845)
[2025-02-16 11:25:38,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:38,953][root][INFO] - Training Epoch: 1/2, step 3757/23838 completed (loss: 0.513908326625824, acc: 0.8350515365600586)
[2025-02-16 11:25:39,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:39,385][root][INFO] - Training Epoch: 1/2, step 3758/23838 completed (loss: 0.2608998417854309, acc: 0.9610389471054077)
[2025-02-16 11:25:39,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:39,832][root][INFO] - Training Epoch: 1/2, step 3759/23838 completed (loss: 0.3330429196357727, acc: 0.8983050584793091)
[2025-02-16 11:25:40,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:40,270][root][INFO] - Training Epoch: 1/2, step 3760/23838 completed (loss: 0.6166812777519226, acc: 0.8380952477455139)
[2025-02-16 11:25:40,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:40,735][root][INFO] - Training Epoch: 1/2, step 3761/23838 completed (loss: 0.44157180190086365, acc: 0.8828828930854797)
[2025-02-16 11:25:40,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:41,194][root][INFO] - Training Epoch: 1/2, step 3762/23838 completed (loss: 0.5239585041999817, acc: 0.8627451062202454)
[2025-02-16 11:25:41,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:41,586][root][INFO] - Training Epoch: 1/2, step 3763/23838 completed (loss: 0.3784019947052002, acc: 0.8799999952316284)
[2025-02-16 11:25:41,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:41,977][root][INFO] - Training Epoch: 1/2, step 3764/23838 completed (loss: 0.5500010251998901, acc: 0.8333333134651184)
[2025-02-16 11:25:42,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:42,353][root][INFO] - Training Epoch: 1/2, step 3765/23838 completed (loss: 0.6144427061080933, acc: 0.7647058963775635)
[2025-02-16 11:25:42,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:42,795][root][INFO] - Training Epoch: 1/2, step 3766/23838 completed (loss: 0.6772773861885071, acc: 0.8317757248878479)
[2025-02-16 11:25:42,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:43,185][root][INFO] - Training Epoch: 1/2, step 3767/23838 completed (loss: 0.9502089023590088, acc: 0.7325581312179565)
[2025-02-16 11:25:43,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:43,602][root][INFO] - Training Epoch: 1/2, step 3768/23838 completed (loss: 0.5206907987594604, acc: 0.8684210777282715)
[2025-02-16 11:25:43,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:44,144][root][INFO] - Training Epoch: 1/2, step 3769/23838 completed (loss: 0.6299009323120117, acc: 0.8062015771865845)
[2025-02-16 11:25:44,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:44,598][root][INFO] - Training Epoch: 1/2, step 3770/23838 completed (loss: 1.0915027856826782, acc: 0.7058823704719543)
[2025-02-16 11:25:44,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:45,051][root][INFO] - Training Epoch: 1/2, step 3771/23838 completed (loss: 0.43320611119270325, acc: 0.931506872177124)
[2025-02-16 11:25:45,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:45,429][root][INFO] - Training Epoch: 1/2, step 3772/23838 completed (loss: 0.5338482856750488, acc: 0.8518518805503845)
[2025-02-16 11:25:45,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:45,858][root][INFO] - Training Epoch: 1/2, step 3773/23838 completed (loss: 0.6520756483078003, acc: 0.8247422575950623)
[2025-02-16 11:25:46,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:46,247][root][INFO] - Training Epoch: 1/2, step 3774/23838 completed (loss: 0.6493604183197021, acc: 0.7647058963775635)
[2025-02-16 11:25:46,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:46,656][root][INFO] - Training Epoch: 1/2, step 3775/23838 completed (loss: 0.35789230465888977, acc: 0.9009901285171509)
[2025-02-16 11:25:46,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:47,041][root][INFO] - Training Epoch: 1/2, step 3776/23838 completed (loss: 0.5157184600830078, acc: 0.8571428656578064)
[2025-02-16 11:25:47,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:47,410][root][INFO] - Training Epoch: 1/2, step 3777/23838 completed (loss: 1.0897984504699707, acc: 0.7435897588729858)
[2025-02-16 11:25:47,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:47,861][root][INFO] - Training Epoch: 1/2, step 3778/23838 completed (loss: 0.4224220812320709, acc: 0.8876404762268066)
[2025-02-16 11:25:48,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:48,273][root][INFO] - Training Epoch: 1/2, step 3779/23838 completed (loss: 1.1300257444381714, acc: 0.703125)
[2025-02-16 11:25:48,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:48,745][root][INFO] - Training Epoch: 1/2, step 3780/23838 completed (loss: 0.2898963987827301, acc: 0.9272727370262146)
[2025-02-16 11:25:49,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:49,381][root][INFO] - Training Epoch: 1/2, step 3781/23838 completed (loss: 0.3858373463153839, acc: 0.9032257795333862)
[2025-02-16 11:25:49,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:49,788][root][INFO] - Training Epoch: 1/2, step 3782/23838 completed (loss: 0.3737145662307739, acc: 0.8777777552604675)
[2025-02-16 11:25:49,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:50,221][root][INFO] - Training Epoch: 1/2, step 3783/23838 completed (loss: 0.9546248316764832, acc: 0.7168141603469849)
[2025-02-16 11:25:50,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:50,661][root][INFO] - Training Epoch: 1/2, step 3784/23838 completed (loss: 0.6275729537010193, acc: 0.8717948794364929)
[2025-02-16 11:25:50,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:51,102][root][INFO] - Training Epoch: 1/2, step 3785/23838 completed (loss: 0.8447943329811096, acc: 0.8059701323509216)
[2025-02-16 11:25:51,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:51,525][root][INFO] - Training Epoch: 1/2, step 3786/23838 completed (loss: 0.3831390142440796, acc: 0.8933333158493042)
[2025-02-16 11:25:51,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:51,967][root][INFO] - Training Epoch: 1/2, step 3787/23838 completed (loss: 0.6486716270446777, acc: 0.8271604776382446)
[2025-02-16 11:25:52,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:52,349][root][INFO] - Training Epoch: 1/2, step 3788/23838 completed (loss: 1.0948039293289185, acc: 0.7450980544090271)
[2025-02-16 11:25:52,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:52,734][root][INFO] - Training Epoch: 1/2, step 3789/23838 completed (loss: 0.8379312753677368, acc: 0.7727272510528564)
[2025-02-16 11:25:52,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:53,111][root][INFO] - Training Epoch: 1/2, step 3790/23838 completed (loss: 0.46316391229629517, acc: 0.8723404407501221)
[2025-02-16 11:25:53,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:53,516][root][INFO] - Training Epoch: 1/2, step 3791/23838 completed (loss: 0.22654731571674347, acc: 0.9729729890823364)
[2025-02-16 11:25:53,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:53,929][root][INFO] - Training Epoch: 1/2, step 3792/23838 completed (loss: 0.6468937397003174, acc: 0.8389830589294434)
[2025-02-16 11:25:54,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:54,379][root][INFO] - Training Epoch: 1/2, step 3793/23838 completed (loss: 0.6585622429847717, acc: 0.7980769276618958)
[2025-02-16 11:25:54,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:54,823][root][INFO] - Training Epoch: 1/2, step 3794/23838 completed (loss: 0.39141154289245605, acc: 0.9090909361839294)
[2025-02-16 11:25:55,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:55,284][root][INFO] - Training Epoch: 1/2, step 3795/23838 completed (loss: 1.9637746810913086, acc: 0.5079365372657776)
[2025-02-16 11:25:55,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:55,722][root][INFO] - Training Epoch: 1/2, step 3796/23838 completed (loss: 0.5439749360084534, acc: 0.8571428656578064)
[2025-02-16 11:25:55,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:56,089][root][INFO] - Training Epoch: 1/2, step 3797/23838 completed (loss: 1.5466914176940918, acc: 0.7096773982048035)
[2025-02-16 11:25:56,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:56,511][root][INFO] - Training Epoch: 1/2, step 3798/23838 completed (loss: 0.9360604286193848, acc: 0.7368420958518982)
[2025-02-16 11:25:56,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:56,904][root][INFO] - Training Epoch: 1/2, step 3799/23838 completed (loss: 1.0886924266815186, acc: 0.692307710647583)
[2025-02-16 11:25:57,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:57,331][root][INFO] - Training Epoch: 1/2, step 3800/23838 completed (loss: 1.0966298580169678, acc: 0.6666666865348816)
[2025-02-16 11:25:57,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:57,836][root][INFO] - Training Epoch: 1/2, step 3801/23838 completed (loss: 0.566826581954956, acc: 0.8857142925262451)
[2025-02-16 11:25:58,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:58,278][root][INFO] - Training Epoch: 1/2, step 3802/23838 completed (loss: 0.5800575613975525, acc: 0.8269230723381042)
[2025-02-16 11:25:58,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:58,747][root][INFO] - Training Epoch: 1/2, step 3803/23838 completed (loss: 0.4082125723361969, acc: 0.8548387289047241)
[2025-02-16 11:25:58,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:59,169][root][INFO] - Training Epoch: 1/2, step 3804/23838 completed (loss: 0.7473403811454773, acc: 0.760869562625885)
[2025-02-16 11:25:59,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:25:59,626][root][INFO] - Training Epoch: 1/2, step 3805/23838 completed (loss: 1.2250927686691284, acc: 0.65625)
[2025-02-16 11:25:59,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:00,030][root][INFO] - Training Epoch: 1/2, step 3806/23838 completed (loss: 0.660730242729187, acc: 0.800000011920929)
[2025-02-16 11:26:00,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:00,441][root][INFO] - Training Epoch: 1/2, step 3807/23838 completed (loss: 0.9285985231399536, acc: 0.675000011920929)
[2025-02-16 11:26:00,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:00,890][root][INFO] - Training Epoch: 1/2, step 3808/23838 completed (loss: 1.2404979467391968, acc: 0.625)
[2025-02-16 11:26:01,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:01,306][root][INFO] - Training Epoch: 1/2, step 3809/23838 completed (loss: 1.4469074010849, acc: 0.644444465637207)
[2025-02-16 11:26:01,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:01,720][root][INFO] - Training Epoch: 1/2, step 3810/23838 completed (loss: 1.3186354637145996, acc: 0.6875)
[2025-02-16 11:26:01,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:02,140][root][INFO] - Training Epoch: 1/2, step 3811/23838 completed (loss: 1.1239529848098755, acc: 0.6875)
[2025-02-16 11:26:02,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:02,566][root][INFO] - Training Epoch: 1/2, step 3812/23838 completed (loss: 2.363713502883911, acc: 0.2857142984867096)
[2025-02-16 11:26:02,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:03,006][root][INFO] - Training Epoch: 1/2, step 3813/23838 completed (loss: 1.264482021331787, acc: 0.6333333253860474)
[2025-02-16 11:26:03,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:03,418][root][INFO] - Training Epoch: 1/2, step 3814/23838 completed (loss: 0.7133342623710632, acc: 0.8039215803146362)
[2025-02-16 11:26:03,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:03,833][root][INFO] - Training Epoch: 1/2, step 3815/23838 completed (loss: 0.7590689659118652, acc: 0.7758620977401733)
[2025-02-16 11:26:03,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:04,238][root][INFO] - Training Epoch: 1/2, step 3816/23838 completed (loss: 0.9305312633514404, acc: 0.6818181872367859)
[2025-02-16 11:26:04,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:04,654][root][INFO] - Training Epoch: 1/2, step 3817/23838 completed (loss: 1.0519592761993408, acc: 0.7291666865348816)
[2025-02-16 11:26:04,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:05,079][root][INFO] - Training Epoch: 1/2, step 3818/23838 completed (loss: 0.6009710431098938, acc: 0.8205128312110901)
[2025-02-16 11:26:05,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:05,450][root][INFO] - Training Epoch: 1/2, step 3819/23838 completed (loss: 0.6741031408309937, acc: 0.8260869383811951)
[2025-02-16 11:26:05,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:05,841][root][INFO] - Training Epoch: 1/2, step 3820/23838 completed (loss: 0.712488055229187, acc: 0.8205128312110901)
[2025-02-16 11:26:06,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:06,254][root][INFO] - Training Epoch: 1/2, step 3821/23838 completed (loss: 0.6942086219787598, acc: 0.8571428656578064)
[2025-02-16 11:26:06,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:06,665][root][INFO] - Training Epoch: 1/2, step 3822/23838 completed (loss: 0.8063687682151794, acc: 0.760869562625885)
[2025-02-16 11:26:06,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:07,071][root][INFO] - Training Epoch: 1/2, step 3823/23838 completed (loss: 0.41664355993270874, acc: 0.7647058963775635)
[2025-02-16 11:26:07,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:07,490][root][INFO] - Training Epoch: 1/2, step 3824/23838 completed (loss: 1.3145052194595337, acc: 0.5277777910232544)
[2025-02-16 11:26:07,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:07,930][root][INFO] - Training Epoch: 1/2, step 3825/23838 completed (loss: 0.24163098633289337, acc: 0.949999988079071)
[2025-02-16 11:26:08,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:08,393][root][INFO] - Training Epoch: 1/2, step 3826/23838 completed (loss: 0.6117153167724609, acc: 0.8208954930305481)
[2025-02-16 11:26:08,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:08,831][root][INFO] - Training Epoch: 1/2, step 3827/23838 completed (loss: 1.309625267982483, acc: 0.6739130616188049)
[2025-02-16 11:26:08,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:09,210][root][INFO] - Training Epoch: 1/2, step 3828/23838 completed (loss: 1.094688892364502, acc: 0.6285714507102966)
[2025-02-16 11:26:09,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:09,647][root][INFO] - Training Epoch: 1/2, step 3829/23838 completed (loss: 1.64226496219635, acc: 0.6041666865348816)
[2025-02-16 11:26:09,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:10,026][root][INFO] - Training Epoch: 1/2, step 3830/23838 completed (loss: 0.8162968158721924, acc: 0.7857142686843872)
[2025-02-16 11:26:10,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:10,437][root][INFO] - Training Epoch: 1/2, step 3831/23838 completed (loss: 0.7612672448158264, acc: 0.8148148059844971)
[2025-02-16 11:26:10,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:10,862][root][INFO] - Training Epoch: 1/2, step 3832/23838 completed (loss: 0.44736725091934204, acc: 0.8095238208770752)
[2025-02-16 11:26:11,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:11,248][root][INFO] - Training Epoch: 1/2, step 3833/23838 completed (loss: 0.585430383682251, acc: 0.8285714387893677)
[2025-02-16 11:26:11,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:11,650][root][INFO] - Training Epoch: 1/2, step 3834/23838 completed (loss: 0.3902994394302368, acc: 0.9032257795333862)
[2025-02-16 11:26:11,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:12,086][root][INFO] - Training Epoch: 1/2, step 3835/23838 completed (loss: 0.930919349193573, acc: 0.7346938848495483)
[2025-02-16 11:26:12,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:12,523][root][INFO] - Training Epoch: 1/2, step 3836/23838 completed (loss: 0.6641308665275574, acc: 0.8285714387893677)
[2025-02-16 11:26:12,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:12,908][root][INFO] - Training Epoch: 1/2, step 3837/23838 completed (loss: 1.1216776371002197, acc: 0.6904761791229248)
[2025-02-16 11:26:13,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:13,362][root][INFO] - Training Epoch: 1/2, step 3838/23838 completed (loss: 0.8076525330543518, acc: 0.725806474685669)
[2025-02-16 11:26:13,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:13,797][root][INFO] - Training Epoch: 1/2, step 3839/23838 completed (loss: 0.9705586433410645, acc: 0.7368420958518982)
[2025-02-16 11:26:14,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:14,261][root][INFO] - Training Epoch: 1/2, step 3840/23838 completed (loss: 1.4351472854614258, acc: 0.6184210777282715)
[2025-02-16 11:26:14,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:14,702][root][INFO] - Training Epoch: 1/2, step 3841/23838 completed (loss: 0.5118668675422668, acc: 0.8372092843055725)
[2025-02-16 11:26:14,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:15,096][root][INFO] - Training Epoch: 1/2, step 3842/23838 completed (loss: 1.5140019655227661, acc: 0.7083333134651184)
[2025-02-16 11:26:15,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:15,510][root][INFO] - Training Epoch: 1/2, step 3843/23838 completed (loss: 0.7598623037338257, acc: 0.7368420958518982)
[2025-02-16 11:26:15,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:15,908][root][INFO] - Training Epoch: 1/2, step 3844/23838 completed (loss: 0.5809446573257446, acc: 0.8108108043670654)
[2025-02-16 11:26:16,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:16,345][root][INFO] - Training Epoch: 1/2, step 3845/23838 completed (loss: 1.1156889200210571, acc: 0.644444465637207)
[2025-02-16 11:26:16,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:16,750][root][INFO] - Training Epoch: 1/2, step 3846/23838 completed (loss: 0.8503621220588684, acc: 0.8387096524238586)
[2025-02-16 11:26:16,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:17,112][root][INFO] - Training Epoch: 1/2, step 3847/23838 completed (loss: 0.9852898716926575, acc: 0.7222222089767456)
[2025-02-16 11:26:17,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:17,540][root][INFO] - Training Epoch: 1/2, step 3848/23838 completed (loss: 0.3128972053527832, acc: 0.9090909361839294)
[2025-02-16 11:26:17,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:17,939][root][INFO] - Training Epoch: 1/2, step 3849/23838 completed (loss: 0.8666666150093079, acc: 0.7777777910232544)
[2025-02-16 11:26:18,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:18,303][root][INFO] - Training Epoch: 1/2, step 3850/23838 completed (loss: 0.5017473697662354, acc: 0.8983050584793091)
[2025-02-16 11:26:18,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:18,731][root][INFO] - Training Epoch: 1/2, step 3851/23838 completed (loss: 0.897766649723053, acc: 0.7317073345184326)
[2025-02-16 11:26:18,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:19,183][root][INFO] - Training Epoch: 1/2, step 3852/23838 completed (loss: 0.602736234664917, acc: 0.807692289352417)
[2025-02-16 11:26:19,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:19,676][root][INFO] - Training Epoch: 1/2, step 3853/23838 completed (loss: 0.8323224186897278, acc: 0.7676767706871033)
[2025-02-16 11:26:19,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:20,200][root][INFO] - Training Epoch: 1/2, step 3854/23838 completed (loss: 0.9745872616767883, acc: 0.730434775352478)
[2025-02-16 11:26:20,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:20,645][root][INFO] - Training Epoch: 1/2, step 3855/23838 completed (loss: 1.6894514560699463, acc: 0.5)
[2025-02-16 11:26:20,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:21,022][root][INFO] - Training Epoch: 1/2, step 3856/23838 completed (loss: 0.8447962403297424, acc: 0.7792207598686218)
[2025-02-16 11:26:21,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:21,428][root][INFO] - Training Epoch: 1/2, step 3857/23838 completed (loss: 1.0681954622268677, acc: 0.6808510422706604)
[2025-02-16 11:26:21,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:21,935][root][INFO] - Training Epoch: 1/2, step 3858/23838 completed (loss: 1.0136901140213013, acc: 0.7362637519836426)
[2025-02-16 11:26:22,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:22,404][root][INFO] - Training Epoch: 1/2, step 3859/23838 completed (loss: 1.5454349517822266, acc: 0.529411792755127)
[2025-02-16 11:26:22,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:23,012][root][INFO] - Training Epoch: 1/2, step 3860/23838 completed (loss: 1.0213491916656494, acc: 0.6867470145225525)
[2025-02-16 11:26:23,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:23,794][root][INFO] - Training Epoch: 1/2, step 3861/23838 completed (loss: 1.31869637966156, acc: 0.658823549747467)
[2025-02-16 11:26:24,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:24,267][root][INFO] - Training Epoch: 1/2, step 3862/23838 completed (loss: 0.617638349533081, acc: 0.8426966071128845)
[2025-02-16 11:26:24,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:24,861][root][INFO] - Training Epoch: 1/2, step 3863/23838 completed (loss: 0.6067181825637817, acc: 0.8500000238418579)
[2025-02-16 11:26:25,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:25,771][root][INFO] - Training Epoch: 1/2, step 3864/23838 completed (loss: 0.844995379447937, acc: 0.7549669146537781)
[2025-02-16 11:26:26,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:26,728][root][INFO] - Training Epoch: 1/2, step 3865/23838 completed (loss: 0.9467227458953857, acc: 0.7333333492279053)
[2025-02-16 11:26:27,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:27,503][root][INFO] - Training Epoch: 1/2, step 3866/23838 completed (loss: 1.3797149658203125, acc: 0.6285714507102966)
[2025-02-16 11:26:27,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:28,056][root][INFO] - Training Epoch: 1/2, step 3867/23838 completed (loss: 1.3166593313217163, acc: 0.6666666865348816)
[2025-02-16 11:26:28,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:28,464][root][INFO] - Training Epoch: 1/2, step 3868/23838 completed (loss: 0.7452155947685242, acc: 0.7878788113594055)
[2025-02-16 11:26:28,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:28,877][root][INFO] - Training Epoch: 1/2, step 3869/23838 completed (loss: 0.6792189478874207, acc: 0.7647058963775635)
[2025-02-16 11:26:29,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:29,865][root][INFO] - Training Epoch: 1/2, step 3870/23838 completed (loss: 1.0509494543075562, acc: 0.6964285969734192)
[2025-02-16 11:26:30,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:30,250][root][INFO] - Training Epoch: 1/2, step 3871/23838 completed (loss: 0.8608312606811523, acc: 0.7475728392601013)
[2025-02-16 11:26:30,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:30,718][root][INFO] - Training Epoch: 1/2, step 3872/23838 completed (loss: 1.0091632604599, acc: 0.7591241002082825)
[2025-02-16 11:26:30,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:31,196][root][INFO] - Training Epoch: 1/2, step 3873/23838 completed (loss: 0.761310875415802, acc: 0.804347813129425)
[2025-02-16 11:26:31,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:31,645][root][INFO] - Training Epoch: 1/2, step 3874/23838 completed (loss: 1.1928153038024902, acc: 0.6746987700462341)
[2025-02-16 11:26:31,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:32,100][root][INFO] - Training Epoch: 1/2, step 3875/23838 completed (loss: 0.7027934789657593, acc: 0.7971014380455017)
[2025-02-16 11:26:32,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:32,568][root][INFO] - Training Epoch: 1/2, step 3876/23838 completed (loss: 0.6847711205482483, acc: 0.7880434989929199)
[2025-02-16 11:26:32,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:33,015][root][INFO] - Training Epoch: 1/2, step 3877/23838 completed (loss: 0.9443613290786743, acc: 0.7263157963752747)
[2025-02-16 11:26:33,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:33,508][root][INFO] - Training Epoch: 1/2, step 3878/23838 completed (loss: 0.6417293548583984, acc: 0.7866666913032532)
[2025-02-16 11:26:33,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:33,911][root][INFO] - Training Epoch: 1/2, step 3879/23838 completed (loss: 0.7105101346969604, acc: 0.792682945728302)
[2025-02-16 11:26:34,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:34,338][root][INFO] - Training Epoch: 1/2, step 3880/23838 completed (loss: 1.070803165435791, acc: 0.6610169410705566)
[2025-02-16 11:26:34,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:34,783][root][INFO] - Training Epoch: 1/2, step 3881/23838 completed (loss: 0.7828330397605896, acc: 0.7865168452262878)
[2025-02-16 11:26:34,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:35,209][root][INFO] - Training Epoch: 1/2, step 3882/23838 completed (loss: 0.7125716209411621, acc: 0.8372092843055725)
[2025-02-16 11:26:35,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:35,644][root][INFO] - Training Epoch: 1/2, step 3883/23838 completed (loss: 0.9141291379928589, acc: 0.7090908885002136)
[2025-02-16 11:26:35,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:36,056][root][INFO] - Training Epoch: 1/2, step 3884/23838 completed (loss: 0.7976555228233337, acc: 0.7472527623176575)
[2025-02-16 11:26:36,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:36,524][root][INFO] - Training Epoch: 1/2, step 3885/23838 completed (loss: 0.557314932346344, acc: 0.8771929740905762)
[2025-02-16 11:26:36,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:36,923][root][INFO] - Training Epoch: 1/2, step 3886/23838 completed (loss: 0.9692327380180359, acc: 0.7454545497894287)
[2025-02-16 11:26:37,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:37,381][root][INFO] - Training Epoch: 1/2, step 3887/23838 completed (loss: 0.8965938687324524, acc: 0.7297297120094299)
[2025-02-16 11:26:37,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:37,848][root][INFO] - Training Epoch: 1/2, step 3888/23838 completed (loss: 0.9033291935920715, acc: 0.7777777910232544)
[2025-02-16 11:26:38,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:38,306][root][INFO] - Training Epoch: 1/2, step 3889/23838 completed (loss: 0.4591488242149353, acc: 0.8653846383094788)
[2025-02-16 11:26:38,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:38,718][root][INFO] - Training Epoch: 1/2, step 3890/23838 completed (loss: 0.575334906578064, acc: 0.8586956262588501)
[2025-02-16 11:26:38,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:39,120][root][INFO] - Training Epoch: 1/2, step 3891/23838 completed (loss: 0.6961330771446228, acc: 0.8253968358039856)
[2025-02-16 11:26:39,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:39,537][root][INFO] - Training Epoch: 1/2, step 3892/23838 completed (loss: 1.1375972032546997, acc: 0.7027027010917664)
[2025-02-16 11:26:39,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:39,930][root][INFO] - Training Epoch: 1/2, step 3893/23838 completed (loss: 0.9687206745147705, acc: 0.7444444298744202)
[2025-02-16 11:26:40,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:40,314][root][INFO] - Training Epoch: 1/2, step 3894/23838 completed (loss: 0.6639572381973267, acc: 0.808080792427063)
[2025-02-16 11:26:40,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:40,782][root][INFO] - Training Epoch: 1/2, step 3895/23838 completed (loss: 0.5455695986747742, acc: 0.8345864415168762)
[2025-02-16 11:26:40,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:41,221][root][INFO] - Training Epoch: 1/2, step 3896/23838 completed (loss: 1.0579934120178223, acc: 0.6888889074325562)
[2025-02-16 11:26:41,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:41,659][root][INFO] - Training Epoch: 1/2, step 3897/23838 completed (loss: 0.543526828289032, acc: 0.7948718070983887)
[2025-02-16 11:26:41,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:42,055][root][INFO] - Training Epoch: 1/2, step 3898/23838 completed (loss: 0.8399899005889893, acc: 0.7101449370384216)
[2025-02-16 11:26:42,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:42,482][root][INFO] - Training Epoch: 1/2, step 3899/23838 completed (loss: 0.7147329449653625, acc: 0.7983193397521973)
[2025-02-16 11:26:42,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:42,908][root][INFO] - Training Epoch: 1/2, step 3900/23838 completed (loss: 0.869864821434021, acc: 0.7755101919174194)
[2025-02-16 11:26:43,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:43,363][root][INFO] - Training Epoch: 1/2, step 3901/23838 completed (loss: 1.0205910205841064, acc: 0.7083333134651184)
[2025-02-16 11:26:43,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:43,782][root][INFO] - Training Epoch: 1/2, step 3902/23838 completed (loss: 0.7098140120506287, acc: 0.8118811845779419)
[2025-02-16 11:26:43,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:44,157][root][INFO] - Training Epoch: 1/2, step 3903/23838 completed (loss: 0.8122982978820801, acc: 0.7126436829566956)
[2025-02-16 11:26:44,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:44,625][root][INFO] - Training Epoch: 1/2, step 3904/23838 completed (loss: 0.897516131401062, acc: 0.8181818127632141)
[2025-02-16 11:26:44,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:45,080][root][INFO] - Training Epoch: 1/2, step 3905/23838 completed (loss: 0.941241979598999, acc: 0.730434775352478)
[2025-02-16 11:26:45,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:45,450][root][INFO] - Training Epoch: 1/2, step 3906/23838 completed (loss: 0.7574161291122437, acc: 0.8271604776382446)
[2025-02-16 11:26:45,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:45,894][root][INFO] - Training Epoch: 1/2, step 3907/23838 completed (loss: 0.8114572763442993, acc: 0.7872340679168701)
[2025-02-16 11:26:46,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:46,319][root][INFO] - Training Epoch: 1/2, step 3908/23838 completed (loss: 0.7410044074058533, acc: 0.7941176295280457)
[2025-02-16 11:26:46,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:46,751][root][INFO] - Training Epoch: 1/2, step 3909/23838 completed (loss: 0.7277873754501343, acc: 0.752293586730957)
[2025-02-16 11:26:47,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:47,242][root][INFO] - Training Epoch: 1/2, step 3910/23838 completed (loss: 0.7449349164962769, acc: 0.7886179089546204)
[2025-02-16 11:26:47,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:47,715][root][INFO] - Training Epoch: 1/2, step 3911/23838 completed (loss: 0.4427347183227539, acc: 0.8461538553237915)
[2025-02-16 11:26:47,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:48,097][root][INFO] - Training Epoch: 1/2, step 3912/23838 completed (loss: 0.4803639054298401, acc: 0.8505747318267822)
[2025-02-16 11:26:48,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:48,516][root][INFO] - Training Epoch: 1/2, step 3913/23838 completed (loss: 1.0055853128433228, acc: 0.7264150977134705)
[2025-02-16 11:26:48,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:48,935][root][INFO] - Training Epoch: 1/2, step 3914/23838 completed (loss: 0.4383716583251953, acc: 0.8985507488250732)
[2025-02-16 11:26:49,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:49,431][root][INFO] - Training Epoch: 1/2, step 3915/23838 completed (loss: 0.5736162662506104, acc: 0.8357142806053162)
[2025-02-16 11:26:49,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:49,889][root][INFO] - Training Epoch: 1/2, step 3916/23838 completed (loss: 0.914550244808197, acc: 0.7441860437393188)
[2025-02-16 11:26:50,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:50,385][root][INFO] - Training Epoch: 1/2, step 3917/23838 completed (loss: 0.6931949257850647, acc: 0.8017241358757019)
[2025-02-16 11:26:50,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:50,813][root][INFO] - Training Epoch: 1/2, step 3918/23838 completed (loss: 0.8304429650306702, acc: 0.7545454502105713)
[2025-02-16 11:26:50,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:51,219][root][INFO] - Training Epoch: 1/2, step 3919/23838 completed (loss: 0.246088907122612, acc: 0.931034505367279)
[2025-02-16 11:26:51,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:51,683][root][INFO] - Training Epoch: 1/2, step 3920/23838 completed (loss: 0.5573583245277405, acc: 0.8387096524238586)
[2025-02-16 11:26:51,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:52,134][root][INFO] - Training Epoch: 1/2, step 3921/23838 completed (loss: 0.9093405604362488, acc: 0.760869562625885)
[2025-02-16 11:26:52,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:52,580][root][INFO] - Training Epoch: 1/2, step 3922/23838 completed (loss: 0.7942477464675903, acc: 0.7749999761581421)
[2025-02-16 11:26:52,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:53,024][root][INFO] - Training Epoch: 1/2, step 3923/23838 completed (loss: 0.4034910202026367, acc: 0.875)
[2025-02-16 11:26:53,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:53,480][root][INFO] - Training Epoch: 1/2, step 3924/23838 completed (loss: 0.6226456165313721, acc: 0.8571428656578064)
[2025-02-16 11:26:53,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:53,918][root][INFO] - Training Epoch: 1/2, step 3925/23838 completed (loss: 1.1930102109909058, acc: 0.6438356041908264)
[2025-02-16 11:26:54,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:54,282][root][INFO] - Training Epoch: 1/2, step 3926/23838 completed (loss: 0.8662799596786499, acc: 0.7761194109916687)
[2025-02-16 11:26:54,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:54,708][root][INFO] - Training Epoch: 1/2, step 3927/23838 completed (loss: 1.0540416240692139, acc: 0.7066666483879089)
[2025-02-16 11:26:54,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:55,121][root][INFO] - Training Epoch: 1/2, step 3928/23838 completed (loss: 0.4185754060745239, acc: 0.8720930218696594)
[2025-02-16 11:26:55,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:55,552][root][INFO] - Training Epoch: 1/2, step 3929/23838 completed (loss: 0.8803848624229431, acc: 0.7241379022598267)
[2025-02-16 11:26:55,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:55,964][root][INFO] - Training Epoch: 1/2, step 3930/23838 completed (loss: 0.47562605142593384, acc: 0.868686854839325)
[2025-02-16 11:26:56,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:56,361][root][INFO] - Training Epoch: 1/2, step 3931/23838 completed (loss: 0.49643853306770325, acc: 0.8421052694320679)
[2025-02-16 11:26:56,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:56,743][root][INFO] - Training Epoch: 1/2, step 3932/23838 completed (loss: 0.9761343002319336, acc: 0.7124999761581421)
[2025-02-16 11:26:56,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:57,131][root][INFO] - Training Epoch: 1/2, step 3933/23838 completed (loss: 1.0786646604537964, acc: 0.7162162065505981)
[2025-02-16 11:26:57,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:57,528][root][INFO] - Training Epoch: 1/2, step 3934/23838 completed (loss: 0.7178274393081665, acc: 0.7866666913032532)
[2025-02-16 11:26:57,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:57,978][root][INFO] - Training Epoch: 1/2, step 3935/23838 completed (loss: 0.5107793807983398, acc: 0.843137264251709)
[2025-02-16 11:26:58,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:58,383][root][INFO] - Training Epoch: 1/2, step 3936/23838 completed (loss: 0.9175893664360046, acc: 0.7413793206214905)
[2025-02-16 11:26:58,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:58,794][root][INFO] - Training Epoch: 1/2, step 3937/23838 completed (loss: 0.6275611519813538, acc: 0.8441558480262756)
[2025-02-16 11:26:58,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:59,179][root][INFO] - Training Epoch: 1/2, step 3938/23838 completed (loss: 0.7940397262573242, acc: 0.795918345451355)
[2025-02-16 11:26:59,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:26:59,637][root][INFO] - Training Epoch: 1/2, step 3939/23838 completed (loss: 0.3992457389831543, acc: 0.9130434989929199)
[2025-02-16 11:26:59,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:00,114][root][INFO] - Training Epoch: 1/2, step 3940/23838 completed (loss: 0.7461153864860535, acc: 0.8181818127632141)
[2025-02-16 11:27:00,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:00,578][root][INFO] - Training Epoch: 1/2, step 3941/23838 completed (loss: 0.6229420304298401, acc: 0.8275862336158752)
[2025-02-16 11:27:00,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:00,984][root][INFO] - Training Epoch: 1/2, step 3942/23838 completed (loss: 0.6152029037475586, acc: 0.852173924446106)
[2025-02-16 11:27:01,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:01,415][root][INFO] - Training Epoch: 1/2, step 3943/23838 completed (loss: 0.8353450298309326, acc: 0.734375)
[2025-02-16 11:27:01,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:01,865][root][INFO] - Training Epoch: 1/2, step 3944/23838 completed (loss: 0.9003634452819824, acc: 0.7411764860153198)
[2025-02-16 11:27:02,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:02,259][root][INFO] - Training Epoch: 1/2, step 3945/23838 completed (loss: 0.7004348635673523, acc: 0.8275862336158752)
[2025-02-16 11:27:02,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:02,645][root][INFO] - Training Epoch: 1/2, step 3946/23838 completed (loss: 1.1884459257125854, acc: 0.7301587462425232)
[2025-02-16 11:27:02,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:03,114][root][INFO] - Training Epoch: 1/2, step 3947/23838 completed (loss: 0.8796600699424744, acc: 0.75)
[2025-02-16 11:27:03,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:03,982][root][INFO] - Training Epoch: 1/2, step 3948/23838 completed (loss: 1.0655856132507324, acc: 0.6973684430122375)
[2025-02-16 11:27:04,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:04,409][root][INFO] - Training Epoch: 1/2, step 3949/23838 completed (loss: 1.6104369163513184, acc: 0.6111111044883728)
[2025-02-16 11:27:04,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:04,785][root][INFO] - Training Epoch: 1/2, step 3950/23838 completed (loss: 1.15463125705719, acc: 0.681034505367279)
[2025-02-16 11:27:04,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:05,229][root][INFO] - Training Epoch: 1/2, step 3951/23838 completed (loss: 0.8110529780387878, acc: 0.7551020383834839)
[2025-02-16 11:27:05,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:05,674][root][INFO] - Training Epoch: 1/2, step 3952/23838 completed (loss: 0.9354782700538635, acc: 0.6911764740943909)
[2025-02-16 11:27:05,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:06,094][root][INFO] - Training Epoch: 1/2, step 3953/23838 completed (loss: 0.765648365020752, acc: 0.7799999713897705)
[2025-02-16 11:27:06,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:06,562][root][INFO] - Training Epoch: 1/2, step 3954/23838 completed (loss: 0.6923791170120239, acc: 0.8181818127632141)
[2025-02-16 11:27:06,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:06,944][root][INFO] - Training Epoch: 1/2, step 3955/23838 completed (loss: 1.1930930614471436, acc: 0.6666666865348816)
[2025-02-16 11:27:07,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:07,391][root][INFO] - Training Epoch: 1/2, step 3956/23838 completed (loss: 0.6062655448913574, acc: 0.8196721076965332)
[2025-02-16 11:27:07,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:07,845][root][INFO] - Training Epoch: 1/2, step 3957/23838 completed (loss: 1.3905898332595825, acc: 0.5890411138534546)
[2025-02-16 11:27:08,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:08,274][root][INFO] - Training Epoch: 1/2, step 3958/23838 completed (loss: 1.135910153388977, acc: 0.6590909361839294)
[2025-02-16 11:27:08,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:08,670][root][INFO] - Training Epoch: 1/2, step 3959/23838 completed (loss: 0.896730363368988, acc: 0.8125)
[2025-02-16 11:27:08,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:09,096][root][INFO] - Training Epoch: 1/2, step 3960/23838 completed (loss: 0.7579058408737183, acc: 0.8333333134651184)
[2025-02-16 11:27:09,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:09,533][root][INFO] - Training Epoch: 1/2, step 3961/23838 completed (loss: 1.05620539188385, acc: 0.7118644118309021)
[2025-02-16 11:27:09,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:09,963][root][INFO] - Training Epoch: 1/2, step 3962/23838 completed (loss: 0.7974668145179749, acc: 0.7272727489471436)
[2025-02-16 11:27:10,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:10,390][root][INFO] - Training Epoch: 1/2, step 3963/23838 completed (loss: 1.0298728942871094, acc: 0.7580645084381104)
[2025-02-16 11:27:10,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:10,800][root][INFO] - Training Epoch: 1/2, step 3964/23838 completed (loss: 1.0378798246383667, acc: 0.6818181872367859)
[2025-02-16 11:27:10,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:11,171][root][INFO] - Training Epoch: 1/2, step 3965/23838 completed (loss: 0.8317479491233826, acc: 0.6896551847457886)
[2025-02-16 11:27:11,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:11,596][root][INFO] - Training Epoch: 1/2, step 3966/23838 completed (loss: 1.3683162927627563, acc: 0.6595744490623474)
[2025-02-16 11:27:11,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:11,995][root][INFO] - Training Epoch: 1/2, step 3967/23838 completed (loss: 1.3781324625015259, acc: 0.6458333134651184)
[2025-02-16 11:27:12,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:12,377][root][INFO] - Training Epoch: 1/2, step 3968/23838 completed (loss: 0.8351770043373108, acc: 0.7083333134651184)
[2025-02-16 11:27:12,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:12,804][root][INFO] - Training Epoch: 1/2, step 3969/23838 completed (loss: 1.087090015411377, acc: 0.7419354915618896)
[2025-02-16 11:27:12,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:13,246][root][INFO] - Training Epoch: 1/2, step 3970/23838 completed (loss: 0.8112375140190125, acc: 0.7749999761581421)
[2025-02-16 11:27:13,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:13,703][root][INFO] - Training Epoch: 1/2, step 3971/23838 completed (loss: 0.8169369697570801, acc: 0.7777777910232544)
[2025-02-16 11:27:13,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:14,082][root][INFO] - Training Epoch: 1/2, step 3972/23838 completed (loss: 0.5910743474960327, acc: 0.8152173757553101)
[2025-02-16 11:27:14,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:14,443][root][INFO] - Training Epoch: 1/2, step 3973/23838 completed (loss: 1.0874680280685425, acc: 0.7121211886405945)
[2025-02-16 11:27:14,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:14,894][root][INFO] - Training Epoch: 1/2, step 3974/23838 completed (loss: 1.5485211610794067, acc: 0.574999988079071)
[2025-02-16 11:27:15,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:15,315][root][INFO] - Training Epoch: 1/2, step 3975/23838 completed (loss: 0.9997228980064392, acc: 0.7288135886192322)
[2025-02-16 11:27:15,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:15,692][root][INFO] - Training Epoch: 1/2, step 3976/23838 completed (loss: 1.035058617591858, acc: 0.7105262875556946)
[2025-02-16 11:27:15,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:16,132][root][INFO] - Training Epoch: 1/2, step 3977/23838 completed (loss: 0.9460581541061401, acc: 0.7407407164573669)
[2025-02-16 11:27:16,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:16,545][root][INFO] - Training Epoch: 1/2, step 3978/23838 completed (loss: 0.8932035565376282, acc: 0.699999988079071)
[2025-02-16 11:27:16,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:16,965][root][INFO] - Training Epoch: 1/2, step 3979/23838 completed (loss: 0.7744206786155701, acc: 0.7936508059501648)
[2025-02-16 11:27:17,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:17,356][root][INFO] - Training Epoch: 1/2, step 3980/23838 completed (loss: 0.6664407253265381, acc: 0.8196721076965332)
[2025-02-16 11:27:17,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:17,839][root][INFO] - Training Epoch: 1/2, step 3981/23838 completed (loss: 1.0758113861083984, acc: 0.7179487347602844)
[2025-02-16 11:27:18,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:18,363][root][INFO] - Training Epoch: 1/2, step 3982/23838 completed (loss: 0.6457233428955078, acc: 0.84375)
[2025-02-16 11:27:18,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:18,877][root][INFO] - Training Epoch: 1/2, step 3983/23838 completed (loss: 0.9459296464920044, acc: 0.7636363506317139)
[2025-02-16 11:27:19,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:19,287][root][INFO] - Training Epoch: 1/2, step 3984/23838 completed (loss: 1.2583063840866089, acc: 0.6419752836227417)
[2025-02-16 11:27:19,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:19,684][root][INFO] - Training Epoch: 1/2, step 3985/23838 completed (loss: 0.8314653038978577, acc: 0.779411792755127)
[2025-02-16 11:27:19,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:20,105][root][INFO] - Training Epoch: 1/2, step 3986/23838 completed (loss: 0.5831907391548157, acc: 0.8333333134651184)
[2025-02-16 11:27:20,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:20,533][root][INFO] - Training Epoch: 1/2, step 3987/23838 completed (loss: 0.7927078604698181, acc: 0.75)
[2025-02-16 11:27:20,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:21,028][root][INFO] - Training Epoch: 1/2, step 3988/23838 completed (loss: 1.131211757659912, acc: 0.7536231875419617)
[2025-02-16 11:27:21,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:21,478][root][INFO] - Training Epoch: 1/2, step 3989/23838 completed (loss: 0.934060275554657, acc: 0.7341772317886353)
[2025-02-16 11:27:21,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:21,907][root][INFO] - Training Epoch: 1/2, step 3990/23838 completed (loss: 0.7537007331848145, acc: 0.7777777910232544)
[2025-02-16 11:27:22,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:22,380][root][INFO] - Training Epoch: 1/2, step 3991/23838 completed (loss: 0.9087551236152649, acc: 0.7647058963775635)
[2025-02-16 11:27:22,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:22,809][root][INFO] - Training Epoch: 1/2, step 3992/23838 completed (loss: 1.0720586776733398, acc: 0.7068965435028076)
[2025-02-16 11:27:22,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:23,197][root][INFO] - Training Epoch: 1/2, step 3993/23838 completed (loss: 1.1671298742294312, acc: 0.6617646813392639)
[2025-02-16 11:27:23,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:23,574][root][INFO] - Training Epoch: 1/2, step 3994/23838 completed (loss: 0.6400655508041382, acc: 0.7922077775001526)
[2025-02-16 11:27:23,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:23,990][root][INFO] - Training Epoch: 1/2, step 3995/23838 completed (loss: 1.0415951013565063, acc: 0.6938775777816772)
[2025-02-16 11:27:24,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:24,414][root][INFO] - Training Epoch: 1/2, step 3996/23838 completed (loss: 1.6245800256729126, acc: 0.5111111402511597)
[2025-02-16 11:27:24,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:24,854][root][INFO] - Training Epoch: 1/2, step 3997/23838 completed (loss: 0.9243541955947876, acc: 0.6769230961799622)
[2025-02-16 11:27:25,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:25,280][root][INFO] - Training Epoch: 1/2, step 3998/23838 completed (loss: 0.9381225109100342, acc: 0.7272727489471436)
[2025-02-16 11:27:25,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:25,731][root][INFO] - Training Epoch: 1/2, step 3999/23838 completed (loss: 0.5606817007064819, acc: 0.8723404407501221)
[2025-02-16 11:27:25,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:26,199][root][INFO] - Training Epoch: 1/2, step 4000/23838 completed (loss: 0.9248760938644409, acc: 0.7285714149475098)
[2025-02-16 11:27:26,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:26,602][root][INFO] - Training Epoch: 1/2, step 4001/23838 completed (loss: 1.219387173652649, acc: 0.60317462682724)
[2025-02-16 11:27:26,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:27,020][root][INFO] - Training Epoch: 1/2, step 4002/23838 completed (loss: 1.1617170572280884, acc: 0.6623376607894897)
[2025-02-16 11:27:27,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:27,400][root][INFO] - Training Epoch: 1/2, step 4003/23838 completed (loss: 1.2298948764801025, acc: 0.6034482717514038)
[2025-02-16 11:27:27,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:27,810][root][INFO] - Training Epoch: 1/2, step 4004/23838 completed (loss: 1.2039347887039185, acc: 0.75)
[2025-02-16 11:27:27,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:28,206][root][INFO] - Training Epoch: 1/2, step 4005/23838 completed (loss: 1.1899478435516357, acc: 0.6458333134651184)
[2025-02-16 11:27:28,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:28,587][root][INFO] - Training Epoch: 1/2, step 4006/23838 completed (loss: 0.9388608336448669, acc: 0.6935483813285828)
[2025-02-16 11:27:28,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:29,011][root][INFO] - Training Epoch: 1/2, step 4007/23838 completed (loss: 0.5451019406318665, acc: 0.8399999737739563)
[2025-02-16 11:27:29,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:29,414][root][INFO] - Training Epoch: 1/2, step 4008/23838 completed (loss: 1.3371576070785522, acc: 0.6000000238418579)
[2025-02-16 11:27:29,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:29,829][root][INFO] - Training Epoch: 1/2, step 4009/23838 completed (loss: 1.1816996335983276, acc: 0.7200000286102295)
[2025-02-16 11:27:29,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:30,222][root][INFO] - Training Epoch: 1/2, step 4010/23838 completed (loss: 0.8668001890182495, acc: 0.7068965435028076)
[2025-02-16 11:27:30,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:30,658][root][INFO] - Training Epoch: 1/2, step 4011/23838 completed (loss: 1.0090022087097168, acc: 0.7731958627700806)
[2025-02-16 11:27:30,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:31,096][root][INFO] - Training Epoch: 1/2, step 4012/23838 completed (loss: 1.0409810543060303, acc: 0.6379310488700867)
[2025-02-16 11:27:31,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:31,539][root][INFO] - Training Epoch: 1/2, step 4013/23838 completed (loss: 1.1046828031539917, acc: 0.6595744490623474)
[2025-02-16 11:27:31,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:31,944][root][INFO] - Training Epoch: 1/2, step 4014/23838 completed (loss: 1.123403549194336, acc: 0.6829268336296082)
[2025-02-16 11:27:32,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:32,372][root][INFO] - Training Epoch: 1/2, step 4015/23838 completed (loss: 1.3931035995483398, acc: 0.5930232405662537)
[2025-02-16 11:27:32,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:32,741][root][INFO] - Training Epoch: 1/2, step 4016/23838 completed (loss: 0.993515133857727, acc: 0.7592592835426331)
[2025-02-16 11:27:32,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:33,210][root][INFO] - Training Epoch: 1/2, step 4017/23838 completed (loss: 0.7767766714096069, acc: 0.8139534592628479)
[2025-02-16 11:27:33,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:33,634][root][INFO] - Training Epoch: 1/2, step 4018/23838 completed (loss: 0.23695595562458038, acc: 0.9729729890823364)
[2025-02-16 11:27:33,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:34,067][root][INFO] - Training Epoch: 1/2, step 4019/23838 completed (loss: 0.35671931505203247, acc: 0.9173553586006165)
[2025-02-16 11:27:34,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:34,498][root][INFO] - Training Epoch: 1/2, step 4020/23838 completed (loss: 0.4068138897418976, acc: 0.9113923907279968)
[2025-02-16 11:27:34,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:34,919][root][INFO] - Training Epoch: 1/2, step 4021/23838 completed (loss: 0.3185403048992157, acc: 0.9607843160629272)
[2025-02-16 11:27:35,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:35,378][root][INFO] - Training Epoch: 1/2, step 4022/23838 completed (loss: 0.6712973117828369, acc: 0.8173912763595581)
[2025-02-16 11:27:35,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:35,756][root][INFO] - Training Epoch: 1/2, step 4023/23838 completed (loss: 0.2650975286960602, acc: 0.9069767594337463)
[2025-02-16 11:27:35,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:36,190][root][INFO] - Training Epoch: 1/2, step 4024/23838 completed (loss: 0.2750205993652344, acc: 0.9102563858032227)
[2025-02-16 11:27:36,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:36,678][root][INFO] - Training Epoch: 1/2, step 4025/23838 completed (loss: 0.31918084621429443, acc: 0.9333333373069763)
[2025-02-16 11:27:36,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:37,133][root][INFO] - Training Epoch: 1/2, step 4026/23838 completed (loss: 0.4459790289402008, acc: 0.8952381014823914)
[2025-02-16 11:27:37,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:37,585][root][INFO] - Training Epoch: 1/2, step 4027/23838 completed (loss: 0.4600569009780884, acc: 0.9014084339141846)
[2025-02-16 11:27:37,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:38,041][root][INFO] - Training Epoch: 1/2, step 4028/23838 completed (loss: 0.29276835918426514, acc: 0.9137930870056152)
[2025-02-16 11:27:38,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:38,481][root][INFO] - Training Epoch: 1/2, step 4029/23838 completed (loss: 0.6547146439552307, acc: 0.8260869383811951)
[2025-02-16 11:27:38,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:38,883][root][INFO] - Training Epoch: 1/2, step 4030/23838 completed (loss: 0.2594265043735504, acc: 0.9523809552192688)
[2025-02-16 11:27:39,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:39,362][root][INFO] - Training Epoch: 1/2, step 4031/23838 completed (loss: 0.33433282375335693, acc: 0.9186992049217224)
[2025-02-16 11:27:39,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:39,849][root][INFO] - Training Epoch: 1/2, step 4032/23838 completed (loss: 0.3172406256198883, acc: 0.9090909361839294)
[2025-02-16 11:27:39,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:40,212][root][INFO] - Training Epoch: 1/2, step 4033/23838 completed (loss: 0.5971974730491638, acc: 0.8958333134651184)
[2025-02-16 11:27:40,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:40,633][root][INFO] - Training Epoch: 1/2, step 4034/23838 completed (loss: 0.3249184787273407, acc: 0.9277108311653137)
[2025-02-16 11:27:40,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:41,054][root][INFO] - Training Epoch: 1/2, step 4035/23838 completed (loss: 0.5824516415596008, acc: 0.8347107172012329)
[2025-02-16 11:27:41,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:41,563][root][INFO] - Training Epoch: 1/2, step 4036/23838 completed (loss: 0.2853219211101532, acc: 0.9210526347160339)
[2025-02-16 11:27:41,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:42,267][root][INFO] - Training Epoch: 1/2, step 4037/23838 completed (loss: 0.5056484937667847, acc: 0.9007633328437805)
[2025-02-16 11:27:42,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:42,673][root][INFO] - Training Epoch: 1/2, step 4038/23838 completed (loss: 0.2808673083782196, acc: 0.9189189076423645)
[2025-02-16 11:27:42,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:43,084][root][INFO] - Training Epoch: 1/2, step 4039/23838 completed (loss: 0.5625584721565247, acc: 0.8205128312110901)
[2025-02-16 11:27:43,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:43,557][root][INFO] - Training Epoch: 1/2, step 4040/23838 completed (loss: 0.3067170977592468, acc: 0.9365079402923584)
[2025-02-16 11:27:43,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:44,208][root][INFO] - Training Epoch: 1/2, step 4041/23838 completed (loss: 0.34900176525115967, acc: 0.918367326259613)
[2025-02-16 11:27:44,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:44,625][root][INFO] - Training Epoch: 1/2, step 4042/23838 completed (loss: 0.30825692415237427, acc: 0.9468085169792175)
[2025-02-16 11:27:44,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:45,346][root][INFO] - Training Epoch: 1/2, step 4043/23838 completed (loss: 0.5507510304450989, acc: 0.8470588326454163)
[2025-02-16 11:27:45,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:45,887][root][INFO] - Training Epoch: 1/2, step 4044/23838 completed (loss: 0.21035516262054443, acc: 0.9247311949729919)
[2025-02-16 11:27:46,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:46,240][root][INFO] - Training Epoch: 1/2, step 4045/23838 completed (loss: 0.6244130730628967, acc: 0.8103448152542114)
[2025-02-16 11:27:46,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:46,722][root][INFO] - Training Epoch: 1/2, step 4046/23838 completed (loss: 0.45714113116264343, acc: 0.892307698726654)
[2025-02-16 11:27:46,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:47,158][root][INFO] - Training Epoch: 1/2, step 4047/23838 completed (loss: 0.7493385672569275, acc: 0.8571428656578064)
[2025-02-16 11:27:47,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:47,666][root][INFO] - Training Epoch: 1/2, step 4048/23838 completed (loss: 0.38863879442214966, acc: 0.875)
[2025-02-16 11:27:47,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:48,124][root][INFO] - Training Epoch: 1/2, step 4049/23838 completed (loss: 0.8139976859092712, acc: 0.692307710647583)
[2025-02-16 11:27:48,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:48,556][root][INFO] - Training Epoch: 1/2, step 4050/23838 completed (loss: 2.00956130027771, acc: 0.49056604504585266)
[2025-02-16 11:27:48,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:48,957][root][INFO] - Training Epoch: 1/2, step 4051/23838 completed (loss: 1.6086972951889038, acc: 0.6226415038108826)
[2025-02-16 11:27:49,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:49,393][root][INFO] - Training Epoch: 1/2, step 4052/23838 completed (loss: 1.209006905555725, acc: 0.6888889074325562)
[2025-02-16 11:27:49,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:49,868][root][INFO] - Training Epoch: 1/2, step 4053/23838 completed (loss: 1.9359782934188843, acc: 0.5757575631141663)
[2025-02-16 11:27:50,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:50,277][root][INFO] - Training Epoch: 1/2, step 4054/23838 completed (loss: 0.9486736059188843, acc: 0.717391312122345)
[2025-02-16 11:27:50,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:50,743][root][INFO] - Training Epoch: 1/2, step 4055/23838 completed (loss: 0.5775967240333557, acc: 0.8484848737716675)
[2025-02-16 11:27:50,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:51,142][root][INFO] - Training Epoch: 1/2, step 4056/23838 completed (loss: 0.4817495048046112, acc: 0.9090909361839294)
[2025-02-16 11:27:51,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:51,535][root][INFO] - Training Epoch: 1/2, step 4057/23838 completed (loss: 2.3063347339630127, acc: 0.20000000298023224)
[2025-02-16 11:27:51,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:51,939][root][INFO] - Training Epoch: 1/2, step 4058/23838 completed (loss: 0.6173158884048462, acc: 0.71875)
[2025-02-16 11:27:52,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:52,385][root][INFO] - Training Epoch: 1/2, step 4059/23838 completed (loss: 0.7932313680648804, acc: 0.8181818127632141)
[2025-02-16 11:27:52,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:52,827][root][INFO] - Training Epoch: 1/2, step 4060/23838 completed (loss: 1.9116913080215454, acc: 0.5897436141967773)
[2025-02-16 11:27:53,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:53,317][root][INFO] - Training Epoch: 1/2, step 4061/23838 completed (loss: 1.361862063407898, acc: 0.6410256624221802)
[2025-02-16 11:27:53,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:53,774][root][INFO] - Training Epoch: 1/2, step 4062/23838 completed (loss: 0.3867866098880768, acc: 0.9189189076423645)
[2025-02-16 11:27:53,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:54,216][root][INFO] - Training Epoch: 1/2, step 4063/23838 completed (loss: 0.6654736399650574, acc: 0.800000011920929)
[2025-02-16 11:27:54,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:54,643][root][INFO] - Training Epoch: 1/2, step 4064/23838 completed (loss: 1.000808596611023, acc: 0.7384615540504456)
[2025-02-16 11:27:54,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:55,071][root][INFO] - Training Epoch: 1/2, step 4065/23838 completed (loss: 0.9014840722084045, acc: 0.7407407164573669)
[2025-02-16 11:27:55,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:55,446][root][INFO] - Training Epoch: 1/2, step 4066/23838 completed (loss: 0.5232753753662109, acc: 0.8421052694320679)
[2025-02-16 11:27:55,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:55,841][root][INFO] - Training Epoch: 1/2, step 4067/23838 completed (loss: 0.8977587223052979, acc: 0.8260869383811951)
[2025-02-16 11:27:56,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:56,271][root][INFO] - Training Epoch: 1/2, step 4068/23838 completed (loss: 0.745258629322052, acc: 0.7777777910232544)
[2025-02-16 11:27:56,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:56,728][root][INFO] - Training Epoch: 1/2, step 4069/23838 completed (loss: 1.0278524160385132, acc: 0.7454545497894287)
[2025-02-16 11:27:56,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:57,179][root][INFO] - Training Epoch: 1/2, step 4070/23838 completed (loss: 0.7390277981758118, acc: 0.738095223903656)
[2025-02-16 11:27:57,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:57,629][root][INFO] - Training Epoch: 1/2, step 4071/23838 completed (loss: 0.8134890198707581, acc: 0.8387096524238586)
[2025-02-16 11:27:57,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:58,068][root][INFO] - Training Epoch: 1/2, step 4072/23838 completed (loss: 0.5273708701133728, acc: 0.8478260636329651)
[2025-02-16 11:27:58,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:58,488][root][INFO] - Training Epoch: 1/2, step 4073/23838 completed (loss: 0.9709333181381226, acc: 0.675000011920929)
[2025-02-16 11:27:58,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:58,886][root][INFO] - Training Epoch: 1/2, step 4074/23838 completed (loss: 1.1785478591918945, acc: 0.7209302186965942)
[2025-02-16 11:27:59,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:59,316][root][INFO] - Training Epoch: 1/2, step 4075/23838 completed (loss: 1.2803874015808105, acc: 0.699999988079071)
[2025-02-16 11:27:59,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:27:59,761][root][INFO] - Training Epoch: 1/2, step 4076/23838 completed (loss: 0.7700610756874084, acc: 0.8269230723381042)
[2025-02-16 11:27:59,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:00,254][root][INFO] - Training Epoch: 1/2, step 4077/23838 completed (loss: 0.8250226378440857, acc: 0.7631579041481018)
[2025-02-16 11:28:00,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:00,641][root][INFO] - Training Epoch: 1/2, step 4078/23838 completed (loss: 0.6135730147361755, acc: 0.7924528121948242)
[2025-02-16 11:28:00,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:01,077][root][INFO] - Training Epoch: 1/2, step 4079/23838 completed (loss: 0.5575461983680725, acc: 0.8524590134620667)
[2025-02-16 11:28:01,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:01,457][root][INFO] - Training Epoch: 1/2, step 4080/23838 completed (loss: 1.2063871622085571, acc: 0.5833333134651184)
[2025-02-16 11:28:01,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:01,911][root][INFO] - Training Epoch: 1/2, step 4081/23838 completed (loss: 1.1315373182296753, acc: 0.6666666865348816)
[2025-02-16 11:28:02,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:02,352][root][INFO] - Training Epoch: 1/2, step 4082/23838 completed (loss: 0.478458046913147, acc: 0.8965517282485962)
[2025-02-16 11:28:02,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:02,763][root][INFO] - Training Epoch: 1/2, step 4083/23838 completed (loss: 0.8224217891693115, acc: 0.7786885499954224)
[2025-02-16 11:28:02,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:03,144][root][INFO] - Training Epoch: 1/2, step 4084/23838 completed (loss: 0.720085859298706, acc: 0.7727272510528564)
[2025-02-16 11:28:03,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:03,553][root][INFO] - Training Epoch: 1/2, step 4085/23838 completed (loss: 0.9070897102355957, acc: 0.8214285969734192)
[2025-02-16 11:28:03,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:03,998][root][INFO] - Training Epoch: 1/2, step 4086/23838 completed (loss: 0.5073392987251282, acc: 0.8541666865348816)
[2025-02-16 11:28:04,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:04,376][root][INFO] - Training Epoch: 1/2, step 4087/23838 completed (loss: 0.7197561264038086, acc: 0.7916666865348816)
[2025-02-16 11:28:04,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:04,751][root][INFO] - Training Epoch: 1/2, step 4088/23838 completed (loss: 0.9221903681755066, acc: 0.8163265585899353)
[2025-02-16 11:28:04,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:05,269][root][INFO] - Training Epoch: 1/2, step 4089/23838 completed (loss: 0.9969541430473328, acc: 0.6865671873092651)
[2025-02-16 11:28:05,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:05,713][root][INFO] - Training Epoch: 1/2, step 4090/23838 completed (loss: 1.3242253065109253, acc: 0.6428571343421936)
[2025-02-16 11:28:05,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:06,170][root][INFO] - Training Epoch: 1/2, step 4091/23838 completed (loss: 0.6229453682899475, acc: 0.8461538553237915)
[2025-02-16 11:28:06,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:06,611][root][INFO] - Training Epoch: 1/2, step 4092/23838 completed (loss: 0.5320090055465698, acc: 0.875)
[2025-02-16 11:28:06,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:07,106][root][INFO] - Training Epoch: 1/2, step 4093/23838 completed (loss: 1.1283159255981445, acc: 0.7083333134651184)
[2025-02-16 11:28:07,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:07,561][root][INFO] - Training Epoch: 1/2, step 4094/23838 completed (loss: 0.9307020902633667, acc: 0.7333333492279053)
[2025-02-16 11:28:07,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:07,966][root][INFO] - Training Epoch: 1/2, step 4095/23838 completed (loss: 1.2107967138290405, acc: 0.6216216087341309)
[2025-02-16 11:28:08,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:08,528][root][INFO] - Training Epoch: 1/2, step 4096/23838 completed (loss: 0.8336791396141052, acc: 0.7948718070983887)
[2025-02-16 11:28:08,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:08,956][root][INFO] - Training Epoch: 1/2, step 4097/23838 completed (loss: 0.9426859021186829, acc: 0.8068181872367859)
[2025-02-16 11:28:09,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:09,315][root][INFO] - Training Epoch: 1/2, step 4098/23838 completed (loss: 1.086213231086731, acc: 0.7916666865348816)
[2025-02-16 11:28:09,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:09,752][root][INFO] - Training Epoch: 1/2, step 4099/23838 completed (loss: 0.9894682168960571, acc: 0.6551724076271057)
[2025-02-16 11:28:09,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:10,207][root][INFO] - Training Epoch: 1/2, step 4100/23838 completed (loss: 0.8051382899284363, acc: 0.760869562625885)
[2025-02-16 11:28:10,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:10,608][root][INFO] - Training Epoch: 1/2, step 4101/23838 completed (loss: 1.0047751665115356, acc: 0.739130437374115)
[2025-02-16 11:28:10,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:11,005][root][INFO] - Training Epoch: 1/2, step 4102/23838 completed (loss: 1.1012901067733765, acc: 0.739130437374115)
[2025-02-16 11:28:11,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:11,448][root][INFO] - Training Epoch: 1/2, step 4103/23838 completed (loss: 0.974172830581665, acc: 0.6987951993942261)
[2025-02-16 11:28:11,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:11,903][root][INFO] - Training Epoch: 1/2, step 4104/23838 completed (loss: 0.6197507977485657, acc: 0.8510638475418091)
[2025-02-16 11:28:12,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:12,345][root][INFO] - Training Epoch: 1/2, step 4105/23838 completed (loss: 0.5043476819992065, acc: 0.835616409778595)
[2025-02-16 11:28:12,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:12,757][root][INFO] - Training Epoch: 1/2, step 4106/23838 completed (loss: 0.9881037473678589, acc: 0.7872340679168701)
[2025-02-16 11:28:12,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:13,150][root][INFO] - Training Epoch: 1/2, step 4107/23838 completed (loss: 0.3436976373195648, acc: 0.9375)
[2025-02-16 11:28:13,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:13,588][root][INFO] - Training Epoch: 1/2, step 4108/23838 completed (loss: 0.3041943907737732, acc: 0.8989899158477783)
[2025-02-16 11:28:13,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:14,008][root][INFO] - Training Epoch: 1/2, step 4109/23838 completed (loss: 0.5917468667030334, acc: 0.8235294222831726)
[2025-02-16 11:28:14,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:14,426][root][INFO] - Training Epoch: 1/2, step 4110/23838 completed (loss: 0.5719434022903442, acc: 0.800000011920929)
[2025-02-16 11:28:14,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:14,909][root][INFO] - Training Epoch: 1/2, step 4111/23838 completed (loss: 0.6312756538391113, acc: 0.8118811845779419)
[2025-02-16 11:28:15,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:15,348][root][INFO] - Training Epoch: 1/2, step 4112/23838 completed (loss: 0.5112585425376892, acc: 0.8706896305084229)
[2025-02-16 11:28:15,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:15,918][root][INFO] - Training Epoch: 1/2, step 4113/23838 completed (loss: 0.4965474009513855, acc: 0.8791208863258362)
[2025-02-16 11:28:16,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:16,350][root][INFO] - Training Epoch: 1/2, step 4114/23838 completed (loss: 0.9240230917930603, acc: 0.75)
[2025-02-16 11:28:16,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:16,769][root][INFO] - Training Epoch: 1/2, step 4115/23838 completed (loss: 0.6918444037437439, acc: 0.8367347121238708)
[2025-02-16 11:28:16,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:17,207][root][INFO] - Training Epoch: 1/2, step 4116/23838 completed (loss: 0.6870355010032654, acc: 0.8374999761581421)
[2025-02-16 11:28:17,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:17,622][root][INFO] - Training Epoch: 1/2, step 4117/23838 completed (loss: 0.7621122598648071, acc: 0.7808219194412231)
[2025-02-16 11:28:17,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:18,104][root][INFO] - Training Epoch: 1/2, step 4118/23838 completed (loss: 0.8491902351379395, acc: 0.7580645084381104)
[2025-02-16 11:28:18,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:18,528][root][INFO] - Training Epoch: 1/2, step 4119/23838 completed (loss: 0.7543960809707642, acc: 0.8135592937469482)
[2025-02-16 11:28:18,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:18,932][root][INFO] - Training Epoch: 1/2, step 4120/23838 completed (loss: 0.5349905490875244, acc: 0.8421052694320679)
[2025-02-16 11:28:19,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:19,415][root][INFO] - Training Epoch: 1/2, step 4121/23838 completed (loss: 0.7583189606666565, acc: 0.8139534592628479)
[2025-02-16 11:28:19,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:19,850][root][INFO] - Training Epoch: 1/2, step 4122/23838 completed (loss: 0.9035655856132507, acc: 0.7479674816131592)
[2025-02-16 11:28:20,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:20,284][root][INFO] - Training Epoch: 1/2, step 4123/23838 completed (loss: 0.7780466675758362, acc: 0.7909091114997864)
[2025-02-16 11:28:20,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:20,675][root][INFO] - Training Epoch: 1/2, step 4124/23838 completed (loss: 0.4875960350036621, acc: 0.8297872543334961)
[2025-02-16 11:28:20,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:21,062][root][INFO] - Training Epoch: 1/2, step 4125/23838 completed (loss: 0.6280447840690613, acc: 0.8157894611358643)
[2025-02-16 11:28:21,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:21,493][root][INFO] - Training Epoch: 1/2, step 4126/23838 completed (loss: 0.6937021613121033, acc: 0.795918345451355)
[2025-02-16 11:28:21,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:21,963][root][INFO] - Training Epoch: 1/2, step 4127/23838 completed (loss: 0.6551159620285034, acc: 0.800000011920929)
[2025-02-16 11:28:22,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:22,396][root][INFO] - Training Epoch: 1/2, step 4128/23838 completed (loss: 0.42797383666038513, acc: 0.8947368264198303)
[2025-02-16 11:28:22,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:22,840][root][INFO] - Training Epoch: 1/2, step 4129/23838 completed (loss: 0.9248614311218262, acc: 0.734375)
[2025-02-16 11:28:23,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:23,285][root][INFO] - Training Epoch: 1/2, step 4130/23838 completed (loss: 0.6025415658950806, acc: 0.8392857313156128)
[2025-02-16 11:28:23,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:23,728][root][INFO] - Training Epoch: 1/2, step 4131/23838 completed (loss: 0.36025193333625793, acc: 0.9047619104385376)
[2025-02-16 11:28:23,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:24,250][root][INFO] - Training Epoch: 1/2, step 4132/23838 completed (loss: 0.9101709723472595, acc: 0.7745097875595093)
[2025-02-16 11:28:24,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:24,631][root][INFO] - Training Epoch: 1/2, step 4133/23838 completed (loss: 0.7918137311935425, acc: 0.7340425252914429)
[2025-02-16 11:28:24,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:25,044][root][INFO] - Training Epoch: 1/2, step 4134/23838 completed (loss: 0.8392701148986816, acc: 0.8125)
[2025-02-16 11:28:25,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:25,491][root][INFO] - Training Epoch: 1/2, step 4135/23838 completed (loss: 0.5831170082092285, acc: 0.837837815284729)
[2025-02-16 11:28:25,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:25,937][root][INFO] - Training Epoch: 1/2, step 4136/23838 completed (loss: 1.0856709480285645, acc: 0.7333333492279053)
[2025-02-16 11:28:26,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:26,402][root][INFO] - Training Epoch: 1/2, step 4137/23838 completed (loss: 0.6017054915428162, acc: 0.8103448152542114)
[2025-02-16 11:28:26,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:26,858][root][INFO] - Training Epoch: 1/2, step 4138/23838 completed (loss: 0.8634700179100037, acc: 0.7580645084381104)
[2025-02-16 11:28:27,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:27,315][root][INFO] - Training Epoch: 1/2, step 4139/23838 completed (loss: 0.6844760179519653, acc: 0.8478260636329651)
[2025-02-16 11:28:27,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:27,771][root][INFO] - Training Epoch: 1/2, step 4140/23838 completed (loss: 0.4450378119945526, acc: 0.8644067645072937)
[2025-02-16 11:28:27,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:28,191][root][INFO] - Training Epoch: 1/2, step 4141/23838 completed (loss: 1.0029562711715698, acc: 0.7111111283302307)
[2025-02-16 11:28:28,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:28,632][root][INFO] - Training Epoch: 1/2, step 4142/23838 completed (loss: 0.5389782786369324, acc: 0.8414633870124817)
[2025-02-16 11:28:28,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:29,065][root][INFO] - Training Epoch: 1/2, step 4143/23838 completed (loss: 0.8188301920890808, acc: 0.8253968358039856)
[2025-02-16 11:28:29,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:29,476][root][INFO] - Training Epoch: 1/2, step 4144/23838 completed (loss: 0.7541510462760925, acc: 0.8550724387168884)
[2025-02-16 11:28:29,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:29,845][root][INFO] - Training Epoch: 1/2, step 4145/23838 completed (loss: 0.5273466110229492, acc: 0.9024389982223511)
[2025-02-16 11:28:30,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:30,230][root][INFO] - Training Epoch: 1/2, step 4146/23838 completed (loss: 0.4920027256011963, acc: 0.8536585569381714)
[2025-02-16 11:28:30,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:30,624][root][INFO] - Training Epoch: 1/2, step 4147/23838 completed (loss: 0.9987190365791321, acc: 0.699999988079071)
[2025-02-16 11:28:30,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:31,025][root][INFO] - Training Epoch: 1/2, step 4148/23838 completed (loss: 0.43235260248184204, acc: 0.8787878751754761)
[2025-02-16 11:28:31,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:31,410][root][INFO] - Training Epoch: 1/2, step 4149/23838 completed (loss: 0.70879727602005, acc: 0.8117647171020508)
[2025-02-16 11:28:31,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:31,854][root][INFO] - Training Epoch: 1/2, step 4150/23838 completed (loss: 0.544407069683075, acc: 0.8441558480262756)
[2025-02-16 11:28:32,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:32,334][root][INFO] - Training Epoch: 1/2, step 4151/23838 completed (loss: 0.9519366025924683, acc: 0.752136766910553)
[2025-02-16 11:28:32,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:32,802][root][INFO] - Training Epoch: 1/2, step 4152/23838 completed (loss: 1.346944808959961, acc: 0.6640625)
[2025-02-16 11:28:32,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:33,218][root][INFO] - Training Epoch: 1/2, step 4153/23838 completed (loss: 1.399845004081726, acc: 0.6395348906517029)
[2025-02-16 11:28:33,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:33,636][root][INFO] - Training Epoch: 1/2, step 4154/23838 completed (loss: 1.6518893241882324, acc: 0.5)
[2025-02-16 11:28:33,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:34,046][root][INFO] - Training Epoch: 1/2, step 4155/23838 completed (loss: 1.338973879814148, acc: 0.5479452013969421)
[2025-02-16 11:28:34,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:34,517][root][INFO] - Training Epoch: 1/2, step 4156/23838 completed (loss: 1.3073294162750244, acc: 0.6261682510375977)
[2025-02-16 11:28:34,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:34,918][root][INFO] - Training Epoch: 1/2, step 4157/23838 completed (loss: 1.375911831855774, acc: 0.6481481194496155)
[2025-02-16 11:28:35,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:35,388][root][INFO] - Training Epoch: 1/2, step 4158/23838 completed (loss: 1.3985896110534668, acc: 0.5952380895614624)
[2025-02-16 11:28:35,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:35,829][root][INFO] - Training Epoch: 1/2, step 4159/23838 completed (loss: 1.4294006824493408, acc: 0.6274510025978088)
[2025-02-16 11:28:35,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:36,213][root][INFO] - Training Epoch: 1/2, step 4160/23838 completed (loss: 0.9923827052116394, acc: 0.6666666865348816)
[2025-02-16 11:28:36,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:36,596][root][INFO] - Training Epoch: 1/2, step 4161/23838 completed (loss: 0.9158580303192139, acc: 0.7560975551605225)
[2025-02-16 11:28:36,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:37,075][root][INFO] - Training Epoch: 1/2, step 4162/23838 completed (loss: 1.011755347251892, acc: 0.7142857313156128)
[2025-02-16 11:28:37,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:37,510][root][INFO] - Training Epoch: 1/2, step 4163/23838 completed (loss: 1.1134997606277466, acc: 0.6399999856948853)
[2025-02-16 11:28:37,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:37,924][root][INFO] - Training Epoch: 1/2, step 4164/23838 completed (loss: 1.6101871728897095, acc: 0.5593220591545105)
[2025-02-16 11:28:38,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:38,373][root][INFO] - Training Epoch: 1/2, step 4165/23838 completed (loss: 0.9562916159629822, acc: 0.7078651785850525)
[2025-02-16 11:28:38,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:38,779][root][INFO] - Training Epoch: 1/2, step 4166/23838 completed (loss: 1.3653091192245483, acc: 0.6219512224197388)
[2025-02-16 11:28:38,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:39,156][root][INFO] - Training Epoch: 1/2, step 4167/23838 completed (loss: 0.863581120967865, acc: 0.7446808218955994)
[2025-02-16 11:28:39,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:39,517][root][INFO] - Training Epoch: 1/2, step 4168/23838 completed (loss: 1.3600069284439087, acc: 0.6976743936538696)
[2025-02-16 11:28:39,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:39,953][root][INFO] - Training Epoch: 1/2, step 4169/23838 completed (loss: 1.2587941884994507, acc: 0.5555555820465088)
[2025-02-16 11:28:40,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:40,403][root][INFO] - Training Epoch: 1/2, step 4170/23838 completed (loss: 1.167175054550171, acc: 0.649350643157959)
[2025-02-16 11:28:40,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:40,791][root][INFO] - Training Epoch: 1/2, step 4171/23838 completed (loss: 0.9354498982429504, acc: 0.7017543911933899)
[2025-02-16 11:28:40,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:41,226][root][INFO] - Training Epoch: 1/2, step 4172/23838 completed (loss: 0.7345647215843201, acc: 0.8271604776382446)
[2025-02-16 11:28:41,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:41,641][root][INFO] - Training Epoch: 1/2, step 4173/23838 completed (loss: 0.8312767148017883, acc: 0.7586206793785095)
[2025-02-16 11:28:41,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:42,093][root][INFO] - Training Epoch: 1/2, step 4174/23838 completed (loss: 1.4716808795928955, acc: 0.5588235259056091)
[2025-02-16 11:28:42,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:42,529][root][INFO] - Training Epoch: 1/2, step 4175/23838 completed (loss: 1.0809258222579956, acc: 0.7234042286872864)
[2025-02-16 11:28:42,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:42,947][root][INFO] - Training Epoch: 1/2, step 4176/23838 completed (loss: 1.0824006795883179, acc: 0.7222222089767456)
[2025-02-16 11:28:43,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:43,398][root][INFO] - Training Epoch: 1/2, step 4177/23838 completed (loss: 1.280296802520752, acc: 0.6438356041908264)
[2025-02-16 11:28:43,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:43,805][root][INFO] - Training Epoch: 1/2, step 4178/23838 completed (loss: 1.444502353668213, acc: 0.5568181872367859)
[2025-02-16 11:28:44,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:44,250][root][INFO] - Training Epoch: 1/2, step 4179/23838 completed (loss: 1.379589557647705, acc: 0.5873016119003296)
[2025-02-16 11:28:44,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:44,666][root][INFO] - Training Epoch: 1/2, step 4180/23838 completed (loss: 1.0997350215911865, acc: 0.7216494679450989)
[2025-02-16 11:28:44,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:45,032][root][INFO] - Training Epoch: 1/2, step 4181/23838 completed (loss: 1.5796642303466797, acc: 0.5714285969734192)
[2025-02-16 11:28:45,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:45,465][root][INFO] - Training Epoch: 1/2, step 4182/23838 completed (loss: 1.3166640996932983, acc: 0.574999988079071)
[2025-02-16 11:28:45,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:45,932][root][INFO] - Training Epoch: 1/2, step 4183/23838 completed (loss: 1.2293769121170044, acc: 0.649350643157959)
[2025-02-16 11:28:46,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:46,390][root][INFO] - Training Epoch: 1/2, step 4184/23838 completed (loss: 1.0329495668411255, acc: 0.6944444179534912)
[2025-02-16 11:28:46,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:46,843][root][INFO] - Training Epoch: 1/2, step 4185/23838 completed (loss: 1.082211971282959, acc: 0.6428571343421936)
[2025-02-16 11:28:47,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:47,242][root][INFO] - Training Epoch: 1/2, step 4186/23838 completed (loss: 1.3505942821502686, acc: 0.6475409865379333)
[2025-02-16 11:28:47,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:47,614][root][INFO] - Training Epoch: 1/2, step 4187/23838 completed (loss: 1.2227116823196411, acc: 0.6617646813392639)
[2025-02-16 11:28:47,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:48,086][root][INFO] - Training Epoch: 1/2, step 4188/23838 completed (loss: 1.1894363164901733, acc: 0.6521739363670349)
[2025-02-16 11:28:48,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:48,525][root][INFO] - Training Epoch: 1/2, step 4189/23838 completed (loss: 1.2575732469558716, acc: 0.6172839403152466)
[2025-02-16 11:28:48,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:48,972][root][INFO] - Training Epoch: 1/2, step 4190/23838 completed (loss: 1.1468888521194458, acc: 0.6315789222717285)
[2025-02-16 11:28:49,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:49,400][root][INFO] - Training Epoch: 1/2, step 4191/23838 completed (loss: 1.1208422183990479, acc: 0.6790123581886292)
[2025-02-16 11:28:49,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:49,792][root][INFO] - Training Epoch: 1/2, step 4192/23838 completed (loss: 1.329013705253601, acc: 0.6538461446762085)
[2025-02-16 11:28:50,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:50,254][root][INFO] - Training Epoch: 1/2, step 4193/23838 completed (loss: 1.4513648748397827, acc: 0.6202531456947327)
[2025-02-16 11:28:50,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:50,696][root][INFO] - Training Epoch: 1/2, step 4194/23838 completed (loss: 1.6405490636825562, acc: 0.5714285969734192)
[2025-02-16 11:28:50,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:51,085][root][INFO] - Training Epoch: 1/2, step 4195/23838 completed (loss: 1.2437533140182495, acc: 0.5789473652839661)
[2025-02-16 11:28:51,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:51,498][root][INFO] - Training Epoch: 1/2, step 4196/23838 completed (loss: 1.006982684135437, acc: 0.7215189933776855)
[2025-02-16 11:28:51,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:51,880][root][INFO] - Training Epoch: 1/2, step 4197/23838 completed (loss: 1.41762113571167, acc: 0.6263736486434937)
[2025-02-16 11:28:52,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:52,342][root][INFO] - Training Epoch: 1/2, step 4198/23838 completed (loss: 1.184095025062561, acc: 0.6760563254356384)
[2025-02-16 11:28:52,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:52,821][root][INFO] - Training Epoch: 1/2, step 4199/23838 completed (loss: 1.2446794509887695, acc: 0.625)
[2025-02-16 11:28:53,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:53,288][root][INFO] - Training Epoch: 1/2, step 4200/23838 completed (loss: 1.2445478439331055, acc: 0.6883116960525513)
[2025-02-16 11:28:53,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:53,756][root][INFO] - Training Epoch: 1/2, step 4201/23838 completed (loss: 1.3987478017807007, acc: 0.6195651888847351)
[2025-02-16 11:28:53,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:54,216][root][INFO] - Training Epoch: 1/2, step 4202/23838 completed (loss: 1.1502822637557983, acc: 0.6526315808296204)
[2025-02-16 11:28:54,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:54,607][root][INFO] - Training Epoch: 1/2, step 4203/23838 completed (loss: 1.1509133577346802, acc: 0.7108433842658997)
[2025-02-16 11:28:54,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:55,078][root][INFO] - Training Epoch: 1/2, step 4204/23838 completed (loss: 1.586507797241211, acc: 0.5405405163764954)
[2025-02-16 11:28:55,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:55,507][root][INFO] - Training Epoch: 1/2, step 4205/23838 completed (loss: 1.3792279958724976, acc: 0.5874999761581421)
[2025-02-16 11:28:55,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:55,937][root][INFO] - Training Epoch: 1/2, step 4206/23838 completed (loss: 1.5153679847717285, acc: 0.6018518805503845)
[2025-02-16 11:28:56,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:56,387][root][INFO] - Training Epoch: 1/2, step 4207/23838 completed (loss: 1.4348191022872925, acc: 0.5871559381484985)
[2025-02-16 11:28:56,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:56,848][root][INFO] - Training Epoch: 1/2, step 4208/23838 completed (loss: 1.0916194915771484, acc: 0.6666666865348816)
[2025-02-16 11:28:57,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:57,310][root][INFO] - Training Epoch: 1/2, step 4209/23838 completed (loss: 1.2194383144378662, acc: 0.6285714507102966)
[2025-02-16 11:28:57,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:57,677][root][INFO] - Training Epoch: 1/2, step 4210/23838 completed (loss: 1.1473414897918701, acc: 0.6857143044471741)
[2025-02-16 11:28:57,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:58,102][root][INFO] - Training Epoch: 1/2, step 4211/23838 completed (loss: 1.0281548500061035, acc: 0.7727272510528564)
[2025-02-16 11:28:58,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:58,542][root][INFO] - Training Epoch: 1/2, step 4212/23838 completed (loss: 0.9858132004737854, acc: 0.7415730357170105)
[2025-02-16 11:28:58,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:59,032][root][INFO] - Training Epoch: 1/2, step 4213/23838 completed (loss: 0.9325540065765381, acc: 0.7272727489471436)
[2025-02-16 11:28:59,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:59,428][root][INFO] - Training Epoch: 1/2, step 4214/23838 completed (loss: 0.8487039804458618, acc: 0.8421052694320679)
[2025-02-16 11:28:59,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:28:59,906][root][INFO] - Training Epoch: 1/2, step 4215/23838 completed (loss: 1.3400602340698242, acc: 0.6037735939025879)
[2025-02-16 11:29:00,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:00,329][root][INFO] - Training Epoch: 1/2, step 4216/23838 completed (loss: 0.8654778003692627, acc: 0.6823529601097107)
[2025-02-16 11:29:00,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:00,700][root][INFO] - Training Epoch: 1/2, step 4217/23838 completed (loss: 0.8841800689697266, acc: 0.7857142686843872)
[2025-02-16 11:29:00,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:01,084][root][INFO] - Training Epoch: 1/2, step 4218/23838 completed (loss: 1.5523451566696167, acc: 0.47560974955558777)
[2025-02-16 11:29:01,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:01,463][root][INFO] - Training Epoch: 1/2, step 4219/23838 completed (loss: 0.9257917404174805, acc: 0.75)
[2025-02-16 11:29:01,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:02,164][root][INFO] - Training Epoch: 1/2, step 4220/23838 completed (loss: 1.074772834777832, acc: 0.6951219439506531)
[2025-02-16 11:29:02,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:02,590][root][INFO] - Training Epoch: 1/2, step 4221/23838 completed (loss: 1.2475942373275757, acc: 0.6470588445663452)
[2025-02-16 11:29:02,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:03,067][root][INFO] - Training Epoch: 1/2, step 4222/23838 completed (loss: 1.1090799570083618, acc: 0.6504854559898376)
[2025-02-16 11:29:03,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:03,543][root][INFO] - Training Epoch: 1/2, step 4223/23838 completed (loss: 1.430798053741455, acc: 0.654321014881134)
[2025-02-16 11:29:03,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:04,021][root][INFO] - Training Epoch: 1/2, step 4224/23838 completed (loss: 0.9798916578292847, acc: 0.7528089880943298)
[2025-02-16 11:29:04,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:04,568][root][INFO] - Training Epoch: 1/2, step 4225/23838 completed (loss: 1.0908753871917725, acc: 0.7200000286102295)
[2025-02-16 11:29:04,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:05,038][root][INFO] - Training Epoch: 1/2, step 4226/23838 completed (loss: 0.8138229846954346, acc: 0.800000011920929)
[2025-02-16 11:29:05,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:05,475][root][INFO] - Training Epoch: 1/2, step 4227/23838 completed (loss: 0.41841253638267517, acc: 0.9076923131942749)
[2025-02-16 11:29:05,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:05,866][root][INFO] - Training Epoch: 1/2, step 4228/23838 completed (loss: 0.44962078332901, acc: 0.875)
[2025-02-16 11:29:06,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:06,293][root][INFO] - Training Epoch: 1/2, step 4229/23838 completed (loss: 0.5988107919692993, acc: 0.8558558821678162)
[2025-02-16 11:29:06,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:06,717][root][INFO] - Training Epoch: 1/2, step 4230/23838 completed (loss: 0.3139091432094574, acc: 0.9166666865348816)
[2025-02-16 11:29:06,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:07,132][root][INFO] - Training Epoch: 1/2, step 4231/23838 completed (loss: 0.4705459475517273, acc: 0.8604651093482971)
[2025-02-16 11:29:07,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:07,534][root][INFO] - Training Epoch: 1/2, step 4232/23838 completed (loss: 0.6959508061408997, acc: 0.7987421154975891)
[2025-02-16 11:29:07,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:07,975][root][INFO] - Training Epoch: 1/2, step 4233/23838 completed (loss: 0.5133034586906433, acc: 0.8518518805503845)
[2025-02-16 11:29:08,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:08,440][root][INFO] - Training Epoch: 1/2, step 4234/23838 completed (loss: 0.5088189244270325, acc: 0.884353756904602)
[2025-02-16 11:29:08,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:08,913][root][INFO] - Training Epoch: 1/2, step 4235/23838 completed (loss: 0.5096486806869507, acc: 0.8833333253860474)
[2025-02-16 11:29:09,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:09,329][root][INFO] - Training Epoch: 1/2, step 4236/23838 completed (loss: 0.7106029987335205, acc: 0.7916666865348816)
[2025-02-16 11:29:09,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:09,798][root][INFO] - Training Epoch: 1/2, step 4237/23838 completed (loss: 0.5952982306480408, acc: 0.8372092843055725)
[2025-02-16 11:29:10,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:10,235][root][INFO] - Training Epoch: 1/2, step 4238/23838 completed (loss: 0.5062060952186584, acc: 0.8285714387893677)
[2025-02-16 11:29:10,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:10,611][root][INFO] - Training Epoch: 1/2, step 4239/23838 completed (loss: 0.6930557489395142, acc: 0.8051947951316833)
[2025-02-16 11:29:11,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:11,529][root][INFO] - Training Epoch: 1/2, step 4240/23838 completed (loss: 0.5794171690940857, acc: 0.8504672646522522)
[2025-02-16 11:29:11,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:11,908][root][INFO] - Training Epoch: 1/2, step 4241/23838 completed (loss: 0.9675827026367188, acc: 0.6739130616188049)
[2025-02-16 11:29:12,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:12,426][root][INFO] - Training Epoch: 1/2, step 4242/23838 completed (loss: 0.7086536884307861, acc: 0.7986111044883728)
[2025-02-16 11:29:12,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:12,939][root][INFO] - Training Epoch: 1/2, step 4243/23838 completed (loss: 0.6067635416984558, acc: 0.8064516186714172)
[2025-02-16 11:29:13,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:13,438][root][INFO] - Training Epoch: 1/2, step 4244/23838 completed (loss: 0.48832786083221436, acc: 0.8852459192276001)
[2025-02-16 11:29:13,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:14,140][root][INFO] - Training Epoch: 1/2, step 4245/23838 completed (loss: 0.5759140849113464, acc: 0.8592592477798462)
[2025-02-16 11:29:14,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:14,782][root][INFO] - Training Epoch: 1/2, step 4246/23838 completed (loss: 0.5463907122612, acc: 0.876288652420044)
[2025-02-16 11:29:15,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:15,259][root][INFO] - Training Epoch: 1/2, step 4247/23838 completed (loss: 0.7333360910415649, acc: 0.8217821717262268)
[2025-02-16 11:29:15,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:15,715][root][INFO] - Training Epoch: 1/2, step 4248/23838 completed (loss: 0.798614501953125, acc: 0.7857142686843872)
[2025-02-16 11:29:15,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:16,150][root][INFO] - Training Epoch: 1/2, step 4249/23838 completed (loss: 0.40911468863487244, acc: 0.8823529481887817)
[2025-02-16 11:29:16,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:16,646][root][INFO] - Training Epoch: 1/2, step 4250/23838 completed (loss: 0.40105029940605164, acc: 0.8543689250946045)
[2025-02-16 11:29:17,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:17,371][root][INFO] - Training Epoch: 1/2, step 4251/23838 completed (loss: 0.8232370018959045, acc: 0.7575757503509521)
[2025-02-16 11:29:17,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:17,730][root][INFO] - Training Epoch: 1/2, step 4252/23838 completed (loss: 0.7138295769691467, acc: 0.8450704216957092)
[2025-02-16 11:29:17,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:18,216][root][INFO] - Training Epoch: 1/2, step 4253/23838 completed (loss: 0.7814772129058838, acc: 0.75)
[2025-02-16 11:29:18,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:18,625][root][INFO] - Training Epoch: 1/2, step 4254/23838 completed (loss: 0.7600876092910767, acc: 0.7647058963775635)
[2025-02-16 11:29:18,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:19,056][root][INFO] - Training Epoch: 1/2, step 4255/23838 completed (loss: 0.33859965205192566, acc: 0.920634925365448)
[2025-02-16 11:29:19,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:19,472][root][INFO] - Training Epoch: 1/2, step 4256/23838 completed (loss: 0.5090045928955078, acc: 0.8705882430076599)
[2025-02-16 11:29:19,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:20,126][root][INFO] - Training Epoch: 1/2, step 4257/23838 completed (loss: 0.8062835335731506, acc: 0.7721518874168396)
[2025-02-16 11:29:20,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:20,651][root][INFO] - Training Epoch: 1/2, step 4258/23838 completed (loss: 0.39891454577445984, acc: 0.8999999761581421)
[2025-02-16 11:29:20,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:21,151][root][INFO] - Training Epoch: 1/2, step 4259/23838 completed (loss: 0.838022768497467, acc: 0.7183098793029785)
[2025-02-16 11:29:21,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:21,920][root][INFO] - Training Epoch: 1/2, step 4260/23838 completed (loss: 0.5840805172920227, acc: 0.8365384340286255)
[2025-02-16 11:29:22,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:22,593][root][INFO] - Training Epoch: 1/2, step 4261/23838 completed (loss: 0.3642033636569977, acc: 0.8691588640213013)
[2025-02-16 11:29:22,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:23,420][root][INFO] - Training Epoch: 1/2, step 4262/23838 completed (loss: 0.5493362545967102, acc: 0.8176100850105286)
[2025-02-16 11:29:23,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:24,039][root][INFO] - Training Epoch: 1/2, step 4263/23838 completed (loss: 0.40888461470603943, acc: 0.8782051205635071)
[2025-02-16 11:29:24,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:24,496][root][INFO] - Training Epoch: 1/2, step 4264/23838 completed (loss: 0.6078031659126282, acc: 0.8414633870124817)
[2025-02-16 11:29:24,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:25,009][root][INFO] - Training Epoch: 1/2, step 4265/23838 completed (loss: 0.7748333215713501, acc: 0.7757009267807007)
[2025-02-16 11:29:25,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:25,408][root][INFO] - Training Epoch: 1/2, step 4266/23838 completed (loss: 1.7770811319351196, acc: 0.4523809552192688)
[2025-02-16 11:29:25,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:25,871][root][INFO] - Training Epoch: 1/2, step 4267/23838 completed (loss: 1.1860748529434204, acc: 0.6732673048973083)
[2025-02-16 11:29:26,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:26,236][root][INFO] - Training Epoch: 1/2, step 4268/23838 completed (loss: 1.3585082292556763, acc: 0.7115384340286255)
[2025-02-16 11:29:26,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:26,648][root][INFO] - Training Epoch: 1/2, step 4269/23838 completed (loss: 1.0843170881271362, acc: 0.707317054271698)
[2025-02-16 11:29:26,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:27,054][root][INFO] - Training Epoch: 1/2, step 4270/23838 completed (loss: 1.3951972723007202, acc: 0.6415094137191772)
[2025-02-16 11:29:27,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:27,524][root][INFO] - Training Epoch: 1/2, step 4271/23838 completed (loss: 1.2025946378707886, acc: 0.6888889074325562)
[2025-02-16 11:29:27,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:27,919][root][INFO] - Training Epoch: 1/2, step 4272/23838 completed (loss: 1.1862024068832397, acc: 0.6578947305679321)
[2025-02-16 11:29:28,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:28,370][root][INFO] - Training Epoch: 1/2, step 4273/23838 completed (loss: 0.9926015138626099, acc: 0.6951219439506531)
[2025-02-16 11:29:28,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:28,806][root][INFO] - Training Epoch: 1/2, step 4274/23838 completed (loss: 1.349265694618225, acc: 0.5714285969734192)
[2025-02-16 11:29:28,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:29,254][root][INFO] - Training Epoch: 1/2, step 4275/23838 completed (loss: 1.1198711395263672, acc: 0.686274528503418)
[2025-02-16 11:29:29,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:29,693][root][INFO] - Training Epoch: 1/2, step 4276/23838 completed (loss: 0.8589653372764587, acc: 0.7916666865348816)
[2025-02-16 11:29:29,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:30,085][root][INFO] - Training Epoch: 1/2, step 4277/23838 completed (loss: 0.7744602560997009, acc: 0.7906976938247681)
[2025-02-16 11:29:30,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:30,498][root][INFO] - Training Epoch: 1/2, step 4278/23838 completed (loss: 1.1418505907058716, acc: 0.7142857313156128)
[2025-02-16 11:29:30,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:30,867][root][INFO] - Training Epoch: 1/2, step 4279/23838 completed (loss: 1.2492822408676147, acc: 0.6153846383094788)
[2025-02-16 11:29:31,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:31,273][root][INFO] - Training Epoch: 1/2, step 4280/23838 completed (loss: 0.9069761037826538, acc: 0.7169811129570007)
[2025-02-16 11:29:31,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:31,691][root][INFO] - Training Epoch: 1/2, step 4281/23838 completed (loss: 0.8011031746864319, acc: 0.7307692170143127)
[2025-02-16 11:29:31,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:32,132][root][INFO] - Training Epoch: 1/2, step 4282/23838 completed (loss: 1.4388208389282227, acc: 0.6279069781303406)
[2025-02-16 11:29:32,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:32,574][root][INFO] - Training Epoch: 1/2, step 4283/23838 completed (loss: 0.8628547787666321, acc: 0.75)
[2025-02-16 11:29:32,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:33,011][root][INFO] - Training Epoch: 1/2, step 4284/23838 completed (loss: 1.3044133186340332, acc: 0.578125)
[2025-02-16 11:29:33,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:33,411][root][INFO] - Training Epoch: 1/2, step 4285/23838 completed (loss: 1.1420871019363403, acc: 0.6857143044471741)
[2025-02-16 11:29:33,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:33,796][root][INFO] - Training Epoch: 1/2, step 4286/23838 completed (loss: 0.8627910017967224, acc: 0.7400000095367432)
[2025-02-16 11:29:33,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:34,201][root][INFO] - Training Epoch: 1/2, step 4287/23838 completed (loss: 1.669340968132019, acc: 0.45098039507865906)
[2025-02-16 11:29:34,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:34,568][root][INFO] - Training Epoch: 1/2, step 4288/23838 completed (loss: 0.8793651461601257, acc: 0.7272727489471436)
[2025-02-16 11:29:34,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:34,945][root][INFO] - Training Epoch: 1/2, step 4289/23838 completed (loss: 1.016210913658142, acc: 0.7321428656578064)
[2025-02-16 11:29:35,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:35,398][root][INFO] - Training Epoch: 1/2, step 4290/23838 completed (loss: 1.0279695987701416, acc: 0.7291666865348816)
[2025-02-16 11:29:35,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:35,790][root][INFO] - Training Epoch: 1/2, step 4291/23838 completed (loss: 1.024993658065796, acc: 0.6190476417541504)
[2025-02-16 11:29:35,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:36,210][root][INFO] - Training Epoch: 1/2, step 4292/23838 completed (loss: 0.8019424676895142, acc: 0.75)
[2025-02-16 11:29:36,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:36,615][root][INFO] - Training Epoch: 1/2, step 4293/23838 completed (loss: 0.8685385584831238, acc: 0.725806474685669)
[2025-02-16 11:29:36,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:37,100][root][INFO] - Training Epoch: 1/2, step 4294/23838 completed (loss: 0.646152138710022, acc: 0.7976190447807312)
[2025-02-16 11:29:37,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:37,505][root][INFO] - Training Epoch: 1/2, step 4295/23838 completed (loss: 0.8406959772109985, acc: 0.7749999761581421)
[2025-02-16 11:29:37,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:38,019][root][INFO] - Training Epoch: 1/2, step 4296/23838 completed (loss: 0.6714663505554199, acc: 0.8461538553237915)
[2025-02-16 11:29:38,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:38,445][root][INFO] - Training Epoch: 1/2, step 4297/23838 completed (loss: 1.3962467908859253, acc: 0.6399999856948853)
[2025-02-16 11:29:38,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:38,899][root][INFO] - Training Epoch: 1/2, step 4298/23838 completed (loss: 1.0359200239181519, acc: 0.6111111044883728)
[2025-02-16 11:29:39,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:39,346][root][INFO] - Training Epoch: 1/2, step 4299/23838 completed (loss: 0.7161243557929993, acc: 0.8037382960319519)
[2025-02-16 11:29:39,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:39,800][root][INFO] - Training Epoch: 1/2, step 4300/23838 completed (loss: 0.7373270392417908, acc: 0.759036123752594)
[2025-02-16 11:29:39,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:40,215][root][INFO] - Training Epoch: 1/2, step 4301/23838 completed (loss: 1.340638279914856, acc: 0.6071428656578064)
[2025-02-16 11:29:40,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:40,647][root][INFO] - Training Epoch: 1/2, step 4302/23838 completed (loss: 0.7764672636985779, acc: 0.7981651425361633)
[2025-02-16 11:29:40,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:41,141][root][INFO] - Training Epoch: 1/2, step 4303/23838 completed (loss: 0.9964029788970947, acc: 0.7099999785423279)
[2025-02-16 11:29:41,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:41,593][root][INFO] - Training Epoch: 1/2, step 4304/23838 completed (loss: 0.9681066274642944, acc: 0.737500011920929)
[2025-02-16 11:29:41,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:42,182][root][INFO] - Training Epoch: 1/2, step 4305/23838 completed (loss: 1.0582181215286255, acc: 0.7319587469100952)
[2025-02-16 11:29:42,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:42,670][root][INFO] - Training Epoch: 1/2, step 4306/23838 completed (loss: 2.2681546211242676, acc: 0.5111111402511597)
[2025-02-16 11:29:42,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:43,137][root][INFO] - Training Epoch: 1/2, step 4307/23838 completed (loss: 1.267501711845398, acc: 0.6440678238868713)
[2025-02-16 11:29:43,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:43,592][root][INFO] - Training Epoch: 1/2, step 4308/23838 completed (loss: 1.0219306945800781, acc: 0.7777777910232544)
[2025-02-16 11:29:43,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:44,029][root][INFO] - Training Epoch: 1/2, step 4309/23838 completed (loss: 1.2598211765289307, acc: 0.5967742204666138)
[2025-02-16 11:29:44,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:44,452][root][INFO] - Training Epoch: 1/2, step 4310/23838 completed (loss: 1.0954718589782715, acc: 0.6909090876579285)
[2025-02-16 11:29:44,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:44,923][root][INFO] - Training Epoch: 1/2, step 4311/23838 completed (loss: 1.0835133790969849, acc: 0.7209302186965942)
[2025-02-16 11:29:45,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:45,392][root][INFO] - Training Epoch: 1/2, step 4312/23838 completed (loss: 1.0410187244415283, acc: 0.6666666865348816)
[2025-02-16 11:29:45,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:45,842][root][INFO] - Training Epoch: 1/2, step 4313/23838 completed (loss: 1.204053521156311, acc: 0.6911764740943909)
[2025-02-16 11:29:46,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:46,329][root][INFO] - Training Epoch: 1/2, step 4314/23838 completed (loss: 0.9797282218933105, acc: 0.7313432693481445)
[2025-02-16 11:29:46,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:46,803][root][INFO] - Training Epoch: 1/2, step 4315/23838 completed (loss: 1.1084545850753784, acc: 0.6296296119689941)
[2025-02-16 11:29:47,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:47,273][root][INFO] - Training Epoch: 1/2, step 4316/23838 completed (loss: 0.6881937980651855, acc: 0.8113207817077637)
[2025-02-16 11:29:47,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:47,720][root][INFO] - Training Epoch: 1/2, step 4317/23838 completed (loss: 0.9042767882347107, acc: 0.7735849022865295)
[2025-02-16 11:29:47,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:48,144][root][INFO] - Training Epoch: 1/2, step 4318/23838 completed (loss: 0.558310866355896, acc: 0.8181818127632141)
[2025-02-16 11:29:48,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:48,543][root][INFO] - Training Epoch: 1/2, step 4319/23838 completed (loss: 0.49935150146484375, acc: 0.8363636136054993)
[2025-02-16 11:29:48,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:48,953][root][INFO] - Training Epoch: 1/2, step 4320/23838 completed (loss: 0.8285929560661316, acc: 0.7857142686843872)
[2025-02-16 11:29:49,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:49,362][root][INFO] - Training Epoch: 1/2, step 4321/23838 completed (loss: 0.6888602375984192, acc: 0.8928571343421936)
[2025-02-16 11:29:49,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:49,819][root][INFO] - Training Epoch: 1/2, step 4322/23838 completed (loss: 0.7582889795303345, acc: 0.8222222328186035)
[2025-02-16 11:29:50,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:50,295][root][INFO] - Training Epoch: 1/2, step 4323/23838 completed (loss: 0.7576088905334473, acc: 0.779411792755127)
[2025-02-16 11:29:50,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:50,754][root][INFO] - Training Epoch: 1/2, step 4324/23838 completed (loss: 0.5849870443344116, acc: 0.7837837934494019)
[2025-02-16 11:29:51,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:51,354][root][INFO] - Training Epoch: 1/2, step 4325/23838 completed (loss: 1.4530234336853027, acc: 0.5671641826629639)
[2025-02-16 11:29:51,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:51,804][root][INFO] - Training Epoch: 1/2, step 4326/23838 completed (loss: 1.7740466594696045, acc: 0.515625)
[2025-02-16 11:29:52,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:52,238][root][INFO] - Training Epoch: 1/2, step 4327/23838 completed (loss: 1.3956055641174316, acc: 0.625)
[2025-02-16 11:29:52,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:52,677][root][INFO] - Training Epoch: 1/2, step 4328/23838 completed (loss: 0.5172224044799805, acc: 0.8947368264198303)
[2025-02-16 11:29:52,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:53,129][root][INFO] - Training Epoch: 1/2, step 4329/23838 completed (loss: 0.8401110172271729, acc: 0.7745097875595093)
[2025-02-16 11:29:53,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:53,630][root][INFO] - Training Epoch: 1/2, step 4330/23838 completed (loss: 1.2431381940841675, acc: 0.7083333134651184)
[2025-02-16 11:29:53,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:54,047][root][INFO] - Training Epoch: 1/2, step 4331/23838 completed (loss: 1.5051047801971436, acc: 0.5869565010070801)
[2025-02-16 11:29:54,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:54,484][root][INFO] - Training Epoch: 1/2, step 4332/23838 completed (loss: 1.2351981401443481, acc: 0.5454545617103577)
[2025-02-16 11:29:54,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:54,889][root][INFO] - Training Epoch: 1/2, step 4333/23838 completed (loss: 1.2132881879806519, acc: 0.6206896305084229)
[2025-02-16 11:29:55,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:55,260][root][INFO] - Training Epoch: 1/2, step 4334/23838 completed (loss: 1.174625277519226, acc: 0.6585366129875183)
[2025-02-16 11:29:55,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:55,705][root][INFO] - Training Epoch: 1/2, step 4335/23838 completed (loss: 1.1908454895019531, acc: 0.6086956262588501)
[2025-02-16 11:29:55,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:56,153][root][INFO] - Training Epoch: 1/2, step 4336/23838 completed (loss: 0.8436633944511414, acc: 0.7272727489471436)
[2025-02-16 11:29:56,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:56,556][root][INFO] - Training Epoch: 1/2, step 4337/23838 completed (loss: 0.5758626461029053, acc: 0.7931034564971924)
[2025-02-16 11:29:56,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:57,034][root][INFO] - Training Epoch: 1/2, step 4338/23838 completed (loss: 0.34719812870025635, acc: 0.8799999952316284)
[2025-02-16 11:29:57,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:57,506][root][INFO] - Training Epoch: 1/2, step 4339/23838 completed (loss: 0.9306575655937195, acc: 0.75)
[2025-02-16 11:29:57,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:57,983][root][INFO] - Training Epoch: 1/2, step 4340/23838 completed (loss: 0.4087735712528229, acc: 0.8513513803482056)
[2025-02-16 11:29:58,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:58,406][root][INFO] - Training Epoch: 1/2, step 4341/23838 completed (loss: 1.8027925491333008, acc: 0.6086956262588501)
[2025-02-16 11:29:58,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:58,815][root][INFO] - Training Epoch: 1/2, step 4342/23838 completed (loss: 0.6811676621437073, acc: 0.7833333611488342)
[2025-02-16 11:29:58,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:59,192][root][INFO] - Training Epoch: 1/2, step 4343/23838 completed (loss: 0.6884715557098389, acc: 0.7941176295280457)
[2025-02-16 11:29:59,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:29:59,625][root][INFO] - Training Epoch: 1/2, step 4344/23838 completed (loss: 0.7646903991699219, acc: 0.75)
[2025-02-16 11:29:59,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:00,018][root][INFO] - Training Epoch: 1/2, step 4345/23838 completed (loss: 0.8725659847259521, acc: 0.800000011920929)
[2025-02-16 11:30:00,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:00,398][root][INFO] - Training Epoch: 1/2, step 4346/23838 completed (loss: 1.0198384523391724, acc: 0.7285714149475098)
[2025-02-16 11:30:00,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:00,791][root][INFO] - Training Epoch: 1/2, step 4347/23838 completed (loss: 1.0919958353042603, acc: 0.7090908885002136)
[2025-02-16 11:30:01,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:01,257][root][INFO] - Training Epoch: 1/2, step 4348/23838 completed (loss: 0.6341990232467651, acc: 0.800000011920929)
[2025-02-16 11:30:01,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:01,760][root][INFO] - Training Epoch: 1/2, step 4349/23838 completed (loss: 1.0488439798355103, acc: 0.7246376872062683)
[2025-02-16 11:30:02,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:02,267][root][INFO] - Training Epoch: 1/2, step 4350/23838 completed (loss: 0.9456747770309448, acc: 0.7115384340286255)
[2025-02-16 11:30:02,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:02,719][root][INFO] - Training Epoch: 1/2, step 4351/23838 completed (loss: 0.6005308032035828, acc: 0.8867924809455872)
[2025-02-16 11:30:02,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:03,183][root][INFO] - Training Epoch: 1/2, step 4352/23838 completed (loss: 0.8917413353919983, acc: 0.734375)
[2025-02-16 11:30:03,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:03,647][root][INFO] - Training Epoch: 1/2, step 4353/23838 completed (loss: 0.7230889201164246, acc: 0.75)
[2025-02-16 11:30:03,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:04,108][root][INFO] - Training Epoch: 1/2, step 4354/23838 completed (loss: 0.7726840376853943, acc: 0.7317073345184326)
[2025-02-16 11:30:04,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:04,542][root][INFO] - Training Epoch: 1/2, step 4355/23838 completed (loss: 0.8194399476051331, acc: 0.7727272510528564)
[2025-02-16 11:30:04,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:04,929][root][INFO] - Training Epoch: 1/2, step 4356/23838 completed (loss: 1.0431926250457764, acc: 0.6875)
[2025-02-16 11:30:05,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:05,379][root][INFO] - Training Epoch: 1/2, step 4357/23838 completed (loss: 1.0226978063583374, acc: 0.6896551847457886)
[2025-02-16 11:30:05,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:05,813][root][INFO] - Training Epoch: 1/2, step 4358/23838 completed (loss: 0.6301438212394714, acc: 0.875)
[2025-02-16 11:30:06,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:06,244][root][INFO] - Training Epoch: 1/2, step 4359/23838 completed (loss: 1.1654108762741089, acc: 0.6399999856948853)
[2025-02-16 11:30:06,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:06,613][root][INFO] - Training Epoch: 1/2, step 4360/23838 completed (loss: 0.41495683789253235, acc: 0.8695651888847351)
[2025-02-16 11:30:06,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:07,017][root][INFO] - Training Epoch: 1/2, step 4361/23838 completed (loss: 0.8312616944313049, acc: 0.7142857313156128)
[2025-02-16 11:30:07,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:07,429][root][INFO] - Training Epoch: 1/2, step 4362/23838 completed (loss: 0.4343607425689697, acc: 0.8235294222831726)
[2025-02-16 11:30:07,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:07,827][root][INFO] - Training Epoch: 1/2, step 4363/23838 completed (loss: 1.0254594087600708, acc: 0.7142857313156128)
[2025-02-16 11:30:08,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:08,259][root][INFO] - Training Epoch: 1/2, step 4364/23838 completed (loss: 0.5356418490409851, acc: 0.8717948794364929)
[2025-02-16 11:30:08,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:08,692][root][INFO] - Training Epoch: 1/2, step 4365/23838 completed (loss: 0.8581362962722778, acc: 0.7241379022598267)
[2025-02-16 11:30:08,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:09,140][root][INFO] - Training Epoch: 1/2, step 4366/23838 completed (loss: 0.7932977676391602, acc: 0.7837837934494019)
[2025-02-16 11:30:09,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:09,574][root][INFO] - Training Epoch: 1/2, step 4367/23838 completed (loss: 1.4442509412765503, acc: 0.5714285969734192)
[2025-02-16 11:30:09,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:10,017][root][INFO] - Training Epoch: 1/2, step 4368/23838 completed (loss: 1.3581607341766357, acc: 0.692307710647583)
[2025-02-16 11:30:10,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:10,458][root][INFO] - Training Epoch: 1/2, step 4369/23838 completed (loss: 0.2980763018131256, acc: 0.9444444179534912)
[2025-02-16 11:30:10,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:10,915][root][INFO] - Training Epoch: 1/2, step 4370/23838 completed (loss: 0.8879038095474243, acc: 0.7368420958518982)
[2025-02-16 11:30:11,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:11,319][root][INFO] - Training Epoch: 1/2, step 4371/23838 completed (loss: 1.1285994052886963, acc: 0.8333333134651184)
[2025-02-16 11:30:11,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:11,723][root][INFO] - Training Epoch: 1/2, step 4372/23838 completed (loss: 0.5556326508522034, acc: 0.8947368264198303)
[2025-02-16 11:30:11,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:12,159][root][INFO] - Training Epoch: 1/2, step 4373/23838 completed (loss: 1.1202188730239868, acc: 0.6666666865348816)
[2025-02-16 11:30:12,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:12,626][root][INFO] - Training Epoch: 1/2, step 4374/23838 completed (loss: 0.6158691048622131, acc: 0.84375)
[2025-02-16 11:30:12,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:13,041][root][INFO] - Training Epoch: 1/2, step 4375/23838 completed (loss: 0.7336654663085938, acc: 0.7761194109916687)
[2025-02-16 11:30:13,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:13,425][root][INFO] - Training Epoch: 1/2, step 4376/23838 completed (loss: 0.28846612572669983, acc: 0.875)
[2025-02-16 11:30:13,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:13,848][root][INFO] - Training Epoch: 1/2, step 4377/23838 completed (loss: 0.6019733548164368, acc: 0.8333333134651184)
[2025-02-16 11:30:14,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:14,297][root][INFO] - Training Epoch: 1/2, step 4378/23838 completed (loss: 1.0551245212554932, acc: 0.7777777910232544)
[2025-02-16 11:30:14,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:14,703][root][INFO] - Training Epoch: 1/2, step 4379/23838 completed (loss: 1.282548427581787, acc: 0.5757575631141663)
[2025-02-16 11:30:14,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:15,104][root][INFO] - Training Epoch: 1/2, step 4380/23838 completed (loss: 1.6055173873901367, acc: 0.6578947305679321)
[2025-02-16 11:30:15,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:15,503][root][INFO] - Training Epoch: 1/2, step 4381/23838 completed (loss: 1.2304874658584595, acc: 0.6829268336296082)
[2025-02-16 11:30:15,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:15,934][root][INFO] - Training Epoch: 1/2, step 4382/23838 completed (loss: 1.4061453342437744, acc: 0.625)
[2025-02-16 11:30:16,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:16,340][root][INFO] - Training Epoch: 1/2, step 4383/23838 completed (loss: 1.1848347187042236, acc: 0.6595744490623474)
[2025-02-16 11:30:16,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:16,740][root][INFO] - Training Epoch: 1/2, step 4384/23838 completed (loss: 0.5537945032119751, acc: 0.8529411554336548)
[2025-02-16 11:30:16,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:17,190][root][INFO] - Training Epoch: 1/2, step 4385/23838 completed (loss: 0.6829063892364502, acc: 0.800000011920929)
[2025-02-16 11:30:17,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:17,588][root][INFO] - Training Epoch: 1/2, step 4386/23838 completed (loss: 1.869152307510376, acc: 0.625)
[2025-02-16 11:30:17,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:18,045][root][INFO] - Training Epoch: 1/2, step 4387/23838 completed (loss: 0.7103872895240784, acc: 0.7428571581840515)
[2025-02-16 11:30:18,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:18,477][root][INFO] - Training Epoch: 1/2, step 4388/23838 completed (loss: 1.298771858215332, acc: 0.5925925970077515)
[2025-02-16 11:30:18,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:18,944][root][INFO] - Training Epoch: 1/2, step 4389/23838 completed (loss: 1.630746841430664, acc: 0.5853658318519592)
[2025-02-16 11:30:19,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:19,378][root][INFO] - Training Epoch: 1/2, step 4390/23838 completed (loss: 1.9413483142852783, acc: 0.45945945382118225)
[2025-02-16 11:30:19,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:19,870][root][INFO] - Training Epoch: 1/2, step 4391/23838 completed (loss: 0.9186925888061523, acc: 0.7111111283302307)
[2025-02-16 11:30:20,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:20,311][root][INFO] - Training Epoch: 1/2, step 4392/23838 completed (loss: 1.8615890741348267, acc: 0.4545454680919647)
[2025-02-16 11:30:20,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:20,695][root][INFO] - Training Epoch: 1/2, step 4393/23838 completed (loss: 0.41296860575675964, acc: 0.8936170339584351)
[2025-02-16 11:30:20,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:21,081][root][INFO] - Training Epoch: 1/2, step 4394/23838 completed (loss: 1.0104349851608276, acc: 0.699999988079071)
[2025-02-16 11:30:21,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:21,441][root][INFO] - Training Epoch: 1/2, step 4395/23838 completed (loss: 0.6849219799041748, acc: 0.7272727489471436)
[2025-02-16 11:30:21,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:21,812][root][INFO] - Training Epoch: 1/2, step 4396/23838 completed (loss: 0.5024734735488892, acc: 0.8928571343421936)
[2025-02-16 11:30:22,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:22,256][root][INFO] - Training Epoch: 1/2, step 4397/23838 completed (loss: 1.0817965269088745, acc: 0.8055555820465088)
[2025-02-16 11:30:22,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:22,711][root][INFO] - Training Epoch: 1/2, step 4398/23838 completed (loss: 0.3259538412094116, acc: 1.0)
[2025-02-16 11:30:22,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:23,157][root][INFO] - Training Epoch: 1/2, step 4399/23838 completed (loss: 0.8930736184120178, acc: 0.6944444179534912)
[2025-02-16 11:30:23,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:23,588][root][INFO] - Training Epoch: 1/2, step 4400/23838 completed (loss: 1.2498575448989868, acc: 0.6785714030265808)
[2025-02-16 11:30:23,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:24,031][root][INFO] - Training Epoch: 1/2, step 4401/23838 completed (loss: 0.4900467097759247, acc: 0.8723404407501221)
[2025-02-16 11:30:24,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:24,455][root][INFO] - Training Epoch: 1/2, step 4402/23838 completed (loss: 0.4671948254108429, acc: 0.7948718070983887)
[2025-02-16 11:30:24,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:24,896][root][INFO] - Training Epoch: 1/2, step 4403/23838 completed (loss: 0.4780890941619873, acc: 0.8235294222831726)
[2025-02-16 11:30:25,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:25,396][root][INFO] - Training Epoch: 1/2, step 4404/23838 completed (loss: 0.5123504996299744, acc: 0.8571428656578064)
[2025-02-16 11:30:25,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:25,778][root][INFO] - Training Epoch: 1/2, step 4405/23838 completed (loss: 1.1029629707336426, acc: 0.7307692170143127)
[2025-02-16 11:30:25,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:26,251][root][INFO] - Training Epoch: 1/2, step 4406/23838 completed (loss: 1.1375303268432617, acc: 0.695652186870575)
[2025-02-16 11:30:26,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:26,675][root][INFO] - Training Epoch: 1/2, step 4407/23838 completed (loss: 1.0271234512329102, acc: 0.7361111044883728)
[2025-02-16 11:30:26,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:27,114][root][INFO] - Training Epoch: 1/2, step 4408/23838 completed (loss: 0.2423347532749176, acc: 0.8636363744735718)
[2025-02-16 11:30:27,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:27,575][root][INFO] - Training Epoch: 1/2, step 4409/23838 completed (loss: 0.48801231384277344, acc: 0.8484848737716675)
[2025-02-16 11:30:27,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:27,973][root][INFO] - Training Epoch: 1/2, step 4410/23838 completed (loss: 0.569333016872406, acc: 0.7692307829856873)
[2025-02-16 11:30:28,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:28,363][root][INFO] - Training Epoch: 1/2, step 4411/23838 completed (loss: 1.368282675743103, acc: 0.7551020383834839)
[2025-02-16 11:30:28,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:28,832][root][INFO] - Training Epoch: 1/2, step 4412/23838 completed (loss: 1.047339677810669, acc: 0.6499999761581421)
[2025-02-16 11:30:29,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:29,275][root][INFO] - Training Epoch: 1/2, step 4413/23838 completed (loss: 0.9289268851280212, acc: 0.7450980544090271)
[2025-02-16 11:30:29,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:29,687][root][INFO] - Training Epoch: 1/2, step 4414/23838 completed (loss: 0.8811678290367126, acc: 0.7647058963775635)
[2025-02-16 11:30:29,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:30,160][root][INFO] - Training Epoch: 1/2, step 4415/23838 completed (loss: 0.6666797995567322, acc: 0.837837815284729)
[2025-02-16 11:30:30,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:30,604][root][INFO] - Training Epoch: 1/2, step 4416/23838 completed (loss: 0.9665389657020569, acc: 0.7368420958518982)
[2025-02-16 11:30:30,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:31,022][root][INFO] - Training Epoch: 1/2, step 4417/23838 completed (loss: 1.153334379196167, acc: 0.7234042286872864)
[2025-02-16 11:30:31,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:31,469][root][INFO] - Training Epoch: 1/2, step 4418/23838 completed (loss: 1.4142069816589355, acc: 0.6190476417541504)
[2025-02-16 11:30:31,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:31,926][root][INFO] - Training Epoch: 1/2, step 4419/23838 completed (loss: 0.8638824224472046, acc: 0.8181818127632141)
[2025-02-16 11:30:32,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:32,392][root][INFO] - Training Epoch: 1/2, step 4420/23838 completed (loss: 1.9335688352584839, acc: 0.5573770403862)
[2025-02-16 11:30:32,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:32,833][root][INFO] - Training Epoch: 1/2, step 4421/23838 completed (loss: 0.5585922002792358, acc: 0.8048780560493469)
[2025-02-16 11:30:33,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:33,297][root][INFO] - Training Epoch: 1/2, step 4422/23838 completed (loss: 0.9560114145278931, acc: 0.59375)
[2025-02-16 11:30:33,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:33,704][root][INFO] - Training Epoch: 1/2, step 4423/23838 completed (loss: 1.0621334314346313, acc: 0.6206896305084229)
[2025-02-16 11:30:33,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:34,080][root][INFO] - Training Epoch: 1/2, step 4424/23838 completed (loss: 0.9427563548088074, acc: 0.7368420958518982)
[2025-02-16 11:30:34,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:34,563][root][INFO] - Training Epoch: 1/2, step 4425/23838 completed (loss: 0.8742828369140625, acc: 0.6976743936538696)
[2025-02-16 11:30:34,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:34,992][root][INFO] - Training Epoch: 1/2, step 4426/23838 completed (loss: 0.6890457272529602, acc: 0.7599999904632568)
[2025-02-16 11:30:35,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:35,430][root][INFO] - Training Epoch: 1/2, step 4427/23838 completed (loss: 0.9443135261535645, acc: 0.7368420958518982)
[2025-02-16 11:30:35,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:35,859][root][INFO] - Training Epoch: 1/2, step 4428/23838 completed (loss: 0.8766461610794067, acc: 0.7407407164573669)
[2025-02-16 11:30:36,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:36,296][root][INFO] - Training Epoch: 1/2, step 4429/23838 completed (loss: 1.270967721939087, acc: 0.6333333253860474)
[2025-02-16 11:30:36,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:36,726][root][INFO] - Training Epoch: 1/2, step 4430/23838 completed (loss: 0.6987448334693909, acc: 0.7837837934494019)
[2025-02-16 11:30:36,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:37,177][root][INFO] - Training Epoch: 1/2, step 4431/23838 completed (loss: 0.40017685294151306, acc: 0.849056601524353)
[2025-02-16 11:30:37,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:37,635][root][INFO] - Training Epoch: 1/2, step 4432/23838 completed (loss: 1.048789620399475, acc: 0.7647058963775635)
[2025-02-16 11:30:37,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:38,079][root][INFO] - Training Epoch: 1/2, step 4433/23838 completed (loss: 1.600152850151062, acc: 0.6111111044883728)
[2025-02-16 11:30:38,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:38,558][root][INFO] - Training Epoch: 1/2, step 4434/23838 completed (loss: 0.897230863571167, acc: 0.7755101919174194)
[2025-02-16 11:30:38,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:38,947][root][INFO] - Training Epoch: 1/2, step 4435/23838 completed (loss: 0.9221083521842957, acc: 0.761904776096344)
[2025-02-16 11:30:39,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:39,384][root][INFO] - Training Epoch: 1/2, step 4436/23838 completed (loss: 0.9198911190032959, acc: 0.7593985199928284)
[2025-02-16 11:30:39,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:39,791][root][INFO] - Training Epoch: 1/2, step 4437/23838 completed (loss: 0.7069141268730164, acc: 0.8028169274330139)
[2025-02-16 11:30:39,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:40,239][root][INFO] - Training Epoch: 1/2, step 4438/23838 completed (loss: 0.8346328735351562, acc: 0.7611111402511597)
[2025-02-16 11:30:40,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:40,620][root][INFO] - Training Epoch: 1/2, step 4439/23838 completed (loss: 0.9554705619812012, acc: 0.7053571343421936)
[2025-02-16 11:30:40,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:41,078][root][INFO] - Training Epoch: 1/2, step 4440/23838 completed (loss: 1.206085443496704, acc: 0.7179487347602844)
[2025-02-16 11:30:41,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:41,530][root][INFO] - Training Epoch: 1/2, step 4441/23838 completed (loss: 0.3558140695095062, acc: 0.9166666865348816)
[2025-02-16 11:30:41,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:42,003][root][INFO] - Training Epoch: 1/2, step 4442/23838 completed (loss: 0.7111583352088928, acc: 0.7870967984199524)
[2025-02-16 11:30:42,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:42,455][root][INFO] - Training Epoch: 1/2, step 4443/23838 completed (loss: 0.6100320219993591, acc: 0.84375)
[2025-02-16 11:30:42,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:42,857][root][INFO] - Training Epoch: 1/2, step 4444/23838 completed (loss: 0.5582045912742615, acc: 0.8388888835906982)
[2025-02-16 11:30:42,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:43,245][root][INFO] - Training Epoch: 1/2, step 4445/23838 completed (loss: 0.4312859773635864, acc: 0.875)
[2025-02-16 11:30:43,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:43,655][root][INFO] - Training Epoch: 1/2, step 4446/23838 completed (loss: 0.5544499158859253, acc: 0.8733333349227905)
[2025-02-16 11:30:43,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:44,099][root][INFO] - Training Epoch: 1/2, step 4447/23838 completed (loss: 0.6509812474250793, acc: 0.8145161271095276)
[2025-02-16 11:30:44,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:44,531][root][INFO] - Training Epoch: 1/2, step 4448/23838 completed (loss: 0.44109368324279785, acc: 0.875)
[2025-02-16 11:30:44,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:44,974][root][INFO] - Training Epoch: 1/2, step 4449/23838 completed (loss: 0.5155673623085022, acc: 0.8508771657943726)
[2025-02-16 11:30:45,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:45,431][root][INFO] - Training Epoch: 1/2, step 4450/23838 completed (loss: 0.9179860353469849, acc: 0.7307692170143127)
[2025-02-16 11:30:45,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:45,811][root][INFO] - Training Epoch: 1/2, step 4451/23838 completed (loss: 0.5220474004745483, acc: 0.8692307472229004)
[2025-02-16 11:30:46,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:46,321][root][INFO] - Training Epoch: 1/2, step 4452/23838 completed (loss: 0.4834253489971161, acc: 0.8439024686813354)
[2025-02-16 11:30:46,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:46,739][root][INFO] - Training Epoch: 1/2, step 4453/23838 completed (loss: 0.47281062602996826, acc: 0.849397599697113)
[2025-02-16 11:30:46,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:47,134][root][INFO] - Training Epoch: 1/2, step 4454/23838 completed (loss: 0.7050497531890869, acc: 0.8194444179534912)
[2025-02-16 11:30:47,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:47,567][root][INFO] - Training Epoch: 1/2, step 4455/23838 completed (loss: 0.5154362320899963, acc: 0.8444444537162781)
[2025-02-16 11:30:47,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:47,972][root][INFO] - Training Epoch: 1/2, step 4456/23838 completed (loss: 0.34460482001304626, acc: 0.8980891704559326)
[2025-02-16 11:30:48,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:48,424][root][INFO] - Training Epoch: 1/2, step 4457/23838 completed (loss: 0.9553537368774414, acc: 0.7638888955116272)
[2025-02-16 11:30:48,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:48,908][root][INFO] - Training Epoch: 1/2, step 4458/23838 completed (loss: 0.6085870265960693, acc: 0.8380952477455139)
[2025-02-16 11:30:49,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:49,352][root][INFO] - Training Epoch: 1/2, step 4459/23838 completed (loss: 0.49034783244132996, acc: 0.8990825414657593)
[2025-02-16 11:30:49,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:49,815][root][INFO] - Training Epoch: 1/2, step 4460/23838 completed (loss: 0.5812221169471741, acc: 0.8640776872634888)
[2025-02-16 11:30:49,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:50,222][root][INFO] - Training Epoch: 1/2, step 4461/23838 completed (loss: 0.8225892186164856, acc: 0.760869562625885)
[2025-02-16 11:30:50,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:50,677][root][INFO] - Training Epoch: 1/2, step 4462/23838 completed (loss: 0.3241317570209503, acc: 0.9274193644523621)
[2025-02-16 11:30:50,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:51,169][root][INFO] - Training Epoch: 1/2, step 4463/23838 completed (loss: 0.5984846949577332, acc: 0.8283582329750061)
[2025-02-16 11:30:51,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:51,654][root][INFO] - Training Epoch: 1/2, step 4464/23838 completed (loss: 0.32937705516815186, acc: 0.9047619104385376)
[2025-02-16 11:30:51,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:52,088][root][INFO] - Training Epoch: 1/2, step 4465/23838 completed (loss: 0.2267242968082428, acc: 0.9357798099517822)
[2025-02-16 11:30:52,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:52,538][root][INFO] - Training Epoch: 1/2, step 4466/23838 completed (loss: 0.3775418996810913, acc: 0.9047619104385376)
[2025-02-16 11:30:52,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:52,980][root][INFO] - Training Epoch: 1/2, step 4467/23838 completed (loss: 0.4349479675292969, acc: 0.9021739363670349)
[2025-02-16 11:30:53,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:53,385][root][INFO] - Training Epoch: 1/2, step 4468/23838 completed (loss: 0.6368775963783264, acc: 0.8450704216957092)
[2025-02-16 11:30:53,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:53,799][root][INFO] - Training Epoch: 1/2, step 4469/23838 completed (loss: 0.4049854576587677, acc: 0.8875739574432373)
[2025-02-16 11:30:53,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:54,198][root][INFO] - Training Epoch: 1/2, step 4470/23838 completed (loss: 0.5533239841461182, acc: 0.8246753215789795)
[2025-02-16 11:30:54,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:54,630][root][INFO] - Training Epoch: 1/2, step 4471/23838 completed (loss: 0.49392133951187134, acc: 0.8526315689086914)
[2025-02-16 11:30:54,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:55,109][root][INFO] - Training Epoch: 1/2, step 4472/23838 completed (loss: 0.19890299439430237, acc: 0.9417475461959839)
[2025-02-16 11:30:55,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:55,533][root][INFO] - Training Epoch: 1/2, step 4473/23838 completed (loss: 0.3303697407245636, acc: 0.9035087823867798)
[2025-02-16 11:30:55,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:55,899][root][INFO] - Training Epoch: 1/2, step 4474/23838 completed (loss: 0.8457536101341248, acc: 0.7575757503509521)
[2025-02-16 11:30:56,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:56,354][root][INFO] - Training Epoch: 1/2, step 4475/23838 completed (loss: 0.3972240388393402, acc: 0.9454545378684998)
[2025-02-16 11:30:56,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:56,819][root][INFO] - Training Epoch: 1/2, step 4476/23838 completed (loss: 0.6448796391487122, acc: 0.804347813129425)
[2025-02-16 11:30:57,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:57,239][root][INFO] - Training Epoch: 1/2, step 4477/23838 completed (loss: 0.6111973524093628, acc: 0.8450704216957092)
[2025-02-16 11:30:57,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:57,673][root][INFO] - Training Epoch: 1/2, step 4478/23838 completed (loss: 0.7390572428703308, acc: 0.7971014380455017)
[2025-02-16 11:30:57,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:58,126][root][INFO] - Training Epoch: 1/2, step 4479/23838 completed (loss: 1.1889508962631226, acc: 0.6896551847457886)
[2025-02-16 11:30:58,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:58,512][root][INFO] - Training Epoch: 1/2, step 4480/23838 completed (loss: 0.48455989360809326, acc: 0.8679245114326477)
[2025-02-16 11:30:58,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:58,937][root][INFO] - Training Epoch: 1/2, step 4481/23838 completed (loss: 0.3579864799976349, acc: 0.901098906993866)
[2025-02-16 11:30:59,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:59,342][root][INFO] - Training Epoch: 1/2, step 4482/23838 completed (loss: 1.5806055068969727, acc: 0.625)
[2025-02-16 11:30:59,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:30:59,772][root][INFO] - Training Epoch: 1/2, step 4483/23838 completed (loss: 1.1524291038513184, acc: 0.6101694703102112)
[2025-02-16 11:30:59,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:00,157][root][INFO] - Training Epoch: 1/2, step 4484/23838 completed (loss: 0.9413250684738159, acc: 0.7021276354789734)
[2025-02-16 11:31:00,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:00,580][root][INFO] - Training Epoch: 1/2, step 4485/23838 completed (loss: 0.9136329889297485, acc: 0.7450980544090271)
[2025-02-16 11:31:00,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:01,034][root][INFO] - Training Epoch: 1/2, step 4486/23838 completed (loss: 1.1331640481948853, acc: 0.7407407164573669)
[2025-02-16 11:31:01,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:01,467][root][INFO] - Training Epoch: 1/2, step 4487/23838 completed (loss: 0.7199523448944092, acc: 0.8571428656578064)
[2025-02-16 11:31:01,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:01,853][root][INFO] - Training Epoch: 1/2, step 4488/23838 completed (loss: 1.0055909156799316, acc: 0.7692307829856873)
[2025-02-16 11:31:02,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:02,248][root][INFO] - Training Epoch: 1/2, step 4489/23838 completed (loss: 0.9691771864891052, acc: 0.7441860437393188)
[2025-02-16 11:31:02,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:02,659][root][INFO] - Training Epoch: 1/2, step 4490/23838 completed (loss: 0.7522847652435303, acc: 0.797468364238739)
[2025-02-16 11:31:02,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:03,056][root][INFO] - Training Epoch: 1/2, step 4491/23838 completed (loss: 1.4143348932266235, acc: 0.5862069129943848)
[2025-02-16 11:31:03,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:03,433][root][INFO] - Training Epoch: 1/2, step 4492/23838 completed (loss: 0.7470486760139465, acc: 0.8135592937469482)
[2025-02-16 11:31:03,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:03,871][root][INFO] - Training Epoch: 1/2, step 4493/23838 completed (loss: 1.018651008605957, acc: 0.779411792755127)
[2025-02-16 11:31:04,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:04,328][root][INFO] - Training Epoch: 1/2, step 4494/23838 completed (loss: 0.6494410037994385, acc: 0.8085106611251831)
[2025-02-16 11:31:04,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:04,764][root][INFO] - Training Epoch: 1/2, step 4495/23838 completed (loss: 0.8080998063087463, acc: 0.78125)
[2025-02-16 11:31:04,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:05,172][root][INFO] - Training Epoch: 1/2, step 4496/23838 completed (loss: 0.3672145903110504, acc: 0.9473684430122375)
[2025-02-16 11:31:05,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:05,620][root][INFO] - Training Epoch: 1/2, step 4497/23838 completed (loss: 1.303822636604309, acc: 0.5849056839942932)
[2025-02-16 11:31:05,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:06,030][root][INFO] - Training Epoch: 1/2, step 4498/23838 completed (loss: 1.2593809366226196, acc: 0.6440678238868713)
[2025-02-16 11:31:06,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:06,476][root][INFO] - Training Epoch: 1/2, step 4499/23838 completed (loss: 0.7028397917747498, acc: 0.8275862336158752)
[2025-02-16 11:31:06,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:06,930][root][INFO] - Training Epoch: 1/2, step 4500/23838 completed (loss: 0.6374947428703308, acc: 0.8441558480262756)
[2025-02-16 11:31:07,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:07,343][root][INFO] - Training Epoch: 1/2, step 4501/23838 completed (loss: 0.6764375567436218, acc: 0.782608687877655)
[2025-02-16 11:31:07,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:07,790][root][INFO] - Training Epoch: 1/2, step 4502/23838 completed (loss: 1.167414665222168, acc: 0.7021276354789734)
[2025-02-16 11:31:08,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:08,239][root][INFO] - Training Epoch: 1/2, step 4503/23838 completed (loss: 1.224308967590332, acc: 0.6582278609275818)
[2025-02-16 11:31:08,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:08,615][root][INFO] - Training Epoch: 1/2, step 4504/23838 completed (loss: 0.8884629011154175, acc: 0.7599999904632568)
[2025-02-16 11:31:08,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:08,994][root][INFO] - Training Epoch: 1/2, step 4505/23838 completed (loss: 0.6747121810913086, acc: 0.7816091775894165)
[2025-02-16 11:31:09,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:09,469][root][INFO] - Training Epoch: 1/2, step 4506/23838 completed (loss: 0.9750589728355408, acc: 0.7400000095367432)
[2025-02-16 11:31:09,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:09,876][root][INFO] - Training Epoch: 1/2, step 4507/23838 completed (loss: 0.9096154570579529, acc: 0.734375)
[2025-02-16 11:31:10,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:10,269][root][INFO] - Training Epoch: 1/2, step 4508/23838 completed (loss: 0.9973212480545044, acc: 0.7384615540504456)
[2025-02-16 11:31:10,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:10,739][root][INFO] - Training Epoch: 1/2, step 4509/23838 completed (loss: 1.1331596374511719, acc: 0.7361111044883728)
[2025-02-16 11:31:10,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:11,177][root][INFO] - Training Epoch: 1/2, step 4510/23838 completed (loss: 1.2037239074707031, acc: 0.6268656849861145)
[2025-02-16 11:31:11,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:11,543][root][INFO] - Training Epoch: 1/2, step 4511/23838 completed (loss: 1.0019344091415405, acc: 0.6867470145225525)
[2025-02-16 11:31:11,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:11,959][root][INFO] - Training Epoch: 1/2, step 4512/23838 completed (loss: 0.7305870056152344, acc: 0.800000011920929)
[2025-02-16 11:31:12,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:12,429][root][INFO] - Training Epoch: 1/2, step 4513/23838 completed (loss: 1.1438308954238892, acc: 0.6428571343421936)
[2025-02-16 11:31:12,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:12,898][root][INFO] - Training Epoch: 1/2, step 4514/23838 completed (loss: 1.2138580083847046, acc: 0.6486486196517944)
[2025-02-16 11:31:13,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:13,301][root][INFO] - Training Epoch: 1/2, step 4515/23838 completed (loss: 0.9089059829711914, acc: 0.75)
[2025-02-16 11:31:13,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:13,722][root][INFO] - Training Epoch: 1/2, step 4516/23838 completed (loss: 0.8291698694229126, acc: 0.779411792755127)
[2025-02-16 11:31:13,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:14,119][root][INFO] - Training Epoch: 1/2, step 4517/23838 completed (loss: 0.867408275604248, acc: 0.795918345451355)
[2025-02-16 11:31:14,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:14,548][root][INFO] - Training Epoch: 1/2, step 4518/23838 completed (loss: 0.990202784538269, acc: 0.7678571343421936)
[2025-02-16 11:31:14,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:14,987][root][INFO] - Training Epoch: 1/2, step 4519/23838 completed (loss: 1.0752955675125122, acc: 0.7058823704719543)
[2025-02-16 11:31:15,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:15,367][root][INFO] - Training Epoch: 1/2, step 4520/23838 completed (loss: 0.6665540933609009, acc: 0.7916666865348816)
[2025-02-16 11:31:15,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:15,799][root][INFO] - Training Epoch: 1/2, step 4521/23838 completed (loss: 1.1473439931869507, acc: 0.6769230961799622)
[2025-02-16 11:31:15,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:16,183][root][INFO] - Training Epoch: 1/2, step 4522/23838 completed (loss: 0.6153604984283447, acc: 0.7941176295280457)
[2025-02-16 11:31:16,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:16,607][root][INFO] - Training Epoch: 1/2, step 4523/23838 completed (loss: 1.262834072113037, acc: 0.6774193644523621)
[2025-02-16 11:31:16,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:16,975][root][INFO] - Training Epoch: 1/2, step 4524/23838 completed (loss: 1.4750391244888306, acc: 0.6290322542190552)
[2025-02-16 11:31:17,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:17,347][root][INFO] - Training Epoch: 1/2, step 4525/23838 completed (loss: 1.0277190208435059, acc: 0.6935483813285828)
[2025-02-16 11:31:17,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:17,873][root][INFO] - Training Epoch: 1/2, step 4526/23838 completed (loss: 0.6936737298965454, acc: 0.800000011920929)
[2025-02-16 11:31:18,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:18,263][root][INFO] - Training Epoch: 1/2, step 4527/23838 completed (loss: 1.218741536140442, acc: 0.6290322542190552)
[2025-02-16 11:31:18,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:18,625][root][INFO] - Training Epoch: 1/2, step 4528/23838 completed (loss: 0.33737900853157043, acc: 0.9240506291389465)
[2025-02-16 11:31:18,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:19,057][root][INFO] - Training Epoch: 1/2, step 4529/23838 completed (loss: 0.8910588026046753, acc: 0.7142857313156128)
[2025-02-16 11:31:19,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:19,489][root][INFO] - Training Epoch: 1/2, step 4530/23838 completed (loss: 0.632607638835907, acc: 0.782608687877655)
[2025-02-16 11:31:19,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:19,851][root][INFO] - Training Epoch: 1/2, step 4531/23838 completed (loss: 0.8972581028938293, acc: 0.7758620977401733)
[2025-02-16 11:31:20,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:20,230][root][INFO] - Training Epoch: 1/2, step 4532/23838 completed (loss: 1.3090550899505615, acc: 0.6274510025978088)
[2025-02-16 11:31:20,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:20,614][root][INFO] - Training Epoch: 1/2, step 4533/23838 completed (loss: 0.6626057028770447, acc: 0.8372092843055725)
[2025-02-16 11:31:20,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:21,058][root][INFO] - Training Epoch: 1/2, step 4534/23838 completed (loss: 0.9276662468910217, acc: 0.6888889074325562)
[2025-02-16 11:31:21,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:21,458][root][INFO] - Training Epoch: 1/2, step 4535/23838 completed (loss: 0.8457595109939575, acc: 0.8055555820465088)
[2025-02-16 11:31:21,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:21,827][root][INFO] - Training Epoch: 1/2, step 4536/23838 completed (loss: 0.9382495284080505, acc: 0.7435897588729858)
[2025-02-16 11:31:22,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:22,261][root][INFO] - Training Epoch: 1/2, step 4537/23838 completed (loss: 0.658338725566864, acc: 0.7457627058029175)
[2025-02-16 11:31:22,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:22,657][root][INFO] - Training Epoch: 1/2, step 4538/23838 completed (loss: 1.235095500946045, acc: 0.6935483813285828)
[2025-02-16 11:31:22,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:23,071][root][INFO] - Training Epoch: 1/2, step 4539/23838 completed (loss: 1.4225218296051025, acc: 0.6176470518112183)
[2025-02-16 11:31:23,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:23,479][root][INFO] - Training Epoch: 1/2, step 4540/23838 completed (loss: 1.4696359634399414, acc: 0.523809552192688)
[2025-02-16 11:31:23,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:23,880][root][INFO] - Training Epoch: 1/2, step 4541/23838 completed (loss: 1.116166114807129, acc: 0.6818181872367859)
[2025-02-16 11:31:24,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:24,484][root][INFO] - Training Epoch: 1/2, step 4542/23838 completed (loss: 1.5825942754745483, acc: 0.5978260636329651)
[2025-02-16 11:31:24,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:24,951][root][INFO] - Training Epoch: 1/2, step 4543/23838 completed (loss: 1.0656545162200928, acc: 0.7692307829856873)
[2025-02-16 11:31:25,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:25,406][root][INFO] - Training Epoch: 1/2, step 4544/23838 completed (loss: 0.45125076174736023, acc: 0.8681318759918213)
[2025-02-16 11:31:25,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:25,863][root][INFO] - Training Epoch: 1/2, step 4545/23838 completed (loss: 1.214477300643921, acc: 0.6962025165557861)
[2025-02-16 11:31:26,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:26,278][root][INFO] - Training Epoch: 1/2, step 4546/23838 completed (loss: 0.5052546262741089, acc: 0.8374999761581421)
[2025-02-16 11:31:26,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:26,754][root][INFO] - Training Epoch: 1/2, step 4547/23838 completed (loss: 0.5965303778648376, acc: 0.8461538553237915)
[2025-02-16 11:31:26,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:27,198][root][INFO] - Training Epoch: 1/2, step 4548/23838 completed (loss: 0.5799075961112976, acc: 0.8333333134651184)
[2025-02-16 11:31:27,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:27,683][root][INFO] - Training Epoch: 1/2, step 4549/23838 completed (loss: 0.5607381463050842, acc: 0.8333333134651184)
[2025-02-16 11:31:27,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:28,168][root][INFO] - Training Epoch: 1/2, step 4550/23838 completed (loss: 1.3889811038970947, acc: 0.6756756901741028)
[2025-02-16 11:31:28,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:28,649][root][INFO] - Training Epoch: 1/2, step 4551/23838 completed (loss: 0.713261604309082, acc: 0.8181818127632141)
[2025-02-16 11:31:28,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:29,089][root][INFO] - Training Epoch: 1/2, step 4552/23838 completed (loss: 0.8449552059173584, acc: 0.7611940503120422)
[2025-02-16 11:31:29,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:29,513][root][INFO] - Training Epoch: 1/2, step 4553/23838 completed (loss: 0.7059250473976135, acc: 0.8205128312110901)
[2025-02-16 11:31:29,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:30,047][root][INFO] - Training Epoch: 1/2, step 4554/23838 completed (loss: 0.6345692873001099, acc: 0.8333333134651184)
[2025-02-16 11:31:30,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:30,523][root][INFO] - Training Epoch: 1/2, step 4555/23838 completed (loss: 1.423755407333374, acc: 0.6666666865348816)
[2025-02-16 11:31:30,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:30,937][root][INFO] - Training Epoch: 1/2, step 4556/23838 completed (loss: 1.6967188119888306, acc: 0.5588235259056091)
[2025-02-16 11:31:31,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:31,402][root][INFO] - Training Epoch: 1/2, step 4557/23838 completed (loss: 1.0194268226623535, acc: 0.7555555701255798)
[2025-02-16 11:31:31,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:31,873][root][INFO] - Training Epoch: 1/2, step 4558/23838 completed (loss: 0.7689906358718872, acc: 0.774193525314331)
[2025-02-16 11:31:32,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:32,351][root][INFO] - Training Epoch: 1/2, step 4559/23838 completed (loss: 0.6197262406349182, acc: 0.8205128312110901)
[2025-02-16 11:31:32,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:32,762][root][INFO] - Training Epoch: 1/2, step 4560/23838 completed (loss: 0.47349467873573303, acc: 0.8888888955116272)
[2025-02-16 11:31:32,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:33,205][root][INFO] - Training Epoch: 1/2, step 4561/23838 completed (loss: 0.530913233757019, acc: 0.8405796885490417)
[2025-02-16 11:31:33,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:33,674][root][INFO] - Training Epoch: 1/2, step 4562/23838 completed (loss: 0.8416362404823303, acc: 0.8160919547080994)
[2025-02-16 11:31:33,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:34,095][root][INFO] - Training Epoch: 1/2, step 4563/23838 completed (loss: 0.5991088151931763, acc: 0.8125)
[2025-02-16 11:31:34,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:34,565][root][INFO] - Training Epoch: 1/2, step 4564/23838 completed (loss: 0.6205459237098694, acc: 0.7435897588729858)
[2025-02-16 11:31:34,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:35,032][root][INFO] - Training Epoch: 1/2, step 4565/23838 completed (loss: 0.880857527256012, acc: 0.7948718070983887)
[2025-02-16 11:31:35,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:35,459][root][INFO] - Training Epoch: 1/2, step 4566/23838 completed (loss: 0.9717435240745544, acc: 0.8181818127632141)
[2025-02-16 11:31:35,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:35,880][root][INFO] - Training Epoch: 1/2, step 4567/23838 completed (loss: 0.5537620782852173, acc: 0.8275862336158752)
[2025-02-16 11:31:36,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:36,283][root][INFO] - Training Epoch: 1/2, step 4568/23838 completed (loss: 1.219779372215271, acc: 0.6200000047683716)
[2025-02-16 11:31:36,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:36,720][root][INFO] - Training Epoch: 1/2, step 4569/23838 completed (loss: 0.6509933471679688, acc: 0.8148148059844971)
[2025-02-16 11:31:36,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:37,151][root][INFO] - Training Epoch: 1/2, step 4570/23838 completed (loss: 0.7554911375045776, acc: 0.8155339956283569)
[2025-02-16 11:31:37,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:37,577][root][INFO] - Training Epoch: 1/2, step 4571/23838 completed (loss: 0.43052777647972107, acc: 0.8586956262588501)
[2025-02-16 11:31:37,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:37,985][root][INFO] - Training Epoch: 1/2, step 4572/23838 completed (loss: 0.9181733727455139, acc: 0.7708333134651184)
[2025-02-16 11:31:38,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:38,387][root][INFO] - Training Epoch: 1/2, step 4573/23838 completed (loss: 0.5971594452857971, acc: 0.8305084705352783)
[2025-02-16 11:31:38,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:38,875][root][INFO] - Training Epoch: 1/2, step 4574/23838 completed (loss: 0.8637044429779053, acc: 0.7523809671401978)
[2025-02-16 11:31:39,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:39,271][root][INFO] - Training Epoch: 1/2, step 4575/23838 completed (loss: 0.7802852988243103, acc: 0.75)
[2025-02-16 11:31:39,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:39,672][root][INFO] - Training Epoch: 1/2, step 4576/23838 completed (loss: 0.4030510187149048, acc: 0.931034505367279)
[2025-02-16 11:31:39,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:40,145][root][INFO] - Training Epoch: 1/2, step 4577/23838 completed (loss: 0.5931897163391113, acc: 0.8135592937469482)
[2025-02-16 11:31:40,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:40,616][root][INFO] - Training Epoch: 1/2, step 4578/23838 completed (loss: 0.7391851544380188, acc: 0.8139534592628479)
[2025-02-16 11:31:40,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:41,020][root][INFO] - Training Epoch: 1/2, step 4579/23838 completed (loss: 0.6615604758262634, acc: 0.8214285969734192)
[2025-02-16 11:31:41,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:41,452][root][INFO] - Training Epoch: 1/2, step 4580/23838 completed (loss: 1.2184836864471436, acc: 0.7446808218955994)
[2025-02-16 11:31:41,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:41,895][root][INFO] - Training Epoch: 1/2, step 4581/23838 completed (loss: 1.6284006834030151, acc: 0.47727271914482117)
[2025-02-16 11:31:42,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:42,308][root][INFO] - Training Epoch: 1/2, step 4582/23838 completed (loss: 1.725311517715454, acc: 0.5483871102333069)
[2025-02-16 11:31:42,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:42,771][root][INFO] - Training Epoch: 1/2, step 4583/23838 completed (loss: 1.0749133825302124, acc: 0.6857143044471741)
[2025-02-16 11:31:42,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:43,197][root][INFO] - Training Epoch: 1/2, step 4584/23838 completed (loss: 1.354959487915039, acc: 0.5961538553237915)
[2025-02-16 11:31:43,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:43,588][root][INFO] - Training Epoch: 1/2, step 4585/23838 completed (loss: 0.9136737585067749, acc: 0.7169811129570007)
[2025-02-16 11:31:43,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:43,999][root][INFO] - Training Epoch: 1/2, step 4586/23838 completed (loss: 1.2156144380569458, acc: 0.6865671873092651)
[2025-02-16 11:31:44,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:44,395][root][INFO] - Training Epoch: 1/2, step 4587/23838 completed (loss: 1.2191379070281982, acc: 0.6666666865348816)
[2025-02-16 11:31:44,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:44,848][root][INFO] - Training Epoch: 1/2, step 4588/23838 completed (loss: 1.1905137300491333, acc: 0.6741573214530945)
[2025-02-16 11:31:45,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:45,337][root][INFO] - Training Epoch: 1/2, step 4589/23838 completed (loss: 1.6134788990020752, acc: 0.5921052694320679)
[2025-02-16 11:31:45,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:45,709][root][INFO] - Training Epoch: 1/2, step 4590/23838 completed (loss: 1.6332714557647705, acc: 0.5657894611358643)
[2025-02-16 11:31:45,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:46,187][root][INFO] - Training Epoch: 1/2, step 4591/23838 completed (loss: 1.1881719827651978, acc: 0.7045454382896423)
[2025-02-16 11:31:46,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:46,580][root][INFO] - Training Epoch: 1/2, step 4592/23838 completed (loss: 1.3545019626617432, acc: 0.6428571343421936)
[2025-02-16 11:31:46,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:46,957][root][INFO] - Training Epoch: 1/2, step 4593/23838 completed (loss: 1.1179016828536987, acc: 0.6941176652908325)
[2025-02-16 11:31:47,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:47,378][root][INFO] - Training Epoch: 1/2, step 4594/23838 completed (loss: 1.675602674484253, acc: 0.4941176474094391)
[2025-02-16 11:31:47,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:47,812][root][INFO] - Training Epoch: 1/2, step 4595/23838 completed (loss: 1.5017421245574951, acc: 0.6034482717514038)
[2025-02-16 11:31:47,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:48,238][root][INFO] - Training Epoch: 1/2, step 4596/23838 completed (loss: 1.5154505968093872, acc: 0.5892857313156128)
[2025-02-16 11:31:48,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:48,678][root][INFO] - Training Epoch: 1/2, step 4597/23838 completed (loss: 1.2347638607025146, acc: 0.6451612710952759)
[2025-02-16 11:31:48,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:49,139][root][INFO] - Training Epoch: 1/2, step 4598/23838 completed (loss: 1.5230919122695923, acc: 0.6216216087341309)
[2025-02-16 11:31:49,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:49,549][root][INFO] - Training Epoch: 1/2, step 4599/23838 completed (loss: 1.1945286989212036, acc: 0.6557376980781555)
[2025-02-16 11:31:49,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:49,920][root][INFO] - Training Epoch: 1/2, step 4600/23838 completed (loss: 1.1881393194198608, acc: 0.6800000071525574)
[2025-02-16 11:31:50,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:50,366][root][INFO] - Training Epoch: 1/2, step 4601/23838 completed (loss: 0.9652455449104309, acc: 0.71875)
[2025-02-16 11:31:50,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:50,807][root][INFO] - Training Epoch: 1/2, step 4602/23838 completed (loss: 0.8080680966377258, acc: 0.7599999904632568)
[2025-02-16 11:31:50,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:51,259][root][INFO] - Training Epoch: 1/2, step 4603/23838 completed (loss: 1.197056531906128, acc: 0.6304348111152649)
[2025-02-16 11:31:51,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:51,672][root][INFO] - Training Epoch: 1/2, step 4604/23838 completed (loss: 0.9327400922775269, acc: 0.7567567825317383)
[2025-02-16 11:31:51,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:52,092][root][INFO] - Training Epoch: 1/2, step 4605/23838 completed (loss: 0.9918539524078369, acc: 0.7400000095367432)
[2025-02-16 11:31:52,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:52,509][root][INFO] - Training Epoch: 1/2, step 4606/23838 completed (loss: 1.1390806436538696, acc: 0.7179487347602844)
[2025-02-16 11:31:52,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:52,899][root][INFO] - Training Epoch: 1/2, step 4607/23838 completed (loss: 1.3461484909057617, acc: 0.6666666865348816)
[2025-02-16 11:31:53,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:53,326][root][INFO] - Training Epoch: 1/2, step 4608/23838 completed (loss: 1.3556923866271973, acc: 0.5901639461517334)
[2025-02-16 11:31:53,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:53,742][root][INFO] - Training Epoch: 1/2, step 4609/23838 completed (loss: 1.0310182571411133, acc: 0.6363636255264282)
[2025-02-16 11:31:53,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:54,137][root][INFO] - Training Epoch: 1/2, step 4610/23838 completed (loss: 1.4455885887145996, acc: 0.5714285969734192)
[2025-02-16 11:31:54,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:54,631][root][INFO] - Training Epoch: 1/2, step 4611/23838 completed (loss: 1.490972876548767, acc: 0.5882353186607361)
[2025-02-16 11:31:54,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:55,032][root][INFO] - Training Epoch: 1/2, step 4612/23838 completed (loss: 1.872340440750122, acc: 0.4545454680919647)
[2025-02-16 11:31:55,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:55,414][root][INFO] - Training Epoch: 1/2, step 4613/23838 completed (loss: 1.4439202547073364, acc: 0.59375)
[2025-02-16 11:31:55,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:55,860][root][INFO] - Training Epoch: 1/2, step 4614/23838 completed (loss: 0.6388044357299805, acc: 0.849056601524353)
[2025-02-16 11:31:56,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:56,243][root][INFO] - Training Epoch: 1/2, step 4615/23838 completed (loss: 0.9916648268699646, acc: 0.7547169923782349)
[2025-02-16 11:31:56,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:56,686][root][INFO] - Training Epoch: 1/2, step 4616/23838 completed (loss: 1.171545147895813, acc: 0.6304348111152649)
[2025-02-16 11:31:56,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:57,117][root][INFO] - Training Epoch: 1/2, step 4617/23838 completed (loss: 1.5404807329177856, acc: 0.5777778029441833)
[2025-02-16 11:31:57,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:57,578][root][INFO] - Training Epoch: 1/2, step 4618/23838 completed (loss: 1.2002280950546265, acc: 0.6388888955116272)
[2025-02-16 11:31:57,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:57,964][root][INFO] - Training Epoch: 1/2, step 4619/23838 completed (loss: 1.3744657039642334, acc: 0.6290322542190552)
[2025-02-16 11:31:58,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:58,425][root][INFO] - Training Epoch: 1/2, step 4620/23838 completed (loss: 0.7565760612487793, acc: 0.7586206793785095)
[2025-02-16 11:31:58,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:58,832][root][INFO] - Training Epoch: 1/2, step 4621/23838 completed (loss: 1.11383855342865, acc: 0.692307710647583)
[2025-02-16 11:31:59,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:59,268][root][INFO] - Training Epoch: 1/2, step 4622/23838 completed (loss: 1.1997017860412598, acc: 0.695652186870575)
[2025-02-16 11:31:59,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:31:59,661][root][INFO] - Training Epoch: 1/2, step 4623/23838 completed (loss: 0.560462236404419, acc: 0.8421052694320679)
[2025-02-16 11:31:59,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:00,036][root][INFO] - Training Epoch: 1/2, step 4624/23838 completed (loss: 0.6065658330917358, acc: 0.8245614171028137)
[2025-02-16 11:32:00,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:00,413][root][INFO] - Training Epoch: 1/2, step 4625/23838 completed (loss: 0.9490891695022583, acc: 0.6551724076271057)
[2025-02-16 11:32:00,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:00,881][root][INFO] - Training Epoch: 1/2, step 4626/23838 completed (loss: 1.102121114730835, acc: 0.6896551847457886)
[2025-02-16 11:32:01,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:01,278][root][INFO] - Training Epoch: 1/2, step 4627/23838 completed (loss: 0.964903712272644, acc: 0.7796609997749329)
[2025-02-16 11:32:01,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:01,691][root][INFO] - Training Epoch: 1/2, step 4628/23838 completed (loss: 0.5696574449539185, acc: 0.8799999952316284)
[2025-02-16 11:32:01,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:02,102][root][INFO] - Training Epoch: 1/2, step 4629/23838 completed (loss: 1.2628052234649658, acc: 0.6707317233085632)
[2025-02-16 11:32:02,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:02,518][root][INFO] - Training Epoch: 1/2, step 4630/23838 completed (loss: 0.9056734442710876, acc: 0.7540983557701111)
[2025-02-16 11:32:02,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:02,940][root][INFO] - Training Epoch: 1/2, step 4631/23838 completed (loss: 1.0783811807632446, acc: 0.75)
[2025-02-16 11:32:03,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:03,364][root][INFO] - Training Epoch: 1/2, step 4632/23838 completed (loss: 1.1738619804382324, acc: 0.6617646813392639)
[2025-02-16 11:32:03,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:03,780][root][INFO] - Training Epoch: 1/2, step 4633/23838 completed (loss: 1.3514193296432495, acc: 0.6071428656578064)
[2025-02-16 11:32:03,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:04,192][root][INFO] - Training Epoch: 1/2, step 4634/23838 completed (loss: 0.8779579401016235, acc: 0.7402597665786743)
[2025-02-16 11:32:04,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:04,548][root][INFO] - Training Epoch: 1/2, step 4635/23838 completed (loss: 0.9669766426086426, acc: 0.707317054271698)
[2025-02-16 11:32:04,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:04,926][root][INFO] - Training Epoch: 1/2, step 4636/23838 completed (loss: 1.3221367597579956, acc: 0.6393442749977112)
[2025-02-16 11:32:05,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:05,287][root][INFO] - Training Epoch: 1/2, step 4637/23838 completed (loss: 0.44385066628456116, acc: 0.8947368264198303)
[2025-02-16 11:32:05,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:05,675][root][INFO] - Training Epoch: 1/2, step 4638/23838 completed (loss: 1.3491488695144653, acc: 0.6363636255264282)
[2025-02-16 11:32:05,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:06,117][root][INFO] - Training Epoch: 1/2, step 4639/23838 completed (loss: 1.090611219406128, acc: 0.7142857313156128)
[2025-02-16 11:32:06,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:06,670][root][INFO] - Training Epoch: 1/2, step 4640/23838 completed (loss: 1.3430885076522827, acc: 0.7586206793785095)
[2025-02-16 11:32:06,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:07,073][root][INFO] - Training Epoch: 1/2, step 4641/23838 completed (loss: 0.37000685930252075, acc: 0.9411764740943909)
[2025-02-16 11:32:07,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:07,493][root][INFO] - Training Epoch: 1/2, step 4642/23838 completed (loss: 0.9300286173820496, acc: 0.7361111044883728)
[2025-02-16 11:32:07,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:07,968][root][INFO] - Training Epoch: 1/2, step 4643/23838 completed (loss: 1.7440242767333984, acc: 0.5194805264472961)
[2025-02-16 11:32:08,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:08,532][root][INFO] - Training Epoch: 1/2, step 4644/23838 completed (loss: 1.5733916759490967, acc: 0.5869565010070801)
[2025-02-16 11:32:08,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:08,940][root][INFO] - Training Epoch: 1/2, step 4645/23838 completed (loss: 0.9840836524963379, acc: 0.6896551847457886)
[2025-02-16 11:32:09,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:09,370][root][INFO] - Training Epoch: 1/2, step 4646/23838 completed (loss: 0.7967642545700073, acc: 0.8024691343307495)
[2025-02-16 11:32:09,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:09,793][root][INFO] - Training Epoch: 1/2, step 4647/23838 completed (loss: 0.9027002453804016, acc: 0.7234042286872864)
[2025-02-16 11:32:10,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:10,229][root][INFO] - Training Epoch: 1/2, step 4648/23838 completed (loss: 1.3231995105743408, acc: 0.6760563254356384)
[2025-02-16 11:32:10,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:10,659][root][INFO] - Training Epoch: 1/2, step 4649/23838 completed (loss: 1.134378433227539, acc: 0.699999988079071)
[2025-02-16 11:32:10,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:11,256][root][INFO] - Training Epoch: 1/2, step 4650/23838 completed (loss: 0.9983626008033752, acc: 0.7289719581604004)
[2025-02-16 11:32:11,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:11,707][root][INFO] - Training Epoch: 1/2, step 4651/23838 completed (loss: 0.9298039078712463, acc: 0.7460317611694336)
[2025-02-16 11:32:11,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:12,134][root][INFO] - Training Epoch: 1/2, step 4652/23838 completed (loss: 0.6932735443115234, acc: 0.8269230723381042)
[2025-02-16 11:32:12,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:12,622][root][INFO] - Training Epoch: 1/2, step 4653/23838 completed (loss: 0.9349706768989563, acc: 0.8018018007278442)
[2025-02-16 11:32:12,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:13,082][root][INFO] - Training Epoch: 1/2, step 4654/23838 completed (loss: 0.9311683177947998, acc: 0.7407407164573669)
[2025-02-16 11:32:13,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:13,551][root][INFO] - Training Epoch: 1/2, step 4655/23838 completed (loss: 1.054524302482605, acc: 0.737500011920929)
[2025-02-16 11:32:13,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:13,966][root][INFO] - Training Epoch: 1/2, step 4656/23838 completed (loss: 1.4256439208984375, acc: 0.6428571343421936)
[2025-02-16 11:32:14,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:14,351][root][INFO] - Training Epoch: 1/2, step 4657/23838 completed (loss: 1.7692818641662598, acc: 0.5098039507865906)
[2025-02-16 11:32:14,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:14,729][root][INFO] - Training Epoch: 1/2, step 4658/23838 completed (loss: 1.2418144941329956, acc: 0.6521739363670349)
[2025-02-16 11:32:14,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:15,125][root][INFO] - Training Epoch: 1/2, step 4659/23838 completed (loss: 1.3105424642562866, acc: 0.6585366129875183)
[2025-02-16 11:32:15,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:15,549][root][INFO] - Training Epoch: 1/2, step 4660/23838 completed (loss: 1.3139067888259888, acc: 0.7142857313156128)
[2025-02-16 11:32:15,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:15,976][root][INFO] - Training Epoch: 1/2, step 4661/23838 completed (loss: 1.2768999338150024, acc: 0.7142857313156128)
[2025-02-16 11:32:16,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:16,408][root][INFO] - Training Epoch: 1/2, step 4662/23838 completed (loss: 1.0087801218032837, acc: 0.6333333253860474)
[2025-02-16 11:32:16,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:16,900][root][INFO] - Training Epoch: 1/2, step 4663/23838 completed (loss: 0.548592209815979, acc: 0.8823529481887817)
[2025-02-16 11:32:17,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:17,411][root][INFO] - Training Epoch: 1/2, step 4664/23838 completed (loss: 0.49286600947380066, acc: 0.837837815284729)
[2025-02-16 11:32:17,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:17,867][root][INFO] - Training Epoch: 1/2, step 4665/23838 completed (loss: 0.8584654927253723, acc: 0.7368420958518982)
[2025-02-16 11:32:18,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:18,351][root][INFO] - Training Epoch: 1/2, step 4666/23838 completed (loss: 0.6433442234992981, acc: 0.8225806355476379)
[2025-02-16 11:32:18,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:18,823][root][INFO] - Training Epoch: 1/2, step 4667/23838 completed (loss: 1.1082565784454346, acc: 0.7252747416496277)
[2025-02-16 11:32:19,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:19,254][root][INFO] - Training Epoch: 1/2, step 4668/23838 completed (loss: 0.5716965198516846, acc: 0.8125)
[2025-02-16 11:32:19,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:19,701][root][INFO] - Training Epoch: 1/2, step 4669/23838 completed (loss: 0.8357788920402527, acc: 0.761904776096344)
[2025-02-16 11:32:19,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:20,123][root][INFO] - Training Epoch: 1/2, step 4670/23838 completed (loss: 0.9840371608734131, acc: 0.7272727489471436)
[2025-02-16 11:32:20,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:20,594][root][INFO] - Training Epoch: 1/2, step 4671/23838 completed (loss: 0.7312239408493042, acc: 0.7701149582862854)
[2025-02-16 11:32:20,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:21,084][root][INFO] - Training Epoch: 1/2, step 4672/23838 completed (loss: 0.3838852345943451, acc: 0.9098360538482666)
[2025-02-16 11:32:21,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:21,545][root][INFO] - Training Epoch: 1/2, step 4673/23838 completed (loss: 0.6094070672988892, acc: 0.8048780560493469)
[2025-02-16 11:32:21,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:21,962][root][INFO] - Training Epoch: 1/2, step 4674/23838 completed (loss: 0.42177921533584595, acc: 0.8691588640213013)
[2025-02-16 11:32:22,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:22,491][root][INFO] - Training Epoch: 1/2, step 4675/23838 completed (loss: 0.6192391514778137, acc: 0.8247422575950623)
[2025-02-16 11:32:22,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:22,924][root][INFO] - Training Epoch: 1/2, step 4676/23838 completed (loss: 0.5087085962295532, acc: 0.8899999856948853)
[2025-02-16 11:32:23,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:23,381][root][INFO] - Training Epoch: 1/2, step 4677/23838 completed (loss: 0.8887876868247986, acc: 0.7407407164573669)
[2025-02-16 11:32:23,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:23,871][root][INFO] - Training Epoch: 1/2, step 4678/23838 completed (loss: 1.1302558183670044, acc: 0.6721311211585999)
[2025-02-16 11:32:24,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:24,437][root][INFO] - Training Epoch: 1/2, step 4679/23838 completed (loss: 0.6477750539779663, acc: 0.84375)
[2025-02-16 11:32:24,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:24,870][root][INFO] - Training Epoch: 1/2, step 4680/23838 completed (loss: 0.6868469715118408, acc: 0.8048780560493469)
[2025-02-16 11:32:25,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:25,344][root][INFO] - Training Epoch: 1/2, step 4681/23838 completed (loss: 1.8102995157241821, acc: 0.5454545617103577)
[2025-02-16 11:32:25,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:25,750][root][INFO] - Training Epoch: 1/2, step 4682/23838 completed (loss: 1.828755497932434, acc: 0.5384615659713745)
[2025-02-16 11:32:25,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:26,186][root][INFO] - Training Epoch: 1/2, step 4683/23838 completed (loss: 0.3805249035358429, acc: 0.8399999737739563)
[2025-02-16 11:32:26,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:26,582][root][INFO] - Training Epoch: 1/2, step 4684/23838 completed (loss: 1.4053246974945068, acc: 0.699999988079071)
[2025-02-16 11:32:26,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:27,005][root][INFO] - Training Epoch: 1/2, step 4685/23838 completed (loss: 0.9837141633033752, acc: 0.6842105388641357)
[2025-02-16 11:32:27,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:27,388][root][INFO] - Training Epoch: 1/2, step 4686/23838 completed (loss: 2.2128026485443115, acc: 0.5526315569877625)
[2025-02-16 11:32:27,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:27,773][root][INFO] - Training Epoch: 1/2, step 4687/23838 completed (loss: 0.5081358551979065, acc: 0.8260869383811951)
[2025-02-16 11:32:27,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:28,195][root][INFO] - Training Epoch: 1/2, step 4688/23838 completed (loss: 0.2189764380455017, acc: 0.9333333373069763)
[2025-02-16 11:32:28,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:28,581][root][INFO] - Training Epoch: 1/2, step 4689/23838 completed (loss: 1.1705158948898315, acc: 0.692307710647583)
[2025-02-16 11:32:28,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:29,010][root][INFO] - Training Epoch: 1/2, step 4690/23838 completed (loss: 0.3307722806930542, acc: 1.0)
[2025-02-16 11:32:29,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:29,452][root][INFO] - Training Epoch: 1/2, step 4691/23838 completed (loss: 0.34714874625205994, acc: 0.8928571343421936)
[2025-02-16 11:32:29,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:29,877][root][INFO] - Training Epoch: 1/2, step 4692/23838 completed (loss: 0.3110218942165375, acc: 0.9316239356994629)
[2025-02-16 11:32:30,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:30,287][root][INFO] - Training Epoch: 1/2, step 4693/23838 completed (loss: 0.5730593800544739, acc: 0.8396226167678833)
[2025-02-16 11:32:30,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:30,696][root][INFO] - Training Epoch: 1/2, step 4694/23838 completed (loss: 0.3241257667541504, acc: 0.9245283007621765)
[2025-02-16 11:32:30,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:31,183][root][INFO] - Training Epoch: 1/2, step 4695/23838 completed (loss: 0.2972989082336426, acc: 0.9111111164093018)
[2025-02-16 11:32:31,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:31,607][root][INFO] - Training Epoch: 1/2, step 4696/23838 completed (loss: 0.42482972145080566, acc: 0.8852459192276001)
[2025-02-16 11:32:31,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:32,114][root][INFO] - Training Epoch: 1/2, step 4697/23838 completed (loss: 0.3897532522678375, acc: 0.8857142925262451)
[2025-02-16 11:32:32,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:32,584][root][INFO] - Training Epoch: 1/2, step 4698/23838 completed (loss: 0.20904606580734253, acc: 0.9298245906829834)
[2025-02-16 11:32:32,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:33,035][root][INFO] - Training Epoch: 1/2, step 4699/23838 completed (loss: 0.6674879193305969, acc: 0.8531468510627747)
[2025-02-16 11:32:33,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:33,444][root][INFO] - Training Epoch: 1/2, step 4700/23838 completed (loss: 0.3617786169052124, acc: 0.9257143139839172)
[2025-02-16 11:32:33,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:33,867][root][INFO] - Training Epoch: 1/2, step 4701/23838 completed (loss: 0.46201470494270325, acc: 0.9122806787490845)
[2025-02-16 11:32:34,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:34,240][root][INFO] - Training Epoch: 1/2, step 4702/23838 completed (loss: 0.3223021924495697, acc: 0.9047619104385376)
[2025-02-16 11:32:34,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:34,673][root][INFO] - Training Epoch: 1/2, step 4703/23838 completed (loss: 0.3636972904205322, acc: 0.9210526347160339)
[2025-02-16 11:32:34,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:35,147][root][INFO] - Training Epoch: 1/2, step 4704/23838 completed (loss: 0.37469932436943054, acc: 0.8888888955116272)
[2025-02-16 11:32:35,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:35,626][root][INFO] - Training Epoch: 1/2, step 4705/23838 completed (loss: 0.5249485969543457, acc: 0.8782608509063721)
[2025-02-16 11:32:35,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:36,039][root][INFO] - Training Epoch: 1/2, step 4706/23838 completed (loss: 0.4984833598136902, acc: 0.8686131238937378)
[2025-02-16 11:32:36,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:36,420][root][INFO] - Training Epoch: 1/2, step 4707/23838 completed (loss: 0.7157606482505798, acc: 0.8484848737716675)
[2025-02-16 11:32:36,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:36,813][root][INFO] - Training Epoch: 1/2, step 4708/23838 completed (loss: 0.5235211253166199, acc: 0.9166666865348816)
[2025-02-16 11:32:36,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:37,232][root][INFO] - Training Epoch: 1/2, step 4709/23838 completed (loss: 0.3067118227481842, acc: 0.9464285969734192)
[2025-02-16 11:32:37,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:37,646][root][INFO] - Training Epoch: 1/2, step 4710/23838 completed (loss: 0.8234866857528687, acc: 0.7850467562675476)
[2025-02-16 11:32:37,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:38,024][root][INFO] - Training Epoch: 1/2, step 4711/23838 completed (loss: 0.43074557185173035, acc: 0.9146341681480408)
[2025-02-16 11:32:38,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:38,488][root][INFO] - Training Epoch: 1/2, step 4712/23838 completed (loss: 0.5610544085502625, acc: 0.875)
[2025-02-16 11:32:38,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:38,936][root][INFO] - Training Epoch: 1/2, step 4713/23838 completed (loss: 0.4389134645462036, acc: 0.888059675693512)
[2025-02-16 11:32:39,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:39,387][root][INFO] - Training Epoch: 1/2, step 4714/23838 completed (loss: 0.32324209809303284, acc: 0.9292035102844238)
[2025-02-16 11:32:39,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:39,852][root][INFO] - Training Epoch: 1/2, step 4715/23838 completed (loss: 0.5732225775718689, acc: 0.9029126167297363)
[2025-02-16 11:32:39,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:40,234][root][INFO] - Training Epoch: 1/2, step 4716/23838 completed (loss: 0.25319597125053406, acc: 0.9292035102844238)
[2025-02-16 11:32:40,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:40,676][root][INFO] - Training Epoch: 1/2, step 4717/23838 completed (loss: 0.4350320100784302, acc: 0.882022500038147)
[2025-02-16 11:32:40,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:41,204][root][INFO] - Training Epoch: 1/2, step 4718/23838 completed (loss: 0.574420690536499, acc: 0.8918918967247009)
[2025-02-16 11:32:41,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:41,660][root][INFO] - Training Epoch: 1/2, step 4719/23838 completed (loss: 0.7690287232398987, acc: 0.7909091114997864)
[2025-02-16 11:32:41,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:42,146][root][INFO] - Training Epoch: 1/2, step 4720/23838 completed (loss: 0.6829618811607361, acc: 0.8055555820465088)
[2025-02-16 11:32:42,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:42,616][root][INFO] - Training Epoch: 1/2, step 4721/23838 completed (loss: 0.2727154493331909, acc: 0.9213483333587646)
[2025-02-16 11:32:42,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:43,076][root][INFO] - Training Epoch: 1/2, step 4722/23838 completed (loss: 0.17870253324508667, acc: 0.95652174949646)
[2025-02-16 11:32:43,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:43,585][root][INFO] - Training Epoch: 1/2, step 4723/23838 completed (loss: 0.2235403060913086, acc: 0.9466666579246521)
[2025-02-16 11:32:43,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:44,009][root][INFO] - Training Epoch: 1/2, step 4724/23838 completed (loss: 0.1399381160736084, acc: 0.9586777091026306)
[2025-02-16 11:32:44,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:44,419][root][INFO] - Training Epoch: 1/2, step 4725/23838 completed (loss: 0.2138860821723938, acc: 0.9436619877815247)
[2025-02-16 11:32:44,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:44,856][root][INFO] - Training Epoch: 1/2, step 4726/23838 completed (loss: 0.42182374000549316, acc: 0.8857142925262451)
[2025-02-16 11:32:44,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:45,288][root][INFO] - Training Epoch: 1/2, step 4727/23838 completed (loss: 0.11597701907157898, acc: 0.9772727489471436)
[2025-02-16 11:32:45,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:45,710][root][INFO] - Training Epoch: 1/2, step 4728/23838 completed (loss: 0.21883781254291534, acc: 0.9214285612106323)
[2025-02-16 11:32:45,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:46,150][root][INFO] - Training Epoch: 1/2, step 4729/23838 completed (loss: 0.21544736623764038, acc: 0.949367105960846)
[2025-02-16 11:32:46,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:46,532][root][INFO] - Training Epoch: 1/2, step 4730/23838 completed (loss: 0.27985450625419617, acc: 0.929411768913269)
[2025-02-16 11:32:46,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:46,954][root][INFO] - Training Epoch: 1/2, step 4731/23838 completed (loss: 0.29572200775146484, acc: 0.9076923131942749)
[2025-02-16 11:32:47,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:47,326][root][INFO] - Training Epoch: 1/2, step 4732/23838 completed (loss: 0.6727979183197021, acc: 0.7222222089767456)
[2025-02-16 11:32:47,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:47,763][root][INFO] - Training Epoch: 1/2, step 4733/23838 completed (loss: 0.8485093116760254, acc: 0.71875)
[2025-02-16 11:32:47,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:48,215][root][INFO] - Training Epoch: 1/2, step 4734/23838 completed (loss: 0.9997788667678833, acc: 0.5789473652839661)
[2025-02-16 11:32:48,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:48,649][root][INFO] - Training Epoch: 1/2, step 4735/23838 completed (loss: 0.7397348284721375, acc: 0.75)
[2025-02-16 11:32:48,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:49,120][root][INFO] - Training Epoch: 1/2, step 4736/23838 completed (loss: 1.1652500629425049, acc: 0.6785714030265808)
[2025-02-16 11:32:49,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:49,564][root][INFO] - Training Epoch: 1/2, step 4737/23838 completed (loss: 0.6227781772613525, acc: 0.8500000238418579)
[2025-02-16 11:32:49,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:50,010][root][INFO] - Training Epoch: 1/2, step 4738/23838 completed (loss: 1.3213759660720825, acc: 0.6521739363670349)
[2025-02-16 11:32:50,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:50,441][root][INFO] - Training Epoch: 1/2, step 4739/23838 completed (loss: 1.5383508205413818, acc: 0.6000000238418579)
[2025-02-16 11:32:50,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:50,831][root][INFO] - Training Epoch: 1/2, step 4740/23838 completed (loss: 0.9501756429672241, acc: 0.7234042286872864)
[2025-02-16 11:32:51,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:51,290][root][INFO] - Training Epoch: 1/2, step 4741/23838 completed (loss: 1.5968350172042847, acc: 0.5625)
[2025-02-16 11:32:51,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:51,746][root][INFO] - Training Epoch: 1/2, step 4742/23838 completed (loss: 1.2861871719360352, acc: 0.5714285969734192)
[2025-02-16 11:32:51,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:52,270][root][INFO] - Training Epoch: 1/2, step 4743/23838 completed (loss: 1.506461262702942, acc: 0.5151515007019043)
[2025-02-16 11:32:52,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:52,680][root][INFO] - Training Epoch: 1/2, step 4744/23838 completed (loss: 1.4196984767913818, acc: 0.5869565010070801)
[2025-02-16 11:32:52,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:53,158][root][INFO] - Training Epoch: 1/2, step 4745/23838 completed (loss: 1.4067050218582153, acc: 0.5945945978164673)
[2025-02-16 11:32:53,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:53,551][root][INFO] - Training Epoch: 1/2, step 4746/23838 completed (loss: 0.9012590050697327, acc: 0.6666666865348816)
[2025-02-16 11:32:54,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:54,450][root][INFO] - Training Epoch: 1/2, step 4747/23838 completed (loss: 1.5521365404129028, acc: 0.6000000238418579)
[2025-02-16 11:32:54,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:55,336][root][INFO] - Training Epoch: 1/2, step 4748/23838 completed (loss: 1.2669631242752075, acc: 0.7142857313156128)
[2025-02-16 11:32:55,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:55,823][root][INFO] - Training Epoch: 1/2, step 4749/23838 completed (loss: 0.7182499170303345, acc: 0.7631579041481018)
[2025-02-16 11:32:56,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:56,265][root][INFO] - Training Epoch: 1/2, step 4750/23838 completed (loss: 0.5274928212165833, acc: 0.8474576473236084)
[2025-02-16 11:32:56,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:56,702][root][INFO] - Training Epoch: 1/2, step 4751/23838 completed (loss: 1.6894487142562866, acc: 0.6428571343421936)
[2025-02-16 11:32:56,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:57,178][root][INFO] - Training Epoch: 1/2, step 4752/23838 completed (loss: 0.3642083704471588, acc: 0.871999979019165)
[2025-02-16 11:32:57,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:57,583][root][INFO] - Training Epoch: 1/2, step 4753/23838 completed (loss: 1.111687183380127, acc: 0.7027027010917664)
[2025-02-16 11:32:57,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:57,982][root][INFO] - Training Epoch: 1/2, step 4754/23838 completed (loss: 1.084925651550293, acc: 0.6987951993942261)
[2025-02-16 11:32:58,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:58,422][root][INFO] - Training Epoch: 1/2, step 4755/23838 completed (loss: 0.8237146139144897, acc: 0.800000011920929)
[2025-02-16 11:32:58,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:58,875][root][INFO] - Training Epoch: 1/2, step 4756/23838 completed (loss: 0.9436118006706238, acc: 0.7222222089767456)
[2025-02-16 11:32:59,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:59,341][root][INFO] - Training Epoch: 1/2, step 4757/23838 completed (loss: 0.46339505910873413, acc: 0.84375)
[2025-02-16 11:32:59,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:32:59,825][root][INFO] - Training Epoch: 1/2, step 4758/23838 completed (loss: 0.5645984411239624, acc: 0.871999979019165)
[2025-02-16 11:33:00,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:00,247][root][INFO] - Training Epoch: 1/2, step 4759/23838 completed (loss: 0.7350221276283264, acc: 0.7762237787246704)
[2025-02-16 11:33:00,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:00,694][root][INFO] - Training Epoch: 1/2, step 4760/23838 completed (loss: 0.6468135118484497, acc: 0.800000011920929)
[2025-02-16 11:33:00,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:01,120][root][INFO] - Training Epoch: 1/2, step 4761/23838 completed (loss: 0.7160854339599609, acc: 0.8156028389930725)
[2025-02-16 11:33:01,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:01,554][root][INFO] - Training Epoch: 1/2, step 4762/23838 completed (loss: 0.7253293991088867, acc: 0.8166666626930237)
[2025-02-16 11:33:01,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:01,961][root][INFO] - Training Epoch: 1/2, step 4763/23838 completed (loss: 0.6174373030662537, acc: 0.8492063283920288)
[2025-02-16 11:33:02,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:02,351][root][INFO] - Training Epoch: 1/2, step 4764/23838 completed (loss: 0.7070516347885132, acc: 0.8134328126907349)
[2025-02-16 11:33:02,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:02,722][root][INFO] - Training Epoch: 1/2, step 4765/23838 completed (loss: 0.7055591940879822, acc: 0.8111110925674438)
[2025-02-16 11:33:02,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:03,126][root][INFO] - Training Epoch: 1/2, step 4766/23838 completed (loss: 0.4707854390144348, acc: 0.8829787373542786)
[2025-02-16 11:33:03,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:03,524][root][INFO] - Training Epoch: 1/2, step 4767/23838 completed (loss: 0.7023686170578003, acc: 0.8642857074737549)
[2025-02-16 11:33:03,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:03,991][root][INFO] - Training Epoch: 1/2, step 4768/23838 completed (loss: 0.787363588809967, acc: 0.7808219194412231)
[2025-02-16 11:33:04,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:04,425][root][INFO] - Training Epoch: 1/2, step 4769/23838 completed (loss: 0.5744084119796753, acc: 0.8630136847496033)
[2025-02-16 11:33:04,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:04,870][root][INFO] - Training Epoch: 1/2, step 4770/23838 completed (loss: 0.5170698761940002, acc: 0.8620689511299133)
[2025-02-16 11:33:05,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:05,278][root][INFO] - Training Epoch: 1/2, step 4771/23838 completed (loss: 1.3286092281341553, acc: 0.6349206566810608)
[2025-02-16 11:33:05,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:05,699][root][INFO] - Training Epoch: 1/2, step 4772/23838 completed (loss: 0.700640857219696, acc: 0.8317757248878479)
[2025-02-16 11:33:05,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:06,210][root][INFO] - Training Epoch: 1/2, step 4773/23838 completed (loss: 0.6002464294433594, acc: 0.8343558311462402)
[2025-02-16 11:33:06,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:06,655][root][INFO] - Training Epoch: 1/2, step 4774/23838 completed (loss: 0.4249117076396942, acc: 0.9032257795333862)
[2025-02-16 11:33:06,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:07,145][root][INFO] - Training Epoch: 1/2, step 4775/23838 completed (loss: 0.6309596300125122, acc: 0.8474576473236084)
[2025-02-16 11:33:07,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:07,605][root][INFO] - Training Epoch: 1/2, step 4776/23838 completed (loss: 0.7592118382453918, acc: 0.7843137383460999)
[2025-02-16 11:33:07,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:08,101][root][INFO] - Training Epoch: 1/2, step 4777/23838 completed (loss: 0.38089239597320557, acc: 0.9032257795333862)
[2025-02-16 11:33:08,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:08,572][root][INFO] - Training Epoch: 1/2, step 4778/23838 completed (loss: 0.4758673906326294, acc: 0.8578947186470032)
[2025-02-16 11:33:08,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:09,020][root][INFO] - Training Epoch: 1/2, step 4779/23838 completed (loss: 0.5769452452659607, acc: 0.8321678042411804)
[2025-02-16 11:33:09,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:09,472][root][INFO] - Training Epoch: 1/2, step 4780/23838 completed (loss: 0.5216035842895508, acc: 0.8628571629524231)
[2025-02-16 11:33:09,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:09,867][root][INFO] - Training Epoch: 1/2, step 4781/23838 completed (loss: 0.45839935541152954, acc: 0.8656716346740723)
[2025-02-16 11:33:10,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:10,320][root][INFO] - Training Epoch: 1/2, step 4782/23838 completed (loss: 0.7085469961166382, acc: 0.7731958627700806)
[2025-02-16 11:33:10,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:10,728][root][INFO] - Training Epoch: 1/2, step 4783/23838 completed (loss: 0.3268703818321228, acc: 0.9038461446762085)
[2025-02-16 11:33:10,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:11,198][root][INFO] - Training Epoch: 1/2, step 4784/23838 completed (loss: 0.8910999298095703, acc: 0.7583333253860474)
[2025-02-16 11:33:11,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:11,688][root][INFO] - Training Epoch: 1/2, step 4785/23838 completed (loss: 0.5760178565979004, acc: 0.8518518805503845)
[2025-02-16 11:33:11,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:12,144][root][INFO] - Training Epoch: 1/2, step 4786/23838 completed (loss: 0.8764337301254272, acc: 0.7558139562606812)
[2025-02-16 11:33:12,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:12,621][root][INFO] - Training Epoch: 1/2, step 4787/23838 completed (loss: 0.5845893621444702, acc: 0.8225806355476379)
[2025-02-16 11:33:12,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:13,111][root][INFO] - Training Epoch: 1/2, step 4788/23838 completed (loss: 0.9749454259872437, acc: 0.7222222089767456)
[2025-02-16 11:33:13,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:13,515][root][INFO] - Training Epoch: 1/2, step 4789/23838 completed (loss: 0.43362998962402344, acc: 0.8666666746139526)
[2025-02-16 11:33:13,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:13,900][root][INFO] - Training Epoch: 1/2, step 4790/23838 completed (loss: 0.4224589467048645, acc: 0.8999999761581421)
[2025-02-16 11:33:14,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:14,352][root][INFO] - Training Epoch: 1/2, step 4791/23838 completed (loss: 0.5503763556480408, acc: 0.8584905862808228)
[2025-02-16 11:33:14,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:14,765][root][INFO] - Training Epoch: 1/2, step 4792/23838 completed (loss: 0.49355947971343994, acc: 0.8666666746139526)
[2025-02-16 11:33:14,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:15,170][root][INFO] - Training Epoch: 1/2, step 4793/23838 completed (loss: 0.5125935673713684, acc: 0.8651685118675232)
[2025-02-16 11:33:15,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:15,689][root][INFO] - Training Epoch: 1/2, step 4794/23838 completed (loss: 0.30890148878097534, acc: 0.9230769276618958)
[2025-02-16 11:33:15,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:16,236][root][INFO] - Training Epoch: 1/2, step 4795/23838 completed (loss: 0.6593091487884521, acc: 0.8372092843055725)
[2025-02-16 11:33:16,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:16,677][root][INFO] - Training Epoch: 1/2, step 4796/23838 completed (loss: 0.5934382677078247, acc: 0.8742514848709106)
[2025-02-16 11:33:16,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:17,045][root][INFO] - Training Epoch: 1/2, step 4797/23838 completed (loss: 0.9877995252609253, acc: 0.7669172883033752)
[2025-02-16 11:33:17,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:17,664][root][INFO] - Training Epoch: 1/2, step 4798/23838 completed (loss: 0.5540657639503479, acc: 0.8426966071128845)
[2025-02-16 11:33:17,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:18,049][root][INFO] - Training Epoch: 1/2, step 4799/23838 completed (loss: 1.1327707767486572, acc: 0.6595744490623474)
[2025-02-16 11:33:18,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:18,446][root][INFO] - Training Epoch: 1/2, step 4800/23838 completed (loss: 0.7458357810974121, acc: 0.8095238208770752)
[2025-02-16 11:33:18,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:18,880][root][INFO] - Training Epoch: 1/2, step 4801/23838 completed (loss: 0.20639660954475403, acc: 0.9333333373069763)
[2025-02-16 11:33:19,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:19,320][root][INFO] - Training Epoch: 1/2, step 4802/23838 completed (loss: 0.4299570620059967, acc: 0.8666666746139526)
[2025-02-16 11:33:19,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:19,753][root][INFO] - Training Epoch: 1/2, step 4803/23838 completed (loss: 0.575298547744751, acc: 0.8316831588745117)
[2025-02-16 11:33:20,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:20,353][root][INFO] - Training Epoch: 1/2, step 4804/23838 completed (loss: 0.36820051074028015, acc: 0.896039605140686)
[2025-02-16 11:33:20,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:20,767][root][INFO] - Training Epoch: 1/2, step 4805/23838 completed (loss: 0.6124909520149231, acc: 0.8571428656578064)
[2025-02-16 11:33:21,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:21,300][root][INFO] - Training Epoch: 1/2, step 4806/23838 completed (loss: 0.5389434695243835, acc: 0.8367347121238708)
[2025-02-16 11:33:21,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:21,688][root][INFO] - Training Epoch: 1/2, step 4807/23838 completed (loss: 0.7017660140991211, acc: 0.8085106611251831)
[2025-02-16 11:33:21,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:22,170][root][INFO] - Training Epoch: 1/2, step 4808/23838 completed (loss: 0.918687641620636, acc: 0.7829457521438599)
[2025-02-16 11:33:22,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:22,769][root][INFO] - Training Epoch: 1/2, step 4809/23838 completed (loss: 0.6804062724113464, acc: 0.8028169274330139)
[2025-02-16 11:33:22,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:23,205][root][INFO] - Training Epoch: 1/2, step 4810/23838 completed (loss: 0.5178136229515076, acc: 0.875)
[2025-02-16 11:33:23,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:23,845][root][INFO] - Training Epoch: 1/2, step 4811/23838 completed (loss: 0.4864647388458252, acc: 0.8724489808082581)
[2025-02-16 11:33:24,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:24,271][root][INFO] - Training Epoch: 1/2, step 4812/23838 completed (loss: 0.475530743598938, acc: 0.8916666507720947)
[2025-02-16 11:33:24,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:24,825][root][INFO] - Training Epoch: 1/2, step 4813/23838 completed (loss: 0.4642007648944855, acc: 0.867521345615387)
[2025-02-16 11:33:25,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:25,444][root][INFO] - Training Epoch: 1/2, step 4814/23838 completed (loss: 0.6713014245033264, acc: 0.8471337556838989)
[2025-02-16 11:33:25,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:26,460][root][INFO] - Training Epoch: 1/2, step 4815/23838 completed (loss: 0.6801815629005432, acc: 0.8215962648391724)
[2025-02-16 11:33:26,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:26,931][root][INFO] - Training Epoch: 1/2, step 4816/23838 completed (loss: 0.5849645733833313, acc: 0.8484848737716675)
[2025-02-16 11:33:27,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:27,444][root][INFO] - Training Epoch: 1/2, step 4817/23838 completed (loss: 0.7791579961776733, acc: 0.7540983557701111)
[2025-02-16 11:33:27,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:27,877][root][INFO] - Training Epoch: 1/2, step 4818/23838 completed (loss: 0.7215943336486816, acc: 0.8141025900840759)
[2025-02-16 11:33:28,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:28,356][root][INFO] - Training Epoch: 1/2, step 4819/23838 completed (loss: 0.49188855290412903, acc: 0.8976377844810486)
[2025-02-16 11:33:28,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:28,870][root][INFO] - Training Epoch: 1/2, step 4820/23838 completed (loss: 0.6668518781661987, acc: 0.8199999928474426)
[2025-02-16 11:33:29,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:29,272][root][INFO] - Training Epoch: 1/2, step 4821/23838 completed (loss: 1.303391933441162, acc: 0.6338028311729431)
[2025-02-16 11:33:29,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:29,750][root][INFO] - Training Epoch: 1/2, step 4822/23838 completed (loss: 0.46987292170524597, acc: 0.9009901285171509)
[2025-02-16 11:33:29,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:30,258][root][INFO] - Training Epoch: 1/2, step 4823/23838 completed (loss: 0.11660119146108627, acc: 0.9599999785423279)
[2025-02-16 11:33:30,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:30,636][root][INFO] - Training Epoch: 1/2, step 4824/23838 completed (loss: 0.6759799122810364, acc: 0.8105263113975525)
[2025-02-16 11:33:30,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:31,018][root][INFO] - Training Epoch: 1/2, step 4825/23838 completed (loss: 0.3457934558391571, acc: 0.8909090757369995)
[2025-02-16 11:33:31,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:31,380][root][INFO] - Training Epoch: 1/2, step 4826/23838 completed (loss: 0.5422212481498718, acc: 0.8571428656578064)
[2025-02-16 11:33:31,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:31,759][root][INFO] - Training Epoch: 1/2, step 4827/23838 completed (loss: 0.3820067048072815, acc: 0.8780487775802612)
[2025-02-16 11:33:31,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:32,166][root][INFO] - Training Epoch: 1/2, step 4828/23838 completed (loss: 0.7700177431106567, acc: 0.7924528121948242)
[2025-02-16 11:33:32,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:32,568][root][INFO] - Training Epoch: 1/2, step 4829/23838 completed (loss: 0.2391519844532013, acc: 0.939393937587738)
[2025-02-16 11:33:32,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:32,932][root][INFO] - Training Epoch: 1/2, step 4830/23838 completed (loss: 1.2429473400115967, acc: 0.5714285969734192)
[2025-02-16 11:33:33,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:33,361][root][INFO] - Training Epoch: 1/2, step 4831/23838 completed (loss: 1.0763590335845947, acc: 0.6176470518112183)
[2025-02-16 11:33:33,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:33,898][root][INFO] - Training Epoch: 1/2, step 4832/23838 completed (loss: 0.36927109956741333, acc: 0.9555555582046509)
[2025-02-16 11:33:34,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:34,325][root][INFO] - Training Epoch: 1/2, step 4833/23838 completed (loss: 0.5887027382850647, acc: 0.8571428656578064)
[2025-02-16 11:33:34,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:34,741][root][INFO] - Training Epoch: 1/2, step 4834/23838 completed (loss: 1.856555700302124, acc: 0.5365853905677795)
[2025-02-16 11:33:34,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:35,212][root][INFO] - Training Epoch: 1/2, step 4835/23838 completed (loss: 0.7615600228309631, acc: 0.7945205569267273)
[2025-02-16 11:33:35,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:35,682][root][INFO] - Training Epoch: 1/2, step 4836/23838 completed (loss: 0.5075728297233582, acc: 0.8409090638160706)
[2025-02-16 11:33:35,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:36,119][root][INFO] - Training Epoch: 1/2, step 4837/23838 completed (loss: 0.6861969232559204, acc: 0.7704917788505554)
[2025-02-16 11:33:36,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:36,526][root][INFO] - Training Epoch: 1/2, step 4838/23838 completed (loss: 1.0017688274383545, acc: 0.7313432693481445)
[2025-02-16 11:33:36,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:36,962][root][INFO] - Training Epoch: 1/2, step 4839/23838 completed (loss: 1.0985621213912964, acc: 0.699999988079071)
[2025-02-16 11:33:37,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:37,375][root][INFO] - Training Epoch: 1/2, step 4840/23838 completed (loss: 1.1173619031906128, acc: 0.7128713130950928)
[2025-02-16 11:33:37,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:37,836][root][INFO] - Training Epoch: 1/2, step 4841/23838 completed (loss: 0.7150036692619324, acc: 0.8591549396514893)
[2025-02-16 11:33:38,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:38,383][root][INFO] - Training Epoch: 1/2, step 4842/23838 completed (loss: 0.5514665842056274, acc: 0.8823529481887817)
[2025-02-16 11:33:38,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:38,831][root][INFO] - Training Epoch: 1/2, step 4843/23838 completed (loss: 0.2977532744407654, acc: 0.9166666865348816)
[2025-02-16 11:33:39,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:39,256][root][INFO] - Training Epoch: 1/2, step 4844/23838 completed (loss: 0.8351426124572754, acc: 0.7916666865348816)
[2025-02-16 11:33:39,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:39,713][root][INFO] - Training Epoch: 1/2, step 4845/23838 completed (loss: 0.7440006136894226, acc: 0.7419354915618896)
[2025-02-16 11:33:39,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:40,092][root][INFO] - Training Epoch: 1/2, step 4846/23838 completed (loss: 0.5537543892860413, acc: 0.8913043737411499)
[2025-02-16 11:33:40,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:40,484][root][INFO] - Training Epoch: 1/2, step 4847/23838 completed (loss: 0.31456273794174194, acc: 0.9230769276618958)
[2025-02-16 11:33:40,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:40,956][root][INFO] - Training Epoch: 1/2, step 4848/23838 completed (loss: 0.37486883997917175, acc: 0.914893627166748)
[2025-02-16 11:33:41,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:41,387][root][INFO] - Training Epoch: 1/2, step 4849/23838 completed (loss: 1.1818597316741943, acc: 0.7341772317886353)
[2025-02-16 11:33:41,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:41,767][root][INFO] - Training Epoch: 1/2, step 4850/23838 completed (loss: 0.7064614295959473, acc: 0.7868852615356445)
[2025-02-16 11:33:41,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:42,136][root][INFO] - Training Epoch: 1/2, step 4851/23838 completed (loss: 0.9538005590438843, acc: 0.7857142686843872)
[2025-02-16 11:33:42,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:42,551][root][INFO] - Training Epoch: 1/2, step 4852/23838 completed (loss: 0.4560041129589081, acc: 0.8787878751754761)
[2025-02-16 11:33:42,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:43,028][root][INFO] - Training Epoch: 1/2, step 4853/23838 completed (loss: 0.4897451102733612, acc: 0.9090909361839294)
[2025-02-16 11:33:43,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:43,458][root][INFO] - Training Epoch: 1/2, step 4854/23838 completed (loss: 0.4127792418003082, acc: 0.8823529481887817)
[2025-02-16 11:33:43,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:43,861][root][INFO] - Training Epoch: 1/2, step 4855/23838 completed (loss: 0.9797164797782898, acc: 0.75)
[2025-02-16 11:33:44,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:44,287][root][INFO] - Training Epoch: 1/2, step 4856/23838 completed (loss: 0.9079853296279907, acc: 0.75)
[2025-02-16 11:33:44,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:44,735][root][INFO] - Training Epoch: 1/2, step 4857/23838 completed (loss: 0.8906648755073547, acc: 0.8285714387893677)
[2025-02-16 11:33:44,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:45,202][root][INFO] - Training Epoch: 1/2, step 4858/23838 completed (loss: 0.9292499423027039, acc: 0.8275862336158752)
[2025-02-16 11:33:45,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:45,689][root][INFO] - Training Epoch: 1/2, step 4859/23838 completed (loss: 0.8899018168449402, acc: 0.7538461685180664)
[2025-02-16 11:33:46,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:46,433][root][INFO] - Training Epoch: 1/2, step 4860/23838 completed (loss: 0.6630807518959045, acc: 0.8103448152542114)
[2025-02-16 11:33:46,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:46,829][root][INFO] - Training Epoch: 1/2, step 4861/23838 completed (loss: 0.9737227559089661, acc: 0.699999988079071)
[2025-02-16 11:33:47,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:47,290][root][INFO] - Training Epoch: 1/2, step 4862/23838 completed (loss: 0.6131709218025208, acc: 0.8717948794364929)
[2025-02-16 11:33:47,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:47,728][root][INFO] - Training Epoch: 1/2, step 4863/23838 completed (loss: 0.49517732858657837, acc: 0.8888888955116272)
[2025-02-16 11:33:47,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:48,177][root][INFO] - Training Epoch: 1/2, step 4864/23838 completed (loss: 0.9374915361404419, acc: 0.7222222089767456)
[2025-02-16 11:33:48,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:48,673][root][INFO] - Training Epoch: 1/2, step 4865/23838 completed (loss: 0.9551770687103271, acc: 0.7377049326896667)
[2025-02-16 11:33:48,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:49,180][root][INFO] - Training Epoch: 1/2, step 4866/23838 completed (loss: 0.7254658341407776, acc: 0.75)
[2025-02-16 11:33:49,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:49,640][root][INFO] - Training Epoch: 1/2, step 4867/23838 completed (loss: 0.3885623812675476, acc: 0.90625)
[2025-02-16 11:33:49,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:50,125][root][INFO] - Training Epoch: 1/2, step 4868/23838 completed (loss: 1.146395206451416, acc: 0.7017543911933899)
[2025-02-16 11:33:50,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:50,626][root][INFO] - Training Epoch: 1/2, step 4869/23838 completed (loss: 1.465596079826355, acc: 0.5925925970077515)
[2025-02-16 11:33:50,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:51,010][root][INFO] - Training Epoch: 1/2, step 4870/23838 completed (loss: 0.9206180572509766, acc: 0.7407407164573669)
[2025-02-16 11:33:51,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:51,446][root][INFO] - Training Epoch: 1/2, step 4871/23838 completed (loss: 0.38225266337394714, acc: 0.8409090638160706)
[2025-02-16 11:33:51,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:51,859][root][INFO] - Training Epoch: 1/2, step 4872/23838 completed (loss: 0.8815829157829285, acc: 0.8103448152542114)
[2025-02-16 11:33:52,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:52,387][root][INFO] - Training Epoch: 1/2, step 4873/23838 completed (loss: 0.9239919185638428, acc: 0.7416666746139526)
[2025-02-16 11:33:52,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:52,791][root][INFO] - Training Epoch: 1/2, step 4874/23838 completed (loss: 1.3185701370239258, acc: 0.6451612710952759)
[2025-02-16 11:33:52,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:53,176][root][INFO] - Training Epoch: 1/2, step 4875/23838 completed (loss: 1.3283339738845825, acc: 0.604651153087616)
[2025-02-16 11:33:53,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:53,718][root][INFO] - Training Epoch: 1/2, step 4876/23838 completed (loss: 0.9565715789794922, acc: 0.7183098793029785)
[2025-02-16 11:33:53,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:54,125][root][INFO] - Training Epoch: 1/2, step 4877/23838 completed (loss: 0.9334853291511536, acc: 0.7250000238418579)
[2025-02-16 11:33:54,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:54,515][root][INFO] - Training Epoch: 1/2, step 4878/23838 completed (loss: 1.100071668624878, acc: 0.719298243522644)
[2025-02-16 11:33:54,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:54,918][root][INFO] - Training Epoch: 1/2, step 4879/23838 completed (loss: 1.097104787826538, acc: 0.6363636255264282)
[2025-02-16 11:33:55,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:55,467][root][INFO] - Training Epoch: 1/2, step 4880/23838 completed (loss: 0.8054782748222351, acc: 0.8235294222831726)
[2025-02-16 11:33:55,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:56,163][root][INFO] - Training Epoch: 1/2, step 4881/23838 completed (loss: 0.8539658188819885, acc: 0.8095238208770752)
[2025-02-16 11:33:56,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:56,583][root][INFO] - Training Epoch: 1/2, step 4882/23838 completed (loss: 0.9619753956794739, acc: 0.738095223903656)
[2025-02-16 11:33:56,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:57,063][root][INFO] - Training Epoch: 1/2, step 4883/23838 completed (loss: 0.9247295260429382, acc: 0.7234042286872864)
[2025-02-16 11:33:57,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:57,540][root][INFO] - Training Epoch: 1/2, step 4884/23838 completed (loss: 0.4377424716949463, acc: 0.8723404407501221)
[2025-02-16 11:33:57,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:58,004][root][INFO] - Training Epoch: 1/2, step 4885/23838 completed (loss: 0.7855333089828491, acc: 0.6944444179534912)
[2025-02-16 11:33:58,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:58,443][root][INFO] - Training Epoch: 1/2, step 4886/23838 completed (loss: 1.0212883949279785, acc: 0.6382978558540344)
[2025-02-16 11:33:58,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:58,908][root][INFO] - Training Epoch: 1/2, step 4887/23838 completed (loss: 1.2113040685653687, acc: 0.6315789222717285)
[2025-02-16 11:33:59,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:59,293][root][INFO] - Training Epoch: 1/2, step 4888/23838 completed (loss: 1.483898401260376, acc: 0.5777778029441833)
[2025-02-16 11:33:59,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:33:59,724][root][INFO] - Training Epoch: 1/2, step 4889/23838 completed (loss: 1.2860918045043945, acc: 0.6790123581886292)
[2025-02-16 11:33:59,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:00,125][root][INFO] - Training Epoch: 1/2, step 4890/23838 completed (loss: 1.1451479196548462, acc: 0.6639999747276306)
[2025-02-16 11:34:00,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:00,503][root][INFO] - Training Epoch: 1/2, step 4891/23838 completed (loss: 0.9247239232063293, acc: 0.6969696879386902)
[2025-02-16 11:34:00,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:00,935][root][INFO] - Training Epoch: 1/2, step 4892/23838 completed (loss: 1.0446196794509888, acc: 0.6279069781303406)
[2025-02-16 11:34:01,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:01,297][root][INFO] - Training Epoch: 1/2, step 4893/23838 completed (loss: 1.0015536546707153, acc: 0.7179487347602844)
[2025-02-16 11:34:01,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:01,725][root][INFO] - Training Epoch: 1/2, step 4894/23838 completed (loss: 1.2018400430679321, acc: 0.6964285969734192)
[2025-02-16 11:34:01,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:02,123][root][INFO] - Training Epoch: 1/2, step 4895/23838 completed (loss: 1.285296082496643, acc: 0.6260162591934204)
[2025-02-16 11:34:02,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:02,570][root][INFO] - Training Epoch: 1/2, step 4896/23838 completed (loss: 0.9250987768173218, acc: 0.7402597665786743)
[2025-02-16 11:34:02,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:02,991][root][INFO] - Training Epoch: 1/2, step 4897/23838 completed (loss: 0.777475893497467, acc: 0.7857142686843872)
[2025-02-16 11:34:03,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:03,447][root][INFO] - Training Epoch: 1/2, step 4898/23838 completed (loss: 1.0911877155303955, acc: 0.6875)
[2025-02-16 11:34:03,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:03,888][root][INFO] - Training Epoch: 1/2, step 4899/23838 completed (loss: 0.8209436535835266, acc: 0.756302535533905)
[2025-02-16 11:34:04,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:04,329][root][INFO] - Training Epoch: 1/2, step 4900/23838 completed (loss: 1.0751781463623047, acc: 0.6744186282157898)
[2025-02-16 11:34:04,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:04,746][root][INFO] - Training Epoch: 1/2, step 4901/23838 completed (loss: 0.8675614595413208, acc: 0.7894737124443054)
[2025-02-16 11:34:04,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:05,119][root][INFO] - Training Epoch: 1/2, step 4902/23838 completed (loss: 0.865441083908081, acc: 0.7263157963752747)
[2025-02-16 11:34:05,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:05,509][root][INFO] - Training Epoch: 1/2, step 4903/23838 completed (loss: 1.0347164869308472, acc: 0.707317054271698)
[2025-02-16 11:34:05,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:05,979][root][INFO] - Training Epoch: 1/2, step 4904/23838 completed (loss: 0.8888862133026123, acc: 0.7213114500045776)
[2025-02-16 11:34:06,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:06,485][root][INFO] - Training Epoch: 1/2, step 4905/23838 completed (loss: 0.7313969135284424, acc: 0.792792797088623)
[2025-02-16 11:34:06,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:06,878][root][INFO] - Training Epoch: 1/2, step 4906/23838 completed (loss: 0.6476511359214783, acc: 0.8534482717514038)
[2025-02-16 11:34:07,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:07,275][root][INFO] - Training Epoch: 1/2, step 4907/23838 completed (loss: 0.838723361492157, acc: 0.8130841255187988)
[2025-02-16 11:34:07,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:07,744][root][INFO] - Training Epoch: 1/2, step 4908/23838 completed (loss: 0.6504475474357605, acc: 0.8251748085021973)
[2025-02-16 11:34:07,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:08,155][root][INFO] - Training Epoch: 1/2, step 4909/23838 completed (loss: 0.7907795310020447, acc: 0.75)
[2025-02-16 11:34:08,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:08,578][root][INFO] - Training Epoch: 1/2, step 4910/23838 completed (loss: 0.9344906806945801, acc: 0.6666666865348816)
[2025-02-16 11:34:08,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:08,977][root][INFO] - Training Epoch: 1/2, step 4911/23838 completed (loss: 0.8356125950813293, acc: 0.7475728392601013)
[2025-02-16 11:34:09,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:09,391][root][INFO] - Training Epoch: 1/2, step 4912/23838 completed (loss: 0.6121705174446106, acc: 0.7938144207000732)
[2025-02-16 11:34:09,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:09,777][root][INFO] - Training Epoch: 1/2, step 4913/23838 completed (loss: 1.5303574800491333, acc: 0.6138613820075989)
[2025-02-16 11:34:09,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:10,169][root][INFO] - Training Epoch: 1/2, step 4914/23838 completed (loss: 1.20710289478302, acc: 0.6666666865348816)
[2025-02-16 11:34:10,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:10,560][root][INFO] - Training Epoch: 1/2, step 4915/23838 completed (loss: 1.0069985389709473, acc: 0.7289719581604004)
[2025-02-16 11:34:10,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:10,979][root][INFO] - Training Epoch: 1/2, step 4916/23838 completed (loss: 0.9503133893013, acc: 0.7906976938247681)
[2025-02-16 11:34:11,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:11,394][root][INFO] - Training Epoch: 1/2, step 4917/23838 completed (loss: 0.8236035704612732, acc: 0.808080792427063)
[2025-02-16 11:34:11,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:11,777][root][INFO] - Training Epoch: 1/2, step 4918/23838 completed (loss: 0.7665151953697205, acc: 0.7764706015586853)
[2025-02-16 11:34:11,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:12,234][root][INFO] - Training Epoch: 1/2, step 4919/23838 completed (loss: 0.9247603416442871, acc: 0.7160493731498718)
[2025-02-16 11:34:12,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:12,631][root][INFO] - Training Epoch: 1/2, step 4920/23838 completed (loss: 0.7252957224845886, acc: 0.8045976758003235)
[2025-02-16 11:34:12,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:13,015][root][INFO] - Training Epoch: 1/2, step 4921/23838 completed (loss: 0.7778664827346802, acc: 0.7666666507720947)
[2025-02-16 11:34:13,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:13,451][root][INFO] - Training Epoch: 1/2, step 4922/23838 completed (loss: 0.5939544439315796, acc: 0.8644067645072937)
[2025-02-16 11:34:13,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:13,883][root][INFO] - Training Epoch: 1/2, step 4923/23838 completed (loss: 0.6401259303092957, acc: 0.8313252925872803)
[2025-02-16 11:34:14,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:14,341][root][INFO] - Training Epoch: 1/2, step 4924/23838 completed (loss: 0.6171879172325134, acc: 0.8571428656578064)
[2025-02-16 11:34:14,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:14,728][root][INFO] - Training Epoch: 1/2, step 4925/23838 completed (loss: 0.8869482278823853, acc: 0.75)
[2025-02-16 11:34:14,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:15,166][root][INFO] - Training Epoch: 1/2, step 4926/23838 completed (loss: 0.8828636407852173, acc: 0.7844827771186829)
[2025-02-16 11:34:15,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:15,590][root][INFO] - Training Epoch: 1/2, step 4927/23838 completed (loss: 0.8527505993843079, acc: 0.8191489577293396)
[2025-02-16 11:34:15,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:15,979][root][INFO] - Training Epoch: 1/2, step 4928/23838 completed (loss: 0.7059661746025085, acc: 0.8395061492919922)
[2025-02-16 11:34:16,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:16,399][root][INFO] - Training Epoch: 1/2, step 4929/23838 completed (loss: 0.8891092538833618, acc: 0.7579618096351624)
[2025-02-16 11:34:16,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:16,847][root][INFO] - Training Epoch: 1/2, step 4930/23838 completed (loss: 0.576598584651947, acc: 0.8492063283920288)
[2025-02-16 11:34:17,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:17,272][root][INFO] - Training Epoch: 1/2, step 4931/23838 completed (loss: 0.5961828827857971, acc: 0.8468468189239502)
[2025-02-16 11:34:17,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:17,705][root][INFO] - Training Epoch: 1/2, step 4932/23838 completed (loss: 0.8761602640151978, acc: 0.7211538553237915)
[2025-02-16 11:34:17,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:18,097][root][INFO] - Training Epoch: 1/2, step 4933/23838 completed (loss: 0.7656816840171814, acc: 0.7613636255264282)
[2025-02-16 11:34:18,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:18,486][root][INFO] - Training Epoch: 1/2, step 4934/23838 completed (loss: 0.7484279870986938, acc: 0.8128654956817627)
[2025-02-16 11:34:18,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:18,895][root][INFO] - Training Epoch: 1/2, step 4935/23838 completed (loss: 1.1108580827713013, acc: 0.6629213690757751)
[2025-02-16 11:34:19,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:19,346][root][INFO] - Training Epoch: 1/2, step 4936/23838 completed (loss: 0.5688403248786926, acc: 0.8271604776382446)
[2025-02-16 11:34:19,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:19,794][root][INFO] - Training Epoch: 1/2, step 4937/23838 completed (loss: 1.3815338611602783, acc: 0.523809552192688)
[2025-02-16 11:34:20,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:20,207][root][INFO] - Training Epoch: 1/2, step 4938/23838 completed (loss: 0.7067571878433228, acc: 0.7900000214576721)
[2025-02-16 11:34:20,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:20,592][root][INFO] - Training Epoch: 1/2, step 4939/23838 completed (loss: 0.798891007900238, acc: 0.7941176295280457)
[2025-02-16 11:34:20,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:21,022][root][INFO] - Training Epoch: 1/2, step 4940/23838 completed (loss: 0.9050904512405396, acc: 0.7169811129570007)
[2025-02-16 11:34:21,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:21,445][root][INFO] - Training Epoch: 1/2, step 4941/23838 completed (loss: 1.2358371019363403, acc: 0.611940324306488)
[2025-02-16 11:34:21,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:21,919][root][INFO] - Training Epoch: 1/2, step 4942/23838 completed (loss: 0.6966843605041504, acc: 0.8088235259056091)
[2025-02-16 11:34:22,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:22,334][root][INFO] - Training Epoch: 1/2, step 4943/23838 completed (loss: 0.6356931328773499, acc: 0.8030303120613098)
[2025-02-16 11:34:22,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:22,813][root][INFO] - Training Epoch: 1/2, step 4944/23838 completed (loss: 0.8356261253356934, acc: 0.7835820913314819)
[2025-02-16 11:34:23,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:23,257][root][INFO] - Training Epoch: 1/2, step 4945/23838 completed (loss: 1.1329485177993774, acc: 0.6470588445663452)
[2025-02-16 11:34:23,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:23,653][root][INFO] - Training Epoch: 1/2, step 4946/23838 completed (loss: 1.343569278717041, acc: 0.6933333277702332)
[2025-02-16 11:34:23,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:24,096][root][INFO] - Training Epoch: 1/2, step 4947/23838 completed (loss: 1.0207527875900269, acc: 0.7118644118309021)
[2025-02-16 11:34:24,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:24,522][root][INFO] - Training Epoch: 1/2, step 4948/23838 completed (loss: 1.0372673273086548, acc: 0.7195122241973877)
[2025-02-16 11:34:24,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:24,967][root][INFO] - Training Epoch: 1/2, step 4949/23838 completed (loss: 0.6949368715286255, acc: 0.7681159377098083)
[2025-02-16 11:34:25,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:25,398][root][INFO] - Training Epoch: 1/2, step 4950/23838 completed (loss: 0.6123834252357483, acc: 0.8363636136054993)
[2025-02-16 11:34:25,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:25,872][root][INFO] - Training Epoch: 1/2, step 4951/23838 completed (loss: 0.9602037072181702, acc: 0.746268630027771)
[2025-02-16 11:34:26,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:26,278][root][INFO] - Training Epoch: 1/2, step 4952/23838 completed (loss: 0.5731902122497559, acc: 0.8726114630699158)
[2025-02-16 11:34:26,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:26,650][root][INFO] - Training Epoch: 1/2, step 4953/23838 completed (loss: 0.7121368646621704, acc: 0.7910447716712952)
[2025-02-16 11:34:26,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:27,049][root][INFO] - Training Epoch: 1/2, step 4954/23838 completed (loss: 0.8884429335594177, acc: 0.7422680258750916)
[2025-02-16 11:34:27,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:27,457][root][INFO] - Training Epoch: 1/2, step 4955/23838 completed (loss: 0.711592435836792, acc: 0.7692307829856873)
[2025-02-16 11:34:27,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:27,891][root][INFO] - Training Epoch: 1/2, step 4956/23838 completed (loss: 0.7442060112953186, acc: 0.7727272510528564)
[2025-02-16 11:34:28,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:28,343][root][INFO] - Training Epoch: 1/2, step 4957/23838 completed (loss: 0.7950804233551025, acc: 0.7924528121948242)
[2025-02-16 11:34:28,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:28,783][root][INFO] - Training Epoch: 1/2, step 4958/23838 completed (loss: 0.7946488261222839, acc: 0.8024691343307495)
[2025-02-16 11:34:28,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:29,227][root][INFO] - Training Epoch: 1/2, step 4959/23838 completed (loss: 0.8338592052459717, acc: 0.7790697813034058)
[2025-02-16 11:34:29,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:29,672][root][INFO] - Training Epoch: 1/2, step 4960/23838 completed (loss: 0.684389054775238, acc: 0.813725471496582)
[2025-02-16 11:34:29,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:30,121][root][INFO] - Training Epoch: 1/2, step 4961/23838 completed (loss: 0.45980989933013916, acc: 0.8613138794898987)
[2025-02-16 11:34:30,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:30,564][root][INFO] - Training Epoch: 1/2, step 4962/23838 completed (loss: 0.5262410044670105, acc: 0.8661417365074158)
[2025-02-16 11:34:30,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:31,013][root][INFO] - Training Epoch: 1/2, step 4963/23838 completed (loss: 0.6453536152839661, acc: 0.800000011920929)
[2025-02-16 11:34:31,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:31,417][root][INFO] - Training Epoch: 1/2, step 4964/23838 completed (loss: 0.4040410816669464, acc: 0.8985507488250732)
[2025-02-16 11:34:31,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:31,888][root][INFO] - Training Epoch: 1/2, step 4965/23838 completed (loss: 0.8743472695350647, acc: 0.7207207083702087)
[2025-02-16 11:34:32,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:32,249][root][INFO] - Training Epoch: 1/2, step 4966/23838 completed (loss: 0.7344328761100769, acc: 0.7884615659713745)
[2025-02-16 11:34:32,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:32,640][root][INFO] - Training Epoch: 1/2, step 4967/23838 completed (loss: 0.5579169988632202, acc: 0.8275862336158752)
[2025-02-16 11:34:32,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:33,060][root][INFO] - Training Epoch: 1/2, step 4968/23838 completed (loss: 0.5363501310348511, acc: 0.8160919547080994)
[2025-02-16 11:34:33,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:33,530][root][INFO] - Training Epoch: 1/2, step 4969/23838 completed (loss: 0.668910801410675, acc: 0.7853107452392578)
[2025-02-16 11:34:33,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:33,983][root][INFO] - Training Epoch: 1/2, step 4970/23838 completed (loss: 0.6402612328529358, acc: 0.7777777910232544)
[2025-02-16 11:34:34,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:34,394][root][INFO] - Training Epoch: 1/2, step 4971/23838 completed (loss: 0.5703724026679993, acc: 0.8773006200790405)
[2025-02-16 11:34:34,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:34,796][root][INFO] - Training Epoch: 1/2, step 4972/23838 completed (loss: 0.41001656651496887, acc: 0.8813559412956238)
[2025-02-16 11:34:34,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:35,221][root][INFO] - Training Epoch: 1/2, step 4973/23838 completed (loss: 0.8618363738059998, acc: 0.7846153974533081)
[2025-02-16 11:34:35,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:35,635][root][INFO] - Training Epoch: 1/2, step 4974/23838 completed (loss: 0.8053334951400757, acc: 0.760869562625885)
[2025-02-16 11:34:35,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:36,091][root][INFO] - Training Epoch: 1/2, step 4975/23838 completed (loss: 0.7829288840293884, acc: 0.7727272510528564)
[2025-02-16 11:34:36,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:36,515][root][INFO] - Training Epoch: 1/2, step 4976/23838 completed (loss: 0.759916365146637, acc: 0.8205128312110901)
[2025-02-16 11:34:36,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:36,961][root][INFO] - Training Epoch: 1/2, step 4977/23838 completed (loss: 0.7121647596359253, acc: 0.8347107172012329)
[2025-02-16 11:34:37,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:37,413][root][INFO] - Training Epoch: 1/2, step 4978/23838 completed (loss: 0.3899306356906891, acc: 0.8648648858070374)
[2025-02-16 11:34:37,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:37,863][root][INFO] - Training Epoch: 1/2, step 4979/23838 completed (loss: 0.841508686542511, acc: 0.7377049326896667)
[2025-02-16 11:34:38,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:38,279][root][INFO] - Training Epoch: 1/2, step 4980/23838 completed (loss: 0.6937309503555298, acc: 0.795918345451355)
[2025-02-16 11:34:38,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:38,719][root][INFO] - Training Epoch: 1/2, step 4981/23838 completed (loss: 0.8024266958236694, acc: 0.8461538553237915)
[2025-02-16 11:34:38,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:39,196][root][INFO] - Training Epoch: 1/2, step 4982/23838 completed (loss: 0.5740874409675598, acc: 0.8382353186607361)
[2025-02-16 11:34:39,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:39,609][root][INFO] - Training Epoch: 1/2, step 4983/23838 completed (loss: 1.0854123830795288, acc: 0.7654321193695068)
[2025-02-16 11:34:39,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:40,000][root][INFO] - Training Epoch: 1/2, step 4984/23838 completed (loss: 0.5412997007369995, acc: 0.8320610523223877)
[2025-02-16 11:34:40,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:40,471][root][INFO] - Training Epoch: 1/2, step 4985/23838 completed (loss: 0.4562915563583374, acc: 0.8571428656578064)
[2025-02-16 11:34:40,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:40,912][root][INFO] - Training Epoch: 1/2, step 4986/23838 completed (loss: 0.6693795919418335, acc: 0.780701756477356)
[2025-02-16 11:34:41,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:41,382][root][INFO] - Training Epoch: 1/2, step 4987/23838 completed (loss: 0.5994179844856262, acc: 0.8703703880310059)
[2025-02-16 11:34:41,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:41,889][root][INFO] - Training Epoch: 1/2, step 4988/23838 completed (loss: 0.6441597938537598, acc: 0.8428571224212646)
[2025-02-16 11:34:42,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:42,267][root][INFO] - Training Epoch: 1/2, step 4989/23838 completed (loss: 0.3889959752559662, acc: 0.9237288236618042)
[2025-02-16 11:34:42,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:42,675][root][INFO] - Training Epoch: 1/2, step 4990/23838 completed (loss: 0.4793507158756256, acc: 0.8913043737411499)
[2025-02-16 11:34:42,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:43,051][root][INFO] - Training Epoch: 1/2, step 4991/23838 completed (loss: 0.42655545473098755, acc: 0.8522727489471436)
[2025-02-16 11:34:43,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:43,525][root][INFO] - Training Epoch: 1/2, step 4992/23838 completed (loss: 0.39742138981819153, acc: 0.8977272510528564)
[2025-02-16 11:34:43,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:43,980][root][INFO] - Training Epoch: 1/2, step 4993/23838 completed (loss: 0.49376535415649414, acc: 0.8484848737716675)
[2025-02-16 11:34:44,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:44,445][root][INFO] - Training Epoch: 1/2, step 4994/23838 completed (loss: 0.3778385519981384, acc: 0.9047619104385376)
[2025-02-16 11:34:44,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:44,917][root][INFO] - Training Epoch: 1/2, step 4995/23838 completed (loss: 0.556176483631134, acc: 0.8579545617103577)
[2025-02-16 11:34:45,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:45,351][root][INFO] - Training Epoch: 1/2, step 4996/23838 completed (loss: 0.25869280099868774, acc: 0.9375)
[2025-02-16 11:34:45,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:45,804][root][INFO] - Training Epoch: 1/2, step 4997/23838 completed (loss: 0.8389937877655029, acc: 0.779411792755127)
[2025-02-16 11:34:45,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:46,239][root][INFO] - Training Epoch: 1/2, step 4998/23838 completed (loss: 0.7391343116760254, acc: 0.8131868243217468)
[2025-02-16 11:34:46,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:46,678][root][INFO] - Training Epoch: 1/2, step 4999/23838 completed (loss: 0.7756883502006531, acc: 0.7916666865348816)
[2025-02-16 11:34:46,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:47,129][root][INFO] - Training Epoch: 1/2, step 5000/23838 completed (loss: 0.7727248668670654, acc: 0.7962962985038757)
[2025-02-16 11:34:47,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:47,559][root][INFO] - Training Epoch: 1/2, step 5001/23838 completed (loss: 0.6691197752952576, acc: 0.8303571343421936)
[2025-02-16 11:34:47,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:48,001][root][INFO] - Training Epoch: 1/2, step 5002/23838 completed (loss: 1.4497617483139038, acc: 0.5735294222831726)
[2025-02-16 11:34:48,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:48,434][root][INFO] - Training Epoch: 1/2, step 5003/23838 completed (loss: 0.9621685743331909, acc: 0.699999988079071)
[2025-02-16 11:34:48,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:48,855][root][INFO] - Training Epoch: 1/2, step 5004/23838 completed (loss: 0.8925846219062805, acc: 0.7017543911933899)
[2025-02-16 11:34:49,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:49,274][root][INFO] - Training Epoch: 1/2, step 5005/23838 completed (loss: 0.41717973351478577, acc: 0.89552241563797)
[2025-02-16 11:34:49,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:49,744][root][INFO] - Training Epoch: 1/2, step 5006/23838 completed (loss: 0.7634473443031311, acc: 0.8059701323509216)
[2025-02-16 11:34:49,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:50,238][root][INFO] - Training Epoch: 1/2, step 5007/23838 completed (loss: 0.845508337020874, acc: 0.7613636255264282)
[2025-02-16 11:34:50,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:50,714][root][INFO] - Training Epoch: 1/2, step 5008/23838 completed (loss: 0.573965847492218, acc: 0.7761194109916687)
[2025-02-16 11:34:50,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:51,184][root][INFO] - Training Epoch: 1/2, step 5009/23838 completed (loss: 0.6341853737831116, acc: 0.8173912763595581)
[2025-02-16 11:34:51,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:51,584][root][INFO] - Training Epoch: 1/2, step 5010/23838 completed (loss: 0.6321143507957458, acc: 0.7543859481811523)
[2025-02-16 11:34:51,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:52,044][root][INFO] - Training Epoch: 1/2, step 5011/23838 completed (loss: 0.9965204000473022, acc: 0.7543859481811523)
[2025-02-16 11:34:52,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:52,591][root][INFO] - Training Epoch: 1/2, step 5012/23838 completed (loss: 0.8525254726409912, acc: 0.765625)
[2025-02-16 11:34:52,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:53,031][root][INFO] - Training Epoch: 1/2, step 5013/23838 completed (loss: 1.0275001525878906, acc: 0.7333333492279053)
[2025-02-16 11:34:53,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:53,454][root][INFO] - Training Epoch: 1/2, step 5014/23838 completed (loss: 0.7298995852470398, acc: 0.7906976938247681)
[2025-02-16 11:34:53,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:53,890][root][INFO] - Training Epoch: 1/2, step 5015/23838 completed (loss: 0.6215881705284119, acc: 0.8474576473236084)
[2025-02-16 11:34:54,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:54,327][root][INFO] - Training Epoch: 1/2, step 5016/23838 completed (loss: 0.6910111308097839, acc: 0.8333333134651184)
[2025-02-16 11:34:54,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:54,985][root][INFO] - Training Epoch: 1/2, step 5017/23838 completed (loss: 0.7770628929138184, acc: 0.8059701323509216)
[2025-02-16 11:34:55,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:55,502][root][INFO] - Training Epoch: 1/2, step 5018/23838 completed (loss: 0.8689314723014832, acc: 0.7333333492279053)
[2025-02-16 11:34:55,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:55,998][root][INFO] - Training Epoch: 1/2, step 5019/23838 completed (loss: 0.5207560062408447, acc: 0.8467742204666138)
[2025-02-16 11:34:56,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:56,504][root][INFO] - Training Epoch: 1/2, step 5020/23838 completed (loss: 0.45673760771751404, acc: 0.8842975497245789)
[2025-02-16 11:34:56,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:56,947][root][INFO] - Training Epoch: 1/2, step 5021/23838 completed (loss: 0.48401859402656555, acc: 0.8512396812438965)
[2025-02-16 11:34:57,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:57,367][root][INFO] - Training Epoch: 1/2, step 5022/23838 completed (loss: 0.5769302845001221, acc: 0.8322147727012634)
[2025-02-16 11:34:57,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:57,802][root][INFO] - Training Epoch: 1/2, step 5023/23838 completed (loss: 0.776809811592102, acc: 0.7714285850524902)
[2025-02-16 11:34:57,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:58,233][root][INFO] - Training Epoch: 1/2, step 5024/23838 completed (loss: 0.3271194100379944, acc: 0.9015151262283325)
[2025-02-16 11:34:58,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:58,646][root][INFO] - Training Epoch: 1/2, step 5025/23838 completed (loss: 1.149831771850586, acc: 0.6941176652908325)
[2025-02-16 11:34:58,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:59,110][root][INFO] - Training Epoch: 1/2, step 5026/23838 completed (loss: 0.3939417898654938, acc: 0.8709677457809448)
[2025-02-16 11:34:59,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:59,605][root][INFO] - Training Epoch: 1/2, step 5027/23838 completed (loss: 0.614665150642395, acc: 0.8676470518112183)
[2025-02-16 11:34:59,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:34:59,967][root][INFO] - Training Epoch: 1/2, step 5028/23838 completed (loss: 0.6156207919120789, acc: 0.84112149477005)
[2025-02-16 11:35:00,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:00,433][root][INFO] - Training Epoch: 1/2, step 5029/23838 completed (loss: 0.992771565914154, acc: 0.746835470199585)
[2025-02-16 11:35:00,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:00,886][root][INFO] - Training Epoch: 1/2, step 5030/23838 completed (loss: 0.7446494698524475, acc: 0.7837837934494019)
[2025-02-16 11:35:01,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:01,362][root][INFO] - Training Epoch: 1/2, step 5031/23838 completed (loss: 0.7574989199638367, acc: 0.7804877758026123)
[2025-02-16 11:35:01,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:01,724][root][INFO] - Training Epoch: 1/2, step 5032/23838 completed (loss: 0.5097858309745789, acc: 0.8918918967247009)
[2025-02-16 11:35:01,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:02,077][root][INFO] - Training Epoch: 1/2, step 5033/23838 completed (loss: 0.6386913657188416, acc: 0.84375)
[2025-02-16 11:35:02,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:02,550][root][INFO] - Training Epoch: 1/2, step 5034/23838 completed (loss: 0.46704816818237305, acc: 0.8905109763145447)
[2025-02-16 11:35:02,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:02,979][root][INFO] - Training Epoch: 1/2, step 5035/23838 completed (loss: 0.7654544711112976, acc: 0.7916666865348816)
[2025-02-16 11:35:03,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:03,372][root][INFO] - Training Epoch: 1/2, step 5036/23838 completed (loss: 0.6166470050811768, acc: 0.8303030133247375)
[2025-02-16 11:35:03,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:03,809][root][INFO] - Training Epoch: 1/2, step 5037/23838 completed (loss: 0.736142098903656, acc: 0.8235294222831726)
[2025-02-16 11:35:04,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:04,212][root][INFO] - Training Epoch: 1/2, step 5038/23838 completed (loss: 0.359197735786438, acc: 0.9224137663841248)
[2025-02-16 11:35:04,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:04,606][root][INFO] - Training Epoch: 1/2, step 5039/23838 completed (loss: 0.6591328382492065, acc: 0.7931034564971924)
[2025-02-16 11:35:04,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:05,054][root][INFO] - Training Epoch: 1/2, step 5040/23838 completed (loss: 0.5349563360214233, acc: 0.8653846383094788)
[2025-02-16 11:35:05,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:05,430][root][INFO] - Training Epoch: 1/2, step 5041/23838 completed (loss: 0.4090465009212494, acc: 0.8907563090324402)
[2025-02-16 11:35:05,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:05,928][root][INFO] - Training Epoch: 1/2, step 5042/23838 completed (loss: 0.4703359305858612, acc: 0.8936170339584351)
[2025-02-16 11:35:06,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:06,415][root][INFO] - Training Epoch: 1/2, step 5043/23838 completed (loss: 0.3136669397354126, acc: 0.9285714030265808)
[2025-02-16 11:35:06,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:06,842][root][INFO] - Training Epoch: 1/2, step 5044/23838 completed (loss: 0.36733347177505493, acc: 0.876288652420044)
[2025-02-16 11:35:07,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:07,274][root][INFO] - Training Epoch: 1/2, step 5045/23838 completed (loss: 0.4749824106693268, acc: 0.8571428656578064)
[2025-02-16 11:35:07,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:07,756][root][INFO] - Training Epoch: 1/2, step 5046/23838 completed (loss: 0.4001280665397644, acc: 0.9154929518699646)
[2025-02-16 11:35:07,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:08,203][root][INFO] - Training Epoch: 1/2, step 5047/23838 completed (loss: 0.2504394054412842, acc: 0.9444444179534912)
[2025-02-16 11:35:08,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:08,617][root][INFO] - Training Epoch: 1/2, step 5048/23838 completed (loss: 0.4602040946483612, acc: 0.8352941274642944)
[2025-02-16 11:35:08,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:09,052][root][INFO] - Training Epoch: 1/2, step 5049/23838 completed (loss: 0.3107984960079193, acc: 0.8472222089767456)
[2025-02-16 11:35:09,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:09,513][root][INFO] - Training Epoch: 1/2, step 5050/23838 completed (loss: 0.48374855518341064, acc: 0.8536585569381714)
[2025-02-16 11:35:09,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:09,957][root][INFO] - Training Epoch: 1/2, step 5051/23838 completed (loss: 0.539124071598053, acc: 0.9032257795333862)
[2025-02-16 11:35:10,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:10,327][root][INFO] - Training Epoch: 1/2, step 5052/23838 completed (loss: 0.2078268826007843, acc: 0.9178082346916199)
[2025-02-16 11:35:10,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:10,767][root][INFO] - Training Epoch: 1/2, step 5053/23838 completed (loss: 0.26048120856285095, acc: 0.9069767594337463)
[2025-02-16 11:35:10,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:11,170][root][INFO] - Training Epoch: 1/2, step 5054/23838 completed (loss: 0.92345130443573, acc: 0.7083333134651184)
[2025-02-16 11:35:11,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:11,591][root][INFO] - Training Epoch: 1/2, step 5055/23838 completed (loss: 0.5204645395278931, acc: 0.800000011920929)
[2025-02-16 11:35:11,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:11,966][root][INFO] - Training Epoch: 1/2, step 5056/23838 completed (loss: 0.5685552358627319, acc: 0.8372092843055725)
[2025-02-16 11:35:12,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:12,390][root][INFO] - Training Epoch: 1/2, step 5057/23838 completed (loss: 0.6015627384185791, acc: 0.8333333134651184)
[2025-02-16 11:35:12,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:12,854][root][INFO] - Training Epoch: 1/2, step 5058/23838 completed (loss: 0.43067666888237, acc: 0.8837209343910217)
[2025-02-16 11:35:13,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:13,302][root][INFO] - Training Epoch: 1/2, step 5059/23838 completed (loss: 0.6728180646896362, acc: 0.8500000238418579)
[2025-02-16 11:35:13,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:13,753][root][INFO] - Training Epoch: 1/2, step 5060/23838 completed (loss: 0.19133241474628448, acc: 0.9636363387107849)
[2025-02-16 11:35:13,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:14,207][root][INFO] - Training Epoch: 1/2, step 5061/23838 completed (loss: 0.15062296390533447, acc: 0.9743589758872986)
[2025-02-16 11:35:14,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:14,667][root][INFO] - Training Epoch: 1/2, step 5062/23838 completed (loss: 0.6122350692749023, acc: 0.8403361439704895)
[2025-02-16 11:35:14,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:15,062][root][INFO] - Training Epoch: 1/2, step 5063/23838 completed (loss: 0.5609450340270996, acc: 0.8870967626571655)
[2025-02-16 11:35:15,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:15,539][root][INFO] - Training Epoch: 1/2, step 5064/23838 completed (loss: 0.5205339193344116, acc: 0.8780487775802612)
[2025-02-16 11:35:15,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:15,921][root][INFO] - Training Epoch: 1/2, step 5065/23838 completed (loss: 0.4321448504924774, acc: 0.8777777552604675)
[2025-02-16 11:35:16,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:16,314][root][INFO] - Training Epoch: 1/2, step 5066/23838 completed (loss: 0.24940094351768494, acc: 0.9459459185600281)
[2025-02-16 11:35:16,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:16,753][root][INFO] - Training Epoch: 1/2, step 5067/23838 completed (loss: 0.5319333672523499, acc: 0.8571428656578064)
[2025-02-16 11:35:16,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:17,162][root][INFO] - Training Epoch: 1/2, step 5068/23838 completed (loss: 0.4967024326324463, acc: 0.9047619104385376)
[2025-02-16 11:35:17,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:17,626][root][INFO] - Training Epoch: 1/2, step 5069/23838 completed (loss: 0.7294135093688965, acc: 0.8421052694320679)
[2025-02-16 11:35:17,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:18,098][root][INFO] - Training Epoch: 1/2, step 5070/23838 completed (loss: 0.2040412724018097, acc: 0.949367105960846)
[2025-02-16 11:35:18,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:18,505][root][INFO] - Training Epoch: 1/2, step 5071/23838 completed (loss: 0.43551355600357056, acc: 0.8514851331710815)
[2025-02-16 11:35:18,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:18,985][root][INFO] - Training Epoch: 1/2, step 5072/23838 completed (loss: 0.5339620113372803, acc: 0.8541666865348816)
[2025-02-16 11:35:19,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:19,484][root][INFO] - Training Epoch: 1/2, step 5073/23838 completed (loss: 0.2958216071128845, acc: 0.8907563090324402)
[2025-02-16 11:35:19,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:19,965][root][INFO] - Training Epoch: 1/2, step 5074/23838 completed (loss: 0.42936110496520996, acc: 0.9047619104385376)
[2025-02-16 11:35:20,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:20,448][root][INFO] - Training Epoch: 1/2, step 5075/23838 completed (loss: 0.35859426856040955, acc: 0.8834951519966125)
[2025-02-16 11:35:20,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:20,901][root][INFO] - Training Epoch: 1/2, step 5076/23838 completed (loss: 0.626805305480957, acc: 0.8207547068595886)
[2025-02-16 11:35:21,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:21,259][root][INFO] - Training Epoch: 1/2, step 5077/23838 completed (loss: 0.34909772872924805, acc: 0.9027777910232544)
[2025-02-16 11:35:21,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:21,724][root][INFO] - Training Epoch: 1/2, step 5078/23838 completed (loss: 0.15704742074012756, acc: 0.9677419066429138)
[2025-02-16 11:35:21,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:22,212][root][INFO] - Training Epoch: 1/2, step 5079/23838 completed (loss: 0.17264515161514282, acc: 0.9367088675498962)
[2025-02-16 11:35:22,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:22,653][root][INFO] - Training Epoch: 1/2, step 5080/23838 completed (loss: 0.8072391152381897, acc: 0.7435897588729858)
[2025-02-16 11:35:22,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:23,044][root][INFO] - Training Epoch: 1/2, step 5081/23838 completed (loss: 0.9248237609863281, acc: 0.760869562625885)
[2025-02-16 11:35:23,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:23,391][root][INFO] - Training Epoch: 1/2, step 5082/23838 completed (loss: 1.138097882270813, acc: 0.5813953280448914)
[2025-02-16 11:35:23,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:23,806][root][INFO] - Training Epoch: 1/2, step 5083/23838 completed (loss: 0.8082071542739868, acc: 0.7727272510528564)
[2025-02-16 11:35:23,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:24,213][root][INFO] - Training Epoch: 1/2, step 5084/23838 completed (loss: 0.6889753937721252, acc: 0.7815126180648804)
[2025-02-16 11:35:24,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:24,597][root][INFO] - Training Epoch: 1/2, step 5085/23838 completed (loss: 1.114693284034729, acc: 0.6037735939025879)
[2025-02-16 11:35:24,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:25,072][root][INFO] - Training Epoch: 1/2, step 5086/23838 completed (loss: 0.7334944009780884, acc: 0.7731958627700806)
[2025-02-16 11:35:25,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:25,498][root][INFO] - Training Epoch: 1/2, step 5087/23838 completed (loss: 0.9272233843803406, acc: 0.7053571343421936)
[2025-02-16 11:35:25,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:25,892][root][INFO] - Training Epoch: 1/2, step 5088/23838 completed (loss: 1.1353915929794312, acc: 0.7340425252914429)
[2025-02-16 11:35:26,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:26,335][root][INFO] - Training Epoch: 1/2, step 5089/23838 completed (loss: 0.9086563587188721, acc: 0.7727272510528564)
[2025-02-16 11:35:26,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:26,830][root][INFO] - Training Epoch: 1/2, step 5090/23838 completed (loss: 0.9577168226242065, acc: 0.7419354915618896)
[2025-02-16 11:35:27,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:27,306][root][INFO] - Training Epoch: 1/2, step 5091/23838 completed (loss: 0.9843590259552002, acc: 0.7115384340286255)
[2025-02-16 11:35:27,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:27,702][root][INFO] - Training Epoch: 1/2, step 5092/23838 completed (loss: 0.7941705584526062, acc: 0.7678571343421936)
[2025-02-16 11:35:27,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:28,091][root][INFO] - Training Epoch: 1/2, step 5093/23838 completed (loss: 1.2287517786026, acc: 0.6805555820465088)
[2025-02-16 11:35:28,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:28,486][root][INFO] - Training Epoch: 1/2, step 5094/23838 completed (loss: 0.9804590940475464, acc: 0.7450980544090271)
[2025-02-16 11:35:28,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:28,920][root][INFO] - Training Epoch: 1/2, step 5095/23838 completed (loss: 0.6823372840881348, acc: 0.7887324094772339)
[2025-02-16 11:35:29,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:29,319][root][INFO] - Training Epoch: 1/2, step 5096/23838 completed (loss: 0.647010862827301, acc: 0.8256880640983582)
[2025-02-16 11:35:29,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:29,785][root][INFO] - Training Epoch: 1/2, step 5097/23838 completed (loss: 0.5414620041847229, acc: 0.8484848737716675)
[2025-02-16 11:35:29,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:30,214][root][INFO] - Training Epoch: 1/2, step 5098/23838 completed (loss: 0.3476863503456116, acc: 0.8857142925262451)
[2025-02-16 11:35:30,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:30,636][root][INFO] - Training Epoch: 1/2, step 5099/23838 completed (loss: 0.6235852837562561, acc: 0.8208954930305481)
[2025-02-16 11:35:30,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:31,012][root][INFO] - Training Epoch: 1/2, step 5100/23838 completed (loss: 0.5941848754882812, acc: 0.8571428656578064)
[2025-02-16 11:35:31,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:31,464][root][INFO] - Training Epoch: 1/2, step 5101/23838 completed (loss: 0.6997790932655334, acc: 0.8208954930305481)
[2025-02-16 11:35:31,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:31,922][root][INFO] - Training Epoch: 1/2, step 5102/23838 completed (loss: 0.6676701307296753, acc: 0.84375)
[2025-02-16 11:35:32,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:32,374][root][INFO] - Training Epoch: 1/2, step 5103/23838 completed (loss: 0.668846607208252, acc: 0.8072289228439331)
[2025-02-16 11:35:32,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:32,872][root][INFO] - Training Epoch: 1/2, step 5104/23838 completed (loss: 0.909869372844696, acc: 0.790123462677002)
[2025-02-16 11:35:33,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:33,273][root][INFO] - Training Epoch: 1/2, step 5105/23838 completed (loss: 1.4084393978118896, acc: 0.6106194853782654)
[2025-02-16 11:35:33,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:33,654][root][INFO] - Training Epoch: 1/2, step 5106/23838 completed (loss: 0.6236047744750977, acc: 0.8347107172012329)
[2025-02-16 11:35:33,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:34,074][root][INFO] - Training Epoch: 1/2, step 5107/23838 completed (loss: 0.6562943458557129, acc: 0.8125)
[2025-02-16 11:35:34,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:34,507][root][INFO] - Training Epoch: 1/2, step 5108/23838 completed (loss: 0.5724086761474609, acc: 0.7948718070983887)
[2025-02-16 11:35:34,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:34,939][root][INFO] - Training Epoch: 1/2, step 5109/23838 completed (loss: 0.8556676506996155, acc: 0.7692307829856873)
[2025-02-16 11:35:35,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:35,355][root][INFO] - Training Epoch: 1/2, step 5110/23838 completed (loss: 1.0121208429336548, acc: 0.7184466123580933)
[2025-02-16 11:35:35,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:35,955][root][INFO] - Training Epoch: 1/2, step 5111/23838 completed (loss: 0.6077470779418945, acc: 0.8139534592628479)
[2025-02-16 11:35:36,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:36,427][root][INFO] - Training Epoch: 1/2, step 5112/23838 completed (loss: 0.7573080658912659, acc: 0.8117647171020508)
[2025-02-16 11:35:36,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:36,795][root][INFO] - Training Epoch: 1/2, step 5113/23838 completed (loss: 0.5634967088699341, acc: 0.8474576473236084)
[2025-02-16 11:35:37,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:37,273][root][INFO] - Training Epoch: 1/2, step 5114/23838 completed (loss: 0.6480506658554077, acc: 0.7952755689620972)
[2025-02-16 11:35:37,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:37,692][root][INFO] - Training Epoch: 1/2, step 5115/23838 completed (loss: 0.6734662055969238, acc: 0.8333333134651184)
[2025-02-16 11:35:37,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:38,070][root][INFO] - Training Epoch: 1/2, step 5116/23838 completed (loss: 0.7878292798995972, acc: 0.7575757503509521)
[2025-02-16 11:35:38,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:38,603][root][INFO] - Training Epoch: 1/2, step 5117/23838 completed (loss: 0.8525222539901733, acc: 0.8227847814559937)
[2025-02-16 11:35:38,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:39,031][root][INFO] - Training Epoch: 1/2, step 5118/23838 completed (loss: 0.8272692561149597, acc: 0.7798165082931519)
[2025-02-16 11:35:39,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:39,608][root][INFO] - Training Epoch: 1/2, step 5119/23838 completed (loss: 1.1056253910064697, acc: 0.732758641242981)
[2025-02-16 11:35:39,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:40,043][root][INFO] - Training Epoch: 1/2, step 5120/23838 completed (loss: 0.754534125328064, acc: 0.834782600402832)
[2025-02-16 11:35:40,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:40,413][root][INFO] - Training Epoch: 1/2, step 5121/23838 completed (loss: 0.2494107484817505, acc: 0.949999988079071)
[2025-02-16 11:35:40,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:40,963][root][INFO] - Training Epoch: 1/2, step 5122/23838 completed (loss: 0.5702943205833435, acc: 0.8496732115745544)
[2025-02-16 11:35:41,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:41,482][root][INFO] - Training Epoch: 1/2, step 5123/23838 completed (loss: 0.3437437117099762, acc: 0.9240506291389465)
[2025-02-16 11:35:41,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:42,351][root][INFO] - Training Epoch: 1/2, step 5124/23838 completed (loss: 0.8053197860717773, acc: 0.7785714268684387)
[2025-02-16 11:35:42,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:42,871][root][INFO] - Training Epoch: 1/2, step 5125/23838 completed (loss: 0.4814358949661255, acc: 0.8880000114440918)
[2025-02-16 11:35:43,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:43,785][root][INFO] - Training Epoch: 1/2, step 5126/23838 completed (loss: 0.6616807579994202, acc: 0.8145161271095276)
[2025-02-16 11:35:44,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:44,301][root][INFO] - Training Epoch: 1/2, step 5127/23838 completed (loss: 0.6285850405693054, acc: 0.8641975522041321)
[2025-02-16 11:35:44,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:44,788][root][INFO] - Training Epoch: 1/2, step 5128/23838 completed (loss: 0.3948858380317688, acc: 0.8965517282485962)
[2025-02-16 11:35:44,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:45,227][root][INFO] - Training Epoch: 1/2, step 5129/23838 completed (loss: 0.5274766087532043, acc: 0.9075630307197571)
[2025-02-16 11:35:45,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:45,818][root][INFO] - Training Epoch: 1/2, step 5130/23838 completed (loss: 0.4850406348705292, acc: 0.8604651093482971)
[2025-02-16 11:35:46,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:46,237][root][INFO] - Training Epoch: 1/2, step 5131/23838 completed (loss: 0.39540988206863403, acc: 0.90625)
[2025-02-16 11:35:46,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:46,663][root][INFO] - Training Epoch: 1/2, step 5132/23838 completed (loss: 0.40926438570022583, acc: 0.9017857313156128)
[2025-02-16 11:35:46,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:47,215][root][INFO] - Training Epoch: 1/2, step 5133/23838 completed (loss: 0.4691542088985443, acc: 0.8674699068069458)
[2025-02-16 11:35:47,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:47,857][root][INFO] - Training Epoch: 1/2, step 5134/23838 completed (loss: 0.4219173192977905, acc: 0.9037036895751953)
[2025-02-16 11:35:48,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:48,289][root][INFO] - Training Epoch: 1/2, step 5135/23838 completed (loss: 0.6363192200660706, acc: 0.8623853325843811)
[2025-02-16 11:35:48,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:48,693][root][INFO] - Training Epoch: 1/2, step 5136/23838 completed (loss: 1.5296021699905396, acc: 0.4912280738353729)
[2025-02-16 11:35:48,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:49,071][root][INFO] - Training Epoch: 1/2, step 5137/23838 completed (loss: 0.4692310392856598, acc: 0.8799999952316284)
[2025-02-16 11:35:49,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:49,640][root][INFO] - Training Epoch: 1/2, step 5138/23838 completed (loss: 0.6727859377861023, acc: 0.8504672646522522)
[2025-02-16 11:35:50,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:50,562][root][INFO] - Training Epoch: 1/2, step 5139/23838 completed (loss: 0.6573920845985413, acc: 0.8585858345031738)
[2025-02-16 11:35:51,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:51,598][root][INFO] - Training Epoch: 1/2, step 5140/23838 completed (loss: 0.510125994682312, acc: 0.869918704032898)
[2025-02-16 11:35:52,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:52,529][root][INFO] - Training Epoch: 1/2, step 5141/23838 completed (loss: 0.470113605260849, acc: 0.8952381014823914)
[2025-02-16 11:35:52,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:53,022][root][INFO] - Training Epoch: 1/2, step 5142/23838 completed (loss: 0.7705985307693481, acc: 0.7966101765632629)
[2025-02-16 11:35:53,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:53,483][root][INFO] - Training Epoch: 1/2, step 5143/23838 completed (loss: 0.8053805828094482, acc: 0.7164179086685181)
[2025-02-16 11:35:53,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:53,985][root][INFO] - Training Epoch: 1/2, step 5144/23838 completed (loss: 0.6212172508239746, acc: 0.8695651888847351)
[2025-02-16 11:35:54,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:54,474][root][INFO] - Training Epoch: 1/2, step 5145/23838 completed (loss: 0.2839980125427246, acc: 0.9438202381134033)
[2025-02-16 11:35:54,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:54,979][root][INFO] - Training Epoch: 1/2, step 5146/23838 completed (loss: 0.3679235279560089, acc: 0.887499988079071)
[2025-02-16 11:35:55,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:55,442][root][INFO] - Training Epoch: 1/2, step 5147/23838 completed (loss: 0.4183645248413086, acc: 0.9242424368858337)
[2025-02-16 11:35:55,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:55,848][root][INFO] - Training Epoch: 1/2, step 5148/23838 completed (loss: 1.3452068567276, acc: 0.6363636255264282)
[2025-02-16 11:35:56,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:56,309][root][INFO] - Training Epoch: 1/2, step 5149/23838 completed (loss: 0.4489041566848755, acc: 0.8983050584793091)
[2025-02-16 11:35:56,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:56,758][root][INFO] - Training Epoch: 1/2, step 5150/23838 completed (loss: 0.07872287184000015, acc: 0.9863013625144958)
[2025-02-16 11:35:56,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:57,265][root][INFO] - Training Epoch: 1/2, step 5151/23838 completed (loss: 0.3590939939022064, acc: 0.8648648858070374)
[2025-02-16 11:35:57,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:57,750][root][INFO] - Training Epoch: 1/2, step 5152/23838 completed (loss: 0.3663328289985657, acc: 0.9090909361839294)
[2025-02-16 11:35:57,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:58,199][root][INFO] - Training Epoch: 1/2, step 5153/23838 completed (loss: 0.19514261186122894, acc: 0.9638554453849792)
[2025-02-16 11:35:58,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:58,642][root][INFO] - Training Epoch: 1/2, step 5154/23838 completed (loss: 0.37853410840034485, acc: 0.890625)
[2025-02-16 11:35:58,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:59,034][root][INFO] - Training Epoch: 1/2, step 5155/23838 completed (loss: 0.2521193027496338, acc: 0.9454545378684998)
[2025-02-16 11:35:59,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:59,561][root][INFO] - Training Epoch: 1/2, step 5156/23838 completed (loss: 0.3200330138206482, acc: 0.8867924809455872)
[2025-02-16 11:35:59,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:35:59,953][root][INFO] - Training Epoch: 1/2, step 5157/23838 completed (loss: 0.49606871604919434, acc: 0.8461538553237915)
[2025-02-16 11:36:00,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:00,377][root][INFO] - Training Epoch: 1/2, step 5158/23838 completed (loss: 0.47944074869155884, acc: 0.8823529481887817)
[2025-02-16 11:36:00,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:00,768][root][INFO] - Training Epoch: 1/2, step 5159/23838 completed (loss: 0.6065906286239624, acc: 0.8703703880310059)
[2025-02-16 11:36:00,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:01,251][root][INFO] - Training Epoch: 1/2, step 5160/23838 completed (loss: 0.49448665976524353, acc: 0.8999999761581421)
[2025-02-16 11:36:01,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:01,690][root][INFO] - Training Epoch: 1/2, step 5161/23838 completed (loss: 0.42986422777175903, acc: 0.9193548560142517)
[2025-02-16 11:36:01,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:02,147][root][INFO] - Training Epoch: 1/2, step 5162/23838 completed (loss: 0.21143624186515808, acc: 0.9285714030265808)
[2025-02-16 11:36:02,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:02,629][root][INFO] - Training Epoch: 1/2, step 5163/23838 completed (loss: 0.37670227885246277, acc: 0.8965517282485962)
[2025-02-16 11:36:02,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:03,095][root][INFO] - Training Epoch: 1/2, step 5164/23838 completed (loss: 0.3954833447933197, acc: 0.8888888955116272)
[2025-02-16 11:36:03,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:03,515][root][INFO] - Training Epoch: 1/2, step 5165/23838 completed (loss: 0.5036559700965881, acc: 0.9111111164093018)
[2025-02-16 11:36:03,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:03,955][root][INFO] - Training Epoch: 1/2, step 5166/23838 completed (loss: 0.6445690989494324, acc: 0.8354430198669434)
[2025-02-16 11:36:04,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:04,418][root][INFO] - Training Epoch: 1/2, step 5167/23838 completed (loss: 0.44875749945640564, acc: 0.9193548560142517)
[2025-02-16 11:36:04,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:04,816][root][INFO] - Training Epoch: 1/2, step 5168/23838 completed (loss: 0.5127951502799988, acc: 0.8787878751754761)
[2025-02-16 11:36:04,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:05,266][root][INFO] - Training Epoch: 1/2, step 5169/23838 completed (loss: 0.07389453798532486, acc: 1.0)
[2025-02-16 11:36:05,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:05,705][root][INFO] - Training Epoch: 1/2, step 5170/23838 completed (loss: 0.8966898322105408, acc: 0.7303370833396912)
[2025-02-16 11:36:05,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:06,110][root][INFO] - Training Epoch: 1/2, step 5171/23838 completed (loss: 0.5863060355186462, acc: 0.8448275923728943)
[2025-02-16 11:36:06,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:06,595][root][INFO] - Training Epoch: 1/2, step 5172/23838 completed (loss: 0.09257902950048447, acc: 0.97826087474823)
[2025-02-16 11:36:06,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:07,042][root][INFO] - Training Epoch: 1/2, step 5173/23838 completed (loss: 0.16842307150363922, acc: 0.9523809552192688)
[2025-02-16 11:36:07,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:07,479][root][INFO] - Training Epoch: 1/2, step 5174/23838 completed (loss: 0.7741415500640869, acc: 0.807692289352417)
[2025-02-16 11:36:07,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:07,884][root][INFO] - Training Epoch: 1/2, step 5175/23838 completed (loss: 0.6636914610862732, acc: 0.8799999952316284)
[2025-02-16 11:36:08,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:08,262][root][INFO] - Training Epoch: 1/2, step 5176/23838 completed (loss: 0.5596845149993896, acc: 0.8630136847496033)
[2025-02-16 11:36:08,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:08,695][root][INFO] - Training Epoch: 1/2, step 5177/23838 completed (loss: 0.47911590337753296, acc: 0.8974359035491943)
[2025-02-16 11:36:08,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:09,134][root][INFO] - Training Epoch: 1/2, step 5178/23838 completed (loss: 0.6000359058380127, acc: 0.844660222530365)
[2025-02-16 11:36:09,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:09,624][root][INFO] - Training Epoch: 1/2, step 5179/23838 completed (loss: 0.2174774408340454, acc: 0.9365079402923584)
[2025-02-16 11:36:09,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:10,100][root][INFO] - Training Epoch: 1/2, step 5180/23838 completed (loss: 0.41781798005104065, acc: 0.8620689511299133)
[2025-02-16 11:36:10,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:10,555][root][INFO] - Training Epoch: 1/2, step 5181/23838 completed (loss: 0.48492178320884705, acc: 0.8666666746139526)
[2025-02-16 11:36:10,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:10,946][root][INFO] - Training Epoch: 1/2, step 5182/23838 completed (loss: 0.25995180010795593, acc: 0.9122806787490845)
[2025-02-16 11:36:11,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:11,359][root][INFO] - Training Epoch: 1/2, step 5183/23838 completed (loss: 0.3948352038860321, acc: 0.875)
[2025-02-16 11:36:11,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:11,740][root][INFO] - Training Epoch: 1/2, step 5184/23838 completed (loss: 0.10933832824230194, acc: 0.9642857313156128)
[2025-02-16 11:36:11,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:12,214][root][INFO] - Training Epoch: 1/2, step 5185/23838 completed (loss: 0.33682316541671753, acc: 0.8947368264198303)
[2025-02-16 11:36:12,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:12,628][root][INFO] - Training Epoch: 1/2, step 5186/23838 completed (loss: 0.5392476916313171, acc: 0.8604651093482971)
[2025-02-16 11:36:12,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:13,065][root][INFO] - Training Epoch: 1/2, step 5187/23838 completed (loss: 0.8986061811447144, acc: 0.8125)
[2025-02-16 11:36:13,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:13,496][root][INFO] - Training Epoch: 1/2, step 5188/23838 completed (loss: 0.8794003129005432, acc: 0.7288135886192322)
[2025-02-16 11:36:13,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:13,904][root][INFO] - Training Epoch: 1/2, step 5189/23838 completed (loss: 0.39483538269996643, acc: 0.8828125)
[2025-02-16 11:36:14,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:14,269][root][INFO] - Training Epoch: 1/2, step 5190/23838 completed (loss: 0.8038786053657532, acc: 0.7631579041481018)
[2025-02-16 11:36:14,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:14,670][root][INFO] - Training Epoch: 1/2, step 5191/23838 completed (loss: 0.2780741751194, acc: 0.8961748480796814)
[2025-02-16 11:36:14,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:15,123][root][INFO] - Training Epoch: 1/2, step 5192/23838 completed (loss: 0.6845249533653259, acc: 0.8493150472640991)
[2025-02-16 11:36:15,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:15,529][root][INFO] - Training Epoch: 1/2, step 5193/23838 completed (loss: 0.5094672441482544, acc: 0.800000011920929)
[2025-02-16 11:36:15,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:15,961][root][INFO] - Training Epoch: 1/2, step 5194/23838 completed (loss: 0.5128040313720703, acc: 0.8690476417541504)
[2025-02-16 11:36:16,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:16,362][root][INFO] - Training Epoch: 1/2, step 5195/23838 completed (loss: 0.2587502598762512, acc: 0.9200000166893005)
[2025-02-16 11:36:16,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:16,788][root][INFO] - Training Epoch: 1/2, step 5196/23838 completed (loss: 0.4246187210083008, acc: 0.8783783912658691)
[2025-02-16 11:36:16,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:17,196][root][INFO] - Training Epoch: 1/2, step 5197/23838 completed (loss: 1.070478081703186, acc: 0.7571428418159485)
[2025-02-16 11:36:17,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:17,603][root][INFO] - Training Epoch: 1/2, step 5198/23838 completed (loss: 0.3974749445915222, acc: 0.9024389982223511)
[2025-02-16 11:36:17,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:18,017][root][INFO] - Training Epoch: 1/2, step 5199/23838 completed (loss: 0.17357514798641205, acc: 0.9411764740943909)
[2025-02-16 11:36:18,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:18,463][root][INFO] - Training Epoch: 1/2, step 5200/23838 completed (loss: 0.2740415036678314, acc: 0.9285714030265808)
[2025-02-16 11:36:18,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:18,859][root][INFO] - Training Epoch: 1/2, step 5201/23838 completed (loss: 0.40658894181251526, acc: 0.9113923907279968)
[2025-02-16 11:36:19,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:19,265][root][INFO] - Training Epoch: 1/2, step 5202/23838 completed (loss: 0.7957373261451721, acc: 0.8363636136054993)
[2025-02-16 11:36:19,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:19,649][root][INFO] - Training Epoch: 1/2, step 5203/23838 completed (loss: 0.28549930453300476, acc: 0.947826087474823)
[2025-02-16 11:36:19,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:20,021][root][INFO] - Training Epoch: 1/2, step 5204/23838 completed (loss: 1.2153339385986328, acc: 0.694915235042572)
[2025-02-16 11:36:20,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:20,456][root][INFO] - Training Epoch: 1/2, step 5205/23838 completed (loss: 0.1783246099948883, acc: 0.96875)
[2025-02-16 11:36:20,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:20,842][root][INFO] - Training Epoch: 1/2, step 5206/23838 completed (loss: 0.7667438983917236, acc: 0.7272727489471436)
[2025-02-16 11:36:21,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:21,253][root][INFO] - Training Epoch: 1/2, step 5207/23838 completed (loss: 1.163069486618042, acc: 0.6693548560142517)
[2025-02-16 11:36:21,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:21,627][root][INFO] - Training Epoch: 1/2, step 5208/23838 completed (loss: 0.700527012348175, acc: 0.8095238208770752)
[2025-02-16 11:36:21,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:22,082][root][INFO] - Training Epoch: 1/2, step 5209/23838 completed (loss: 0.8429352641105652, acc: 0.7358490824699402)
[2025-02-16 11:36:22,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:22,535][root][INFO] - Training Epoch: 1/2, step 5210/23838 completed (loss: 1.1832444667816162, acc: 0.6346153616905212)
[2025-02-16 11:36:22,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:23,029][root][INFO] - Training Epoch: 1/2, step 5211/23838 completed (loss: 0.5947363972663879, acc: 0.8529411554336548)
[2025-02-16 11:36:23,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:23,783][root][INFO] - Training Epoch: 1/2, step 5212/23838 completed (loss: 1.1026514768600464, acc: 0.6896551847457886)
[2025-02-16 11:36:24,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:24,427][root][INFO] - Training Epoch: 1/2, step 5213/23838 completed (loss: 0.8803534507751465, acc: 0.7407407164573669)
[2025-02-16 11:36:24,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:24,858][root][INFO] - Training Epoch: 1/2, step 5214/23838 completed (loss: 1.6164978742599487, acc: 0.4878048896789551)
[2025-02-16 11:36:25,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:25,429][root][INFO] - Training Epoch: 1/2, step 5215/23838 completed (loss: 0.7333331108093262, acc: 0.6842105388641357)
[2025-02-16 11:36:25,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:26,141][root][INFO] - Training Epoch: 1/2, step 5216/23838 completed (loss: 1.0928442478179932, acc: 0.746666669845581)
[2025-02-16 11:36:26,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:26,644][root][INFO] - Training Epoch: 1/2, step 5217/23838 completed (loss: 1.6209442615509033, acc: 0.5333333611488342)
[2025-02-16 11:36:26,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:27,102][root][INFO] - Training Epoch: 1/2, step 5218/23838 completed (loss: 1.070860743522644, acc: 0.6229507923126221)
[2025-02-16 11:36:27,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:27,493][root][INFO] - Training Epoch: 1/2, step 5219/23838 completed (loss: 1.3185639381408691, acc: 0.5806451439857483)
[2025-02-16 11:36:27,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:27,932][root][INFO] - Training Epoch: 1/2, step 5220/23838 completed (loss: 1.127424716949463, acc: 0.6746987700462341)
[2025-02-16 11:36:28,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:28,431][root][INFO] - Training Epoch: 1/2, step 5221/23838 completed (loss: 0.7263302206993103, acc: 0.7804877758026123)
[2025-02-16 11:36:28,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:28,937][root][INFO] - Training Epoch: 1/2, step 5222/23838 completed (loss: 1.0587096214294434, acc: 0.7058823704719543)
[2025-02-16 11:36:29,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:29,360][root][INFO] - Training Epoch: 1/2, step 5223/23838 completed (loss: 1.4648340940475464, acc: 0.5625)
[2025-02-16 11:36:29,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:29,832][root][INFO] - Training Epoch: 1/2, step 5224/23838 completed (loss: 1.0434874296188354, acc: 0.6600000262260437)
[2025-02-16 11:36:30,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:30,244][root][INFO] - Training Epoch: 1/2, step 5225/23838 completed (loss: 1.455683946609497, acc: 0.5686274766921997)
[2025-02-16 11:36:30,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:30,630][root][INFO] - Training Epoch: 1/2, step 5226/23838 completed (loss: 1.4189846515655518, acc: 0.5660377144813538)
[2025-02-16 11:36:30,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:31,100][root][INFO] - Training Epoch: 1/2, step 5227/23838 completed (loss: 1.2597556114196777, acc: 0.6190476417541504)
[2025-02-16 11:36:31,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:31,625][root][INFO] - Training Epoch: 1/2, step 5228/23838 completed (loss: 1.7294645309448242, acc: 0.5333333611488342)
[2025-02-16 11:36:31,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:32,049][root][INFO] - Training Epoch: 1/2, step 5229/23838 completed (loss: 1.4943112134933472, acc: 0.65625)
[2025-02-16 11:36:32,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:32,415][root][INFO] - Training Epoch: 1/2, step 5230/23838 completed (loss: 1.8324735164642334, acc: 0.5263158082962036)
[2025-02-16 11:36:32,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:32,934][root][INFO] - Training Epoch: 1/2, step 5231/23838 completed (loss: 1.1744146347045898, acc: 0.6666666865348816)
[2025-02-16 11:36:33,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:33,417][root][INFO] - Training Epoch: 1/2, step 5232/23838 completed (loss: 1.3122845888137817, acc: 0.6666666865348816)
[2025-02-16 11:36:33,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:33,883][root][INFO] - Training Epoch: 1/2, step 5233/23838 completed (loss: 0.7452461123466492, acc: 0.7415730357170105)
[2025-02-16 11:36:34,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:34,348][root][INFO] - Training Epoch: 1/2, step 5234/23838 completed (loss: 0.8824984431266785, acc: 0.760869562625885)
[2025-02-16 11:36:34,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:34,751][root][INFO] - Training Epoch: 1/2, step 5235/23838 completed (loss: 2.0374581813812256, acc: 0.4318181872367859)
[2025-02-16 11:36:34,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:35,139][root][INFO] - Training Epoch: 1/2, step 5236/23838 completed (loss: 1.837170124053955, acc: 0.5405405163764954)
[2025-02-16 11:36:35,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:35,555][root][INFO] - Training Epoch: 1/2, step 5237/23838 completed (loss: 1.721903920173645, acc: 0.5526315569877625)
[2025-02-16 11:36:35,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:35,967][root][INFO] - Training Epoch: 1/2, step 5238/23838 completed (loss: 1.1119608879089355, acc: 0.6296296119689941)
[2025-02-16 11:36:36,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:36,386][root][INFO] - Training Epoch: 1/2, step 5239/23838 completed (loss: 1.2832049131393433, acc: 0.625)
[2025-02-16 11:36:36,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:36,821][root][INFO] - Training Epoch: 1/2, step 5240/23838 completed (loss: 1.2261148691177368, acc: 0.6666666865348816)
[2025-02-16 11:36:36,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:37,244][root][INFO] - Training Epoch: 1/2, step 5241/23838 completed (loss: 1.4364162683486938, acc: 0.6266666650772095)
[2025-02-16 11:36:37,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:37,693][root][INFO] - Training Epoch: 1/2, step 5242/23838 completed (loss: 0.9246789813041687, acc: 0.761904776096344)
[2025-02-16 11:36:37,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:38,158][root][INFO] - Training Epoch: 1/2, step 5243/23838 completed (loss: 0.7670060396194458, acc: 0.7460317611694336)
[2025-02-16 11:36:38,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:38,599][root][INFO] - Training Epoch: 1/2, step 5244/23838 completed (loss: 0.6871370673179626, acc: 0.7777777910232544)
[2025-02-16 11:36:38,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:39,018][root][INFO] - Training Epoch: 1/2, step 5245/23838 completed (loss: 0.7926750183105469, acc: 0.774193525314331)
[2025-02-16 11:36:39,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:39,492][root][INFO] - Training Epoch: 1/2, step 5246/23838 completed (loss: 1.0646171569824219, acc: 0.7435897588729858)
[2025-02-16 11:36:39,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:39,936][root][INFO] - Training Epoch: 1/2, step 5247/23838 completed (loss: 1.0592055320739746, acc: 0.7142857313156128)
[2025-02-16 11:36:40,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:40,413][root][INFO] - Training Epoch: 1/2, step 5248/23838 completed (loss: 1.306079387664795, acc: 0.6315789222717285)
[2025-02-16 11:36:40,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:40,850][root][INFO] - Training Epoch: 1/2, step 5249/23838 completed (loss: 1.1911370754241943, acc: 0.675000011920929)
[2025-02-16 11:36:41,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:41,365][root][INFO] - Training Epoch: 1/2, step 5250/23838 completed (loss: 1.301857590675354, acc: 0.5806451439857483)
[2025-02-16 11:36:41,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:41,806][root][INFO] - Training Epoch: 1/2, step 5251/23838 completed (loss: 1.081336498260498, acc: 0.6976743936538696)
[2025-02-16 11:36:41,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:42,222][root][INFO] - Training Epoch: 1/2, step 5252/23838 completed (loss: 0.711237370967865, acc: 0.8155339956283569)
[2025-02-16 11:36:42,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:42,645][root][INFO] - Training Epoch: 1/2, step 5253/23838 completed (loss: 0.7122458219528198, acc: 0.7747747898101807)
[2025-02-16 11:36:42,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:43,065][root][INFO] - Training Epoch: 1/2, step 5254/23838 completed (loss: 0.9602342844009399, acc: 0.7712418437004089)
[2025-02-16 11:36:43,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:43,526][root][INFO] - Training Epoch: 1/2, step 5255/23838 completed (loss: 0.844742476940155, acc: 0.7682926654815674)
[2025-02-16 11:36:43,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:43,889][root][INFO] - Training Epoch: 1/2, step 5256/23838 completed (loss: 0.6951513290405273, acc: 0.7875000238418579)
[2025-02-16 11:36:44,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:44,341][root][INFO] - Training Epoch: 1/2, step 5257/23838 completed (loss: 0.3759123384952545, acc: 0.8818181753158569)
[2025-02-16 11:36:44,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:44,734][root][INFO] - Training Epoch: 1/2, step 5258/23838 completed (loss: 0.49378007650375366, acc: 0.860927164554596)
[2025-02-16 11:36:44,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:45,157][root][INFO] - Training Epoch: 1/2, step 5259/23838 completed (loss: 0.7029328942298889, acc: 0.8045976758003235)
[2025-02-16 11:36:45,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:45,611][root][INFO] - Training Epoch: 1/2, step 5260/23838 completed (loss: 0.5729286074638367, acc: 0.8571428656578064)
[2025-02-16 11:36:45,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:46,087][root][INFO] - Training Epoch: 1/2, step 5261/23838 completed (loss: 0.5386778116226196, acc: 0.8493150472640991)
[2025-02-16 11:36:46,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:46,496][root][INFO] - Training Epoch: 1/2, step 5262/23838 completed (loss: 0.39506441354751587, acc: 0.8730158805847168)
[2025-02-16 11:36:46,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:46,938][root][INFO] - Training Epoch: 1/2, step 5263/23838 completed (loss: 0.7095022797584534, acc: 0.7947019934654236)
[2025-02-16 11:36:47,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:47,396][root][INFO] - Training Epoch: 1/2, step 5264/23838 completed (loss: 0.523195743560791, acc: 0.8888888955116272)
[2025-02-16 11:36:47,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:47,864][root][INFO] - Training Epoch: 1/2, step 5265/23838 completed (loss: 0.6476808190345764, acc: 0.7634408473968506)
[2025-02-16 11:36:48,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:48,331][root][INFO] - Training Epoch: 1/2, step 5266/23838 completed (loss: 0.5106498003005981, acc: 0.8617886304855347)
[2025-02-16 11:36:48,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:48,724][root][INFO] - Training Epoch: 1/2, step 5267/23838 completed (loss: 0.34558233618736267, acc: 0.9176470637321472)
[2025-02-16 11:36:48,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:49,133][root][INFO] - Training Epoch: 1/2, step 5268/23838 completed (loss: 0.3864700496196747, acc: 0.8870967626571655)
[2025-02-16 11:36:49,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:49,583][root][INFO] - Training Epoch: 1/2, step 5269/23838 completed (loss: 0.4239863157272339, acc: 0.890625)
[2025-02-16 11:36:49,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:50,070][root][INFO] - Training Epoch: 1/2, step 5270/23838 completed (loss: 0.684447705745697, acc: 0.8139534592628479)
[2025-02-16 11:36:50,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:50,535][root][INFO] - Training Epoch: 1/2, step 5271/23838 completed (loss: 0.6089654564857483, acc: 0.8349514603614807)
[2025-02-16 11:36:50,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:50,952][root][INFO] - Training Epoch: 1/2, step 5272/23838 completed (loss: 0.7959238886833191, acc: 0.8031495809555054)
[2025-02-16 11:36:51,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:51,374][root][INFO] - Training Epoch: 1/2, step 5273/23838 completed (loss: 0.6128425598144531, acc: 0.8023256063461304)
[2025-02-16 11:36:51,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:51,827][root][INFO] - Training Epoch: 1/2, step 5274/23838 completed (loss: 0.7097923755645752, acc: 0.8452380895614624)
[2025-02-16 11:36:52,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:52,332][root][INFO] - Training Epoch: 1/2, step 5275/23838 completed (loss: 0.8326538801193237, acc: 0.7870370149612427)
[2025-02-16 11:36:52,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:52,791][root][INFO] - Training Epoch: 1/2, step 5276/23838 completed (loss: 0.4923841655254364, acc: 0.8604651093482971)
[2025-02-16 11:36:52,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:53,198][root][INFO] - Training Epoch: 1/2, step 5277/23838 completed (loss: 1.313148021697998, acc: 0.6499999761581421)
[2025-02-16 11:36:53,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:53,620][root][INFO] - Training Epoch: 1/2, step 5278/23838 completed (loss: 0.9002161026000977, acc: 0.7559055089950562)
[2025-02-16 11:36:53,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:54,025][root][INFO] - Training Epoch: 1/2, step 5279/23838 completed (loss: 0.7587733864784241, acc: 0.7681159377098083)
[2025-02-16 11:36:54,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:54,528][root][INFO] - Training Epoch: 1/2, step 5280/23838 completed (loss: 0.5111051797866821, acc: 0.800000011920929)
[2025-02-16 11:36:54,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:54,999][root][INFO] - Training Epoch: 1/2, step 5281/23838 completed (loss: 1.269485354423523, acc: 0.6428571343421936)
[2025-02-16 11:36:55,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:55,390][root][INFO] - Training Epoch: 1/2, step 5282/23838 completed (loss: 0.5980644822120667, acc: 0.8387096524238586)
[2025-02-16 11:36:55,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:55,774][root][INFO] - Training Epoch: 1/2, step 5283/23838 completed (loss: 0.6470057964324951, acc: 0.800000011920929)
[2025-02-16 11:36:55,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:56,209][root][INFO] - Training Epoch: 1/2, step 5284/23838 completed (loss: 0.46680670976638794, acc: 0.8879310488700867)
[2025-02-16 11:36:56,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:56,598][root][INFO] - Training Epoch: 1/2, step 5285/23838 completed (loss: 0.26504987478256226, acc: 0.9358974099159241)
[2025-02-16 11:36:56,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:56,975][root][INFO] - Training Epoch: 1/2, step 5286/23838 completed (loss: 0.6208041906356812, acc: 0.8219178318977356)
[2025-02-16 11:36:57,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:57,427][root][INFO] - Training Epoch: 1/2, step 5287/23838 completed (loss: 0.4481052756309509, acc: 0.8787878751754761)
[2025-02-16 11:36:57,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:57,803][root][INFO] - Training Epoch: 1/2, step 5288/23838 completed (loss: 0.6314958333969116, acc: 0.7719298005104065)
[2025-02-16 11:36:57,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:58,149][root][INFO] - Training Epoch: 1/2, step 5289/23838 completed (loss: 0.706610381603241, acc: 0.8035714030265808)
[2025-02-16 11:36:58,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:58,609][root][INFO] - Training Epoch: 1/2, step 5290/23838 completed (loss: 0.8674327731132507, acc: 0.7272727489471436)
[2025-02-16 11:36:58,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:59,007][root][INFO] - Training Epoch: 1/2, step 5291/23838 completed (loss: 0.2758921980857849, acc: 0.9358974099159241)
[2025-02-16 11:36:59,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:59,438][root][INFO] - Training Epoch: 1/2, step 5292/23838 completed (loss: 1.1508656740188599, acc: 0.6966292262077332)
[2025-02-16 11:36:59,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:36:59,835][root][INFO] - Training Epoch: 1/2, step 5293/23838 completed (loss: 0.5000101327896118, acc: 0.8461538553237915)
[2025-02-16 11:36:59,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:00,214][root][INFO] - Training Epoch: 1/2, step 5294/23838 completed (loss: 0.5744785666465759, acc: 0.8773584961891174)
[2025-02-16 11:37:00,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:00,720][root][INFO] - Training Epoch: 1/2, step 5295/23838 completed (loss: 0.6157131195068359, acc: 0.8705882430076599)
[2025-02-16 11:37:00,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:01,142][root][INFO] - Training Epoch: 1/2, step 5296/23838 completed (loss: 0.7155749797821045, acc: 0.8170731663703918)
[2025-02-16 11:37:01,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:01,629][root][INFO] - Training Epoch: 1/2, step 5297/23838 completed (loss: 0.32267674803733826, acc: 0.9193548560142517)
[2025-02-16 11:37:01,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:02,085][root][INFO] - Training Epoch: 1/2, step 5298/23838 completed (loss: 0.5113281607627869, acc: 0.8648648858070374)
[2025-02-16 11:37:02,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:02,483][root][INFO] - Training Epoch: 1/2, step 5299/23838 completed (loss: 0.22901807725429535, acc: 0.9462365508079529)
[2025-02-16 11:37:02,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:02,915][root][INFO] - Training Epoch: 1/2, step 5300/23838 completed (loss: 0.43270474672317505, acc: 0.8405796885490417)
[2025-02-16 11:37:03,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:03,303][root][INFO] - Training Epoch: 1/2, step 5301/23838 completed (loss: 0.5677399039268494, acc: 0.8541666865348816)
[2025-02-16 11:37:03,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:03,730][root][INFO] - Training Epoch: 1/2, step 5302/23838 completed (loss: 0.7386221289634705, acc: 0.7948718070983887)
[2025-02-16 11:37:03,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:04,133][root][INFO] - Training Epoch: 1/2, step 5303/23838 completed (loss: 0.6059349775314331, acc: 0.7924528121948242)
[2025-02-16 11:37:04,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:04,558][root][INFO] - Training Epoch: 1/2, step 5304/23838 completed (loss: 0.4901107847690582, acc: 0.8984375)
[2025-02-16 11:37:04,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:05,000][root][INFO] - Training Epoch: 1/2, step 5305/23838 completed (loss: 0.7363717555999756, acc: 0.800000011920929)
[2025-02-16 11:37:05,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:05,397][root][INFO] - Training Epoch: 1/2, step 5306/23838 completed (loss: 0.7511700987815857, acc: 0.7777777910232544)
[2025-02-16 11:37:05,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:05,845][root][INFO] - Training Epoch: 1/2, step 5307/23838 completed (loss: 0.6467689275741577, acc: 0.849711000919342)
[2025-02-16 11:37:06,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:06,290][root][INFO] - Training Epoch: 1/2, step 5308/23838 completed (loss: 0.45600876212120056, acc: 0.8985507488250732)
[2025-02-16 11:37:06,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:06,708][root][INFO] - Training Epoch: 1/2, step 5309/23838 completed (loss: 0.6740813255310059, acc: 0.8294573426246643)
[2025-02-16 11:37:06,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:07,108][root][INFO] - Training Epoch: 1/2, step 5310/23838 completed (loss: 0.42803123593330383, acc: 0.8900523781776428)
[2025-02-16 11:37:07,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:07,598][root][INFO] - Training Epoch: 1/2, step 5311/23838 completed (loss: 0.37468966841697693, acc: 0.8958333134651184)
[2025-02-16 11:37:07,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:08,018][root][INFO] - Training Epoch: 1/2, step 5312/23838 completed (loss: 0.478248655796051, acc: 0.8785714507102966)
[2025-02-16 11:37:08,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:08,407][root][INFO] - Training Epoch: 1/2, step 5313/23838 completed (loss: 0.4721532464027405, acc: 0.8859649300575256)
[2025-02-16 11:37:08,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:08,781][root][INFO] - Training Epoch: 1/2, step 5314/23838 completed (loss: 0.43565633893013, acc: 0.8939393758773804)
[2025-02-16 11:37:08,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:09,203][root][INFO] - Training Epoch: 1/2, step 5315/23838 completed (loss: 1.0582486391067505, acc: 0.7333333492279053)
[2025-02-16 11:37:09,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:09,650][root][INFO] - Training Epoch: 1/2, step 5316/23838 completed (loss: 0.3591041564941406, acc: 0.8888888955116272)
[2025-02-16 11:37:09,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:10,101][root][INFO] - Training Epoch: 1/2, step 5317/23838 completed (loss: 0.8715336322784424, acc: 0.7222222089767456)
[2025-02-16 11:37:10,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:10,516][root][INFO] - Training Epoch: 1/2, step 5318/23838 completed (loss: 1.0330944061279297, acc: 0.7037037014961243)
[2025-02-16 11:37:10,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:10,951][root][INFO] - Training Epoch: 1/2, step 5319/23838 completed (loss: 0.9221252799034119, acc: 0.7745097875595093)
[2025-02-16 11:37:11,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:11,403][root][INFO] - Training Epoch: 1/2, step 5320/23838 completed (loss: 0.5081146359443665, acc: 0.8369565010070801)
[2025-02-16 11:37:11,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:11,843][root][INFO] - Training Epoch: 1/2, step 5321/23838 completed (loss: 0.6947616934776306, acc: 0.811965823173523)
[2025-02-16 11:37:12,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:12,290][root][INFO] - Training Epoch: 1/2, step 5322/23838 completed (loss: 0.8213865756988525, acc: 0.7236841917037964)
[2025-02-16 11:37:12,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:12,743][root][INFO] - Training Epoch: 1/2, step 5323/23838 completed (loss: 0.6466016173362732, acc: 0.8316831588745117)
[2025-02-16 11:37:12,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:13,194][root][INFO] - Training Epoch: 1/2, step 5324/23838 completed (loss: 0.45035088062286377, acc: 0.9120879173278809)
[2025-02-16 11:37:13,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:13,615][root][INFO] - Training Epoch: 1/2, step 5325/23838 completed (loss: 0.5867316722869873, acc: 0.8512396812438965)
[2025-02-16 11:37:13,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:14,021][root][INFO] - Training Epoch: 1/2, step 5326/23838 completed (loss: 0.7647517919540405, acc: 0.8333333134651184)
[2025-02-16 11:37:14,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:14,462][root][INFO] - Training Epoch: 1/2, step 5327/23838 completed (loss: 0.5611953735351562, acc: 0.8120300769805908)
[2025-02-16 11:37:14,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:14,890][root][INFO] - Training Epoch: 1/2, step 5328/23838 completed (loss: 0.94361412525177, acc: 0.7818182110786438)
[2025-02-16 11:37:15,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:15,306][root][INFO] - Training Epoch: 1/2, step 5329/23838 completed (loss: 0.9482343792915344, acc: 0.6913580298423767)
[2025-02-16 11:37:15,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:15,748][root][INFO] - Training Epoch: 1/2, step 5330/23838 completed (loss: 0.7035320401191711, acc: 0.7460317611694336)
[2025-02-16 11:37:15,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:16,139][root][INFO] - Training Epoch: 1/2, step 5331/23838 completed (loss: 0.9139577746391296, acc: 0.7333333492279053)
[2025-02-16 11:37:16,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:16,518][root][INFO] - Training Epoch: 1/2, step 5332/23838 completed (loss: 0.9206236600875854, acc: 0.7464788556098938)
[2025-02-16 11:37:16,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:16,959][root][INFO] - Training Epoch: 1/2, step 5333/23838 completed (loss: 0.9242481589317322, acc: 0.6979166865348816)
[2025-02-16 11:37:17,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:17,362][root][INFO] - Training Epoch: 1/2, step 5334/23838 completed (loss: 0.5710758566856384, acc: 0.8333333134651184)
[2025-02-16 11:37:17,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:17,782][root][INFO] - Training Epoch: 1/2, step 5335/23838 completed (loss: 0.20250475406646729, acc: 0.934959352016449)
[2025-02-16 11:37:17,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:18,231][root][INFO] - Training Epoch: 1/2, step 5336/23838 completed (loss: 0.7836478352546692, acc: 0.8285714387893677)
[2025-02-16 11:37:18,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:18,632][root][INFO] - Training Epoch: 1/2, step 5337/23838 completed (loss: 0.6711636185646057, acc: 0.8333333134651184)
[2025-02-16 11:37:18,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:19,092][root][INFO] - Training Epoch: 1/2, step 5338/23838 completed (loss: 0.901968240737915, acc: 0.7297297120094299)
[2025-02-16 11:37:19,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:19,597][root][INFO] - Training Epoch: 1/2, step 5339/23838 completed (loss: 0.37077775597572327, acc: 0.9120879173278809)
[2025-02-16 11:37:19,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:20,054][root][INFO] - Training Epoch: 1/2, step 5340/23838 completed (loss: 0.9332436919212341, acc: 0.6969696879386902)
[2025-02-16 11:37:20,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:20,503][root][INFO] - Training Epoch: 1/2, step 5341/23838 completed (loss: 0.7134782075881958, acc: 0.7966101765632629)
[2025-02-16 11:37:20,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:20,925][root][INFO] - Training Epoch: 1/2, step 5342/23838 completed (loss: 0.3462246358394623, acc: 0.9252336621284485)
[2025-02-16 11:37:21,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:21,362][root][INFO] - Training Epoch: 1/2, step 5343/23838 completed (loss: 0.6523854732513428, acc: 0.8181818127632141)
[2025-02-16 11:37:21,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:21,756][root][INFO] - Training Epoch: 1/2, step 5344/23838 completed (loss: 0.8091986179351807, acc: 0.7909091114997864)
[2025-02-16 11:37:21,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:22,200][root][INFO] - Training Epoch: 1/2, step 5345/23838 completed (loss: 0.75522381067276, acc: 0.7777777910232544)
[2025-02-16 11:37:22,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:22,683][root][INFO] - Training Epoch: 1/2, step 5346/23838 completed (loss: 0.9878072738647461, acc: 0.75)
[2025-02-16 11:37:22,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:23,085][root][INFO] - Training Epoch: 1/2, step 5347/23838 completed (loss: 0.6374917030334473, acc: 0.8095238208770752)
[2025-02-16 11:37:23,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:23,559][root][INFO] - Training Epoch: 1/2, step 5348/23838 completed (loss: 0.40107157826423645, acc: 0.89552241563797)
[2025-02-16 11:37:23,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:24,003][root][INFO] - Training Epoch: 1/2, step 5349/23838 completed (loss: 0.3210436701774597, acc: 0.9011628031730652)
[2025-02-16 11:37:24,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:24,399][root][INFO] - Training Epoch: 1/2, step 5350/23838 completed (loss: 0.6953858733177185, acc: 0.761904776096344)
[2025-02-16 11:37:24,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:24,794][root][INFO] - Training Epoch: 1/2, step 5351/23838 completed (loss: 0.9354687333106995, acc: 0.739130437374115)
[2025-02-16 11:37:24,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:25,242][root][INFO] - Training Epoch: 1/2, step 5352/23838 completed (loss: 0.6803777813911438, acc: 0.818965494632721)
[2025-02-16 11:37:25,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:25,673][root][INFO] - Training Epoch: 1/2, step 5353/23838 completed (loss: 0.6386351585388184, acc: 0.8218390941619873)
[2025-02-16 11:37:25,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:26,053][root][INFO] - Training Epoch: 1/2, step 5354/23838 completed (loss: 0.6684446930885315, acc: 0.8510638475418091)
[2025-02-16 11:37:26,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:26,462][root][INFO] - Training Epoch: 1/2, step 5355/23838 completed (loss: 0.5567404627799988, acc: 0.8503937125205994)
[2025-02-16 11:37:26,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:26,849][root][INFO] - Training Epoch: 1/2, step 5356/23838 completed (loss: 1.0266331434249878, acc: 0.7760000228881836)
[2025-02-16 11:37:27,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:27,277][root][INFO] - Training Epoch: 1/2, step 5357/23838 completed (loss: 0.9708864688873291, acc: 0.7727272510528564)
[2025-02-16 11:37:27,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:27,701][root][INFO] - Training Epoch: 1/2, step 5358/23838 completed (loss: 0.22300373017787933, acc: 0.9316239356994629)
[2025-02-16 11:37:27,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:28,092][root][INFO] - Training Epoch: 1/2, step 5359/23838 completed (loss: 0.6437244415283203, acc: 0.8316831588745117)
[2025-02-16 11:37:28,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:28,534][root][INFO] - Training Epoch: 1/2, step 5360/23838 completed (loss: 0.36439234018325806, acc: 0.9268292784690857)
[2025-02-16 11:37:28,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:28,880][root][INFO] - Training Epoch: 1/2, step 5361/23838 completed (loss: 0.47977325320243835, acc: 0.8409090638160706)
[2025-02-16 11:37:29,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:29,265][root][INFO] - Training Epoch: 1/2, step 5362/23838 completed (loss: 0.6641183495521545, acc: 0.8139534592628479)
[2025-02-16 11:37:29,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:29,641][root][INFO] - Training Epoch: 1/2, step 5363/23838 completed (loss: 0.840137243270874, acc: 0.7890625)
[2025-02-16 11:37:29,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:30,033][root][INFO] - Training Epoch: 1/2, step 5364/23838 completed (loss: 0.8748841881752014, acc: 0.7716535329818726)
[2025-02-16 11:37:30,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:30,498][root][INFO] - Training Epoch: 1/2, step 5365/23838 completed (loss: 0.6306876540184021, acc: 0.8524590134620667)
[2025-02-16 11:37:30,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:30,888][root][INFO] - Training Epoch: 1/2, step 5366/23838 completed (loss: 0.4081646203994751, acc: 0.8916666507720947)
[2025-02-16 11:37:31,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:31,325][root][INFO] - Training Epoch: 1/2, step 5367/23838 completed (loss: 0.660947322845459, acc: 0.8148148059844971)
[2025-02-16 11:37:31,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:31,782][root][INFO] - Training Epoch: 1/2, step 5368/23838 completed (loss: 0.456664502620697, acc: 0.8717948794364929)
[2025-02-16 11:37:31,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:32,171][root][INFO] - Training Epoch: 1/2, step 5369/23838 completed (loss: 0.4390101432800293, acc: 0.874015748500824)
[2025-02-16 11:37:32,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:32,648][root][INFO] - Training Epoch: 1/2, step 5370/23838 completed (loss: 0.46701428294181824, acc: 0.8502673506736755)
[2025-02-16 11:37:32,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:33,096][root][INFO] - Training Epoch: 1/2, step 5371/23838 completed (loss: 0.39063599705696106, acc: 0.8926174640655518)
[2025-02-16 11:37:33,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:33,523][root][INFO] - Training Epoch: 1/2, step 5372/23838 completed (loss: 0.5000792741775513, acc: 0.8974359035491943)
[2025-02-16 11:37:33,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:33,891][root][INFO] - Training Epoch: 1/2, step 5373/23838 completed (loss: 0.8229381442070007, acc: 0.8113207817077637)
[2025-02-16 11:37:34,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:34,361][root][INFO] - Training Epoch: 1/2, step 5374/23838 completed (loss: 0.5642310380935669, acc: 0.8362069129943848)
[2025-02-16 11:37:34,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:34,828][root][INFO] - Training Epoch: 1/2, step 5375/23838 completed (loss: 0.36836305260658264, acc: 0.8918918967247009)
[2025-02-16 11:37:35,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:35,301][root][INFO] - Training Epoch: 1/2, step 5376/23838 completed (loss: 0.8801676630973816, acc: 0.78125)
[2025-02-16 11:37:35,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:35,728][root][INFO] - Training Epoch: 1/2, step 5377/23838 completed (loss: 0.8970485329627991, acc: 0.774193525314331)
[2025-02-16 11:37:35,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:36,141][root][INFO] - Training Epoch: 1/2, step 5378/23838 completed (loss: 0.4703989028930664, acc: 0.8791208863258362)
[2025-02-16 11:37:36,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:36,572][root][INFO] - Training Epoch: 1/2, step 5379/23838 completed (loss: 0.4613357484340668, acc: 0.8641975522041321)
[2025-02-16 11:37:36,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:36,961][root][INFO] - Training Epoch: 1/2, step 5380/23838 completed (loss: 0.5284683704376221, acc: 0.8727272748947144)
[2025-02-16 11:37:37,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:37,379][root][INFO] - Training Epoch: 1/2, step 5381/23838 completed (loss: 1.2960453033447266, acc: 0.6231883764266968)
[2025-02-16 11:37:37,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:37,802][root][INFO] - Training Epoch: 1/2, step 5382/23838 completed (loss: 1.3466205596923828, acc: 0.5833333134651184)
[2025-02-16 11:37:37,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:38,191][root][INFO] - Training Epoch: 1/2, step 5383/23838 completed (loss: 0.8410263657569885, acc: 0.7159090638160706)
[2025-02-16 11:37:38,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:38,559][root][INFO] - Training Epoch: 1/2, step 5384/23838 completed (loss: 0.5571379065513611, acc: 0.8653846383094788)
[2025-02-16 11:37:38,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:38,935][root][INFO] - Training Epoch: 1/2, step 5385/23838 completed (loss: 0.5834325551986694, acc: 0.774193525314331)
[2025-02-16 11:37:39,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:39,315][root][INFO] - Training Epoch: 1/2, step 5386/23838 completed (loss: 0.47803476452827454, acc: 0.8598130941390991)
[2025-02-16 11:37:39,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:39,728][root][INFO] - Training Epoch: 1/2, step 5387/23838 completed (loss: 0.8922271728515625, acc: 0.7575757503509521)
[2025-02-16 11:37:39,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:40,155][root][INFO] - Training Epoch: 1/2, step 5388/23838 completed (loss: 0.6644307971000671, acc: 0.8105263113975525)
[2025-02-16 11:37:40,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:40,666][root][INFO] - Training Epoch: 1/2, step 5389/23838 completed (loss: 1.1725895404815674, acc: 0.6666666865348816)
[2025-02-16 11:37:40,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:41,152][root][INFO] - Training Epoch: 1/2, step 5390/23838 completed (loss: 1.0898739099502563, acc: 0.7260273694992065)
[2025-02-16 11:37:41,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:41,598][root][INFO] - Training Epoch: 1/2, step 5391/23838 completed (loss: 0.7126649618148804, acc: 0.8023256063461304)
[2025-02-16 11:37:41,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:42,079][root][INFO] - Training Epoch: 1/2, step 5392/23838 completed (loss: 0.9319987893104553, acc: 0.7473683953285217)
[2025-02-16 11:37:42,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:42,478][root][INFO] - Training Epoch: 1/2, step 5393/23838 completed (loss: 1.042879581451416, acc: 0.7749999761581421)
[2025-02-16 11:37:42,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:42,860][root][INFO] - Training Epoch: 1/2, step 5394/23838 completed (loss: 0.8335716128349304, acc: 0.7361111044883728)
[2025-02-16 11:37:43,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:43,292][root][INFO] - Training Epoch: 1/2, step 5395/23838 completed (loss: 0.5942108631134033, acc: 0.8058252334594727)
[2025-02-16 11:37:43,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:43,714][root][INFO] - Training Epoch: 1/2, step 5396/23838 completed (loss: 0.7020187377929688, acc: 0.8170731663703918)
[2025-02-16 11:37:43,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:44,146][root][INFO] - Training Epoch: 1/2, step 5397/23838 completed (loss: 0.9954140782356262, acc: 0.7571428418159485)
[2025-02-16 11:37:44,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:44,571][root][INFO] - Training Epoch: 1/2, step 5398/23838 completed (loss: 0.7516167759895325, acc: 0.7938144207000732)
[2025-02-16 11:37:44,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:44,928][root][INFO] - Training Epoch: 1/2, step 5399/23838 completed (loss: 1.038454532623291, acc: 0.7413793206214905)
[2025-02-16 11:37:45,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:45,340][root][INFO] - Training Epoch: 1/2, step 5400/23838 completed (loss: 0.7677351236343384, acc: 0.800000011920929)
[2025-02-16 11:37:45,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:45,719][root][INFO] - Training Epoch: 1/2, step 5401/23838 completed (loss: 0.9368717670440674, acc: 0.7291666865348816)
[2025-02-16 11:37:45,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:46,173][root][INFO] - Training Epoch: 1/2, step 5402/23838 completed (loss: 1.0481479167938232, acc: 0.7471264600753784)
[2025-02-16 11:37:46,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:46,611][root][INFO] - Training Epoch: 1/2, step 5403/23838 completed (loss: 0.5834062099456787, acc: 0.835616409778595)
[2025-02-16 11:37:46,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:47,035][root][INFO] - Training Epoch: 1/2, step 5404/23838 completed (loss: 1.134744644165039, acc: 0.6515151262283325)
[2025-02-16 11:37:47,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:47,508][root][INFO] - Training Epoch: 1/2, step 5405/23838 completed (loss: 0.7412999272346497, acc: 0.7674418687820435)
[2025-02-16 11:37:47,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:47,961][root][INFO] - Training Epoch: 1/2, step 5406/23838 completed (loss: 0.8605103492736816, acc: 0.761904776096344)
[2025-02-16 11:37:48,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:48,408][root][INFO] - Training Epoch: 1/2, step 5407/23838 completed (loss: 0.5704089999198914, acc: 0.843478262424469)
[2025-02-16 11:37:48,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:48,838][root][INFO] - Training Epoch: 1/2, step 5408/23838 completed (loss: 0.8369328379631042, acc: 0.7849462628364563)
[2025-02-16 11:37:48,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:49,214][root][INFO] - Training Epoch: 1/2, step 5409/23838 completed (loss: 0.9715520143508911, acc: 0.747474730014801)
[2025-02-16 11:37:49,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:49,592][root][INFO] - Training Epoch: 1/2, step 5410/23838 completed (loss: 0.579802393913269, acc: 0.8316831588745117)
[2025-02-16 11:37:49,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:49,984][root][INFO] - Training Epoch: 1/2, step 5411/23838 completed (loss: 0.6308523416519165, acc: 0.8444444537162781)
[2025-02-16 11:37:50,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:50,442][root][INFO] - Training Epoch: 1/2, step 5412/23838 completed (loss: 0.5376176238059998, acc: 0.8615384697914124)
[2025-02-16 11:37:50,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:50,824][root][INFO] - Training Epoch: 1/2, step 5413/23838 completed (loss: 0.5374982357025146, acc: 0.8518518805503845)
[2025-02-16 11:37:50,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:51,249][root][INFO] - Training Epoch: 1/2, step 5414/23838 completed (loss: 0.6278281807899475, acc: 0.8409090638160706)
[2025-02-16 11:37:51,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:51,651][root][INFO] - Training Epoch: 1/2, step 5415/23838 completed (loss: 1.1803414821624756, acc: 0.6875)
[2025-02-16 11:37:51,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:52,088][root][INFO] - Training Epoch: 1/2, step 5416/23838 completed (loss: 0.44884252548217773, acc: 0.876288652420044)
[2025-02-16 11:37:52,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:52,517][root][INFO] - Training Epoch: 1/2, step 5417/23838 completed (loss: 0.9559605121612549, acc: 0.7317073345184326)
[2025-02-16 11:37:52,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:52,945][root][INFO] - Training Epoch: 1/2, step 5418/23838 completed (loss: 0.6520494222640991, acc: 0.8139534592628479)
[2025-02-16 11:37:53,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:53,354][root][INFO] - Training Epoch: 1/2, step 5419/23838 completed (loss: 0.8700714111328125, acc: 0.7289719581604004)
[2025-02-16 11:37:53,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:53,801][root][INFO] - Training Epoch: 1/2, step 5420/23838 completed (loss: 0.7287256717681885, acc: 0.8190476298332214)
[2025-02-16 11:37:54,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:54,267][root][INFO] - Training Epoch: 1/2, step 5421/23838 completed (loss: 1.1125874519348145, acc: 0.6666666865348816)
[2025-02-16 11:37:54,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:54,727][root][INFO] - Training Epoch: 1/2, step 5422/23838 completed (loss: 0.7106920480728149, acc: 0.7837837934494019)
[2025-02-16 11:37:54,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:55,134][root][INFO] - Training Epoch: 1/2, step 5423/23838 completed (loss: 0.2665244936943054, acc: 0.9325153231620789)
[2025-02-16 11:37:55,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:55,513][root][INFO] - Training Epoch: 1/2, step 5424/23838 completed (loss: 0.38666588068008423, acc: 0.8913043737411499)
[2025-02-16 11:37:55,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:55,926][root][INFO] - Training Epoch: 1/2, step 5425/23838 completed (loss: 0.7120044827461243, acc: 0.8470588326454163)
[2025-02-16 11:37:56,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:56,319][root][INFO] - Training Epoch: 1/2, step 5426/23838 completed (loss: 0.4820325970649719, acc: 0.8880000114440918)
[2025-02-16 11:37:56,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:56,680][root][INFO] - Training Epoch: 1/2, step 5427/23838 completed (loss: 0.923341691493988, acc: 0.7291666865348816)
[2025-02-16 11:37:56,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:57,152][root][INFO] - Training Epoch: 1/2, step 5428/23838 completed (loss: 0.7893916964530945, acc: 0.7547169923782349)
[2025-02-16 11:37:57,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:57,524][root][INFO] - Training Epoch: 1/2, step 5429/23838 completed (loss: 0.966269850730896, acc: 0.7301587462425232)
[2025-02-16 11:37:57,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:57,904][root][INFO] - Training Epoch: 1/2, step 5430/23838 completed (loss: 0.8321895599365234, acc: 0.800000011920929)
[2025-02-16 11:37:58,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:58,358][root][INFO] - Training Epoch: 1/2, step 5431/23838 completed (loss: 0.5167715549468994, acc: 0.8571428656578064)
[2025-02-16 11:37:58,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:58,752][root][INFO] - Training Epoch: 1/2, step 5432/23838 completed (loss: 0.6883214712142944, acc: 0.8333333134651184)
[2025-02-16 11:37:58,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:59,133][root][INFO] - Training Epoch: 1/2, step 5433/23838 completed (loss: 0.6484248042106628, acc: 0.8190476298332214)
[2025-02-16 11:37:59,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:37:59,587][root][INFO] - Training Epoch: 1/2, step 5434/23838 completed (loss: 0.34845972061157227, acc: 0.8730158805847168)
[2025-02-16 11:37:59,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:00,047][root][INFO] - Training Epoch: 1/2, step 5435/23838 completed (loss: 0.5709438920021057, acc: 0.8550724387168884)
[2025-02-16 11:38:00,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:00,441][root][INFO] - Training Epoch: 1/2, step 5436/23838 completed (loss: 0.7033882737159729, acc: 0.75)
[2025-02-16 11:38:00,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:00,855][root][INFO] - Training Epoch: 1/2, step 5437/23838 completed (loss: 0.8010019659996033, acc: 0.808080792427063)
[2025-02-16 11:38:01,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:01,301][root][INFO] - Training Epoch: 1/2, step 5438/23838 completed (loss: 0.575824499130249, acc: 0.8534482717514038)
[2025-02-16 11:38:01,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:01,765][root][INFO] - Training Epoch: 1/2, step 5439/23838 completed (loss: 0.5322754383087158, acc: 0.8548387289047241)
[2025-02-16 11:38:01,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:02,170][root][INFO] - Training Epoch: 1/2, step 5440/23838 completed (loss: 0.5143923759460449, acc: 0.8421052694320679)
[2025-02-16 11:38:02,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:02,583][root][INFO] - Training Epoch: 1/2, step 5441/23838 completed (loss: 0.3800864517688751, acc: 0.8971962332725525)
[2025-02-16 11:38:02,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:03,006][root][INFO] - Training Epoch: 1/2, step 5442/23838 completed (loss: 0.9292051196098328, acc: 0.7204301357269287)
[2025-02-16 11:38:03,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:03,470][root][INFO] - Training Epoch: 1/2, step 5443/23838 completed (loss: 0.9940004348754883, acc: 0.7339449524879456)
[2025-02-16 11:38:03,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:03,909][root][INFO] - Training Epoch: 1/2, step 5444/23838 completed (loss: 0.6227149367332458, acc: 0.8709677457809448)
[2025-02-16 11:38:04,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:04,379][root][INFO] - Training Epoch: 1/2, step 5445/23838 completed (loss: 0.6453331708908081, acc: 0.8333333134651184)
[2025-02-16 11:38:04,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:04,795][root][INFO] - Training Epoch: 1/2, step 5446/23838 completed (loss: 0.5512731671333313, acc: 0.8305084705352783)
[2025-02-16 11:38:04,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:05,162][root][INFO] - Training Epoch: 1/2, step 5447/23838 completed (loss: 0.3034043610095978, acc: 0.9178082346916199)
[2025-02-16 11:38:05,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:05,566][root][INFO] - Training Epoch: 1/2, step 5448/23838 completed (loss: 0.9733366966247559, acc: 0.7254902124404907)
[2025-02-16 11:38:05,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:05,951][root][INFO] - Training Epoch: 1/2, step 5449/23838 completed (loss: 0.6889186501502991, acc: 0.8488371968269348)
[2025-02-16 11:38:06,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:06,393][root][INFO] - Training Epoch: 1/2, step 5450/23838 completed (loss: 0.30282095074653625, acc: 0.9090909361839294)
[2025-02-16 11:38:06,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:06,869][root][INFO] - Training Epoch: 1/2, step 5451/23838 completed (loss: 0.5112314224243164, acc: 0.8571428656578064)
[2025-02-16 11:38:07,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:07,384][root][INFO] - Training Epoch: 1/2, step 5452/23838 completed (loss: 0.38502007722854614, acc: 0.8983050584793091)
[2025-02-16 11:38:07,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:07,804][root][INFO] - Training Epoch: 1/2, step 5453/23838 completed (loss: 0.652958333492279, acc: 0.8188976645469666)
[2025-02-16 11:38:07,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:08,271][root][INFO] - Training Epoch: 1/2, step 5454/23838 completed (loss: 0.5050427913665771, acc: 0.8620689511299133)
[2025-02-16 11:38:08,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:08,694][root][INFO] - Training Epoch: 1/2, step 5455/23838 completed (loss: 0.49415725469589233, acc: 0.9223300814628601)
[2025-02-16 11:38:08,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:09,078][root][INFO] - Training Epoch: 1/2, step 5456/23838 completed (loss: 0.49324965476989746, acc: 0.849056601524353)
[2025-02-16 11:38:09,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:09,500][root][INFO] - Training Epoch: 1/2, step 5457/23838 completed (loss: 0.6711995601654053, acc: 0.8064516186714172)
[2025-02-16 11:38:09,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:09,967][root][INFO] - Training Epoch: 1/2, step 5458/23838 completed (loss: 1.008702039718628, acc: 0.7049180269241333)
[2025-02-16 11:38:10,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:10,416][root][INFO] - Training Epoch: 1/2, step 5459/23838 completed (loss: 0.7927315831184387, acc: 0.7875000238418579)
[2025-02-16 11:38:10,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:10,850][root][INFO] - Training Epoch: 1/2, step 5460/23838 completed (loss: 0.32285580039024353, acc: 0.9405940771102905)
[2025-02-16 11:38:11,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:11,317][root][INFO] - Training Epoch: 1/2, step 5461/23838 completed (loss: 1.0217363834381104, acc: 0.7641509175300598)
[2025-02-16 11:38:11,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:11,698][root][INFO] - Training Epoch: 1/2, step 5462/23838 completed (loss: 0.918043851852417, acc: 0.7291666865348816)
[2025-02-16 11:38:11,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:12,100][root][INFO] - Training Epoch: 1/2, step 5463/23838 completed (loss: 0.7603489756584167, acc: 0.75)
[2025-02-16 11:38:12,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:12,480][root][INFO] - Training Epoch: 1/2, step 5464/23838 completed (loss: 0.39946237206459045, acc: 0.90625)
[2025-02-16 11:38:12,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:12,928][root][INFO] - Training Epoch: 1/2, step 5465/23838 completed (loss: 0.4778182804584503, acc: 0.8602150678634644)
[2025-02-16 11:38:13,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:13,387][root][INFO] - Training Epoch: 1/2, step 5466/23838 completed (loss: 0.6839786767959595, acc: 0.7702702879905701)
[2025-02-16 11:38:13,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:13,833][root][INFO] - Training Epoch: 1/2, step 5467/23838 completed (loss: 0.9575114250183105, acc: 0.7346938848495483)
[2025-02-16 11:38:13,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:14,210][root][INFO] - Training Epoch: 1/2, step 5468/23838 completed (loss: 0.6448739767074585, acc: 0.8194444179534912)
[2025-02-16 11:38:14,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:14,620][root][INFO] - Training Epoch: 1/2, step 5469/23838 completed (loss: 0.6493574976921082, acc: 0.8279569745063782)
[2025-02-16 11:38:14,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:15,008][root][INFO] - Training Epoch: 1/2, step 5470/23838 completed (loss: 1.0662282705307007, acc: 0.6758620738983154)
[2025-02-16 11:38:15,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:15,405][root][INFO] - Training Epoch: 1/2, step 5471/23838 completed (loss: 0.5414692759513855, acc: 0.8600000143051147)
[2025-02-16 11:38:15,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:15,908][root][INFO] - Training Epoch: 1/2, step 5472/23838 completed (loss: 0.6605041027069092, acc: 0.8311688303947449)
[2025-02-16 11:38:16,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:16,344][root][INFO] - Training Epoch: 1/2, step 5473/23838 completed (loss: 0.5064892768859863, acc: 0.8595041036605835)
[2025-02-16 11:38:16,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:16,819][root][INFO] - Training Epoch: 1/2, step 5474/23838 completed (loss: 0.8457911610603333, acc: 0.7661290168762207)
[2025-02-16 11:38:16,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:17,217][root][INFO] - Training Epoch: 1/2, step 5475/23838 completed (loss: 0.6810353398323059, acc: 0.8349514603614807)
[2025-02-16 11:38:17,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:17,665][root][INFO] - Training Epoch: 1/2, step 5476/23838 completed (loss: 0.45478349924087524, acc: 0.8839285969734192)
[2025-02-16 11:38:17,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:18,095][root][INFO] - Training Epoch: 1/2, step 5477/23838 completed (loss: 0.5488730072975159, acc: 0.8444444537162781)
[2025-02-16 11:38:18,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:18,514][root][INFO] - Training Epoch: 1/2, step 5478/23838 completed (loss: 0.9955050349235535, acc: 0.7627118825912476)
[2025-02-16 11:38:18,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:18,957][root][INFO] - Training Epoch: 1/2, step 5479/23838 completed (loss: 0.7076045870780945, acc: 0.8235294222831726)
[2025-02-16 11:38:19,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:19,396][root][INFO] - Training Epoch: 1/2, step 5480/23838 completed (loss: 0.6529123783111572, acc: 0.8235294222831726)
[2025-02-16 11:38:19,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:19,803][root][INFO] - Training Epoch: 1/2, step 5481/23838 completed (loss: 0.21566277742385864, acc: 0.9528301954269409)
[2025-02-16 11:38:19,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:20,181][root][INFO] - Training Epoch: 1/2, step 5482/23838 completed (loss: 0.6106324195861816, acc: 0.8314606547355652)
[2025-02-16 11:38:20,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:20,584][root][INFO] - Training Epoch: 1/2, step 5483/23838 completed (loss: 0.3589233458042145, acc: 0.8947368264198303)
[2025-02-16 11:38:20,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:21,006][root][INFO] - Training Epoch: 1/2, step 5484/23838 completed (loss: 0.31125059723854065, acc: 0.9025974273681641)
[2025-02-16 11:38:21,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:21,382][root][INFO] - Training Epoch: 1/2, step 5485/23838 completed (loss: 0.41157588362693787, acc: 0.8837209343910217)
[2025-02-16 11:38:21,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:21,842][root][INFO] - Training Epoch: 1/2, step 5486/23838 completed (loss: 0.5339837074279785, acc: 0.8563535809516907)
[2025-02-16 11:38:22,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:22,291][root][INFO] - Training Epoch: 1/2, step 5487/23838 completed (loss: 0.6137793660163879, acc: 0.8227847814559937)
[2025-02-16 11:38:22,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:22,662][root][INFO] - Training Epoch: 1/2, step 5488/23838 completed (loss: 0.7302190065383911, acc: 0.748031497001648)
[2025-02-16 11:38:22,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:23,106][root][INFO] - Training Epoch: 1/2, step 5489/23838 completed (loss: 1.1798896789550781, acc: 0.6811594367027283)
[2025-02-16 11:38:23,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:23,582][root][INFO] - Training Epoch: 1/2, step 5490/23838 completed (loss: 0.40872839093208313, acc: 0.8771929740905762)
[2025-02-16 11:38:23,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:23,995][root][INFO] - Training Epoch: 1/2, step 5491/23838 completed (loss: 0.6791749000549316, acc: 0.8055555820465088)
[2025-02-16 11:38:24,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:24,365][root][INFO] - Training Epoch: 1/2, step 5492/23838 completed (loss: 0.6224567294120789, acc: 0.8278145790100098)
[2025-02-16 11:38:24,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:24,760][root][INFO] - Training Epoch: 1/2, step 5493/23838 completed (loss: 1.0367218255996704, acc: 0.746835470199585)
[2025-02-16 11:38:24,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:25,173][root][INFO] - Training Epoch: 1/2, step 5494/23838 completed (loss: 0.5078224539756775, acc: 0.8839285969734192)
[2025-02-16 11:38:25,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:25,626][root][INFO] - Training Epoch: 1/2, step 5495/23838 completed (loss: 0.41383862495422363, acc: 0.875)
[2025-02-16 11:38:25,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:26,011][root][INFO] - Training Epoch: 1/2, step 5496/23838 completed (loss: 0.8140485882759094, acc: 0.8125)
[2025-02-16 11:38:26,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:26,474][root][INFO] - Training Epoch: 1/2, step 5497/23838 completed (loss: 0.7279649376869202, acc: 0.8133333325386047)
[2025-02-16 11:38:26,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:26,937][root][INFO] - Training Epoch: 1/2, step 5498/23838 completed (loss: 0.3074536919593811, acc: 0.9122806787490845)
[2025-02-16 11:38:27,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:27,330][root][INFO] - Training Epoch: 1/2, step 5499/23838 completed (loss: 0.2871077358722687, acc: 0.9366196990013123)
[2025-02-16 11:38:27,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:27,803][root][INFO] - Training Epoch: 1/2, step 5500/23838 completed (loss: 0.5886372327804565, acc: 0.8151260614395142)
[2025-02-16 11:38:28,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:28,255][root][INFO] - Training Epoch: 1/2, step 5501/23838 completed (loss: 0.4388265907764435, acc: 0.9100000262260437)
[2025-02-16 11:38:28,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:28,695][root][INFO] - Training Epoch: 1/2, step 5502/23838 completed (loss: 0.4172552824020386, acc: 0.871999979019165)
[2025-02-16 11:38:28,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:29,118][root][INFO] - Training Epoch: 1/2, step 5503/23838 completed (loss: 0.4360191226005554, acc: 0.875)
[2025-02-16 11:38:29,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:29,588][root][INFO] - Training Epoch: 1/2, step 5504/23838 completed (loss: 0.5003575086593628, acc: 0.862500011920929)
[2025-02-16 11:38:29,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:30,008][root][INFO] - Training Epoch: 1/2, step 5505/23838 completed (loss: 0.325960248708725, acc: 0.9142857193946838)
[2025-02-16 11:38:30,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:30,448][root][INFO] - Training Epoch: 1/2, step 5506/23838 completed (loss: 0.22390936315059662, acc: 0.9370078444480896)
[2025-02-16 11:38:30,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:30,896][root][INFO] - Training Epoch: 1/2, step 5507/23838 completed (loss: 0.5472610592842102, acc: 0.845588207244873)
[2025-02-16 11:38:31,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:31,323][root][INFO] - Training Epoch: 1/2, step 5508/23838 completed (loss: 0.37039363384246826, acc: 0.898876428604126)
[2025-02-16 11:38:31,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:31,726][root][INFO] - Training Epoch: 1/2, step 5509/23838 completed (loss: 0.7618797421455383, acc: 0.7863247990608215)
[2025-02-16 11:38:31,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:32,142][root][INFO] - Training Epoch: 1/2, step 5510/23838 completed (loss: 0.44664305448532104, acc: 0.8484848737716675)
[2025-02-16 11:38:32,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:32,598][root][INFO] - Training Epoch: 1/2, step 5511/23838 completed (loss: 0.752407431602478, acc: 0.8055555820465088)
[2025-02-16 11:38:32,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:32,972][root][INFO] - Training Epoch: 1/2, step 5512/23838 completed (loss: 0.21721895039081573, acc: 0.9320388436317444)
[2025-02-16 11:38:33,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:33,383][root][INFO] - Training Epoch: 1/2, step 5513/23838 completed (loss: 0.44649627804756165, acc: 0.8672566413879395)
[2025-02-16 11:38:33,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:33,831][root][INFO] - Training Epoch: 1/2, step 5514/23838 completed (loss: 0.41672056913375854, acc: 0.875)
[2025-02-16 11:38:34,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:34,232][root][INFO] - Training Epoch: 1/2, step 5515/23838 completed (loss: 0.5473760962486267, acc: 0.8181818127632141)
[2025-02-16 11:38:34,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:34,652][root][INFO] - Training Epoch: 1/2, step 5516/23838 completed (loss: 0.4690112769603729, acc: 0.8478260636329651)
[2025-02-16 11:38:34,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:35,039][root][INFO] - Training Epoch: 1/2, step 5517/23838 completed (loss: 0.7927834987640381, acc: 0.800000011920929)
[2025-02-16 11:38:35,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:35,443][root][INFO] - Training Epoch: 1/2, step 5518/23838 completed (loss: 0.7296335101127625, acc: 0.7967479825019836)
[2025-02-16 11:38:35,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:35,878][root][INFO] - Training Epoch: 1/2, step 5519/23838 completed (loss: 1.069201111793518, acc: 0.7777777910232544)
[2025-02-16 11:38:36,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:36,256][root][INFO] - Training Epoch: 1/2, step 5520/23838 completed (loss: 0.6592581272125244, acc: 0.8125)
[2025-02-16 11:38:36,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:36,689][root][INFO] - Training Epoch: 1/2, step 5521/23838 completed (loss: 0.5942428112030029, acc: 0.817307710647583)
[2025-02-16 11:38:36,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:37,106][root][INFO] - Training Epoch: 1/2, step 5522/23838 completed (loss: 0.42800647020339966, acc: 0.8738738894462585)
[2025-02-16 11:38:37,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:37,597][root][INFO] - Training Epoch: 1/2, step 5523/23838 completed (loss: 0.7805862426757812, acc: 0.800000011920929)
[2025-02-16 11:38:37,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:38,029][root][INFO] - Training Epoch: 1/2, step 5524/23838 completed (loss: 0.6928451657295227, acc: 0.7714285850524902)
[2025-02-16 11:38:38,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:38,438][root][INFO] - Training Epoch: 1/2, step 5525/23838 completed (loss: 0.30945298075675964, acc: 0.9173553586006165)
[2025-02-16 11:38:38,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:38,899][root][INFO] - Training Epoch: 1/2, step 5526/23838 completed (loss: 0.822198212146759, acc: 0.7711864113807678)
[2025-02-16 11:38:39,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:39,290][root][INFO] - Training Epoch: 1/2, step 5527/23838 completed (loss: 0.46948114037513733, acc: 0.8717948794364929)
[2025-02-16 11:38:39,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:39,771][root][INFO] - Training Epoch: 1/2, step 5528/23838 completed (loss: 0.4986841678619385, acc: 0.8470588326454163)
[2025-02-16 11:38:39,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:40,275][root][INFO] - Training Epoch: 1/2, step 5529/23838 completed (loss: 0.47312256693840027, acc: 0.8407079577445984)
[2025-02-16 11:38:40,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:40,677][root][INFO] - Training Epoch: 1/2, step 5530/23838 completed (loss: 0.517672598361969, acc: 0.8524590134620667)
[2025-02-16 11:38:40,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:41,078][root][INFO] - Training Epoch: 1/2, step 5531/23838 completed (loss: 0.34637922048568726, acc: 0.8947368264198303)
[2025-02-16 11:38:41,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:41,496][root][INFO] - Training Epoch: 1/2, step 5532/23838 completed (loss: 0.6997698545455933, acc: 0.8170731663703918)
[2025-02-16 11:38:41,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:41,878][root][INFO] - Training Epoch: 1/2, step 5533/23838 completed (loss: 0.18063272535800934, acc: 0.9375)
[2025-02-16 11:38:42,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:42,261][root][INFO] - Training Epoch: 1/2, step 5534/23838 completed (loss: 0.6001733541488647, acc: 0.8032786846160889)
[2025-02-16 11:38:42,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:42,633][root][INFO] - Training Epoch: 1/2, step 5535/23838 completed (loss: 0.40758562088012695, acc: 0.8876404762268066)
[2025-02-16 11:38:42,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:43,103][root][INFO] - Training Epoch: 1/2, step 5536/23838 completed (loss: 0.660752534866333, acc: 0.8644067645072937)
[2025-02-16 11:38:43,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:43,569][root][INFO] - Training Epoch: 1/2, step 5537/23838 completed (loss: 0.4458223581314087, acc: 0.8571428656578064)
[2025-02-16 11:38:43,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:43,992][root][INFO] - Training Epoch: 1/2, step 5538/23838 completed (loss: 0.5132402777671814, acc: 0.8582677245140076)
[2025-02-16 11:38:44,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:44,405][root][INFO] - Training Epoch: 1/2, step 5539/23838 completed (loss: 0.21255114674568176, acc: 0.9320388436317444)
[2025-02-16 11:38:44,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:44,858][root][INFO] - Training Epoch: 1/2, step 5540/23838 completed (loss: 0.26090458035469055, acc: 0.9333333373069763)
[2025-02-16 11:38:45,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:45,316][root][INFO] - Training Epoch: 1/2, step 5541/23838 completed (loss: 0.5174834132194519, acc: 0.8518518805503845)
[2025-02-16 11:38:45,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:45,723][root][INFO] - Training Epoch: 1/2, step 5542/23838 completed (loss: 0.49141260981559753, acc: 0.8933333158493042)
[2025-02-16 11:38:45,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:46,139][root][INFO] - Training Epoch: 1/2, step 5543/23838 completed (loss: 0.705553412437439, acc: 0.8021978139877319)
[2025-02-16 11:38:46,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:46,519][root][INFO] - Training Epoch: 1/2, step 5544/23838 completed (loss: 1.5072021484375, acc: 0.5633803009986877)
[2025-02-16 11:38:46,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:46,961][root][INFO] - Training Epoch: 1/2, step 5545/23838 completed (loss: 0.597423255443573, acc: 0.779411792755127)
[2025-02-16 11:38:47,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:47,371][root][INFO] - Training Epoch: 1/2, step 5546/23838 completed (loss: 0.7278078198432922, acc: 0.7954545617103577)
[2025-02-16 11:38:47,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:47,848][root][INFO] - Training Epoch: 1/2, step 5547/23838 completed (loss: 0.39548566937446594, acc: 0.8916666507720947)
[2025-02-16 11:38:48,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:48,296][root][INFO] - Training Epoch: 1/2, step 5548/23838 completed (loss: 0.2810116410255432, acc: 0.9207921028137207)
[2025-02-16 11:38:48,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:48,667][root][INFO] - Training Epoch: 1/2, step 5549/23838 completed (loss: 0.48908165097236633, acc: 0.8602941036224365)
[2025-02-16 11:38:48,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:49,149][root][INFO] - Training Epoch: 1/2, step 5550/23838 completed (loss: 0.20217160880565643, acc: 0.9465240836143494)
[2025-02-16 11:38:49,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:49,530][root][INFO] - Training Epoch: 1/2, step 5551/23838 completed (loss: 0.7029070854187012, acc: 0.8452380895614624)
[2025-02-16 11:38:49,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:49,923][root][INFO] - Training Epoch: 1/2, step 5552/23838 completed (loss: 0.3114059865474701, acc: 0.90625)
[2025-02-16 11:38:50,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:50,418][root][INFO] - Training Epoch: 1/2, step 5553/23838 completed (loss: 0.5368902683258057, acc: 0.8613138794898987)
[2025-02-16 11:38:50,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:50,856][root][INFO] - Training Epoch: 1/2, step 5554/23838 completed (loss: 0.4402640759944916, acc: 0.8651685118675232)
[2025-02-16 11:38:51,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:51,257][root][INFO] - Training Epoch: 1/2, step 5555/23838 completed (loss: 0.9192456603050232, acc: 0.7307692170143127)
[2025-02-16 11:38:51,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:51,657][root][INFO] - Training Epoch: 1/2, step 5556/23838 completed (loss: 0.5769505500793457, acc: 0.8513513803482056)
[2025-02-16 11:38:51,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:52,020][root][INFO] - Training Epoch: 1/2, step 5557/23838 completed (loss: 0.7629018425941467, acc: 0.8125)
[2025-02-16 11:38:52,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:52,401][root][INFO] - Training Epoch: 1/2, step 5558/23838 completed (loss: 0.4338371455669403, acc: 0.8571428656578064)
[2025-02-16 11:38:52,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:52,842][root][INFO] - Training Epoch: 1/2, step 5559/23838 completed (loss: 0.8351947665214539, acc: 0.7883211970329285)
[2025-02-16 11:38:53,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:53,243][root][INFO] - Training Epoch: 1/2, step 5560/23838 completed (loss: 0.6973705887794495, acc: 0.8199999928474426)
[2025-02-16 11:38:53,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:53,670][root][INFO] - Training Epoch: 1/2, step 5561/23838 completed (loss: 0.3918571472167969, acc: 0.8913043737411499)
[2025-02-16 11:38:53,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:54,131][root][INFO] - Training Epoch: 1/2, step 5562/23838 completed (loss: 0.3883109986782074, acc: 0.8709677457809448)
[2025-02-16 11:38:54,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:54,519][root][INFO] - Training Epoch: 1/2, step 5563/23838 completed (loss: 1.0039682388305664, acc: 0.7413793206214905)
[2025-02-16 11:38:54,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:54,984][root][INFO] - Training Epoch: 1/2, step 5564/23838 completed (loss: 0.4936548173427582, acc: 0.8282828330993652)
[2025-02-16 11:38:55,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:55,441][root][INFO] - Training Epoch: 1/2, step 5565/23838 completed (loss: 0.37941405177116394, acc: 0.895348846912384)
[2025-02-16 11:38:55,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:55,859][root][INFO] - Training Epoch: 1/2, step 5566/23838 completed (loss: 0.3430671691894531, acc: 0.9076923131942749)
[2025-02-16 11:38:56,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:56,303][root][INFO] - Training Epoch: 1/2, step 5567/23838 completed (loss: 0.8290790319442749, acc: 0.7613636255264282)
[2025-02-16 11:38:56,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:56,769][root][INFO] - Training Epoch: 1/2, step 5568/23838 completed (loss: 0.23700346052646637, acc: 0.9130434989929199)
[2025-02-16 11:38:56,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:57,221][root][INFO] - Training Epoch: 1/2, step 5569/23838 completed (loss: 0.34275534749031067, acc: 0.8914728760719299)
[2025-02-16 11:38:57,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:57,691][root][INFO] - Training Epoch: 1/2, step 5570/23838 completed (loss: 0.5800210237503052, acc: 0.8557692170143127)
[2025-02-16 11:38:57,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:58,143][root][INFO] - Training Epoch: 1/2, step 5571/23838 completed (loss: 0.7731112837791443, acc: 0.7708333134651184)
[2025-02-16 11:38:58,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:58,584][root][INFO] - Training Epoch: 1/2, step 5572/23838 completed (loss: 0.7507085204124451, acc: 0.8125)
[2025-02-16 11:38:58,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:59,025][root][INFO] - Training Epoch: 1/2, step 5573/23838 completed (loss: 0.9759042263031006, acc: 0.6707317233085632)
[2025-02-16 11:38:59,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:59,484][root][INFO] - Training Epoch: 1/2, step 5574/23838 completed (loss: 0.6144810914993286, acc: 0.853210985660553)
[2025-02-16 11:38:59,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:38:59,941][root][INFO] - Training Epoch: 1/2, step 5575/23838 completed (loss: 0.7681075930595398, acc: 0.7714285850524902)
[2025-02-16 11:39:00,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:00,360][root][INFO] - Training Epoch: 1/2, step 5576/23838 completed (loss: 0.8343662619590759, acc: 0.7903226017951965)
[2025-02-16 11:39:00,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:00,769][root][INFO] - Training Epoch: 1/2, step 5577/23838 completed (loss: 0.6219775676727295, acc: 0.8823529481887817)
[2025-02-16 11:39:00,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:01,203][root][INFO] - Training Epoch: 1/2, step 5578/23838 completed (loss: 0.7916275858879089, acc: 0.7904762029647827)
[2025-02-16 11:39:01,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:01,644][root][INFO] - Training Epoch: 1/2, step 5579/23838 completed (loss: 1.3956551551818848, acc: 0.568965494632721)
[2025-02-16 11:39:01,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:02,013][root][INFO] - Training Epoch: 1/2, step 5580/23838 completed (loss: 0.4152749478816986, acc: 0.8823529481887817)
[2025-02-16 11:39:02,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:02,429][root][INFO] - Training Epoch: 1/2, step 5581/23838 completed (loss: 0.7835286259651184, acc: 0.7669903039932251)
[2025-02-16 11:39:02,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:02,898][root][INFO] - Training Epoch: 1/2, step 5582/23838 completed (loss: 0.6001507639884949, acc: 0.8503401279449463)
[2025-02-16 11:39:03,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:03,343][root][INFO] - Training Epoch: 1/2, step 5583/23838 completed (loss: 1.0755653381347656, acc: 0.8208954930305481)
[2025-02-16 11:39:03,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:03,744][root][INFO] - Training Epoch: 1/2, step 5584/23838 completed (loss: 0.9015174508094788, acc: 0.7413793206214905)
[2025-02-16 11:39:03,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:04,181][root][INFO] - Training Epoch: 1/2, step 5585/23838 completed (loss: 0.7336599826812744, acc: 0.7744361162185669)
[2025-02-16 11:39:04,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:04,680][root][INFO] - Training Epoch: 1/2, step 5586/23838 completed (loss: 0.5255587697029114, acc: 0.8760330677032471)
[2025-02-16 11:39:04,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:05,120][root][INFO] - Training Epoch: 1/2, step 5587/23838 completed (loss: 0.5863174200057983, acc: 0.8413792848587036)
[2025-02-16 11:39:05,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:05,597][root][INFO] - Training Epoch: 1/2, step 5588/23838 completed (loss: 0.5874962210655212, acc: 0.8311688303947449)
[2025-02-16 11:39:05,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:06,046][root][INFO] - Training Epoch: 1/2, step 5589/23838 completed (loss: 0.6490952372550964, acc: 0.8433734774589539)
[2025-02-16 11:39:06,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:06,447][root][INFO] - Training Epoch: 1/2, step 5590/23838 completed (loss: 0.40805163979530334, acc: 0.9066666960716248)
[2025-02-16 11:39:06,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:06,826][root][INFO] - Training Epoch: 1/2, step 5591/23838 completed (loss: 0.7685086131095886, acc: 0.75)
[2025-02-16 11:39:06,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:07,253][root][INFO] - Training Epoch: 1/2, step 5592/23838 completed (loss: 0.6890074610710144, acc: 0.8131868243217468)
[2025-02-16 11:39:07,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:07,725][root][INFO] - Training Epoch: 1/2, step 5593/23838 completed (loss: 0.9555398225784302, acc: 0.7222222089767456)
[2025-02-16 11:39:07,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:08,104][root][INFO] - Training Epoch: 1/2, step 5594/23838 completed (loss: 0.6921873688697815, acc: 0.800000011920929)
[2025-02-16 11:39:08,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:08,552][root][INFO] - Training Epoch: 1/2, step 5595/23838 completed (loss: 0.6151623129844666, acc: 0.8220338821411133)
[2025-02-16 11:39:08,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:08,938][root][INFO] - Training Epoch: 1/2, step 5596/23838 completed (loss: 0.9020745754241943, acc: 0.7538461685180664)
[2025-02-16 11:39:09,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:09,390][root][INFO] - Training Epoch: 1/2, step 5597/23838 completed (loss: 0.7232068777084351, acc: 0.800000011920929)
[2025-02-16 11:39:09,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:09,835][root][INFO] - Training Epoch: 1/2, step 5598/23838 completed (loss: 0.8912383317947388, acc: 0.7837837934494019)
[2025-02-16 11:39:10,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:10,309][root][INFO] - Training Epoch: 1/2, step 5599/23838 completed (loss: 0.2847631871700287, acc: 0.9285714030265808)
[2025-02-16 11:39:10,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:10,916][root][INFO] - Training Epoch: 1/2, step 5600/23838 completed (loss: 0.8927892446517944, acc: 0.768750011920929)
[2025-02-16 11:39:11,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:11,347][root][INFO] - Training Epoch: 1/2, step 5601/23838 completed (loss: 0.6182324290275574, acc: 0.8125)
[2025-02-16 11:39:11,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:11,766][root][INFO] - Training Epoch: 1/2, step 5602/23838 completed (loss: 0.3712577819824219, acc: 0.9013158082962036)
[2025-02-16 11:39:11,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:12,188][root][INFO] - Training Epoch: 1/2, step 5603/23838 completed (loss: 0.31589990854263306, acc: 0.9113923907279968)
[2025-02-16 11:39:12,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:12,620][root][INFO] - Training Epoch: 1/2, step 5604/23838 completed (loss: 0.6239447593688965, acc: 0.8111110925674438)
[2025-02-16 11:39:12,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:13,055][root][INFO] - Training Epoch: 1/2, step 5605/23838 completed (loss: 0.8600186705589294, acc: 0.7300000190734863)
[2025-02-16 11:39:13,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:13,438][root][INFO] - Training Epoch: 1/2, step 5606/23838 completed (loss: 0.470249742269516, acc: 0.8611111044883728)
[2025-02-16 11:39:13,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:13,848][root][INFO] - Training Epoch: 1/2, step 5607/23838 completed (loss: 0.27029427886009216, acc: 0.9074074029922485)
[2025-02-16 11:39:14,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:14,249][root][INFO] - Training Epoch: 1/2, step 5608/23838 completed (loss: 0.7899103164672852, acc: 0.761904776096344)
[2025-02-16 11:39:14,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:14,685][root][INFO] - Training Epoch: 1/2, step 5609/23838 completed (loss: 0.6621769666671753, acc: 0.8273381590843201)
[2025-02-16 11:39:14,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:15,122][root][INFO] - Training Epoch: 1/2, step 5610/23838 completed (loss: 0.6597306728363037, acc: 0.8082191944122314)
[2025-02-16 11:39:15,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:15,594][root][INFO] - Training Epoch: 1/2, step 5611/23838 completed (loss: 0.5454577803611755, acc: 0.8741722106933594)
[2025-02-16 11:39:15,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:16,033][root][INFO] - Training Epoch: 1/2, step 5612/23838 completed (loss: 0.4048464894294739, acc: 0.8992805480957031)
[2025-02-16 11:39:16,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:16,425][root][INFO] - Training Epoch: 1/2, step 5613/23838 completed (loss: 0.3837113678455353, acc: 0.8831169009208679)
[2025-02-16 11:39:16,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:16,890][root][INFO] - Training Epoch: 1/2, step 5614/23838 completed (loss: 0.5722075700759888, acc: 0.8296296000480652)
[2025-02-16 11:39:17,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:17,387][root][INFO] - Training Epoch: 1/2, step 5615/23838 completed (loss: 0.5931097269058228, acc: 0.800000011920929)
[2025-02-16 11:39:17,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:17,808][root][INFO] - Training Epoch: 1/2, step 5616/23838 completed (loss: 0.8602205514907837, acc: 0.8048780560493469)
[2025-02-16 11:39:18,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:18,286][root][INFO] - Training Epoch: 1/2, step 5617/23838 completed (loss: 0.8149093389511108, acc: 0.7596153616905212)
[2025-02-16 11:39:18,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:18,706][root][INFO] - Training Epoch: 1/2, step 5618/23838 completed (loss: 0.25399911403656006, acc: 0.9333333373069763)
[2025-02-16 11:39:18,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:19,141][root][INFO] - Training Epoch: 1/2, step 5619/23838 completed (loss: 0.8406879305839539, acc: 0.7848101258277893)
[2025-02-16 11:39:19,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:19,593][root][INFO] - Training Epoch: 1/2, step 5620/23838 completed (loss: 0.2941577136516571, acc: 0.8899999856948853)
[2025-02-16 11:39:19,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:20,012][root][INFO] - Training Epoch: 1/2, step 5621/23838 completed (loss: 0.44423165917396545, acc: 0.8735632300376892)
[2025-02-16 11:39:20,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:20,558][root][INFO] - Training Epoch: 1/2, step 5622/23838 completed (loss: 0.39679816365242004, acc: 0.8503937125205994)
[2025-02-16 11:39:20,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:20,918][root][INFO] - Training Epoch: 1/2, step 5623/23838 completed (loss: 0.41538065671920776, acc: 0.8805969953536987)
[2025-02-16 11:39:21,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:21,301][root][INFO] - Training Epoch: 1/2, step 5624/23838 completed (loss: 0.7419577240943909, acc: 0.797468364238739)
[2025-02-16 11:39:21,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:21,739][root][INFO] - Training Epoch: 1/2, step 5625/23838 completed (loss: 1.2542788982391357, acc: 0.7580645084381104)
[2025-02-16 11:39:21,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:22,178][root][INFO] - Training Epoch: 1/2, step 5626/23838 completed (loss: 0.703960120677948, acc: 0.7882353067398071)
[2025-02-16 11:39:22,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:22,623][root][INFO] - Training Epoch: 1/2, step 5627/23838 completed (loss: 0.6973204016685486, acc: 0.7735849022865295)
[2025-02-16 11:39:22,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:23,081][root][INFO] - Training Epoch: 1/2, step 5628/23838 completed (loss: 0.7367342710494995, acc: 0.7413793206214905)
[2025-02-16 11:39:23,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:23,518][root][INFO] - Training Epoch: 1/2, step 5629/23838 completed (loss: 0.6383545398712158, acc: 0.8253968358039856)
[2025-02-16 11:39:23,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:23,934][root][INFO] - Training Epoch: 1/2, step 5630/23838 completed (loss: 0.3813798129558563, acc: 0.9200000166893005)
[2025-02-16 11:39:24,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:24,324][root][INFO] - Training Epoch: 1/2, step 5631/23838 completed (loss: 0.847597062587738, acc: 0.782608687877655)
[2025-02-16 11:39:24,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:24,748][root][INFO] - Training Epoch: 1/2, step 5632/23838 completed (loss: 0.40775564312934875, acc: 0.8595041036605835)
[2025-02-16 11:39:24,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:25,145][root][INFO] - Training Epoch: 1/2, step 5633/23838 completed (loss: 0.4766124188899994, acc: 0.824999988079071)
[2025-02-16 11:39:25,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:25,565][root][INFO] - Training Epoch: 1/2, step 5634/23838 completed (loss: 0.7996231913566589, acc: 0.7397260069847107)
[2025-02-16 11:39:25,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:25,970][root][INFO] - Training Epoch: 1/2, step 5635/23838 completed (loss: 0.9443336129188538, acc: 0.7543859481811523)
[2025-02-16 11:39:26,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:26,334][root][INFO] - Training Epoch: 1/2, step 5636/23838 completed (loss: 0.5198127031326294, acc: 0.8730158805847168)
[2025-02-16 11:39:26,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:26,722][root][INFO] - Training Epoch: 1/2, step 5637/23838 completed (loss: 0.39274361729621887, acc: 0.8992248177528381)
[2025-02-16 11:39:26,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:27,076][root][INFO] - Training Epoch: 1/2, step 5638/23838 completed (loss: 0.6268475651741028, acc: 0.8658536672592163)
[2025-02-16 11:39:27,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:27,443][root][INFO] - Training Epoch: 1/2, step 5639/23838 completed (loss: 1.2552275657653809, acc: 0.7014925479888916)
[2025-02-16 11:39:27,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:27,931][root][INFO] - Training Epoch: 1/2, step 5640/23838 completed (loss: 0.5548721551895142, acc: 0.8333333134651184)
[2025-02-16 11:39:28,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:28,314][root][INFO] - Training Epoch: 1/2, step 5641/23838 completed (loss: 0.4980650544166565, acc: 0.8679245114326477)
[2025-02-16 11:39:28,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:28,759][root][INFO] - Training Epoch: 1/2, step 5642/23838 completed (loss: 0.5351336598396301, acc: 0.8809523582458496)
[2025-02-16 11:39:28,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:29,198][root][INFO] - Training Epoch: 1/2, step 5643/23838 completed (loss: 0.784426748752594, acc: 0.7570621371269226)
[2025-02-16 11:39:29,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:29,599][root][INFO] - Training Epoch: 1/2, step 5644/23838 completed (loss: 0.6630761027336121, acc: 0.8202247023582458)
[2025-02-16 11:39:29,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:29,997][root][INFO] - Training Epoch: 1/2, step 5645/23838 completed (loss: 0.6934627890586853, acc: 0.7731092572212219)
[2025-02-16 11:39:30,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:30,431][root][INFO] - Training Epoch: 1/2, step 5646/23838 completed (loss: 0.6493372321128845, acc: 0.75)
[2025-02-16 11:39:30,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:30,925][root][INFO] - Training Epoch: 1/2, step 5647/23838 completed (loss: 0.6054500937461853, acc: 0.800000011920929)
[2025-02-16 11:39:31,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:31,325][root][INFO] - Training Epoch: 1/2, step 5648/23838 completed (loss: 0.649507462978363, acc: 0.8358209133148193)
[2025-02-16 11:39:31,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:31,726][root][INFO] - Training Epoch: 1/2, step 5649/23838 completed (loss: 0.7298634648323059, acc: 0.8026315569877625)
[2025-02-16 11:39:31,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:32,144][root][INFO] - Training Epoch: 1/2, step 5650/23838 completed (loss: 0.7587373852729797, acc: 0.769911527633667)
[2025-02-16 11:39:32,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:32,528][root][INFO] - Training Epoch: 1/2, step 5651/23838 completed (loss: 0.38937515020370483, acc: 0.8999999761581421)
[2025-02-16 11:39:32,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:32,987][root][INFO] - Training Epoch: 1/2, step 5652/23838 completed (loss: 0.829241931438446, acc: 0.7279999852180481)
[2025-02-16 11:39:33,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:33,443][root][INFO] - Training Epoch: 1/2, step 5653/23838 completed (loss: 0.6915188431739807, acc: 0.8467742204666138)
[2025-02-16 11:39:33,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:33,837][root][INFO] - Training Epoch: 1/2, step 5654/23838 completed (loss: 0.2859620451927185, acc: 0.9108911156654358)
[2025-02-16 11:39:33,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:34,207][root][INFO] - Training Epoch: 1/2, step 5655/23838 completed (loss: 0.4059833288192749, acc: 0.8795180916786194)
[2025-02-16 11:39:34,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:34,634][root][INFO] - Training Epoch: 1/2, step 5656/23838 completed (loss: 0.8156279921531677, acc: 0.7631579041481018)
[2025-02-16 11:39:34,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:35,058][root][INFO] - Training Epoch: 1/2, step 5657/23838 completed (loss: 0.5507597923278809, acc: 0.843137264251709)
[2025-02-16 11:39:35,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:35,440][root][INFO] - Training Epoch: 1/2, step 5658/23838 completed (loss: 0.8814929723739624, acc: 0.7126436829566956)
[2025-02-16 11:39:35,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:35,896][root][INFO] - Training Epoch: 1/2, step 5659/23838 completed (loss: 0.7998651266098022, acc: 0.7356321811676025)
[2025-02-16 11:39:36,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:36,329][root][INFO] - Training Epoch: 1/2, step 5660/23838 completed (loss: 0.503906786441803, acc: 0.8374999761581421)
[2025-02-16 11:39:36,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:36,712][root][INFO] - Training Epoch: 1/2, step 5661/23838 completed (loss: 0.648327648639679, acc: 0.7790697813034058)
[2025-02-16 11:39:36,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:37,084][root][INFO] - Training Epoch: 1/2, step 5662/23838 completed (loss: 0.3810056447982788, acc: 0.8690476417541504)
[2025-02-16 11:39:37,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:37,503][root][INFO] - Training Epoch: 1/2, step 5663/23838 completed (loss: 0.4756689965724945, acc: 0.8571428656578064)
[2025-02-16 11:39:37,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:37,970][root][INFO] - Training Epoch: 1/2, step 5664/23838 completed (loss: 0.5104029774665833, acc: 0.8582677245140076)
[2025-02-16 11:39:38,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:38,363][root][INFO] - Training Epoch: 1/2, step 5665/23838 completed (loss: 0.5656371116638184, acc: 0.797468364238739)
[2025-02-16 11:39:38,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:38,866][root][INFO] - Training Epoch: 1/2, step 5666/23838 completed (loss: 0.512200653553009, acc: 0.8283582329750061)
[2025-02-16 11:39:39,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:39,229][root][INFO] - Training Epoch: 1/2, step 5667/23838 completed (loss: 0.7078905701637268, acc: 0.8095238208770752)
[2025-02-16 11:39:39,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:39,651][root][INFO] - Training Epoch: 1/2, step 5668/23838 completed (loss: 0.45621201395988464, acc: 0.8658536672592163)
[2025-02-16 11:39:39,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:40,076][root][INFO] - Training Epoch: 1/2, step 5669/23838 completed (loss: 0.35615071654319763, acc: 0.9054054021835327)
[2025-02-16 11:39:40,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:40,519][root][INFO] - Training Epoch: 1/2, step 5670/23838 completed (loss: 0.8939132690429688, acc: 0.7121211886405945)
[2025-02-16 11:39:40,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:40,927][root][INFO] - Training Epoch: 1/2, step 5671/23838 completed (loss: 0.5067281126976013, acc: 0.8472222089767456)
[2025-02-16 11:39:41,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:41,360][root][INFO] - Training Epoch: 1/2, step 5672/23838 completed (loss: 0.6334115862846375, acc: 0.7857142686843872)
[2025-02-16 11:39:41,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:41,807][root][INFO] - Training Epoch: 1/2, step 5673/23838 completed (loss: 0.6020450592041016, acc: 0.8130841255187988)
[2025-02-16 11:39:42,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:42,281][root][INFO] - Training Epoch: 1/2, step 5674/23838 completed (loss: 0.5119730830192566, acc: 0.8484848737716675)
[2025-02-16 11:39:42,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:42,736][root][INFO] - Training Epoch: 1/2, step 5675/23838 completed (loss: 0.4875919818878174, acc: 0.8571428656578064)
[2025-02-16 11:39:42,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:43,217][root][INFO] - Training Epoch: 1/2, step 5676/23838 completed (loss: 0.37812185287475586, acc: 0.9344262480735779)
[2025-02-16 11:39:43,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:43,695][root][INFO] - Training Epoch: 1/2, step 5677/23838 completed (loss: 0.6625376343727112, acc: 0.8263888955116272)
[2025-02-16 11:39:43,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:44,234][root][INFO] - Training Epoch: 1/2, step 5678/23838 completed (loss: 0.39345720410346985, acc: 0.89570552110672)
[2025-02-16 11:39:44,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:44,668][root][INFO] - Training Epoch: 1/2, step 5679/23838 completed (loss: 0.7546324729919434, acc: 0.7611940503120422)
[2025-02-16 11:39:44,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:45,169][root][INFO] - Training Epoch: 1/2, step 5680/23838 completed (loss: 0.6660975217819214, acc: 0.8859649300575256)
[2025-02-16 11:39:45,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:45,625][root][INFO] - Training Epoch: 1/2, step 5681/23838 completed (loss: 0.7618823647499084, acc: 0.7538461685180664)
[2025-02-16 11:39:45,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:46,052][root][INFO] - Training Epoch: 1/2, step 5682/23838 completed (loss: 0.2428942620754242, acc: 0.9166666865348816)
[2025-02-16 11:39:46,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:46,438][root][INFO] - Training Epoch: 1/2, step 5683/23838 completed (loss: 0.3736492097377777, acc: 0.8839285969734192)
[2025-02-16 11:39:46,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:46,859][root][INFO] - Training Epoch: 1/2, step 5684/23838 completed (loss: 0.23298408091068268, acc: 0.915032684803009)
[2025-02-16 11:39:47,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:47,278][root][INFO] - Training Epoch: 1/2, step 5685/23838 completed (loss: 0.7211524844169617, acc: 0.7931034564971924)
[2025-02-16 11:39:47,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:47,665][root][INFO] - Training Epoch: 1/2, step 5686/23838 completed (loss: 0.7663614153862, acc: 0.7301587462425232)
[2025-02-16 11:39:47,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:48,092][root][INFO] - Training Epoch: 1/2, step 5687/23838 completed (loss: 0.20358943939208984, acc: 0.9407407641410828)
[2025-02-16 11:39:48,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:48,456][root][INFO] - Training Epoch: 1/2, step 5688/23838 completed (loss: 0.5740746855735779, acc: 0.8243243098258972)
[2025-02-16 11:39:48,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:48,913][root][INFO] - Training Epoch: 1/2, step 5689/23838 completed (loss: 0.49823394417762756, acc: 0.887499988079071)
[2025-02-16 11:39:49,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:49,390][root][INFO] - Training Epoch: 1/2, step 5690/23838 completed (loss: 0.6236308813095093, acc: 0.8275862336158752)
[2025-02-16 11:39:49,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:49,797][root][INFO] - Training Epoch: 1/2, step 5691/23838 completed (loss: 0.5242705941200256, acc: 0.8842975497245789)
[2025-02-16 11:39:49,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:50,179][root][INFO] - Training Epoch: 1/2, step 5692/23838 completed (loss: 0.5762425661087036, acc: 0.8333333134651184)
[2025-02-16 11:39:50,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:50,608][root][INFO] - Training Epoch: 1/2, step 5693/23838 completed (loss: 0.38424360752105713, acc: 0.8877550959587097)
[2025-02-16 11:39:50,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:51,071][root][INFO] - Training Epoch: 1/2, step 5694/23838 completed (loss: 0.656670868396759, acc: 0.8110235929489136)
[2025-02-16 11:39:51,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:51,494][root][INFO] - Training Epoch: 1/2, step 5695/23838 completed (loss: 1.100354552268982, acc: 0.7246376872062683)
[2025-02-16 11:39:51,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:51,880][root][INFO] - Training Epoch: 1/2, step 5696/23838 completed (loss: 0.9702667593955994, acc: 0.7323943376541138)
[2025-02-16 11:39:52,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:52,258][root][INFO] - Training Epoch: 1/2, step 5697/23838 completed (loss: 0.5598360300064087, acc: 0.8571428656578064)
[2025-02-16 11:39:52,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:52,651][root][INFO] - Training Epoch: 1/2, step 5698/23838 completed (loss: 0.4344038963317871, acc: 0.8881118893623352)
[2025-02-16 11:39:52,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:53,082][root][INFO] - Training Epoch: 1/2, step 5699/23838 completed (loss: 0.7634953260421753, acc: 0.748031497001648)
[2025-02-16 11:39:53,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:53,475][root][INFO] - Training Epoch: 1/2, step 5700/23838 completed (loss: 0.1791856586933136, acc: 0.9795918464660645)
[2025-02-16 11:39:53,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:53,899][root][INFO] - Training Epoch: 1/2, step 5701/23838 completed (loss: 0.6661593317985535, acc: 0.8172042965888977)
[2025-02-16 11:39:54,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:54,273][root][INFO] - Training Epoch: 1/2, step 5702/23838 completed (loss: 0.4573218524456024, acc: 0.8928571343421936)
[2025-02-16 11:39:54,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:54,722][root][INFO] - Training Epoch: 1/2, step 5703/23838 completed (loss: 0.422148734331131, acc: 0.84375)
[2025-02-16 11:39:54,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:55,153][root][INFO] - Training Epoch: 1/2, step 5704/23838 completed (loss: 0.4385533928871155, acc: 0.8901734352111816)
[2025-02-16 11:39:55,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:55,592][root][INFO] - Training Epoch: 1/2, step 5705/23838 completed (loss: 0.7365490794181824, acc: 0.7714285850524902)
[2025-02-16 11:39:55,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:56,005][root][INFO] - Training Epoch: 1/2, step 5706/23838 completed (loss: 0.5233896374702454, acc: 0.84375)
[2025-02-16 11:39:56,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:56,398][root][INFO] - Training Epoch: 1/2, step 5707/23838 completed (loss: 1.0940049886703491, acc: 0.6842105388641357)
[2025-02-16 11:39:56,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:56,834][root][INFO] - Training Epoch: 1/2, step 5708/23838 completed (loss: 1.097679853439331, acc: 0.7169811129570007)
[2025-02-16 11:39:56,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:57,219][root][INFO] - Training Epoch: 1/2, step 5709/23838 completed (loss: 0.498522013425827, acc: 0.8615384697914124)
[2025-02-16 11:39:57,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:57,621][root][INFO] - Training Epoch: 1/2, step 5710/23838 completed (loss: 0.6710640788078308, acc: 0.8214285969734192)
[2025-02-16 11:39:57,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:58,058][root][INFO] - Training Epoch: 1/2, step 5711/23838 completed (loss: 0.3943932354450226, acc: 0.8823529481887817)
[2025-02-16 11:39:58,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:58,515][root][INFO] - Training Epoch: 1/2, step 5712/23838 completed (loss: 1.1849256753921509, acc: 0.7200000286102295)
[2025-02-16 11:39:58,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:58,879][root][INFO] - Training Epoch: 1/2, step 5713/23838 completed (loss: 0.28934845328330994, acc: 0.9021739363670349)
[2025-02-16 11:39:59,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:59,247][root][INFO] - Training Epoch: 1/2, step 5714/23838 completed (loss: 0.41466841101646423, acc: 0.9135802388191223)
[2025-02-16 11:39:59,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:39:59,637][root][INFO] - Training Epoch: 1/2, step 5715/23838 completed (loss: 0.49243271350860596, acc: 0.8799999952316284)
[2025-02-16 11:39:59,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:00,091][root][INFO] - Training Epoch: 1/2, step 5716/23838 completed (loss: 0.4907566010951996, acc: 0.8508771657943726)
[2025-02-16 11:40:00,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:00,534][root][INFO] - Training Epoch: 1/2, step 5717/23838 completed (loss: 0.8493232131004333, acc: 0.78125)
[2025-02-16 11:40:00,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:00,964][root][INFO] - Training Epoch: 1/2, step 5718/23838 completed (loss: 0.30391988158226013, acc: 0.9300000071525574)
[2025-02-16 11:40:01,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:01,412][root][INFO] - Training Epoch: 1/2, step 5719/23838 completed (loss: 0.6529998183250427, acc: 0.8494623899459839)
[2025-02-16 11:40:01,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:01,828][root][INFO] - Training Epoch: 1/2, step 5720/23838 completed (loss: 0.25751492381095886, acc: 0.8888888955116272)
[2025-02-16 11:40:02,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:02,235][root][INFO] - Training Epoch: 1/2, step 5721/23838 completed (loss: 0.5648277997970581, acc: 0.8474576473236084)
[2025-02-16 11:40:02,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:02,646][root][INFO] - Training Epoch: 1/2, step 5722/23838 completed (loss: 0.76537024974823, acc: 0.7272727489471436)
[2025-02-16 11:40:02,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:03,091][root][INFO] - Training Epoch: 1/2, step 5723/23838 completed (loss: 0.7618597149848938, acc: 0.8152173757553101)
[2025-02-16 11:40:03,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:03,512][root][INFO] - Training Epoch: 1/2, step 5724/23838 completed (loss: 0.4274573028087616, acc: 0.8620689511299133)
[2025-02-16 11:40:03,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:03,974][root][INFO] - Training Epoch: 1/2, step 5725/23838 completed (loss: 0.41026780009269714, acc: 0.8719512224197388)
[2025-02-16 11:40:04,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:04,423][root][INFO] - Training Epoch: 1/2, step 5726/23838 completed (loss: 0.3964337706565857, acc: 0.8703703880310059)
[2025-02-16 11:40:04,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:04,818][root][INFO] - Training Epoch: 1/2, step 5727/23838 completed (loss: 0.707705557346344, acc: 0.8247422575950623)
[2025-02-16 11:40:04,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:05,235][root][INFO] - Training Epoch: 1/2, step 5728/23838 completed (loss: 0.8648458123207092, acc: 0.759036123752594)
[2025-02-16 11:40:05,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:05,635][root][INFO] - Training Epoch: 1/2, step 5729/23838 completed (loss: 0.4334047734737396, acc: 0.9027777910232544)
[2025-02-16 11:40:05,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:06,116][root][INFO] - Training Epoch: 1/2, step 5730/23838 completed (loss: 0.81339031457901, acc: 0.7166666388511658)
[2025-02-16 11:40:06,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:06,528][root][INFO] - Training Epoch: 1/2, step 5731/23838 completed (loss: 0.4072210490703583, acc: 0.8969072103500366)
[2025-02-16 11:40:06,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:06,967][root][INFO] - Training Epoch: 1/2, step 5732/23838 completed (loss: 0.7806257009506226, acc: 0.7699999809265137)
[2025-02-16 11:40:07,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:07,359][root][INFO] - Training Epoch: 1/2, step 5733/23838 completed (loss: 0.37555238604545593, acc: 0.8703703880310059)
[2025-02-16 11:40:07,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:07,751][root][INFO] - Training Epoch: 1/2, step 5734/23838 completed (loss: 0.4985855221748352, acc: 0.8659793734550476)
[2025-02-16 11:40:07,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:08,175][root][INFO] - Training Epoch: 1/2, step 5735/23838 completed (loss: 0.3785112500190735, acc: 0.8974359035491943)
[2025-02-16 11:40:08,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:08,560][root][INFO] - Training Epoch: 1/2, step 5736/23838 completed (loss: 0.8531431555747986, acc: 0.7021276354789734)
[2025-02-16 11:40:08,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:08,939][root][INFO] - Training Epoch: 1/2, step 5737/23838 completed (loss: 0.5833587050437927, acc: 0.8285714387893677)
[2025-02-16 11:40:09,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:09,408][root][INFO] - Training Epoch: 1/2, step 5738/23838 completed (loss: 0.6089848875999451, acc: 0.8089887499809265)
[2025-02-16 11:40:09,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:09,858][root][INFO] - Training Epoch: 1/2, step 5739/23838 completed (loss: 0.40612149238586426, acc: 0.8823529481887817)
[2025-02-16 11:40:10,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:10,279][root][INFO] - Training Epoch: 1/2, step 5740/23838 completed (loss: 0.4392504096031189, acc: 0.8913043737411499)
[2025-02-16 11:40:10,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:10,657][root][INFO] - Training Epoch: 1/2, step 5741/23838 completed (loss: 0.3062410354614258, acc: 0.938144326210022)
[2025-02-16 11:40:10,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:11,010][root][INFO] - Training Epoch: 1/2, step 5742/23838 completed (loss: 1.0228219032287598, acc: 0.6823529601097107)
[2025-02-16 11:40:11,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:11,426][root][INFO] - Training Epoch: 1/2, step 5743/23838 completed (loss: 0.8213207721710205, acc: 0.8181818127632141)
[2025-02-16 11:40:11,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:11,829][root][INFO] - Training Epoch: 1/2, step 5744/23838 completed (loss: 0.39043688774108887, acc: 0.8735632300376892)
[2025-02-16 11:40:11,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:12,188][root][INFO] - Training Epoch: 1/2, step 5745/23838 completed (loss: 0.9064581394195557, acc: 0.7121211886405945)
[2025-02-16 11:40:12,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:12,541][root][INFO] - Training Epoch: 1/2, step 5746/23838 completed (loss: 0.32739463448524475, acc: 0.9027777910232544)
[2025-02-16 11:40:12,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:12,958][root][INFO] - Training Epoch: 1/2, step 5747/23838 completed (loss: 0.4704957604408264, acc: 0.8970588445663452)
[2025-02-16 11:40:13,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:13,400][root][INFO] - Training Epoch: 1/2, step 5748/23838 completed (loss: 0.7446013689041138, acc: 0.746835470199585)
[2025-02-16 11:40:13,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:13,856][root][INFO] - Training Epoch: 1/2, step 5749/23838 completed (loss: 0.36383044719696045, acc: 0.9102563858032227)
[2025-02-16 11:40:14,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:14,318][root][INFO] - Training Epoch: 1/2, step 5750/23838 completed (loss: 0.23081053793430328, acc: 0.9285714030265808)
[2025-02-16 11:40:14,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:14,750][root][INFO] - Training Epoch: 1/2, step 5751/23838 completed (loss: 0.41525378823280334, acc: 0.901098906993866)
[2025-02-16 11:40:14,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:15,204][root][INFO] - Training Epoch: 1/2, step 5752/23838 completed (loss: 0.279521107673645, acc: 0.9039999842643738)
[2025-02-16 11:40:15,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:15,671][root][INFO] - Training Epoch: 1/2, step 5753/23838 completed (loss: 0.2779863178730011, acc: 0.9200000166893005)
[2025-02-16 11:40:15,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:16,119][root][INFO] - Training Epoch: 1/2, step 5754/23838 completed (loss: 0.5504686832427979, acc: 0.8641975522041321)
[2025-02-16 11:40:16,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:16,552][root][INFO] - Training Epoch: 1/2, step 5755/23838 completed (loss: 0.4426819682121277, acc: 0.8815789222717285)
[2025-02-16 11:40:16,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:16,991][root][INFO] - Training Epoch: 1/2, step 5756/23838 completed (loss: 0.545436441898346, acc: 0.8591549396514893)
[2025-02-16 11:40:17,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:17,433][root][INFO] - Training Epoch: 1/2, step 5757/23838 completed (loss: 0.38800978660583496, acc: 0.9085366129875183)
[2025-02-16 11:40:17,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:17,820][root][INFO] - Training Epoch: 1/2, step 5758/23838 completed (loss: 0.44893500208854675, acc: 0.8552631735801697)
[2025-02-16 11:40:17,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:18,184][root][INFO] - Training Epoch: 1/2, step 5759/23838 completed (loss: 0.7058407068252563, acc: 0.8082191944122314)
[2025-02-16 11:40:18,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:18,655][root][INFO] - Training Epoch: 1/2, step 5760/23838 completed (loss: 0.4908340573310852, acc: 0.8550724387168884)
[2025-02-16 11:40:18,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:19,104][root][INFO] - Training Epoch: 1/2, step 5761/23838 completed (loss: 0.318246454000473, acc: 0.8961039185523987)
[2025-02-16 11:40:19,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:19,554][root][INFO] - Training Epoch: 1/2, step 5762/23838 completed (loss: 0.8856742978096008, acc: 0.747474730014801)
[2025-02-16 11:40:19,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:19,959][root][INFO] - Training Epoch: 1/2, step 5763/23838 completed (loss: 0.40359753370285034, acc: 0.8764045238494873)
[2025-02-16 11:40:20,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:20,340][root][INFO] - Training Epoch: 1/2, step 5764/23838 completed (loss: 0.38678744435310364, acc: 0.9052631855010986)
[2025-02-16 11:40:20,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:20,754][root][INFO] - Training Epoch: 1/2, step 5765/23838 completed (loss: 0.9044440388679504, acc: 0.800000011920929)
[2025-02-16 11:40:20,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:21,165][root][INFO] - Training Epoch: 1/2, step 5766/23838 completed (loss: 0.39705413579940796, acc: 0.8666666746139526)
[2025-02-16 11:40:21,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:21,588][root][INFO] - Training Epoch: 1/2, step 5767/23838 completed (loss: 0.6020781397819519, acc: 0.8571428656578064)
[2025-02-16 11:40:21,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:22,006][root][INFO] - Training Epoch: 1/2, step 5768/23838 completed (loss: 0.4537806510925293, acc: 0.8888888955116272)
[2025-02-16 11:40:22,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:22,388][root][INFO] - Training Epoch: 1/2, step 5769/23838 completed (loss: 0.8871729373931885, acc: 0.7083333134651184)
[2025-02-16 11:40:22,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:22,787][root][INFO] - Training Epoch: 1/2, step 5770/23838 completed (loss: 0.21529091894626617, acc: 0.9166666865348816)
[2025-02-16 11:40:22,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:23,197][root][INFO] - Training Epoch: 1/2, step 5771/23838 completed (loss: 0.7125809788703918, acc: 0.8125)
[2025-02-16 11:40:23,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:23,614][root][INFO] - Training Epoch: 1/2, step 5772/23838 completed (loss: 0.39955848455429077, acc: 0.8561643958091736)
[2025-02-16 11:40:23,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:24,066][root][INFO] - Training Epoch: 1/2, step 5773/23838 completed (loss: 0.4875974655151367, acc: 0.8670520186424255)
[2025-02-16 11:40:24,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:24,485][root][INFO] - Training Epoch: 1/2, step 5774/23838 completed (loss: 0.4269201457500458, acc: 0.8666666746139526)
[2025-02-16 11:40:24,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:24,939][root][INFO] - Training Epoch: 1/2, step 5775/23838 completed (loss: 0.9832460284233093, acc: 0.71875)
[2025-02-16 11:40:25,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:25,372][root][INFO] - Training Epoch: 1/2, step 5776/23838 completed (loss: 0.38021451234817505, acc: 0.9083969593048096)
[2025-02-16 11:40:25,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:25,816][root][INFO] - Training Epoch: 1/2, step 5777/23838 completed (loss: 0.3707093298435211, acc: 0.9021739363670349)
[2025-02-16 11:40:25,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:26,183][root][INFO] - Training Epoch: 1/2, step 5778/23838 completed (loss: 0.3891592025756836, acc: 0.9032257795333862)
[2025-02-16 11:40:26,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:26,621][root][INFO] - Training Epoch: 1/2, step 5779/23838 completed (loss: 0.613861620426178, acc: 0.8272727131843567)
[2025-02-16 11:40:26,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:27,054][root][INFO] - Training Epoch: 1/2, step 5780/23838 completed (loss: 0.5629957914352417, acc: 0.8382353186607361)
[2025-02-16 11:40:27,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:27,502][root][INFO] - Training Epoch: 1/2, step 5781/23838 completed (loss: 0.6220232844352722, acc: 0.8442623019218445)
[2025-02-16 11:40:27,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:27,929][root][INFO] - Training Epoch: 1/2, step 5782/23838 completed (loss: 0.81136554479599, acc: 0.8135592937469482)
[2025-02-16 11:40:28,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:28,409][root][INFO] - Training Epoch: 1/2, step 5783/23838 completed (loss: 0.3596462607383728, acc: 0.875)
[2025-02-16 11:40:28,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:28,813][root][INFO] - Training Epoch: 1/2, step 5784/23838 completed (loss: 0.5080881714820862, acc: 0.8507462739944458)
[2025-02-16 11:40:29,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:29,241][root][INFO] - Training Epoch: 1/2, step 5785/23838 completed (loss: 0.4129328727722168, acc: 0.8783783912658691)
[2025-02-16 11:40:29,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:29,667][root][INFO] - Training Epoch: 1/2, step 5786/23838 completed (loss: 0.30327311158180237, acc: 0.9333333373069763)
[2025-02-16 11:40:29,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:30,119][root][INFO] - Training Epoch: 1/2, step 5787/23838 completed (loss: 0.4408549666404724, acc: 0.8817204236984253)
[2025-02-16 11:40:30,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:30,556][root][INFO] - Training Epoch: 1/2, step 5788/23838 completed (loss: 0.7300224304199219, acc: 0.8275862336158752)
[2025-02-16 11:40:30,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:30,997][root][INFO] - Training Epoch: 1/2, step 5789/23838 completed (loss: 0.30871081352233887, acc: 0.9097744226455688)
[2025-02-16 11:40:31,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:31,444][root][INFO] - Training Epoch: 1/2, step 5790/23838 completed (loss: 0.3332158327102661, acc: 0.885496199131012)
[2025-02-16 11:40:31,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:31,859][root][INFO] - Training Epoch: 1/2, step 5791/23838 completed (loss: 0.4106017053127289, acc: 0.8911564350128174)
[2025-02-16 11:40:32,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:32,214][root][INFO] - Training Epoch: 1/2, step 5792/23838 completed (loss: 0.3945661187171936, acc: 0.8940397500991821)
[2025-02-16 11:40:32,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:32,598][root][INFO] - Training Epoch: 1/2, step 5793/23838 completed (loss: 0.4369373619556427, acc: 0.8715596199035645)
[2025-02-16 11:40:32,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:32,982][root][INFO] - Training Epoch: 1/2, step 5794/23838 completed (loss: 0.49638622999191284, acc: 0.8500000238418579)
[2025-02-16 11:40:33,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:33,412][root][INFO] - Training Epoch: 1/2, step 5795/23838 completed (loss: 0.6974937319755554, acc: 0.8196721076965332)
[2025-02-16 11:40:33,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:33,804][root][INFO] - Training Epoch: 1/2, step 5796/23838 completed (loss: 0.4570494294166565, acc: 0.8404255509376526)
[2025-02-16 11:40:33,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:34,215][root][INFO] - Training Epoch: 1/2, step 5797/23838 completed (loss: 0.6035193800926208, acc: 0.7978723645210266)
[2025-02-16 11:40:34,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:34,686][root][INFO] - Training Epoch: 1/2, step 5798/23838 completed (loss: 0.6124793291091919, acc: 0.8058252334594727)
[2025-02-16 11:40:34,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:35,085][root][INFO] - Training Epoch: 1/2, step 5799/23838 completed (loss: 0.3739730417728424, acc: 0.8854166865348816)
[2025-02-16 11:40:35,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:35,469][root][INFO] - Training Epoch: 1/2, step 5800/23838 completed (loss: 0.562197744846344, acc: 0.8582677245140076)
[2025-02-16 11:40:35,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:35,821][root][INFO] - Training Epoch: 1/2, step 5801/23838 completed (loss: 0.6065675616264343, acc: 0.8450704216957092)
[2025-02-16 11:40:36,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:36,256][root][INFO] - Training Epoch: 1/2, step 5802/23838 completed (loss: 0.39020827412605286, acc: 0.9039999842643738)
[2025-02-16 11:40:36,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:36,679][root][INFO] - Training Epoch: 1/2, step 5803/23838 completed (loss: 0.5791269540786743, acc: 0.8315789699554443)
[2025-02-16 11:40:36,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:37,055][root][INFO] - Training Epoch: 1/2, step 5804/23838 completed (loss: 0.20002108812332153, acc: 0.9800000190734863)
[2025-02-16 11:40:37,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:37,471][root][INFO] - Training Epoch: 1/2, step 5805/23838 completed (loss: 0.5417929291725159, acc: 0.8333333134651184)
[2025-02-16 11:40:37,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:37,914][root][INFO] - Training Epoch: 1/2, step 5806/23838 completed (loss: 0.5739505887031555, acc: 0.8292682766914368)
[2025-02-16 11:40:38,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:38,325][root][INFO] - Training Epoch: 1/2, step 5807/23838 completed (loss: 0.6585249304771423, acc: 0.808080792427063)
[2025-02-16 11:40:38,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:38,773][root][INFO] - Training Epoch: 1/2, step 5808/23838 completed (loss: 0.5844440460205078, acc: 0.782608687877655)
[2025-02-16 11:40:38,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:39,176][root][INFO] - Training Epoch: 1/2, step 5809/23838 completed (loss: 0.5931956171989441, acc: 0.8493150472640991)
[2025-02-16 11:40:39,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:39,628][root][INFO] - Training Epoch: 1/2, step 5810/23838 completed (loss: 0.4670020639896393, acc: 0.8823529481887817)
[2025-02-16 11:40:39,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:40,020][root][INFO] - Training Epoch: 1/2, step 5811/23838 completed (loss: 0.2956824004650116, acc: 0.926174521446228)
[2025-02-16 11:40:40,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:40,432][root][INFO] - Training Epoch: 1/2, step 5812/23838 completed (loss: 0.3320145010948181, acc: 0.9126213788986206)
[2025-02-16 11:40:40,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:40,837][root][INFO] - Training Epoch: 1/2, step 5813/23838 completed (loss: 0.6854720115661621, acc: 0.8225806355476379)
[2025-02-16 11:40:40,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:41,215][root][INFO] - Training Epoch: 1/2, step 5814/23838 completed (loss: 0.7840904593467712, acc: 0.7692307829856873)
[2025-02-16 11:40:41,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:41,635][root][INFO] - Training Epoch: 1/2, step 5815/23838 completed (loss: 0.5549312829971313, acc: 0.8607594966888428)
[2025-02-16 11:40:41,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:42,085][root][INFO] - Training Epoch: 1/2, step 5816/23838 completed (loss: 0.5088812112808228, acc: 0.859649121761322)
[2025-02-16 11:40:42,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:42,526][root][INFO] - Training Epoch: 1/2, step 5817/23838 completed (loss: 0.9096627235412598, acc: 0.8089887499809265)
[2025-02-16 11:40:42,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:42,916][root][INFO] - Training Epoch: 1/2, step 5818/23838 completed (loss: 0.2999541461467743, acc: 0.930232584476471)
[2025-02-16 11:40:43,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:43,351][root][INFO] - Training Epoch: 1/2, step 5819/23838 completed (loss: 0.5359099507331848, acc: 0.8965517282485962)
[2025-02-16 11:40:43,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:43,864][root][INFO] - Training Epoch: 1/2, step 5820/23838 completed (loss: 0.5210944414138794, acc: 0.8365384340286255)
[2025-02-16 11:40:44,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:44,291][root][INFO] - Training Epoch: 1/2, step 5821/23838 completed (loss: 0.3939026892185211, acc: 0.898876428604126)
[2025-02-16 11:40:44,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:44,728][root][INFO] - Training Epoch: 1/2, step 5822/23838 completed (loss: 0.3212282955646515, acc: 0.8823529481887817)
[2025-02-16 11:40:44,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:45,098][root][INFO] - Training Epoch: 1/2, step 5823/23838 completed (loss: 0.3369934558868408, acc: 0.8903225660324097)
[2025-02-16 11:40:45,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:45,561][root][INFO] - Training Epoch: 1/2, step 5824/23838 completed (loss: 0.8185208439826965, acc: 0.800000011920929)
[2025-02-16 11:40:45,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:45,977][root][INFO] - Training Epoch: 1/2, step 5825/23838 completed (loss: 0.8098495006561279, acc: 0.7777777910232544)
[2025-02-16 11:40:46,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:46,394][root][INFO] - Training Epoch: 1/2, step 5826/23838 completed (loss: 0.7536186575889587, acc: 0.800000011920929)
[2025-02-16 11:40:46,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:46,850][root][INFO] - Training Epoch: 1/2, step 5827/23838 completed (loss: 0.3492555320262909, acc: 0.9375)
[2025-02-16 11:40:47,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:47,299][root][INFO] - Training Epoch: 1/2, step 5828/23838 completed (loss: 0.534048318862915, acc: 0.8461538553237915)
[2025-02-16 11:40:47,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:47,714][root][INFO] - Training Epoch: 1/2, step 5829/23838 completed (loss: 1.2856805324554443, acc: 0.5925925970077515)
[2025-02-16 11:40:47,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:48,175][root][INFO] - Training Epoch: 1/2, step 5830/23838 completed (loss: 0.5494641661643982, acc: 0.8999999761581421)
[2025-02-16 11:40:48,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:48,632][root][INFO] - Training Epoch: 1/2, step 5831/23838 completed (loss: 0.33297619223594666, acc: 0.8999999761581421)
[2025-02-16 11:40:48,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:49,033][root][INFO] - Training Epoch: 1/2, step 5832/23838 completed (loss: 0.3914388418197632, acc: 0.8958333134651184)
[2025-02-16 11:40:49,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:49,457][root][INFO] - Training Epoch: 1/2, step 5833/23838 completed (loss: 0.46802523732185364, acc: 0.8846153616905212)
[2025-02-16 11:40:49,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:49,888][root][INFO] - Training Epoch: 1/2, step 5834/23838 completed (loss: 0.5129892826080322, acc: 0.8600000143051147)
[2025-02-16 11:40:50,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:50,280][root][INFO] - Training Epoch: 1/2, step 5835/23838 completed (loss: 0.3975174129009247, acc: 0.9032257795333862)
[2025-02-16 11:40:50,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:50,642][root][INFO] - Training Epoch: 1/2, step 5836/23838 completed (loss: 0.3398860692977905, acc: 0.9428571462631226)
[2025-02-16 11:40:50,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:50,994][root][INFO] - Training Epoch: 1/2, step 5837/23838 completed (loss: 0.7892742156982422, acc: 0.7714285850524902)
[2025-02-16 11:40:51,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:51,356][root][INFO] - Training Epoch: 1/2, step 5838/23838 completed (loss: 0.5477657318115234, acc: 0.782608687877655)
[2025-02-16 11:40:51,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:51,730][root][INFO] - Training Epoch: 1/2, step 5839/23838 completed (loss: 0.3514705300331116, acc: 0.8095238208770752)
[2025-02-16 11:40:51,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:52,236][root][INFO] - Training Epoch: 1/2, step 5840/23838 completed (loss: 0.6739217638969421, acc: 0.859649121761322)
[2025-02-16 11:40:52,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:52,651][root][INFO] - Training Epoch: 1/2, step 5841/23838 completed (loss: 1.1156237125396729, acc: 0.6842105388641357)
[2025-02-16 11:40:52,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:53,063][root][INFO] - Training Epoch: 1/2, step 5842/23838 completed (loss: 0.4647485017776489, acc: 0.8888888955116272)
[2025-02-16 11:40:53,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:53,513][root][INFO] - Training Epoch: 1/2, step 5843/23838 completed (loss: 0.7661314606666565, acc: 0.7647058963775635)
[2025-02-16 11:40:53,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:54,007][root][INFO] - Training Epoch: 1/2, step 5844/23838 completed (loss: 0.4235738515853882, acc: 0.8780487775802612)
[2025-02-16 11:40:54,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:54,452][root][INFO] - Training Epoch: 1/2, step 5845/23838 completed (loss: 1.3158718347549438, acc: 0.699999988079071)
[2025-02-16 11:40:54,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:54,890][root][INFO] - Training Epoch: 1/2, step 5846/23838 completed (loss: 0.48525455594062805, acc: 0.8541666865348816)
[2025-02-16 11:40:55,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:55,265][root][INFO] - Training Epoch: 1/2, step 5847/23838 completed (loss: 0.3497968912124634, acc: 0.9055117964744568)
[2025-02-16 11:40:55,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:55,672][root][INFO] - Training Epoch: 1/2, step 5848/23838 completed (loss: 0.6240186095237732, acc: 0.7727272510528564)
[2025-02-16 11:40:55,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:56,040][root][INFO] - Training Epoch: 1/2, step 5849/23838 completed (loss: 0.9341030120849609, acc: 0.782608687877655)
[2025-02-16 11:40:56,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:56,410][root][INFO] - Training Epoch: 1/2, step 5850/23838 completed (loss: 0.582319438457489, acc: 0.824999988079071)
[2025-02-16 11:40:56,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:56,860][root][INFO] - Training Epoch: 1/2, step 5851/23838 completed (loss: 1.1558440923690796, acc: 0.7857142686843872)
[2025-02-16 11:40:57,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:57,238][root][INFO] - Training Epoch: 1/2, step 5852/23838 completed (loss: 0.9030412435531616, acc: 0.7941176295280457)
[2025-02-16 11:40:57,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:57,695][root][INFO] - Training Epoch: 1/2, step 5853/23838 completed (loss: 0.747177004814148, acc: 0.7948718070983887)
[2025-02-16 11:40:57,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:58,088][root][INFO] - Training Epoch: 1/2, step 5854/23838 completed (loss: 0.35197216272354126, acc: 0.8888888955116272)
[2025-02-16 11:40:58,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:58,503][root][INFO] - Training Epoch: 1/2, step 5855/23838 completed (loss: 0.7460260987281799, acc: 0.8571428656578064)
[2025-02-16 11:40:58,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:58,921][root][INFO] - Training Epoch: 1/2, step 5856/23838 completed (loss: 0.28040942549705505, acc: 0.9210526347160339)
[2025-02-16 11:40:59,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:59,315][root][INFO] - Training Epoch: 1/2, step 5857/23838 completed (loss: 0.19676093757152557, acc: 0.9333333373069763)
[2025-02-16 11:40:59,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:40:59,769][root][INFO] - Training Epoch: 1/2, step 5858/23838 completed (loss: 0.4558185040950775, acc: 0.8275862336158752)
[2025-02-16 11:40:59,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:00,166][root][INFO] - Training Epoch: 1/2, step 5859/23838 completed (loss: 0.5504470467567444, acc: 0.8604651093482971)
[2025-02-16 11:41:00,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:00,660][root][INFO] - Training Epoch: 1/2, step 5860/23838 completed (loss: 0.1847383677959442, acc: 0.9589040875434875)
[2025-02-16 11:41:00,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:01,070][root][INFO] - Training Epoch: 1/2, step 5861/23838 completed (loss: 0.3031730353832245, acc: 0.9375)
[2025-02-16 11:41:01,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:01,523][root][INFO] - Training Epoch: 1/2, step 5862/23838 completed (loss: 0.3443291187286377, acc: 0.9130434989929199)
[2025-02-16 11:41:01,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:02,017][root][INFO] - Training Epoch: 1/2, step 5863/23838 completed (loss: 0.19961212575435638, acc: 0.9433962106704712)
[2025-02-16 11:41:02,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:02,496][root][INFO] - Training Epoch: 1/2, step 5864/23838 completed (loss: 0.24276886880397797, acc: 0.9305555820465088)
[2025-02-16 11:41:02,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:02,929][root][INFO] - Training Epoch: 1/2, step 5865/23838 completed (loss: 0.14018496870994568, acc: 0.9599999785423279)
[2025-02-16 11:41:03,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:03,306][root][INFO] - Training Epoch: 1/2, step 5866/23838 completed (loss: 0.3824578523635864, acc: 0.8974359035491943)
[2025-02-16 11:41:03,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:03,789][root][INFO] - Training Epoch: 1/2, step 5867/23838 completed (loss: 0.30592063069343567, acc: 0.918367326259613)
[2025-02-16 11:41:03,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:04,213][root][INFO] - Training Epoch: 1/2, step 5868/23838 completed (loss: 0.917717456817627, acc: 0.8095238208770752)
[2025-02-16 11:41:04,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:04,754][root][INFO] - Training Epoch: 1/2, step 5869/23838 completed (loss: 0.8260425329208374, acc: 0.8198198080062866)
[2025-02-16 11:41:05,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:05,263][root][INFO] - Training Epoch: 1/2, step 5870/23838 completed (loss: 1.3962122201919556, acc: 0.6476190686225891)
[2025-02-16 11:41:05,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:05,882][root][INFO] - Training Epoch: 1/2, step 5871/23838 completed (loss: 0.4014466404914856, acc: 0.9175257682800293)
[2025-02-16 11:41:06,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:06,263][root][INFO] - Training Epoch: 1/2, step 5872/23838 completed (loss: 1.1745482683181763, acc: 0.6458333134651184)
[2025-02-16 11:41:06,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:06,744][root][INFO] - Training Epoch: 1/2, step 5873/23838 completed (loss: 0.6919883489608765, acc: 0.774193525314331)
[2025-02-16 11:41:06,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:07,115][root][INFO] - Training Epoch: 1/2, step 5874/23838 completed (loss: 0.9042047262191772, acc: 0.75)
[2025-02-16 11:41:07,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:07,574][root][INFO] - Training Epoch: 1/2, step 5875/23838 completed (loss: 0.30077916383743286, acc: 0.9615384340286255)
[2025-02-16 11:41:07,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:08,002][root][INFO] - Training Epoch: 1/2, step 5876/23838 completed (loss: 1.4550224542617798, acc: 0.6000000238418579)
[2025-02-16 11:41:08,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:08,472][root][INFO] - Training Epoch: 1/2, step 5877/23838 completed (loss: 0.24431659281253815, acc: 0.8888888955116272)
[2025-02-16 11:41:08,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:08,874][root][INFO] - Training Epoch: 1/2, step 5878/23838 completed (loss: 0.2892886996269226, acc: 0.9130434989929199)
[2025-02-16 11:41:09,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:09,340][root][INFO] - Training Epoch: 1/2, step 5879/23838 completed (loss: 0.5311042666435242, acc: 0.8461538553237915)
[2025-02-16 11:41:09,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:09,766][root][INFO] - Training Epoch: 1/2, step 5880/23838 completed (loss: 0.8421160578727722, acc: 0.7755101919174194)
[2025-02-16 11:41:09,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:10,153][root][INFO] - Training Epoch: 1/2, step 5881/23838 completed (loss: 0.753719687461853, acc: 0.8500000238418579)
[2025-02-16 11:41:10,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:10,532][root][INFO] - Training Epoch: 1/2, step 5882/23838 completed (loss: 0.658314049243927, acc: 0.8461538553237915)
[2025-02-16 11:41:10,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:10,935][root][INFO] - Training Epoch: 1/2, step 5883/23838 completed (loss: 1.1451895236968994, acc: 0.7272727489471436)
[2025-02-16 11:41:11,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:11,357][root][INFO] - Training Epoch: 1/2, step 5884/23838 completed (loss: 0.15791356563568115, acc: 0.9599999785423279)
[2025-02-16 11:41:11,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:11,872][root][INFO] - Training Epoch: 1/2, step 5885/23838 completed (loss: 1.6507329940795898, acc: 0.6190476417541504)
[2025-02-16 11:41:12,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:12,319][root][INFO] - Training Epoch: 1/2, step 5886/23838 completed (loss: 0.5868536233901978, acc: 0.8965517282485962)
[2025-02-16 11:41:12,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:12,858][root][INFO] - Training Epoch: 1/2, step 5887/23838 completed (loss: 1.5983747243881226, acc: 0.5652173757553101)
[2025-02-16 11:41:13,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:13,449][root][INFO] - Training Epoch: 1/2, step 5888/23838 completed (loss: 1.1633728742599487, acc: 0.6842105388641357)
[2025-02-16 11:41:13,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:13,836][root][INFO] - Training Epoch: 1/2, step 5889/23838 completed (loss: 1.3404873609542847, acc: 0.6571428775787354)
[2025-02-16 11:41:14,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:14,493][root][INFO] - Training Epoch: 1/2, step 5890/23838 completed (loss: 1.0162910223007202, acc: 0.6774193644523621)
[2025-02-16 11:41:14,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:14,967][root][INFO] - Training Epoch: 1/2, step 5891/23838 completed (loss: 0.699242115020752, acc: 0.7058823704719543)
[2025-02-16 11:41:15,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:15,700][root][INFO] - Training Epoch: 1/2, step 5892/23838 completed (loss: 0.7564485669136047, acc: 0.8245614171028137)
[2025-02-16 11:41:15,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:16,106][root][INFO] - Training Epoch: 1/2, step 5893/23838 completed (loss: 0.5541549921035767, acc: 0.843137264251709)
[2025-02-16 11:41:16,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:16,487][root][INFO] - Training Epoch: 1/2, step 5894/23838 completed (loss: 1.0665781497955322, acc: 0.6829268336296082)
[2025-02-16 11:41:16,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:17,312][root][INFO] - Training Epoch: 1/2, step 5895/23838 completed (loss: 1.0870916843414307, acc: 0.7230769395828247)
[2025-02-16 11:41:17,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:17,750][root][INFO] - Training Epoch: 1/2, step 5896/23838 completed (loss: 0.6023573875427246, acc: 0.8163265585899353)
[2025-02-16 11:41:18,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:18,291][root][INFO] - Training Epoch: 1/2, step 5897/23838 completed (loss: 0.33955708146095276, acc: 0.9036144614219666)
[2025-02-16 11:41:18,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:18,792][root][INFO] - Training Epoch: 1/2, step 5898/23838 completed (loss: 0.15893211960792542, acc: 0.9285714030265808)
[2025-02-16 11:41:19,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:19,296][root][INFO] - Training Epoch: 1/2, step 5899/23838 completed (loss: 0.4235897362232208, acc: 0.8620689511299133)
[2025-02-16 11:41:19,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:19,722][root][INFO] - Training Epoch: 1/2, step 5900/23838 completed (loss: 0.3419843912124634, acc: 0.875)
[2025-02-16 11:41:19,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:20,180][root][INFO] - Training Epoch: 1/2, step 5901/23838 completed (loss: 0.39421483874320984, acc: 0.925000011920929)
[2025-02-16 11:41:20,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:20,737][root][INFO] - Training Epoch: 1/2, step 5902/23838 completed (loss: 0.27765950560569763, acc: 0.898876428604126)
[2025-02-16 11:41:21,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:21,318][root][INFO] - Training Epoch: 1/2, step 5903/23838 completed (loss: 0.14503264427185059, acc: 0.9626168012619019)
[2025-02-16 11:41:21,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:22,090][root][INFO] - Training Epoch: 1/2, step 5904/23838 completed (loss: 0.38386186957359314, acc: 0.9090909361839294)
[2025-02-16 11:41:22,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:22,623][root][INFO] - Training Epoch: 1/2, step 5905/23838 completed (loss: 1.123807430267334, acc: 0.6666666865348816)
[2025-02-16 11:41:22,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:23,001][root][INFO] - Training Epoch: 1/2, step 5906/23838 completed (loss: 0.1985340267419815, acc: 0.9473684430122375)
[2025-02-16 11:41:23,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:23,396][root][INFO] - Training Epoch: 1/2, step 5907/23838 completed (loss: 1.0595694780349731, acc: 0.6499999761581421)
[2025-02-16 11:41:23,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:23,887][root][INFO] - Training Epoch: 1/2, step 5908/23838 completed (loss: 1.065929651260376, acc: 0.7674418687820435)
[2025-02-16 11:41:24,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:24,318][root][INFO] - Training Epoch: 1/2, step 5909/23838 completed (loss: 0.29937735199928284, acc: 0.9333333373069763)
[2025-02-16 11:41:24,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:24,711][root][INFO] - Training Epoch: 1/2, step 5910/23838 completed (loss: 0.21411502361297607, acc: 0.9399999976158142)
[2025-02-16 11:41:24,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:25,094][root][INFO] - Training Epoch: 1/2, step 5911/23838 completed (loss: 0.022694405168294907, acc: 1.0)
[2025-02-16 11:41:25,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:25,895][root][INFO] - Training Epoch: 1/2, step 5912/23838 completed (loss: 0.08655810356140137, acc: 0.9852941036224365)
[2025-02-16 11:41:26,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:26,337][root][INFO] - Training Epoch: 1/2, step 5913/23838 completed (loss: 0.3392111361026764, acc: 0.8867924809455872)
[2025-02-16 11:41:26,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:26,804][root][INFO] - Training Epoch: 1/2, step 5914/23838 completed (loss: 0.1388508677482605, acc: 0.9439252614974976)
[2025-02-16 11:41:26,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:27,179][root][INFO] - Training Epoch: 1/2, step 5915/23838 completed (loss: 0.24329224228858948, acc: 0.9387755393981934)
[2025-02-16 11:41:27,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:27,628][root][INFO] - Training Epoch: 1/2, step 5916/23838 completed (loss: 0.30387574434280396, acc: 0.90625)
[2025-02-16 11:41:27,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:28,072][root][INFO] - Training Epoch: 1/2, step 5917/23838 completed (loss: 0.07452642172574997, acc: 1.0)
[2025-02-16 11:41:28,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:28,592][root][INFO] - Training Epoch: 1/2, step 5918/23838 completed (loss: 0.7811276316642761, acc: 0.8333333134651184)
[2025-02-16 11:41:28,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:29,061][root][INFO] - Training Epoch: 1/2, step 5919/23838 completed (loss: 0.07793863862752914, acc: 0.987500011920929)
[2025-02-16 11:41:29,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:29,591][root][INFO] - Training Epoch: 1/2, step 5920/23838 completed (loss: 0.2507118880748749, acc: 0.9638554453849792)
[2025-02-16 11:41:29,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:30,007][root][INFO] - Training Epoch: 1/2, step 5921/23838 completed (loss: 0.23550854623317719, acc: 0.9756097793579102)
[2025-02-16 11:41:30,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:30,428][root][INFO] - Training Epoch: 1/2, step 5922/23838 completed (loss: 0.014155661687254906, acc: 1.0)
[2025-02-16 11:41:30,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:30,830][root][INFO] - Training Epoch: 1/2, step 5923/23838 completed (loss: 0.6599435806274414, acc: 0.8500000238418579)
[2025-02-16 11:41:31,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:31,345][root][INFO] - Training Epoch: 1/2, step 5924/23838 completed (loss: 0.3768264055252075, acc: 0.8958333134651184)
[2025-02-16 11:41:31,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:31,733][root][INFO] - Training Epoch: 1/2, step 5925/23838 completed (loss: 0.3549145460128784, acc: 0.9056603908538818)
[2025-02-16 11:41:31,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:32,097][root][INFO] - Training Epoch: 1/2, step 5926/23838 completed (loss: 0.5168570876121521, acc: 0.8055555820465088)
[2025-02-16 11:41:32,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:32,602][root][INFO] - Training Epoch: 1/2, step 5927/23838 completed (loss: 1.3110822439193726, acc: 0.6461538672447205)
[2025-02-16 11:41:32,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:32,984][root][INFO] - Training Epoch: 1/2, step 5928/23838 completed (loss: 1.0811718702316284, acc: 0.7272727489471436)
[2025-02-16 11:41:33,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:33,390][root][INFO] - Training Epoch: 1/2, step 5929/23838 completed (loss: 0.6084302663803101, acc: 0.824999988079071)
[2025-02-16 11:41:33,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:33,924][root][INFO] - Training Epoch: 1/2, step 5930/23838 completed (loss: 0.12621748447418213, acc: 0.9873417615890503)
[2025-02-16 11:41:34,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:34,510][root][INFO] - Training Epoch: 1/2, step 5931/23838 completed (loss: 0.1957869529724121, acc: 0.938144326210022)
[2025-02-16 11:41:34,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:34,944][root][INFO] - Training Epoch: 1/2, step 5932/23838 completed (loss: 0.3731982707977295, acc: 0.8947368264198303)
[2025-02-16 11:41:35,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:35,989][root][INFO] - Training Epoch: 1/2, step 5933/23838 completed (loss: 0.31878966093063354, acc: 0.9204545617103577)
[2025-02-16 11:41:36,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:36,415][root][INFO] - Training Epoch: 1/2, step 5934/23838 completed (loss: 0.45080122351646423, acc: 0.9047619104385376)
[2025-02-16 11:41:36,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:37,158][root][INFO] - Training Epoch: 1/2, step 5935/23838 completed (loss: 0.5121787786483765, acc: 0.8309859037399292)
[2025-02-16 11:41:37,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:37,704][root][INFO] - Training Epoch: 1/2, step 5936/23838 completed (loss: 0.853434145450592, acc: 0.75)
[2025-02-16 11:41:37,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:38,111][root][INFO] - Training Epoch: 1/2, step 5937/23838 completed (loss: 0.5494586229324341, acc: 0.8648648858070374)
[2025-02-16 11:41:38,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:38,523][root][INFO] - Training Epoch: 1/2, step 5938/23838 completed (loss: 0.10268361866474152, acc: 0.9777777791023254)
[2025-02-16 11:41:38,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:39,308][root][INFO] - Training Epoch: 1/2, step 5939/23838 completed (loss: 0.4715999960899353, acc: 0.8712871074676514)
[2025-02-16 11:41:39,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:39,872][root][INFO] - Training Epoch: 1/2, step 5940/23838 completed (loss: 0.2689303159713745, acc: 0.9418604373931885)
[2025-02-16 11:41:40,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:40,325][root][INFO] - Training Epoch: 1/2, step 5941/23838 completed (loss: 0.9763367176055908, acc: 0.7553191781044006)
[2025-02-16 11:41:40,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:40,721][root][INFO] - Training Epoch: 1/2, step 5942/23838 completed (loss: 0.4566619396209717, acc: 0.8552631735801697)
[2025-02-16 11:41:40,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:41,174][root][INFO] - Training Epoch: 1/2, step 5943/23838 completed (loss: 0.7507749199867249, acc: 0.7758620977401733)
[2025-02-16 11:41:41,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:41,597][root][INFO] - Training Epoch: 1/2, step 5944/23838 completed (loss: 0.30273324251174927, acc: 0.9145299196243286)
[2025-02-16 11:41:41,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:42,030][root][INFO] - Training Epoch: 1/2, step 5945/23838 completed (loss: 1.3916574716567993, acc: 0.6065573692321777)
[2025-02-16 11:41:42,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:42,466][root][INFO] - Training Epoch: 1/2, step 5946/23838 completed (loss: 0.8292630910873413, acc: 0.7789473533630371)
[2025-02-16 11:41:42,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:42,871][root][INFO] - Training Epoch: 1/2, step 5947/23838 completed (loss: 0.5378710031509399, acc: 0.7936508059501648)
[2025-02-16 11:41:43,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:43,305][root][INFO] - Training Epoch: 1/2, step 5948/23838 completed (loss: 0.9333981275558472, acc: 0.782608687877655)
[2025-02-16 11:41:43,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:43,868][root][INFO] - Training Epoch: 1/2, step 5949/23838 completed (loss: 0.9007360935211182, acc: 0.7395833134651184)
[2025-02-16 11:41:44,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:44,284][root][INFO] - Training Epoch: 1/2, step 5950/23838 completed (loss: 1.0113459825515747, acc: 0.7142857313156128)
[2025-02-16 11:41:44,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:44,698][root][INFO] - Training Epoch: 1/2, step 5951/23838 completed (loss: 0.6950778365135193, acc: 0.8518518805503845)
[2025-02-16 11:41:44,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:45,089][root][INFO] - Training Epoch: 1/2, step 5952/23838 completed (loss: 0.5966225862503052, acc: 0.84375)
[2025-02-16 11:41:45,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:45,496][root][INFO] - Training Epoch: 1/2, step 5953/23838 completed (loss: 0.8365378975868225, acc: 0.769911527633667)
[2025-02-16 11:41:45,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:45,918][root][INFO] - Training Epoch: 1/2, step 5954/23838 completed (loss: 0.584205150604248, acc: 0.8039215803146362)
[2025-02-16 11:41:46,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:46,315][root][INFO] - Training Epoch: 1/2, step 5955/23838 completed (loss: 0.7877483367919922, acc: 0.7564102411270142)
[2025-02-16 11:41:46,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:46,826][root][INFO] - Training Epoch: 1/2, step 5956/23838 completed (loss: 0.8038735389709473, acc: 0.7831325531005859)
[2025-02-16 11:41:47,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:47,258][root][INFO] - Training Epoch: 1/2, step 5957/23838 completed (loss: 0.2795014977455139, acc: 0.9438202381134033)
[2025-02-16 11:41:47,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:47,636][root][INFO] - Training Epoch: 1/2, step 5958/23838 completed (loss: 0.5180067420005798, acc: 0.901098906993866)
[2025-02-16 11:41:48,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:49,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:49,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:50,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:51,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:51,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:52,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:52,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:53,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:53,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:54,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:55,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:56,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:56,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:57,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:57,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:58,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:58,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:59,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:41:59,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:00,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:00,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:01,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:01,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:02,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:02,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:03,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:03,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:04,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:04,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:05,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:05,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:06,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:06,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:07,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:07,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:08,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:08,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:09,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:09,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:10,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:10,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:11,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:11,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:12,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:13,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:13,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:14,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:14,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:15,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:15,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:16,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:16,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:17,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:17,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:18,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:18,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:19,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:19,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:20,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:20,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:21,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:21,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:21,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:22,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:22,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:23,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:23,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:24,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:24,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:25,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:26,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:26,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:27,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:27,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:27,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:28,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:28,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:29,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:29,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:30,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:30,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:30,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:31,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:31,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:32,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:33,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:33,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:33,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:34,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:35,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:35,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:35,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:36,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:36,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:37,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:37,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:38,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:38,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:39,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:39,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:40,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:40,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:41,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:41,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:42,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:42,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:43,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:43,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:44,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:44,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:45,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:46,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:46,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:47,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:47,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:48,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:48,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:49,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:49,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:50,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:50,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:51,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:51,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:52,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:52,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:53,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:53,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:54,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:54,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:55,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:55,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:56,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:56,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:57,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:57,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:58,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:58,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:59,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:59,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:42:59,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:00,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:00,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:01,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:01,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:02,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:02,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:03,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:03,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:04,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:04,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:05,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:05,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:06,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:06,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:07,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:07,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:08,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:08,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:08,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:09,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:09,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:10,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:10,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:11,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:11,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:12,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:12,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:13,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:13,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:14,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:14,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:15,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:15,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:16,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:16,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:17,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:17,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:18,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:18,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:19,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:19,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:20,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:20,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:21,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:21,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:22,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:22,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:23,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:23,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:24,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:24,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:25,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:25,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:26,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:26,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:26,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:27,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:27,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:28,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:28,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:29,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:29,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:30,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:30,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:31,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:32,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:32,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:32,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:33,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:33,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:34,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:34,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:35,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:35,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:36,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:36,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:37,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:38,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:38,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:39,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:39,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:40,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:40,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:40,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:41,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:41,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:42,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:42,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:43,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:44,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:44,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:45,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:45,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:46,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:46,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:47,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:47,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:47,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:48,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:48,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:49,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:49,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:50,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:50,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:51,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:51,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:52,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:52,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:53,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:53,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:54,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:54,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:55,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:55,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:56,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:56,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:57,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:57,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:58,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:58,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:59,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:43:59,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:00,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:00,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:01,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:01,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:02,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:02,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:03,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:03,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:04,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:04,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:05,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:05,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:06,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:06,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:06,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:07,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:07,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:08,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:08,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:09,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:09,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:09,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:10,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:10,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:11,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:11,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:12,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:12,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:13,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:13,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:13,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:14,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:15,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:15,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:16,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:16,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:17,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:17,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:18,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:18,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:19,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:19,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:20,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:20,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:21,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:21,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:22,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:22,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:23,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:23,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:24,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:24,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:25,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:25,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:26,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:26,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:26,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:27,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:28,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:28,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:28,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:29,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:29,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:30,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:30,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:31,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:31,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:31,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:32,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:32,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:33,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:33,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:34,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:34,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:35,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:35,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:36,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:36,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:37,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:38,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:38,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:39,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:39,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:39,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:40,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:40,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:41,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:41,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:42,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:43,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:43,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:44,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:44,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:45,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:45,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:45,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:46,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:46,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:47,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:47,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:48,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:48,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:49,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:49,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:50,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:50,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:51,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:51,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:52,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:52,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:53,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:53,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:54,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:55,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:55,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:55,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:56,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:56,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:57,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:57,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:58,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:58,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:59,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:44:59,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:00,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:00,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:01,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:01,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:02,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:02,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:03,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:04,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:04,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:04,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:05,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:05,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:06,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:06,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:07,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:07,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:08,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:08,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:09,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:10,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:10,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:11,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:11,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:12,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:12,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:13,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:13,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:14,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:14,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:15,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:15,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:16,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:16,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:17,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:17,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:18,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:18,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:19,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:19,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:20,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:20,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:21,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:21,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:22,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:22,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:23,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:23,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:24,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:24,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:25,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:25,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:26,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:26,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:27,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:27,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:28,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:28,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:29,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:29,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:30,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:31,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:31,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:32,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:32,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:33,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:33,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:34,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:34,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:35,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:36,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:36,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:37,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:37,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:38,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:38,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:39,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:39,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:40,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:40,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:41,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:41,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:42,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:42,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:43,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:43,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:44,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:44,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:45,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:45,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:46,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:46,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:47,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:47,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:48,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:48,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:49,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:49,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:49,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:50,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:50,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:51,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:51,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:52,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:52,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:53,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:53,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:54,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:54,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:55,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:55,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:56,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:56,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:57,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:57,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:58,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:58,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:59,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:45:59,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:00,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:00,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:01,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:01,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:02,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:02,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:03,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:04,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:04,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:05,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:05,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:06,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:06,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:07,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:07,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:08,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:08,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:09,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:09,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:10,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:10,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:11,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:11,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:12,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:12,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:13,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:13,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:14,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:14,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:15,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:15,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:16,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:16,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:17,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:17,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:17,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:18,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:18,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:19,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:20,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:20,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:21,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:21,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:22,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:22,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:23,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:23,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:24,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:24,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:25,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:25,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:26,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:26,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:27,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:27,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:28,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:28,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:29,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:29,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:30,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:30,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:31,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:31,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:32,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:33,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:33,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:34,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:34,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:35,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:35,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:36,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:37,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:37,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:38,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:38,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:39,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:40,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:40,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:41,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:42,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:42,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:43,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:44,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:44,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:45,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:45,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:46,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:46,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:47,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:48,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:48,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:49,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:49,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:50,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:50,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:51,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:51,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:52,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:53,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:53,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:54,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:54,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:55,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:55,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:56,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:56,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:57,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:57,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:58,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:58,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:59,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:46:59,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:00,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:00,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:01,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:02,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:02,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:02,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:03,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:03,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:04,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:05,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:05,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:06,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:06,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:07,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:07,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:07,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:08,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:09,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:09,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:10,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:10,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:11,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:11,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:12,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:12,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:13,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:13,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:14,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:14,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:15,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:15,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:16,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:16,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:17,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:17,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:18,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:18,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:19,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:19,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:20,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:20,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:21,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:21,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:22,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:22,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:23,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:23,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:24,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:25,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:25,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:26,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:26,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:27,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:27,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:28,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:28,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:29,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:29,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:30,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:30,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:31,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:31,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:32,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:32,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:33,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:33,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:34,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:34,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:35,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:35,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:36,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:36,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:37,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:37,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:38,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:38,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:39,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:39,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:40,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:40,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:41,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:41,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:42,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:42,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:43,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:44,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:44,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:45,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:45,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:46,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:46,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:46,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:47,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:47,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:48,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:48,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:49,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:49,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:50,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:51,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:51,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:51,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:52,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:52,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:53,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:54,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:54,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:55,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:55,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:56,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:56,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:57,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:58,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:58,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:59,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:47:59,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:00,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:00,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:01,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:01,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:02,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:02,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:03,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:03,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:04,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:04,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:05,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:05,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:06,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:06,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:07,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:07,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:08,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:09,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:09,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:10,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:10,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:10,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:11,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:12,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:12,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:13,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:13,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:13,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:14,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:14,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:15,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:15,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:16,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:16,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:17,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:18,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:18,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:19,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:19,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:20,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:20,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:20,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:21,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:22,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:22,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:23,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:23,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:24,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:24,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:25,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:25,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:26,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:26,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:27,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:27,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:28,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:28,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:29,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:29,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:30,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:30,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:31,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:31,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:32,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:32,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:33,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:34,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:34,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:34,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:35,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:35,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:36,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:36,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:37,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:38,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:38,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:39,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:39,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:40,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:40,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:41,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:41,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:42,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:42,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:43,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:44,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:44,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:45,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:45,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:46,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:47,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:47,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:47,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:48,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:49,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:49,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:50,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:50,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:51,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:51,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:52,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:52,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:53,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:53,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:54,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:54,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:55,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:55,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:56,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:56,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:57,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:57,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:58,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:58,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:59,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:48:59,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:00,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:00,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:01,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:01,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:02,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:02,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:03,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:03,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:04,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:05,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:05,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:06,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:06,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:07,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:07,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:08,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:08,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:09,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:09,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:10,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:10,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:11,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:11,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:12,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:12,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:13,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:13,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:14,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:14,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:15,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:15,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:15,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:16,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:16,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:17,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:17,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:18,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:18,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:19,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:19,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:20,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:20,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:21,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:21,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:22,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:22,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:23,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:23,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:24,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:24,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:25,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:26,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:26,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:26,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:27,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:27,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:28,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:28,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:29,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:29,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:30,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:30,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:31,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:31,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:32,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:32,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:33,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:33,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:34,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:34,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:35,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:35,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:36,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:36,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:37,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:37,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:38,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:38,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:39,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:39,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:40,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:40,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:41,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:41,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:42,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:42,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:43,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:43,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:44,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:44,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:45,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:45,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:46,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:46,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:47,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:47,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:47,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:48,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:48,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:49,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:49,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:50,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:50,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:51,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:51,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:52,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:52,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:53,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:53,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:54,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:54,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:55,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:55,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:56,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:56,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:57,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:57,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:58,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:58,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:59,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:49:59,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:00,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:00,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:01,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:01,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:02,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:02,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:03,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:03,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:04,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:04,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:05,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:05,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:06,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:06,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:07,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:07,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:08,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:08,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:09,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:09,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:10,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:10,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:11,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:11,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:12,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:12,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:13,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:13,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:14,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:14,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:15,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:15,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:16,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:16,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:17,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:17,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:18,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:18,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:19,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:20,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:20,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:21,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:21,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:22,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:23,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:23,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:24,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:25,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:25,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:26,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:26,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:27,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:28,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:28,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:28,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:29,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:29,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:30,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:31,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:31,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:32,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:32,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:33,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:33,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:34,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:34,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:35,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:35,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:36,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:36,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:37,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:37,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:38,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:38,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:39,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:39,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:40,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:40,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:41,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:41,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:42,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:43,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:43,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:44,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:44,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:45,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:45,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:46,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:46,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:47,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:47,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:48,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:48,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:49,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:49,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:50,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:50,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:51,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:51,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:52,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:52,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:53,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:53,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:54,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:54,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:55,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:55,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:56,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:56,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:57,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:57,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:58,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:58,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:59,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:50:59,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:00,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:00,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:01,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:01,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:02,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:02,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:03,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:03,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:04,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:04,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:05,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:05,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:06,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:06,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:07,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:07,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:08,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:08,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:09,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:09,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:10,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:10,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:11,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:11,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:12,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:12,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:13,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:13,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:14,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:14,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:15,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:15,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:16,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:16,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:17,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:17,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:18,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:18,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:19,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:19,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:20,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:20,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:21,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:21,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:22,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:22,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:23,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:23,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:24,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:24,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:25,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:25,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:26,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:27,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:27,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:28,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:28,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:29,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:29,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:30,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:30,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:31,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:31,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:32,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:32,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:32,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:33,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:33,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:34,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:34,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:35,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:35,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:36,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:36,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:37,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:37,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:38,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:38,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:39,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:39,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:40,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:40,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:41,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:41,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:42,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:43,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:43,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:44,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:44,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:45,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:45,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:46,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:46,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:47,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:47,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:48,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:48,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:49,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:49,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:50,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:50,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:51,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:51,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:52,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:52,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:53,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:53,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:54,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:54,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:55,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:55,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:56,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:56,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:57,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:57,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:58,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:58,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:59,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:51:59,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:00,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:00,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:01,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:02,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:02,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:03,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:03,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:04,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:04,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:05,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:05,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:06,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:06,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:07,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:07,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:07,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:08,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:08,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:09,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:09,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:10,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:10,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:11,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:11,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:12,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:12,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:12,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:13,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:14,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:14,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:15,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:15,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:16,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:16,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:17,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:17,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:18,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:18,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:19,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:19,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:20,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:20,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:21,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:21,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:22,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:22,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:23,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:23,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:24,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:25,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:25,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:26,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:27,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:27,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:28,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:28,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:29,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:29,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:30,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:30,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:31,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:31,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:32,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:33,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:33,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:34,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:34,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:35,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:35,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:36,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:37,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:37,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:38,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:38,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:39,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:39,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:40,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:40,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:41,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:41,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:42,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:42,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:43,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:44,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:44,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:45,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:45,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:46,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:47,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:47,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:48,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:48,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:49,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:49,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:50,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:51,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:52,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:52,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:53,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:53,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:54,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:54,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:55,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:56,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:56,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:57,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:57,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:58,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:59,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:52:59,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:00,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:00,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:01,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:01,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:02,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:02,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:03,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:03,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:04,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:04,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:05,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:05,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:06,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:07,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:07,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:07,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:08,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:09,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:09,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:10,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:10,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:11,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:11,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:12,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:12,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:13,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:13,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:14,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:14,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:15,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:15,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:16,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:16,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:17,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:17,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:18,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:19,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:19,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:20,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:20,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:21,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:22,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:22,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:23,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:24,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:24,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:25,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:25,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:26,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:26,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:27,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:27,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:28,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:29,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:29,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:30,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:30,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:31,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:31,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:32,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:32,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:33,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:33,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:34,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:34,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:35,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:35,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:36,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:36,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:37,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:38,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:38,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:39,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:40,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:40,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:41,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:41,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:42,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:42,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:43,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:43,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:44,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:45,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:45,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:46,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:46,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:47,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:48,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:48,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:49,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:49,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:50,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:50,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:51,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:51,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:52,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:52,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:53,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:53,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:54,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:54,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:55,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:56,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:56,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:57,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:57,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:58,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:58,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:59,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:53:59,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:00,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:00,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:01,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:01,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:02,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:02,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:03,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:04,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:04,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:05,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:05,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:06,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:06,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:07,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:07,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:08,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:08,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:09,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:09,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:10,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:10,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:11,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:11,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:12,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:13,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:13,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:14,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:14,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:15,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:15,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:16,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:16,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:17,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:17,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:18,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:19,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:19,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:20,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:20,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:21,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:22,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:22,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:23,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:23,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:24,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:24,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:25,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:25,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:26,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:26,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:27,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:27,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:28,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:28,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:29,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:30,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:30,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:31,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:31,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:32,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:32,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:33,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:34,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:34,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:35,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:35,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:36,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:36,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:37,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:37,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:38,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:38,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:39,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:39,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:40,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:41,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:41,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:42,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:42,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:43,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:43,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:44,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:44,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:45,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:45,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:46,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:46,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:47,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:48,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:48,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:49,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:49,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:50,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:50,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:51,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:51,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:52,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:52,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:53,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:54,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:54,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:55,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:55,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:56,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:57,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:58,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:58,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:59,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:54:59,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:00,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:00,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:01,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:01,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:02,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:02,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:03,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:03,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:04,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:04,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:05,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:06,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:06,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:07,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:07,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:08,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:09,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:09,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:10,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:10,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:11,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:11,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:12,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:13,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:13,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:13,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:14,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:14,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:15,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:15,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:16,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:16,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:17,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:17,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:18,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:18,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:19,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:19,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:20,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:20,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:21,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:21,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:22,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:22,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:23,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:23,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:24,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:24,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:25,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:25,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:26,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:26,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:27,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:27,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:28,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:28,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:29,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:30,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:30,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:31,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:31,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:32,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:32,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:33,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:33,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:34,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:34,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:34,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:35,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:35,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:36,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:36,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:37,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:37,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:38,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:38,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:39,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:39,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:40,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:40,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:41,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:42,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:42,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:43,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:43,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:44,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:44,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:45,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:45,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:46,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:46,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:47,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:47,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:48,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:49,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:49,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:50,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:50,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:51,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:51,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:52,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:52,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:53,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:53,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:54,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:54,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:55,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:56,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:56,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:57,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:57,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:58,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:58,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:59,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:55:59,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:00,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:00,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:01,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:01,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:02,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:02,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:03,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:03,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:04,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:04,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:05,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:05,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:06,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:06,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:06,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:07,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:07,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:08,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:08,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:09,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:09,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:10,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:11,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:11,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:12,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:12,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:12,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:13,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:14,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:14,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:15,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:15,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:16,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:16,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:16,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:17,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:17,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:18,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:19,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:19,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:20,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:20,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:21,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:21,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:22,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:22,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:23,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:23,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:24,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:25,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:25,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:26,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:26,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:27,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:27,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:28,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:28,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:29,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:29,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:29,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:30,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:31,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:31,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:32,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:32,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:33,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:33,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:34,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:34,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:35,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:36,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:36,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:37,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:37,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:38,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:39,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:39,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:40,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:40,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:41,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:41,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:42,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:42,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:43,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:43,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:44,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:44,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:45,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:45,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:46,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:46,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:47,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:47,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:48,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:48,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:49,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:49,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:50,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:51,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:51,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:52,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:52,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:53,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:53,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:54,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:54,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:55,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:55,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:56,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:57,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:57,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:57,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:58,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:58,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:56:59,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:00,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:00,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:01,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:01,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:02,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:02,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:03,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:03,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:04,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:05,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:05,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:06,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:06,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:07,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:07,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:08,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:08,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:09,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:09,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:10,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:10,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:11,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:11,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:12,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:12,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:13,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:13,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:14,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:14,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:15,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:15,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:16,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:16,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:17,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:17,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:18,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:18,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:19,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:19,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:20,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:20,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:20,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:21,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:21,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:22,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:22,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:23,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:24,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:24,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:25,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:25,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:26,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:26,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:27,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:27,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:28,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:29,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:29,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:30,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:30,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:31,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:31,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:32,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:32,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:33,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:34,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:34,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:35,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:35,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:36,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:36,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:37,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:37,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:38,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:38,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:39,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:40,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:40,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:40,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:41,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:41,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:42,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:43,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:43,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:44,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:44,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:45,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:45,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:46,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:46,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:47,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:47,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:48,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:48,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:49,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:49,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:50,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:50,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:51,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:51,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:52,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:52,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:53,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:53,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:54,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:54,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:55,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:55,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:56,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:56,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:57,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:57,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:58,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:58,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:59,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:57:59,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:00,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:01,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:01,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:02,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:02,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:03,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:03,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:04,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:04,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:05,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:05,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:06,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:06,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:07,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:08,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:08,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:08,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:09,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:09,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:10,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:10,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:11,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:11,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:12,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:12,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:13,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:13,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:14,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:14,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:15,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:15,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:16,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:16,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:16,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:17,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:17,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:18,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:18,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:19,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:19,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:20,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:20,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:21,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:21,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:22,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:22,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:23,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:23,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:24,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:24,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:25,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:25,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:26,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:26,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:27,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:27,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:28,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:28,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:29,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:29,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:29,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:30,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:30,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:31,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:31,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:32,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:32,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:33,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:33,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:34,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:34,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:35,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:35,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:36,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:36,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:37,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:37,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:38,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:38,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:39,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:39,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:40,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:40,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:41,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:41,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:42,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:42,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:43,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:43,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:44,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:45,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:45,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:46,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:46,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:47,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:47,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:48,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:48,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:49,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:49,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:50,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:50,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:51,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:52,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:52,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:53,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:53,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:54,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:54,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:55,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:55,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:56,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:56,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:57,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:57,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:58,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:58,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:59,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:59,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:58:59,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:00,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:00,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:01,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:01,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:02,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:02,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:03,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:03,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:04,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:05,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:05,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:06,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:06,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:07,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:07,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:07,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:08,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:08,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:09,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:10,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:10,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:11,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:11,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:12,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:12,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:13,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:13,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:14,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:14,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:15,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:15,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:16,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:16,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:17,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:17,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:18,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:18,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:19,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:19,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:20,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:20,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:21,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:21,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:22,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:22,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:23,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:23,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:24,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:24,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:25,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:25,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:26,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:26,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:27,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:27,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:28,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:28,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:29,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:30,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:30,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:31,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:31,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:32,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:32,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:33,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:33,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:34,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:34,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:35,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:35,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:36,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:37,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:37,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:38,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:39,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:39,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:40,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:40,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:41,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:41,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:41,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:42,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:43,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:43,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:43,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:44,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:45,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:45,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:45,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:46,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:46,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:47,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:47,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:48,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:48,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:49,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:49,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:50,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:50,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:51,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:51,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:52,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:52,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:53,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:54,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:54,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:55,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:55,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:56,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:56,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:57,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:57,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:58,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:58,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:59,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 11:59:59,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:00,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:00,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:01,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:02,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:02,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:02,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:03,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:03,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:04,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:04,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:05,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:05,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:06,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:06,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:07,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:07,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:07,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:08,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:08,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:09,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:09,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:10,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:10,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:11,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:11,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:12,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:12,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:13,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:13,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:14,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:14,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:15,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:15,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:16,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:16,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:17,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:18,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:18,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:19,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:19,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:20,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:20,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:20,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:21,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:22,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:22,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:23,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:23,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:23,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:24,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:25,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:25,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:26,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:26,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:27,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:28,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:28,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:28,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:29,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:29,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:30,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:30,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:31,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:31,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:32,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:32,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:33,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:33,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:34,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:34,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:35,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:36,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:36,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:37,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:37,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:38,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:39,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:39,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:40,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:40,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:41,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:41,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:42,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:42,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:43,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:44,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:44,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:45,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:45,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:46,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:46,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:47,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:47,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:48,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:48,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:49,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:49,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:50,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:50,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:51,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:51,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:52,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:53,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:53,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:54,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:54,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:55,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:55,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:56,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:56,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:57,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:57,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:58,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:59,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:00:59,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:00,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:01,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:01,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:02,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:02,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:03,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:03,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:04,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:04,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:05,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:06,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:06,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:07,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:07,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:08,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:08,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:09,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:09,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:10,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:10,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:11,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:12,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:12,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:13,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:13,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:14,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:14,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:15,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:15,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:16,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:16,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:17,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:17,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:18,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:18,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:19,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:19,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:20,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:20,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:21,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:21,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:22,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:23,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:23,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:24,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:24,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:25,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:25,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:26,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:26,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:27,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:28,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:28,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:29,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:29,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:30,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:30,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:31,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:31,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:31,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:32,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:33,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:33,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:34,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:34,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:35,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:35,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:36,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:36,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:37,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:37,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:38,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:39,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:39,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:40,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:40,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:41,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:41,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:42,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:42,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:43,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:44,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:44,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:45,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:45,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:46,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:46,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:47,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:47,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:48,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:48,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:49,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:50,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:50,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:51,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:51,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:52,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:52,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:53,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:53,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:54,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:54,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:54,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:55,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:56,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:56,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:57,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:57,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:58,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:58,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:59,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:01:59,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:00,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:00,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:00,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:01,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:01,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:02,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:02,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:03,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:03,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:04,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:05,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:05,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:05,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:06,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:07,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:07,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:08,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:09,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:09,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:10,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:10,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:11,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:11,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:12,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:12,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:13,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:13,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:14,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:14,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:15,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:15,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:16,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:16,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:17,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:17,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:18,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:18,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:19,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:19,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:20,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:20,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:21,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:21,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:22,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:22,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:23,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:23,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:24,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:24,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:24,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:25,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:25,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:26,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:26,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:27,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:28,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:28,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:29,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:29,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:30,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:30,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:31,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:31,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:32,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:32,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:33,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:34,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:35,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:35,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:36,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:37,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:37,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:38,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:39,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:40,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:40,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:41,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:42,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:43,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:43,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:44,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:44,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:45,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:45,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:46,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:47,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:47,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:48,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:48,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:49,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:49,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:49,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:50,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:50,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:51,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:51,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:52,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:52,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:53,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:53,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:54,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:55,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:55,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:56,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:56,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:57,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:57,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:57,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:58,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:58,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:59,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:02:59,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:00,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:00,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:01,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:01,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:02,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:02,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:03,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:03,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:04,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:04,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:05,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:05,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:06,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:06,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:06,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:07,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:08,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:08,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:09,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:09,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:10,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:10,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:11,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:11,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:12,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:12,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:13,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:13,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:14,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:14,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:15,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:15,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:16,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:16,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:17,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:18,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:18,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:18,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:19,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:19,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:20,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:20,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:21,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:21,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:22,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:22,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:23,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:23,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:23,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:24,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:25,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:25,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:26,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:26,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:27,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:28,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:28,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:29,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:29,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:30,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:30,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:31,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:31,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:32,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:32,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:33,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:33,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:34,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:34,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:35,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:36,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:36,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:37,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:37,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:38,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:38,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:39,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:39,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:40,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:40,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:41,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:41,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:42,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:42,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:43,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:43,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:44,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:44,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:45,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:46,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:46,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:47,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:47,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:48,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:48,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:49,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:49,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:50,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:50,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:51,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:51,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:52,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:52,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:53,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:54,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:54,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:55,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:55,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:56,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:56,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:57,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:57,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:58,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:58,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:59,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:03:59,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:00,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:00,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:01,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:01,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:02,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:02,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:03,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:03,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:04,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:04,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:05,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:05,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:06,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:06,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:07,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:08,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:08,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:09,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:09,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:10,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:10,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:11,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:11,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:12,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:12,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:13,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:13,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:14,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:14,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:15,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:15,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:16,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:16,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:17,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:17,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:18,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:18,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:19,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:19,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:20,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:21,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:21,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:22,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:22,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:23,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:23,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:24,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:24,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:25,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:25,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:26,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:26,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:27,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:27,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:28,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:29,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:30,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:30,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:31,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:31,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:32,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:33,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:33,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:34,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:34,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:35,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:35,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:36,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:36,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:37,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:37,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:38,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:39,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:39,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:40,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:40,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:41,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:41,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:42,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:42,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:43,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:43,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:44,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:44,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:45,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:45,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:46,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:46,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:47,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:47,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:48,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:48,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:49,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:49,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:49,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:50,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:50,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:51,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:51,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:52,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:53,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:53,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:54,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:54,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:55,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:55,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:56,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:56,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:57,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:57,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:58,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:58,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:59,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:04:59,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:00,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:00,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:01,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:01,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:02,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:02,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:03,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:03,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:04,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:04,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:05,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:05,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:06,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:06,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:07,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:07,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:08,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:08,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:09,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:09,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:10,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:10,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:11,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:11,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:12,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:12,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:13,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:13,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:14,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:14,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:15,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:15,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:16,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:16,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:17,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:17,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:18,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:18,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:19,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:19,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:20,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:20,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:21,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:21,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:22,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:22,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:23,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:23,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:24,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:24,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:25,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:26,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:26,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:27,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:27,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:28,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:28,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:28,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:29,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:29,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:30,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:30,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:31,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:31,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:32,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:32,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:33,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:33,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:34,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:34,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:35,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:35,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:36,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:36,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:37,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:37,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:38,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:38,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:39,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:39,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:40,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:40,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:41,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:41,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:42,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:42,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:43,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:43,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:44,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:44,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:45,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:45,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:46,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:46,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:47,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:47,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:48,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:48,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:49,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:49,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:49,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:50,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:50,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:51,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:52,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:52,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:52,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:53,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:54,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:54,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:55,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:55,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:55,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:56,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:56,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:57,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:57,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:58,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:58,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:59,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:05:59,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:00,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:01,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:01,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:02,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:02,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:03,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:03,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:04,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:04,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:05,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:05,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:06,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:06,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:07,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:07,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:08,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:08,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:09,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:09,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:10,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:10,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:11,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:11,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:12,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:12,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:13,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:14,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:14,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:15,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:15,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:16,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:16,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:17,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:17,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:18,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:18,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:19,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:19,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:20,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:20,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:21,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:21,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:22,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:22,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:23,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:23,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:24,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:24,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:25,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:25,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:26,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:26,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:27,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:27,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:28,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:28,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:29,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:29,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:30,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:30,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:31,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:31,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:32,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:32,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:33,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:33,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:34,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:34,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:35,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:35,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:36,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:36,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:37,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:37,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:38,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:38,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:39,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:39,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:40,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:40,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:41,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:41,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:42,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:43,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:43,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:44,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:44,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:45,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:46,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:46,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:47,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:47,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:48,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:48,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:49,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:49,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:50,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:51,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:51,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:52,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:52,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:53,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:53,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:54,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:54,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:55,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:55,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:56,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:56,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:57,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:57,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:58,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:58,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:59,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:06:59,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:00,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:00,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:01,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:02,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:02,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:03,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:03,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:04,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:04,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:05,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:06,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:06,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:07,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:08,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:08,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:09,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:09,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:10,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:11,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:11,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:12,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:12,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:13,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:13,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:14,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:15,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:15,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:16,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:16,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:17,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:17,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:18,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:18,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:19,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:19,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:20,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:20,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:21,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:21,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:21,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:22,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:23,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:23,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:24,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:24,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:25,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:25,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:26,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:26,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:27,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:28,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:28,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:29,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:29,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:30,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:31,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:31,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:32,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:33,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:33,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:34,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:35,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:36,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:36,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:37,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:37,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:38,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:38,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:38,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:39,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:40,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:40,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:41,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:41,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:42,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:42,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:43,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:44,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:44,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:45,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:45,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:46,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:47,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:47,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:48,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:48,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:49,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:49,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:50,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:50,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:51,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:51,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:52,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:53,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:53,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:54,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:54,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:55,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:55,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:56,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:56,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:57,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:57,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:58,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:58,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:59,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:07:59,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:00,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:00,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:01,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:01,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:02,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:02,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:03,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:03,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:04,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:04,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:05,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:06,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:06,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:07,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:07,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:08,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:08,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:09,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:09,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:10,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:10,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:11,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:11,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:12,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:13,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:13,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:14,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:14,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:15,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:15,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:15,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:16,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:16,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:17,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:17,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:18,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:18,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:19,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:19,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:20,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:20,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:21,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:21,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:21,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:22,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:22,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:23,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:23,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:24,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:24,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:25,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:25,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:26,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:26,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:27,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:27,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:27,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:28,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:28,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:29,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:29,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:30,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:30,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:31,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:31,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:32,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:32,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:33,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:33,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:34,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:34,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:35,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:35,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:36,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:36,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:37,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:37,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:38,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:38,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:39,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:39,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:40,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:40,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:41,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:41,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:42,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:42,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:42,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:43,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:44,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:44,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:44,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:45,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:45,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:46,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:46,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:47,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:47,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:48,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:48,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:49,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:50,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:50,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:51,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:51,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:52,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:52,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:53,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:53,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:54,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:54,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:55,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:55,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:56,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:57,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:57,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:58,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:58,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:59,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:59,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:08:59,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:00,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:00,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:01,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:01,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:02,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:02,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:03,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:03,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:04,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:05,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:05,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:06,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:06,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:07,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:07,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:08,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:08,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:09,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:09,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:10,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:10,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:11,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:11,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:12,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:12,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:13,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:13,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:14,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:14,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:15,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:15,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:15,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:16,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:16,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:17,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:17,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:18,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:18,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:19,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:19,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:20,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:20,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:21,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:21,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:22,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:22,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:23,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:23,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:24,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:24,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:25,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:25,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:26,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:26,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:27,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:27,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:27,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:28,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:28,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:29,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:29,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:30,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:30,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:31,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:31,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:32,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:33,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:33,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:33,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:34,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:34,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:35,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:35,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:36,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:36,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:37,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:37,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:38,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:38,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:39,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:39,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:40,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:40,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:41,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:41,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:42,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:42,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:43,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:43,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:44,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:44,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:44,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:45,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:45,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:46,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:46,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:47,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:47,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:48,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:48,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:49,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:49,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:50,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:50,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:51,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:51,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:52,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:52,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:53,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:53,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:54,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:54,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:55,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:55,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:56,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:56,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:56,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:57,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:57,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:58,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:58,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:59,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:09:59,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:00,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:00,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:01,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:01,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:02,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:02,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:03,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:03,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:04,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:04,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:05,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:05,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:06,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:06,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:07,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:08,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:08,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:09,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:09,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:10,397][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0907, device='cuda:0') eval_epoch_loss=tensor(0.7375, device='cuda:0') eval_epoch_acc=tensor(0.7989, device='cuda:0')
[2025-02-16 12:10:10,400][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-16 12:10:10,400][slam_llm.utils.checkpoint_handler][INFO] - --> saving model checkpoint...
[2025-02-16 12:10:11,410][slam_llm.utils.checkpoint_handler][INFO] - Checkpoint saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_dual_peft_seed_42/asr_epoch_1_step_5959_loss_0.7375224232673645/model.pt
[2025-02-16 12:10:11,414][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_dual_peft_seed_42 directory
[2025-02-16 12:10:11,415][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.7375224232673645
[2025-02-16 12:10:11,416][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7988721132278442
[2025-02-16 12:10:11,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:12,010][root][INFO] - Training Epoch: 1/2, step 5959/23838 completed (loss: 0.9160057902336121, acc: 0.7916666865348816)
[2025-02-16 12:10:12,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:12,466][root][INFO] - Training Epoch: 1/2, step 5960/23838 completed (loss: 1.1528289318084717, acc: 0.699999988079071)
[2025-02-16 12:10:12,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:12,956][root][INFO] - Training Epoch: 1/2, step 5961/23838 completed (loss: 0.6783558130264282, acc: 0.8425925970077515)
[2025-02-16 12:10:13,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:13,998][root][INFO] - Training Epoch: 1/2, step 5962/23838 completed (loss: 0.478206068277359, acc: 0.8758170008659363)
[2025-02-16 12:10:14,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:14,647][root][INFO] - Training Epoch: 1/2, step 5963/23838 completed (loss: 0.4668833613395691, acc: 0.8723404407501221)
[2025-02-16 12:10:14,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:15,084][root][INFO] - Training Epoch: 1/2, step 5964/23838 completed (loss: 0.680605947971344, acc: 0.8105263113975525)
[2025-02-16 12:10:15,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:15,896][root][INFO] - Training Epoch: 1/2, step 5965/23838 completed (loss: 0.8648058176040649, acc: 0.7817258834838867)
[2025-02-16 12:10:16,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:16,353][root][INFO] - Training Epoch: 1/2, step 5966/23838 completed (loss: 0.4516258239746094, acc: 0.8571428656578064)
[2025-02-16 12:10:16,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:16,806][root][INFO] - Training Epoch: 1/2, step 5967/23838 completed (loss: 0.49038705229759216, acc: 0.8513513803482056)
[2025-02-16 12:10:16,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:17,228][root][INFO] - Training Epoch: 1/2, step 5968/23838 completed (loss: 1.3403534889221191, acc: 0.6521739363670349)
[2025-02-16 12:10:17,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:17,687][root][INFO] - Training Epoch: 1/2, step 5969/23838 completed (loss: 0.917357861995697, acc: 0.8148148059844971)
[2025-02-16 12:10:17,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:18,172][root][INFO] - Training Epoch: 1/2, step 5970/23838 completed (loss: 0.6132330298423767, acc: 0.8125)
[2025-02-16 12:10:18,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:18,646][root][INFO] - Training Epoch: 1/2, step 5971/23838 completed (loss: 0.3404926359653473, acc: 0.8823529481887817)
[2025-02-16 12:10:18,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:19,162][root][INFO] - Training Epoch: 1/2, step 5972/23838 completed (loss: 0.38544225692749023, acc: 0.8720930218696594)
[2025-02-16 12:10:19,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:19,681][root][INFO] - Training Epoch: 1/2, step 5973/23838 completed (loss: 0.6938230991363525, acc: 0.8317757248878479)
[2025-02-16 12:10:19,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:20,167][root][INFO] - Training Epoch: 1/2, step 5974/23838 completed (loss: 0.7218429446220398, acc: 0.7941176295280457)
[2025-02-16 12:10:20,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:20,789][root][INFO] - Training Epoch: 1/2, step 5975/23838 completed (loss: 0.8546428680419922, acc: 0.77173912525177)
[2025-02-16 12:10:20,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:21,233][root][INFO] - Training Epoch: 1/2, step 5976/23838 completed (loss: 0.8827005624771118, acc: 0.800000011920929)
[2025-02-16 12:10:21,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:21,749][root][INFO] - Training Epoch: 1/2, step 5977/23838 completed (loss: 0.7882670760154724, acc: 0.8026315569877625)
[2025-02-16 12:10:21,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:22,176][root][INFO] - Training Epoch: 1/2, step 5978/23838 completed (loss: 0.6770079135894775, acc: 0.7936508059501648)
[2025-02-16 12:10:22,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:22,793][root][INFO] - Training Epoch: 1/2, step 5979/23838 completed (loss: 0.3169522285461426, acc: 0.8838709592819214)
[2025-02-16 12:10:22,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:23,204][root][INFO] - Training Epoch: 1/2, step 5980/23838 completed (loss: 1.287347674369812, acc: 0.71875)
[2025-02-16 12:10:23,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:23,697][root][INFO] - Training Epoch: 1/2, step 5981/23838 completed (loss: 0.6304916143417358, acc: 0.8230769038200378)
[2025-02-16 12:10:23,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:24,129][root][INFO] - Training Epoch: 1/2, step 5982/23838 completed (loss: 0.9638674855232239, acc: 0.7051281929016113)
[2025-02-16 12:10:24,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:24,565][root][INFO] - Training Epoch: 1/2, step 5983/23838 completed (loss: 0.41132646799087524, acc: 0.8999999761581421)
[2025-02-16 12:10:24,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:25,005][root][INFO] - Training Epoch: 1/2, step 5984/23838 completed (loss: 0.9995869994163513, acc: 0.7142857313156128)
[2025-02-16 12:10:25,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:25,406][root][INFO] - Training Epoch: 1/2, step 5985/23838 completed (loss: 1.39971923828125, acc: 0.7241379022598267)
[2025-02-16 12:10:25,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:25,840][root][INFO] - Training Epoch: 1/2, step 5986/23838 completed (loss: 0.9478904604911804, acc: 0.737500011920929)
[2025-02-16 12:10:26,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:26,271][root][INFO] - Training Epoch: 1/2, step 5987/23838 completed (loss: 0.28321996331214905, acc: 0.9420289993286133)
[2025-02-16 12:10:26,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:26,707][root][INFO] - Training Epoch: 1/2, step 5988/23838 completed (loss: 0.8174676895141602, acc: 0.7636363506317139)
[2025-02-16 12:10:26,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:27,196][root][INFO] - Training Epoch: 1/2, step 5989/23838 completed (loss: 0.3901931047439575, acc: 0.8852459192276001)
[2025-02-16 12:10:27,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:27,654][root][INFO] - Training Epoch: 1/2, step 5990/23838 completed (loss: 1.0367079973220825, acc: 0.71875)
[2025-02-16 12:10:27,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:28,065][root][INFO] - Training Epoch: 1/2, step 5991/23838 completed (loss: 0.4780067205429077, acc: 0.8846153616905212)
[2025-02-16 12:10:28,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:28,716][root][INFO] - Training Epoch: 1/2, step 5992/23838 completed (loss: 0.6272633671760559, acc: 0.7761194109916687)
[2025-02-16 12:10:28,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:29,129][root][INFO] - Training Epoch: 1/2, step 5993/23838 completed (loss: 0.4834120571613312, acc: 0.8658536672592163)
[2025-02-16 12:10:29,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:29,744][root][INFO] - Training Epoch: 1/2, step 5994/23838 completed (loss: 0.5684084892272949, acc: 0.8508771657943726)
[2025-02-16 12:10:30,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:30,330][root][INFO] - Training Epoch: 1/2, step 5995/23838 completed (loss: 0.5447817444801331, acc: 0.8367347121238708)
[2025-02-16 12:10:30,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:30,959][root][INFO] - Training Epoch: 1/2, step 5996/23838 completed (loss: 0.6210615038871765, acc: 0.8155339956283569)
[2025-02-16 12:10:31,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:31,439][root][INFO] - Training Epoch: 1/2, step 5997/23838 completed (loss: 0.9342316389083862, acc: 0.7204301357269287)
[2025-02-16 12:10:31,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:31,852][root][INFO] - Training Epoch: 1/2, step 5998/23838 completed (loss: 0.49329841136932373, acc: 0.8429751992225647)
[2025-02-16 12:10:32,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:32,364][root][INFO] - Training Epoch: 1/2, step 5999/23838 completed (loss: 0.7662068605422974, acc: 0.7931034564971924)
[2025-02-16 12:10:32,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:32,933][root][INFO] - Training Epoch: 1/2, step 6000/23838 completed (loss: 0.7006372809410095, acc: 0.8227847814559937)
[2025-02-16 12:10:33,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:33,429][root][INFO] - Training Epoch: 1/2, step 6001/23838 completed (loss: 1.2035768032073975, acc: 0.6753246784210205)
[2025-02-16 12:10:33,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:33,973][root][INFO] - Training Epoch: 1/2, step 6002/23838 completed (loss: 0.7937047481536865, acc: 0.8071428537368774)
[2025-02-16 12:10:34,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:34,385][root][INFO] - Training Epoch: 1/2, step 6003/23838 completed (loss: 0.7141919732093811, acc: 0.8260869383811951)
[2025-02-16 12:10:34,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:34,825][root][INFO] - Training Epoch: 1/2, step 6004/23838 completed (loss: 1.0461755990982056, acc: 0.7272727489471436)
[2025-02-16 12:10:34,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:35,208][root][INFO] - Training Epoch: 1/2, step 6005/23838 completed (loss: 0.7880726456642151, acc: 0.7972972989082336)
[2025-02-16 12:10:35,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:35,724][root][INFO] - Training Epoch: 1/2, step 6006/23838 completed (loss: 0.7992093563079834, acc: 0.7651006579399109)
[2025-02-16 12:10:35,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:36,122][root][INFO] - Training Epoch: 1/2, step 6007/23838 completed (loss: 1.189469575881958, acc: 0.6482412219047546)
[2025-02-16 12:10:36,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:36,555][root][INFO] - Training Epoch: 1/2, step 6008/23838 completed (loss: 0.8986880779266357, acc: 0.807692289352417)
[2025-02-16 12:10:36,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:37,012][root][INFO] - Training Epoch: 1/2, step 6009/23838 completed (loss: 1.3038344383239746, acc: 0.6315789222717285)
[2025-02-16 12:10:37,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:37,446][root][INFO] - Training Epoch: 1/2, step 6010/23838 completed (loss: 1.0580413341522217, acc: 0.7142857313156128)
[2025-02-16 12:10:37,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:37,842][root][INFO] - Training Epoch: 1/2, step 6011/23838 completed (loss: 1.2169959545135498, acc: 0.6377952694892883)
[2025-02-16 12:10:38,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:38,245][root][INFO] - Training Epoch: 1/2, step 6012/23838 completed (loss: 0.8740212321281433, acc: 0.7417582273483276)
[2025-02-16 12:10:38,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:38,644][root][INFO] - Training Epoch: 1/2, step 6013/23838 completed (loss: 1.1635973453521729, acc: 0.7076923251152039)
[2025-02-16 12:10:38,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:39,193][root][INFO] - Training Epoch: 1/2, step 6014/23838 completed (loss: 0.7064728140830994, acc: 0.8376068472862244)
[2025-02-16 12:10:39,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:39,654][root][INFO] - Training Epoch: 1/2, step 6015/23838 completed (loss: 0.8584621548652649, acc: 0.7650273442268372)
[2025-02-16 12:10:39,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:40,132][root][INFO] - Training Epoch: 1/2, step 6016/23838 completed (loss: 1.0589505434036255, acc: 0.6994219422340393)
[2025-02-16 12:10:40,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:40,578][root][INFO] - Training Epoch: 1/2, step 6017/23838 completed (loss: 0.5690506100654602, acc: 0.8497652411460876)
[2025-02-16 12:10:40,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:41,049][root][INFO] - Training Epoch: 1/2, step 6018/23838 completed (loss: 0.6890873312950134, acc: 0.7956989407539368)
[2025-02-16 12:10:41,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:41,495][root][INFO] - Training Epoch: 1/2, step 6019/23838 completed (loss: 0.7324739694595337, acc: 0.7808219194412231)
[2025-02-16 12:10:41,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:41,860][root][INFO] - Training Epoch: 1/2, step 6020/23838 completed (loss: 0.6790742874145508, acc: 0.824999988079071)
[2025-02-16 12:10:42,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:42,308][root][INFO] - Training Epoch: 1/2, step 6021/23838 completed (loss: 0.736182451248169, acc: 0.7808219194412231)
[2025-02-16 12:10:42,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:42,800][root][INFO] - Training Epoch: 1/2, step 6022/23838 completed (loss: 0.5827257633209229, acc: 0.8608695864677429)
[2025-02-16 12:10:42,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:43,240][root][INFO] - Training Epoch: 1/2, step 6023/23838 completed (loss: 1.0045456886291504, acc: 0.7253521084785461)
[2025-02-16 12:10:43,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:43,684][root][INFO] - Training Epoch: 1/2, step 6024/23838 completed (loss: 0.6303600072860718, acc: 0.8313252925872803)
[2025-02-16 12:10:43,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:44,096][root][INFO] - Training Epoch: 1/2, step 6025/23838 completed (loss: 0.7497535943984985, acc: 0.8214285969734192)
[2025-02-16 12:10:44,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:44,471][root][INFO] - Training Epoch: 1/2, step 6026/23838 completed (loss: 0.47155988216400146, acc: 0.8305084705352783)
[2025-02-16 12:10:44,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:44,883][root][INFO] - Training Epoch: 1/2, step 6027/23838 completed (loss: 0.9346510767936707, acc: 0.7124999761581421)
[2025-02-16 12:10:45,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:45,265][root][INFO] - Training Epoch: 1/2, step 6028/23838 completed (loss: 0.8816555142402649, acc: 0.7419354915618896)
[2025-02-16 12:10:45,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:45,713][root][INFO] - Training Epoch: 1/2, step 6029/23838 completed (loss: 0.6434350609779358, acc: 0.8092485666275024)
[2025-02-16 12:10:45,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:46,101][root][INFO] - Training Epoch: 1/2, step 6030/23838 completed (loss: 0.604697048664093, acc: 0.8156028389930725)
[2025-02-16 12:10:46,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:46,479][root][INFO] - Training Epoch: 1/2, step 6031/23838 completed (loss: 0.4638430178165436, acc: 0.8943089246749878)
[2025-02-16 12:10:46,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:47,108][root][INFO] - Training Epoch: 1/2, step 6032/23838 completed (loss: 0.7961543798446655, acc: 0.7802197933197021)
[2025-02-16 12:10:47,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:47,515][root][INFO] - Training Epoch: 1/2, step 6033/23838 completed (loss: 0.588912308216095, acc: 0.8058823347091675)
[2025-02-16 12:10:47,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:48,050][root][INFO] - Training Epoch: 1/2, step 6034/23838 completed (loss: 0.8267853260040283, acc: 0.7845304012298584)
[2025-02-16 12:10:48,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:48,568][root][INFO] - Training Epoch: 1/2, step 6035/23838 completed (loss: 0.6056336164474487, acc: 0.8296703100204468)
[2025-02-16 12:10:48,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:49,131][root][INFO] - Training Epoch: 1/2, step 6036/23838 completed (loss: 0.7319859862327576, acc: 0.7869822382926941)
[2025-02-16 12:10:49,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:49,646][root][INFO] - Training Epoch: 1/2, step 6037/23838 completed (loss: 0.6527023911476135, acc: 0.8361344337463379)
[2025-02-16 12:10:49,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:50,107][root][INFO] - Training Epoch: 1/2, step 6038/23838 completed (loss: 0.8595223426818848, acc: 0.7571428418159485)
[2025-02-16 12:10:50,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:50,668][root][INFO] - Training Epoch: 1/2, step 6039/23838 completed (loss: 0.7560518383979797, acc: 0.7836257219314575)
[2025-02-16 12:10:50,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:51,137][root][INFO] - Training Epoch: 1/2, step 6040/23838 completed (loss: 0.8973135948181152, acc: 0.7469135522842407)
[2025-02-16 12:10:51,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:51,597][root][INFO] - Training Epoch: 1/2, step 6041/23838 completed (loss: 0.4627567231655121, acc: 0.8876404762268066)
[2025-02-16 12:10:51,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:51,988][root][INFO] - Training Epoch: 1/2, step 6042/23838 completed (loss: 0.6094329357147217, acc: 0.8282208442687988)
[2025-02-16 12:10:52,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:52,415][root][INFO] - Training Epoch: 1/2, step 6043/23838 completed (loss: 0.48680320382118225, acc: 0.8648648858070374)
[2025-02-16 12:10:52,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:52,879][root][INFO] - Training Epoch: 1/2, step 6044/23838 completed (loss: 0.7564066648483276, acc: 0.8081395626068115)
[2025-02-16 12:10:53,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:53,351][root][INFO] - Training Epoch: 1/2, step 6045/23838 completed (loss: 0.5309377312660217, acc: 0.8468468189239502)
[2025-02-16 12:10:53,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:53,779][root][INFO] - Training Epoch: 1/2, step 6046/23838 completed (loss: 0.6065191030502319, acc: 0.8181818127632141)
[2025-02-16 12:10:53,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:54,150][root][INFO] - Training Epoch: 1/2, step 6047/23838 completed (loss: 0.9085784554481506, acc: 0.7358490824699402)
[2025-02-16 12:10:54,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:54,634][root][INFO] - Training Epoch: 1/2, step 6048/23838 completed (loss: 0.697565495967865, acc: 0.7986111044883728)
[2025-02-16 12:10:54,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:55,089][root][INFO] - Training Epoch: 1/2, step 6049/23838 completed (loss: 1.0048155784606934, acc: 0.737500011920929)
[2025-02-16 12:10:55,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:55,502][root][INFO] - Training Epoch: 1/2, step 6050/23838 completed (loss: 0.5512652397155762, acc: 0.875)
[2025-02-16 12:10:55,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:56,122][root][INFO] - Training Epoch: 1/2, step 6051/23838 completed (loss: 0.3442748486995697, acc: 0.9060402512550354)
[2025-02-16 12:10:56,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:56,536][root][INFO] - Training Epoch: 1/2, step 6052/23838 completed (loss: 0.35924288630485535, acc: 0.8854166865348816)
[2025-02-16 12:10:56,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:56,897][root][INFO] - Training Epoch: 1/2, step 6053/23838 completed (loss: 0.5695423483848572, acc: 0.8484848737716675)
[2025-02-16 12:10:57,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:57,393][root][INFO] - Training Epoch: 1/2, step 6054/23838 completed (loss: 0.5887022614479065, acc: 0.8723404407501221)
[2025-02-16 12:10:57,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:57,891][root][INFO] - Training Epoch: 1/2, step 6055/23838 completed (loss: 0.772037923336029, acc: 0.795918345451355)
[2025-02-16 12:10:58,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:58,326][root][INFO] - Training Epoch: 1/2, step 6056/23838 completed (loss: 0.24562007188796997, acc: 0.9027777910232544)
[2025-02-16 12:10:58,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:58,716][root][INFO] - Training Epoch: 1/2, step 6057/23838 completed (loss: 0.26729074120521545, acc: 0.9406779408454895)
[2025-02-16 12:10:58,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:59,234][root][INFO] - Training Epoch: 1/2, step 6058/23838 completed (loss: 0.38925066590309143, acc: 0.8780487775802612)
[2025-02-16 12:10:59,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:10:59,710][root][INFO] - Training Epoch: 1/2, step 6059/23838 completed (loss: 0.7206063270568848, acc: 0.8088235259056091)
[2025-02-16 12:10:59,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:00,217][root][INFO] - Training Epoch: 1/2, step 6060/23838 completed (loss: 0.3117465376853943, acc: 0.9251337051391602)
[2025-02-16 12:11:00,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:00,629][root][INFO] - Training Epoch: 1/2, step 6061/23838 completed (loss: 0.46250638365745544, acc: 0.8809523582458496)
[2025-02-16 12:11:00,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:01,175][root][INFO] - Training Epoch: 1/2, step 6062/23838 completed (loss: 0.5062521696090698, acc: 0.8571428656578064)
[2025-02-16 12:11:01,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:01,645][root][INFO] - Training Epoch: 1/2, step 6063/23838 completed (loss: 0.3753184378147125, acc: 0.8866666555404663)
[2025-02-16 12:11:01,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:02,081][root][INFO] - Training Epoch: 1/2, step 6064/23838 completed (loss: 0.6258907318115234, acc: 0.8243243098258972)
[2025-02-16 12:11:02,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:02,984][root][INFO] - Training Epoch: 1/2, step 6065/23838 completed (loss: 0.3432294428348541, acc: 0.9074074029922485)
[2025-02-16 12:11:03,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:03,479][root][INFO] - Training Epoch: 1/2, step 6066/23838 completed (loss: 0.48895227909088135, acc: 0.8742514848709106)
[2025-02-16 12:11:03,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:03,960][root][INFO] - Training Epoch: 1/2, step 6067/23838 completed (loss: 0.3982456922531128, acc: 0.8926174640655518)
[2025-02-16 12:11:04,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:04,356][root][INFO] - Training Epoch: 1/2, step 6068/23838 completed (loss: 0.5389339923858643, acc: 0.8333333134651184)
[2025-02-16 12:11:04,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:04,816][root][INFO] - Training Epoch: 1/2, step 6069/23838 completed (loss: 0.5023543238639832, acc: 0.8479999899864197)
[2025-02-16 12:11:04,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:05,231][root][INFO] - Training Epoch: 1/2, step 6070/23838 completed (loss: 0.4780523478984833, acc: 0.8712871074676514)
[2025-02-16 12:11:05,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:05,704][root][INFO] - Training Epoch: 1/2, step 6071/23838 completed (loss: 0.5351905822753906, acc: 0.8421052694320679)
[2025-02-16 12:11:05,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:06,179][root][INFO] - Training Epoch: 1/2, step 6072/23838 completed (loss: 0.48080602288246155, acc: 0.8592592477798462)
[2025-02-16 12:11:06,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:06,559][root][INFO] - Training Epoch: 1/2, step 6073/23838 completed (loss: 0.6300774812698364, acc: 0.8072289228439331)
[2025-02-16 12:11:06,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:07,175][root][INFO] - Training Epoch: 1/2, step 6074/23838 completed (loss: 0.44296517968177795, acc: 0.868686854839325)
[2025-02-16 12:11:07,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:07,588][root][INFO] - Training Epoch: 1/2, step 6075/23838 completed (loss: 0.46749669313430786, acc: 0.843478262424469)
[2025-02-16 12:11:07,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:07,950][root][INFO] - Training Epoch: 1/2, step 6076/23838 completed (loss: 0.8247753381729126, acc: 0.738095223903656)
[2025-02-16 12:11:08,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:08,331][root][INFO] - Training Epoch: 1/2, step 6077/23838 completed (loss: 0.6981777548789978, acc: 0.7816091775894165)
[2025-02-16 12:11:08,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:08,766][root][INFO] - Training Epoch: 1/2, step 6078/23838 completed (loss: 0.6613661646842957, acc: 0.8630136847496033)
[2025-02-16 12:11:08,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:09,213][root][INFO] - Training Epoch: 1/2, step 6079/23838 completed (loss: 0.5666409730911255, acc: 0.8271604776382446)
[2025-02-16 12:11:09,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:09,632][root][INFO] - Training Epoch: 1/2, step 6080/23838 completed (loss: 0.6290684938430786, acc: 0.8103448152542114)
[2025-02-16 12:11:09,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:10,073][root][INFO] - Training Epoch: 1/2, step 6081/23838 completed (loss: 0.9428742527961731, acc: 0.779411792755127)
[2025-02-16 12:11:10,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:10,532][root][INFO] - Training Epoch: 1/2, step 6082/23838 completed (loss: 0.5732890963554382, acc: 0.8061224222183228)
[2025-02-16 12:11:10,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:10,948][root][INFO] - Training Epoch: 1/2, step 6083/23838 completed (loss: 0.5438138246536255, acc: 0.8260869383811951)
[2025-02-16 12:11:11,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:11,335][root][INFO] - Training Epoch: 1/2, step 6084/23838 completed (loss: 0.443696528673172, acc: 0.8492063283920288)
[2025-02-16 12:11:11,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:11,695][root][INFO] - Training Epoch: 1/2, step 6085/23838 completed (loss: 0.5646982789039612, acc: 0.8292682766914368)
[2025-02-16 12:11:11,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:12,169][root][INFO] - Training Epoch: 1/2, step 6086/23838 completed (loss: 0.5634735822677612, acc: 0.8571428656578064)
[2025-02-16 12:11:12,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:12,587][root][INFO] - Training Epoch: 1/2, step 6087/23838 completed (loss: 0.7333559393882751, acc: 0.8260869383811951)
[2025-02-16 12:11:12,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:13,023][root][INFO] - Training Epoch: 1/2, step 6088/23838 completed (loss: 0.5330823659896851, acc: 0.8421052694320679)
[2025-02-16 12:11:13,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:13,472][root][INFO] - Training Epoch: 1/2, step 6089/23838 completed (loss: 0.6783534288406372, acc: 0.807692289352417)
[2025-02-16 12:11:13,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:13,911][root][INFO] - Training Epoch: 1/2, step 6090/23838 completed (loss: 0.5661919116973877, acc: 0.800000011920929)
[2025-02-16 12:11:14,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:14,307][root][INFO] - Training Epoch: 1/2, step 6091/23838 completed (loss: 0.7874621152877808, acc: 0.782608687877655)
[2025-02-16 12:11:14,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:14,754][root][INFO] - Training Epoch: 1/2, step 6092/23838 completed (loss: 0.9083333611488342, acc: 0.7699999809265137)
[2025-02-16 12:11:14,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:15,149][root][INFO] - Training Epoch: 1/2, step 6093/23838 completed (loss: 0.6339747309684753, acc: 0.8295454382896423)
[2025-02-16 12:11:15,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:15,581][root][INFO] - Training Epoch: 1/2, step 6094/23838 completed (loss: 0.7013189196586609, acc: 0.8039215803146362)
[2025-02-16 12:11:15,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:16,070][root][INFO] - Training Epoch: 1/2, step 6095/23838 completed (loss: 0.5816054344177246, acc: 0.8316831588745117)
[2025-02-16 12:11:16,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:16,505][root][INFO] - Training Epoch: 1/2, step 6096/23838 completed (loss: 0.5105547904968262, acc: 0.8539325594902039)
[2025-02-16 12:11:16,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:16,868][root][INFO] - Training Epoch: 1/2, step 6097/23838 completed (loss: 0.5266470313072205, acc: 0.8676470518112183)
[2025-02-16 12:11:17,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:17,298][root][INFO] - Training Epoch: 1/2, step 6098/23838 completed (loss: 0.5350522398948669, acc: 0.849056601524353)
[2025-02-16 12:11:17,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:17,778][root][INFO] - Training Epoch: 1/2, step 6099/23838 completed (loss: 0.41009196639060974, acc: 0.8615384697914124)
[2025-02-16 12:11:17,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:18,217][root][INFO] - Training Epoch: 1/2, step 6100/23838 completed (loss: 0.8304011821746826, acc: 0.7820512652397156)
[2025-02-16 12:11:18,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:18,629][root][INFO] - Training Epoch: 1/2, step 6101/23838 completed (loss: 0.6575613021850586, acc: 0.8219178318977356)
[2025-02-16 12:11:18,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:19,073][root][INFO] - Training Epoch: 1/2, step 6102/23838 completed (loss: 1.468310832977295, acc: 0.6440678238868713)
[2025-02-16 12:11:19,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:19,507][root][INFO] - Training Epoch: 1/2, step 6103/23838 completed (loss: 0.6484642624855042, acc: 0.821052610874176)
[2025-02-16 12:11:19,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:19,866][root][INFO] - Training Epoch: 1/2, step 6104/23838 completed (loss: 0.9213678240776062, acc: 0.7666666507720947)
[2025-02-16 12:11:20,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:20,296][root][INFO] - Training Epoch: 1/2, step 6105/23838 completed (loss: 0.8172325491905212, acc: 0.7884615659713745)
[2025-02-16 12:11:20,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:20,755][root][INFO] - Training Epoch: 1/2, step 6106/23838 completed (loss: 0.6006685495376587, acc: 0.8461538553237915)
[2025-02-16 12:11:20,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:21,224][root][INFO] - Training Epoch: 1/2, step 6107/23838 completed (loss: 0.5877152681350708, acc: 0.8484848737716675)
[2025-02-16 12:11:21,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:21,680][root][INFO] - Training Epoch: 1/2, step 6108/23838 completed (loss: 0.5118878483772278, acc: 0.8545454740524292)
[2025-02-16 12:11:21,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:22,136][root][INFO] - Training Epoch: 1/2, step 6109/23838 completed (loss: 0.5909703373908997, acc: 0.8235294222831726)
[2025-02-16 12:11:22,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:22,571][root][INFO] - Training Epoch: 1/2, step 6110/23838 completed (loss: 0.6290904879570007, acc: 0.8095238208770752)
[2025-02-16 12:11:22,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:22,954][root][INFO] - Training Epoch: 1/2, step 6111/23838 completed (loss: 0.6390535831451416, acc: 0.8294573426246643)
[2025-02-16 12:11:23,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:23,423][root][INFO] - Training Epoch: 1/2, step 6112/23838 completed (loss: 0.5719272494316101, acc: 0.8426966071128845)
[2025-02-16 12:11:23,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:23,873][root][INFO] - Training Epoch: 1/2, step 6113/23838 completed (loss: 0.5476399064064026, acc: 0.8775510191917419)
[2025-02-16 12:11:24,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:24,378][root][INFO] - Training Epoch: 1/2, step 6114/23838 completed (loss: 0.6624282002449036, acc: 0.8129496574401855)
[2025-02-16 12:11:24,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:24,760][root][INFO] - Training Epoch: 1/2, step 6115/23838 completed (loss: 0.5550763607025146, acc: 0.8510638475418091)
[2025-02-16 12:11:24,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:25,190][root][INFO] - Training Epoch: 1/2, step 6116/23838 completed (loss: 0.5982412099838257, acc: 0.8358209133148193)
[2025-02-16 12:11:25,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:25,902][root][INFO] - Training Epoch: 1/2, step 6117/23838 completed (loss: 1.1191412210464478, acc: 0.65625)
[2025-02-16 12:11:26,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:26,616][root][INFO] - Training Epoch: 1/2, step 6118/23838 completed (loss: 0.9017276763916016, acc: 0.7386363744735718)
[2025-02-16 12:11:26,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:27,087][root][INFO] - Training Epoch: 1/2, step 6119/23838 completed (loss: 1.0766313076019287, acc: 0.7037037014961243)
[2025-02-16 12:11:27,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:27,518][root][INFO] - Training Epoch: 1/2, step 6120/23838 completed (loss: 0.7253684997558594, acc: 0.8404255509376526)
[2025-02-16 12:11:27,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:27,903][root][INFO] - Training Epoch: 1/2, step 6121/23838 completed (loss: 0.9480879306793213, acc: 0.7108433842658997)
[2025-02-16 12:11:28,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:28,289][root][INFO] - Training Epoch: 1/2, step 6122/23838 completed (loss: 0.8175473809242249, acc: 0.7755101919174194)
[2025-02-16 12:11:28,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:28,722][root][INFO] - Training Epoch: 1/2, step 6123/23838 completed (loss: 0.889114499092102, acc: 0.7176470756530762)
[2025-02-16 12:11:28,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:29,127][root][INFO] - Training Epoch: 1/2, step 6124/23838 completed (loss: 1.1888130903244019, acc: 0.7051281929016113)
[2025-02-16 12:11:29,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:29,532][root][INFO] - Training Epoch: 1/2, step 6125/23838 completed (loss: 0.6039649844169617, acc: 0.7951807379722595)
[2025-02-16 12:11:29,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:29,931][root][INFO] - Training Epoch: 1/2, step 6126/23838 completed (loss: 0.4894944131374359, acc: 0.8522727489471436)
[2025-02-16 12:11:30,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:30,366][root][INFO] - Training Epoch: 1/2, step 6127/23838 completed (loss: 0.6128895878791809, acc: 0.7836257219314575)
[2025-02-16 12:11:30,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:30,766][root][INFO] - Training Epoch: 1/2, step 6128/23838 completed (loss: 0.36770349740982056, acc: 0.8904109597206116)
[2025-02-16 12:11:30,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:31,179][root][INFO] - Training Epoch: 1/2, step 6129/23838 completed (loss: 0.4993531107902527, acc: 0.890625)
[2025-02-16 12:11:31,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:31,558][root][INFO] - Training Epoch: 1/2, step 6130/23838 completed (loss: 0.5458338260650635, acc: 0.8799999952316284)
[2025-02-16 12:11:31,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:31,951][root][INFO] - Training Epoch: 1/2, step 6131/23838 completed (loss: 0.8774634003639221, acc: 0.7894737124443054)
[2025-02-16 12:11:32,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:32,426][root][INFO] - Training Epoch: 1/2, step 6132/23838 completed (loss: 0.8652556538581848, acc: 0.800000011920929)
[2025-02-16 12:11:32,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:32,845][root][INFO] - Training Epoch: 1/2, step 6133/23838 completed (loss: 0.5966224670410156, acc: 0.7910447716712952)
[2025-02-16 12:11:33,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:33,281][root][INFO] - Training Epoch: 1/2, step 6134/23838 completed (loss: 0.8152680397033691, acc: 0.8311688303947449)
[2025-02-16 12:11:33,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:33,704][root][INFO] - Training Epoch: 1/2, step 6135/23838 completed (loss: 0.7471188902854919, acc: 0.762499988079071)
[2025-02-16 12:11:33,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:34,086][root][INFO] - Training Epoch: 1/2, step 6136/23838 completed (loss: 0.9942439198493958, acc: 0.6960784196853638)
[2025-02-16 12:11:34,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:34,511][root][INFO] - Training Epoch: 1/2, step 6137/23838 completed (loss: 0.9297221302986145, acc: 0.7317073345184326)
[2025-02-16 12:11:34,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:34,930][root][INFO] - Training Epoch: 1/2, step 6138/23838 completed (loss: 0.6207485795021057, acc: 0.8313252925872803)
[2025-02-16 12:11:35,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:35,340][root][INFO] - Training Epoch: 1/2, step 6139/23838 completed (loss: 0.6800976395606995, acc: 0.804347813129425)
[2025-02-16 12:11:35,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:35,764][root][INFO] - Training Epoch: 1/2, step 6140/23838 completed (loss: 1.4715520143508911, acc: 0.6792452931404114)
[2025-02-16 12:11:35,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:36,148][root][INFO] - Training Epoch: 1/2, step 6141/23838 completed (loss: 0.29472383856773376, acc: 0.9152542352676392)
[2025-02-16 12:11:36,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:36,517][root][INFO] - Training Epoch: 1/2, step 6142/23838 completed (loss: 0.7269834280014038, acc: 0.7664233446121216)
[2025-02-16 12:11:36,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:36,887][root][INFO] - Training Epoch: 1/2, step 6143/23838 completed (loss: 0.40601322054862976, acc: 0.8846153616905212)
[2025-02-16 12:11:37,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:37,298][root][INFO] - Training Epoch: 1/2, step 6144/23838 completed (loss: 0.509522020816803, acc: 0.8666666746139526)
[2025-02-16 12:11:37,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:37,791][root][INFO] - Training Epoch: 1/2, step 6145/23838 completed (loss: 0.5537754893302917, acc: 0.8433734774589539)
[2025-02-16 12:11:37,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:38,219][root][INFO] - Training Epoch: 1/2, step 6146/23838 completed (loss: 0.3897063434123993, acc: 0.9024389982223511)
[2025-02-16 12:11:38,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:38,703][root][INFO] - Training Epoch: 1/2, step 6147/23838 completed (loss: 0.5188066959381104, acc: 0.831932783126831)
[2025-02-16 12:11:38,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:39,146][root][INFO] - Training Epoch: 1/2, step 6148/23838 completed (loss: 0.48081618547439575, acc: 0.8507462739944458)
[2025-02-16 12:11:39,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:39,573][root][INFO] - Training Epoch: 1/2, step 6149/23838 completed (loss: 0.9602729678153992, acc: 0.75)
[2025-02-16 12:11:39,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:40,060][root][INFO] - Training Epoch: 1/2, step 6150/23838 completed (loss: 0.703948438167572, acc: 0.7837837934494019)
[2025-02-16 12:11:40,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:40,506][root][INFO] - Training Epoch: 1/2, step 6151/23838 completed (loss: 0.41803687810897827, acc: 0.8879310488700867)
[2025-02-16 12:11:40,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:40,950][root][INFO] - Training Epoch: 1/2, step 6152/23838 completed (loss: 0.7603157162666321, acc: 0.7727272510528564)
[2025-02-16 12:11:41,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:41,411][root][INFO] - Training Epoch: 1/2, step 6153/23838 completed (loss: 1.175918698310852, acc: 0.656862735748291)
[2025-02-16 12:11:41,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:41,812][root][INFO] - Training Epoch: 1/2, step 6154/23838 completed (loss: 1.1447908878326416, acc: 0.699999988079071)
[2025-02-16 12:11:42,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:42,295][root][INFO] - Training Epoch: 1/2, step 6155/23838 completed (loss: 0.6550715565681458, acc: 0.8028169274330139)
[2025-02-16 12:11:42,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:42,760][root][INFO] - Training Epoch: 1/2, step 6156/23838 completed (loss: 0.7962017059326172, acc: 0.7599999904632568)
[2025-02-16 12:11:42,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:43,223][root][INFO] - Training Epoch: 1/2, step 6157/23838 completed (loss: 0.7921099066734314, acc: 0.7407407164573669)
[2025-02-16 12:11:43,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:43,841][root][INFO] - Training Epoch: 1/2, step 6158/23838 completed (loss: 0.5814377665519714, acc: 0.8441558480262756)
[2025-02-16 12:11:44,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:44,341][root][INFO] - Training Epoch: 1/2, step 6159/23838 completed (loss: 0.5158514976501465, acc: 0.8582677245140076)
[2025-02-16 12:11:44,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:44,785][root][INFO] - Training Epoch: 1/2, step 6160/23838 completed (loss: 0.7614333629608154, acc: 0.78899085521698)
[2025-02-16 12:11:45,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:45,266][root][INFO] - Training Epoch: 1/2, step 6161/23838 completed (loss: 0.6728053689002991, acc: 0.8155339956283569)
[2025-02-16 12:11:45,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:45,819][root][INFO] - Training Epoch: 1/2, step 6162/23838 completed (loss: 0.34526386857032776, acc: 0.8793103694915771)
[2025-02-16 12:11:46,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:46,258][root][INFO] - Training Epoch: 1/2, step 6163/23838 completed (loss: 0.6523054242134094, acc: 0.8260869383811951)
[2025-02-16 12:11:46,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:46,704][root][INFO] - Training Epoch: 1/2, step 6164/23838 completed (loss: 0.48119306564331055, acc: 0.8305084705352783)
[2025-02-16 12:11:46,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:47,138][root][INFO] - Training Epoch: 1/2, step 6165/23838 completed (loss: 1.1488219499588013, acc: 0.6547619104385376)
[2025-02-16 12:11:47,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:47,651][root][INFO] - Training Epoch: 1/2, step 6166/23838 completed (loss: 0.40210050344467163, acc: 0.8999999761581421)
[2025-02-16 12:11:47,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:48,099][root][INFO] - Training Epoch: 1/2, step 6167/23838 completed (loss: 0.5121119618415833, acc: 0.8163265585899353)
[2025-02-16 12:11:48,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:48,474][root][INFO] - Training Epoch: 1/2, step 6168/23838 completed (loss: 1.5991891622543335, acc: 0.6296296119689941)
[2025-02-16 12:11:48,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:48,973][root][INFO] - Training Epoch: 1/2, step 6169/23838 completed (loss: 0.435468465089798, acc: 0.8285714387893677)
[2025-02-16 12:11:49,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:49,351][root][INFO] - Training Epoch: 1/2, step 6170/23838 completed (loss: 0.8751657009124756, acc: 0.7254902124404907)
[2025-02-16 12:11:49,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:49,739][root][INFO] - Training Epoch: 1/2, step 6171/23838 completed (loss: 0.8160095810890198, acc: 0.75)
[2025-02-16 12:11:49,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:50,149][root][INFO] - Training Epoch: 1/2, step 6172/23838 completed (loss: 1.0589004755020142, acc: 0.6551724076271057)
[2025-02-16 12:11:50,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:50,594][root][INFO] - Training Epoch: 1/2, step 6173/23838 completed (loss: 0.8945633769035339, acc: 0.800000011920929)
[2025-02-16 12:11:50,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:50,983][root][INFO] - Training Epoch: 1/2, step 6174/23838 completed (loss: 1.3554245233535767, acc: 0.6078431606292725)
[2025-02-16 12:11:51,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:51,648][root][INFO] - Training Epoch: 1/2, step 6175/23838 completed (loss: 0.65913987159729, acc: 0.7956204414367676)
[2025-02-16 12:11:51,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:52,136][root][INFO] - Training Epoch: 1/2, step 6176/23838 completed (loss: 0.7451343536376953, acc: 0.8145695328712463)
[2025-02-16 12:11:52,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:52,570][root][INFO] - Training Epoch: 1/2, step 6177/23838 completed (loss: 0.8694542646408081, acc: 0.762499988079071)
[2025-02-16 12:11:52,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:52,958][root][INFO] - Training Epoch: 1/2, step 6178/23838 completed (loss: 0.4142325818538666, acc: 0.9142857193946838)
[2025-02-16 12:11:53,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:53,384][root][INFO] - Training Epoch: 1/2, step 6179/23838 completed (loss: 0.2995893955230713, acc: 0.9411764740943909)
[2025-02-16 12:11:53,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:53,818][root][INFO] - Training Epoch: 1/2, step 6180/23838 completed (loss: 0.7749274373054504, acc: 0.8095238208770752)
[2025-02-16 12:11:53,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:54,229][root][INFO] - Training Epoch: 1/2, step 6181/23838 completed (loss: 0.5693210363388062, acc: 0.8500000238418579)
[2025-02-16 12:11:54,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:54,603][root][INFO] - Training Epoch: 1/2, step 6182/23838 completed (loss: 0.39652639627456665, acc: 0.8846153616905212)
[2025-02-16 12:11:54,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:54,960][root][INFO] - Training Epoch: 1/2, step 6183/23838 completed (loss: 0.6956497430801392, acc: 0.8117647171020508)
[2025-02-16 12:11:55,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:55,886][root][INFO] - Training Epoch: 1/2, step 6184/23838 completed (loss: 0.4343428909778595, acc: 0.8938547372817993)
[2025-02-16 12:11:56,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:56,473][root][INFO] - Training Epoch: 1/2, step 6185/23838 completed (loss: 0.612565815448761, acc: 0.8484848737716675)
[2025-02-16 12:11:56,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:56,936][root][INFO] - Training Epoch: 1/2, step 6186/23838 completed (loss: 0.6980787515640259, acc: 0.800000011920929)
[2025-02-16 12:11:57,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:57,365][root][INFO] - Training Epoch: 1/2, step 6187/23838 completed (loss: 0.5569327473640442, acc: 0.8461538553237915)
[2025-02-16 12:11:57,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:58,117][root][INFO] - Training Epoch: 1/2, step 6188/23838 completed (loss: 0.9028887748718262, acc: 0.7606837749481201)
[2025-02-16 12:11:58,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:58,520][root][INFO] - Training Epoch: 1/2, step 6189/23838 completed (loss: 0.6024333834648132, acc: 0.8085106611251831)
[2025-02-16 12:11:58,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:58,961][root][INFO] - Training Epoch: 1/2, step 6190/23838 completed (loss: 0.7974646091461182, acc: 0.7638888955116272)
[2025-02-16 12:11:59,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:59,380][root][INFO] - Training Epoch: 1/2, step 6191/23838 completed (loss: 0.46984031796455383, acc: 0.8484848737716675)
[2025-02-16 12:11:59,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:11:59,785][root][INFO] - Training Epoch: 1/2, step 6192/23838 completed (loss: 0.638813316822052, acc: 0.8243243098258972)
[2025-02-16 12:11:59,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:00,186][root][INFO] - Training Epoch: 1/2, step 6193/23838 completed (loss: 0.6443674564361572, acc: 0.8493150472640991)
[2025-02-16 12:12:00,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:00,657][root][INFO] - Training Epoch: 1/2, step 6194/23838 completed (loss: 0.7432074546813965, acc: 0.8360655903816223)
[2025-02-16 12:12:00,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:01,146][root][INFO] - Training Epoch: 1/2, step 6195/23838 completed (loss: 0.4620073437690735, acc: 0.8899082541465759)
[2025-02-16 12:12:01,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:01,575][root][INFO] - Training Epoch: 1/2, step 6196/23838 completed (loss: 0.3708874583244324, acc: 0.9090909361839294)
[2025-02-16 12:12:01,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:02,043][root][INFO] - Training Epoch: 1/2, step 6197/23838 completed (loss: 0.43311506509780884, acc: 0.8918918967247009)
[2025-02-16 12:12:02,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:02,525][root][INFO] - Training Epoch: 1/2, step 6198/23838 completed (loss: 0.4260273575782776, acc: 0.8554216623306274)
[2025-02-16 12:12:02,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:03,040][root][INFO] - Training Epoch: 1/2, step 6199/23838 completed (loss: 0.7392367124557495, acc: 0.8131868243217468)
[2025-02-16 12:12:03,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:03,502][root][INFO] - Training Epoch: 1/2, step 6200/23838 completed (loss: 0.575602114200592, acc: 0.8275862336158752)
[2025-02-16 12:12:03,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:04,039][root][INFO] - Training Epoch: 1/2, step 6201/23838 completed (loss: 0.8455840945243835, acc: 0.7108433842658997)
[2025-02-16 12:12:04,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:04,490][root][INFO] - Training Epoch: 1/2, step 6202/23838 completed (loss: 0.6145051717758179, acc: 0.837837815284729)
[2025-02-16 12:12:04,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:04,939][root][INFO] - Training Epoch: 1/2, step 6203/23838 completed (loss: 0.5239542126655579, acc: 0.8409090638160706)
[2025-02-16 12:12:05,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:05,338][root][INFO] - Training Epoch: 1/2, step 6204/23838 completed (loss: 0.5731238126754761, acc: 0.8139534592628479)
[2025-02-16 12:12:05,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:05,767][root][INFO] - Training Epoch: 1/2, step 6205/23838 completed (loss: 0.5740783214569092, acc: 0.8285714387893677)
[2025-02-16 12:12:05,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:06,159][root][INFO] - Training Epoch: 1/2, step 6206/23838 completed (loss: 0.5899758338928223, acc: 0.8214285969734192)
[2025-02-16 12:12:06,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:06,524][root][INFO] - Training Epoch: 1/2, step 6207/23838 completed (loss: 0.32789045572280884, acc: 0.8734177350997925)
[2025-02-16 12:12:06,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:06,902][root][INFO] - Training Epoch: 1/2, step 6208/23838 completed (loss: 0.4151986539363861, acc: 0.8604651093482971)
[2025-02-16 12:12:07,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:07,312][root][INFO] - Training Epoch: 1/2, step 6209/23838 completed (loss: 0.5245807766914368, acc: 0.8681318759918213)
[2025-02-16 12:12:07,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:07,783][root][INFO] - Training Epoch: 1/2, step 6210/23838 completed (loss: 0.4740937054157257, acc: 0.8194444179534912)
[2025-02-16 12:12:07,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:08,211][root][INFO] - Training Epoch: 1/2, step 6211/23838 completed (loss: 0.39412441849708557, acc: 0.8913043737411499)
[2025-02-16 12:12:08,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:08,658][root][INFO] - Training Epoch: 1/2, step 6212/23838 completed (loss: 0.41407668590545654, acc: 0.90625)
[2025-02-16 12:12:08,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:09,099][root][INFO] - Training Epoch: 1/2, step 6213/23838 completed (loss: 0.5705767273902893, acc: 0.7735849022865295)
[2025-02-16 12:12:09,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:09,499][root][INFO] - Training Epoch: 1/2, step 6214/23838 completed (loss: 0.5539080500602722, acc: 0.8333333134651184)
[2025-02-16 12:12:09,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:09,987][root][INFO] - Training Epoch: 1/2, step 6215/23838 completed (loss: 0.49490392208099365, acc: 0.8541666865348816)
[2025-02-16 12:12:10,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:10,428][root][INFO] - Training Epoch: 1/2, step 6216/23838 completed (loss: 0.4840504229068756, acc: 0.8536585569381714)
[2025-02-16 12:12:10,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:10,835][root][INFO] - Training Epoch: 1/2, step 6217/23838 completed (loss: 0.5425822734832764, acc: 0.8309859037399292)
[2025-02-16 12:12:11,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:11,265][root][INFO] - Training Epoch: 1/2, step 6218/23838 completed (loss: 0.4349612295627594, acc: 0.9032257795333862)
[2025-02-16 12:12:11,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:11,683][root][INFO] - Training Epoch: 1/2, step 6219/23838 completed (loss: 0.5593662261962891, acc: 0.8623853325843811)
[2025-02-16 12:12:11,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:12,143][root][INFO] - Training Epoch: 1/2, step 6220/23838 completed (loss: 1.1972172260284424, acc: 0.7218044996261597)
[2025-02-16 12:12:12,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:12,576][root][INFO] - Training Epoch: 1/2, step 6221/23838 completed (loss: 1.0502101182937622, acc: 0.7307692170143127)
[2025-02-16 12:12:12,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:13,014][root][INFO] - Training Epoch: 1/2, step 6222/23838 completed (loss: 0.6140888333320618, acc: 0.8500000238418579)
[2025-02-16 12:12:13,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:13,444][root][INFO] - Training Epoch: 1/2, step 6223/23838 completed (loss: 0.5740682482719421, acc: 0.8405796885490417)
[2025-02-16 12:12:13,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:13,945][root][INFO] - Training Epoch: 1/2, step 6224/23838 completed (loss: 0.674729585647583, acc: 0.8229166865348816)
[2025-02-16 12:12:14,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:14,407][root][INFO] - Training Epoch: 1/2, step 6225/23838 completed (loss: 0.9841893911361694, acc: 0.7142857313156128)
[2025-02-16 12:12:14,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:14,835][root][INFO] - Training Epoch: 1/2, step 6226/23838 completed (loss: 0.9227349758148193, acc: 0.7803030014038086)
[2025-02-16 12:12:15,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:15,270][root][INFO] - Training Epoch: 1/2, step 6227/23838 completed (loss: 0.6670556664466858, acc: 0.8131868243217468)
[2025-02-16 12:12:15,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:15,660][root][INFO] - Training Epoch: 1/2, step 6228/23838 completed (loss: 0.8119668960571289, acc: 0.7745097875595093)
[2025-02-16 12:12:15,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:16,073][root][INFO] - Training Epoch: 1/2, step 6229/23838 completed (loss: 1.047144889831543, acc: 0.7176470756530762)
[2025-02-16 12:12:16,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:16,504][root][INFO] - Training Epoch: 1/2, step 6230/23838 completed (loss: 1.2025196552276611, acc: 0.6111111044883728)
[2025-02-16 12:12:16,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:16,938][root][INFO] - Training Epoch: 1/2, step 6231/23838 completed (loss: 0.779686450958252, acc: 0.7677419185638428)
[2025-02-16 12:12:17,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:17,337][root][INFO] - Training Epoch: 1/2, step 6232/23838 completed (loss: 0.58787602186203, acc: 0.834782600402832)
[2025-02-16 12:12:17,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:17,821][root][INFO] - Training Epoch: 1/2, step 6233/23838 completed (loss: 1.1430695056915283, acc: 0.695652186870575)
[2025-02-16 12:12:18,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:18,254][root][INFO] - Training Epoch: 1/2, step 6234/23838 completed (loss: 0.8297950029373169, acc: 0.7450980544090271)
[2025-02-16 12:12:18,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:18,714][root][INFO] - Training Epoch: 1/2, step 6235/23838 completed (loss: 1.1531487703323364, acc: 0.699999988079071)
[2025-02-16 12:12:18,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:19,151][root][INFO] - Training Epoch: 1/2, step 6236/23838 completed (loss: 0.7822579741477966, acc: 0.7746478915214539)
[2025-02-16 12:12:19,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:19,557][root][INFO] - Training Epoch: 1/2, step 6237/23838 completed (loss: 1.105918288230896, acc: 0.7196261882781982)
[2025-02-16 12:12:19,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:19,938][root][INFO] - Training Epoch: 1/2, step 6238/23838 completed (loss: 1.0421769618988037, acc: 0.6868686676025391)
[2025-02-16 12:12:20,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:20,399][root][INFO] - Training Epoch: 1/2, step 6239/23838 completed (loss: 0.8676934838294983, acc: 0.7910447716712952)
[2025-02-16 12:12:20,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:20,823][root][INFO] - Training Epoch: 1/2, step 6240/23838 completed (loss: 0.9426206350326538, acc: 0.7326732873916626)
[2025-02-16 12:12:21,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:21,305][root][INFO] - Training Epoch: 1/2, step 6241/23838 completed (loss: 0.870330810546875, acc: 0.7727272510528564)
[2025-02-16 12:12:21,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:21,738][root][INFO] - Training Epoch: 1/2, step 6242/23838 completed (loss: 1.5756789445877075, acc: 0.5811966061592102)
[2025-02-16 12:12:21,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:22,197][root][INFO] - Training Epoch: 1/2, step 6243/23838 completed (loss: 0.49488744139671326, acc: 0.8285714387893677)
[2025-02-16 12:12:22,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:22,652][root][INFO] - Training Epoch: 1/2, step 6244/23838 completed (loss: 1.0965287685394287, acc: 0.6767676472663879)
[2025-02-16 12:12:22,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:23,101][root][INFO] - Training Epoch: 1/2, step 6245/23838 completed (loss: 1.3193315267562866, acc: 0.6800000071525574)
[2025-02-16 12:12:23,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:23,580][root][INFO] - Training Epoch: 1/2, step 6246/23838 completed (loss: 1.0290638208389282, acc: 0.7142857313156128)
[2025-02-16 12:12:23,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:24,022][root][INFO] - Training Epoch: 1/2, step 6247/23838 completed (loss: 0.8999847769737244, acc: 0.7542372941970825)
[2025-02-16 12:12:24,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:24,456][root][INFO] - Training Epoch: 1/2, step 6248/23838 completed (loss: 0.42670556902885437, acc: 0.8951612710952759)
[2025-02-16 12:12:24,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:24,917][root][INFO] - Training Epoch: 1/2, step 6249/23838 completed (loss: 0.6430774331092834, acc: 0.8314606547355652)
[2025-02-16 12:12:25,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:25,423][root][INFO] - Training Epoch: 1/2, step 6250/23838 completed (loss: 0.6335989236831665, acc: 0.8255033493041992)
[2025-02-16 12:12:25,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:25,860][root][INFO] - Training Epoch: 1/2, step 6251/23838 completed (loss: 0.7465236783027649, acc: 0.8090909123420715)
[2025-02-16 12:12:26,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:26,368][root][INFO] - Training Epoch: 1/2, step 6252/23838 completed (loss: 1.1046078205108643, acc: 0.7435897588729858)
[2025-02-16 12:12:26,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:26,852][root][INFO] - Training Epoch: 1/2, step 6253/23838 completed (loss: 0.5609333515167236, acc: 0.84112149477005)
[2025-02-16 12:12:27,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:27,328][root][INFO] - Training Epoch: 1/2, step 6254/23838 completed (loss: 0.7289526462554932, acc: 0.7967479825019836)
[2025-02-16 12:12:27,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:27,776][root][INFO] - Training Epoch: 1/2, step 6255/23838 completed (loss: 1.082338809967041, acc: 0.7142857313156128)
[2025-02-16 12:12:28,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:28,256][root][INFO] - Training Epoch: 1/2, step 6256/23838 completed (loss: 0.9112321138381958, acc: 0.7413793206214905)
[2025-02-16 12:12:28,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:28,687][root][INFO] - Training Epoch: 1/2, step 6257/23838 completed (loss: 0.5360076427459717, acc: 0.8345323801040649)
[2025-02-16 12:12:28,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:29,196][root][INFO] - Training Epoch: 1/2, step 6258/23838 completed (loss: 0.33640655875205994, acc: 0.918181836605072)
[2025-02-16 12:12:29,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:29,615][root][INFO] - Training Epoch: 1/2, step 6259/23838 completed (loss: 1.5085746049880981, acc: 0.5632184147834778)
[2025-02-16 12:12:29,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:30,012][root][INFO] - Training Epoch: 1/2, step 6260/23838 completed (loss: 1.101328730583191, acc: 0.7209302186965942)
[2025-02-16 12:12:30,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:30,514][root][INFO] - Training Epoch: 1/2, step 6261/23838 completed (loss: 0.5646891593933105, acc: 0.8632478713989258)
[2025-02-16 12:12:30,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:30,926][root][INFO] - Training Epoch: 1/2, step 6262/23838 completed (loss: 0.7444633841514587, acc: 0.8095238208770752)
[2025-02-16 12:12:31,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:31,396][root][INFO] - Training Epoch: 1/2, step 6263/23838 completed (loss: 0.850726842880249, acc: 0.8080000281333923)
[2025-02-16 12:12:31,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:31,790][root][INFO] - Training Epoch: 1/2, step 6264/23838 completed (loss: 0.9619002342224121, acc: 0.75)
[2025-02-16 12:12:31,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:32,240][root][INFO] - Training Epoch: 1/2, step 6265/23838 completed (loss: 0.5128440260887146, acc: 0.8531468510627747)
[2025-02-16 12:12:32,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:32,680][root][INFO] - Training Epoch: 1/2, step 6266/23838 completed (loss: 0.650926947593689, acc: 0.8222222328186035)
[2025-02-16 12:12:32,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:33,161][root][INFO] - Training Epoch: 1/2, step 6267/23838 completed (loss: 0.3998681604862213, acc: 0.8877550959587097)
[2025-02-16 12:12:33,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:33,604][root][INFO] - Training Epoch: 1/2, step 6268/23838 completed (loss: 0.6651884913444519, acc: 0.8095238208770752)
[2025-02-16 12:12:33,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:34,011][root][INFO] - Training Epoch: 1/2, step 6269/23838 completed (loss: 0.9352813363075256, acc: 0.7164179086685181)
[2025-02-16 12:12:34,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:34,467][root][INFO] - Training Epoch: 1/2, step 6270/23838 completed (loss: 0.977243185043335, acc: 0.7709923386573792)
[2025-02-16 12:12:34,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:34,914][root][INFO] - Training Epoch: 1/2, step 6271/23838 completed (loss: 0.5640179514884949, acc: 0.805084764957428)
[2025-02-16 12:12:35,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:35,345][root][INFO] - Training Epoch: 1/2, step 6272/23838 completed (loss: 0.5335807204246521, acc: 0.8360655903816223)
[2025-02-16 12:12:35,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:35,836][root][INFO] - Training Epoch: 1/2, step 6273/23838 completed (loss: 0.8230762481689453, acc: 0.7962962985038757)
[2025-02-16 12:12:35,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:36,206][root][INFO] - Training Epoch: 1/2, step 6274/23838 completed (loss: 1.356956124305725, acc: 0.5945945978164673)
[2025-02-16 12:12:36,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:36,592][root][INFO] - Training Epoch: 1/2, step 6275/23838 completed (loss: 1.2229782342910767, acc: 0.6129032373428345)
[2025-02-16 12:12:36,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:36,990][root][INFO] - Training Epoch: 1/2, step 6276/23838 completed (loss: 1.7442466020584106, acc: 0.5833333134651184)
[2025-02-16 12:12:37,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:37,420][root][INFO] - Training Epoch: 1/2, step 6277/23838 completed (loss: 0.7419407367706299, acc: 0.8333333134651184)
[2025-02-16 12:12:37,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:38,021][root][INFO] - Training Epoch: 1/2, step 6278/23838 completed (loss: 0.4235953390598297, acc: 0.8851351141929626)
[2025-02-16 12:12:38,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:38,400][root][INFO] - Training Epoch: 1/2, step 6279/23838 completed (loss: 0.399489164352417, acc: 0.8876404762268066)
[2025-02-16 12:12:38,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:38,826][root][INFO] - Training Epoch: 1/2, step 6280/23838 completed (loss: 0.4240451455116272, acc: 0.8478260636329651)
[2025-02-16 12:12:39,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:39,227][root][INFO] - Training Epoch: 1/2, step 6281/23838 completed (loss: 0.41925716400146484, acc: 0.8888888955116272)
[2025-02-16 12:12:39,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:39,619][root][INFO] - Training Epoch: 1/2, step 6282/23838 completed (loss: 0.5965294241905212, acc: 0.8131868243217468)
[2025-02-16 12:12:39,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:40,039][root][INFO] - Training Epoch: 1/2, step 6283/23838 completed (loss: 1.101521611213684, acc: 0.688524603843689)
[2025-02-16 12:12:40,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:40,439][root][INFO] - Training Epoch: 1/2, step 6284/23838 completed (loss: 0.592722475528717, acc: 0.8169013857841492)
[2025-02-16 12:12:40,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:40,881][root][INFO] - Training Epoch: 1/2, step 6285/23838 completed (loss: 0.6690700054168701, acc: 0.800000011920929)
[2025-02-16 12:12:41,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:41,271][root][INFO] - Training Epoch: 1/2, step 6286/23838 completed (loss: 0.4730064868927002, acc: 0.8877550959587097)
[2025-02-16 12:12:41,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:41,659][root][INFO] - Training Epoch: 1/2, step 6287/23838 completed (loss: 1.1473349332809448, acc: 0.6788991093635559)
[2025-02-16 12:12:41,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:42,099][root][INFO] - Training Epoch: 1/2, step 6288/23838 completed (loss: 0.4965426027774811, acc: 0.8484848737716675)
[2025-02-16 12:12:42,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:42,468][root][INFO] - Training Epoch: 1/2, step 6289/23838 completed (loss: 1.667068362236023, acc: 0.5675675868988037)
[2025-02-16 12:12:42,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:42,962][root][INFO] - Training Epoch: 1/2, step 6290/23838 completed (loss: 1.0878658294677734, acc: 0.6944444179534912)
[2025-02-16 12:12:43,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:43,410][root][INFO] - Training Epoch: 1/2, step 6291/23838 completed (loss: 0.8150122165679932, acc: 0.7570093274116516)
[2025-02-16 12:12:43,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:43,830][root][INFO] - Training Epoch: 1/2, step 6292/23838 completed (loss: 0.6067878007888794, acc: 0.8349514603614807)
[2025-02-16 12:12:44,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:44,248][root][INFO] - Training Epoch: 1/2, step 6293/23838 completed (loss: 0.830077052116394, acc: 0.8390804529190063)
[2025-02-16 12:12:44,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:44,684][root][INFO] - Training Epoch: 1/2, step 6294/23838 completed (loss: 1.0137743949890137, acc: 0.7132353186607361)
[2025-02-16 12:12:44,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:45,167][root][INFO] - Training Epoch: 1/2, step 6295/23838 completed (loss: 0.7570776343345642, acc: 0.767241358757019)
[2025-02-16 12:12:45,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:45,694][root][INFO] - Training Epoch: 1/2, step 6296/23838 completed (loss: 0.6498185396194458, acc: 0.8192771077156067)
[2025-02-16 12:12:45,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:46,096][root][INFO] - Training Epoch: 1/2, step 6297/23838 completed (loss: 0.6565365195274353, acc: 0.8148148059844971)
[2025-02-16 12:12:46,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:46,471][root][INFO] - Training Epoch: 1/2, step 6298/23838 completed (loss: 1.2817367315292358, acc: 0.6197183132171631)
[2025-02-16 12:12:46,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:46,848][root][INFO] - Training Epoch: 1/2, step 6299/23838 completed (loss: 1.0998737812042236, acc: 0.7272727489471436)
[2025-02-16 12:12:47,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:47,268][root][INFO] - Training Epoch: 1/2, step 6300/23838 completed (loss: 0.7947790026664734, acc: 0.7676056623458862)
[2025-02-16 12:12:47,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:47,733][root][INFO] - Training Epoch: 1/2, step 6301/23838 completed (loss: 1.1145800352096558, acc: 0.6842105388641357)
[2025-02-16 12:12:47,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:48,147][root][INFO] - Training Epoch: 1/2, step 6302/23838 completed (loss: 0.7566028833389282, acc: 0.8057143092155457)
[2025-02-16 12:12:48,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:48,529][root][INFO] - Training Epoch: 1/2, step 6303/23838 completed (loss: 0.28798115253448486, acc: 0.9200000166893005)
[2025-02-16 12:12:48,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:48,940][root][INFO] - Training Epoch: 1/2, step 6304/23838 completed (loss: 0.976235032081604, acc: 0.7310344576835632)
[2025-02-16 12:12:49,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:49,425][root][INFO] - Training Epoch: 1/2, step 6305/23838 completed (loss: 0.9645493626594543, acc: 0.761904776096344)
[2025-02-16 12:12:49,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:49,846][root][INFO] - Training Epoch: 1/2, step 6306/23838 completed (loss: 0.7591889500617981, acc: 0.800000011920929)
[2025-02-16 12:12:50,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:50,298][root][INFO] - Training Epoch: 1/2, step 6307/23838 completed (loss: 1.2812106609344482, acc: 0.6290322542190552)
[2025-02-16 12:12:50,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:50,740][root][INFO] - Training Epoch: 1/2, step 6308/23838 completed (loss: 0.8228210806846619, acc: 0.7734806537628174)
[2025-02-16 12:12:50,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:51,187][root][INFO] - Training Epoch: 1/2, step 6309/23838 completed (loss: 0.762869656085968, acc: 0.8055555820465088)
[2025-02-16 12:12:51,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:51,647][root][INFO] - Training Epoch: 1/2, step 6310/23838 completed (loss: 0.604786217212677, acc: 0.8217821717262268)
[2025-02-16 12:12:51,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:52,117][root][INFO] - Training Epoch: 1/2, step 6311/23838 completed (loss: 1.0231914520263672, acc: 0.7349397540092468)
[2025-02-16 12:12:52,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:52,578][root][INFO] - Training Epoch: 1/2, step 6312/23838 completed (loss: 0.7273534536361694, acc: 0.8017241358757019)
[2025-02-16 12:12:52,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:53,005][root][INFO] - Training Epoch: 1/2, step 6313/23838 completed (loss: 0.9160996675491333, acc: 0.7400000095367432)
[2025-02-16 12:12:53,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:53,389][root][INFO] - Training Epoch: 1/2, step 6314/23838 completed (loss: 0.6124463081359863, acc: 0.8290598392486572)
[2025-02-16 12:12:53,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:53,787][root][INFO] - Training Epoch: 1/2, step 6315/23838 completed (loss: 0.4082871079444885, acc: 0.8671875)
[2025-02-16 12:12:54,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:54,264][root][INFO] - Training Epoch: 1/2, step 6316/23838 completed (loss: 0.9032068848609924, acc: 0.6982758641242981)
[2025-02-16 12:12:54,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:54,702][root][INFO] - Training Epoch: 1/2, step 6317/23838 completed (loss: 0.7846496105194092, acc: 0.7692307829856873)
[2025-02-16 12:12:54,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:55,151][root][INFO] - Training Epoch: 1/2, step 6318/23838 completed (loss: 0.8853816390037537, acc: 0.7264957427978516)
[2025-02-16 12:12:55,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:55,586][root][INFO] - Training Epoch: 1/2, step 6319/23838 completed (loss: 0.5712067484855652, acc: 0.8354430198669434)
[2025-02-16 12:12:55,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:56,014][root][INFO] - Training Epoch: 1/2, step 6320/23838 completed (loss: 1.142383337020874, acc: 0.698630154132843)
[2025-02-16 12:12:56,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:56,457][root][INFO] - Training Epoch: 1/2, step 6321/23838 completed (loss: 1.2000713348388672, acc: 0.6395348906517029)
[2025-02-16 12:12:56,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:56,935][root][INFO] - Training Epoch: 1/2, step 6322/23838 completed (loss: 0.6165781617164612, acc: 0.800000011920929)
[2025-02-16 12:12:57,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:57,412][root][INFO] - Training Epoch: 1/2, step 6323/23838 completed (loss: 0.6901395916938782, acc: 0.7941176295280457)
[2025-02-16 12:12:57,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:57,853][root][INFO] - Training Epoch: 1/2, step 6324/23838 completed (loss: 0.7167145609855652, acc: 0.8012820482254028)
[2025-02-16 12:12:58,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:58,262][root][INFO] - Training Epoch: 1/2, step 6325/23838 completed (loss: 0.5506231188774109, acc: 0.8672566413879395)
[2025-02-16 12:12:58,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:58,697][root][INFO] - Training Epoch: 1/2, step 6326/23838 completed (loss: 0.5324329137802124, acc: 0.8512396812438965)
[2025-02-16 12:12:58,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:59,034][root][INFO] - Training Epoch: 1/2, step 6327/23838 completed (loss: 0.6768223643302917, acc: 0.8399999737739563)
[2025-02-16 12:12:59,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:59,484][root][INFO] - Training Epoch: 1/2, step 6328/23838 completed (loss: 0.94896399974823, acc: 0.7244094610214233)
[2025-02-16 12:12:59,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:12:59,955][root][INFO] - Training Epoch: 1/2, step 6329/23838 completed (loss: 0.7742696404457092, acc: 0.7803030014038086)
[2025-02-16 12:13:00,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:00,388][root][INFO] - Training Epoch: 1/2, step 6330/23838 completed (loss: 0.6639296412467957, acc: 0.8349514603614807)
[2025-02-16 12:13:00,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:00,837][root][INFO] - Training Epoch: 1/2, step 6331/23838 completed (loss: 0.6277767419815063, acc: 0.7977527976036072)
[2025-02-16 12:13:01,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:01,262][root][INFO] - Training Epoch: 1/2, step 6332/23838 completed (loss: 0.7171846628189087, acc: 0.8169013857841492)
[2025-02-16 12:13:01,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:01,666][root][INFO] - Training Epoch: 1/2, step 6333/23838 completed (loss: 0.7151166796684265, acc: 0.8086419701576233)
[2025-02-16 12:13:01,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:02,106][root][INFO] - Training Epoch: 1/2, step 6334/23838 completed (loss: 1.0243031978607178, acc: 0.75)
[2025-02-16 12:13:02,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:02,463][root][INFO] - Training Epoch: 1/2, step 6335/23838 completed (loss: 0.7891230583190918, acc: 0.7872340679168701)
[2025-02-16 12:13:02,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:02,870][root][INFO] - Training Epoch: 1/2, step 6336/23838 completed (loss: 0.9774267077445984, acc: 0.6610169410705566)
[2025-02-16 12:13:03,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:03,255][root][INFO] - Training Epoch: 1/2, step 6337/23838 completed (loss: 0.8389992117881775, acc: 0.75)
[2025-02-16 12:13:03,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:03,639][root][INFO] - Training Epoch: 1/2, step 6338/23838 completed (loss: 0.6916667222976685, acc: 0.7809523940086365)
[2025-02-16 12:13:03,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:04,051][root][INFO] - Training Epoch: 1/2, step 6339/23838 completed (loss: 0.8165701627731323, acc: 0.7706422209739685)
[2025-02-16 12:13:04,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:04,467][root][INFO] - Training Epoch: 1/2, step 6340/23838 completed (loss: 0.7316451072692871, acc: 0.7964601516723633)
[2025-02-16 12:13:04,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:04,829][root][INFO] - Training Epoch: 1/2, step 6341/23838 completed (loss: 0.6044787168502808, acc: 0.800000011920929)
[2025-02-16 12:13:05,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:05,265][root][INFO] - Training Epoch: 1/2, step 6342/23838 completed (loss: 0.6069376468658447, acc: 0.8058252334594727)
[2025-02-16 12:13:05,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:05,707][root][INFO] - Training Epoch: 1/2, step 6343/23838 completed (loss: 1.3499107360839844, acc: 0.578125)
[2025-02-16 12:13:05,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:06,136][root][INFO] - Training Epoch: 1/2, step 6344/23838 completed (loss: 0.6883446574211121, acc: 0.7234042286872864)
[2025-02-16 12:13:06,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:06,539][root][INFO] - Training Epoch: 1/2, step 6345/23838 completed (loss: 0.9691521525382996, acc: 0.7113401889801025)
[2025-02-16 12:13:06,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:07,008][root][INFO] - Training Epoch: 1/2, step 6346/23838 completed (loss: 0.6652953624725342, acc: 0.8429751992225647)
[2025-02-16 12:13:07,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:07,435][root][INFO] - Training Epoch: 1/2, step 6347/23838 completed (loss: 0.9813612103462219, acc: 0.7634408473968506)
[2025-02-16 12:13:07,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:07,870][root][INFO] - Training Epoch: 1/2, step 6348/23838 completed (loss: 0.5823724865913391, acc: 0.7961165308952332)
[2025-02-16 12:13:08,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:08,286][root][INFO] - Training Epoch: 1/2, step 6349/23838 completed (loss: 0.5221829414367676, acc: 0.8399999737739563)
[2025-02-16 12:13:08,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:08,645][root][INFO] - Training Epoch: 1/2, step 6350/23838 completed (loss: 0.41462191939353943, acc: 0.9677419066429138)
[2025-02-16 12:13:08,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:09,072][root][INFO] - Training Epoch: 1/2, step 6351/23838 completed (loss: 0.7033940553665161, acc: 0.795918345451355)
[2025-02-16 12:13:09,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:09,527][root][INFO] - Training Epoch: 1/2, step 6352/23838 completed (loss: 0.5278686881065369, acc: 0.8055555820465088)
[2025-02-16 12:13:09,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:09,993][root][INFO] - Training Epoch: 1/2, step 6353/23838 completed (loss: 0.6669942140579224, acc: 0.8287671208381653)
[2025-02-16 12:13:10,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:10,377][root][INFO] - Training Epoch: 1/2, step 6354/23838 completed (loss: 0.4180096387863159, acc: 0.9029850959777832)
[2025-02-16 12:13:10,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:10,803][root][INFO] - Training Epoch: 1/2, step 6355/23838 completed (loss: 0.5521224141120911, acc: 0.8191489577293396)
[2025-02-16 12:13:11,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:11,247][root][INFO] - Training Epoch: 1/2, step 6356/23838 completed (loss: 0.8535048365592957, acc: 0.7730061411857605)
[2025-02-16 12:13:11,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:11,604][root][INFO] - Training Epoch: 1/2, step 6357/23838 completed (loss: 0.5656068921089172, acc: 0.8301886916160583)
[2025-02-16 12:13:11,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:12,008][root][INFO] - Training Epoch: 1/2, step 6358/23838 completed (loss: 1.1368718147277832, acc: 0.6693548560142517)
[2025-02-16 12:13:12,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:12,452][root][INFO] - Training Epoch: 1/2, step 6359/23838 completed (loss: 0.4355034828186035, acc: 0.8598130941390991)
[2025-02-16 12:13:12,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:12,817][root][INFO] - Training Epoch: 1/2, step 6360/23838 completed (loss: 0.6579564809799194, acc: 0.8395061492919922)
[2025-02-16 12:13:12,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:13,216][root][INFO] - Training Epoch: 1/2, step 6361/23838 completed (loss: 0.6087055802345276, acc: 0.8222222328186035)
[2025-02-16 12:13:13,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:13,640][root][INFO] - Training Epoch: 1/2, step 6362/23838 completed (loss: 1.0564534664154053, acc: 0.6285714507102966)
[2025-02-16 12:13:13,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:14,010][root][INFO] - Training Epoch: 1/2, step 6363/23838 completed (loss: 0.7942007780075073, acc: 0.7321428656578064)
[2025-02-16 12:13:14,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:14,391][root][INFO] - Training Epoch: 1/2, step 6364/23838 completed (loss: 0.811622679233551, acc: 0.7977527976036072)
[2025-02-16 12:13:14,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:14,816][root][INFO] - Training Epoch: 1/2, step 6365/23838 completed (loss: 0.857082188129425, acc: 0.7956989407539368)
[2025-02-16 12:13:15,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:15,257][root][INFO] - Training Epoch: 1/2, step 6366/23838 completed (loss: 1.2835956811904907, acc: 0.6266666650772095)
[2025-02-16 12:13:15,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:15,662][root][INFO] - Training Epoch: 1/2, step 6367/23838 completed (loss: 1.094929575920105, acc: 0.6756756901741028)
[2025-02-16 12:13:15,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:16,105][root][INFO] - Training Epoch: 1/2, step 6368/23838 completed (loss: 1.312239646911621, acc: 0.6101694703102112)
[2025-02-16 12:13:16,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:16,569][root][INFO] - Training Epoch: 1/2, step 6369/23838 completed (loss: 1.4264010190963745, acc: 0.5901639461517334)
[2025-02-16 12:13:16,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:17,441][root][INFO] - Training Epoch: 1/2, step 6370/23838 completed (loss: 0.8205677270889282, acc: 0.7612612843513489)
[2025-02-16 12:13:17,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:17,815][root][INFO] - Training Epoch: 1/2, step 6371/23838 completed (loss: 0.6110102534294128, acc: 0.8602150678634644)
[2025-02-16 12:13:17,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:18,228][root][INFO] - Training Epoch: 1/2, step 6372/23838 completed (loss: 0.8516730070114136, acc: 0.7564102411270142)
[2025-02-16 12:13:18,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:18,680][root][INFO] - Training Epoch: 1/2, step 6373/23838 completed (loss: 1.4990683794021606, acc: 0.6666666865348816)
[2025-02-16 12:13:18,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:19,131][root][INFO] - Training Epoch: 1/2, step 6374/23838 completed (loss: 1.4107792377471924, acc: 0.6399999856948853)
[2025-02-16 12:13:19,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:19,597][root][INFO] - Training Epoch: 1/2, step 6375/23838 completed (loss: 1.6445670127868652, acc: 0.5680473446846008)
[2025-02-16 12:13:19,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:20,040][root][INFO] - Training Epoch: 1/2, step 6376/23838 completed (loss: 1.1217546463012695, acc: 0.692307710647583)
[2025-02-16 12:13:20,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:20,486][root][INFO] - Training Epoch: 1/2, step 6377/23838 completed (loss: 1.2088810205459595, acc: 0.6037735939025879)
[2025-02-16 12:13:20,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:20,854][root][INFO] - Training Epoch: 1/2, step 6378/23838 completed (loss: 0.8369160890579224, acc: 0.7745097875595093)
[2025-02-16 12:13:21,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:21,265][root][INFO] - Training Epoch: 1/2, step 6379/23838 completed (loss: 0.9595888257026672, acc: 0.7120000123977661)
[2025-02-16 12:13:21,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:21,709][root][INFO] - Training Epoch: 1/2, step 6380/23838 completed (loss: 0.9431827068328857, acc: 0.7247706651687622)
[2025-02-16 12:13:21,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:22,131][root][INFO] - Training Epoch: 1/2, step 6381/23838 completed (loss: 0.9694987535476685, acc: 0.7372881174087524)
[2025-02-16 12:13:22,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:22,589][root][INFO] - Training Epoch: 1/2, step 6382/23838 completed (loss: 0.9309921264648438, acc: 0.7244094610214233)
[2025-02-16 12:13:22,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:23,037][root][INFO] - Training Epoch: 1/2, step 6383/23838 completed (loss: 1.0194571018218994, acc: 0.6610169410705566)
[2025-02-16 12:13:23,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:23,501][root][INFO] - Training Epoch: 1/2, step 6384/23838 completed (loss: 0.9632702469825745, acc: 0.7477477192878723)
[2025-02-16 12:13:23,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:23,893][root][INFO] - Training Epoch: 1/2, step 6385/23838 completed (loss: 0.6466072201728821, acc: 0.7627118825912476)
[2025-02-16 12:13:24,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:24,277][root][INFO] - Training Epoch: 1/2, step 6386/23838 completed (loss: 0.6940856575965881, acc: 0.7924528121948242)
[2025-02-16 12:13:24,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:24,704][root][INFO] - Training Epoch: 1/2, step 6387/23838 completed (loss: 1.2562741041183472, acc: 0.594936728477478)
[2025-02-16 12:13:24,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:25,167][root][INFO] - Training Epoch: 1/2, step 6388/23838 completed (loss: 0.29042989015579224, acc: 0.9396551847457886)
[2025-02-16 12:13:25,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:25,614][root][INFO] - Training Epoch: 1/2, step 6389/23838 completed (loss: 0.9577580094337463, acc: 0.7558139562606812)
[2025-02-16 12:13:25,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:26,004][root][INFO] - Training Epoch: 1/2, step 6390/23838 completed (loss: 0.8723833560943604, acc: 0.7931034564971924)
[2025-02-16 12:13:26,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:26,412][root][INFO] - Training Epoch: 1/2, step 6391/23838 completed (loss: 0.7438523769378662, acc: 0.75)
[2025-02-16 12:13:26,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:26,834][root][INFO] - Training Epoch: 1/2, step 6392/23838 completed (loss: 1.074639081954956, acc: 0.719298243522644)
[2025-02-16 12:13:27,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:27,288][root][INFO] - Training Epoch: 1/2, step 6393/23838 completed (loss: 0.7045259475708008, acc: 0.807692289352417)
[2025-02-16 12:13:27,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:27,657][root][INFO] - Training Epoch: 1/2, step 6394/23838 completed (loss: 0.852733314037323, acc: 0.7549019455909729)
[2025-02-16 12:13:27,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:28,097][root][INFO] - Training Epoch: 1/2, step 6395/23838 completed (loss: 0.6875128746032715, acc: 0.7669903039932251)
[2025-02-16 12:13:28,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:28,531][root][INFO] - Training Epoch: 1/2, step 6396/23838 completed (loss: 0.6587939262390137, acc: 0.849056601524353)
[2025-02-16 12:13:28,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:28,962][root][INFO] - Training Epoch: 1/2, step 6397/23838 completed (loss: 0.8008144497871399, acc: 0.7663551568984985)
[2025-02-16 12:13:29,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:29,364][root][INFO] - Training Epoch: 1/2, step 6398/23838 completed (loss: 0.6244081854820251, acc: 0.8214285969734192)
[2025-02-16 12:13:29,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:29,794][root][INFO] - Training Epoch: 1/2, step 6399/23838 completed (loss: 1.2536404132843018, acc: 0.6666666865348816)
[2025-02-16 12:13:30,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:30,208][root][INFO] - Training Epoch: 1/2, step 6400/23838 completed (loss: 0.6054935455322266, acc: 0.8199999928474426)
[2025-02-16 12:13:30,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:30,576][root][INFO] - Training Epoch: 1/2, step 6401/23838 completed (loss: 0.5181086659431458, acc: 0.8550724387168884)
[2025-02-16 12:13:30,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:30,939][root][INFO] - Training Epoch: 1/2, step 6402/23838 completed (loss: 0.35464364290237427, acc: 0.9047619104385376)
[2025-02-16 12:13:31,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:31,353][root][INFO] - Training Epoch: 1/2, step 6403/23838 completed (loss: 0.8514108657836914, acc: 0.7263157963752747)
[2025-02-16 12:13:31,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:31,836][root][INFO] - Training Epoch: 1/2, step 6404/23838 completed (loss: 0.7054192423820496, acc: 0.7529411911964417)
[2025-02-16 12:13:32,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:32,265][root][INFO] - Training Epoch: 1/2, step 6405/23838 completed (loss: 0.4385848939418793, acc: 0.890625)
[2025-02-16 12:13:32,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:32,641][root][INFO] - Training Epoch: 1/2, step 6406/23838 completed (loss: 0.42779549956321716, acc: 0.875)
[2025-02-16 12:13:32,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:32,993][root][INFO] - Training Epoch: 1/2, step 6407/23838 completed (loss: 0.6478245854377747, acc: 0.8024691343307495)
[2025-02-16 12:13:33,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:33,424][root][INFO] - Training Epoch: 1/2, step 6408/23838 completed (loss: 1.1770610809326172, acc: 0.7049180269241333)
[2025-02-16 12:13:33,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:33,857][root][INFO] - Training Epoch: 1/2, step 6409/23838 completed (loss: 1.2031105756759644, acc: 0.6000000238418579)
[2025-02-16 12:13:34,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:34,312][root][INFO] - Training Epoch: 1/2, step 6410/23838 completed (loss: 1.4909541606903076, acc: 0.5833333134651184)
[2025-02-16 12:13:34,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:34,729][root][INFO] - Training Epoch: 1/2, step 6411/23838 completed (loss: 0.8184624910354614, acc: 0.7641509175300598)
[2025-02-16 12:13:34,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:35,146][root][INFO] - Training Epoch: 1/2, step 6412/23838 completed (loss: 0.6711640954017639, acc: 0.8125)
[2025-02-16 12:13:35,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:35,533][root][INFO] - Training Epoch: 1/2, step 6413/23838 completed (loss: 0.8735748529434204, acc: 0.7124999761581421)
[2025-02-16 12:13:35,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:35,902][root][INFO] - Training Epoch: 1/2, step 6414/23838 completed (loss: 0.5663776993751526, acc: 0.8064516186714172)
[2025-02-16 12:13:36,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:36,320][root][INFO] - Training Epoch: 1/2, step 6415/23838 completed (loss: 1.2131693363189697, acc: 0.675000011920929)
[2025-02-16 12:13:36,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:36,763][root][INFO] - Training Epoch: 1/2, step 6416/23838 completed (loss: 1.0407952070236206, acc: 0.7586206793785095)
[2025-02-16 12:13:36,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:37,248][root][INFO] - Training Epoch: 1/2, step 6417/23838 completed (loss: 1.4140398502349854, acc: 0.5692307949066162)
[2025-02-16 12:13:37,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:37,660][root][INFO] - Training Epoch: 1/2, step 6418/23838 completed (loss: 0.9298956990242004, acc: 0.7200000286102295)
[2025-02-16 12:13:37,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:38,124][root][INFO] - Training Epoch: 1/2, step 6419/23838 completed (loss: 0.6480062007904053, acc: 0.8333333134651184)
[2025-02-16 12:13:38,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:38,593][root][INFO] - Training Epoch: 1/2, step 6420/23838 completed (loss: 0.7605027556419373, acc: 0.7373737096786499)
[2025-02-16 12:13:38,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:39,033][root][INFO] - Training Epoch: 1/2, step 6421/23838 completed (loss: 0.8135546445846558, acc: 0.7676056623458862)
[2025-02-16 12:13:39,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:39,390][root][INFO] - Training Epoch: 1/2, step 6422/23838 completed (loss: 0.6759932041168213, acc: 0.7792207598686218)
[2025-02-16 12:13:39,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:39,752][root][INFO] - Training Epoch: 1/2, step 6423/23838 completed (loss: 1.0488133430480957, acc: 0.6413043737411499)
[2025-02-16 12:13:39,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:40,146][root][INFO] - Training Epoch: 1/2, step 6424/23838 completed (loss: 0.6444950103759766, acc: 0.7916666865348816)
[2025-02-16 12:13:40,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:40,567][root][INFO] - Training Epoch: 1/2, step 6425/23838 completed (loss: 0.6530721783638, acc: 0.8024691343307495)
[2025-02-16 12:13:40,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:40,959][root][INFO] - Training Epoch: 1/2, step 6426/23838 completed (loss: 1.102563738822937, acc: 0.7323943376541138)
[2025-02-16 12:13:41,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:41,374][root][INFO] - Training Epoch: 1/2, step 6427/23838 completed (loss: 0.86301589012146, acc: 0.7441860437393188)
[2025-02-16 12:13:41,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:41,834][root][INFO] - Training Epoch: 1/2, step 6428/23838 completed (loss: 1.1238945722579956, acc: 0.6666666865348816)
[2025-02-16 12:13:41,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:42,197][root][INFO] - Training Epoch: 1/2, step 6429/23838 completed (loss: 1.4891749620437622, acc: 0.5306122303009033)
[2025-02-16 12:13:42,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:42,555][root][INFO] - Training Epoch: 1/2, step 6430/23838 completed (loss: 1.071526050567627, acc: 0.675000011920929)
[2025-02-16 12:13:42,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:42,933][root][INFO] - Training Epoch: 1/2, step 6431/23838 completed (loss: 1.3234490156173706, acc: 0.6415094137191772)
[2025-02-16 12:13:43,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:43,367][root][INFO] - Training Epoch: 1/2, step 6432/23838 completed (loss: 1.0606744289398193, acc: 0.6952381134033203)
[2025-02-16 12:13:43,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:43,784][root][INFO] - Training Epoch: 1/2, step 6433/23838 completed (loss: 1.0781511068344116, acc: 0.6899224519729614)
[2025-02-16 12:13:43,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:44,211][root][INFO] - Training Epoch: 1/2, step 6434/23838 completed (loss: 0.8226996660232544, acc: 0.7821782231330872)
[2025-02-16 12:13:44,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:44,659][root][INFO] - Training Epoch: 1/2, step 6435/23838 completed (loss: 1.4118927717208862, acc: 0.5679012537002563)
[2025-02-16 12:13:44,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:45,101][root][INFO] - Training Epoch: 1/2, step 6436/23838 completed (loss: 0.8097813129425049, acc: 0.7777777910232544)
[2025-02-16 12:13:45,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:45,445][root][INFO] - Training Epoch: 1/2, step 6437/23838 completed (loss: 0.9423948526382446, acc: 0.7089552283287048)
[2025-02-16 12:13:45,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:45,837][root][INFO] - Training Epoch: 1/2, step 6438/23838 completed (loss: 0.7623364925384521, acc: 0.8409090638160706)
[2025-02-16 12:13:46,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:46,268][root][INFO] - Training Epoch: 1/2, step 6439/23838 completed (loss: 0.6832940578460693, acc: 0.7835820913314819)
[2025-02-16 12:13:46,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:46,660][root][INFO] - Training Epoch: 1/2, step 6440/23838 completed (loss: 1.0780519247055054, acc: 0.7241379022598267)
[2025-02-16 12:13:46,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:47,181][root][INFO] - Training Epoch: 1/2, step 6441/23838 completed (loss: 0.7115769982337952, acc: 0.8135592937469482)
[2025-02-16 12:13:47,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:47,567][root][INFO] - Training Epoch: 1/2, step 6442/23838 completed (loss: 0.6550170183181763, acc: 0.8173912763595581)
[2025-02-16 12:13:47,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:48,009][root][INFO] - Training Epoch: 1/2, step 6443/23838 completed (loss: 0.9631277918815613, acc: 0.7526881694793701)
[2025-02-16 12:13:48,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:48,451][root][INFO] - Training Epoch: 1/2, step 6444/23838 completed (loss: 1.0078563690185547, acc: 0.7218543291091919)
[2025-02-16 12:13:48,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:48,956][root][INFO] - Training Epoch: 1/2, step 6445/23838 completed (loss: 1.224752426147461, acc: 0.6214285492897034)
[2025-02-16 12:13:49,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:49,398][root][INFO] - Training Epoch: 1/2, step 6446/23838 completed (loss: 0.8831467032432556, acc: 0.75)
[2025-02-16 12:13:49,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:49,861][root][INFO] - Training Epoch: 1/2, step 6447/23838 completed (loss: 0.5564568042755127, acc: 0.8425197005271912)
[2025-02-16 12:13:50,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:50,259][root][INFO] - Training Epoch: 1/2, step 6448/23838 completed (loss: 0.5773318409919739, acc: 0.8651685118675232)
[2025-02-16 12:13:50,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:50,694][root][INFO] - Training Epoch: 1/2, step 6449/23838 completed (loss: 0.6017478108406067, acc: 0.8421052694320679)
[2025-02-16 12:13:50,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:51,101][root][INFO] - Training Epoch: 1/2, step 6450/23838 completed (loss: 0.6659374833106995, acc: 0.8095238208770752)
[2025-02-16 12:13:51,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:51,540][root][INFO] - Training Epoch: 1/2, step 6451/23838 completed (loss: 0.6215890645980835, acc: 0.8214285969734192)
[2025-02-16 12:13:51,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:51,936][root][INFO] - Training Epoch: 1/2, step 6452/23838 completed (loss: 0.9911946654319763, acc: 0.7252747416496277)
[2025-02-16 12:13:52,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:52,346][root][INFO] - Training Epoch: 1/2, step 6453/23838 completed (loss: 0.7541564106941223, acc: 0.7599999904632568)
[2025-02-16 12:13:52,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:52,714][root][INFO] - Training Epoch: 1/2, step 6454/23838 completed (loss: 0.7054111361503601, acc: 0.782608687877655)
[2025-02-16 12:13:52,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:53,154][root][INFO] - Training Epoch: 1/2, step 6455/23838 completed (loss: 0.36082178354263306, acc: 0.8823529481887817)
[2025-02-16 12:13:53,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:53,575][root][INFO] - Training Epoch: 1/2, step 6456/23838 completed (loss: 0.5546571016311646, acc: 0.8395061492919922)
[2025-02-16 12:13:53,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:54,043][root][INFO] - Training Epoch: 1/2, step 6457/23838 completed (loss: 0.5024049282073975, acc: 0.8541666865348816)
[2025-02-16 12:13:54,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:54,507][root][INFO] - Training Epoch: 1/2, step 6458/23838 completed (loss: 0.7419461011886597, acc: 0.7875000238418579)
[2025-02-16 12:13:54,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:54,990][root][INFO] - Training Epoch: 1/2, step 6459/23838 completed (loss: 0.3012450337409973, acc: 0.9482758641242981)
[2025-02-16 12:13:55,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:55,460][root][INFO] - Training Epoch: 1/2, step 6460/23838 completed (loss: 0.31467729806900024, acc: 0.913294792175293)
[2025-02-16 12:13:55,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:55,877][root][INFO] - Training Epoch: 1/2, step 6461/23838 completed (loss: 0.4553353190422058, acc: 0.8715596199035645)
[2025-02-16 12:13:56,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:56,301][root][INFO] - Training Epoch: 1/2, step 6462/23838 completed (loss: 0.9317880272865295, acc: 0.7361111044883728)
[2025-02-16 12:13:56,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:56,728][root][INFO] - Training Epoch: 1/2, step 6463/23838 completed (loss: 0.5506471991539001, acc: 0.8409090638160706)
[2025-02-16 12:13:56,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:57,153][root][INFO] - Training Epoch: 1/2, step 6464/23838 completed (loss: 0.5929548144340515, acc: 0.817460298538208)
[2025-02-16 12:13:57,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:57,525][root][INFO] - Training Epoch: 1/2, step 6465/23838 completed (loss: 0.46215352416038513, acc: 0.8541666865348816)
[2025-02-16 12:13:57,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:58,274][root][INFO] - Training Epoch: 1/2, step 6466/23838 completed (loss: 0.5661368370056152, acc: 0.8510638475418091)
[2025-02-16 12:13:58,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:58,759][root][INFO] - Training Epoch: 1/2, step 6467/23838 completed (loss: 0.49954596161842346, acc: 0.8407079577445984)
[2025-02-16 12:13:58,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:59,171][root][INFO] - Training Epoch: 1/2, step 6468/23838 completed (loss: 0.4333578646183014, acc: 0.8689655065536499)
[2025-02-16 12:13:59,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:13:59,595][root][INFO] - Training Epoch: 1/2, step 6469/23838 completed (loss: 0.35455262660980225, acc: 0.9120000004768372)
[2025-02-16 12:13:59,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:00,001][root][INFO] - Training Epoch: 1/2, step 6470/23838 completed (loss: 0.49809709191322327, acc: 0.8666666746139526)
[2025-02-16 12:14:00,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:00,367][root][INFO] - Training Epoch: 1/2, step 6471/23838 completed (loss: 0.49642249941825867, acc: 0.8272727131843567)
[2025-02-16 12:14:00,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:00,834][root][INFO] - Training Epoch: 1/2, step 6472/23838 completed (loss: 0.37359482049942017, acc: 0.8666666746139526)
[2025-02-16 12:14:01,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:01,236][root][INFO] - Training Epoch: 1/2, step 6473/23838 completed (loss: 0.4867706298828125, acc: 0.8888888955116272)
[2025-02-16 12:14:01,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:01,696][root][INFO] - Training Epoch: 1/2, step 6474/23838 completed (loss: 0.3851678967475891, acc: 0.8977272510528564)
[2025-02-16 12:14:01,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:02,134][root][INFO] - Training Epoch: 1/2, step 6475/23838 completed (loss: 0.5292229652404785, acc: 0.8118811845779419)
[2025-02-16 12:14:02,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:02,557][root][INFO] - Training Epoch: 1/2, step 6476/23838 completed (loss: 0.6021215319633484, acc: 0.790123462677002)
[2025-02-16 12:14:02,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:03,004][root][INFO] - Training Epoch: 1/2, step 6477/23838 completed (loss: 1.0006223917007446, acc: 0.7236841917037964)
[2025-02-16 12:14:03,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:03,422][root][INFO] - Training Epoch: 1/2, step 6478/23838 completed (loss: 0.900518536567688, acc: 0.7846153974533081)
[2025-02-16 12:14:03,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:03,853][root][INFO] - Training Epoch: 1/2, step 6479/23838 completed (loss: 0.8172511458396912, acc: 0.8194444179534912)
[2025-02-16 12:14:04,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:04,315][root][INFO] - Training Epoch: 1/2, step 6480/23838 completed (loss: 0.36472031474113464, acc: 0.8631578683853149)
[2025-02-16 12:14:04,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:04,736][root][INFO] - Training Epoch: 1/2, step 6481/23838 completed (loss: 0.630649209022522, acc: 0.797468364238739)
[2025-02-16 12:14:04,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:05,128][root][INFO] - Training Epoch: 1/2, step 6482/23838 completed (loss: 0.5053484439849854, acc: 0.8493150472640991)
[2025-02-16 12:14:05,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:05,525][root][INFO] - Training Epoch: 1/2, step 6483/23838 completed (loss: 0.4045199453830719, acc: 0.9166666865348816)
[2025-02-16 12:14:05,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:05,939][root][INFO] - Training Epoch: 1/2, step 6484/23838 completed (loss: 0.5767204761505127, acc: 0.8265306353569031)
[2025-02-16 12:14:06,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:06,347][root][INFO] - Training Epoch: 1/2, step 6485/23838 completed (loss: 0.4145840108394623, acc: 0.8918918967247009)
[2025-02-16 12:14:06,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:06,760][root][INFO] - Training Epoch: 1/2, step 6486/23838 completed (loss: 0.8582387566566467, acc: 0.7457627058029175)
[2025-02-16 12:14:06,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:07,137][root][INFO] - Training Epoch: 1/2, step 6487/23838 completed (loss: 0.43747371435165405, acc: 0.862500011920929)
[2025-02-16 12:14:07,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:07,572][root][INFO] - Training Epoch: 1/2, step 6488/23838 completed (loss: 0.8569657206535339, acc: 0.7317073345184326)
[2025-02-16 12:14:07,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:07,990][root][INFO] - Training Epoch: 1/2, step 6489/23838 completed (loss: 0.22294534742832184, acc: 0.9210526347160339)
[2025-02-16 12:14:08,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:08,391][root][INFO] - Training Epoch: 1/2, step 6490/23838 completed (loss: 0.942737340927124, acc: 0.7209302186965942)
[2025-02-16 12:14:08,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:08,835][root][INFO] - Training Epoch: 1/2, step 6491/23838 completed (loss: 0.4903601109981537, acc: 0.8313252925872803)
[2025-02-16 12:14:08,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:09,232][root][INFO] - Training Epoch: 1/2, step 6492/23838 completed (loss: 0.7097657918930054, acc: 0.8225806355476379)
[2025-02-16 12:14:09,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:09,590][root][INFO] - Training Epoch: 1/2, step 6493/23838 completed (loss: 0.7870662212371826, acc: 0.7692307829856873)
[2025-02-16 12:14:09,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:10,015][root][INFO] - Training Epoch: 1/2, step 6494/23838 completed (loss: 0.4865608215332031, acc: 0.8714285492897034)
[2025-02-16 12:14:10,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:10,453][root][INFO] - Training Epoch: 1/2, step 6495/23838 completed (loss: 0.7784136533737183, acc: 0.7902097702026367)
[2025-02-16 12:14:10,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:10,887][root][INFO] - Training Epoch: 1/2, step 6496/23838 completed (loss: 0.9047887325286865, acc: 0.7642857432365417)
[2025-02-16 12:14:11,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:11,313][root][INFO] - Training Epoch: 1/2, step 6497/23838 completed (loss: 0.3617451786994934, acc: 0.9069767594337463)
[2025-02-16 12:14:11,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:11,688][root][INFO] - Training Epoch: 1/2, step 6498/23838 completed (loss: 0.3869422376155853, acc: 0.8969072103500366)
[2025-02-16 12:14:11,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:12,063][root][INFO] - Training Epoch: 1/2, step 6499/23838 completed (loss: 0.40393900871276855, acc: 0.8888888955116272)
[2025-02-16 12:14:12,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:12,508][root][INFO] - Training Epoch: 1/2, step 6500/23838 completed (loss: 0.6111186742782593, acc: 0.8111110925674438)
[2025-02-16 12:14:12,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:12,984][root][INFO] - Training Epoch: 1/2, step 6501/23838 completed (loss: 1.1220341920852661, acc: 0.695652186870575)
[2025-02-16 12:14:13,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:13,437][root][INFO] - Training Epoch: 1/2, step 6502/23838 completed (loss: 0.3938741981983185, acc: 0.8928571343421936)
[2025-02-16 12:14:13,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:13,915][root][INFO] - Training Epoch: 1/2, step 6503/23838 completed (loss: 0.7312558889389038, acc: 0.7954545617103577)
[2025-02-16 12:14:14,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:14,384][root][INFO] - Training Epoch: 1/2, step 6504/23838 completed (loss: 1.171805500984192, acc: 0.6290322542190552)
[2025-02-16 12:14:14,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:14,792][root][INFO] - Training Epoch: 1/2, step 6505/23838 completed (loss: 0.6396313905715942, acc: 0.828125)
[2025-02-16 12:14:14,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:15,169][root][INFO] - Training Epoch: 1/2, step 6506/23838 completed (loss: 1.0699536800384521, acc: 0.6666666865348816)
[2025-02-16 12:14:15,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:15,578][root][INFO] - Training Epoch: 1/2, step 6507/23838 completed (loss: 0.9163113236427307, acc: 0.7200000286102295)
[2025-02-16 12:14:15,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:16,032][root][INFO] - Training Epoch: 1/2, step 6508/23838 completed (loss: 0.8690133094787598, acc: 0.7068965435028076)
[2025-02-16 12:14:16,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:16,486][root][INFO] - Training Epoch: 1/2, step 6509/23838 completed (loss: 0.6145364046096802, acc: 0.7980769276618958)
[2025-02-16 12:14:16,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:16,898][root][INFO] - Training Epoch: 1/2, step 6510/23838 completed (loss: 0.6795136332511902, acc: 0.8129032254219055)
[2025-02-16 12:14:17,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:17,490][root][INFO] - Training Epoch: 1/2, step 6511/23838 completed (loss: 0.6836645603179932, acc: 0.8097826242446899)
[2025-02-16 12:14:17,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:17,931][root][INFO] - Training Epoch: 1/2, step 6512/23838 completed (loss: 0.5419667959213257, acc: 0.8662420511245728)
[2025-02-16 12:14:18,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:18,366][root][INFO] - Training Epoch: 1/2, step 6513/23838 completed (loss: 1.4881820678710938, acc: 0.6276595592498779)
[2025-02-16 12:14:18,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:18,810][root][INFO] - Training Epoch: 1/2, step 6514/23838 completed (loss: 0.7591720223426819, acc: 0.791208803653717)
[2025-02-16 12:14:19,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:19,280][root][INFO] - Training Epoch: 1/2, step 6515/23838 completed (loss: 0.8997237086296082, acc: 0.7183098793029785)
[2025-02-16 12:14:19,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:19,644][root][INFO] - Training Epoch: 1/2, step 6516/23838 completed (loss: 0.7996501922607422, acc: 0.8030303120613098)
[2025-02-16 12:14:19,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:20,059][root][INFO] - Training Epoch: 1/2, step 6517/23838 completed (loss: 0.6388508081436157, acc: 0.7916666865348816)
[2025-02-16 12:14:20,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:20,524][root][INFO] - Training Epoch: 1/2, step 6518/23838 completed (loss: 1.0985839366912842, acc: 0.6788321137428284)
[2025-02-16 12:14:20,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:20,959][root][INFO] - Training Epoch: 1/2, step 6519/23838 completed (loss: 1.1448373794555664, acc: 0.6792452931404114)
[2025-02-16 12:14:21,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:21,361][root][INFO] - Training Epoch: 1/2, step 6520/23838 completed (loss: 0.8994329571723938, acc: 0.7682119011878967)
[2025-02-16 12:14:21,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:21,739][root][INFO] - Training Epoch: 1/2, step 6521/23838 completed (loss: 0.8561813831329346, acc: 0.7419354915618896)
[2025-02-16 12:14:21,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:22,115][root][INFO] - Training Epoch: 1/2, step 6522/23838 completed (loss: 0.9158438444137573, acc: 0.7215189933776855)
[2025-02-16 12:14:22,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:22,510][root][INFO] - Training Epoch: 1/2, step 6523/23838 completed (loss: 0.479615718126297, acc: 0.8488371968269348)
[2025-02-16 12:14:22,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:22,890][root][INFO] - Training Epoch: 1/2, step 6524/23838 completed (loss: 0.46825021505355835, acc: 0.853210985660553)
[2025-02-16 12:14:23,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:23,302][root][INFO] - Training Epoch: 1/2, step 6525/23838 completed (loss: 0.4209508001804352, acc: 0.8235294222831726)
[2025-02-16 12:14:23,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:23,719][root][INFO] - Training Epoch: 1/2, step 6526/23838 completed (loss: 0.585656464099884, acc: 0.8155339956283569)
[2025-02-16 12:14:23,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:24,140][root][INFO] - Training Epoch: 1/2, step 6527/23838 completed (loss: 0.4816661775112152, acc: 0.8367347121238708)
[2025-02-16 12:14:24,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:24,507][root][INFO] - Training Epoch: 1/2, step 6528/23838 completed (loss: 0.7382829189300537, acc: 0.8208954930305481)
[2025-02-16 12:14:24,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:24,944][root][INFO] - Training Epoch: 1/2, step 6529/23838 completed (loss: 0.5822198390960693, acc: 0.8307692408561707)
[2025-02-16 12:14:25,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:25,345][root][INFO] - Training Epoch: 1/2, step 6530/23838 completed (loss: 0.40520772337913513, acc: 0.8837209343910217)
[2025-02-16 12:14:25,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:25,767][root][INFO] - Training Epoch: 1/2, step 6531/23838 completed (loss: 0.9437127709388733, acc: 0.7083333134651184)
[2025-02-16 12:14:25,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:26,201][root][INFO] - Training Epoch: 1/2, step 6532/23838 completed (loss: 0.231319397687912, acc: 0.9212598204612732)
[2025-02-16 12:14:26,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:26,624][root][INFO] - Training Epoch: 1/2, step 6533/23838 completed (loss: 0.5253565907478333, acc: 0.8918918967247009)
[2025-02-16 12:14:26,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:27,065][root][INFO] - Training Epoch: 1/2, step 6534/23838 completed (loss: 0.259632408618927, acc: 0.9223300814628601)
[2025-02-16 12:14:27,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:27,512][root][INFO] - Training Epoch: 1/2, step 6535/23838 completed (loss: 0.3574788272380829, acc: 0.9135802388191223)
[2025-02-16 12:14:27,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:27,937][root][INFO] - Training Epoch: 1/2, step 6536/23838 completed (loss: 0.5255662798881531, acc: 0.8514851331710815)
[2025-02-16 12:14:28,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:28,373][root][INFO] - Training Epoch: 1/2, step 6537/23838 completed (loss: 0.42589154839515686, acc: 0.8266666531562805)
[2025-02-16 12:14:28,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:28,809][root][INFO] - Training Epoch: 1/2, step 6538/23838 completed (loss: 0.5238366723060608, acc: 0.8453608155250549)
[2025-02-16 12:14:29,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:29,230][root][INFO] - Training Epoch: 1/2, step 6539/23838 completed (loss: 0.4519656300544739, acc: 0.8590604066848755)
[2025-02-16 12:14:29,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:29,607][root][INFO] - Training Epoch: 1/2, step 6540/23838 completed (loss: 0.39660337567329407, acc: 0.8823529481887817)
[2025-02-16 12:14:29,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:30,047][root][INFO] - Training Epoch: 1/2, step 6541/23838 completed (loss: 0.41444504261016846, acc: 0.875)
[2025-02-16 12:14:30,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:30,494][root][INFO] - Training Epoch: 1/2, step 6542/23838 completed (loss: 0.4075421094894409, acc: 0.8999999761581421)
[2025-02-16 12:14:30,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:30,918][root][INFO] - Training Epoch: 1/2, step 6543/23838 completed (loss: 0.8964972496032715, acc: 0.7697368264198303)
[2025-02-16 12:14:31,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:31,376][root][INFO] - Training Epoch: 1/2, step 6544/23838 completed (loss: 0.6140464544296265, acc: 0.8022598624229431)
[2025-02-16 12:14:31,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:31,815][root][INFO] - Training Epoch: 1/2, step 6545/23838 completed (loss: 0.7150408625602722, acc: 0.8350515365600586)
[2025-02-16 12:14:32,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:32,249][root][INFO] - Training Epoch: 1/2, step 6546/23838 completed (loss: 0.4016081690788269, acc: 0.8441558480262756)
[2025-02-16 12:14:32,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:32,688][root][INFO] - Training Epoch: 1/2, step 6547/23838 completed (loss: 0.40942978858947754, acc: 0.867132842540741)
[2025-02-16 12:14:32,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:33,138][root][INFO] - Training Epoch: 1/2, step 6548/23838 completed (loss: 0.6775166392326355, acc: 0.8235294222831726)
[2025-02-16 12:14:33,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:33,527][root][INFO] - Training Epoch: 1/2, step 6549/23838 completed (loss: 0.3677513599395752, acc: 0.8809523582458496)
[2025-02-16 12:14:33,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:33,960][root][INFO] - Training Epoch: 1/2, step 6550/23838 completed (loss: 0.4798741936683655, acc: 0.8684210777282715)
[2025-02-16 12:14:34,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:34,409][root][INFO] - Training Epoch: 1/2, step 6551/23838 completed (loss: 0.7644780874252319, acc: 0.8163265585899353)
[2025-02-16 12:14:34,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:34,812][root][INFO] - Training Epoch: 1/2, step 6552/23838 completed (loss: 0.7781464457511902, acc: 0.7692307829856873)
[2025-02-16 12:14:34,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:35,225][root][INFO] - Training Epoch: 1/2, step 6553/23838 completed (loss: 0.9113993644714355, acc: 0.7373737096786499)
[2025-02-16 12:14:35,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:35,677][root][INFO] - Training Epoch: 1/2, step 6554/23838 completed (loss: 0.3235134184360504, acc: 0.8899082541465759)
[2025-02-16 12:14:35,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:36,062][root][INFO] - Training Epoch: 1/2, step 6555/23838 completed (loss: 0.6119553446769714, acc: 0.8352941274642944)
[2025-02-16 12:14:36,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:36,489][root][INFO] - Training Epoch: 1/2, step 6556/23838 completed (loss: 0.4089381992816925, acc: 0.8823529481887817)
[2025-02-16 12:14:36,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:36,967][root][INFO] - Training Epoch: 1/2, step 6557/23838 completed (loss: 0.7125054597854614, acc: 0.8062015771865845)
[2025-02-16 12:14:37,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:37,388][root][INFO] - Training Epoch: 1/2, step 6558/23838 completed (loss: 0.3753913938999176, acc: 0.8659793734550476)
[2025-02-16 12:14:37,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:37,820][root][INFO] - Training Epoch: 1/2, step 6559/23838 completed (loss: 0.19650232791900635, acc: 0.9514563083648682)
[2025-02-16 12:14:38,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:38,241][root][INFO] - Training Epoch: 1/2, step 6560/23838 completed (loss: 0.46286526322364807, acc: 0.8240740895271301)
[2025-02-16 12:14:38,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:38,681][root][INFO] - Training Epoch: 1/2, step 6561/23838 completed (loss: 0.4086117148399353, acc: 0.8861788511276245)
[2025-02-16 12:14:38,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:39,134][root][INFO] - Training Epoch: 1/2, step 6562/23838 completed (loss: 0.5416817665100098, acc: 0.8645161390304565)
[2025-02-16 12:14:39,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:39,618][root][INFO] - Training Epoch: 1/2, step 6563/23838 completed (loss: 0.31061089038848877, acc: 0.9069767594337463)
[2025-02-16 12:14:39,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:39,976][root][INFO] - Training Epoch: 1/2, step 6564/23838 completed (loss: 0.6532863974571228, acc: 0.8415841460227966)
[2025-02-16 12:14:40,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:40,388][root][INFO] - Training Epoch: 1/2, step 6565/23838 completed (loss: 0.6118777990341187, acc: 0.8734177350997925)
[2025-02-16 12:14:40,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:40,833][root][INFO] - Training Epoch: 1/2, step 6566/23838 completed (loss: 0.8642876148223877, acc: 0.7692307829856873)
[2025-02-16 12:14:41,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:41,266][root][INFO] - Training Epoch: 1/2, step 6567/23838 completed (loss: 0.8531049489974976, acc: 0.7250000238418579)
[2025-02-16 12:14:41,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:41,736][root][INFO] - Training Epoch: 1/2, step 6568/23838 completed (loss: 0.5845901966094971, acc: 0.7894737124443054)
[2025-02-16 12:14:41,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:42,195][root][INFO] - Training Epoch: 1/2, step 6569/23838 completed (loss: 0.8801873922348022, acc: 0.717391312122345)
[2025-02-16 12:14:42,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:42,659][root][INFO] - Training Epoch: 1/2, step 6570/23838 completed (loss: 0.8623113036155701, acc: 0.746268630027771)
[2025-02-16 12:14:43,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:43,550][root][INFO] - Training Epoch: 1/2, step 6571/23838 completed (loss: 0.884255588054657, acc: 0.7200000286102295)
[2025-02-16 12:14:43,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:43,990][root][INFO] - Training Epoch: 1/2, step 6572/23838 completed (loss: 1.1484167575836182, acc: 0.6415094137191772)
[2025-02-16 12:14:44,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:44,398][root][INFO] - Training Epoch: 1/2, step 6573/23838 completed (loss: 1.1011335849761963, acc: 0.654321014881134)
[2025-02-16 12:14:44,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:44,841][root][INFO] - Training Epoch: 1/2, step 6574/23838 completed (loss: 0.970254123210907, acc: 0.717391312122345)
[2025-02-16 12:14:45,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:45,260][root][INFO] - Training Epoch: 1/2, step 6575/23838 completed (loss: 0.7781099677085876, acc: 0.7435897588729858)
[2025-02-16 12:14:45,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:45,684][root][INFO] - Training Epoch: 1/2, step 6576/23838 completed (loss: 0.8665029406547546, acc: 0.8032786846160889)
[2025-02-16 12:14:45,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:46,220][root][INFO] - Training Epoch: 1/2, step 6577/23838 completed (loss: 1.5382778644561768, acc: 0.557692289352417)
[2025-02-16 12:14:46,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:46,654][root][INFO] - Training Epoch: 1/2, step 6578/23838 completed (loss: 0.9321355223655701, acc: 0.7575757503509521)
[2025-02-16 12:14:46,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:47,100][root][INFO] - Training Epoch: 1/2, step 6579/23838 completed (loss: 1.0376942157745361, acc: 0.774193525314331)
[2025-02-16 12:14:47,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:47,519][root][INFO] - Training Epoch: 1/2, step 6580/23838 completed (loss: 0.24732641875743866, acc: 0.9375)
[2025-02-16 12:14:47,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:47,968][root][INFO] - Training Epoch: 1/2, step 6581/23838 completed (loss: 0.2390582412481308, acc: 0.9516128897666931)
[2025-02-16 12:14:48,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:48,429][root][INFO] - Training Epoch: 1/2, step 6582/23838 completed (loss: 0.4852564036846161, acc: 0.8620689511299133)
[2025-02-16 12:14:48,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:48,796][root][INFO] - Training Epoch: 1/2, step 6583/23838 completed (loss: 0.5990146994590759, acc: 0.8367347121238708)
[2025-02-16 12:14:49,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:49,344][root][INFO] - Training Epoch: 1/2, step 6584/23838 completed (loss: 0.8427673578262329, acc: 0.7704917788505554)
[2025-02-16 12:14:49,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:49,802][root][INFO] - Training Epoch: 1/2, step 6585/23838 completed (loss: 0.973574161529541, acc: 0.7941176295280457)
[2025-02-16 12:14:49,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:50,212][root][INFO] - Training Epoch: 1/2, step 6586/23838 completed (loss: 0.6141084432601929, acc: 0.800000011920929)
[2025-02-16 12:14:50,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:50,666][root][INFO] - Training Epoch: 1/2, step 6587/23838 completed (loss: 0.9488909840583801, acc: 0.7222222089767456)
[2025-02-16 12:14:50,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:51,044][root][INFO] - Training Epoch: 1/2, step 6588/23838 completed (loss: 0.3669656813144684, acc: 0.8545454740524292)
[2025-02-16 12:14:51,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:51,400][root][INFO] - Training Epoch: 1/2, step 6589/23838 completed (loss: 1.4881560802459717, acc: 0.6538461446762085)
[2025-02-16 12:14:51,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:51,833][root][INFO] - Training Epoch: 1/2, step 6590/23838 completed (loss: 0.7399212718009949, acc: 0.7884615659713745)
[2025-02-16 12:14:52,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:52,301][root][INFO] - Training Epoch: 1/2, step 6591/23838 completed (loss: 0.41887447237968445, acc: 0.8571428656578064)
[2025-02-16 12:14:52,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:52,728][root][INFO] - Training Epoch: 1/2, step 6592/23838 completed (loss: 0.8031648993492126, acc: 0.8048780560493469)
[2025-02-16 12:14:52,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:53,093][root][INFO] - Training Epoch: 1/2, step 6593/23838 completed (loss: 1.1640424728393555, acc: 0.7368420958518982)
[2025-02-16 12:14:53,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:53,724][root][INFO] - Training Epoch: 1/2, step 6594/23838 completed (loss: 0.52369624376297, acc: 0.8297872543334961)
[2025-02-16 12:14:53,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:54,134][root][INFO] - Training Epoch: 1/2, step 6595/23838 completed (loss: 1.2780424356460571, acc: 0.6271186470985413)
[2025-02-16 12:14:54,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:54,551][root][INFO] - Training Epoch: 1/2, step 6596/23838 completed (loss: 0.8254346251487732, acc: 0.8333333134651184)
[2025-02-16 12:14:54,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:54,954][root][INFO] - Training Epoch: 1/2, step 6597/23838 completed (loss: 0.3348265290260315, acc: 0.8823529481887817)
[2025-02-16 12:14:55,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:55,304][root][INFO] - Training Epoch: 1/2, step 6598/23838 completed (loss: 0.8396347165107727, acc: 0.692307710647583)
[2025-02-16 12:14:55,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:55,788][root][INFO] - Training Epoch: 1/2, step 6599/23838 completed (loss: 0.3498466908931732, acc: 0.8970588445663452)
[2025-02-16 12:14:55,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:56,209][root][INFO] - Training Epoch: 1/2, step 6600/23838 completed (loss: 0.2198949009180069, acc: 0.9629629850387573)
[2025-02-16 12:14:56,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:56,684][root][INFO] - Training Epoch: 1/2, step 6601/23838 completed (loss: 0.9976285099983215, acc: 0.6875)
[2025-02-16 12:14:56,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:57,216][root][INFO] - Training Epoch: 1/2, step 6602/23838 completed (loss: 1.3614997863769531, acc: 0.6612903475761414)
[2025-02-16 12:14:57,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:57,652][root][INFO] - Training Epoch: 1/2, step 6603/23838 completed (loss: 1.2159817218780518, acc: 0.5957446694374084)
[2025-02-16 12:14:57,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:58,120][root][INFO] - Training Epoch: 1/2, step 6604/23838 completed (loss: 1.3907241821289062, acc: 0.5789473652839661)
[2025-02-16 12:14:58,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:58,509][root][INFO] - Training Epoch: 1/2, step 6605/23838 completed (loss: 0.8290047645568848, acc: 0.7777777910232544)
[2025-02-16 12:14:58,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:58,949][root][INFO] - Training Epoch: 1/2, step 6606/23838 completed (loss: 1.2033097743988037, acc: 0.6779661178588867)
[2025-02-16 12:14:59,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:59,372][root][INFO] - Training Epoch: 1/2, step 6607/23838 completed (loss: 0.5271763205528259, acc: 0.7777777910232544)
[2025-02-16 12:14:59,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:14:59,811][root][INFO] - Training Epoch: 1/2, step 6608/23838 completed (loss: 0.41903868317604065, acc: 0.8815789222717285)
[2025-02-16 12:14:59,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:00,212][root][INFO] - Training Epoch: 1/2, step 6609/23838 completed (loss: 1.33847975730896, acc: 0.6964285969734192)
[2025-02-16 12:15:00,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:00,647][root][INFO] - Training Epoch: 1/2, step 6610/23838 completed (loss: 0.6429749727249146, acc: 0.7692307829856873)
[2025-02-16 12:15:00,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:01,001][root][INFO] - Training Epoch: 1/2, step 6611/23838 completed (loss: 0.1960034817457199, acc: 0.9298245906829834)
[2025-02-16 12:15:01,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:01,376][root][INFO] - Training Epoch: 1/2, step 6612/23838 completed (loss: 0.1791389137506485, acc: 0.9666666388511658)
[2025-02-16 12:15:01,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:01,817][root][INFO] - Training Epoch: 1/2, step 6613/23838 completed (loss: 0.8241324424743652, acc: 0.7586206793785095)
[2025-02-16 12:15:02,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:02,236][root][INFO] - Training Epoch: 1/2, step 6614/23838 completed (loss: 0.8828681707382202, acc: 0.75)
[2025-02-16 12:15:02,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:02,775][root][INFO] - Training Epoch: 1/2, step 6615/23838 completed (loss: 1.195366621017456, acc: 0.6197183132171631)
[2025-02-16 12:15:02,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:03,210][root][INFO] - Training Epoch: 1/2, step 6616/23838 completed (loss: 0.7233519554138184, acc: 0.7894737124443054)
[2025-02-16 12:15:03,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:03,621][root][INFO] - Training Epoch: 1/2, step 6617/23838 completed (loss: 0.3350033760070801, acc: 0.9268292784690857)
[2025-02-16 12:15:03,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:04,031][root][INFO] - Training Epoch: 1/2, step 6618/23838 completed (loss: 0.6753832101821899, acc: 0.7692307829856873)
[2025-02-16 12:15:04,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:04,476][root][INFO] - Training Epoch: 1/2, step 6619/23838 completed (loss: 0.6288730502128601, acc: 0.8478260636329651)
[2025-02-16 12:15:04,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:04,888][root][INFO] - Training Epoch: 1/2, step 6620/23838 completed (loss: 2.0222153663635254, acc: 0.529411792755127)
[2025-02-16 12:15:05,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:05,334][root][INFO] - Training Epoch: 1/2, step 6621/23838 completed (loss: 1.2672590017318726, acc: 0.7045454382896423)
[2025-02-16 12:15:05,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:06,033][root][INFO] - Training Epoch: 1/2, step 6622/23838 completed (loss: 1.2830219268798828, acc: 0.6301369667053223)
[2025-02-16 12:15:06,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:06,452][root][INFO] - Training Epoch: 1/2, step 6623/23838 completed (loss: 0.6979780793190002, acc: 0.7971014380455017)
[2025-02-16 12:15:06,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:06,851][root][INFO] - Training Epoch: 1/2, step 6624/23838 completed (loss: 0.8503299355506897, acc: 0.8297872543334961)
[2025-02-16 12:15:07,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:07,285][root][INFO] - Training Epoch: 1/2, step 6625/23838 completed (loss: 0.9759964346885681, acc: 0.7868852615356445)
[2025-02-16 12:15:07,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:07,749][root][INFO] - Training Epoch: 1/2, step 6626/23838 completed (loss: 0.40568622946739197, acc: 0.8653846383094788)
[2025-02-16 12:15:07,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:08,172][root][INFO] - Training Epoch: 1/2, step 6627/23838 completed (loss: 1.2758355140686035, acc: 0.625)
[2025-02-16 12:15:08,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:08,603][root][INFO] - Training Epoch: 1/2, step 6628/23838 completed (loss: 0.38169652223587036, acc: 0.8392857313156128)
[2025-02-16 12:15:08,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:09,019][root][INFO] - Training Epoch: 1/2, step 6629/23838 completed (loss: 0.31227433681488037, acc: 0.8999999761581421)
[2025-02-16 12:15:09,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:09,440][root][INFO] - Training Epoch: 1/2, step 6630/23838 completed (loss: 0.4893207252025604, acc: 0.9038461446762085)
[2025-02-16 12:15:09,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:09,878][root][INFO] - Training Epoch: 1/2, step 6631/23838 completed (loss: 0.7304201722145081, acc: 0.7179487347602844)
[2025-02-16 12:15:10,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:10,369][root][INFO] - Training Epoch: 1/2, step 6632/23838 completed (loss: 1.0976866483688354, acc: 0.7708333134651184)
[2025-02-16 12:15:10,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:10,866][root][INFO] - Training Epoch: 1/2, step 6633/23838 completed (loss: 0.8100289702415466, acc: 0.7599999904632568)
[2025-02-16 12:15:11,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:11,297][root][INFO] - Training Epoch: 1/2, step 6634/23838 completed (loss: 0.5007275342941284, acc: 0.8461538553237915)
[2025-02-16 12:15:11,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:11,750][root][INFO] - Training Epoch: 1/2, step 6635/23838 completed (loss: 0.7450190186500549, acc: 0.7857142686843872)
[2025-02-16 12:15:11,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:12,240][root][INFO] - Training Epoch: 1/2, step 6636/23838 completed (loss: 0.6808524131774902, acc: 0.8314606547355652)
[2025-02-16 12:15:12,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:12,659][root][INFO] - Training Epoch: 1/2, step 6637/23838 completed (loss: 1.5828347206115723, acc: 0.529411792755127)
[2025-02-16 12:15:12,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:13,071][root][INFO] - Training Epoch: 1/2, step 6638/23838 completed (loss: 0.8214520812034607, acc: 0.75)
[2025-02-16 12:15:13,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:13,518][root][INFO] - Training Epoch: 1/2, step 6639/23838 completed (loss: 0.7910977005958557, acc: 0.765625)
[2025-02-16 12:15:13,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:13,994][root][INFO] - Training Epoch: 1/2, step 6640/23838 completed (loss: 1.14497971534729, acc: 0.6666666865348816)
[2025-02-16 12:15:14,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:14,423][root][INFO] - Training Epoch: 1/2, step 6641/23838 completed (loss: 1.0610852241516113, acc: 0.6979166865348816)
[2025-02-16 12:15:14,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:14,887][root][INFO] - Training Epoch: 1/2, step 6642/23838 completed (loss: 0.9775899052619934, acc: 0.7171717286109924)
[2025-02-16 12:15:15,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:15,318][root][INFO] - Training Epoch: 1/2, step 6643/23838 completed (loss: 0.8089041113853455, acc: 0.7698412537574768)
[2025-02-16 12:15:15,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:15,727][root][INFO] - Training Epoch: 1/2, step 6644/23838 completed (loss: 0.884373128414154, acc: 0.7471264600753784)
[2025-02-16 12:15:15,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:16,202][root][INFO] - Training Epoch: 1/2, step 6645/23838 completed (loss: 0.720775306224823, acc: 0.7987805008888245)
[2025-02-16 12:15:16,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:16,730][root][INFO] - Training Epoch: 1/2, step 6646/23838 completed (loss: 0.7476556301116943, acc: 0.8052631616592407)
[2025-02-16 12:15:16,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:17,114][root][INFO] - Training Epoch: 1/2, step 6647/23838 completed (loss: 0.8463329076766968, acc: 0.7802197933197021)
[2025-02-16 12:15:17,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:17,523][root][INFO] - Training Epoch: 1/2, step 6648/23838 completed (loss: 0.8282771110534668, acc: 0.7151898741722107)
[2025-02-16 12:15:17,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:17,966][root][INFO] - Training Epoch: 1/2, step 6649/23838 completed (loss: 0.7524755001068115, acc: 0.7983193397521973)
[2025-02-16 12:15:18,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:18,404][root][INFO] - Training Epoch: 1/2, step 6650/23838 completed (loss: 0.7890599966049194, acc: 0.7647058963775635)
[2025-02-16 12:15:18,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:18,786][root][INFO] - Training Epoch: 1/2, step 6651/23838 completed (loss: 0.6741015315055847, acc: 0.8248175382614136)
[2025-02-16 12:15:18,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:19,226][root][INFO] - Training Epoch: 1/2, step 6652/23838 completed (loss: 0.6347293853759766, acc: 0.8461538553237915)
[2025-02-16 12:15:19,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:19,654][root][INFO] - Training Epoch: 1/2, step 6653/23838 completed (loss: 1.3258018493652344, acc: 0.6296296119689941)
[2025-02-16 12:15:19,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:20,055][root][INFO] - Training Epoch: 1/2, step 6654/23838 completed (loss: 1.3276773691177368, acc: 0.662162184715271)
[2025-02-16 12:15:20,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:20,497][root][INFO] - Training Epoch: 1/2, step 6655/23838 completed (loss: 1.1301889419555664, acc: 0.6796875)
[2025-02-16 12:15:20,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:20,938][root][INFO] - Training Epoch: 1/2, step 6656/23838 completed (loss: 0.9006410241127014, acc: 0.7628205418586731)
[2025-02-16 12:15:21,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:21,299][root][INFO] - Training Epoch: 1/2, step 6657/23838 completed (loss: 0.8646041750907898, acc: 0.7317073345184326)
[2025-02-16 12:15:21,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:21,748][root][INFO] - Training Epoch: 1/2, step 6658/23838 completed (loss: 0.967025101184845, acc: 0.7304964661598206)
[2025-02-16 12:15:21,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:22,202][root][INFO] - Training Epoch: 1/2, step 6659/23838 completed (loss: 0.982891857624054, acc: 0.7279411554336548)
[2025-02-16 12:15:22,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:22,612][root][INFO] - Training Epoch: 1/2, step 6660/23838 completed (loss: 0.8249717354774475, acc: 0.8023256063461304)
[2025-02-16 12:15:22,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:22,990][root][INFO] - Training Epoch: 1/2, step 6661/23838 completed (loss: 0.7945151925086975, acc: 0.7613636255264282)
[2025-02-16 12:15:23,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:23,352][root][INFO] - Training Epoch: 1/2, step 6662/23838 completed (loss: 1.0516921281814575, acc: 0.6625000238418579)
[2025-02-16 12:15:23,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:23,720][root][INFO] - Training Epoch: 1/2, step 6663/23838 completed (loss: 1.247232437133789, acc: 0.6351351141929626)
[2025-02-16 12:15:23,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:24,156][root][INFO] - Training Epoch: 1/2, step 6664/23838 completed (loss: 1.235917568206787, acc: 0.6557376980781555)
[2025-02-16 12:15:24,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:24,587][root][INFO] - Training Epoch: 1/2, step 6665/23838 completed (loss: 1.1753056049346924, acc: 0.6382978558540344)
[2025-02-16 12:15:24,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:24,992][root][INFO] - Training Epoch: 1/2, step 6666/23838 completed (loss: 0.7039649486541748, acc: 0.7828282713890076)
[2025-02-16 12:15:25,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:25,456][root][INFO] - Training Epoch: 1/2, step 6667/23838 completed (loss: 1.352316975593567, acc: 0.643410861492157)
[2025-02-16 12:15:25,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:25,927][root][INFO] - Training Epoch: 1/2, step 6668/23838 completed (loss: 0.9067408442497253, acc: 0.7666666507720947)
[2025-02-16 12:15:26,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:26,314][root][INFO] - Training Epoch: 1/2, step 6669/23838 completed (loss: 1.0460467338562012, acc: 0.7204301357269287)
[2025-02-16 12:15:26,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:26,689][root][INFO] - Training Epoch: 1/2, step 6670/23838 completed (loss: 0.5775520205497742, acc: 0.8243243098258972)
[2025-02-16 12:15:26,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:27,147][root][INFO] - Training Epoch: 1/2, step 6671/23838 completed (loss: 0.6784102916717529, acc: 0.796875)
[2025-02-16 12:15:27,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:27,572][root][INFO] - Training Epoch: 1/2, step 6672/23838 completed (loss: 0.7369440793991089, acc: 0.7894737124443054)
[2025-02-16 12:15:27,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:28,028][root][INFO] - Training Epoch: 1/2, step 6673/23838 completed (loss: 0.593172550201416, acc: 0.8736842274665833)
[2025-02-16 12:15:28,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:28,498][root][INFO] - Training Epoch: 1/2, step 6674/23838 completed (loss: 0.5034211874008179, acc: 0.8600000143051147)
[2025-02-16 12:15:28,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:28,969][root][INFO] - Training Epoch: 1/2, step 6675/23838 completed (loss: 0.6190776228904724, acc: 0.8285714387893677)
[2025-02-16 12:15:29,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:29,425][root][INFO] - Training Epoch: 1/2, step 6676/23838 completed (loss: 0.7903812527656555, acc: 0.7434210777282715)
[2025-02-16 12:15:29,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:29,858][root][INFO] - Training Epoch: 1/2, step 6677/23838 completed (loss: 1.2898781299591064, acc: 0.641791045665741)
[2025-02-16 12:15:30,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:30,288][root][INFO] - Training Epoch: 1/2, step 6678/23838 completed (loss: 0.6589229106903076, acc: 0.7903226017951965)
[2025-02-16 12:15:30,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:30,871][root][INFO] - Training Epoch: 1/2, step 6679/23838 completed (loss: 0.8151257038116455, acc: 0.7567567825317383)
[2025-02-16 12:15:31,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:31,246][root][INFO] - Training Epoch: 1/2, step 6680/23838 completed (loss: 0.987687885761261, acc: 0.7108433842658997)
[2025-02-16 12:15:31,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:31,629][root][INFO] - Training Epoch: 1/2, step 6681/23838 completed (loss: 0.9854807257652283, acc: 0.7916666865348816)
[2025-02-16 12:15:31,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:32,145][root][INFO] - Training Epoch: 1/2, step 6682/23838 completed (loss: 0.7098971605300903, acc: 0.8115941882133484)
[2025-02-16 12:15:32,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:32,597][root][INFO] - Training Epoch: 1/2, step 6683/23838 completed (loss: 0.5465627312660217, acc: 0.8414633870124817)
[2025-02-16 12:15:32,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:33,047][root][INFO] - Training Epoch: 1/2, step 6684/23838 completed (loss: 0.5567271709442139, acc: 0.8552631735801697)
[2025-02-16 12:15:33,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:33,455][root][INFO] - Training Epoch: 1/2, step 6685/23838 completed (loss: 0.7303114533424377, acc: 0.8120805621147156)
[2025-02-16 12:15:33,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:33,897][root][INFO] - Training Epoch: 1/2, step 6686/23838 completed (loss: 0.7250246405601501, acc: 0.7721518874168396)
[2025-02-16 12:15:34,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:34,353][root][INFO] - Training Epoch: 1/2, step 6687/23838 completed (loss: 0.4870292544364929, acc: 0.8640000224113464)
[2025-02-16 12:15:34,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:34,875][root][INFO] - Training Epoch: 1/2, step 6688/23838 completed (loss: 1.0225540399551392, acc: 0.6800000071525574)
[2025-02-16 12:15:35,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:35,311][root][INFO] - Training Epoch: 1/2, step 6689/23838 completed (loss: 1.0441581010818481, acc: 0.7840909361839294)
[2025-02-16 12:15:35,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:35,705][root][INFO] - Training Epoch: 1/2, step 6690/23838 completed (loss: 0.7538308501243591, acc: 0.7368420958518982)
[2025-02-16 12:15:35,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:36,133][root][INFO] - Training Epoch: 1/2, step 6691/23838 completed (loss: 0.6012823581695557, acc: 0.7904762029647827)
[2025-02-16 12:15:36,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:36,534][root][INFO] - Training Epoch: 1/2, step 6692/23838 completed (loss: 0.8830848932266235, acc: 0.7716535329818726)
[2025-02-16 12:15:36,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:36,977][root][INFO] - Training Epoch: 1/2, step 6693/23838 completed (loss: 0.9694574475288391, acc: 0.699999988079071)
[2025-02-16 12:15:37,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:37,400][root][INFO] - Training Epoch: 1/2, step 6694/23838 completed (loss: 0.5934393405914307, acc: 0.8297872543334961)
[2025-02-16 12:15:37,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:37,969][root][INFO] - Training Epoch: 1/2, step 6695/23838 completed (loss: 1.1650006771087646, acc: 0.6904761791229248)
[2025-02-16 12:15:38,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:38,462][root][INFO] - Training Epoch: 1/2, step 6696/23838 completed (loss: 1.0522347688674927, acc: 0.7272727489471436)
[2025-02-16 12:15:38,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:38,820][root][INFO] - Training Epoch: 1/2, step 6697/23838 completed (loss: 1.1180331707000732, acc: 0.6285714507102966)
[2025-02-16 12:15:38,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:39,237][root][INFO] - Training Epoch: 1/2, step 6698/23838 completed (loss: 0.7054485082626343, acc: 0.8214285969734192)
[2025-02-16 12:15:39,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:39,709][root][INFO] - Training Epoch: 1/2, step 6699/23838 completed (loss: 0.5287748575210571, acc: 0.8405796885490417)
[2025-02-16 12:15:39,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:40,129][root][INFO] - Training Epoch: 1/2, step 6700/23838 completed (loss: 0.671988308429718, acc: 0.8148148059844971)
[2025-02-16 12:15:40,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:40,506][root][INFO] - Training Epoch: 1/2, step 6701/23838 completed (loss: 0.5198963284492493, acc: 0.8205128312110901)
[2025-02-16 12:15:40,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:40,900][root][INFO] - Training Epoch: 1/2, step 6702/23838 completed (loss: 0.8123090267181396, acc: 0.7753623127937317)
[2025-02-16 12:15:41,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:41,303][root][INFO] - Training Epoch: 1/2, step 6703/23838 completed (loss: 0.7018375992774963, acc: 0.8264462947845459)
[2025-02-16 12:15:41,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:41,738][root][INFO] - Training Epoch: 1/2, step 6704/23838 completed (loss: 0.8047888875007629, acc: 0.7857142686843872)
[2025-02-16 12:15:41,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:42,203][root][INFO] - Training Epoch: 1/2, step 6705/23838 completed (loss: 0.8265904188156128, acc: 0.7580645084381104)
[2025-02-16 12:15:42,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:42,703][root][INFO] - Training Epoch: 1/2, step 6706/23838 completed (loss: 0.7520387768745422, acc: 0.7948718070983887)
[2025-02-16 12:15:42,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:43,119][root][INFO] - Training Epoch: 1/2, step 6707/23838 completed (loss: 0.8893966674804688, acc: 0.6805555820465088)
[2025-02-16 12:15:43,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:43,592][root][INFO] - Training Epoch: 1/2, step 6708/23838 completed (loss: 0.8214541673660278, acc: 0.7582417726516724)
[2025-02-16 12:15:43,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:44,050][root][INFO] - Training Epoch: 1/2, step 6709/23838 completed (loss: 0.7484977841377258, acc: 0.7976190447807312)
[2025-02-16 12:15:44,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:44,477][root][INFO] - Training Epoch: 1/2, step 6710/23838 completed (loss: 0.9305680990219116, acc: 0.7524752616882324)
[2025-02-16 12:15:44,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:45,041][root][INFO] - Training Epoch: 1/2, step 6711/23838 completed (loss: 0.5857037901878357, acc: 0.8118811845779419)
[2025-02-16 12:15:45,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:45,414][root][INFO] - Training Epoch: 1/2, step 6712/23838 completed (loss: 0.6396329998970032, acc: 0.8101266026496887)
[2025-02-16 12:15:45,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:45,822][root][INFO] - Training Epoch: 1/2, step 6713/23838 completed (loss: 0.6041246056556702, acc: 0.8208954930305481)
[2025-02-16 12:15:46,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:46,243][root][INFO] - Training Epoch: 1/2, step 6714/23838 completed (loss: 0.5939907431602478, acc: 0.8145161271095276)
[2025-02-16 12:15:46,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:46,655][root][INFO] - Training Epoch: 1/2, step 6715/23838 completed (loss: 0.39999857544898987, acc: 0.8666666746139526)
[2025-02-16 12:15:46,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:47,038][root][INFO] - Training Epoch: 1/2, step 6716/23838 completed (loss: 0.21816276013851166, acc: 1.0)
[2025-02-16 12:15:47,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:47,416][root][INFO] - Training Epoch: 1/2, step 6717/23838 completed (loss: 0.4799235463142395, acc: 0.8636363744735718)
[2025-02-16 12:15:47,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:47,779][root][INFO] - Training Epoch: 1/2, step 6718/23838 completed (loss: 0.4592612385749817, acc: 1.0)
[2025-02-16 12:15:47,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:48,166][root][INFO] - Training Epoch: 1/2, step 6719/23838 completed (loss: 0.2723146080970764, acc: 0.8947368264198303)
[2025-02-16 12:15:48,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:48,572][root][INFO] - Training Epoch: 1/2, step 6720/23838 completed (loss: 0.8230882883071899, acc: 0.6666666865348816)
[2025-02-16 12:15:48,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:48,990][root][INFO] - Training Epoch: 1/2, step 6721/23838 completed (loss: 0.7611817121505737, acc: 0.8888888955116272)
[2025-02-16 12:15:49,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:49,366][root][INFO] - Training Epoch: 1/2, step 6722/23838 completed (loss: 1.6908105611801147, acc: 0.4166666567325592)
[2025-02-16 12:15:49,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:49,819][root][INFO] - Training Epoch: 1/2, step 6723/23838 completed (loss: 0.5267187356948853, acc: 0.800000011920929)
[2025-02-16 12:15:50,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:50,265][root][INFO] - Training Epoch: 1/2, step 6724/23838 completed (loss: 0.7984053492546082, acc: 0.8947368264198303)
[2025-02-16 12:15:50,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:51,007][root][INFO] - Training Epoch: 1/2, step 6725/23838 completed (loss: 1.1935251951217651, acc: 0.6666666865348816)
[2025-02-16 12:15:51,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:51,440][root][INFO] - Training Epoch: 1/2, step 6726/23838 completed (loss: 0.9994924664497375, acc: 0.75)
[2025-02-16 12:15:51,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:51,755][root][INFO] - Training Epoch: 1/2, step 6727/23838 completed (loss: 0.31693413853645325, acc: 0.9230769276618958)
[2025-02-16 12:15:51,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:52,130][root][INFO] - Training Epoch: 1/2, step 6728/23838 completed (loss: 1.9231380224227905, acc: 0.6071428656578064)
[2025-02-16 12:15:52,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:52,563][root][INFO] - Training Epoch: 1/2, step 6729/23838 completed (loss: 1.15817391872406, acc: 0.6875)
[2025-02-16 12:15:52,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:53,011][root][INFO] - Training Epoch: 1/2, step 6730/23838 completed (loss: 1.6481101512908936, acc: 0.6071428656578064)
[2025-02-16 12:15:53,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:53,432][root][INFO] - Training Epoch: 1/2, step 6731/23838 completed (loss: 1.410009741783142, acc: 0.6399999856948853)
[2025-02-16 12:15:53,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:53,891][root][INFO] - Training Epoch: 1/2, step 6732/23838 completed (loss: 1.3316287994384766, acc: 0.7222222089767456)
[2025-02-16 12:15:54,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:54,330][root][INFO] - Training Epoch: 1/2, step 6733/23838 completed (loss: 0.10836761444807053, acc: 1.0)
[2025-02-16 12:15:54,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:54,727][root][INFO] - Training Epoch: 1/2, step 6734/23838 completed (loss: 0.1325896978378296, acc: 1.0)
[2025-02-16 12:15:55,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:55,339][root][INFO] - Training Epoch: 1/2, step 6735/23838 completed (loss: 1.6761014461517334, acc: 0.6428571343421936)
[2025-02-16 12:15:55,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:55,745][root][INFO] - Training Epoch: 1/2, step 6736/23838 completed (loss: 1.6720101833343506, acc: 0.46666666865348816)
[2025-02-16 12:15:55,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:56,133][root][INFO] - Training Epoch: 1/2, step 6737/23838 completed (loss: 0.5791319012641907, acc: 0.8571428656578064)
[2025-02-16 12:15:56,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:56,501][root][INFO] - Training Epoch: 1/2, step 6738/23838 completed (loss: 0.9970351457595825, acc: 0.7142857313156128)
[2025-02-16 12:15:56,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:56,882][root][INFO] - Training Epoch: 1/2, step 6739/23838 completed (loss: 1.1657283306121826, acc: 0.699999988079071)
[2025-02-16 12:15:57,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:57,315][root][INFO] - Training Epoch: 1/2, step 6740/23838 completed (loss: 0.9278990030288696, acc: 0.692307710647583)
[2025-02-16 12:15:57,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:57,768][root][INFO] - Training Epoch: 1/2, step 6741/23838 completed (loss: 0.7465843558311462, acc: 0.8181818127632141)
[2025-02-16 12:15:57,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:58,162][root][INFO] - Training Epoch: 1/2, step 6742/23838 completed (loss: 1.2841178178787231, acc: 0.5833333134651184)
[2025-02-16 12:15:58,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:58,566][root][INFO] - Training Epoch: 1/2, step 6743/23838 completed (loss: 1.352041244506836, acc: 0.6666666865348816)
[2025-02-16 12:15:58,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:58,926][root][INFO] - Training Epoch: 1/2, step 6744/23838 completed (loss: 0.5638002753257751, acc: 0.8181818127632141)
[2025-02-16 12:15:59,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:59,311][root][INFO] - Training Epoch: 1/2, step 6745/23838 completed (loss: 0.700225293636322, acc: 0.7931034564971924)
[2025-02-16 12:15:59,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:15:59,675][root][INFO] - Training Epoch: 1/2, step 6746/23838 completed (loss: 0.3814474940299988, acc: 0.8666666746139526)
[2025-02-16 12:15:59,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:00,127][root][INFO] - Training Epoch: 1/2, step 6747/23838 completed (loss: 0.639984130859375, acc: 0.9230769276618958)
[2025-02-16 12:16:00,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:00,566][root][INFO] - Training Epoch: 1/2, step 6748/23838 completed (loss: 0.9203894734382629, acc: 0.6666666865348816)
[2025-02-16 12:16:00,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:01,059][root][INFO] - Training Epoch: 1/2, step 6749/23838 completed (loss: 2.283818006515503, acc: 0.4166666567325592)
[2025-02-16 12:16:01,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:01,528][root][INFO] - Training Epoch: 1/2, step 6750/23838 completed (loss: 0.6208083629608154, acc: 0.8095238208770752)
[2025-02-16 12:16:01,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:02,004][root][INFO] - Training Epoch: 1/2, step 6751/23838 completed (loss: 1.1435686349868774, acc: 0.7321428656578064)
[2025-02-16 12:16:02,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:02,483][root][INFO] - Training Epoch: 1/2, step 6752/23838 completed (loss: 1.1642440557479858, acc: 0.7289719581604004)
[2025-02-16 12:16:02,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:02,962][root][INFO] - Training Epoch: 1/2, step 6753/23838 completed (loss: 1.087130069732666, acc: 0.7377049326896667)
[2025-02-16 12:16:03,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:03,391][root][INFO] - Training Epoch: 1/2, step 6754/23838 completed (loss: 0.6190633773803711, acc: 0.8333333134651184)
[2025-02-16 12:16:03,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:03,877][root][INFO] - Training Epoch: 1/2, step 6755/23838 completed (loss: 0.8581083416938782, acc: 0.7755101919174194)
[2025-02-16 12:16:04,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:04,312][root][INFO] - Training Epoch: 1/2, step 6756/23838 completed (loss: 0.5106833577156067, acc: 0.8629032373428345)
[2025-02-16 12:16:04,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:04,763][root][INFO] - Training Epoch: 1/2, step 6757/23838 completed (loss: 0.7486114501953125, acc: 0.7840909361839294)
[2025-02-16 12:16:04,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:05,158][root][INFO] - Training Epoch: 1/2, step 6758/23838 completed (loss: 0.9234369397163391, acc: 0.7349397540092468)
[2025-02-16 12:16:05,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:05,558][root][INFO] - Training Epoch: 1/2, step 6759/23838 completed (loss: 0.5261957049369812, acc: 0.8390804529190063)
[2025-02-16 12:16:05,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:05,960][root][INFO] - Training Epoch: 1/2, step 6760/23838 completed (loss: 0.8101009130477905, acc: 0.75)
[2025-02-16 12:16:06,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:06,396][root][INFO] - Training Epoch: 1/2, step 6761/23838 completed (loss: 0.46805331110954285, acc: 0.8796296119689941)
[2025-02-16 12:16:06,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:06,822][root][INFO] - Training Epoch: 1/2, step 6762/23838 completed (loss: 0.8790489435195923, acc: 0.7866666913032532)
[2025-02-16 12:16:07,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:07,242][root][INFO] - Training Epoch: 1/2, step 6763/23838 completed (loss: 0.6111719608306885, acc: 0.8148148059844971)
[2025-02-16 12:16:07,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:07,639][root][INFO] - Training Epoch: 1/2, step 6764/23838 completed (loss: 0.6115018129348755, acc: 0.8526315689086914)
[2025-02-16 12:16:07,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:08,074][root][INFO] - Training Epoch: 1/2, step 6765/23838 completed (loss: 0.6584189534187317, acc: 0.8478260636329651)
[2025-02-16 12:16:08,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:08,515][root][INFO] - Training Epoch: 1/2, step 6766/23838 completed (loss: 0.6434619426727295, acc: 0.7777777910232544)
[2025-02-16 12:16:08,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:09,016][root][INFO] - Training Epoch: 1/2, step 6767/23838 completed (loss: 0.6255000829696655, acc: 0.8362573385238647)
[2025-02-16 12:16:09,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:09,469][root][INFO] - Training Epoch: 1/2, step 6768/23838 completed (loss: 0.926118016242981, acc: 0.7945205569267273)
[2025-02-16 12:16:09,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:09,953][root][INFO] - Training Epoch: 1/2, step 6769/23838 completed (loss: 1.0655877590179443, acc: 0.7752808928489685)
[2025-02-16 12:16:10,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:10,455][root][INFO] - Training Epoch: 1/2, step 6770/23838 completed (loss: 0.8104287981987, acc: 0.7983871102333069)
[2025-02-16 12:16:10,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:10,837][root][INFO] - Training Epoch: 1/2, step 6771/23838 completed (loss: 1.066455602645874, acc: 0.7540983557701111)
[2025-02-16 12:16:11,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:11,374][root][INFO] - Training Epoch: 1/2, step 6772/23838 completed (loss: 0.6258921027183533, acc: 0.8489208817481995)
[2025-02-16 12:16:11,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:11,930][root][INFO] - Training Epoch: 1/2, step 6773/23838 completed (loss: 0.7372246980667114, acc: 0.8248175382614136)
[2025-02-16 12:16:12,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:12,383][root][INFO] - Training Epoch: 1/2, step 6774/23838 completed (loss: 0.8060499429702759, acc: 0.8275862336158752)
[2025-02-16 12:16:12,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:12,849][root][INFO] - Training Epoch: 1/2, step 6775/23838 completed (loss: 1.087591528892517, acc: 0.738095223903656)
[2025-02-16 12:16:13,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:13,290][root][INFO] - Training Epoch: 1/2, step 6776/23838 completed (loss: 0.4583200216293335, acc: 0.8761062026023865)
[2025-02-16 12:16:13,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:13,688][root][INFO] - Training Epoch: 1/2, step 6777/23838 completed (loss: 0.47644224762916565, acc: 0.8601398468017578)
[2025-02-16 12:16:13,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:14,116][root][INFO] - Training Epoch: 1/2, step 6778/23838 completed (loss: 0.5362637639045715, acc: 0.8757396340370178)
[2025-02-16 12:16:14,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:14,498][root][INFO] - Training Epoch: 1/2, step 6779/23838 completed (loss: 0.7434549331665039, acc: 0.7865168452262878)
[2025-02-16 12:16:14,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:14,892][root][INFO] - Training Epoch: 1/2, step 6780/23838 completed (loss: 0.7156524658203125, acc: 0.8225806355476379)
[2025-02-16 12:16:15,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:15,294][root][INFO] - Training Epoch: 1/2, step 6781/23838 completed (loss: 0.8889956474304199, acc: 0.7804877758026123)
[2025-02-16 12:16:15,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:15,754][root][INFO] - Training Epoch: 1/2, step 6782/23838 completed (loss: 0.37732836604118347, acc: 0.8645833134651184)
[2025-02-16 12:16:15,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:16,237][root][INFO] - Training Epoch: 1/2, step 6783/23838 completed (loss: 0.6155780553817749, acc: 0.791208803653717)
[2025-02-16 12:16:16,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:16,713][root][INFO] - Training Epoch: 1/2, step 6784/23838 completed (loss: 0.20728915929794312, acc: 0.9340659379959106)
[2025-02-16 12:16:17,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:17,304][root][INFO] - Training Epoch: 1/2, step 6785/23838 completed (loss: 0.6903592944145203, acc: 0.8134328126907349)
[2025-02-16 12:16:17,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:18,024][root][INFO] - Training Epoch: 1/2, step 6786/23838 completed (loss: 0.5466450452804565, acc: 0.8931297659873962)
[2025-02-16 12:16:18,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:18,474][root][INFO] - Training Epoch: 1/2, step 6787/23838 completed (loss: 0.4902421832084656, acc: 0.8837209343910217)
[2025-02-16 12:16:18,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:18,916][root][INFO] - Training Epoch: 1/2, step 6788/23838 completed (loss: 0.8347716331481934, acc: 0.7977527976036072)
[2025-02-16 12:16:19,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:19,508][root][INFO] - Training Epoch: 1/2, step 6789/23838 completed (loss: 0.7709929347038269, acc: 0.8014705777168274)
[2025-02-16 12:16:19,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:19,957][root][INFO] - Training Epoch: 1/2, step 6790/23838 completed (loss: 0.5427992343902588, acc: 0.8320000171661377)
[2025-02-16 12:16:20,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:20,406][root][INFO] - Training Epoch: 1/2, step 6791/23838 completed (loss: 0.22416271269321442, acc: 0.976190447807312)
[2025-02-16 12:16:20,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:20,824][root][INFO] - Training Epoch: 1/2, step 6792/23838 completed (loss: 0.7492315173149109, acc: 0.78125)
[2025-02-16 12:16:21,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:21,273][root][INFO] - Training Epoch: 1/2, step 6793/23838 completed (loss: 0.6379820108413696, acc: 0.8255813717842102)
[2025-02-16 12:16:21,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:21,711][root][INFO] - Training Epoch: 1/2, step 6794/23838 completed (loss: 1.3484214544296265, acc: 0.6410256624221802)
[2025-02-16 12:16:21,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:22,174][root][INFO] - Training Epoch: 1/2, step 6795/23838 completed (loss: 1.0889428853988647, acc: 0.7303370833396912)
[2025-02-16 12:16:22,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:22,582][root][INFO] - Training Epoch: 1/2, step 6796/23838 completed (loss: 0.21362803876399994, acc: 0.9382715821266174)
[2025-02-16 12:16:22,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:22,973][root][INFO] - Training Epoch: 1/2, step 6797/23838 completed (loss: 0.4995523989200592, acc: 0.8225806355476379)
[2025-02-16 12:16:23,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:23,480][root][INFO] - Training Epoch: 1/2, step 6798/23838 completed (loss: 1.1113358736038208, acc: 0.7014925479888916)
[2025-02-16 12:16:23,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:23,946][root][INFO] - Training Epoch: 1/2, step 6799/23838 completed (loss: 0.8914210200309753, acc: 0.7790697813034058)
[2025-02-16 12:16:24,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:24,414][root][INFO] - Training Epoch: 1/2, step 6800/23838 completed (loss: 0.6040687561035156, acc: 0.8241758346557617)
[2025-02-16 12:16:24,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:24,864][root][INFO] - Training Epoch: 1/2, step 6801/23838 completed (loss: 1.5394339561462402, acc: 0.5396825671195984)
[2025-02-16 12:16:25,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:25,301][root][INFO] - Training Epoch: 1/2, step 6802/23838 completed (loss: 0.6626137495040894, acc: 0.8235294222831726)
[2025-02-16 12:16:25,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:25,790][root][INFO] - Training Epoch: 1/2, step 6803/23838 completed (loss: 0.5053789019584656, acc: 0.84375)
[2025-02-16 12:16:26,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:26,278][root][INFO] - Training Epoch: 1/2, step 6804/23838 completed (loss: 0.45010143518447876, acc: 0.8644067645072937)
[2025-02-16 12:16:26,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:26,755][root][INFO] - Training Epoch: 1/2, step 6805/23838 completed (loss: 0.5286566615104675, acc: 0.8771929740905762)
[2025-02-16 12:16:26,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:27,171][root][INFO] - Training Epoch: 1/2, step 6806/23838 completed (loss: 0.6327347755432129, acc: 0.813725471496582)
[2025-02-16 12:16:27,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:27,551][root][INFO] - Training Epoch: 1/2, step 6807/23838 completed (loss: 0.5653935670852661, acc: 0.8552631735801697)
[2025-02-16 12:16:27,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:27,923][root][INFO] - Training Epoch: 1/2, step 6808/23838 completed (loss: 0.5326870679855347, acc: 0.8571428656578064)
[2025-02-16 12:16:28,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:28,741][root][INFO] - Training Epoch: 1/2, step 6809/23838 completed (loss: 0.8152353763580322, acc: 0.8299999833106995)
[2025-02-16 12:16:28,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:29,217][root][INFO] - Training Epoch: 1/2, step 6810/23838 completed (loss: 0.28878316283226013, acc: 0.936170220375061)
[2025-02-16 12:16:29,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:29,679][root][INFO] - Training Epoch: 1/2, step 6811/23838 completed (loss: 0.8043917417526245, acc: 0.762499988079071)
[2025-02-16 12:16:30,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:30,731][root][INFO] - Training Epoch: 1/2, step 6812/23838 completed (loss: 0.25008660554885864, acc: 0.9350649118423462)
[2025-02-16 12:16:30,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:31,198][root][INFO] - Training Epoch: 1/2, step 6813/23838 completed (loss: 0.7953844666481018, acc: 0.8444444537162781)
[2025-02-16 12:16:31,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:31,733][root][INFO] - Training Epoch: 1/2, step 6814/23838 completed (loss: 0.675270140171051, acc: 0.8226950168609619)
[2025-02-16 12:16:31,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:32,212][root][INFO] - Training Epoch: 1/2, step 6815/23838 completed (loss: 0.2847193479537964, acc: 0.9268292784690857)
[2025-02-16 12:16:32,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:32,681][root][INFO] - Training Epoch: 1/2, step 6816/23838 completed (loss: 0.371209055185318, acc: 0.9247311949729919)
[2025-02-16 12:16:32,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:33,250][root][INFO] - Training Epoch: 1/2, step 6817/23838 completed (loss: 0.3661980628967285, acc: 0.8863636255264282)
[2025-02-16 12:16:33,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:33,697][root][INFO] - Training Epoch: 1/2, step 6818/23838 completed (loss: 0.23316690325737, acc: 0.9390243887901306)
[2025-02-16 12:16:33,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:34,161][root][INFO] - Training Epoch: 1/2, step 6819/23838 completed (loss: 0.6004071831703186, acc: 0.8235294222831726)
[2025-02-16 12:16:34,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:34,622][root][INFO] - Training Epoch: 1/2, step 6820/23838 completed (loss: 0.4251018464565277, acc: 0.8899999856948853)
[2025-02-16 12:16:34,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:35,109][root][INFO] - Training Epoch: 1/2, step 6821/23838 completed (loss: 0.5942643880844116, acc: 0.850649356842041)
[2025-02-16 12:16:35,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:35,486][root][INFO] - Training Epoch: 1/2, step 6822/23838 completed (loss: 0.6702373027801514, acc: 0.8714285492897034)
[2025-02-16 12:16:35,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:35,945][root][INFO] - Training Epoch: 1/2, step 6823/23838 completed (loss: 0.32775160670280457, acc: 0.9247311949729919)
[2025-02-16 12:16:36,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:36,470][root][INFO] - Training Epoch: 1/2, step 6824/23838 completed (loss: 0.5425238013267517, acc: 0.8833333253860474)
[2025-02-16 12:16:36,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:36,943][root][INFO] - Training Epoch: 1/2, step 6825/23838 completed (loss: 0.7674373984336853, acc: 0.7943925261497498)
[2025-02-16 12:16:37,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:37,368][root][INFO] - Training Epoch: 1/2, step 6826/23838 completed (loss: 0.8264073133468628, acc: 0.8039215803146362)
[2025-02-16 12:16:37,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:38,171][root][INFO] - Training Epoch: 1/2, step 6827/23838 completed (loss: 0.5763866305351257, acc: 0.8571428656578064)
[2025-02-16 12:16:38,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:38,852][root][INFO] - Training Epoch: 1/2, step 6828/23838 completed (loss: 0.4908926784992218, acc: 0.8842592835426331)
[2025-02-16 12:16:39,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:39,816][root][INFO] - Training Epoch: 1/2, step 6829/23838 completed (loss: 0.4717414677143097, acc: 0.8681318759918213)
[2025-02-16 12:16:40,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:40,551][root][INFO] - Training Epoch: 1/2, step 6830/23838 completed (loss: 0.8382968902587891, acc: 0.8072289228439331)
[2025-02-16 12:16:40,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:41,365][root][INFO] - Training Epoch: 1/2, step 6831/23838 completed (loss: 0.6319399476051331, acc: 0.8514285683631897)
[2025-02-16 12:16:41,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:41,934][root][INFO] - Training Epoch: 1/2, step 6832/23838 completed (loss: 0.4407137930393219, acc: 0.8977272510528564)
[2025-02-16 12:16:42,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:42,535][root][INFO] - Training Epoch: 1/2, step 6833/23838 completed (loss: 0.3623849153518677, acc: 0.9166666865348816)
[2025-02-16 12:16:42,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:43,255][root][INFO] - Training Epoch: 1/2, step 6834/23838 completed (loss: 0.34071844816207886, acc: 0.8961039185523987)
[2025-02-16 12:16:43,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:43,744][root][INFO] - Training Epoch: 1/2, step 6835/23838 completed (loss: 0.74358731508255, acc: 0.7878788113594055)
[2025-02-16 12:16:44,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:44,431][root][INFO] - Training Epoch: 1/2, step 6836/23838 completed (loss: 0.6188318133354187, acc: 0.8807339668273926)
[2025-02-16 12:16:44,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:44,898][root][INFO] - Training Epoch: 1/2, step 6837/23838 completed (loss: 0.252188503742218, acc: 0.8905109763145447)
[2025-02-16 12:16:45,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:45,338][root][INFO] - Training Epoch: 1/2, step 6838/23838 completed (loss: 0.4395054578781128, acc: 0.90625)
[2025-02-16 12:16:45,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:45,763][root][INFO] - Training Epoch: 1/2, step 6839/23838 completed (loss: 0.7173764705657959, acc: 0.7796609997749329)
[2025-02-16 12:16:45,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:46,175][root][INFO] - Training Epoch: 1/2, step 6840/23838 completed (loss: 0.60057133436203, acc: 0.8877550959587097)
[2025-02-16 12:16:46,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:46,631][root][INFO] - Training Epoch: 1/2, step 6841/23838 completed (loss: 0.4059911072254181, acc: 0.9155844449996948)
[2025-02-16 12:16:46,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:47,087][root][INFO] - Training Epoch: 1/2, step 6842/23838 completed (loss: 0.43825775384902954, acc: 0.9245283007621765)
[2025-02-16 12:16:47,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:47,448][root][INFO] - Training Epoch: 1/2, step 6843/23838 completed (loss: 0.4887571632862091, acc: 0.8859649300575256)
[2025-02-16 12:16:47,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:47,855][root][INFO] - Training Epoch: 1/2, step 6844/23838 completed (loss: 0.12590311467647552, acc: 0.9632353186607361)
[2025-02-16 12:16:48,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:48,251][root][INFO] - Training Epoch: 1/2, step 6845/23838 completed (loss: 0.2012435495853424, acc: 0.9215686321258545)
[2025-02-16 12:16:48,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:48,673][root][INFO] - Training Epoch: 1/2, step 6846/23838 completed (loss: 0.3625856935977936, acc: 0.8902438879013062)
[2025-02-16 12:16:48,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:49,132][root][INFO] - Training Epoch: 1/2, step 6847/23838 completed (loss: 0.4643089771270752, acc: 0.9124087691307068)
[2025-02-16 12:16:49,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:49,559][root][INFO] - Training Epoch: 1/2, step 6848/23838 completed (loss: 0.34014269709587097, acc: 0.9462365508079529)
[2025-02-16 12:16:49,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:49,966][root][INFO] - Training Epoch: 1/2, step 6849/23838 completed (loss: 0.5828039646148682, acc: 0.8571428656578064)
[2025-02-16 12:16:50,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:50,398][root][INFO] - Training Epoch: 1/2, step 6850/23838 completed (loss: 0.2373354285955429, acc: 0.9473684430122375)
[2025-02-16 12:16:50,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:50,873][root][INFO] - Training Epoch: 1/2, step 6851/23838 completed (loss: 0.26729243993759155, acc: 0.9337748289108276)
[2025-02-16 12:16:51,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:51,301][root][INFO] - Training Epoch: 1/2, step 6852/23838 completed (loss: 0.3261485695838928, acc: 0.9086294174194336)
[2025-02-16 12:16:51,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:51,703][root][INFO] - Training Epoch: 1/2, step 6853/23838 completed (loss: 0.2289440780878067, acc: 0.9452054500579834)
[2025-02-16 12:16:51,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:52,096][root][INFO] - Training Epoch: 1/2, step 6854/23838 completed (loss: 0.2715485095977783, acc: 0.9047619104385376)
[2025-02-16 12:16:52,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:52,547][root][INFO] - Training Epoch: 1/2, step 6855/23838 completed (loss: 0.6002842783927917, acc: 0.8285714387893677)
[2025-02-16 12:16:52,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:52,995][root][INFO] - Training Epoch: 1/2, step 6856/23838 completed (loss: 0.2926577627658844, acc: 0.9272727370262146)
[2025-02-16 12:16:53,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:53,440][root][INFO] - Training Epoch: 1/2, step 6857/23838 completed (loss: 0.4486522674560547, acc: 0.8571428656578064)
[2025-02-16 12:16:53,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:53,853][root][INFO] - Training Epoch: 1/2, step 6858/23838 completed (loss: 0.38182663917541504, acc: 0.8640776872634888)
[2025-02-16 12:16:54,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:54,284][root][INFO] - Training Epoch: 1/2, step 6859/23838 completed (loss: 0.475018173456192, acc: 0.8706896305084229)
[2025-02-16 12:16:54,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:54,704][root][INFO] - Training Epoch: 1/2, step 6860/23838 completed (loss: 0.6352035999298096, acc: 0.8196721076965332)
[2025-02-16 12:16:54,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:55,135][root][INFO] - Training Epoch: 1/2, step 6861/23838 completed (loss: 0.4185696244239807, acc: 0.8805969953536987)
[2025-02-16 12:16:55,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:55,580][root][INFO] - Training Epoch: 1/2, step 6862/23838 completed (loss: 0.7484785318374634, acc: 0.7799999713897705)
[2025-02-16 12:16:55,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:56,061][root][INFO] - Training Epoch: 1/2, step 6863/23838 completed (loss: 0.2608852982521057, acc: 0.9150943160057068)
[2025-02-16 12:16:56,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:56,553][root][INFO] - Training Epoch: 1/2, step 6864/23838 completed (loss: 0.15180684626102448, acc: 0.9624060392379761)
[2025-02-16 12:16:57,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:57,455][root][INFO] - Training Epoch: 1/2, step 6865/23838 completed (loss: 0.22051192820072174, acc: 0.9526315927505493)
[2025-02-16 12:16:57,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:57,873][root][INFO] - Training Epoch: 1/2, step 6866/23838 completed (loss: 0.4755401909351349, acc: 0.8518518805503845)
[2025-02-16 12:16:58,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:58,362][root][INFO] - Training Epoch: 1/2, step 6867/23838 completed (loss: 0.4016571342945099, acc: 0.8461538553237915)
[2025-02-16 12:16:58,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:58,808][root][INFO] - Training Epoch: 1/2, step 6868/23838 completed (loss: 0.46624261140823364, acc: 0.847328245639801)
[2025-02-16 12:16:58,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:59,197][root][INFO] - Training Epoch: 1/2, step 6869/23838 completed (loss: 0.4482994079589844, acc: 0.8399999737739563)
[2025-02-16 12:16:59,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:16:59,569][root][INFO] - Training Epoch: 1/2, step 6870/23838 completed (loss: 0.4924273192882538, acc: 0.8552631735801697)
[2025-02-16 12:16:59,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:00,054][root][INFO] - Training Epoch: 1/2, step 6871/23838 completed (loss: 0.2676003873348236, acc: 0.9119496941566467)
[2025-02-16 12:17:00,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:00,483][root][INFO] - Training Epoch: 1/2, step 6872/23838 completed (loss: 0.44012945890426636, acc: 0.8965517282485962)
[2025-02-16 12:17:00,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:00,967][root][INFO] - Training Epoch: 1/2, step 6873/23838 completed (loss: 0.624717652797699, acc: 0.843137264251709)
[2025-02-16 12:17:01,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:01,404][root][INFO] - Training Epoch: 1/2, step 6874/23838 completed (loss: 0.23730754852294922, acc: 0.95652174949646)
[2025-02-16 12:17:01,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:01,885][root][INFO] - Training Epoch: 1/2, step 6875/23838 completed (loss: 0.7659400701522827, acc: 0.8888888955116272)
[2025-02-16 12:17:02,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:02,401][root][INFO] - Training Epoch: 1/2, step 6876/23838 completed (loss: 0.2108900547027588, acc: 0.932692289352417)
[2025-02-16 12:17:02,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:02,799][root][INFO] - Training Epoch: 1/2, step 6877/23838 completed (loss: 0.21377134323120117, acc: 0.9469696879386902)
[2025-02-16 12:17:02,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:03,220][root][INFO] - Training Epoch: 1/2, step 6878/23838 completed (loss: 0.314413845539093, acc: 0.9248120188713074)
[2025-02-16 12:17:03,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:03,665][root][INFO] - Training Epoch: 1/2, step 6879/23838 completed (loss: 0.9253233671188354, acc: 0.7627118825912476)
[2025-02-16 12:17:03,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:04,113][root][INFO] - Training Epoch: 1/2, step 6880/23838 completed (loss: 0.4501502513885498, acc: 0.887499988079071)
[2025-02-16 12:17:04,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:04,982][root][INFO] - Training Epoch: 1/2, step 6881/23838 completed (loss: 0.5138051509857178, acc: 0.8678414225578308)
[2025-02-16 12:17:05,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:05,433][root][INFO] - Training Epoch: 1/2, step 6882/23838 completed (loss: 0.5246064066886902, acc: 0.8700000047683716)
[2025-02-16 12:17:05,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:05,876][root][INFO] - Training Epoch: 1/2, step 6883/23838 completed (loss: 0.3597683012485504, acc: 0.9172932505607605)
[2025-02-16 12:17:06,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:06,281][root][INFO] - Training Epoch: 1/2, step 6884/23838 completed (loss: 1.275647759437561, acc: 0.6461538672447205)
[2025-02-16 12:17:06,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:06,727][root][INFO] - Training Epoch: 1/2, step 6885/23838 completed (loss: 0.25729745626449585, acc: 0.9506173133850098)
[2025-02-16 12:17:06,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:07,102][root][INFO] - Training Epoch: 1/2, step 6886/23838 completed (loss: 0.2577996551990509, acc: 0.9108280539512634)
[2025-02-16 12:17:07,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:07,518][root][INFO] - Training Epoch: 1/2, step 6887/23838 completed (loss: 0.2720990777015686, acc: 0.9375)
[2025-02-16 12:17:07,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:07,892][root][INFO] - Training Epoch: 1/2, step 6888/23838 completed (loss: 0.45691850781440735, acc: 0.8461538553237915)
[2025-02-16 12:17:08,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:08,364][root][INFO] - Training Epoch: 1/2, step 6889/23838 completed (loss: 0.4431966543197632, acc: 0.8684210777282715)
[2025-02-16 12:17:08,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:08,767][root][INFO] - Training Epoch: 1/2, step 6890/23838 completed (loss: 0.5300770998001099, acc: 0.8095238208770752)
[2025-02-16 12:17:08,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:09,096][root][INFO] - Training Epoch: 1/2, step 6891/23838 completed (loss: 0.2070421427488327, acc: 0.9466666579246521)
[2025-02-16 12:17:09,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:09,574][root][INFO] - Training Epoch: 1/2, step 6892/23838 completed (loss: 0.5852270722389221, acc: 0.8607594966888428)
[2025-02-16 12:17:09,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:09,941][root][INFO] - Training Epoch: 1/2, step 6893/23838 completed (loss: 0.1703660935163498, acc: 0.9496402740478516)
[2025-02-16 12:17:10,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:10,318][root][INFO] - Training Epoch: 1/2, step 6894/23838 completed (loss: 0.38241133093833923, acc: 0.9104477763175964)
[2025-02-16 12:17:10,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:10,750][root][INFO] - Training Epoch: 1/2, step 6895/23838 completed (loss: 0.4298504889011383, acc: 0.9027777910232544)
[2025-02-16 12:17:10,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:11,169][root][INFO] - Training Epoch: 1/2, step 6896/23838 completed (loss: 0.405087411403656, acc: 0.8627451062202454)
[2025-02-16 12:17:11,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:11,627][root][INFO] - Training Epoch: 1/2, step 6897/23838 completed (loss: 0.11020845919847488, acc: 0.9741935729980469)
[2025-02-16 12:17:11,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:12,073][root][INFO] - Training Epoch: 1/2, step 6898/23838 completed (loss: 0.33036622405052185, acc: 0.9191918969154358)
[2025-02-16 12:17:12,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:13,067][root][INFO] - Training Epoch: 1/2, step 6899/23838 completed (loss: 0.40538930892944336, acc: 0.896774172782898)
[2025-02-16 12:17:13,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:13,491][root][INFO] - Training Epoch: 1/2, step 6900/23838 completed (loss: 0.21247150003910065, acc: 0.9411764740943909)
[2025-02-16 12:17:13,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:13,914][root][INFO] - Training Epoch: 1/2, step 6901/23838 completed (loss: 0.48124831914901733, acc: 0.8589743375778198)
[2025-02-16 12:17:14,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:14,296][root][INFO] - Training Epoch: 1/2, step 6902/23838 completed (loss: 1.3111364841461182, acc: 0.671875)
[2025-02-16 12:17:14,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:14,696][root][INFO] - Training Epoch: 1/2, step 6903/23838 completed (loss: 0.44166603684425354, acc: 0.859649121761322)
[2025-02-16 12:17:14,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:15,130][root][INFO] - Training Epoch: 1/2, step 6904/23838 completed (loss: 0.29441431164741516, acc: 0.9312499761581421)
[2025-02-16 12:17:15,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:15,596][root][INFO] - Training Epoch: 1/2, step 6905/23838 completed (loss: 0.1493408977985382, acc: 0.9609375)
[2025-02-16 12:17:15,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:16,046][root][INFO] - Training Epoch: 1/2, step 6906/23838 completed (loss: 1.0317667722702026, acc: 0.6760563254356384)
[2025-02-16 12:17:16,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:16,502][root][INFO] - Training Epoch: 1/2, step 6907/23838 completed (loss: 0.5358935594558716, acc: 0.8404255509376526)
[2025-02-16 12:17:16,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:16,900][root][INFO] - Training Epoch: 1/2, step 6908/23838 completed (loss: 0.24057939648628235, acc: 0.929411768913269)
[2025-02-16 12:17:17,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:17,609][root][INFO] - Training Epoch: 1/2, step 6909/23838 completed (loss: 0.3804122507572174, acc: 0.8983957171440125)
[2025-02-16 12:17:17,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:18,048][root][INFO] - Training Epoch: 1/2, step 6910/23838 completed (loss: 0.12725290656089783, acc: 0.9647058844566345)
[2025-02-16 12:17:18,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:18,420][root][INFO] - Training Epoch: 1/2, step 6911/23838 completed (loss: 0.367776483297348, acc: 0.8554216623306274)
[2025-02-16 12:17:18,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:18,791][root][INFO] - Training Epoch: 1/2, step 6912/23838 completed (loss: 0.38527706265449524, acc: 0.8876404762268066)
[2025-02-16 12:17:19,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:19,289][root][INFO] - Training Epoch: 1/2, step 6913/23838 completed (loss: 0.510840654373169, acc: 0.8406593203544617)
[2025-02-16 12:17:19,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:19,803][root][INFO] - Training Epoch: 1/2, step 6914/23838 completed (loss: 0.5611724853515625, acc: 0.8300653696060181)
[2025-02-16 12:17:20,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:20,276][root][INFO] - Training Epoch: 1/2, step 6915/23838 completed (loss: 0.5872685313224792, acc: 0.8387096524238586)
[2025-02-16 12:17:20,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:20,686][root][INFO] - Training Epoch: 1/2, step 6916/23838 completed (loss: 0.2070302814245224, acc: 0.9354838728904724)
[2025-02-16 12:17:20,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:21,108][root][INFO] - Training Epoch: 1/2, step 6917/23838 completed (loss: 0.5664829015731812, acc: 0.8354430198669434)
[2025-02-16 12:17:21,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:21,583][root][INFO] - Training Epoch: 1/2, step 6918/23838 completed (loss: 0.6635660529136658, acc: 0.805084764957428)
[2025-02-16 12:17:21,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:22,021][root][INFO] - Training Epoch: 1/2, step 6919/23838 completed (loss: 0.3006400763988495, acc: 0.921875)
[2025-02-16 12:17:22,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:22,476][root][INFO] - Training Epoch: 1/2, step 6920/23838 completed (loss: 0.4264073669910431, acc: 0.8828125)
[2025-02-16 12:17:22,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:22,909][root][INFO] - Training Epoch: 1/2, step 6921/23838 completed (loss: 0.5415438413619995, acc: 0.8681318759918213)
[2025-02-16 12:17:23,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:23,315][root][INFO] - Training Epoch: 1/2, step 6922/23838 completed (loss: 0.08131501078605652, acc: 0.9898989796638489)
[2025-02-16 12:17:23,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:23,729][root][INFO] - Training Epoch: 1/2, step 6923/23838 completed (loss: 0.2021109014749527, acc: 0.936170220375061)
[2025-02-16 12:17:23,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:24,179][root][INFO] - Training Epoch: 1/2, step 6924/23838 completed (loss: 0.07673246413469315, acc: 0.9759036302566528)
[2025-02-16 12:17:24,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:24,600][root][INFO] - Training Epoch: 1/2, step 6925/23838 completed (loss: 0.2714754045009613, acc: 0.9139785170555115)
[2025-02-16 12:17:24,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:25,031][root][INFO] - Training Epoch: 1/2, step 6926/23838 completed (loss: 0.4447679817676544, acc: 0.8974359035491943)
[2025-02-16 12:17:25,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:25,405][root][INFO] - Training Epoch: 1/2, step 6927/23838 completed (loss: 0.12551525235176086, acc: 0.969072163105011)
[2025-02-16 12:17:25,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:26,135][root][INFO] - Training Epoch: 1/2, step 6928/23838 completed (loss: 0.40615251660346985, acc: 0.89552241563797)
[2025-02-16 12:17:26,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:26,600][root][INFO] - Training Epoch: 1/2, step 6929/23838 completed (loss: 0.24699638783931732, acc: 0.9160839319229126)
[2025-02-16 12:17:26,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:27,010][root][INFO] - Training Epoch: 1/2, step 6930/23838 completed (loss: 0.16074508428573608, acc: 0.9722222089767456)
[2025-02-16 12:17:27,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:27,460][root][INFO] - Training Epoch: 1/2, step 6931/23838 completed (loss: 0.504022479057312, acc: 0.8850574493408203)
[2025-02-16 12:17:27,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:27,954][root][INFO] - Training Epoch: 1/2, step 6932/23838 completed (loss: 0.16882309317588806, acc: 0.9655172228813171)
[2025-02-16 12:17:28,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:28,420][root][INFO] - Training Epoch: 1/2, step 6933/23838 completed (loss: 0.7537183165550232, acc: 0.800000011920929)
[2025-02-16 12:17:28,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:28,848][root][INFO] - Training Epoch: 1/2, step 6934/23838 completed (loss: 0.6800597310066223, acc: 0.8428571224212646)
[2025-02-16 12:17:29,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:29,312][root][INFO] - Training Epoch: 1/2, step 6935/23838 completed (loss: 0.08332373946905136, acc: 0.9729729890823364)
[2025-02-16 12:17:29,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:29,728][root][INFO] - Training Epoch: 1/2, step 6936/23838 completed (loss: 0.19285409152507782, acc: 0.9174311757087708)
[2025-02-16 12:17:29,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:30,184][root][INFO] - Training Epoch: 1/2, step 6937/23838 completed (loss: 0.14952412247657776, acc: 0.9636363387107849)
[2025-02-16 12:17:30,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:30,650][root][INFO] - Training Epoch: 1/2, step 6938/23838 completed (loss: 0.28058698773384094, acc: 0.9158415794372559)
[2025-02-16 12:17:30,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:31,107][root][INFO] - Training Epoch: 1/2, step 6939/23838 completed (loss: 0.43590110540390015, acc: 0.8774510025978088)
[2025-02-16 12:17:31,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:31,506][root][INFO] - Training Epoch: 1/2, step 6940/23838 completed (loss: 0.9810004234313965, acc: 0.7105262875556946)
[2025-02-16 12:17:31,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:31,964][root][INFO] - Training Epoch: 1/2, step 6941/23838 completed (loss: 1.1167360544204712, acc: 0.7368420958518982)
[2025-02-16 12:17:32,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:32,350][root][INFO] - Training Epoch: 1/2, step 6942/23838 completed (loss: 0.22254107892513275, acc: 0.9296875)
[2025-02-16 12:17:32,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:32,805][root][INFO] - Training Epoch: 1/2, step 6943/23838 completed (loss: 0.27303415536880493, acc: 0.9210526347160339)
[2025-02-16 12:17:32,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:33,224][root][INFO] - Training Epoch: 1/2, step 6944/23838 completed (loss: 0.4004811942577362, acc: 0.9174311757087708)
[2025-02-16 12:17:33,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:33,637][root][INFO] - Training Epoch: 1/2, step 6945/23838 completed (loss: 0.41891244053840637, acc: 0.8818181753158569)
[2025-02-16 12:17:33,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:34,029][root][INFO] - Training Epoch: 1/2, step 6946/23838 completed (loss: 0.16383935511112213, acc: 0.9626168012619019)
[2025-02-16 12:17:34,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:34,430][root][INFO] - Training Epoch: 1/2, step 6947/23838 completed (loss: 0.25960689783096313, acc: 0.9253731369972229)
[2025-02-16 12:17:34,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:35,220][root][INFO] - Training Epoch: 1/2, step 6948/23838 completed (loss: 0.5100576281547546, acc: 0.8725489974021912)
[2025-02-16 12:17:35,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:35,654][root][INFO] - Training Epoch: 1/2, step 6949/23838 completed (loss: 1.1504989862442017, acc: 0.7285714149475098)
[2025-02-16 12:17:35,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:36,106][root][INFO] - Training Epoch: 1/2, step 6950/23838 completed (loss: 0.5066899061203003, acc: 0.8690476417541504)
[2025-02-16 12:17:36,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:36,536][root][INFO] - Training Epoch: 1/2, step 6951/23838 completed (loss: 0.2137736678123474, acc: 0.9391891956329346)
[2025-02-16 12:17:36,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:36,956][root][INFO] - Training Epoch: 1/2, step 6952/23838 completed (loss: 0.7107474207878113, acc: 0.7777777910232544)
[2025-02-16 12:17:37,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:37,402][root][INFO] - Training Epoch: 1/2, step 6953/23838 completed (loss: 0.19176791608333588, acc: 0.9318181872367859)
[2025-02-16 12:17:37,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:37,820][root][INFO] - Training Epoch: 1/2, step 6954/23838 completed (loss: 0.2541310489177704, acc: 0.949999988079071)
[2025-02-16 12:17:37,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:38,216][root][INFO] - Training Epoch: 1/2, step 6955/23838 completed (loss: 0.9832953214645386, acc: 0.7564102411270142)
[2025-02-16 12:17:38,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:38,639][root][INFO] - Training Epoch: 1/2, step 6956/23838 completed (loss: 0.702863872051239, acc: 0.8275862336158752)
[2025-02-16 12:17:38,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:39,073][root][INFO] - Training Epoch: 1/2, step 6957/23838 completed (loss: 1.0044474601745605, acc: 0.7317073345184326)
[2025-02-16 12:17:39,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:39,647][root][INFO] - Training Epoch: 1/2, step 6958/23838 completed (loss: 0.1933240294456482, acc: 0.960629940032959)
[2025-02-16 12:17:39,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:40,094][root][INFO] - Training Epoch: 1/2, step 6959/23838 completed (loss: 0.4060703217983246, acc: 0.855555534362793)
[2025-02-16 12:17:40,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:40,489][root][INFO] - Training Epoch: 1/2, step 6960/23838 completed (loss: 0.4287741780281067, acc: 0.9090909361839294)
[2025-02-16 12:17:40,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:40,863][root][INFO] - Training Epoch: 1/2, step 6961/23838 completed (loss: 0.5015333890914917, acc: 0.8615384697914124)
[2025-02-16 12:17:41,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:41,230][root][INFO] - Training Epoch: 1/2, step 6962/23838 completed (loss: 0.41722074151039124, acc: 0.9041095972061157)
[2025-02-16 12:17:41,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:41,679][root][INFO] - Training Epoch: 1/2, step 6963/23838 completed (loss: 0.47765836119651794, acc: 0.8695651888847351)
[2025-02-16 12:17:41,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:42,092][root][INFO] - Training Epoch: 1/2, step 6964/23838 completed (loss: 1.1181612014770508, acc: 0.6428571343421936)
[2025-02-16 12:17:42,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:42,460][root][INFO] - Training Epoch: 1/2, step 6965/23838 completed (loss: 0.18902145326137543, acc: 0.9365079402923584)
[2025-02-16 12:17:42,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:42,855][root][INFO] - Training Epoch: 1/2, step 6966/23838 completed (loss: 0.9500331282615662, acc: 0.7288135886192322)
[2025-02-16 12:17:43,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:43,229][root][INFO] - Training Epoch: 1/2, step 6967/23838 completed (loss: 0.47643959522247314, acc: 0.8828828930854797)
[2025-02-16 12:17:43,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:43,671][root][INFO] - Training Epoch: 1/2, step 6968/23838 completed (loss: 0.2910706102848053, acc: 0.9090909361839294)
[2025-02-16 12:17:43,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:44,036][root][INFO] - Training Epoch: 1/2, step 6969/23838 completed (loss: 0.3672216534614563, acc: 0.8860759735107422)
[2025-02-16 12:17:44,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:44,461][root][INFO] - Training Epoch: 1/2, step 6970/23838 completed (loss: 0.16243994235992432, acc: 0.9605262875556946)
[2025-02-16 12:17:44,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:44,882][root][INFO] - Training Epoch: 1/2, step 6971/23838 completed (loss: 0.707142174243927, acc: 0.7938144207000732)
[2025-02-16 12:17:45,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:45,287][root][INFO] - Training Epoch: 1/2, step 6972/23838 completed (loss: 0.25316622853279114, acc: 0.9399999976158142)
[2025-02-16 12:17:45,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:45,724][root][INFO] - Training Epoch: 1/2, step 6973/23838 completed (loss: 0.12544073164463043, acc: 0.9695122241973877)
[2025-02-16 12:17:45,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:46,154][root][INFO] - Training Epoch: 1/2, step 6974/23838 completed (loss: 0.4627569615840912, acc: 0.8484848737716675)
[2025-02-16 12:17:46,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:46,600][root][INFO] - Training Epoch: 1/2, step 6975/23838 completed (loss: 0.1576576828956604, acc: 0.9691358208656311)
[2025-02-16 12:17:46,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:47,195][root][INFO] - Training Epoch: 1/2, step 6976/23838 completed (loss: 0.21095268428325653, acc: 0.9556962251663208)
[2025-02-16 12:17:47,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:47,940][root][INFO] - Training Epoch: 1/2, step 6977/23838 completed (loss: 0.16136913001537323, acc: 0.954081654548645)
[2025-02-16 12:17:48,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:48,399][root][INFO] - Training Epoch: 1/2, step 6978/23838 completed (loss: 0.4512513279914856, acc: 0.886956512928009)
[2025-02-16 12:17:48,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:48,823][root][INFO] - Training Epoch: 1/2, step 6979/23838 completed (loss: 0.16093821823596954, acc: 0.9470587968826294)
[2025-02-16 12:17:48,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:49,193][root][INFO] - Training Epoch: 1/2, step 6980/23838 completed (loss: 0.11556892842054367, acc: 0.9645389914512634)
[2025-02-16 12:17:49,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:49,601][root][INFO] - Training Epoch: 1/2, step 6981/23838 completed (loss: 0.4985921382904053, acc: 0.8552631735801697)
[2025-02-16 12:17:49,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:50,061][root][INFO] - Training Epoch: 1/2, step 6982/23838 completed (loss: 0.25068214535713196, acc: 0.925000011920929)
[2025-02-16 12:17:50,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:50,437][root][INFO] - Training Epoch: 1/2, step 6983/23838 completed (loss: 1.1062142848968506, acc: 0.6984127163887024)
[2025-02-16 12:17:50,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:50,851][root][INFO] - Training Epoch: 1/2, step 6984/23838 completed (loss: 0.18655304610729218, acc: 0.9487179517745972)
[2025-02-16 12:17:51,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:51,291][root][INFO] - Training Epoch: 1/2, step 6985/23838 completed (loss: 0.26296430826187134, acc: 0.9041916131973267)
[2025-02-16 12:17:51,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:51,775][root][INFO] - Training Epoch: 1/2, step 6986/23838 completed (loss: 0.2891557514667511, acc: 0.9433962106704712)
[2025-02-16 12:17:51,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:52,213][root][INFO] - Training Epoch: 1/2, step 6987/23838 completed (loss: 0.3238544762134552, acc: 0.9239130616188049)
[2025-02-16 12:17:52,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:52,638][root][INFO] - Training Epoch: 1/2, step 6988/23838 completed (loss: 0.39005550742149353, acc: 0.859375)
[2025-02-16 12:17:52,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:53,288][root][INFO] - Training Epoch: 1/2, step 6989/23838 completed (loss: 0.4745608866214752, acc: 0.8358209133148193)
[2025-02-16 12:17:53,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:53,729][root][INFO] - Training Epoch: 1/2, step 6990/23838 completed (loss: 0.48567962646484375, acc: 0.8295454382896423)
[2025-02-16 12:17:53,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:54,139][root][INFO] - Training Epoch: 1/2, step 6991/23838 completed (loss: 0.3526240587234497, acc: 0.8934426307678223)
[2025-02-16 12:17:54,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:54,634][root][INFO] - Training Epoch: 1/2, step 6992/23838 completed (loss: 0.36462077498435974, acc: 0.9118942618370056)
[2025-02-16 12:17:54,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:55,049][root][INFO] - Training Epoch: 1/2, step 6993/23838 completed (loss: 0.30057618021965027, acc: 0.9343065619468689)
[2025-02-16 12:17:55,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:55,461][root][INFO] - Training Epoch: 1/2, step 6994/23838 completed (loss: 0.23690903186798096, acc: 0.9266666769981384)
[2025-02-16 12:17:55,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:55,859][root][INFO] - Training Epoch: 1/2, step 6995/23838 completed (loss: 0.24973833560943604, acc: 0.9346405267715454)
[2025-02-16 12:17:56,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:56,291][root][INFO] - Training Epoch: 1/2, step 6996/23838 completed (loss: 0.31755897402763367, acc: 0.9076923131942749)
[2025-02-16 12:17:56,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:56,801][root][INFO] - Training Epoch: 1/2, step 6997/23838 completed (loss: 0.23878580331802368, acc: 0.9424083828926086)
[2025-02-16 12:17:56,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:57,187][root][INFO] - Training Epoch: 1/2, step 6998/23838 completed (loss: 0.27318963408470154, acc: 0.9389312863349915)
[2025-02-16 12:17:57,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:57,589][root][INFO] - Training Epoch: 1/2, step 6999/23838 completed (loss: 0.39089202880859375, acc: 0.9147287011146545)
[2025-02-16 12:17:57,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:58,063][root][INFO] - Training Epoch: 1/2, step 7000/23838 completed (loss: 0.15732240676879883, acc: 0.9618320465087891)
[2025-02-16 12:17:58,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:58,578][root][INFO] - Training Epoch: 1/2, step 7001/23838 completed (loss: 0.32471707463264465, acc: 0.8978102207183838)
[2025-02-16 12:17:58,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:59,051][root][INFO] - Training Epoch: 1/2, step 7002/23838 completed (loss: 0.37702152132987976, acc: 0.9166666865348816)
[2025-02-16 12:17:59,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:59,472][root][INFO] - Training Epoch: 1/2, step 7003/23838 completed (loss: 0.14638780057430267, acc: 0.96875)
[2025-02-16 12:17:59,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:17:59,855][root][INFO] - Training Epoch: 1/2, step 7004/23838 completed (loss: 0.15594084560871124, acc: 0.9520000219345093)
[2025-02-16 12:18:00,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:00,268][root][INFO] - Training Epoch: 1/2, step 7005/23838 completed (loss: 0.16264060139656067, acc: 0.9736841917037964)
[2025-02-16 12:18:00,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:00,763][root][INFO] - Training Epoch: 1/2, step 7006/23838 completed (loss: 0.07733696699142456, acc: 0.9922480583190918)
[2025-02-16 12:18:00,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:01,150][root][INFO] - Training Epoch: 1/2, step 7007/23838 completed (loss: 0.15528976917266846, acc: 0.9518072009086609)
[2025-02-16 12:18:01,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:01,614][root][INFO] - Training Epoch: 1/2, step 7008/23838 completed (loss: 0.45313215255737305, acc: 0.8571428656578064)
[2025-02-16 12:18:01,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:02,033][root][INFO] - Training Epoch: 1/2, step 7009/23838 completed (loss: 0.1811470240354538, acc: 0.9275362491607666)
[2025-02-16 12:18:02,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:02,500][root][INFO] - Training Epoch: 1/2, step 7010/23838 completed (loss: 0.6605073809623718, acc: 0.8111110925674438)
[2025-02-16 12:18:02,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:03,087][root][INFO] - Training Epoch: 1/2, step 7011/23838 completed (loss: 1.5254504680633545, acc: 0.5563380122184753)
[2025-02-16 12:18:03,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:03,544][root][INFO] - Training Epoch: 1/2, step 7012/23838 completed (loss: 1.2801094055175781, acc: 0.6111111044883728)
[2025-02-16 12:18:03,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:04,042][root][INFO] - Training Epoch: 1/2, step 7013/23838 completed (loss: 0.6847710013389587, acc: 0.7786259651184082)
[2025-02-16 12:18:04,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:04,481][root][INFO] - Training Epoch: 1/2, step 7014/23838 completed (loss: 0.859524130821228, acc: 0.7200000286102295)
[2025-02-16 12:18:04,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:05,005][root][INFO] - Training Epoch: 1/2, step 7015/23838 completed (loss: 1.385100245475769, acc: 0.5802469253540039)
[2025-02-16 12:18:05,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:05,474][root][INFO] - Training Epoch: 1/2, step 7016/23838 completed (loss: 0.7683127522468567, acc: 0.807692289352417)
[2025-02-16 12:18:05,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:05,941][root][INFO] - Training Epoch: 1/2, step 7017/23838 completed (loss: 0.8126440048217773, acc: 0.7790697813034058)
[2025-02-16 12:18:06,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:07,013][root][INFO] - Training Epoch: 1/2, step 7018/23838 completed (loss: 0.5798356533050537, acc: 0.837837815284729)
[2025-02-16 12:18:07,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:07,435][root][INFO] - Training Epoch: 1/2, step 7019/23838 completed (loss: 0.7353271842002869, acc: 0.760869562625885)
[2025-02-16 12:18:07,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:07,853][root][INFO] - Training Epoch: 1/2, step 7020/23838 completed (loss: 0.7589223980903625, acc: 0.75)
[2025-02-16 12:18:08,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:08,289][root][INFO] - Training Epoch: 1/2, step 7021/23838 completed (loss: 0.6631878018379211, acc: 0.7894737124443054)
[2025-02-16 12:18:08,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:09,018][root][INFO] - Training Epoch: 1/2, step 7022/23838 completed (loss: 0.6466988325119019, acc: 0.8348624110221863)
[2025-02-16 12:18:09,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:09,437][root][INFO] - Training Epoch: 1/2, step 7023/23838 completed (loss: 0.8377402424812317, acc: 0.7349397540092468)
[2025-02-16 12:18:09,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:09,964][root][INFO] - Training Epoch: 1/2, step 7024/23838 completed (loss: 2.234943151473999, acc: 0.4027777910232544)
[2025-02-16 12:18:10,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:10,452][root][INFO] - Training Epoch: 1/2, step 7025/23838 completed (loss: 0.906741201877594, acc: 0.7191011309623718)
[2025-02-16 12:18:10,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:11,102][root][INFO] - Training Epoch: 1/2, step 7026/23838 completed (loss: 0.7671151757240295, acc: 0.800000011920929)
[2025-02-16 12:18:11,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:11,620][root][INFO] - Training Epoch: 1/2, step 7027/23838 completed (loss: 0.914456844329834, acc: 0.7272727489471436)
[2025-02-16 12:18:11,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:12,016][root][INFO] - Training Epoch: 1/2, step 7028/23838 completed (loss: 0.8645438551902771, acc: 0.7530864477157593)
[2025-02-16 12:18:12,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:12,455][root][INFO] - Training Epoch: 1/2, step 7029/23838 completed (loss: 1.4067952632904053, acc: 0.6176470518112183)
[2025-02-16 12:18:12,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:12,978][root][INFO] - Training Epoch: 1/2, step 7030/23838 completed (loss: 1.5024532079696655, acc: 0.5833333134651184)
[2025-02-16 12:18:13,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:13,573][root][INFO] - Training Epoch: 1/2, step 7031/23838 completed (loss: 1.3600386381149292, acc: 0.6477272510528564)
[2025-02-16 12:18:13,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:14,204][root][INFO] - Training Epoch: 1/2, step 7032/23838 completed (loss: 0.9842711091041565, acc: 0.739130437374115)
[2025-02-16 12:18:14,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:14,637][root][INFO] - Training Epoch: 1/2, step 7033/23838 completed (loss: 0.9676229953765869, acc: 0.7196261882781982)
[2025-02-16 12:18:14,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:15,069][root][INFO] - Training Epoch: 1/2, step 7034/23838 completed (loss: 1.3089008331298828, acc: 0.625)
[2025-02-16 12:18:15,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:15,523][root][INFO] - Training Epoch: 1/2, step 7035/23838 completed (loss: 1.3045953512191772, acc: 0.6666666865348816)
[2025-02-16 12:18:15,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:15,909][root][INFO] - Training Epoch: 1/2, step 7036/23838 completed (loss: 1.145654559135437, acc: 0.7076923251152039)
[2025-02-16 12:18:16,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:16,373][root][INFO] - Training Epoch: 1/2, step 7037/23838 completed (loss: 1.184011459350586, acc: 0.6027397513389587)
[2025-02-16 12:18:16,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:16,925][root][INFO] - Training Epoch: 1/2, step 7038/23838 completed (loss: 0.6629825830459595, acc: 0.792682945728302)
[2025-02-16 12:18:17,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:17,403][root][INFO] - Training Epoch: 1/2, step 7039/23838 completed (loss: 1.083184003829956, acc: 0.692307710647583)
[2025-02-16 12:18:17,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:17,870][root][INFO] - Training Epoch: 1/2, step 7040/23838 completed (loss: 0.8249717950820923, acc: 0.7611940503120422)
[2025-02-16 12:18:18,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:18,419][root][INFO] - Training Epoch: 1/2, step 7041/23838 completed (loss: 1.3404366970062256, acc: 0.6136363744735718)
[2025-02-16 12:18:18,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:18,934][root][INFO] - Training Epoch: 1/2, step 7042/23838 completed (loss: 0.8511195182800293, acc: 0.7910447716712952)
[2025-02-16 12:18:19,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:19,392][root][INFO] - Training Epoch: 1/2, step 7043/23838 completed (loss: 1.093855619430542, acc: 0.6756756901741028)
[2025-02-16 12:18:19,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:19,871][root][INFO] - Training Epoch: 1/2, step 7044/23838 completed (loss: 0.29105937480926514, acc: 0.8974359035491943)
[2025-02-16 12:18:20,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:20,334][root][INFO] - Training Epoch: 1/2, step 7045/23838 completed (loss: 0.7577729225158691, acc: 0.7699999809265137)
[2025-02-16 12:18:20,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:20,933][root][INFO] - Training Epoch: 1/2, step 7046/23838 completed (loss: 0.7525776028633118, acc: 0.7613636255264282)
[2025-02-16 12:18:21,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:21,436][root][INFO] - Training Epoch: 1/2, step 7047/23838 completed (loss: 0.6966609954833984, acc: 0.7701149582862854)
[2025-02-16 12:18:21,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:21,990][root][INFO] - Training Epoch: 1/2, step 7048/23838 completed (loss: 1.2191674709320068, acc: 0.6349206566810608)
[2025-02-16 12:18:22,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:22,441][root][INFO] - Training Epoch: 1/2, step 7049/23838 completed (loss: 1.0872570276260376, acc: 0.6739130616188049)
[2025-02-16 12:18:22,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:22,959][root][INFO] - Training Epoch: 1/2, step 7050/23838 completed (loss: 0.6186984181404114, acc: 0.8333333134651184)
[2025-02-16 12:18:23,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:23,690][root][INFO] - Training Epoch: 1/2, step 7051/23838 completed (loss: 0.8926439881324768, acc: 0.800000011920929)
[2025-02-16 12:18:23,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:24,099][root][INFO] - Training Epoch: 1/2, step 7052/23838 completed (loss: 0.8065459728240967, acc: 0.7599999904632568)
[2025-02-16 12:18:24,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:24,522][root][INFO] - Training Epoch: 1/2, step 7053/23838 completed (loss: 0.5829537510871887, acc: 0.8292682766914368)
[2025-02-16 12:18:24,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:25,086][root][INFO] - Training Epoch: 1/2, step 7054/23838 completed (loss: 0.7404430508613586, acc: 0.7452830076217651)
[2025-02-16 12:18:25,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:25,726][root][INFO] - Training Epoch: 1/2, step 7055/23838 completed (loss: 1.0895987749099731, acc: 0.6947368383407593)
[2025-02-16 12:18:25,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:26,168][root][INFO] - Training Epoch: 1/2, step 7056/23838 completed (loss: 1.1134040355682373, acc: 0.7128713130950928)
[2025-02-16 12:18:26,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:26,661][root][INFO] - Training Epoch: 1/2, step 7057/23838 completed (loss: 0.9657472968101501, acc: 0.7241379022598267)
[2025-02-16 12:18:26,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:27,080][root][INFO] - Training Epoch: 1/2, step 7058/23838 completed (loss: 0.8136376738548279, acc: 0.746666669845581)
[2025-02-16 12:18:27,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:27,476][root][INFO] - Training Epoch: 1/2, step 7059/23838 completed (loss: 1.1262177228927612, acc: 0.7413793206214905)
[2025-02-16 12:18:27,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:28,261][root][INFO] - Training Epoch: 1/2, step 7060/23838 completed (loss: 1.4371246099472046, acc: 0.6232876777648926)
[2025-02-16 12:18:28,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:28,784][root][INFO] - Training Epoch: 1/2, step 7061/23838 completed (loss: 1.6941776275634766, acc: 0.5263158082962036)
[2025-02-16 12:18:29,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:29,280][root][INFO] - Training Epoch: 1/2, step 7062/23838 completed (loss: 1.4103960990905762, acc: 0.6395348906517029)
[2025-02-16 12:18:29,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:30,243][root][INFO] - Training Epoch: 1/2, step 7063/23838 completed (loss: 1.603993535041809, acc: 0.5698924660682678)
[2025-02-16 12:18:30,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:30,782][root][INFO] - Training Epoch: 1/2, step 7064/23838 completed (loss: 1.5449694395065308, acc: 0.5511810779571533)
[2025-02-16 12:18:30,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:31,162][root][INFO] - Training Epoch: 1/2, step 7065/23838 completed (loss: 1.5625945329666138, acc: 0.5517241358757019)
[2025-02-16 12:18:31,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:31,560][root][INFO] - Training Epoch: 1/2, step 7066/23838 completed (loss: 0.9843800067901611, acc: 0.6712328791618347)
[2025-02-16 12:18:31,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:31,993][root][INFO] - Training Epoch: 1/2, step 7067/23838 completed (loss: 1.3591663837432861, acc: 0.6034482717514038)
[2025-02-16 12:18:32,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:32,461][root][INFO] - Training Epoch: 1/2, step 7068/23838 completed (loss: 1.8306465148925781, acc: 0.5333333611488342)
[2025-02-16 12:18:32,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:32,886][root][INFO] - Training Epoch: 1/2, step 7069/23838 completed (loss: 0.8319529294967651, acc: 0.7397260069847107)
[2025-02-16 12:18:33,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:33,446][root][INFO] - Training Epoch: 1/2, step 7070/23838 completed (loss: 0.514375627040863, acc: 0.8518518805503845)
[2025-02-16 12:18:33,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:33,910][root][INFO] - Training Epoch: 1/2, step 7071/23838 completed (loss: 0.9966624975204468, acc: 0.8148148059844971)
[2025-02-16 12:18:34,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:34,528][root][INFO] - Training Epoch: 1/2, step 7072/23838 completed (loss: 0.26794108748435974, acc: 0.9174311757087708)
[2025-02-16 12:18:34,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:34,944][root][INFO] - Training Epoch: 1/2, step 7073/23838 completed (loss: 0.47309714555740356, acc: 0.8684210777282715)
[2025-02-16 12:18:35,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:35,372][root][INFO] - Training Epoch: 1/2, step 7074/23838 completed (loss: 0.6674248576164246, acc: 0.7719298005104065)
[2025-02-16 12:18:35,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:35,770][root][INFO] - Training Epoch: 1/2, step 7075/23838 completed (loss: 0.9596989154815674, acc: 0.746666669845581)
[2025-02-16 12:18:35,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:36,206][root][INFO] - Training Epoch: 1/2, step 7076/23838 completed (loss: 0.7218087911605835, acc: 0.7435897588729858)
[2025-02-16 12:18:36,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:36,637][root][INFO] - Training Epoch: 1/2, step 7077/23838 completed (loss: 1.325986623764038, acc: 0.6851851940155029)
[2025-02-16 12:18:36,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:37,086][root][INFO] - Training Epoch: 1/2, step 7078/23838 completed (loss: 0.5484703779220581, acc: 0.8518518805503845)
[2025-02-16 12:18:37,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:37,669][root][INFO] - Training Epoch: 1/2, step 7079/23838 completed (loss: 0.4421423375606537, acc: 0.8513513803482056)
[2025-02-16 12:18:37,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:38,197][root][INFO] - Training Epoch: 1/2, step 7080/23838 completed (loss: 0.6444035768508911, acc: 0.817307710647583)
[2025-02-16 12:18:38,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:38,570][root][INFO] - Training Epoch: 1/2, step 7081/23838 completed (loss: 0.19080276787281036, acc: 0.9506173133850098)
[2025-02-16 12:18:38,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:39,098][root][INFO] - Training Epoch: 1/2, step 7082/23838 completed (loss: 0.37913423776626587, acc: 0.8790322542190552)
[2025-02-16 12:18:39,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:39,639][root][INFO] - Training Epoch: 1/2, step 7083/23838 completed (loss: 0.5690155625343323, acc: 0.8504672646522522)
[2025-02-16 12:18:39,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:40,215][root][INFO] - Training Epoch: 1/2, step 7084/23838 completed (loss: 0.4044833779335022, acc: 0.8943662047386169)
[2025-02-16 12:18:40,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:40,769][root][INFO] - Training Epoch: 1/2, step 7085/23838 completed (loss: 0.8702383637428284, acc: 0.739130437374115)
[2025-02-16 12:18:41,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:41,251][root][INFO] - Training Epoch: 1/2, step 7086/23838 completed (loss: 0.8086621165275574, acc: 0.7383177280426025)
[2025-02-16 12:18:41,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:42,087][root][INFO] - Training Epoch: 1/2, step 7087/23838 completed (loss: 0.43093881011009216, acc: 0.8733333349227905)
[2025-02-16 12:18:42,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:42,665][root][INFO] - Training Epoch: 1/2, step 7088/23838 completed (loss: 0.570170521736145, acc: 0.8769230842590332)
[2025-02-16 12:18:43,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:43,518][root][INFO] - Training Epoch: 1/2, step 7089/23838 completed (loss: 0.9270836114883423, acc: 0.7425742745399475)
[2025-02-16 12:18:43,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:44,285][root][INFO] - Training Epoch: 1/2, step 7090/23838 completed (loss: 0.6195588111877441, acc: 0.7849462628364563)
[2025-02-16 12:18:44,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:44,854][root][INFO] - Training Epoch: 1/2, step 7091/23838 completed (loss: 0.886780858039856, acc: 0.7460317611694336)
[2025-02-16 12:18:45,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:45,742][root][INFO] - Training Epoch: 1/2, step 7092/23838 completed (loss: 0.8951698541641235, acc: 0.760869562625885)
[2025-02-16 12:18:46,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:46,673][root][INFO] - Training Epoch: 1/2, step 7093/23838 completed (loss: 0.6449920535087585, acc: 0.8275862336158752)
[2025-02-16 12:18:46,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:47,209][root][INFO] - Training Epoch: 1/2, step 7094/23838 completed (loss: 0.8900282382965088, acc: 0.7850467562675476)
[2025-02-16 12:18:47,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:47,790][root][INFO] - Training Epoch: 1/2, step 7095/23838 completed (loss: 0.8119657039642334, acc: 0.8067227005958557)
[2025-02-16 12:18:47,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:48,183][root][INFO] - Training Epoch: 1/2, step 7096/23838 completed (loss: 0.2789895236492157, acc: 0.942307710647583)
[2025-02-16 12:18:48,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:48,771][root][INFO] - Training Epoch: 1/2, step 7097/23838 completed (loss: 0.7131282091140747, acc: 0.7956989407539368)
[2025-02-16 12:18:48,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:49,234][root][INFO] - Training Epoch: 1/2, step 7098/23838 completed (loss: 0.5851272344589233, acc: 0.8399999737739563)
[2025-02-16 12:18:49,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:50,296][root][INFO] - Training Epoch: 1/2, step 7099/23838 completed (loss: 0.6999450922012329, acc: 0.7961165308952332)
[2025-02-16 12:18:50,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:51,277][root][INFO] - Training Epoch: 1/2, step 7100/23838 completed (loss: 0.7270790338516235, acc: 0.7763158082962036)
[2025-02-16 12:18:51,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:52,282][root][INFO] - Training Epoch: 1/2, step 7101/23838 completed (loss: 0.7382187247276306, acc: 0.7878788113594055)
[2025-02-16 12:18:52,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:52,969][root][INFO] - Training Epoch: 1/2, step 7102/23838 completed (loss: 1.1418341398239136, acc: 0.7126436829566956)
[2025-02-16 12:18:53,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:53,453][root][INFO] - Training Epoch: 1/2, step 7103/23838 completed (loss: 1.39974844455719, acc: 0.6000000238418579)
[2025-02-16 12:18:53,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:54,059][root][INFO] - Training Epoch: 1/2, step 7104/23838 completed (loss: 0.7619187831878662, acc: 0.7777777910232544)
[2025-02-16 12:18:54,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:54,999][root][INFO] - Training Epoch: 1/2, step 7105/23838 completed (loss: 1.078566312789917, acc: 0.7092198729515076)
[2025-02-16 12:18:55,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:55,980][root][INFO] - Training Epoch: 1/2, step 7106/23838 completed (loss: 0.9948468804359436, acc: 0.7200000286102295)
[2025-02-16 12:18:56,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:56,520][root][INFO] - Training Epoch: 1/2, step 7107/23838 completed (loss: 0.7385535836219788, acc: 0.7891566157341003)
[2025-02-16 12:18:56,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:56,919][root][INFO] - Training Epoch: 1/2, step 7108/23838 completed (loss: 0.911456823348999, acc: 0.7169811129570007)
[2025-02-16 12:18:57,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:57,334][root][INFO] - Training Epoch: 1/2, step 7109/23838 completed (loss: 0.3306475877761841, acc: 0.9111111164093018)
[2025-02-16 12:18:57,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:57,812][root][INFO] - Training Epoch: 1/2, step 7110/23838 completed (loss: 1.0118341445922852, acc: 0.7540983557701111)
[2025-02-16 12:18:58,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:58,417][root][INFO] - Training Epoch: 1/2, step 7111/23838 completed (loss: 0.19318126142024994, acc: 0.95652174949646)
[2025-02-16 12:18:58,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:58,878][root][INFO] - Training Epoch: 1/2, step 7112/23838 completed (loss: 0.9428471326828003, acc: 0.75)
[2025-02-16 12:18:59,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:59,295][root][INFO] - Training Epoch: 1/2, step 7113/23838 completed (loss: 0.3835943639278412, acc: 0.9189189076423645)
[2025-02-16 12:18:59,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:18:59,710][root][INFO] - Training Epoch: 1/2, step 7114/23838 completed (loss: 0.4271015226840973, acc: 0.8571428656578064)
[2025-02-16 12:18:59,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:00,189][root][INFO] - Training Epoch: 1/2, step 7115/23838 completed (loss: 0.5621506571769714, acc: 0.8666666746139526)
[2025-02-16 12:19:00,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:00,620][root][INFO] - Training Epoch: 1/2, step 7116/23838 completed (loss: 0.5246279835700989, acc: 0.8235294222831726)
[2025-02-16 12:19:01,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:01,387][root][INFO] - Training Epoch: 1/2, step 7117/23838 completed (loss: 0.6170517206192017, acc: 0.8205128312110901)
[2025-02-16 12:19:01,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:01,764][root][INFO] - Training Epoch: 1/2, step 7118/23838 completed (loss: 0.9916842579841614, acc: 0.761904776096344)
[2025-02-16 12:19:01,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:02,169][root][INFO] - Training Epoch: 1/2, step 7119/23838 completed (loss: 0.03586795926094055, acc: 1.0)
[2025-02-16 12:19:02,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:02,781][root][INFO] - Training Epoch: 1/2, step 7120/23838 completed (loss: 0.14070983231067657, acc: 0.9607843160629272)
[2025-02-16 12:19:03,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:03,696][root][INFO] - Training Epoch: 1/2, step 7121/23838 completed (loss: 0.4451523423194885, acc: 0.8421052694320679)
[2025-02-16 12:19:04,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:04,739][root][INFO] - Training Epoch: 1/2, step 7122/23838 completed (loss: 0.3914811611175537, acc: 0.8651685118675232)
[2025-02-16 12:19:05,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:05,267][root][INFO] - Training Epoch: 1/2, step 7123/23838 completed (loss: 0.3087645471096039, acc: 0.9350649118423462)
[2025-02-16 12:19:05,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:05,768][root][INFO] - Training Epoch: 1/2, step 7124/23838 completed (loss: 0.5341542959213257, acc: 0.8653846383094788)
[2025-02-16 12:19:06,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:06,254][root][INFO] - Training Epoch: 1/2, step 7125/23838 completed (loss: 0.4937596619129181, acc: 0.9166666865348816)
[2025-02-16 12:19:06,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:06,948][root][INFO] - Training Epoch: 1/2, step 7126/23838 completed (loss: 0.6530240774154663, acc: 0.8548387289047241)
[2025-02-16 12:19:07,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:07,456][root][INFO] - Training Epoch: 1/2, step 7127/23838 completed (loss: 0.1338111162185669, acc: 0.9589040875434875)
[2025-02-16 12:19:07,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:07,862][root][INFO] - Training Epoch: 1/2, step 7128/23838 completed (loss: 0.3724210560321808, acc: 0.9444444179534912)
[2025-02-16 12:19:08,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:08,299][root][INFO] - Training Epoch: 1/2, step 7129/23838 completed (loss: 0.11084854602813721, acc: 1.0)
[2025-02-16 12:19:08,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:08,856][root][INFO] - Training Epoch: 1/2, step 7130/23838 completed (loss: 0.17646703124046326, acc: 0.9538461565971375)
[2025-02-16 12:19:09,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:09,425][root][INFO] - Training Epoch: 1/2, step 7131/23838 completed (loss: 0.41460588574409485, acc: 0.875)
[2025-02-16 12:19:09,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:09,958][root][INFO] - Training Epoch: 1/2, step 7132/23838 completed (loss: 0.5180429220199585, acc: 0.8775510191917419)
[2025-02-16 12:19:10,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:10,393][root][INFO] - Training Epoch: 1/2, step 7133/23838 completed (loss: 0.2377852350473404, acc: 0.9166666865348816)
[2025-02-16 12:19:10,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:10,829][root][INFO] - Training Epoch: 1/2, step 7134/23838 completed (loss: 0.23261182010173798, acc: 0.9215686321258545)
[2025-02-16 12:19:11,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:11,485][root][INFO] - Training Epoch: 1/2, step 7135/23838 completed (loss: 0.6370876431465149, acc: 0.8636363744735718)
[2025-02-16 12:19:11,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:12,089][root][INFO] - Training Epoch: 1/2, step 7136/23838 completed (loss: 0.806700587272644, acc: 0.8064516186714172)
[2025-02-16 12:19:12,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:12,524][root][INFO] - Training Epoch: 1/2, step 7137/23838 completed (loss: 0.28049489855766296, acc: 0.9111111164093018)
[2025-02-16 12:19:12,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:13,272][root][INFO] - Training Epoch: 1/2, step 7138/23838 completed (loss: 0.2826780378818512, acc: 0.8684210777282715)
[2025-02-16 12:19:13,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:13,665][root][INFO] - Training Epoch: 1/2, step 7139/23838 completed (loss: 0.1378910392522812, acc: 0.9444444179534912)
[2025-02-16 12:19:13,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:14,091][root][INFO] - Training Epoch: 1/2, step 7140/23838 completed (loss: 0.4894240200519562, acc: 0.8703703880310059)
[2025-02-16 12:19:14,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:14,498][root][INFO] - Training Epoch: 1/2, step 7141/23838 completed (loss: 0.47986292839050293, acc: 0.8928571343421936)
[2025-02-16 12:19:14,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:14,966][root][INFO] - Training Epoch: 1/2, step 7142/23838 completed (loss: 0.6580153703689575, acc: 0.8478260636329651)
[2025-02-16 12:19:15,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:15,398][root][INFO] - Training Epoch: 1/2, step 7143/23838 completed (loss: 0.5671030282974243, acc: 0.8518518805503845)
[2025-02-16 12:19:15,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:15,842][root][INFO] - Training Epoch: 1/2, step 7144/23838 completed (loss: 1.855743646621704, acc: 0.48275861144065857)
[2025-02-16 12:19:16,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:16,359][root][INFO] - Training Epoch: 1/2, step 7145/23838 completed (loss: 0.1455937772989273, acc: 0.9666666388511658)
[2025-02-16 12:19:16,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:16,727][root][INFO] - Training Epoch: 1/2, step 7146/23838 completed (loss: 0.6556218862533569, acc: 0.7954545617103577)
[2025-02-16 12:19:16,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:17,266][root][INFO] - Training Epoch: 1/2, step 7147/23838 completed (loss: 0.38137733936309814, acc: 0.8999999761581421)
[2025-02-16 12:19:17,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:17,706][root][INFO] - Training Epoch: 1/2, step 7148/23838 completed (loss: 0.38211312890052795, acc: 0.8918918967247009)
[2025-02-16 12:19:17,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:18,149][root][INFO] - Training Epoch: 1/2, step 7149/23838 completed (loss: 0.40394872426986694, acc: 0.9047619104385376)
[2025-02-16 12:19:18,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:18,545][root][INFO] - Training Epoch: 1/2, step 7150/23838 completed (loss: 0.32903772592544556, acc: 0.8644067645072937)
[2025-02-16 12:19:18,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:18,929][root][INFO] - Training Epoch: 1/2, step 7151/23838 completed (loss: 0.49796923995018005, acc: 0.8333333134651184)
[2025-02-16 12:19:19,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:19,418][root][INFO] - Training Epoch: 1/2, step 7152/23838 completed (loss: 0.06211036443710327, acc: 0.9857142567634583)
[2025-02-16 12:19:19,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:19,867][root][INFO] - Training Epoch: 1/2, step 7153/23838 completed (loss: 0.10103466361761093, acc: 0.9642857313156128)
[2025-02-16 12:19:20,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:20,256][root][INFO] - Training Epoch: 1/2, step 7154/23838 completed (loss: 0.09906928986310959, acc: 0.9677419066429138)
[2025-02-16 12:19:20,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:20,667][root][INFO] - Training Epoch: 1/2, step 7155/23838 completed (loss: 0.2942064702510834, acc: 0.9285714030265808)
[2025-02-16 12:19:20,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:21,147][root][INFO] - Training Epoch: 1/2, step 7156/23838 completed (loss: 0.22119808197021484, acc: 0.9230769276618958)
[2025-02-16 12:19:21,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:21,558][root][INFO] - Training Epoch: 1/2, step 7157/23838 completed (loss: 0.6862936615943909, acc: 0.8292682766914368)
[2025-02-16 12:19:21,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:21,940][root][INFO] - Training Epoch: 1/2, step 7158/23838 completed (loss: 0.41495272517204285, acc: 0.8965517282485962)
[2025-02-16 12:19:22,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:22,363][root][INFO] - Training Epoch: 1/2, step 7159/23838 completed (loss: 0.18192343413829803, acc: 0.949999988079071)
[2025-02-16 12:19:22,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:22,773][root][INFO] - Training Epoch: 1/2, step 7160/23838 completed (loss: 0.3914932906627655, acc: 0.8799999952316284)
[2025-02-16 12:19:22,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:23,116][root][INFO] - Training Epoch: 1/2, step 7161/23838 completed (loss: 0.17423276603221893, acc: 0.9473684430122375)
[2025-02-16 12:19:23,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:23,547][root][INFO] - Training Epoch: 1/2, step 7162/23838 completed (loss: 0.4900493621826172, acc: 0.8684210777282715)
[2025-02-16 12:19:23,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:24,022][root][INFO] - Training Epoch: 1/2, step 7163/23838 completed (loss: 0.09886737912893295, acc: 0.9629629850387573)
[2025-02-16 12:19:24,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:24,416][root][INFO] - Training Epoch: 1/2, step 7164/23838 completed (loss: 0.4893907308578491, acc: 0.9230769276618958)
[2025-02-16 12:19:24,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:24,807][root][INFO] - Training Epoch: 1/2, step 7165/23838 completed (loss: 0.5363582968711853, acc: 0.9104477763175964)
[2025-02-16 12:19:24,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:25,200][root][INFO] - Training Epoch: 1/2, step 7166/23838 completed (loss: 0.025922218337655067, acc: 1.0)
[2025-02-16 12:19:25,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:25,612][root][INFO] - Training Epoch: 1/2, step 7167/23838 completed (loss: 0.1438005417585373, acc: 0.9375)
[2025-02-16 12:19:25,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:26,027][root][INFO] - Training Epoch: 1/2, step 7168/23838 completed (loss: 0.5670417547225952, acc: 0.9130434989929199)
[2025-02-16 12:19:26,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:26,466][root][INFO] - Training Epoch: 1/2, step 7169/23838 completed (loss: 0.14261765778064728, acc: 0.9354838728904724)
[2025-02-16 12:19:26,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:26,931][root][INFO] - Training Epoch: 1/2, step 7170/23838 completed (loss: 0.3171648681163788, acc: 0.9599999785423279)
[2025-02-16 12:19:27,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:27,351][root][INFO] - Training Epoch: 1/2, step 7171/23838 completed (loss: 0.25760671496391296, acc: 0.914893627166748)
[2025-02-16 12:19:27,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:27,754][root][INFO] - Training Epoch: 1/2, step 7172/23838 completed (loss: 0.03486510366201401, acc: 1.0)
[2025-02-16 12:19:27,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:28,192][root][INFO] - Training Epoch: 1/2, step 7173/23838 completed (loss: 0.39426857233047485, acc: 0.8095238208770752)
[2025-02-16 12:19:28,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:28,741][root][INFO] - Training Epoch: 1/2, step 7174/23838 completed (loss: 0.41453224420547485, acc: 0.9069767594337463)
[2025-02-16 12:19:28,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:29,259][root][INFO] - Training Epoch: 1/2, step 7175/23838 completed (loss: 0.1775955855846405, acc: 0.9487179517745972)
[2025-02-16 12:19:29,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:29,685][root][INFO] - Training Epoch: 1/2, step 7176/23838 completed (loss: 0.11898442357778549, acc: 0.9555555582046509)
[2025-02-16 12:19:29,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:30,127][root][INFO] - Training Epoch: 1/2, step 7177/23838 completed (loss: 0.3584319055080414, acc: 0.9047619104385376)
[2025-02-16 12:19:30,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:30,598][root][INFO] - Training Epoch: 1/2, step 7178/23838 completed (loss: 0.17525023221969604, acc: 0.9277108311653137)
[2025-02-16 12:19:30,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:31,025][root][INFO] - Training Epoch: 1/2, step 7179/23838 completed (loss: 0.5300204157829285, acc: 0.8333333134651184)
[2025-02-16 12:19:31,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:31,451][root][INFO] - Training Epoch: 1/2, step 7180/23838 completed (loss: 0.1640452891588211, acc: 0.9666666388511658)
[2025-02-16 12:19:31,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:31,898][root][INFO] - Training Epoch: 1/2, step 7181/23838 completed (loss: 0.4694843590259552, acc: 0.8928571343421936)
[2025-02-16 12:19:32,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:32,398][root][INFO] - Training Epoch: 1/2, step 7182/23838 completed (loss: 0.10647108405828476, acc: 0.9722222089767456)
[2025-02-16 12:19:32,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:32,829][root][INFO] - Training Epoch: 1/2, step 7183/23838 completed (loss: 0.06206401437520981, acc: 0.9636363387107849)
[2025-02-16 12:19:32,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:33,209][root][INFO] - Training Epoch: 1/2, step 7184/23838 completed (loss: 0.42031359672546387, acc: 0.8999999761581421)
[2025-02-16 12:19:33,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:33,652][root][INFO] - Training Epoch: 1/2, step 7185/23838 completed (loss: 0.0532706193625927, acc: 1.0)
[2025-02-16 12:19:33,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:34,041][root][INFO] - Training Epoch: 1/2, step 7186/23838 completed (loss: 1.0044878721237183, acc: 0.800000011920929)
[2025-02-16 12:19:34,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:34,450][root][INFO] - Training Epoch: 1/2, step 7187/23838 completed (loss: 0.4569050669670105, acc: 0.8620689511299133)
[2025-02-16 12:19:34,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:34,886][root][INFO] - Training Epoch: 1/2, step 7188/23838 completed (loss: 0.5061166882514954, acc: 0.8852459192276001)
[2025-02-16 12:19:35,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:35,315][root][INFO] - Training Epoch: 1/2, step 7189/23838 completed (loss: 0.6574158072471619, acc: 0.8181818127632141)
[2025-02-16 12:19:35,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:35,713][root][INFO] - Training Epoch: 1/2, step 7190/23838 completed (loss: 0.5885189175605774, acc: 0.8292682766914368)
[2025-02-16 12:19:35,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:36,147][root][INFO] - Training Epoch: 1/2, step 7191/23838 completed (loss: 1.0678123235702515, acc: 0.699999988079071)
[2025-02-16 12:19:36,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:36,568][root][INFO] - Training Epoch: 1/2, step 7192/23838 completed (loss: 0.43286028504371643, acc: 0.8181818127632141)
[2025-02-16 12:19:36,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:36,961][root][INFO] - Training Epoch: 1/2, step 7193/23838 completed (loss: 0.28151842951774597, acc: 0.9166666865348816)
[2025-02-16 12:19:37,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:37,432][root][INFO] - Training Epoch: 1/2, step 7194/23838 completed (loss: 0.1936240941286087, acc: 0.9534883499145508)
[2025-02-16 12:19:37,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:37,942][root][INFO] - Training Epoch: 1/2, step 7195/23838 completed (loss: 0.31566017866134644, acc: 0.8979591727256775)
[2025-02-16 12:19:38,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:38,451][root][INFO] - Training Epoch: 1/2, step 7196/23838 completed (loss: 0.05119486153125763, acc: 0.9887640476226807)
[2025-02-16 12:19:38,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:38,916][root][INFO] - Training Epoch: 1/2, step 7197/23838 completed (loss: 1.0536904335021973, acc: 0.75)
[2025-02-16 12:19:39,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:39,326][root][INFO] - Training Epoch: 1/2, step 7198/23838 completed (loss: 0.47501707077026367, acc: 0.8536585569381714)
[2025-02-16 12:19:39,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:39,746][root][INFO] - Training Epoch: 1/2, step 7199/23838 completed (loss: 0.22487255930900574, acc: 0.9024389982223511)
[2025-02-16 12:19:39,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:40,158][root][INFO] - Training Epoch: 1/2, step 7200/23838 completed (loss: 0.16958901286125183, acc: 0.9365079402923584)
[2025-02-16 12:19:40,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:40,633][root][INFO] - Training Epoch: 1/2, step 7201/23838 completed (loss: 0.12821774184703827, acc: 0.9420289993286133)
[2025-02-16 12:19:40,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:41,026][root][INFO] - Training Epoch: 1/2, step 7202/23838 completed (loss: 0.567943811416626, acc: 0.859649121761322)
[2025-02-16 12:19:41,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:41,465][root][INFO] - Training Epoch: 1/2, step 7203/23838 completed (loss: 0.18295328319072723, acc: 0.925000011920929)
[2025-02-16 12:19:41,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:41,950][root][INFO] - Training Epoch: 1/2, step 7204/23838 completed (loss: 0.6323502063751221, acc: 0.7971014380455017)
[2025-02-16 12:19:42,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:42,377][root][INFO] - Training Epoch: 1/2, step 7205/23838 completed (loss: 0.8565333485603333, acc: 0.7894737124443054)
[2025-02-16 12:19:42,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:42,848][root][INFO] - Training Epoch: 1/2, step 7206/23838 completed (loss: 0.6117043495178223, acc: 0.7924528121948242)
[2025-02-16 12:19:43,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:43,292][root][INFO] - Training Epoch: 1/2, step 7207/23838 completed (loss: 0.8789631724357605, acc: 0.7835051417350769)
[2025-02-16 12:19:43,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:43,689][root][INFO] - Training Epoch: 1/2, step 7208/23838 completed (loss: 0.9825623631477356, acc: 0.7230769395828247)
[2025-02-16 12:19:43,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:44,143][root][INFO] - Training Epoch: 1/2, step 7209/23838 completed (loss: 0.5923832058906555, acc: 0.8333333134651184)
[2025-02-16 12:19:44,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:44,639][root][INFO] - Training Epoch: 1/2, step 7210/23838 completed (loss: 0.481585294008255, acc: 0.8770949840545654)
[2025-02-16 12:19:44,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:45,135][root][INFO] - Training Epoch: 1/2, step 7211/23838 completed (loss: 0.46047309041023254, acc: 0.8961039185523987)
[2025-02-16 12:19:45,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:45,516][root][INFO] - Training Epoch: 1/2, step 7212/23838 completed (loss: 0.21519498527050018, acc: 0.9244186282157898)
[2025-02-16 12:19:45,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:45,928][root][INFO] - Training Epoch: 1/2, step 7213/23838 completed (loss: 0.7569863200187683, acc: 0.8196721076965332)
[2025-02-16 12:19:46,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:46,330][root][INFO] - Training Epoch: 1/2, step 7214/23838 completed (loss: 0.755088210105896, acc: 0.7910447716712952)
[2025-02-16 12:19:46,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:46,746][root][INFO] - Training Epoch: 1/2, step 7215/23838 completed (loss: 0.5942663550376892, acc: 0.8214285969734192)
[2025-02-16 12:19:46,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:47,152][root][INFO] - Training Epoch: 1/2, step 7216/23838 completed (loss: 0.43445512652397156, acc: 0.8720930218696594)
[2025-02-16 12:19:47,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:47,597][root][INFO] - Training Epoch: 1/2, step 7217/23838 completed (loss: 0.7878363728523254, acc: 0.7799999713897705)
[2025-02-16 12:19:47,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:48,023][root][INFO] - Training Epoch: 1/2, step 7218/23838 completed (loss: 0.8052067756652832, acc: 0.7881355881690979)
[2025-02-16 12:19:48,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:48,487][root][INFO] - Training Epoch: 1/2, step 7219/23838 completed (loss: 0.8887792825698853, acc: 0.7868852615356445)
[2025-02-16 12:19:48,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:48,917][root][INFO] - Training Epoch: 1/2, step 7220/23838 completed (loss: 0.41578352451324463, acc: 0.8730158805847168)
[2025-02-16 12:19:49,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:49,351][root][INFO] - Training Epoch: 1/2, step 7221/23838 completed (loss: 1.0958117246627808, acc: 0.6875)
[2025-02-16 12:19:49,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:49,784][root][INFO] - Training Epoch: 1/2, step 7222/23838 completed (loss: 0.9723368883132935, acc: 0.7662337422370911)
[2025-02-16 12:19:49,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:50,191][root][INFO] - Training Epoch: 1/2, step 7223/23838 completed (loss: 0.8396860957145691, acc: 0.7752808928489685)
[2025-02-16 12:19:50,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:50,578][root][INFO] - Training Epoch: 1/2, step 7224/23838 completed (loss: 0.6797551512718201, acc: 0.8620689511299133)
[2025-02-16 12:19:50,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:51,019][root][INFO] - Training Epoch: 1/2, step 7225/23838 completed (loss: 0.6073746681213379, acc: 0.8229166865348816)
[2025-02-16 12:19:51,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:51,460][root][INFO] - Training Epoch: 1/2, step 7226/23838 completed (loss: 0.7152562141418457, acc: 0.8125)
[2025-02-16 12:19:51,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:51,876][root][INFO] - Training Epoch: 1/2, step 7227/23838 completed (loss: 0.4479902982711792, acc: 0.8453608155250549)
[2025-02-16 12:19:52,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:52,266][root][INFO] - Training Epoch: 1/2, step 7228/23838 completed (loss: 0.26775187253952026, acc: 0.9196428656578064)
[2025-02-16 12:19:52,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:52,721][root][INFO] - Training Epoch: 1/2, step 7229/23838 completed (loss: 0.524385929107666, acc: 0.8641975522041321)
[2025-02-16 12:19:52,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:53,134][root][INFO] - Training Epoch: 1/2, step 7230/23838 completed (loss: 1.0206917524337769, acc: 0.7384615540504456)
[2025-02-16 12:19:53,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:53,840][root][INFO] - Training Epoch: 1/2, step 7231/23838 completed (loss: 0.8715561032295227, acc: 0.7580645084381104)
[2025-02-16 12:19:54,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:54,260][root][INFO] - Training Epoch: 1/2, step 7232/23838 completed (loss: 0.9127347469329834, acc: 0.7692307829856873)
[2025-02-16 12:19:54,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:54,674][root][INFO] - Training Epoch: 1/2, step 7233/23838 completed (loss: 0.8876287937164307, acc: 0.7849462628364563)
[2025-02-16 12:19:54,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:55,107][root][INFO] - Training Epoch: 1/2, step 7234/23838 completed (loss: 0.983143150806427, acc: 0.761904776096344)
[2025-02-16 12:19:55,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:55,532][root][INFO] - Training Epoch: 1/2, step 7235/23838 completed (loss: 1.0006649494171143, acc: 0.7019230723381042)
[2025-02-16 12:19:55,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:55,966][root][INFO] - Training Epoch: 1/2, step 7236/23838 completed (loss: 0.4966171383857727, acc: 0.8809523582458496)
[2025-02-16 12:19:56,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:56,430][root][INFO] - Training Epoch: 1/2, step 7237/23838 completed (loss: 1.3498363494873047, acc: 0.6888889074325562)
[2025-02-16 12:19:56,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:56,809][root][INFO] - Training Epoch: 1/2, step 7238/23838 completed (loss: 0.5708569884300232, acc: 0.8629032373428345)
[2025-02-16 12:19:57,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:57,245][root][INFO] - Training Epoch: 1/2, step 7239/23838 completed (loss: 0.6535877585411072, acc: 0.8358209133148193)
[2025-02-16 12:19:57,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:57,659][root][INFO] - Training Epoch: 1/2, step 7240/23838 completed (loss: 1.6267565488815308, acc: 0.5813953280448914)
[2025-02-16 12:19:57,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:58,120][root][INFO] - Training Epoch: 1/2, step 7241/23838 completed (loss: 0.5333982110023499, acc: 0.8627451062202454)
[2025-02-16 12:19:58,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:58,611][root][INFO] - Training Epoch: 1/2, step 7242/23838 completed (loss: 0.9114879965782166, acc: 0.7358490824699402)
[2025-02-16 12:19:58,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:59,046][root][INFO] - Training Epoch: 1/2, step 7243/23838 completed (loss: 0.5599241256713867, acc: 0.800000011920929)
[2025-02-16 12:19:59,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:59,547][root][INFO] - Training Epoch: 1/2, step 7244/23838 completed (loss: 0.5073597431182861, acc: 0.8640000224113464)
[2025-02-16 12:19:59,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:19:59,930][root][INFO] - Training Epoch: 1/2, step 7245/23838 completed (loss: 0.23636029660701752, acc: 0.9210526347160339)
[2025-02-16 12:20:00,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:00,354][root][INFO] - Training Epoch: 1/2, step 7246/23838 completed (loss: 0.40467381477355957, acc: 0.8933333158493042)
[2025-02-16 12:20:00,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:00,746][root][INFO] - Training Epoch: 1/2, step 7247/23838 completed (loss: 0.46488386392593384, acc: 0.8793103694915771)
[2025-02-16 12:20:00,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:01,151][root][INFO] - Training Epoch: 1/2, step 7248/23838 completed (loss: 0.6497986912727356, acc: 0.8165137767791748)
[2025-02-16 12:20:01,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:01,605][root][INFO] - Training Epoch: 1/2, step 7249/23838 completed (loss: 0.5640434622764587, acc: 0.848739504814148)
[2025-02-16 12:20:01,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:02,024][root][INFO] - Training Epoch: 1/2, step 7250/23838 completed (loss: 0.9781494140625, acc: 0.682539701461792)
[2025-02-16 12:20:02,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:02,430][root][INFO] - Training Epoch: 1/2, step 7251/23838 completed (loss: 0.4298926591873169, acc: 0.8767123222351074)
[2025-02-16 12:20:02,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:02,871][root][INFO] - Training Epoch: 1/2, step 7252/23838 completed (loss: 0.635768711566925, acc: 0.8561643958091736)
[2025-02-16 12:20:03,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:03,266][root][INFO] - Training Epoch: 1/2, step 7253/23838 completed (loss: 0.8721491694450378, acc: 0.7428571581840515)
[2025-02-16 12:20:03,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:03,712][root][INFO] - Training Epoch: 1/2, step 7254/23838 completed (loss: 0.5258105993270874, acc: 0.8421052694320679)
[2025-02-16 12:20:03,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:04,146][root][INFO] - Training Epoch: 1/2, step 7255/23838 completed (loss: 0.34158748388290405, acc: 0.9333333373069763)
[2025-02-16 12:20:04,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:04,577][root][INFO] - Training Epoch: 1/2, step 7256/23838 completed (loss: 0.6838932633399963, acc: 0.800000011920929)
[2025-02-16 12:20:04,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:04,982][root][INFO] - Training Epoch: 1/2, step 7257/23838 completed (loss: 0.8547835350036621, acc: 0.7866666913032532)
[2025-02-16 12:20:05,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:05,440][root][INFO] - Training Epoch: 1/2, step 7258/23838 completed (loss: 0.5681339502334595, acc: 0.8387096524238586)
[2025-02-16 12:20:05,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:05,935][root][INFO] - Training Epoch: 1/2, step 7259/23838 completed (loss: 0.449180543422699, acc: 0.9117646813392639)
[2025-02-16 12:20:06,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:06,365][root][INFO] - Training Epoch: 1/2, step 7260/23838 completed (loss: 0.501850426197052, acc: 0.8958333134651184)
[2025-02-16 12:20:06,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:06,839][root][INFO] - Training Epoch: 1/2, step 7261/23838 completed (loss: 0.43134787678718567, acc: 0.892307698726654)
[2025-02-16 12:20:07,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:07,265][root][INFO] - Training Epoch: 1/2, step 7262/23838 completed (loss: 0.3048335611820221, acc: 0.9090909361839294)
[2025-02-16 12:20:07,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:07,684][root][INFO] - Training Epoch: 1/2, step 7263/23838 completed (loss: 0.831773579120636, acc: 0.7297297120094299)
[2025-02-16 12:20:07,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:08,078][root][INFO] - Training Epoch: 1/2, step 7264/23838 completed (loss: 0.8349611759185791, acc: 0.8199999928474426)
[2025-02-16 12:20:08,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:08,524][root][INFO] - Training Epoch: 1/2, step 7265/23838 completed (loss: 1.0817642211914062, acc: 0.7605633735656738)
[2025-02-16 12:20:08,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:08,937][root][INFO] - Training Epoch: 1/2, step 7266/23838 completed (loss: 0.8418865203857422, acc: 0.7536231875419617)
[2025-02-16 12:20:09,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:09,363][root][INFO] - Training Epoch: 1/2, step 7267/23838 completed (loss: 1.3777576684951782, acc: 0.6774193644523621)
[2025-02-16 12:20:09,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:09,834][root][INFO] - Training Epoch: 1/2, step 7268/23838 completed (loss: 1.2525947093963623, acc: 0.6499999761581421)
[2025-02-16 12:20:10,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:10,273][root][INFO] - Training Epoch: 1/2, step 7269/23838 completed (loss: 0.868074893951416, acc: 0.7200000286102295)
[2025-02-16 12:20:10,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:10,717][root][INFO] - Training Epoch: 1/2, step 7270/23838 completed (loss: 1.5133651494979858, acc: 0.5945945978164673)
[2025-02-16 12:20:10,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:11,153][root][INFO] - Training Epoch: 1/2, step 7271/23838 completed (loss: 1.278454065322876, acc: 0.6857143044471741)
[2025-02-16 12:20:11,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:11,544][root][INFO] - Training Epoch: 1/2, step 7272/23838 completed (loss: 1.3221063613891602, acc: 0.6315789222717285)
[2025-02-16 12:20:11,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:11,976][root][INFO] - Training Epoch: 1/2, step 7273/23838 completed (loss: 1.3764336109161377, acc: 0.6000000238418579)
[2025-02-16 12:20:12,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:12,395][root][INFO] - Training Epoch: 1/2, step 7274/23838 completed (loss: 1.684454083442688, acc: 0.4324324429035187)
[2025-02-16 12:20:12,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:12,805][root][INFO] - Training Epoch: 1/2, step 7275/23838 completed (loss: 1.0689575672149658, acc: 0.7111111283302307)
[2025-02-16 12:20:12,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:13,235][root][INFO] - Training Epoch: 1/2, step 7276/23838 completed (loss: 1.935703158378601, acc: 0.5681818127632141)
[2025-02-16 12:20:13,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:13,845][root][INFO] - Training Epoch: 1/2, step 7277/23838 completed (loss: 1.6172674894332886, acc: 0.6551724076271057)
[2025-02-16 12:20:14,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:14,274][root][INFO] - Training Epoch: 1/2, step 7278/23838 completed (loss: 1.759889006614685, acc: 0.5)
[2025-02-16 12:20:14,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:14,719][root][INFO] - Training Epoch: 1/2, step 7279/23838 completed (loss: 1.7309046983718872, acc: 0.6078431606292725)
[2025-02-16 12:20:14,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:15,136][root][INFO] - Training Epoch: 1/2, step 7280/23838 completed (loss: 1.1572941541671753, acc: 0.5609756112098694)
[2025-02-16 12:20:15,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:15,498][root][INFO] - Training Epoch: 1/2, step 7281/23838 completed (loss: 1.9976716041564941, acc: 0.523809552192688)
[2025-02-16 12:20:15,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:16,090][root][INFO] - Training Epoch: 1/2, step 7282/23838 completed (loss: 2.227522134780884, acc: 0.3461538553237915)
[2025-02-16 12:20:16,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:16,471][root][INFO] - Training Epoch: 1/2, step 7283/23838 completed (loss: 1.6915074586868286, acc: 0.52173912525177)
[2025-02-16 12:20:16,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:16,855][root][INFO] - Training Epoch: 1/2, step 7284/23838 completed (loss: 0.5407206416130066, acc: 0.8148148059844971)
[2025-02-16 12:20:16,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:17,230][root][INFO] - Training Epoch: 1/2, step 7285/23838 completed (loss: 1.413563847541809, acc: 0.6000000238418579)
[2025-02-16 12:20:17,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:17,683][root][INFO] - Training Epoch: 1/2, step 7286/23838 completed (loss: 1.3931695222854614, acc: 0.5952380895614624)
[2025-02-16 12:20:17,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:18,109][root][INFO] - Training Epoch: 1/2, step 7287/23838 completed (loss: 1.5955928564071655, acc: 0.5555555820465088)
[2025-02-16 12:20:18,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:18,604][root][INFO] - Training Epoch: 1/2, step 7288/23838 completed (loss: 0.5080941915512085, acc: 0.875)
[2025-02-16 12:20:18,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:19,213][root][INFO] - Training Epoch: 1/2, step 7289/23838 completed (loss: 1.2055058479309082, acc: 0.5555555820465088)
[2025-02-16 12:20:19,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:19,625][root][INFO] - Training Epoch: 1/2, step 7290/23838 completed (loss: 2.2270259857177734, acc: 0.40625)
[2025-02-16 12:20:19,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:20,041][root][INFO] - Training Epoch: 1/2, step 7291/23838 completed (loss: 0.8862397074699402, acc: 0.7124999761581421)
[2025-02-16 12:20:20,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:20,469][root][INFO] - Training Epoch: 1/2, step 7292/23838 completed (loss: 0.6861191391944885, acc: 0.7457627058029175)
[2025-02-16 12:20:20,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:20,902][root][INFO] - Training Epoch: 1/2, step 7293/23838 completed (loss: 0.6789077520370483, acc: 0.8055555820465088)
[2025-02-16 12:20:21,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:21,314][root][INFO] - Training Epoch: 1/2, step 7294/23838 completed (loss: 1.1099655628204346, acc: 0.7200000286102295)
[2025-02-16 12:20:21,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:21,750][root][INFO] - Training Epoch: 1/2, step 7295/23838 completed (loss: 0.8377344012260437, acc: 0.7456140518188477)
[2025-02-16 12:20:21,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:22,131][root][INFO] - Training Epoch: 1/2, step 7296/23838 completed (loss: 0.7168190479278564, acc: 0.7980769276618958)
[2025-02-16 12:20:22,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:22,577][root][INFO] - Training Epoch: 1/2, step 7297/23838 completed (loss: 0.5050843954086304, acc: 0.835616409778595)
[2025-02-16 12:20:22,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:22,954][root][INFO] - Training Epoch: 1/2, step 7298/23838 completed (loss: 0.8196487426757812, acc: 0.7848101258277893)
[2025-02-16 12:20:23,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:23,328][root][INFO] - Training Epoch: 1/2, step 7299/23838 completed (loss: 0.5717397928237915, acc: 0.8191489577293396)
[2025-02-16 12:20:23,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:23,749][root][INFO] - Training Epoch: 1/2, step 7300/23838 completed (loss: 0.510875940322876, acc: 0.8770492076873779)
[2025-02-16 12:20:23,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:24,115][root][INFO] - Training Epoch: 1/2, step 7301/23838 completed (loss: 0.21209490299224854, acc: 0.9666666388511658)
[2025-02-16 12:20:24,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:24,515][root][INFO] - Training Epoch: 1/2, step 7302/23838 completed (loss: 0.3916821777820587, acc: 0.8867924809455872)
[2025-02-16 12:20:24,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:24,950][root][INFO] - Training Epoch: 1/2, step 7303/23838 completed (loss: 0.8517983555793762, acc: 0.7822580933570862)
[2025-02-16 12:20:25,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:25,392][root][INFO] - Training Epoch: 1/2, step 7304/23838 completed (loss: 0.4519769549369812, acc: 0.8860759735107422)
[2025-02-16 12:20:25,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:25,807][root][INFO] - Training Epoch: 1/2, step 7305/23838 completed (loss: 0.4062671959400177, acc: 0.8901098966598511)
[2025-02-16 12:20:25,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:26,173][root][INFO] - Training Epoch: 1/2, step 7306/23838 completed (loss: 0.29275259375572205, acc: 0.9104477763175964)
[2025-02-16 12:20:26,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:26,595][root][INFO] - Training Epoch: 1/2, step 7307/23838 completed (loss: 0.3279171586036682, acc: 0.8974359035491943)
[2025-02-16 12:20:26,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:26,996][root][INFO] - Training Epoch: 1/2, step 7308/23838 completed (loss: 0.15641890466213226, acc: 0.9652174115180969)
[2025-02-16 12:20:27,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:27,390][root][INFO] - Training Epoch: 1/2, step 7309/23838 completed (loss: 0.6079573035240173, acc: 0.8543046116828918)
[2025-02-16 12:20:27,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:27,763][root][INFO] - Training Epoch: 1/2, step 7310/23838 completed (loss: 0.39300626516342163, acc: 0.8960000276565552)
[2025-02-16 12:20:27,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:28,191][root][INFO] - Training Epoch: 1/2, step 7311/23838 completed (loss: 0.36609435081481934, acc: 0.8958333134651184)
[2025-02-16 12:20:28,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:28,654][root][INFO] - Training Epoch: 1/2, step 7312/23838 completed (loss: 0.37209293246269226, acc: 0.9191918969154358)
[2025-02-16 12:20:28,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:29,196][root][INFO] - Training Epoch: 1/2, step 7313/23838 completed (loss: 0.4464353322982788, acc: 0.8818181753158569)
[2025-02-16 12:20:29,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:29,648][root][INFO] - Training Epoch: 1/2, step 7314/23838 completed (loss: 0.4338511824607849, acc: 0.8766233921051025)
[2025-02-16 12:20:29,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:30,048][root][INFO] - Training Epoch: 1/2, step 7315/23838 completed (loss: 0.18859732151031494, acc: 0.9347826242446899)
[2025-02-16 12:20:30,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:30,500][root][INFO] - Training Epoch: 1/2, step 7316/23838 completed (loss: 0.438353955745697, acc: 0.9080459475517273)
[2025-02-16 12:20:30,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:30,963][root][INFO] - Training Epoch: 1/2, step 7317/23838 completed (loss: 0.5166263580322266, acc: 0.8684210777282715)
[2025-02-16 12:20:31,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:31,358][root][INFO] - Training Epoch: 1/2, step 7318/23838 completed (loss: 0.30690157413482666, acc: 0.930232584476471)
[2025-02-16 12:20:31,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:31,757][root][INFO] - Training Epoch: 1/2, step 7319/23838 completed (loss: 0.445292592048645, acc: 0.8823529481887817)
[2025-02-16 12:20:31,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:32,207][root][INFO] - Training Epoch: 1/2, step 7320/23838 completed (loss: 0.3015517294406891, acc: 0.9030612111091614)
[2025-02-16 12:20:32,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:32,632][root][INFO] - Training Epoch: 1/2, step 7321/23838 completed (loss: 0.20424093306064606, acc: 0.9518072009086609)
[2025-02-16 12:20:32,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:33,072][root][INFO] - Training Epoch: 1/2, step 7322/23838 completed (loss: 0.5362204909324646, acc: 0.8195488452911377)
[2025-02-16 12:20:33,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:33,445][root][INFO] - Training Epoch: 1/2, step 7323/23838 completed (loss: 0.5175173282623291, acc: 0.8608695864677429)
[2025-02-16 12:20:33,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:33,906][root][INFO] - Training Epoch: 1/2, step 7324/23838 completed (loss: 0.7830244898796082, acc: 0.7647058963775635)
[2025-02-16 12:20:34,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:34,308][root][INFO] - Training Epoch: 1/2, step 7325/23838 completed (loss: 1.070061445236206, acc: 0.7413793206214905)
[2025-02-16 12:20:34,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:34,805][root][INFO] - Training Epoch: 1/2, step 7326/23838 completed (loss: 0.802756130695343, acc: 0.8307692408561707)
[2025-02-16 12:20:35,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:35,239][root][INFO] - Training Epoch: 1/2, step 7327/23838 completed (loss: 0.7118311524391174, acc: 0.8358209133148193)
[2025-02-16 12:20:35,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:35,674][root][INFO] - Training Epoch: 1/2, step 7328/23838 completed (loss: 0.4894278347492218, acc: 0.8738738894462585)
[2025-02-16 12:20:35,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:36,094][root][INFO] - Training Epoch: 1/2, step 7329/23838 completed (loss: 0.43002885580062866, acc: 0.8931297659873962)
[2025-02-16 12:20:36,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:36,532][root][INFO] - Training Epoch: 1/2, step 7330/23838 completed (loss: 0.2113935798406601, acc: 0.9441624283790588)
[2025-02-16 12:20:36,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:36,940][root][INFO] - Training Epoch: 1/2, step 7331/23838 completed (loss: 0.33736082911491394, acc: 0.9052631855010986)
[2025-02-16 12:20:37,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:37,389][root][INFO] - Training Epoch: 1/2, step 7332/23838 completed (loss: 0.375287801027298, acc: 0.9479166865348816)
[2025-02-16 12:20:37,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:37,777][root][INFO] - Training Epoch: 1/2, step 7333/23838 completed (loss: 0.1909891813993454, acc: 0.9449541568756104)
[2025-02-16 12:20:37,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:38,233][root][INFO] - Training Epoch: 1/2, step 7334/23838 completed (loss: 0.43146809935569763, acc: 0.8947368264198303)
[2025-02-16 12:20:38,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:38,655][root][INFO] - Training Epoch: 1/2, step 7335/23838 completed (loss: 0.48864442110061646, acc: 0.8601398468017578)
[2025-02-16 12:20:38,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:39,170][root][INFO] - Training Epoch: 1/2, step 7336/23838 completed (loss: 0.5186197757720947, acc: 0.8523489832878113)
[2025-02-16 12:20:39,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:39,612][root][INFO] - Training Epoch: 1/2, step 7337/23838 completed (loss: 0.3662557899951935, acc: 0.8813559412956238)
[2025-02-16 12:20:39,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:40,050][root][INFO] - Training Epoch: 1/2, step 7338/23838 completed (loss: 0.38478583097457886, acc: 0.8818897604942322)
[2025-02-16 12:20:40,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:40,475][root][INFO] - Training Epoch: 1/2, step 7339/23838 completed (loss: 0.28975874185562134, acc: 0.9236640930175781)
[2025-02-16 12:20:40,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:40,856][root][INFO] - Training Epoch: 1/2, step 7340/23838 completed (loss: 0.6181911826133728, acc: 0.8333333134651184)
[2025-02-16 12:20:41,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:41,224][root][INFO] - Training Epoch: 1/2, step 7341/23838 completed (loss: 0.3371669352054596, acc: 0.898876428604126)
[2025-02-16 12:20:41,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:41,709][root][INFO] - Training Epoch: 1/2, step 7342/23838 completed (loss: 0.2522061765193939, acc: 0.936274528503418)
[2025-02-16 12:20:41,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:42,145][root][INFO] - Training Epoch: 1/2, step 7343/23838 completed (loss: 0.18222065269947052, acc: 0.9487179517745972)
[2025-02-16 12:20:42,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:42,619][root][INFO] - Training Epoch: 1/2, step 7344/23838 completed (loss: 0.31088390946388245, acc: 0.9395973086357117)
[2025-02-16 12:20:42,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:43,089][root][INFO] - Training Epoch: 1/2, step 7345/23838 completed (loss: 0.5068882703781128, acc: 0.8931297659873962)
[2025-02-16 12:20:43,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:43,541][root][INFO] - Training Epoch: 1/2, step 7346/23838 completed (loss: 0.1784239262342453, acc: 0.9459459185600281)
[2025-02-16 12:20:43,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:43,970][root][INFO] - Training Epoch: 1/2, step 7347/23838 completed (loss: 0.34991180896759033, acc: 0.8865247964859009)
[2025-02-16 12:20:44,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:44,437][root][INFO] - Training Epoch: 1/2, step 7348/23838 completed (loss: 0.3502703011035919, acc: 0.8648648858070374)
[2025-02-16 12:20:44,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:44,887][root][INFO] - Training Epoch: 1/2, step 7349/23838 completed (loss: 0.4063425362110138, acc: 0.8730158805847168)
[2025-02-16 12:20:45,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:45,448][root][INFO] - Training Epoch: 1/2, step 7350/23838 completed (loss: 0.37352240085601807, acc: 0.9019607901573181)
[2025-02-16 12:20:45,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:45,864][root][INFO] - Training Epoch: 1/2, step 7351/23838 completed (loss: 0.43509262800216675, acc: 0.8854625821113586)
[2025-02-16 12:20:46,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:46,284][root][INFO] - Training Epoch: 1/2, step 7352/23838 completed (loss: 0.7401613593101501, acc: 0.8108108043670654)
[2025-02-16 12:20:46,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:46,803][root][INFO] - Training Epoch: 1/2, step 7353/23838 completed (loss: 0.46997323632240295, acc: 0.8796992301940918)
[2025-02-16 12:20:47,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:47,279][root][INFO] - Training Epoch: 1/2, step 7354/23838 completed (loss: 0.44832971692085266, acc: 0.8589743375778198)
[2025-02-16 12:20:47,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:47,716][root][INFO] - Training Epoch: 1/2, step 7355/23838 completed (loss: 0.21172919869422913, acc: 0.9607843160629272)
[2025-02-16 12:20:47,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:48,194][root][INFO] - Training Epoch: 1/2, step 7356/23838 completed (loss: 0.8084355592727661, acc: 0.7969924807548523)
[2025-02-16 12:20:48,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:48,635][root][INFO] - Training Epoch: 1/2, step 7357/23838 completed (loss: 0.25087302923202515, acc: 0.9117646813392639)
[2025-02-16 12:20:48,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:49,030][root][INFO] - Training Epoch: 1/2, step 7358/23838 completed (loss: 0.4511054456233978, acc: 0.887499988079071)
[2025-02-16 12:20:49,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:49,442][root][INFO] - Training Epoch: 1/2, step 7359/23838 completed (loss: 0.3555966019630432, acc: 0.9160839319229126)
[2025-02-16 12:20:49,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:49,822][root][INFO] - Training Epoch: 1/2, step 7360/23838 completed (loss: 0.37131017446517944, acc: 0.9047619104385376)
[2025-02-16 12:20:50,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:50,250][root][INFO] - Training Epoch: 1/2, step 7361/23838 completed (loss: 0.8387836813926697, acc: 0.75)
[2025-02-16 12:20:50,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:50,681][root][INFO] - Training Epoch: 1/2, step 7362/23838 completed (loss: 0.5158193111419678, acc: 0.8642857074737549)
[2025-02-16 12:20:50,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:51,063][root][INFO] - Training Epoch: 1/2, step 7363/23838 completed (loss: 0.23023441433906555, acc: 0.9159663915634155)
[2025-02-16 12:20:51,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:51,522][root][INFO] - Training Epoch: 1/2, step 7364/23838 completed (loss: 0.5856254696846008, acc: 0.8579235076904297)
[2025-02-16 12:20:51,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:52,239][root][INFO] - Training Epoch: 1/2, step 7365/23838 completed (loss: 0.45125967264175415, acc: 0.8727272748947144)
[2025-02-16 12:20:52,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:52,689][root][INFO] - Training Epoch: 1/2, step 7366/23838 completed (loss: 0.662124752998352, acc: 0.7945205569267273)
[2025-02-16 12:20:52,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:53,134][root][INFO] - Training Epoch: 1/2, step 7367/23838 completed (loss: 0.578548014163971, acc: 0.8558558821678162)
[2025-02-16 12:20:53,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:53,588][root][INFO] - Training Epoch: 1/2, step 7368/23838 completed (loss: 0.4635465145111084, acc: 0.804347813129425)
[2025-02-16 12:20:53,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:54,024][root][INFO] - Training Epoch: 1/2, step 7369/23838 completed (loss: 0.3694802522659302, acc: 0.9186046719551086)
[2025-02-16 12:20:54,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:54,489][root][INFO] - Training Epoch: 1/2, step 7370/23838 completed (loss: 0.45743894577026367, acc: 0.8190476298332214)
[2025-02-16 12:20:54,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:54,937][root][INFO] - Training Epoch: 1/2, step 7371/23838 completed (loss: 0.3369334638118744, acc: 0.8999999761581421)
[2025-02-16 12:20:55,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:55,461][root][INFO] - Training Epoch: 1/2, step 7372/23838 completed (loss: 0.5835148096084595, acc: 0.8301886916160583)
[2025-02-16 12:20:55,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:55,895][root][INFO] - Training Epoch: 1/2, step 7373/23838 completed (loss: 0.33955568075180054, acc: 0.8962963223457336)
[2025-02-16 12:20:56,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:56,302][root][INFO] - Training Epoch: 1/2, step 7374/23838 completed (loss: 0.19814081490039825, acc: 0.9734513163566589)
[2025-02-16 12:20:56,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:56,751][root][INFO] - Training Epoch: 1/2, step 7375/23838 completed (loss: 0.5163025856018066, acc: 0.7881355881690979)
[2025-02-16 12:20:56,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:57,154][root][INFO] - Training Epoch: 1/2, step 7376/23838 completed (loss: 0.5444117188453674, acc: 0.8730158805847168)
[2025-02-16 12:20:57,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:57,601][root][INFO] - Training Epoch: 1/2, step 7377/23838 completed (loss: 0.5977713465690613, acc: 0.8292682766914368)
[2025-02-16 12:20:57,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:58,054][root][INFO] - Training Epoch: 1/2, step 7378/23838 completed (loss: 0.5820118188858032, acc: 0.7931034564971924)
[2025-02-16 12:20:58,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:58,498][root][INFO] - Training Epoch: 1/2, step 7379/23838 completed (loss: 0.20407651364803314, acc: 0.9426229596138)
[2025-02-16 12:20:58,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:58,962][root][INFO] - Training Epoch: 1/2, step 7380/23838 completed (loss: 0.7718388438224792, acc: 0.7977527976036072)
[2025-02-16 12:20:59,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:59,403][root][INFO] - Training Epoch: 1/2, step 7381/23838 completed (loss: 0.15656113624572754, acc: 0.9230769276618958)
[2025-02-16 12:20:59,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:20:59,867][root][INFO] - Training Epoch: 1/2, step 7382/23838 completed (loss: 0.25313541293144226, acc: 0.9126213788986206)
[2025-02-16 12:21:00,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:00,289][root][INFO] - Training Epoch: 1/2, step 7383/23838 completed (loss: 1.1729508638381958, acc: 0.6764705777168274)
[2025-02-16 12:21:00,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:00,673][root][INFO] - Training Epoch: 1/2, step 7384/23838 completed (loss: 0.17313824594020844, acc: 0.960629940032959)
[2025-02-16 12:21:00,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:01,106][root][INFO] - Training Epoch: 1/2, step 7385/23838 completed (loss: 0.3558226227760315, acc: 0.9111111164093018)
[2025-02-16 12:21:01,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:01,553][root][INFO] - Training Epoch: 1/2, step 7386/23838 completed (loss: 0.3895404040813446, acc: 0.875)
[2025-02-16 12:21:01,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:02,001][root][INFO] - Training Epoch: 1/2, step 7387/23838 completed (loss: 0.4899127185344696, acc: 0.8863636255264282)
[2025-02-16 12:21:02,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:02,401][root][INFO] - Training Epoch: 1/2, step 7388/23838 completed (loss: 0.7919377088546753, acc: 0.78899085521698)
[2025-02-16 12:21:02,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:02,860][root][INFO] - Training Epoch: 1/2, step 7389/23838 completed (loss: 0.22825367748737335, acc: 0.9256198406219482)
[2025-02-16 12:21:03,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:03,325][root][INFO] - Training Epoch: 1/2, step 7390/23838 completed (loss: 0.3033060133457184, acc: 0.8933333158493042)
[2025-02-16 12:21:03,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:03,766][root][INFO] - Training Epoch: 1/2, step 7391/23838 completed (loss: 0.7036983370780945, acc: 0.7971014380455017)
[2025-02-16 12:21:03,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:04,250][root][INFO] - Training Epoch: 1/2, step 7392/23838 completed (loss: 0.7609773278236389, acc: 0.7900000214576721)
[2025-02-16 12:21:04,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:04,705][root][INFO] - Training Epoch: 1/2, step 7393/23838 completed (loss: 0.36888745427131653, acc: 0.8888888955116272)
[2025-02-16 12:21:04,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:05,080][root][INFO] - Training Epoch: 1/2, step 7394/23838 completed (loss: 0.531926691532135, acc: 0.8807339668273926)
[2025-02-16 12:21:05,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:05,445][root][INFO] - Training Epoch: 1/2, step 7395/23838 completed (loss: 0.16449443995952606, acc: 0.9327731132507324)
[2025-02-16 12:21:05,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:05,840][root][INFO] - Training Epoch: 1/2, step 7396/23838 completed (loss: 0.3194392919540405, acc: 0.9333333373069763)
[2025-02-16 12:21:06,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:06,304][root][INFO] - Training Epoch: 1/2, step 7397/23838 completed (loss: 0.2508007287979126, acc: 0.9416666626930237)
[2025-02-16 12:21:06,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:06,720][root][INFO] - Training Epoch: 1/2, step 7398/23838 completed (loss: 0.32886335253715515, acc: 0.9152542352676392)
[2025-02-16 12:21:06,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:07,160][root][INFO] - Training Epoch: 1/2, step 7399/23838 completed (loss: 0.3637736737728119, acc: 0.876288652420044)
[2025-02-16 12:21:07,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:07,863][root][INFO] - Training Epoch: 1/2, step 7400/23838 completed (loss: 0.24004623293876648, acc: 0.9444444179534912)
[2025-02-16 12:21:08,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:08,378][root][INFO] - Training Epoch: 1/2, step 7401/23838 completed (loss: 0.34205755591392517, acc: 0.8778625726699829)
[2025-02-16 12:21:08,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:08,846][root][INFO] - Training Epoch: 1/2, step 7402/23838 completed (loss: 0.24039001762866974, acc: 0.9166666865348816)
[2025-02-16 12:21:09,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:09,234][root][INFO] - Training Epoch: 1/2, step 7403/23838 completed (loss: 0.19546325504779816, acc: 0.9408283829689026)
[2025-02-16 12:21:09,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:09,655][root][INFO] - Training Epoch: 1/2, step 7404/23838 completed (loss: 0.6112104654312134, acc: 0.8208954930305481)
[2025-02-16 12:21:09,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:10,078][root][INFO] - Training Epoch: 1/2, step 7405/23838 completed (loss: 0.1885772943496704, acc: 0.9433962106704712)
[2025-02-16 12:21:10,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:10,532][root][INFO] - Training Epoch: 1/2, step 7406/23838 completed (loss: 0.6327011585235596, acc: 0.8181818127632141)
[2025-02-16 12:21:10,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:11,239][root][INFO] - Training Epoch: 1/2, step 7407/23838 completed (loss: 0.2796812355518341, acc: 0.9208333492279053)
[2025-02-16 12:21:11,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:11,684][root][INFO] - Training Epoch: 1/2, step 7408/23838 completed (loss: 0.22370828688144684, acc: 0.9224137663841248)
[2025-02-16 12:21:11,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:12,127][root][INFO] - Training Epoch: 1/2, step 7409/23838 completed (loss: 0.3899465501308441, acc: 0.8947368264198303)
[2025-02-16 12:21:12,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:12,561][root][INFO] - Training Epoch: 1/2, step 7410/23838 completed (loss: 0.3628171384334564, acc: 0.8793103694915771)
[2025-02-16 12:21:12,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:12,962][root][INFO] - Training Epoch: 1/2, step 7411/23838 completed (loss: 0.32220447063446045, acc: 0.929411768913269)
[2025-02-16 12:21:13,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:13,397][root][INFO] - Training Epoch: 1/2, step 7412/23838 completed (loss: 0.19761084020137787, acc: 0.9550561904907227)
[2025-02-16 12:21:13,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:13,845][root][INFO] - Training Epoch: 1/2, step 7413/23838 completed (loss: 0.4753711223602295, acc: 0.8615384697914124)
[2025-02-16 12:21:14,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:14,292][root][INFO] - Training Epoch: 1/2, step 7414/23838 completed (loss: 0.3260653018951416, acc: 0.9200000166893005)
[2025-02-16 12:21:14,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:14,622][root][INFO] - Training Epoch: 1/2, step 7415/23838 completed (loss: 0.9328047633171082, acc: 0.7428571581840515)
[2025-02-16 12:21:14,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:14,980][root][INFO] - Training Epoch: 1/2, step 7416/23838 completed (loss: 0.1804882287979126, acc: 0.9489051103591919)
[2025-02-16 12:21:15,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:15,345][root][INFO] - Training Epoch: 1/2, step 7417/23838 completed (loss: 0.33571603894233704, acc: 0.8720930218696594)
[2025-02-16 12:21:15,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:15,765][root][INFO] - Training Epoch: 1/2, step 7418/23838 completed (loss: 0.7624921202659607, acc: 0.7719298005104065)
[2025-02-16 12:21:15,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:16,227][root][INFO] - Training Epoch: 1/2, step 7419/23838 completed (loss: 0.6819449067115784, acc: 0.8333333134651184)
[2025-02-16 12:21:16,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:16,664][root][INFO] - Training Epoch: 1/2, step 7420/23838 completed (loss: 0.2627562880516052, acc: 0.8962264060974121)
[2025-02-16 12:21:16,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:17,105][root][INFO] - Training Epoch: 1/2, step 7421/23838 completed (loss: 0.5041179656982422, acc: 0.8169013857841492)
[2025-02-16 12:21:17,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:17,507][root][INFO] - Training Epoch: 1/2, step 7422/23838 completed (loss: 0.4827684462070465, acc: 0.8165137767791748)
[2025-02-16 12:21:17,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:17,928][root][INFO] - Training Epoch: 1/2, step 7423/23838 completed (loss: 0.22948241233825684, acc: 0.9242424368858337)
[2025-02-16 12:21:18,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:18,325][root][INFO] - Training Epoch: 1/2, step 7424/23838 completed (loss: 0.3192812204360962, acc: 0.9338235259056091)
[2025-02-16 12:21:18,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:18,763][root][INFO] - Training Epoch: 1/2, step 7425/23838 completed (loss: 0.5912953019142151, acc: 0.8288288116455078)
[2025-02-16 12:21:18,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:19,235][root][INFO] - Training Epoch: 1/2, step 7426/23838 completed (loss: 0.32657092809677124, acc: 0.9019607901573181)
[2025-02-16 12:21:19,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:19,681][root][INFO] - Training Epoch: 1/2, step 7427/23838 completed (loss: 0.8219279050827026, acc: 0.7857142686843872)
[2025-02-16 12:21:19,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:20,103][root][INFO] - Training Epoch: 1/2, step 7428/23838 completed (loss: 0.2844861149787903, acc: 0.904411792755127)
[2025-02-16 12:21:20,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:20,471][root][INFO] - Training Epoch: 1/2, step 7429/23838 completed (loss: 0.44693008065223694, acc: 0.8598130941390991)
[2025-02-16 12:21:20,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:20,956][root][INFO] - Training Epoch: 1/2, step 7430/23838 completed (loss: 0.8149123787879944, acc: 0.7681159377098083)
[2025-02-16 12:21:21,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:21,379][root][INFO] - Training Epoch: 1/2, step 7431/23838 completed (loss: 0.21500955522060394, acc: 0.9461538195610046)
[2025-02-16 12:21:21,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:21,752][root][INFO] - Training Epoch: 1/2, step 7432/23838 completed (loss: 0.18453559279441833, acc: 0.9444444179534912)
[2025-02-16 12:21:21,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:22,159][root][INFO] - Training Epoch: 1/2, step 7433/23838 completed (loss: 0.7183434367179871, acc: 0.8333333134651184)
[2025-02-16 12:21:22,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:22,609][root][INFO] - Training Epoch: 1/2, step 7434/23838 completed (loss: 0.4705434739589691, acc: 0.8536585569381714)
[2025-02-16 12:21:22,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:23,045][root][INFO] - Training Epoch: 1/2, step 7435/23838 completed (loss: 0.4673234522342682, acc: 0.8727272748947144)
[2025-02-16 12:21:23,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:23,434][root][INFO] - Training Epoch: 1/2, step 7436/23838 completed (loss: 0.6131554841995239, acc: 0.8333333134651184)
[2025-02-16 12:21:23,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:23,802][root][INFO] - Training Epoch: 1/2, step 7437/23838 completed (loss: 0.25090843439102173, acc: 0.9200000166893005)
[2025-02-16 12:21:23,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:24,201][root][INFO] - Training Epoch: 1/2, step 7438/23838 completed (loss: 0.29591605067253113, acc: 0.9130434989929199)
[2025-02-16 12:21:24,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:24,636][root][INFO] - Training Epoch: 1/2, step 7439/23838 completed (loss: 0.221620574593544, acc: 0.9406779408454895)
[2025-02-16 12:21:24,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:25,059][root][INFO] - Training Epoch: 1/2, step 7440/23838 completed (loss: 0.1444212645292282, acc: 0.9615384340286255)
[2025-02-16 12:21:25,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:25,516][root][INFO] - Training Epoch: 1/2, step 7441/23838 completed (loss: 0.33232372999191284, acc: 0.885496199131012)
[2025-02-16 12:21:25,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:25,996][root][INFO] - Training Epoch: 1/2, step 7442/23838 completed (loss: 0.7793154716491699, acc: 0.8214285969734192)
[2025-02-16 12:21:26,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:26,423][root][INFO] - Training Epoch: 1/2, step 7443/23838 completed (loss: 0.3857582211494446, acc: 0.8877550959587097)
[2025-02-16 12:21:26,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:26,876][root][INFO] - Training Epoch: 1/2, step 7444/23838 completed (loss: 0.2911604344844818, acc: 0.9058823585510254)
[2025-02-16 12:21:27,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:27,297][root][INFO] - Training Epoch: 1/2, step 7445/23838 completed (loss: 0.26497572660446167, acc: 0.9298245906829834)
[2025-02-16 12:21:27,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:27,737][root][INFO] - Training Epoch: 1/2, step 7446/23838 completed (loss: 0.7072919011116028, acc: 0.800000011920929)
[2025-02-16 12:21:27,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:28,146][root][INFO] - Training Epoch: 1/2, step 7447/23838 completed (loss: 0.3411751091480255, acc: 0.8676470518112183)
[2025-02-16 12:21:28,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:28,517][root][INFO] - Training Epoch: 1/2, step 7448/23838 completed (loss: 0.5791382789611816, acc: 0.8738738894462585)
[2025-02-16 12:21:28,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:28,951][root][INFO] - Training Epoch: 1/2, step 7449/23838 completed (loss: 0.49755290150642395, acc: 0.8666666746139526)
[2025-02-16 12:21:29,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:29,424][root][INFO] - Training Epoch: 1/2, step 7450/23838 completed (loss: 0.35395216941833496, acc: 0.9125683307647705)
[2025-02-16 12:21:29,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:29,829][root][INFO] - Training Epoch: 1/2, step 7451/23838 completed (loss: 0.33913755416870117, acc: 0.9205297827720642)
[2025-02-16 12:21:30,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:30,243][root][INFO] - Training Epoch: 1/2, step 7452/23838 completed (loss: 0.4583224654197693, acc: 0.8734177350997925)
[2025-02-16 12:21:30,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:30,722][root][INFO] - Training Epoch: 1/2, step 7453/23838 completed (loss: 0.9219053387641907, acc: 0.7599999904632568)
[2025-02-16 12:21:30,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:31,171][root][INFO] - Training Epoch: 1/2, step 7454/23838 completed (loss: 0.7441861629486084, acc: 0.7924528121948242)
[2025-02-16 12:21:31,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:31,592][root][INFO] - Training Epoch: 1/2, step 7455/23838 completed (loss: 0.2460756152868271, acc: 0.966292142868042)
[2025-02-16 12:21:31,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:32,029][root][INFO] - Training Epoch: 1/2, step 7456/23838 completed (loss: 0.13400471210479736, acc: 0.9700000286102295)
[2025-02-16 12:21:32,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:32,436][root][INFO] - Training Epoch: 1/2, step 7457/23838 completed (loss: 0.31252536177635193, acc: 0.9354838728904724)
[2025-02-16 12:21:32,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:32,866][root][INFO] - Training Epoch: 1/2, step 7458/23838 completed (loss: 0.5553349852561951, acc: 0.8571428656578064)
[2025-02-16 12:21:33,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:33,295][root][INFO] - Training Epoch: 1/2, step 7459/23838 completed (loss: 0.3812764286994934, acc: 0.89552241563797)
[2025-02-16 12:21:33,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:33,732][root][INFO] - Training Epoch: 1/2, step 7460/23838 completed (loss: 0.45681461691856384, acc: 0.874015748500824)
[2025-02-16 12:21:33,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:34,280][root][INFO] - Training Epoch: 1/2, step 7461/23838 completed (loss: 0.37087884545326233, acc: 0.8999999761581421)
[2025-02-16 12:21:34,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:34,647][root][INFO] - Training Epoch: 1/2, step 7462/23838 completed (loss: 0.5994563698768616, acc: 0.8482142686843872)
[2025-02-16 12:21:34,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:35,021][root][INFO] - Training Epoch: 1/2, step 7463/23838 completed (loss: 0.4981376826763153, acc: 0.8493150472640991)
[2025-02-16 12:21:35,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:35,426][root][INFO] - Training Epoch: 1/2, step 7464/23838 completed (loss: 0.5128844976425171, acc: 0.8531468510627747)
[2025-02-16 12:21:35,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:35,799][root][INFO] - Training Epoch: 1/2, step 7465/23838 completed (loss: 0.2840653359889984, acc: 0.9459459185600281)
[2025-02-16 12:21:35,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:36,183][root][INFO] - Training Epoch: 1/2, step 7466/23838 completed (loss: 0.4356423616409302, acc: 0.8611111044883728)
[2025-02-16 12:21:36,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:36,663][root][INFO] - Training Epoch: 1/2, step 7467/23838 completed (loss: 0.23202255368232727, acc: 0.9518072009086609)
[2025-02-16 12:21:36,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:37,077][root][INFO] - Training Epoch: 1/2, step 7468/23838 completed (loss: 0.4524358808994293, acc: 0.8812500238418579)
[2025-02-16 12:21:37,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:37,472][root][INFO] - Training Epoch: 1/2, step 7469/23838 completed (loss: 0.5222612619400024, acc: 0.8272727131843567)
[2025-02-16 12:21:37,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:38,222][root][INFO] - Training Epoch: 1/2, step 7470/23838 completed (loss: 0.3886522948741913, acc: 0.8949771523475647)
[2025-02-16 12:21:38,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:38,707][root][INFO] - Training Epoch: 1/2, step 7471/23838 completed (loss: 0.5016747713088989, acc: 0.8761062026023865)
[2025-02-16 12:21:38,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:39,195][root][INFO] - Training Epoch: 1/2, step 7472/23838 completed (loss: 0.35893484950065613, acc: 0.9117646813392639)
[2025-02-16 12:21:39,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:39,639][root][INFO] - Training Epoch: 1/2, step 7473/23838 completed (loss: 0.2581233084201813, acc: 0.9477124214172363)
[2025-02-16 12:21:39,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:40,037][root][INFO] - Training Epoch: 1/2, step 7474/23838 completed (loss: 0.4453714191913605, acc: 0.9013158082962036)
[2025-02-16 12:21:40,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:40,471][root][INFO] - Training Epoch: 1/2, step 7475/23838 completed (loss: 0.4129182696342468, acc: 0.9008264541625977)
[2025-02-16 12:21:40,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:40,887][root][INFO] - Training Epoch: 1/2, step 7476/23838 completed (loss: 0.5898601412773132, acc: 0.7808219194412231)
[2025-02-16 12:21:41,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:41,388][root][INFO] - Training Epoch: 1/2, step 7477/23838 completed (loss: 0.5578258037567139, acc: 0.821052610874176)
[2025-02-16 12:21:41,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:41,845][root][INFO] - Training Epoch: 1/2, step 7478/23838 completed (loss: 0.28064173460006714, acc: 0.916167676448822)
[2025-02-16 12:21:42,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:42,313][root][INFO] - Training Epoch: 1/2, step 7479/23838 completed (loss: 0.4019647240638733, acc: 0.8986784219741821)
[2025-02-16 12:21:42,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:42,707][root][INFO] - Training Epoch: 1/2, step 7480/23838 completed (loss: 0.4892255663871765, acc: 0.8169013857841492)
[2025-02-16 12:21:42,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:43,095][root][INFO] - Training Epoch: 1/2, step 7481/23838 completed (loss: 0.33599528670310974, acc: 0.9074074029922485)
[2025-02-16 12:21:43,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:43,499][root][INFO] - Training Epoch: 1/2, step 7482/23838 completed (loss: 0.7121289372444153, acc: 0.7604166865348816)
[2025-02-16 12:21:43,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:43,966][root][INFO] - Training Epoch: 1/2, step 7483/23838 completed (loss: 0.5420140624046326, acc: 0.8488371968269348)
[2025-02-16 12:21:44,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:44,424][root][INFO] - Training Epoch: 1/2, step 7484/23838 completed (loss: 0.4963234066963196, acc: 0.9090909361839294)
[2025-02-16 12:21:44,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:44,833][root][INFO] - Training Epoch: 1/2, step 7485/23838 completed (loss: 0.7309999465942383, acc: 0.8695651888847351)
[2025-02-16 12:21:45,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:45,224][root][INFO] - Training Epoch: 1/2, step 7486/23838 completed (loss: 0.7681747674942017, acc: 0.8111110925674438)
[2025-02-16 12:21:45,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:45,674][root][INFO] - Training Epoch: 1/2, step 7487/23838 completed (loss: 0.5481447577476501, acc: 0.8534482717514038)
[2025-02-16 12:21:45,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:46,081][root][INFO] - Training Epoch: 1/2, step 7488/23838 completed (loss: 0.5273793935775757, acc: 0.875)
[2025-02-16 12:21:46,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:46,539][root][INFO] - Training Epoch: 1/2, step 7489/23838 completed (loss: 0.6242738366127014, acc: 0.887499988079071)
[2025-02-16 12:21:46,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:46,992][root][INFO] - Training Epoch: 1/2, step 7490/23838 completed (loss: 0.279134601354599, acc: 0.9155844449996948)
[2025-02-16 12:21:47,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:47,443][root][INFO] - Training Epoch: 1/2, step 7491/23838 completed (loss: 0.2709029018878937, acc: 0.9300699234008789)
[2025-02-16 12:21:47,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:47,919][root][INFO] - Training Epoch: 1/2, step 7492/23838 completed (loss: 0.41158485412597656, acc: 0.8819875717163086)
[2025-02-16 12:21:48,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:48,358][root][INFO] - Training Epoch: 1/2, step 7493/23838 completed (loss: 0.2774646282196045, acc: 0.926174521446228)
[2025-02-16 12:21:48,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:48,820][root][INFO] - Training Epoch: 1/2, step 7494/23838 completed (loss: 0.43413880467414856, acc: 0.8661971688270569)
[2025-02-16 12:21:49,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:49,277][root][INFO] - Training Epoch: 1/2, step 7495/23838 completed (loss: 0.4218667447566986, acc: 0.9140625)
[2025-02-16 12:21:49,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:49,645][root][INFO] - Training Epoch: 1/2, step 7496/23838 completed (loss: 0.19569794833660126, acc: 0.9173553586006165)
[2025-02-16 12:21:49,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:50,024][root][INFO] - Training Epoch: 1/2, step 7497/23838 completed (loss: 0.3360300064086914, acc: 0.8976377844810486)
[2025-02-16 12:21:50,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:50,493][root][INFO] - Training Epoch: 1/2, step 7498/23838 completed (loss: 1.277095913887024, acc: 0.6296296119689941)
[2025-02-16 12:21:50,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:50,936][root][INFO] - Training Epoch: 1/2, step 7499/23838 completed (loss: 0.14686527848243713, acc: 0.9670329689979553)
[2025-02-16 12:21:51,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:51,358][root][INFO] - Training Epoch: 1/2, step 7500/23838 completed (loss: 0.37803682684898376, acc: 0.8799999952316284)
[2025-02-16 12:21:51,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:51,786][root][INFO] - Training Epoch: 1/2, step 7501/23838 completed (loss: 0.4575691819190979, acc: 0.8606557250022888)
[2025-02-16 12:21:51,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:52,156][root][INFO] - Training Epoch: 1/2, step 7502/23838 completed (loss: 0.683699369430542, acc: 0.8070175647735596)
[2025-02-16 12:21:52,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:52,591][root][INFO] - Training Epoch: 1/2, step 7503/23838 completed (loss: 0.47104713320732117, acc: 0.8648648858070374)
[2025-02-16 12:21:52,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:53,033][root][INFO] - Training Epoch: 1/2, step 7504/23838 completed (loss: 0.7346481084823608, acc: 0.7837837934494019)
[2025-02-16 12:21:53,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:53,453][root][INFO] - Training Epoch: 1/2, step 7505/23838 completed (loss: 0.7745300531387329, acc: 0.75)
[2025-02-16 12:21:53,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:53,840][root][INFO] - Training Epoch: 1/2, step 7506/23838 completed (loss: 0.4561014473438263, acc: 0.8863636255264282)
[2025-02-16 12:21:54,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:54,263][root][INFO] - Training Epoch: 1/2, step 7507/23838 completed (loss: 0.4275360107421875, acc: 0.8656716346740723)
[2025-02-16 12:21:54,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:54,721][root][INFO] - Training Epoch: 1/2, step 7508/23838 completed (loss: 0.47212082147598267, acc: 0.8799999952316284)
[2025-02-16 12:21:54,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:55,088][root][INFO] - Training Epoch: 1/2, step 7509/23838 completed (loss: 0.5114748477935791, acc: 0.835616409778595)
[2025-02-16 12:21:55,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:55,572][root][INFO] - Training Epoch: 1/2, step 7510/23838 completed (loss: 0.5065932869911194, acc: 0.849056601524353)
[2025-02-16 12:21:55,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:56,010][root][INFO] - Training Epoch: 1/2, step 7511/23838 completed (loss: 0.6828033924102783, acc: 0.8316831588745117)
[2025-02-16 12:21:56,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:56,499][root][INFO] - Training Epoch: 1/2, step 7512/23838 completed (loss: 0.2773062586784363, acc: 0.9175257682800293)
[2025-02-16 12:21:56,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:56,913][root][INFO] - Training Epoch: 1/2, step 7513/23838 completed (loss: 0.4713200628757477, acc: 0.8405796885490417)
[2025-02-16 12:21:57,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:57,307][root][INFO] - Training Epoch: 1/2, step 7514/23838 completed (loss: 0.5360466837882996, acc: 0.8238993883132935)
[2025-02-16 12:21:57,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:57,703][root][INFO] - Training Epoch: 1/2, step 7515/23838 completed (loss: 0.6112250685691833, acc: 0.8106508851051331)
[2025-02-16 12:21:57,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:58,124][root][INFO] - Training Epoch: 1/2, step 7516/23838 completed (loss: 0.3943156898021698, acc: 0.8823529481887817)
[2025-02-16 12:21:58,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:58,572][root][INFO] - Training Epoch: 1/2, step 7517/23838 completed (loss: 0.7402634024620056, acc: 0.8181818127632141)
[2025-02-16 12:21:58,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:59,057][root][INFO] - Training Epoch: 1/2, step 7518/23838 completed (loss: 0.30590540170669556, acc: 0.9137930870056152)
[2025-02-16 12:21:59,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:21:59,651][root][INFO] - Training Epoch: 1/2, step 7519/23838 completed (loss: 0.41979408264160156, acc: 0.8926174640655518)
[2025-02-16 12:21:59,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:00,051][root][INFO] - Training Epoch: 1/2, step 7520/23838 completed (loss: 0.5886481404304504, acc: 0.8299319744110107)
[2025-02-16 12:22:00,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:00,508][root][INFO] - Training Epoch: 1/2, step 7521/23838 completed (loss: 0.7934430837631226, acc: 0.762499988079071)
[2025-02-16 12:22:00,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:00,963][root][INFO] - Training Epoch: 1/2, step 7522/23838 completed (loss: 0.14245498180389404, acc: 0.969072163105011)
[2025-02-16 12:22:01,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:01,401][root][INFO] - Training Epoch: 1/2, step 7523/23838 completed (loss: 0.44855356216430664, acc: 0.8775510191917419)
[2025-02-16 12:22:01,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:01,941][root][INFO] - Training Epoch: 1/2, step 7524/23838 completed (loss: 0.50621098279953, acc: 0.8700000047683716)
[2025-02-16 12:22:02,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:02,368][root][INFO] - Training Epoch: 1/2, step 7525/23838 completed (loss: 0.41760045289993286, acc: 0.8805969953536987)
[2025-02-16 12:22:02,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:02,807][root][INFO] - Training Epoch: 1/2, step 7526/23838 completed (loss: 0.4372733235359192, acc: 0.8999999761581421)
[2025-02-16 12:22:03,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:03,504][root][INFO] - Training Epoch: 1/2, step 7527/23838 completed (loss: 0.3386428654193878, acc: 0.8975409865379333)
[2025-02-16 12:22:03,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:04,017][root][INFO] - Training Epoch: 1/2, step 7528/23838 completed (loss: 0.3302193880081177, acc: 0.9122806787490845)
[2025-02-16 12:22:04,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:04,466][root][INFO] - Training Epoch: 1/2, step 7529/23838 completed (loss: 0.23428839445114136, acc: 0.947826087474823)
[2025-02-16 12:22:04,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:04,926][root][INFO] - Training Epoch: 1/2, step 7530/23838 completed (loss: 0.3198270797729492, acc: 0.9130434989929199)
[2025-02-16 12:22:05,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:05,312][root][INFO] - Training Epoch: 1/2, step 7531/23838 completed (loss: 0.27124398946762085, acc: 0.9354838728904724)
[2025-02-16 12:22:05,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:05,752][root][INFO] - Training Epoch: 1/2, step 7532/23838 completed (loss: 0.2622702717781067, acc: 0.9027777910232544)
[2025-02-16 12:22:05,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:06,123][root][INFO] - Training Epoch: 1/2, step 7533/23838 completed (loss: 0.325775146484375, acc: 0.8969072103500366)
[2025-02-16 12:22:06,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:06,563][root][INFO] - Training Epoch: 1/2, step 7534/23838 completed (loss: 0.4269557297229767, acc: 0.8805969953536987)
[2025-02-16 12:22:06,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:06,975][root][INFO] - Training Epoch: 1/2, step 7535/23838 completed (loss: 0.4179249107837677, acc: 0.8999999761581421)
[2025-02-16 12:22:07,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:08,001][root][INFO] - Training Epoch: 1/2, step 7536/23838 completed (loss: 0.3262784779071808, acc: 0.9097222089767456)
[2025-02-16 12:22:08,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:08,481][root][INFO] - Training Epoch: 1/2, step 7537/23838 completed (loss: 0.9431377649307251, acc: 0.707317054271698)
[2025-02-16 12:22:08,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:08,934][root][INFO] - Training Epoch: 1/2, step 7538/23838 completed (loss: 0.3180578947067261, acc: 0.9069767594337463)
[2025-02-16 12:22:09,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:09,436][root][INFO] - Training Epoch: 1/2, step 7539/23838 completed (loss: 0.2179192304611206, acc: 0.9270073175430298)
[2025-02-16 12:22:09,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:09,860][root][INFO] - Training Epoch: 1/2, step 7540/23838 completed (loss: 0.7524556517601013, acc: 0.7887324094772339)
[2025-02-16 12:22:10,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:10,272][root][INFO] - Training Epoch: 1/2, step 7541/23838 completed (loss: 0.2311311513185501, acc: 0.9333333373069763)
[2025-02-16 12:22:10,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:10,753][root][INFO] - Training Epoch: 1/2, step 7542/23838 completed (loss: 0.4944935739040375, acc: 0.8770949840545654)
[2025-02-16 12:22:10,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:11,184][root][INFO] - Training Epoch: 1/2, step 7543/23838 completed (loss: 0.4978310167789459, acc: 0.8850574493408203)
[2025-02-16 12:22:11,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:11,755][root][INFO] - Training Epoch: 1/2, step 7544/23838 completed (loss: 0.3819389045238495, acc: 0.875)
[2025-02-16 12:22:11,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:12,199][root][INFO] - Training Epoch: 1/2, step 7545/23838 completed (loss: 0.4750367999076843, acc: 0.8617886304855347)
[2025-02-16 12:22:12,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:12,603][root][INFO] - Training Epoch: 1/2, step 7546/23838 completed (loss: 0.749917209148407, acc: 0.800000011920929)
[2025-02-16 12:22:12,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:12,991][root][INFO] - Training Epoch: 1/2, step 7547/23838 completed (loss: 0.9386441111564636, acc: 0.7446808218955994)
[2025-02-16 12:22:13,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:13,471][root][INFO] - Training Epoch: 1/2, step 7548/23838 completed (loss: 0.16484187543392181, acc: 0.9436619877815247)
[2025-02-16 12:22:13,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:13,934][root][INFO] - Training Epoch: 1/2, step 7549/23838 completed (loss: 0.6503384113311768, acc: 0.8631578683853149)
[2025-02-16 12:22:14,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:14,322][root][INFO] - Training Epoch: 1/2, step 7550/23838 completed (loss: 0.433848112821579, acc: 0.8999999761581421)
[2025-02-16 12:22:14,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:14,785][root][INFO] - Training Epoch: 1/2, step 7551/23838 completed (loss: 0.189583882689476, acc: 0.9459459185600281)
[2025-02-16 12:22:14,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:15,204][root][INFO] - Training Epoch: 1/2, step 7552/23838 completed (loss: 0.3319239318370819, acc: 0.8934426307678223)
[2025-02-16 12:22:15,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:15,657][root][INFO] - Training Epoch: 1/2, step 7553/23838 completed (loss: 0.5870309472084045, acc: 0.84375)
[2025-02-16 12:22:15,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:16,079][root][INFO] - Training Epoch: 1/2, step 7554/23838 completed (loss: 0.5024858117103577, acc: 0.8333333134651184)
[2025-02-16 12:22:16,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:16,590][root][INFO] - Training Epoch: 1/2, step 7555/23838 completed (loss: 0.3974355459213257, acc: 0.8882681727409363)
[2025-02-16 12:22:16,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:17,022][root][INFO] - Training Epoch: 1/2, step 7556/23838 completed (loss: 0.24497634172439575, acc: 0.9300000071525574)
[2025-02-16 12:22:17,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:17,443][root][INFO] - Training Epoch: 1/2, step 7557/23838 completed (loss: 0.20135118067264557, acc: 0.9485294222831726)
[2025-02-16 12:22:17,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:17,931][root][INFO] - Training Epoch: 1/2, step 7558/23838 completed (loss: 0.22242428362369537, acc: 0.9285714030265808)
[2025-02-16 12:22:18,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:18,310][root][INFO] - Training Epoch: 1/2, step 7559/23838 completed (loss: 0.4401135742664337, acc: 0.8852459192276001)
[2025-02-16 12:22:18,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:18,715][root][INFO] - Training Epoch: 1/2, step 7560/23838 completed (loss: 0.372665137052536, acc: 0.9015544056892395)
[2025-02-16 12:22:18,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:19,193][root][INFO] - Training Epoch: 1/2, step 7561/23838 completed (loss: 0.3969649374485016, acc: 0.8834356069564819)
[2025-02-16 12:22:19,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:19,614][root][INFO] - Training Epoch: 1/2, step 7562/23838 completed (loss: 0.7131803035736084, acc: 0.7428571581840515)
[2025-02-16 12:22:19,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:20,170][root][INFO] - Training Epoch: 1/2, step 7563/23838 completed (loss: 0.1455458551645279, acc: 0.9515418410301208)
[2025-02-16 12:22:20,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:20,716][root][INFO] - Training Epoch: 1/2, step 7564/23838 completed (loss: 0.1731092482805252, acc: 0.9777777791023254)
[2025-02-16 12:22:20,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:21,092][root][INFO] - Training Epoch: 1/2, step 7565/23838 completed (loss: 0.2621934115886688, acc: 0.954954981803894)
[2025-02-16 12:22:21,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:21,720][root][INFO] - Training Epoch: 1/2, step 7566/23838 completed (loss: 0.3474717438220978, acc: 0.9053254723548889)
[2025-02-16 12:22:21,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:22,168][root][INFO] - Training Epoch: 1/2, step 7567/23838 completed (loss: 0.16902682185173035, acc: 0.9433962106704712)
[2025-02-16 12:22:22,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:22,554][root][INFO] - Training Epoch: 1/2, step 7568/23838 completed (loss: 0.34239283204078674, acc: 0.9015544056892395)
[2025-02-16 12:22:22,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:23,006][root][INFO] - Training Epoch: 1/2, step 7569/23838 completed (loss: 0.3810606896877289, acc: 0.9039999842643738)
[2025-02-16 12:22:23,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:23,433][root][INFO] - Training Epoch: 1/2, step 7570/23838 completed (loss: 0.3079111874103546, acc: 0.9180327653884888)
[2025-02-16 12:22:23,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:23,874][root][INFO] - Training Epoch: 1/2, step 7571/23838 completed (loss: 0.23772498965263367, acc: 0.9290322661399841)
[2025-02-16 12:22:24,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:24,311][root][INFO] - Training Epoch: 1/2, step 7572/23838 completed (loss: 0.42769855260849, acc: 0.8857142925262451)
[2025-02-16 12:22:24,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:24,686][root][INFO] - Training Epoch: 1/2, step 7573/23838 completed (loss: 0.4525544047355652, acc: 0.8571428656578064)
[2025-02-16 12:22:24,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:25,157][root][INFO] - Training Epoch: 1/2, step 7574/23838 completed (loss: 0.6365061402320862, acc: 0.8172042965888977)
[2025-02-16 12:22:25,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:25,658][root][INFO] - Training Epoch: 1/2, step 7575/23838 completed (loss: 0.4406420886516571, acc: 0.8680555820465088)
[2025-02-16 12:22:25,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:26,097][root][INFO] - Training Epoch: 1/2, step 7576/23838 completed (loss: 0.6156643629074097, acc: 0.8452380895614624)
[2025-02-16 12:22:26,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:26,506][root][INFO] - Training Epoch: 1/2, step 7577/23838 completed (loss: 0.3610263764858246, acc: 0.8952381014823914)
[2025-02-16 12:22:26,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:26,931][root][INFO] - Training Epoch: 1/2, step 7578/23838 completed (loss: 0.7171797752380371, acc: 0.8425925970077515)
[2025-02-16 12:22:27,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:27,383][root][INFO] - Training Epoch: 1/2, step 7579/23838 completed (loss: 0.19584940373897552, acc: 0.9548386931419373)
[2025-02-16 12:22:27,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:27,888][root][INFO] - Training Epoch: 1/2, step 7580/23838 completed (loss: 0.39886993169784546, acc: 0.8701298832893372)
[2025-02-16 12:22:28,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:28,299][root][INFO] - Training Epoch: 1/2, step 7581/23838 completed (loss: 0.4328920245170593, acc: 0.8809523582458496)
[2025-02-16 12:22:28,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:28,704][root][INFO] - Training Epoch: 1/2, step 7582/23838 completed (loss: 0.48860064148902893, acc: 0.8636363744735718)
[2025-02-16 12:22:28,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:29,173][root][INFO] - Training Epoch: 1/2, step 7583/23838 completed (loss: 0.33637431263923645, acc: 0.9130434989929199)
[2025-02-16 12:22:29,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:29,613][root][INFO] - Training Epoch: 1/2, step 7584/23838 completed (loss: 0.2773550748825073, acc: 0.9375)
[2025-02-16 12:22:29,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:30,033][root][INFO] - Training Epoch: 1/2, step 7585/23838 completed (loss: 0.4042893350124359, acc: 0.8659793734550476)
[2025-02-16 12:22:30,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:30,474][root][INFO] - Training Epoch: 1/2, step 7586/23838 completed (loss: 0.4189615845680237, acc: 0.9285714030265808)
[2025-02-16 12:22:30,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:30,876][root][INFO] - Training Epoch: 1/2, step 7587/23838 completed (loss: 0.38329195976257324, acc: 0.9049999713897705)
[2025-02-16 12:22:31,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:31,338][root][INFO] - Training Epoch: 1/2, step 7588/23838 completed (loss: 0.2788246273994446, acc: 0.9122806787490845)
[2025-02-16 12:22:31,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:31,838][root][INFO] - Training Epoch: 1/2, step 7589/23838 completed (loss: 0.333747535943985, acc: 0.9032257795333862)
[2025-02-16 12:22:32,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:32,284][root][INFO] - Training Epoch: 1/2, step 7590/23838 completed (loss: 0.8976445198059082, acc: 0.7469879388809204)
[2025-02-16 12:22:32,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:32,690][root][INFO] - Training Epoch: 1/2, step 7591/23838 completed (loss: 0.5410729050636292, acc: 0.8253968358039856)
[2025-02-16 12:22:32,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:33,103][root][INFO] - Training Epoch: 1/2, step 7592/23838 completed (loss: 0.09174139052629471, acc: 0.970588207244873)
[2025-02-16 12:22:33,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:33,613][root][INFO] - Training Epoch: 1/2, step 7593/23838 completed (loss: 0.467396080493927, acc: 0.8181818127632141)
[2025-02-16 12:22:33,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:34,090][root][INFO] - Training Epoch: 1/2, step 7594/23838 completed (loss: 0.439548522233963, acc: 0.8814814686775208)
[2025-02-16 12:22:34,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:34,567][root][INFO] - Training Epoch: 1/2, step 7595/23838 completed (loss: 0.6130995750427246, acc: 0.8709677457809448)
[2025-02-16 12:22:34,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:34,996][root][INFO] - Training Epoch: 1/2, step 7596/23838 completed (loss: 0.6949067115783691, acc: 0.8130841255187988)
[2025-02-16 12:22:35,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:35,415][root][INFO] - Training Epoch: 1/2, step 7597/23838 completed (loss: 0.24082954227924347, acc: 0.9210526347160339)
[2025-02-16 12:22:35,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:35,896][root][INFO] - Training Epoch: 1/2, step 7598/23838 completed (loss: 0.23841288685798645, acc: 0.9490445852279663)
[2025-02-16 12:22:36,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:36,323][root][INFO] - Training Epoch: 1/2, step 7599/23838 completed (loss: 1.164559245109558, acc: 0.699999988079071)
[2025-02-16 12:22:36,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:36,803][root][INFO] - Training Epoch: 1/2, step 7600/23838 completed (loss: 0.4029422998428345, acc: 0.8879310488700867)
[2025-02-16 12:22:37,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:37,312][root][INFO] - Training Epoch: 1/2, step 7601/23838 completed (loss: 0.33288753032684326, acc: 0.9078947305679321)
[2025-02-16 12:22:37,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:37,775][root][INFO] - Training Epoch: 1/2, step 7602/23838 completed (loss: 0.3056032657623291, acc: 0.8999999761581421)
[2025-02-16 12:22:37,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:38,172][root][INFO] - Training Epoch: 1/2, step 7603/23838 completed (loss: 0.3368184566497803, acc: 0.8899999856948853)
[2025-02-16 12:22:38,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:38,668][root][INFO] - Training Epoch: 1/2, step 7604/23838 completed (loss: 0.3276779353618622, acc: 0.8979591727256775)
[2025-02-16 12:22:38,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:39,127][root][INFO] - Training Epoch: 1/2, step 7605/23838 completed (loss: 0.17046265304088593, acc: 0.9435483813285828)
[2025-02-16 12:22:39,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:39,601][root][INFO] - Training Epoch: 1/2, step 7606/23838 completed (loss: 0.2119837999343872, acc: 0.93388432264328)
[2025-02-16 12:22:39,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:40,055][root][INFO] - Training Epoch: 1/2, step 7607/23838 completed (loss: 0.2645748555660248, acc: 0.9178082346916199)
[2025-02-16 12:22:40,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:40,497][root][INFO] - Training Epoch: 1/2, step 7608/23838 completed (loss: 0.3593774437904358, acc: 0.8991596698760986)
[2025-02-16 12:22:40,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:40,875][root][INFO] - Training Epoch: 1/2, step 7609/23838 completed (loss: 0.27727964520454407, acc: 0.9204545617103577)
[2025-02-16 12:22:41,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:41,302][root][INFO] - Training Epoch: 1/2, step 7610/23838 completed (loss: 0.4631408751010895, acc: 0.8571428656578064)
[2025-02-16 12:22:41,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:41,703][root][INFO] - Training Epoch: 1/2, step 7611/23838 completed (loss: 0.23375095427036285, acc: 0.9415204524993896)
[2025-02-16 12:22:41,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:42,079][root][INFO] - Training Epoch: 1/2, step 7612/23838 completed (loss: 0.20241934061050415, acc: 0.9468085169792175)
[2025-02-16 12:22:42,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:42,498][root][INFO] - Training Epoch: 1/2, step 7613/23838 completed (loss: 0.29337364435195923, acc: 0.9298245906829834)
[2025-02-16 12:22:42,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:42,852][root][INFO] - Training Epoch: 1/2, step 7614/23838 completed (loss: 1.1004849672317505, acc: 0.6744186282157898)
[2025-02-16 12:22:43,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:43,270][root][INFO] - Training Epoch: 1/2, step 7615/23838 completed (loss: 0.7422475218772888, acc: 0.8035714030265808)
[2025-02-16 12:22:43,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:43,680][root][INFO] - Training Epoch: 1/2, step 7616/23838 completed (loss: 1.1456595659255981, acc: 0.7222222089767456)
[2025-02-16 12:22:43,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:44,180][root][INFO] - Training Epoch: 1/2, step 7617/23838 completed (loss: 0.44078782200813293, acc: 0.8794326186180115)
[2025-02-16 12:22:44,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:44,599][root][INFO] - Training Epoch: 1/2, step 7618/23838 completed (loss: 0.7598363757133484, acc: 0.7899159789085388)
[2025-02-16 12:22:44,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:45,005][root][INFO] - Training Epoch: 1/2, step 7619/23838 completed (loss: 0.5815847516059875, acc: 0.8541666865348816)
[2025-02-16 12:22:45,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:45,386][root][INFO] - Training Epoch: 1/2, step 7620/23838 completed (loss: 0.9612433910369873, acc: 0.7260273694992065)
[2025-02-16 12:22:45,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:45,764][root][INFO] - Training Epoch: 1/2, step 7621/23838 completed (loss: 0.7699277400970459, acc: 0.7971014380455017)
[2025-02-16 12:22:45,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:46,147][root][INFO] - Training Epoch: 1/2, step 7622/23838 completed (loss: 0.598676323890686, acc: 0.8125)
[2025-02-16 12:22:46,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:46,568][root][INFO] - Training Epoch: 1/2, step 7623/23838 completed (loss: 0.29018449783325195, acc: 0.914893627166748)
[2025-02-16 12:22:46,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:46,950][root][INFO] - Training Epoch: 1/2, step 7624/23838 completed (loss: 0.8765650391578674, acc: 0.760869562625885)
[2025-02-16 12:22:47,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:47,388][root][INFO] - Training Epoch: 1/2, step 7625/23838 completed (loss: 0.6525290012359619, acc: 0.8396226167678833)
[2025-02-16 12:22:47,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:47,837][root][INFO] - Training Epoch: 1/2, step 7626/23838 completed (loss: 0.36571210622787476, acc: 0.9272727370262146)
[2025-02-16 12:22:48,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:48,325][root][INFO] - Training Epoch: 1/2, step 7627/23838 completed (loss: 0.5122588276863098, acc: 0.8780487775802612)
[2025-02-16 12:22:48,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:48,707][root][INFO] - Training Epoch: 1/2, step 7628/23838 completed (loss: 0.3149190843105316, acc: 0.9411764740943909)
[2025-02-16 12:22:48,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:49,095][root][INFO] - Training Epoch: 1/2, step 7629/23838 completed (loss: 0.7962279319763184, acc: 0.800000011920929)
[2025-02-16 12:22:49,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:49,553][root][INFO] - Training Epoch: 1/2, step 7630/23838 completed (loss: 0.6018039584159851, acc: 0.8387096524238586)
[2025-02-16 12:22:49,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:49,989][root][INFO] - Training Epoch: 1/2, step 7631/23838 completed (loss: 0.929612398147583, acc: 0.7234042286872864)
[2025-02-16 12:22:50,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:50,419][root][INFO] - Training Epoch: 1/2, step 7632/23838 completed (loss: 0.870456874370575, acc: 0.7307692170143127)
[2025-02-16 12:22:50,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:50,901][root][INFO] - Training Epoch: 1/2, step 7633/23838 completed (loss: 0.9922531843185425, acc: 0.71875)
[2025-02-16 12:22:51,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:51,381][root][INFO] - Training Epoch: 1/2, step 7634/23838 completed (loss: 0.30308493971824646, acc: 0.930232584476471)
[2025-02-16 12:22:51,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:51,808][root][INFO] - Training Epoch: 1/2, step 7635/23838 completed (loss: 1.3365693092346191, acc: 0.6666666865348816)
[2025-02-16 12:22:51,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:52,221][root][INFO] - Training Epoch: 1/2, step 7636/23838 completed (loss: 0.9844465255737305, acc: 0.739130437374115)
[2025-02-16 12:22:52,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:52,603][root][INFO] - Training Epoch: 1/2, step 7637/23838 completed (loss: 1.5577095746994019, acc: 0.5510203838348389)
[2025-02-16 12:22:52,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:52,986][root][INFO] - Training Epoch: 1/2, step 7638/23838 completed (loss: 1.8166879415512085, acc: 0.5199999809265137)
[2025-02-16 12:22:53,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:53,447][root][INFO] - Training Epoch: 1/2, step 7639/23838 completed (loss: 0.3474142253398895, acc: 0.8850574493408203)
[2025-02-16 12:22:53,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:53,865][root][INFO] - Training Epoch: 1/2, step 7640/23838 completed (loss: 0.4450983703136444, acc: 0.8728813529014587)
[2025-02-16 12:22:54,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:54,252][root][INFO] - Training Epoch: 1/2, step 7641/23838 completed (loss: 1.1995078325271606, acc: 0.6181818246841431)
[2025-02-16 12:22:54,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:54,636][root][INFO] - Training Epoch: 1/2, step 7642/23838 completed (loss: 0.5052819848060608, acc: 0.8615384697914124)
[2025-02-16 12:22:54,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:55,070][root][INFO] - Training Epoch: 1/2, step 7643/23838 completed (loss: 1.2329758405685425, acc: 0.6721311211585999)
[2025-02-16 12:22:55,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:55,452][root][INFO] - Training Epoch: 1/2, step 7644/23838 completed (loss: 0.5151572227478027, acc: 0.8550724387168884)
[2025-02-16 12:22:55,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:55,829][root][INFO] - Training Epoch: 1/2, step 7645/23838 completed (loss: 0.6100393533706665, acc: 0.7924528121948242)
[2025-02-16 12:22:56,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:56,284][root][INFO] - Training Epoch: 1/2, step 7646/23838 completed (loss: 0.5771855115890503, acc: 0.8333333134651184)
[2025-02-16 12:22:56,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:56,720][root][INFO] - Training Epoch: 1/2, step 7647/23838 completed (loss: 0.22383223474025726, acc: 0.9418604373931885)
[2025-02-16 12:22:56,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:57,136][root][INFO] - Training Epoch: 1/2, step 7648/23838 completed (loss: 0.322768896818161, acc: 0.8916666507720947)
[2025-02-16 12:22:57,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:57,589][root][INFO] - Training Epoch: 1/2, step 7649/23838 completed (loss: 0.98002690076828, acc: 0.7142857313156128)
[2025-02-16 12:22:57,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:58,062][root][INFO] - Training Epoch: 1/2, step 7650/23838 completed (loss: 0.8284777402877808, acc: 0.7325581312179565)
[2025-02-16 12:22:58,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:58,496][root][INFO] - Training Epoch: 1/2, step 7651/23838 completed (loss: 0.9119744300842285, acc: 0.7523809671401978)
[2025-02-16 12:22:58,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:58,926][root][INFO] - Training Epoch: 1/2, step 7652/23838 completed (loss: 0.7721201181411743, acc: 0.7341772317886353)
[2025-02-16 12:22:59,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:59,329][root][INFO] - Training Epoch: 1/2, step 7653/23838 completed (loss: 0.5750235319137573, acc: 0.875)
[2025-02-16 12:22:59,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:22:59,758][root][INFO] - Training Epoch: 1/2, step 7654/23838 completed (loss: 0.6046488881111145, acc: 0.8166666626930237)
[2025-02-16 12:22:59,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:00,184][root][INFO] - Training Epoch: 1/2, step 7655/23838 completed (loss: 0.48488691449165344, acc: 0.8702290058135986)
[2025-02-16 12:23:00,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:00,586][root][INFO] - Training Epoch: 1/2, step 7656/23838 completed (loss: 1.2987843751907349, acc: 0.5344827771186829)
[2025-02-16 12:23:00,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:00,978][root][INFO] - Training Epoch: 1/2, step 7657/23838 completed (loss: 0.9250615835189819, acc: 0.7702702879905701)
[2025-02-16 12:23:01,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:01,339][root][INFO] - Training Epoch: 1/2, step 7658/23838 completed (loss: 0.5307328701019287, acc: 0.8840579986572266)
[2025-02-16 12:23:01,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:01,768][root][INFO] - Training Epoch: 1/2, step 7659/23838 completed (loss: 0.9681930541992188, acc: 0.7179487347602844)
[2025-02-16 12:23:01,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:02,152][root][INFO] - Training Epoch: 1/2, step 7660/23838 completed (loss: 1.0856726169586182, acc: 0.708737850189209)
[2025-02-16 12:23:02,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:02,581][root][INFO] - Training Epoch: 1/2, step 7661/23838 completed (loss: 0.5575695633888245, acc: 0.8292682766914368)
[2025-02-16 12:23:02,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:03,061][root][INFO] - Training Epoch: 1/2, step 7662/23838 completed (loss: 0.9043394923210144, acc: 0.7446808218955994)
[2025-02-16 12:23:03,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:03,580][root][INFO] - Training Epoch: 1/2, step 7663/23838 completed (loss: 0.421128511428833, acc: 0.9090909361839294)
[2025-02-16 12:23:03,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:04,057][root][INFO] - Training Epoch: 1/2, step 7664/23838 completed (loss: 0.2882121205329895, acc: 0.936170220375061)
[2025-02-16 12:23:04,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:04,490][root][INFO] - Training Epoch: 1/2, step 7665/23838 completed (loss: 0.43632200360298157, acc: 0.859375)
[2025-02-16 12:23:04,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:04,906][root][INFO] - Training Epoch: 1/2, step 7666/23838 completed (loss: 1.1341596841812134, acc: 0.7023809552192688)
[2025-02-16 12:23:05,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:05,348][root][INFO] - Training Epoch: 1/2, step 7667/23838 completed (loss: 0.1385069042444229, acc: 0.978723406791687)
[2025-02-16 12:23:05,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:05,770][root][INFO] - Training Epoch: 1/2, step 7668/23838 completed (loss: 0.6361460089683533, acc: 0.8363636136054993)
[2025-02-16 12:23:05,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:06,223][root][INFO] - Training Epoch: 1/2, step 7669/23838 completed (loss: 0.4818553030490875, acc: 0.8863636255264282)
[2025-02-16 12:23:06,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:06,641][root][INFO] - Training Epoch: 1/2, step 7670/23838 completed (loss: 0.5550180673599243, acc: 0.8613861203193665)
[2025-02-16 12:23:06,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:07,073][root][INFO] - Training Epoch: 1/2, step 7671/23838 completed (loss: 0.3804786205291748, acc: 0.8910890817642212)
[2025-02-16 12:23:07,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:07,609][root][INFO] - Training Epoch: 1/2, step 7672/23838 completed (loss: 0.3209017217159271, acc: 0.9193548560142517)
[2025-02-16 12:23:07,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:08,093][root][INFO] - Training Epoch: 1/2, step 7673/23838 completed (loss: 0.346873939037323, acc: 0.9090909361839294)
[2025-02-16 12:23:08,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:08,515][root][INFO] - Training Epoch: 1/2, step 7674/23838 completed (loss: 0.459168940782547, acc: 0.8684210777282715)
[2025-02-16 12:23:08,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:09,017][root][INFO] - Training Epoch: 1/2, step 7675/23838 completed (loss: 0.15241636335849762, acc: 0.9714285731315613)
[2025-02-16 12:23:09,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:09,507][root][INFO] - Training Epoch: 1/2, step 7676/23838 completed (loss: 0.3560516834259033, acc: 0.9009009003639221)
[2025-02-16 12:23:09,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:09,932][root][INFO] - Training Epoch: 1/2, step 7677/23838 completed (loss: 0.8009020686149597, acc: 0.7657657861709595)
[2025-02-16 12:23:10,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:10,370][root][INFO] - Training Epoch: 1/2, step 7678/23838 completed (loss: 0.4987533986568451, acc: 0.8269230723381042)
[2025-02-16 12:23:10,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:10,822][root][INFO] - Training Epoch: 1/2, step 7679/23838 completed (loss: 0.25494614243507385, acc: 0.9179104566574097)
[2025-02-16 12:23:11,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:11,298][root][INFO] - Training Epoch: 1/2, step 7680/23838 completed (loss: 0.10080209374427795, acc: 0.9722222089767456)
[2025-02-16 12:23:11,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:11,741][root][INFO] - Training Epoch: 1/2, step 7681/23838 completed (loss: 0.3855297863483429, acc: 0.8823529481887817)
[2025-02-16 12:23:11,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:12,236][root][INFO] - Training Epoch: 1/2, step 7682/23838 completed (loss: 0.42247918248176575, acc: 0.8666666746139526)
[2025-02-16 12:23:12,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:12,663][root][INFO] - Training Epoch: 1/2, step 7683/23838 completed (loss: 0.842445969581604, acc: 0.75)
[2025-02-16 12:23:12,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:13,077][root][INFO] - Training Epoch: 1/2, step 7684/23838 completed (loss: 0.4617152214050293, acc: 0.8611111044883728)
[2025-02-16 12:23:13,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:13,457][root][INFO] - Training Epoch: 1/2, step 7685/23838 completed (loss: 0.3337174355983734, acc: 0.915032684803009)
[2025-02-16 12:23:13,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:13,912][root][INFO] - Training Epoch: 1/2, step 7686/23838 completed (loss: 0.24641574919223785, acc: 0.929411768913269)
[2025-02-16 12:23:14,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:14,433][root][INFO] - Training Epoch: 1/2, step 7687/23838 completed (loss: 0.7040709257125854, acc: 0.7956204414367676)
[2025-02-16 12:23:14,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:15,239][root][INFO] - Training Epoch: 1/2, step 7688/23838 completed (loss: 0.2521011233329773, acc: 0.929411768913269)
[2025-02-16 12:23:15,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:15,693][root][INFO] - Training Epoch: 1/2, step 7689/23838 completed (loss: 0.2446068376302719, acc: 0.9189189076423645)
[2025-02-16 12:23:15,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:16,188][root][INFO] - Training Epoch: 1/2, step 7690/23838 completed (loss: 0.43654385209083557, acc: 0.875)
[2025-02-16 12:23:16,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:16,578][root][INFO] - Training Epoch: 1/2, step 7691/23838 completed (loss: 0.14435753226280212, acc: 0.9655172228813171)
[2025-02-16 12:23:16,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:17,002][root][INFO] - Training Epoch: 1/2, step 7692/23838 completed (loss: 0.4012948274612427, acc: 0.9120879173278809)
[2025-02-16 12:23:17,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:17,448][root][INFO] - Training Epoch: 1/2, step 7693/23838 completed (loss: 0.588542103767395, acc: 0.8518518805503845)
[2025-02-16 12:23:17,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:18,041][root][INFO] - Training Epoch: 1/2, step 7694/23838 completed (loss: 0.29213374853134155, acc: 0.9378530979156494)
[2025-02-16 12:23:18,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:18,501][root][INFO] - Training Epoch: 1/2, step 7695/23838 completed (loss: 0.1911660134792328, acc: 0.9462365508079529)
[2025-02-16 12:23:18,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:18,903][root][INFO] - Training Epoch: 1/2, step 7696/23838 completed (loss: 0.4764745831489563, acc: 0.8545454740524292)
[2025-02-16 12:23:19,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:19,276][root][INFO] - Training Epoch: 1/2, step 7697/23838 completed (loss: 0.3880162537097931, acc: 0.8977272510528564)
[2025-02-16 12:23:19,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:19,658][root][INFO] - Training Epoch: 1/2, step 7698/23838 completed (loss: 0.5750970840454102, acc: 0.875)
[2025-02-16 12:23:19,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:20,055][root][INFO] - Training Epoch: 1/2, step 7699/23838 completed (loss: 0.4231397807598114, acc: 0.8888888955116272)
[2025-02-16 12:23:20,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:20,430][root][INFO] - Training Epoch: 1/2, step 7700/23838 completed (loss: 0.7120755314826965, acc: 0.8072289228439331)
[2025-02-16 12:23:20,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:20,814][root][INFO] - Training Epoch: 1/2, step 7701/23838 completed (loss: 0.21885208785533905, acc: 0.9626168012619019)
[2025-02-16 12:23:21,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:21,312][root][INFO] - Training Epoch: 1/2, step 7702/23838 completed (loss: 0.2736184298992157, acc: 0.9379844665527344)
[2025-02-16 12:23:21,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:21,729][root][INFO] - Training Epoch: 1/2, step 7703/23838 completed (loss: 0.573554515838623, acc: 0.8536585569381714)
[2025-02-16 12:23:21,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:22,142][root][INFO] - Training Epoch: 1/2, step 7704/23838 completed (loss: 0.4034735858440399, acc: 0.8861788511276245)
[2025-02-16 12:23:22,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:22,574][root][INFO] - Training Epoch: 1/2, step 7705/23838 completed (loss: 0.38421934843063354, acc: 0.9067796468734741)
[2025-02-16 12:23:22,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:23,028][root][INFO] - Training Epoch: 1/2, step 7706/23838 completed (loss: 0.5298806428909302, acc: 0.8142856955528259)
[2025-02-16 12:23:23,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:23,466][root][INFO] - Training Epoch: 1/2, step 7707/23838 completed (loss: 0.5365737080574036, acc: 0.8636363744735718)
[2025-02-16 12:23:23,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:23,848][root][INFO] - Training Epoch: 1/2, step 7708/23838 completed (loss: 0.5025908946990967, acc: 0.8220338821411133)
[2025-02-16 12:23:24,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:24,252][root][INFO] - Training Epoch: 1/2, step 7709/23838 completed (loss: 0.26089537143707275, acc: 0.9354838728904724)
[2025-02-16 12:23:24,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:24,696][root][INFO] - Training Epoch: 1/2, step 7710/23838 completed (loss: 0.28724437952041626, acc: 0.9240506291389465)
[2025-02-16 12:23:24,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:25,078][root][INFO] - Training Epoch: 1/2, step 7711/23838 completed (loss: 0.41642728447914124, acc: 0.8928571343421936)
[2025-02-16 12:23:25,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:25,507][root][INFO] - Training Epoch: 1/2, step 7712/23838 completed (loss: 0.2908269166946411, acc: 0.9111111164093018)
[2025-02-16 12:23:25,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:25,957][root][INFO] - Training Epoch: 1/2, step 7713/23838 completed (loss: 0.4279496967792511, acc: 0.8941176533699036)
[2025-02-16 12:23:26,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:26,361][root][INFO] - Training Epoch: 1/2, step 7714/23838 completed (loss: 0.4750048816204071, acc: 0.8690476417541504)
[2025-02-16 12:23:26,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:26,762][root][INFO] - Training Epoch: 1/2, step 7715/23838 completed (loss: 0.4505566656589508, acc: 0.8282828330993652)
[2025-02-16 12:23:26,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:27,180][root][INFO] - Training Epoch: 1/2, step 7716/23838 completed (loss: 0.47220584750175476, acc: 0.8272727131843567)
[2025-02-16 12:23:27,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:27,554][root][INFO] - Training Epoch: 1/2, step 7717/23838 completed (loss: 0.8177638053894043, acc: 0.7647058963775635)
[2025-02-16 12:23:27,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:27,998][root][INFO] - Training Epoch: 1/2, step 7718/23838 completed (loss: 0.5164575576782227, acc: 0.8571428656578064)
[2025-02-16 12:23:28,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:28,420][root][INFO] - Training Epoch: 1/2, step 7719/23838 completed (loss: 0.37722963094711304, acc: 0.8863636255264282)
[2025-02-16 12:23:28,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:28,799][root][INFO] - Training Epoch: 1/2, step 7720/23838 completed (loss: 0.46274253726005554, acc: 0.8644067645072937)
[2025-02-16 12:23:28,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:29,229][root][INFO] - Training Epoch: 1/2, step 7721/23838 completed (loss: 0.6616371870040894, acc: 0.8073394298553467)
[2025-02-16 12:23:29,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:29,737][root][INFO] - Training Epoch: 1/2, step 7722/23838 completed (loss: 0.17490732669830322, acc: 0.9465649127960205)
[2025-02-16 12:23:29,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:30,141][root][INFO] - Training Epoch: 1/2, step 7723/23838 completed (loss: 0.6110644936561584, acc: 0.8454545736312866)
[2025-02-16 12:23:30,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:30,747][root][INFO] - Training Epoch: 1/2, step 7724/23838 completed (loss: 0.473105251789093, acc: 0.9007092118263245)
[2025-02-16 12:23:30,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:31,201][root][INFO] - Training Epoch: 1/2, step 7725/23838 completed (loss: 0.45686063170433044, acc: 0.8513513803482056)
[2025-02-16 12:23:31,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:31,619][root][INFO] - Training Epoch: 1/2, step 7726/23838 completed (loss: 1.1351392269134521, acc: 0.6438356041908264)
[2025-02-16 12:23:31,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:32,052][root][INFO] - Training Epoch: 1/2, step 7727/23838 completed (loss: 0.8019775748252869, acc: 0.72826087474823)
[2025-02-16 12:23:32,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:32,472][root][INFO] - Training Epoch: 1/2, step 7728/23838 completed (loss: 0.5649700164794922, acc: 0.7758620977401733)
[2025-02-16 12:23:32,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:32,891][root][INFO] - Training Epoch: 1/2, step 7729/23838 completed (loss: 0.8915307521820068, acc: 0.7788461446762085)
[2025-02-16 12:23:33,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:33,304][root][INFO] - Training Epoch: 1/2, step 7730/23838 completed (loss: 0.4873233437538147, acc: 0.8360655903816223)
[2025-02-16 12:23:33,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:33,729][root][INFO] - Training Epoch: 1/2, step 7731/23838 completed (loss: 0.16389738023281097, acc: 0.9126213788986206)
[2025-02-16 12:23:33,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:34,193][root][INFO] - Training Epoch: 1/2, step 7732/23838 completed (loss: 0.41886240243911743, acc: 0.875)
[2025-02-16 12:23:34,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:34,868][root][INFO] - Training Epoch: 1/2, step 7733/23838 completed (loss: 0.4354532063007355, acc: 0.8703703880310059)
[2025-02-16 12:23:35,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:35,379][root][INFO] - Training Epoch: 1/2, step 7734/23838 completed (loss: 0.28917786478996277, acc: 0.926174521446228)
[2025-02-16 12:23:35,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:35,746][root][INFO] - Training Epoch: 1/2, step 7735/23838 completed (loss: 0.7924094200134277, acc: 0.75)
[2025-02-16 12:23:35,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:36,173][root][INFO] - Training Epoch: 1/2, step 7736/23838 completed (loss: 0.40194258093833923, acc: 0.8839285969734192)
[2025-02-16 12:23:36,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:36,617][root][INFO] - Training Epoch: 1/2, step 7737/23838 completed (loss: 0.5644959211349487, acc: 0.8440366983413696)
[2025-02-16 12:23:36,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:37,068][root][INFO] - Training Epoch: 1/2, step 7738/23838 completed (loss: 0.3843217194080353, acc: 0.8909090757369995)
[2025-02-16 12:23:37,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:37,529][root][INFO] - Training Epoch: 1/2, step 7739/23838 completed (loss: 0.8320572376251221, acc: 0.8095238208770752)
[2025-02-16 12:23:37,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:37,948][root][INFO] - Training Epoch: 1/2, step 7740/23838 completed (loss: 0.7233036756515503, acc: 0.8269230723381042)
[2025-02-16 12:23:38,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:38,386][root][INFO] - Training Epoch: 1/2, step 7741/23838 completed (loss: 0.323816180229187, acc: 0.8837209343910217)
[2025-02-16 12:23:38,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:38,855][root][INFO] - Training Epoch: 1/2, step 7742/23838 completed (loss: 0.46155256032943726, acc: 0.8461538553237915)
[2025-02-16 12:23:39,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:39,277][root][INFO] - Training Epoch: 1/2, step 7743/23838 completed (loss: 0.3101905584335327, acc: 0.886227548122406)
[2025-02-16 12:23:39,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:39,697][root][INFO] - Training Epoch: 1/2, step 7744/23838 completed (loss: 0.8492639064788818, acc: 0.8030303120613098)
[2025-02-16 12:23:39,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:40,088][root][INFO] - Training Epoch: 1/2, step 7745/23838 completed (loss: 0.5518975257873535, acc: 0.868852436542511)
[2025-02-16 12:23:40,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:40,519][root][INFO] - Training Epoch: 1/2, step 7746/23838 completed (loss: 1.1840900182724, acc: 0.6625000238418579)
[2025-02-16 12:23:40,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:40,983][root][INFO] - Training Epoch: 1/2, step 7747/23838 completed (loss: 0.46629250049591064, acc: 0.8351648449897766)
[2025-02-16 12:23:41,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:41,392][root][INFO] - Training Epoch: 1/2, step 7748/23838 completed (loss: 0.5659317970275879, acc: 0.8426966071128845)
[2025-02-16 12:23:41,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:41,904][root][INFO] - Training Epoch: 1/2, step 7749/23838 completed (loss: 0.30790042877197266, acc: 0.9037036895751953)
[2025-02-16 12:23:42,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:42,426][root][INFO] - Training Epoch: 1/2, step 7750/23838 completed (loss: 0.4259302020072937, acc: 0.8777777552604675)
[2025-02-16 12:23:42,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:42,893][root][INFO] - Training Epoch: 1/2, step 7751/23838 completed (loss: 0.5559362769126892, acc: 0.8380281925201416)
[2025-02-16 12:23:43,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:43,300][root][INFO] - Training Epoch: 1/2, step 7752/23838 completed (loss: 0.36887654662132263, acc: 0.9054054021835327)
[2025-02-16 12:23:43,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:43,754][root][INFO] - Training Epoch: 1/2, step 7753/23838 completed (loss: 0.2707155644893646, acc: 0.8742514848709106)
[2025-02-16 12:23:43,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:44,182][root][INFO] - Training Epoch: 1/2, step 7754/23838 completed (loss: 0.5451143980026245, acc: 0.84375)
[2025-02-16 12:23:44,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:44,570][root][INFO] - Training Epoch: 1/2, step 7755/23838 completed (loss: 0.31809279322624207, acc: 0.9241379499435425)
[2025-02-16 12:23:44,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:44,995][root][INFO] - Training Epoch: 1/2, step 7756/23838 completed (loss: 0.7287546992301941, acc: 0.7634408473968506)
[2025-02-16 12:23:45,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:45,449][root][INFO] - Training Epoch: 1/2, step 7757/23838 completed (loss: 0.4082487225532532, acc: 0.8636363744735718)
[2025-02-16 12:23:45,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:45,885][root][INFO] - Training Epoch: 1/2, step 7758/23838 completed (loss: 0.5462892055511475, acc: 0.8584070801734924)
[2025-02-16 12:23:46,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:46,323][root][INFO] - Training Epoch: 1/2, step 7759/23838 completed (loss: 0.7062732577323914, acc: 0.7971014380455017)
[2025-02-16 12:23:46,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:46,783][root][INFO] - Training Epoch: 1/2, step 7760/23838 completed (loss: 0.37883055210113525, acc: 0.925000011920929)
[2025-02-16 12:23:46,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:47,204][root][INFO] - Training Epoch: 1/2, step 7761/23838 completed (loss: 0.49085715413093567, acc: 0.8454545736312866)
[2025-02-16 12:23:47,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:47,640][root][INFO] - Training Epoch: 1/2, step 7762/23838 completed (loss: 0.3471023440361023, acc: 0.9019607901573181)
[2025-02-16 12:23:47,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:48,061][root][INFO] - Training Epoch: 1/2, step 7763/23838 completed (loss: 0.3676277995109558, acc: 0.9019607901573181)
[2025-02-16 12:23:48,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:48,463][root][INFO] - Training Epoch: 1/2, step 7764/23838 completed (loss: 0.596258819103241, acc: 0.8194444179534912)
[2025-02-16 12:23:48,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:48,889][root][INFO] - Training Epoch: 1/2, step 7765/23838 completed (loss: 0.5616235136985779, acc: 0.8548387289047241)
[2025-02-16 12:23:49,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:49,330][root][INFO] - Training Epoch: 1/2, step 7766/23838 completed (loss: 0.3339870274066925, acc: 0.9056603908538818)
[2025-02-16 12:23:49,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:49,851][root][INFO] - Training Epoch: 1/2, step 7767/23838 completed (loss: 0.5527676939964294, acc: 0.8602150678634644)
[2025-02-16 12:23:50,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:50,660][root][INFO] - Training Epoch: 1/2, step 7768/23838 completed (loss: 0.3026833236217499, acc: 0.8985507488250732)
[2025-02-16 12:23:51,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:51,371][root][INFO] - Training Epoch: 1/2, step 7769/23838 completed (loss: 0.26836097240448, acc: 0.9215686321258545)
[2025-02-16 12:23:51,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:51,783][root][INFO] - Training Epoch: 1/2, step 7770/23838 completed (loss: 0.553986668586731, acc: 0.8484848737716675)
[2025-02-16 12:23:52,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:52,302][root][INFO] - Training Epoch: 1/2, step 7771/23838 completed (loss: 0.9161044359207153, acc: 0.7377049326896667)
[2025-02-16 12:23:52,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:52,710][root][INFO] - Training Epoch: 1/2, step 7772/23838 completed (loss: 0.8978176116943359, acc: 0.7701149582862854)
[2025-02-16 12:23:52,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:53,076][root][INFO] - Training Epoch: 1/2, step 7773/23838 completed (loss: 0.3445659875869751, acc: 0.9067796468734741)
[2025-02-16 12:23:53,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:53,451][root][INFO] - Training Epoch: 1/2, step 7774/23838 completed (loss: 0.5107678771018982, acc: 0.8659793734550476)
[2025-02-16 12:23:53,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:53,931][root][INFO] - Training Epoch: 1/2, step 7775/23838 completed (loss: 0.2483927458524704, acc: 0.93388432264328)
[2025-02-16 12:23:54,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:54,376][root][INFO] - Training Epoch: 1/2, step 7776/23838 completed (loss: 0.5989896059036255, acc: 0.8367347121238708)
[2025-02-16 12:23:54,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:54,842][root][INFO] - Training Epoch: 1/2, step 7777/23838 completed (loss: 0.38576534390449524, acc: 0.9175257682800293)
[2025-02-16 12:23:55,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:55,322][root][INFO] - Training Epoch: 1/2, step 7778/23838 completed (loss: 0.3045802116394043, acc: 0.9057971239089966)
[2025-02-16 12:23:55,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:55,773][root][INFO] - Training Epoch: 1/2, step 7779/23838 completed (loss: 0.6676739454269409, acc: 0.7980769276618958)
[2025-02-16 12:23:55,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:56,225][root][INFO] - Training Epoch: 1/2, step 7780/23838 completed (loss: 1.1287782192230225, acc: 0.6639344096183777)
[2025-02-16 12:23:56,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:56,621][root][INFO] - Training Epoch: 1/2, step 7781/23838 completed (loss: 0.5978495478630066, acc: 0.8333333134651184)
[2025-02-16 12:23:56,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:57,038][root][INFO] - Training Epoch: 1/2, step 7782/23838 completed (loss: 0.8888478875160217, acc: 0.7377049326896667)
[2025-02-16 12:23:57,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:57,431][root][INFO] - Training Epoch: 1/2, step 7783/23838 completed (loss: 0.4725642502307892, acc: 0.8666666746139526)
[2025-02-16 12:23:57,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:57,900][root][INFO] - Training Epoch: 1/2, step 7784/23838 completed (loss: 0.1722206473350525, acc: 0.935251772403717)
[2025-02-16 12:23:58,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:58,281][root][INFO] - Training Epoch: 1/2, step 7785/23838 completed (loss: 0.2658737897872925, acc: 0.9245283007621765)
[2025-02-16 12:23:58,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:58,719][root][INFO] - Training Epoch: 1/2, step 7786/23838 completed (loss: 0.44273504614830017, acc: 0.9017857313156128)
[2025-02-16 12:23:58,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:59,099][root][INFO] - Training Epoch: 1/2, step 7787/23838 completed (loss: 0.2419997900724411, acc: 0.9305555820465088)
[2025-02-16 12:23:59,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:59,574][root][INFO] - Training Epoch: 1/2, step 7788/23838 completed (loss: 0.4119965434074402, acc: 0.8735632300376892)
[2025-02-16 12:23:59,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:23:59,995][root][INFO] - Training Epoch: 1/2, step 7789/23838 completed (loss: 0.5804501175880432, acc: 0.8461538553237915)
[2025-02-16 12:24:00,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:00,391][root][INFO] - Training Epoch: 1/2, step 7790/23838 completed (loss: 0.43914538621902466, acc: 0.8780487775802612)
[2025-02-16 12:24:00,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:00,838][root][INFO] - Training Epoch: 1/2, step 7791/23838 completed (loss: 0.36289742588996887, acc: 0.914893627166748)
[2025-02-16 12:24:01,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:01,282][root][INFO] - Training Epoch: 1/2, step 7792/23838 completed (loss: 0.736171543598175, acc: 0.8100000023841858)
[2025-02-16 12:24:01,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:01,681][root][INFO] - Training Epoch: 1/2, step 7793/23838 completed (loss: 0.5424398183822632, acc: 0.8306451439857483)
[2025-02-16 12:24:01,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:02,116][root][INFO] - Training Epoch: 1/2, step 7794/23838 completed (loss: 0.6766029596328735, acc: 0.7941176295280457)
[2025-02-16 12:24:02,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:02,565][root][INFO] - Training Epoch: 1/2, step 7795/23838 completed (loss: 0.40809741616249084, acc: 0.9034482836723328)
[2025-02-16 12:24:02,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:03,011][root][INFO] - Training Epoch: 1/2, step 7796/23838 completed (loss: 0.5000984072685242, acc: 0.8354430198669434)
[2025-02-16 12:24:03,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:03,407][root][INFO] - Training Epoch: 1/2, step 7797/23838 completed (loss: 0.22377899289131165, acc: 0.931034505367279)
[2025-02-16 12:24:03,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:03,854][root][INFO] - Training Epoch: 1/2, step 7798/23838 completed (loss: 1.04930579662323, acc: 0.692307710647583)
[2025-02-16 12:24:04,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:04,327][root][INFO] - Training Epoch: 1/2, step 7799/23838 completed (loss: 0.2298857718706131, acc: 0.945652186870575)
[2025-02-16 12:24:04,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:04,762][root][INFO] - Training Epoch: 1/2, step 7800/23838 completed (loss: 0.4344848394393921, acc: 0.8898305296897888)
[2025-02-16 12:24:04,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:05,168][root][INFO] - Training Epoch: 1/2, step 7801/23838 completed (loss: 0.2637881934642792, acc: 0.9318181872367859)
[2025-02-16 12:24:05,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:05,553][root][INFO] - Training Epoch: 1/2, step 7802/23838 completed (loss: 0.5519415140151978, acc: 0.8064516186714172)
[2025-02-16 12:24:05,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:06,004][root][INFO] - Training Epoch: 1/2, step 7803/23838 completed (loss: 0.5558058619499207, acc: 0.8526315689086914)
[2025-02-16 12:24:06,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:06,440][root][INFO] - Training Epoch: 1/2, step 7804/23838 completed (loss: 0.6407114267349243, acc: 0.8037382960319519)
[2025-02-16 12:24:06,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:06,894][root][INFO] - Training Epoch: 1/2, step 7805/23838 completed (loss: 0.23104019463062286, acc: 0.931034505367279)
[2025-02-16 12:24:07,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:07,279][root][INFO] - Training Epoch: 1/2, step 7806/23838 completed (loss: 0.2618999779224396, acc: 0.8971962332725525)
[2025-02-16 12:24:07,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:07,673][root][INFO] - Training Epoch: 1/2, step 7807/23838 completed (loss: 0.21790820360183716, acc: 0.9479166865348816)
[2025-02-16 12:24:07,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:08,141][root][INFO] - Training Epoch: 1/2, step 7808/23838 completed (loss: 0.19157882034778595, acc: 0.9375)
[2025-02-16 12:24:08,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:08,739][root][INFO] - Training Epoch: 1/2, step 7809/23838 completed (loss: 0.16595330834388733, acc: 0.9420289993286133)
[2025-02-16 12:24:09,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:09,339][root][INFO] - Training Epoch: 1/2, step 7810/23838 completed (loss: 0.533592164516449, acc: 0.8399999737739563)
[2025-02-16 12:24:09,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:09,791][root][INFO] - Training Epoch: 1/2, step 7811/23838 completed (loss: 0.18142680823802948, acc: 0.9576271176338196)
[2025-02-16 12:24:09,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:10,218][root][INFO] - Training Epoch: 1/2, step 7812/23838 completed (loss: 0.5397422909736633, acc: 0.8727272748947144)
[2025-02-16 12:24:10,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:10,788][root][INFO] - Training Epoch: 1/2, step 7813/23838 completed (loss: 0.4369180202484131, acc: 0.8695651888847351)
[2025-02-16 12:24:10,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:11,255][root][INFO] - Training Epoch: 1/2, step 7814/23838 completed (loss: 0.21496902406215668, acc: 0.9466666579246521)
[2025-02-16 12:24:11,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:11,782][root][INFO] - Training Epoch: 1/2, step 7815/23838 completed (loss: 0.4651808440685272, acc: 0.8666666746139526)
[2025-02-16 12:24:11,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:12,222][root][INFO] - Training Epoch: 1/2, step 7816/23838 completed (loss: 0.44068145751953125, acc: 0.8666666746139526)
[2025-02-16 12:24:12,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:12,649][root][INFO] - Training Epoch: 1/2, step 7817/23838 completed (loss: 0.20245076715946198, acc: 0.9240506291389465)
[2025-02-16 12:24:12,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:13,232][root][INFO] - Training Epoch: 1/2, step 7818/23838 completed (loss: 0.32602035999298096, acc: 0.9097222089767456)
[2025-02-16 12:24:13,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:13,707][root][INFO] - Training Epoch: 1/2, step 7819/23838 completed (loss: 0.3919413089752197, acc: 0.800000011920929)
[2025-02-16 12:24:13,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:14,088][root][INFO] - Training Epoch: 1/2, step 7820/23838 completed (loss: 0.6136116981506348, acc: 0.868686854839325)
[2025-02-16 12:24:14,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:14,444][root][INFO] - Training Epoch: 1/2, step 7821/23838 completed (loss: 0.1756667047739029, acc: 0.9438202381134033)
[2025-02-16 12:24:14,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:14,830][root][INFO] - Training Epoch: 1/2, step 7822/23838 completed (loss: 0.16027922928333282, acc: 0.9557521939277649)
[2025-02-16 12:24:14,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:15,206][root][INFO] - Training Epoch: 1/2, step 7823/23838 completed (loss: 0.3532436192035675, acc: 0.9117646813392639)
[2025-02-16 12:24:15,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:15,707][root][INFO] - Training Epoch: 1/2, step 7824/23838 completed (loss: 0.5013567805290222, acc: 0.8601398468017578)
[2025-02-16 12:24:15,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:16,091][root][INFO] - Training Epoch: 1/2, step 7825/23838 completed (loss: 0.5040073394775391, acc: 0.8734177350997925)
[2025-02-16 12:24:16,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:16,551][root][INFO] - Training Epoch: 1/2, step 7826/23838 completed (loss: 0.4431823492050171, acc: 0.8636363744735718)
[2025-02-16 12:24:16,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:17,029][root][INFO] - Training Epoch: 1/2, step 7827/23838 completed (loss: 0.13570648431777954, acc: 0.9834710955619812)
[2025-02-16 12:24:17,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:17,501][root][INFO] - Training Epoch: 1/2, step 7828/23838 completed (loss: 0.32572299242019653, acc: 0.8962264060974121)
[2025-02-16 12:24:17,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:17,961][root][INFO] - Training Epoch: 1/2, step 7829/23838 completed (loss: 0.23476660251617432, acc: 0.908450722694397)
[2025-02-16 12:24:18,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:18,329][root][INFO] - Training Epoch: 1/2, step 7830/23838 completed (loss: 0.27679193019866943, acc: 0.8888888955116272)
[2025-02-16 12:24:18,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:19,247][root][INFO] - Training Epoch: 1/2, step 7831/23838 completed (loss: 0.5668435096740723, acc: 0.8642857074737549)
[2025-02-16 12:24:19,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:19,670][root][INFO] - Training Epoch: 1/2, step 7832/23838 completed (loss: 0.43608927726745605, acc: 0.8405796885490417)
[2025-02-16 12:24:19,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:20,115][root][INFO] - Training Epoch: 1/2, step 7833/23838 completed (loss: 0.5491803288459778, acc: 0.8687499761581421)
[2025-02-16 12:24:20,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:20,662][root][INFO] - Training Epoch: 1/2, step 7834/23838 completed (loss: 0.6402028203010559, acc: 0.8389261960983276)
[2025-02-16 12:24:20,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:21,094][root][INFO] - Training Epoch: 1/2, step 7835/23838 completed (loss: 0.1265910565853119, acc: 0.9759036302566528)
[2025-02-16 12:24:21,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:21,569][root][INFO] - Training Epoch: 1/2, step 7836/23838 completed (loss: 0.45724284648895264, acc: 0.8677685856819153)
[2025-02-16 12:24:21,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:22,103][root][INFO] - Training Epoch: 1/2, step 7837/23838 completed (loss: 0.13779379427433014, acc: 0.9685534834861755)
[2025-02-16 12:24:22,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:22,538][root][INFO] - Training Epoch: 1/2, step 7838/23838 completed (loss: 0.4748576581478119, acc: 0.8857142925262451)
[2025-02-16 12:24:22,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:23,089][root][INFO] - Training Epoch: 1/2, step 7839/23838 completed (loss: 0.3865213096141815, acc: 0.8928571343421936)
[2025-02-16 12:24:23,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:23,590][root][INFO] - Training Epoch: 1/2, step 7840/23838 completed (loss: 0.3025466501712799, acc: 0.9059829115867615)
[2025-02-16 12:24:23,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:24,053][root][INFO] - Training Epoch: 1/2, step 7841/23838 completed (loss: 0.44495031237602234, acc: 0.8933333158493042)
[2025-02-16 12:24:24,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:24,440][root][INFO] - Training Epoch: 1/2, step 7842/23838 completed (loss: 0.5443302392959595, acc: 0.8607594966888428)
[2025-02-16 12:24:24,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:24,839][root][INFO] - Training Epoch: 1/2, step 7843/23838 completed (loss: 0.8683322072029114, acc: 0.7924528121948242)
[2025-02-16 12:24:25,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:25,304][root][INFO] - Training Epoch: 1/2, step 7844/23838 completed (loss: 0.39927294850349426, acc: 0.9239130616188049)
[2025-02-16 12:24:25,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:25,716][root][INFO] - Training Epoch: 1/2, step 7845/23838 completed (loss: 0.1349303424358368, acc: 0.9718309640884399)
[2025-02-16 12:24:25,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:26,140][root][INFO] - Training Epoch: 1/2, step 7846/23838 completed (loss: 0.5081380009651184, acc: 0.8888888955116272)
[2025-02-16 12:24:26,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:26,528][root][INFO] - Training Epoch: 1/2, step 7847/23838 completed (loss: 0.3094356060028076, acc: 0.9166666865348816)
[2025-02-16 12:24:26,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:26,963][root][INFO] - Training Epoch: 1/2, step 7848/23838 completed (loss: 0.12500493228435516, acc: 0.9637681245803833)
[2025-02-16 12:24:27,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:27,357][root][INFO] - Training Epoch: 1/2, step 7849/23838 completed (loss: 0.17545975744724274, acc: 0.9428571462631226)
[2025-02-16 12:24:27,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:27,846][root][INFO] - Training Epoch: 1/2, step 7850/23838 completed (loss: 0.090008445084095, acc: 0.9516128897666931)
[2025-02-16 12:24:28,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:28,386][root][INFO] - Training Epoch: 1/2, step 7851/23838 completed (loss: 0.2718479335308075, acc: 0.9256756901741028)
[2025-02-16 12:24:28,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:28,849][root][INFO] - Training Epoch: 1/2, step 7852/23838 completed (loss: 0.4317784607410431, acc: 0.8661971688270569)
[2025-02-16 12:24:29,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:29,307][root][INFO] - Training Epoch: 1/2, step 7853/23838 completed (loss: 0.36318325996398926, acc: 0.8983050584793091)
[2025-02-16 12:24:29,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:29,754][root][INFO] - Training Epoch: 1/2, step 7854/23838 completed (loss: 0.4450397789478302, acc: 0.8484848737716675)
[2025-02-16 12:24:29,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:30,163][root][INFO] - Training Epoch: 1/2, step 7855/23838 completed (loss: 0.4045954644680023, acc: 0.8433734774589539)
[2025-02-16 12:24:30,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:30,516][root][INFO] - Training Epoch: 1/2, step 7856/23838 completed (loss: 0.41119271516799927, acc: 0.8852459192276001)
[2025-02-16 12:24:30,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:30,887][root][INFO] - Training Epoch: 1/2, step 7857/23838 completed (loss: 0.6070128083229065, acc: 0.7954545617103577)
[2025-02-16 12:24:31,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:31,251][root][INFO] - Training Epoch: 1/2, step 7858/23838 completed (loss: 0.5326628088951111, acc: 0.8235294222831726)
[2025-02-16 12:24:31,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:31,693][root][INFO] - Training Epoch: 1/2, step 7859/23838 completed (loss: 0.5074908137321472, acc: 0.8372092843055725)
[2025-02-16 12:24:31,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:32,141][root][INFO] - Training Epoch: 1/2, step 7860/23838 completed (loss: 0.5501789450645447, acc: 0.8235294222831726)
[2025-02-16 12:24:32,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:32,601][root][INFO] - Training Epoch: 1/2, step 7861/23838 completed (loss: 0.35114672780036926, acc: 0.8983050584793091)
[2025-02-16 12:24:32,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:33,230][root][INFO] - Training Epoch: 1/2, step 7862/23838 completed (loss: 0.5529155135154724, acc: 0.875)
[2025-02-16 12:24:33,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:33,703][root][INFO] - Training Epoch: 1/2, step 7863/23838 completed (loss: 0.5703710913658142, acc: 0.8333333134651184)
[2025-02-16 12:24:33,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:34,178][root][INFO] - Training Epoch: 1/2, step 7864/23838 completed (loss: 0.3802477717399597, acc: 0.9012345671653748)
[2025-02-16 12:24:34,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:35,093][root][INFO] - Training Epoch: 1/2, step 7865/23838 completed (loss: 0.5498210191726685, acc: 0.8316831588745117)
[2025-02-16 12:24:35,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:35,515][root][INFO] - Training Epoch: 1/2, step 7866/23838 completed (loss: 0.43788257241249084, acc: 0.8571428656578064)
[2025-02-16 12:24:35,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:35,958][root][INFO] - Training Epoch: 1/2, step 7867/23838 completed (loss: 0.236493319272995, acc: 0.9054054021835327)
[2025-02-16 12:24:36,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:36,341][root][INFO] - Training Epoch: 1/2, step 7868/23838 completed (loss: 0.6281378269195557, acc: 0.8192771077156067)
[2025-02-16 12:24:36,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:36,783][root][INFO] - Training Epoch: 1/2, step 7869/23838 completed (loss: 0.5601592659950256, acc: 0.8148148059844971)
[2025-02-16 12:24:37,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:37,295][root][INFO] - Training Epoch: 1/2, step 7870/23838 completed (loss: 0.6599290370941162, acc: 0.8382353186607361)
[2025-02-16 12:24:37,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:37,831][root][INFO] - Training Epoch: 1/2, step 7871/23838 completed (loss: 0.25004255771636963, acc: 0.8999999761581421)
[2025-02-16 12:24:38,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:38,291][root][INFO] - Training Epoch: 1/2, step 7872/23838 completed (loss: 0.6067941784858704, acc: 0.8591549396514893)
[2025-02-16 12:24:38,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:38,784][root][INFO] - Training Epoch: 1/2, step 7873/23838 completed (loss: 0.24464398622512817, acc: 0.9298245906829834)
[2025-02-16 12:24:38,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:39,213][root][INFO] - Training Epoch: 1/2, step 7874/23838 completed (loss: 0.4987729489803314, acc: 0.8679245114326477)
[2025-02-16 12:24:39,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:39,660][root][INFO] - Training Epoch: 1/2, step 7875/23838 completed (loss: 1.061247706413269, acc: 0.688524603843689)
[2025-02-16 12:24:39,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:40,124][root][INFO] - Training Epoch: 1/2, step 7876/23838 completed (loss: 1.0023106336593628, acc: 0.7692307829856873)
[2025-02-16 12:24:40,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:40,605][root][INFO] - Training Epoch: 1/2, step 7877/23838 completed (loss: 0.2232615053653717, acc: 0.954285740852356)
[2025-02-16 12:24:40,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:41,156][root][INFO] - Training Epoch: 1/2, step 7878/23838 completed (loss: 0.3067564368247986, acc: 0.905063271522522)
[2025-02-16 12:24:41,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:41,638][root][INFO] - Training Epoch: 1/2, step 7879/23838 completed (loss: 0.26860445737838745, acc: 0.9247311949729919)
[2025-02-16 12:24:41,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:42,094][root][INFO] - Training Epoch: 1/2, step 7880/23838 completed (loss: 1.1812782287597656, acc: 0.7407407164573669)
[2025-02-16 12:24:42,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:42,662][root][INFO] - Training Epoch: 1/2, step 7881/23838 completed (loss: 0.22394752502441406, acc: 0.942307710647583)
[2025-02-16 12:24:42,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:43,232][root][INFO] - Training Epoch: 1/2, step 7882/23838 completed (loss: 0.33339086174964905, acc: 0.8991596698760986)
[2025-02-16 12:24:43,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:43,706][root][INFO] - Training Epoch: 1/2, step 7883/23838 completed (loss: 0.24327971041202545, acc: 0.9191918969154358)
[2025-02-16 12:24:43,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:44,155][root][INFO] - Training Epoch: 1/2, step 7884/23838 completed (loss: 0.42943426966667175, acc: 0.8518518805503845)
[2025-02-16 12:24:44,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:44,542][root][INFO] - Training Epoch: 1/2, step 7885/23838 completed (loss: 0.5065865516662598, acc: 0.8627451062202454)
[2025-02-16 12:24:44,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:44,923][root][INFO] - Training Epoch: 1/2, step 7886/23838 completed (loss: 0.7614265084266663, acc: 0.8214285969734192)
[2025-02-16 12:24:45,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:45,371][root][INFO] - Training Epoch: 1/2, step 7887/23838 completed (loss: 0.47012466192245483, acc: 0.8611111044883728)
[2025-02-16 12:24:45,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:45,813][root][INFO] - Training Epoch: 1/2, step 7888/23838 completed (loss: 0.9030376076698303, acc: 0.7894737124443054)
[2025-02-16 12:24:46,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:46,259][root][INFO] - Training Epoch: 1/2, step 7889/23838 completed (loss: 0.43151649832725525, acc: 0.8692810535430908)
[2025-02-16 12:24:46,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:46,682][root][INFO] - Training Epoch: 1/2, step 7890/23838 completed (loss: 0.4017024636268616, acc: 0.9125000238418579)
[2025-02-16 12:24:46,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:47,101][root][INFO] - Training Epoch: 1/2, step 7891/23838 completed (loss: 0.44466277956962585, acc: 0.8727272748947144)
[2025-02-16 12:24:47,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:47,577][root][INFO] - Training Epoch: 1/2, step 7892/23838 completed (loss: 0.43354952335357666, acc: 0.893203854560852)
[2025-02-16 12:24:47,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:47,992][root][INFO] - Training Epoch: 1/2, step 7893/23838 completed (loss: 0.28027987480163574, acc: 0.893203854560852)
[2025-02-16 12:24:48,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:48,401][root][INFO] - Training Epoch: 1/2, step 7894/23838 completed (loss: 0.3080924451351166, acc: 0.9125000238418579)
[2025-02-16 12:24:48,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:48,844][root][INFO] - Training Epoch: 1/2, step 7895/23838 completed (loss: 0.4440746307373047, acc: 0.8571428656578064)
[2025-02-16 12:24:49,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:49,285][root][INFO] - Training Epoch: 1/2, step 7896/23838 completed (loss: 0.245267853140831, acc: 0.9438202381134033)
[2025-02-16 12:24:49,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:50,319][root][INFO] - Training Epoch: 1/2, step 7897/23838 completed (loss: 0.5913035869598389, acc: 0.8300653696060181)
[2025-02-16 12:24:50,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:50,769][root][INFO] - Training Epoch: 1/2, step 7898/23838 completed (loss: 0.6724528670310974, acc: 0.8181818127632141)
[2025-02-16 12:24:50,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:51,227][root][INFO] - Training Epoch: 1/2, step 7899/23838 completed (loss: 0.23232826590538025, acc: 0.9220778942108154)
[2025-02-16 12:24:51,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:51,946][root][INFO] - Training Epoch: 1/2, step 7900/23838 completed (loss: 0.4061916768550873, acc: 0.9010416865348816)
[2025-02-16 12:24:52,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:52,457][root][INFO] - Training Epoch: 1/2, step 7901/23838 completed (loss: 0.142377570271492, acc: 0.970370352268219)
[2025-02-16 12:24:52,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:52,834][root][INFO] - Training Epoch: 1/2, step 7902/23838 completed (loss: 0.5695756673812866, acc: 0.8727272748947144)
[2025-02-16 12:24:53,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:53,233][root][INFO] - Training Epoch: 1/2, step 7903/23838 completed (loss: 0.21351121366024017, acc: 0.9629629850387573)
[2025-02-16 12:24:53,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:53,962][root][INFO] - Training Epoch: 1/2, step 7904/23838 completed (loss: 0.3402779996395111, acc: 0.9017341136932373)
[2025-02-16 12:24:54,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:54,330][root][INFO] - Training Epoch: 1/2, step 7905/23838 completed (loss: 0.09719223529100418, acc: 0.9826086759567261)
[2025-02-16 12:24:54,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:54,763][root][INFO] - Training Epoch: 1/2, step 7906/23838 completed (loss: 0.16429965198040009, acc: 0.9333333373069763)
[2025-02-16 12:24:54,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:55,207][root][INFO] - Training Epoch: 1/2, step 7907/23838 completed (loss: 0.9716307520866394, acc: 0.734375)
[2025-02-16 12:24:55,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:55,680][root][INFO] - Training Epoch: 1/2, step 7908/23838 completed (loss: 0.4623592793941498, acc: 0.891566276550293)
[2025-02-16 12:24:55,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:56,138][root][INFO] - Training Epoch: 1/2, step 7909/23838 completed (loss: 0.693393886089325, acc: 0.8048780560493469)
[2025-02-16 12:24:56,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:56,592][root][INFO] - Training Epoch: 1/2, step 7910/23838 completed (loss: 0.20497003197669983, acc: 0.9793814420700073)
[2025-02-16 12:24:56,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:57,012][root][INFO] - Training Epoch: 1/2, step 7911/23838 completed (loss: 0.25887441635131836, acc: 0.9252336621284485)
[2025-02-16 12:24:57,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:57,439][root][INFO] - Training Epoch: 1/2, step 7912/23838 completed (loss: 0.33936262130737305, acc: 0.8936170339584351)
[2025-02-16 12:24:57,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:57,916][root][INFO] - Training Epoch: 1/2, step 7913/23838 completed (loss: 0.5593766570091248, acc: 0.8584070801734924)
[2025-02-16 12:24:58,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:58,401][root][INFO] - Training Epoch: 1/2, step 7914/23838 completed (loss: 0.4297056198120117, acc: 0.8533333539962769)
[2025-02-16 12:24:58,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:58,889][root][INFO] - Training Epoch: 1/2, step 7915/23838 completed (loss: 0.3750913143157959, acc: 0.8899999856948853)
[2025-02-16 12:24:59,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:59,361][root][INFO] - Training Epoch: 1/2, step 7916/23838 completed (loss: 0.22854135930538177, acc: 0.9285714030265808)
[2025-02-16 12:24:59,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:24:59,831][root][INFO] - Training Epoch: 1/2, step 7917/23838 completed (loss: 0.11734367161989212, acc: 0.957446813583374)
[2025-02-16 12:25:00,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:00,318][root][INFO] - Training Epoch: 1/2, step 7918/23838 completed (loss: 0.2556346356868744, acc: 0.9322916865348816)
[2025-02-16 12:25:00,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:00,694][root][INFO] - Training Epoch: 1/2, step 7919/23838 completed (loss: 0.6882496476173401, acc: 0.8039215803146362)
[2025-02-16 12:25:00,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:01,065][root][INFO] - Training Epoch: 1/2, step 7920/23838 completed (loss: 0.39244338870048523, acc: 0.9230769276618958)
[2025-02-16 12:25:01,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:02,020][root][INFO] - Training Epoch: 1/2, step 7921/23838 completed (loss: 0.20696397125720978, acc: 0.9411764740943909)
[2025-02-16 12:25:02,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:02,537][root][INFO] - Training Epoch: 1/2, step 7922/23838 completed (loss: 0.3125186264514923, acc: 0.9008264541625977)
[2025-02-16 12:25:02,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:03,057][root][INFO] - Training Epoch: 1/2, step 7923/23838 completed (loss: 0.283459335565567, acc: 0.9251337051391602)
[2025-02-16 12:25:03,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:03,513][root][INFO] - Training Epoch: 1/2, step 7924/23838 completed (loss: 0.32242900133132935, acc: 0.9047619104385376)
[2025-02-16 12:25:03,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:03,974][root][INFO] - Training Epoch: 1/2, step 7925/23838 completed (loss: 0.5480635762214661, acc: 0.8500000238418579)
[2025-02-16 12:25:04,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:04,489][root][INFO] - Training Epoch: 1/2, step 7926/23838 completed (loss: 0.48590201139450073, acc: 0.8983050584793091)
[2025-02-16 12:25:04,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:05,007][root][INFO] - Training Epoch: 1/2, step 7927/23838 completed (loss: 0.309993714094162, acc: 0.9263157844543457)
[2025-02-16 12:25:05,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:05,874][root][INFO] - Training Epoch: 1/2, step 7928/23838 completed (loss: 0.15399764478206635, acc: 0.96875)
[2025-02-16 12:25:06,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:06,309][root][INFO] - Training Epoch: 1/2, step 7929/23838 completed (loss: 0.19880186021327972, acc: 0.9534883499145508)
[2025-02-16 12:25:06,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:06,755][root][INFO] - Training Epoch: 1/2, step 7930/23838 completed (loss: 0.2802712321281433, acc: 0.9200000166893005)
[2025-02-16 12:25:07,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:07,467][root][INFO] - Training Epoch: 1/2, step 7931/23838 completed (loss: 0.3067820072174072, acc: 0.9137930870056152)
[2025-02-16 12:25:07,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:07,876][root][INFO] - Training Epoch: 1/2, step 7932/23838 completed (loss: 0.2323729693889618, acc: 0.9333333373069763)
[2025-02-16 12:25:08,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:08,283][root][INFO] - Training Epoch: 1/2, step 7933/23838 completed (loss: 0.41086357831954956, acc: 0.8823529481887817)
[2025-02-16 12:25:08,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:08,700][root][INFO] - Training Epoch: 1/2, step 7934/23838 completed (loss: 0.3826666474342346, acc: 0.8901098966598511)
[2025-02-16 12:25:08,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:09,174][root][INFO] - Training Epoch: 1/2, step 7935/23838 completed (loss: 0.1903044581413269, acc: 0.9375)
[2025-02-16 12:25:09,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:09,567][root][INFO] - Training Epoch: 1/2, step 7936/23838 completed (loss: 0.36366963386535645, acc: 0.8666666746139526)
[2025-02-16 12:25:09,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:10,018][root][INFO] - Training Epoch: 1/2, step 7937/23838 completed (loss: 0.15372948348522186, acc: 0.969072163105011)
[2025-02-16 12:25:10,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:10,453][root][INFO] - Training Epoch: 1/2, step 7938/23838 completed (loss: 0.16280412673950195, acc: 0.9642857313156128)
[2025-02-16 12:25:10,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:10,936][root][INFO] - Training Epoch: 1/2, step 7939/23838 completed (loss: 0.43250542879104614, acc: 0.8809523582458496)
[2025-02-16 12:25:11,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:11,413][root][INFO] - Training Epoch: 1/2, step 7940/23838 completed (loss: 0.3946840465068817, acc: 0.8783783912658691)
[2025-02-16 12:25:11,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:11,836][root][INFO] - Training Epoch: 1/2, step 7941/23838 completed (loss: 0.16198745369911194, acc: 0.960629940032959)
[2025-02-16 12:25:12,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:12,312][root][INFO] - Training Epoch: 1/2, step 7942/23838 completed (loss: 0.4467957019805908, acc: 0.8756219148635864)
[2025-02-16 12:25:12,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:12,738][root][INFO] - Training Epoch: 1/2, step 7943/23838 completed (loss: 0.8303084373474121, acc: 0.739130437374115)
[2025-02-16 12:25:12,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:13,221][root][INFO] - Training Epoch: 1/2, step 7944/23838 completed (loss: 0.1829645186662674, acc: 0.9520000219345093)
[2025-02-16 12:25:13,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:13,693][root][INFO] - Training Epoch: 1/2, step 7945/23838 completed (loss: 0.27016010880470276, acc: 0.9189189076423645)
[2025-02-16 12:25:13,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:14,177][root][INFO] - Training Epoch: 1/2, step 7946/23838 completed (loss: 0.4694998860359192, acc: 0.859375)
[2025-02-16 12:25:14,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:14,604][root][INFO] - Training Epoch: 1/2, step 7947/23838 completed (loss: 0.43529683351516724, acc: 0.886956512928009)
[2025-02-16 12:25:14,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:14,985][root][INFO] - Training Epoch: 1/2, step 7948/23838 completed (loss: 0.2735113799571991, acc: 0.9340659379959106)
[2025-02-16 12:25:15,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:15,502][root][INFO] - Training Epoch: 1/2, step 7949/23838 completed (loss: 0.47076329588890076, acc: 0.8757764101028442)
[2025-02-16 12:25:15,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:15,932][root][INFO] - Training Epoch: 1/2, step 7950/23838 completed (loss: 0.14709360897541046, acc: 0.9137930870056152)
[2025-02-16 12:25:16,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:16,367][root][INFO] - Training Epoch: 1/2, step 7951/23838 completed (loss: 0.3445188105106354, acc: 0.9428571462631226)
[2025-02-16 12:25:16,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:16,801][root][INFO] - Training Epoch: 1/2, step 7952/23838 completed (loss: 0.09885720908641815, acc: 0.9684210419654846)
[2025-02-16 12:25:16,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:17,181][root][INFO] - Training Epoch: 1/2, step 7953/23838 completed (loss: 0.11518725752830505, acc: 0.9722222089767456)
[2025-02-16 12:25:17,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:17,670][root][INFO] - Training Epoch: 1/2, step 7954/23838 completed (loss: 0.2127009630203247, acc: 0.9485294222831726)
[2025-02-16 12:25:18,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:18,352][root][INFO] - Training Epoch: 1/2, step 7955/23838 completed (loss: 0.09521245211362839, acc: 0.9822485446929932)
[2025-02-16 12:25:18,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:18,895][root][INFO] - Training Epoch: 1/2, step 7956/23838 completed (loss: 0.40515726804733276, acc: 0.8897058963775635)
[2025-02-16 12:25:19,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:19,383][root][INFO] - Training Epoch: 1/2, step 7957/23838 completed (loss: 0.15892037749290466, acc: 0.9719626307487488)
[2025-02-16 12:25:19,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:19,770][root][INFO] - Training Epoch: 1/2, step 7958/23838 completed (loss: 0.39094486832618713, acc: 0.8974359035491943)
[2025-02-16 12:25:19,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:20,141][root][INFO] - Training Epoch: 1/2, step 7959/23838 completed (loss: 0.08740831166505814, acc: 0.9680851101875305)
[2025-02-16 12:25:20,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:20,594][root][INFO] - Training Epoch: 1/2, step 7960/23838 completed (loss: 0.266277551651001, acc: 0.9274193644523621)
[2025-02-16 12:25:20,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:21,204][root][INFO] - Training Epoch: 1/2, step 7961/23838 completed (loss: 0.21037684381008148, acc: 0.9590643048286438)
[2025-02-16 12:25:21,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:21,775][root][INFO] - Training Epoch: 1/2, step 7962/23838 completed (loss: 0.03807447850704193, acc: 0.9904761910438538)
[2025-02-16 12:25:22,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:22,242][root][INFO] - Training Epoch: 1/2, step 7963/23838 completed (loss: 0.30162790417671204, acc: 0.9255319237709045)
[2025-02-16 12:25:22,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:22,687][root][INFO] - Training Epoch: 1/2, step 7964/23838 completed (loss: 0.27879637479782104, acc: 0.9278350472450256)
[2025-02-16 12:25:23,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:23,766][root][INFO] - Training Epoch: 1/2, step 7965/23838 completed (loss: 0.2943997383117676, acc: 0.9305555820465088)
[2025-02-16 12:25:23,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:24,215][root][INFO] - Training Epoch: 1/2, step 7966/23838 completed (loss: 0.35409173369407654, acc: 0.918367326259613)
[2025-02-16 12:25:24,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:24,695][root][INFO] - Training Epoch: 1/2, step 7967/23838 completed (loss: 0.3606605529785156, acc: 0.914893627166748)
[2025-02-16 12:25:24,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:25,084][root][INFO] - Training Epoch: 1/2, step 7968/23838 completed (loss: 0.25385424494743347, acc: 0.9270833134651184)
[2025-02-16 12:25:25,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:25,539][root][INFO] - Training Epoch: 1/2, step 7969/23838 completed (loss: 0.19986699521541595, acc: 0.9664804339408875)
[2025-02-16 12:25:25,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:26,024][root][INFO] - Training Epoch: 1/2, step 7970/23838 completed (loss: 0.145803302526474, acc: 0.9576271176338196)
[2025-02-16 12:25:26,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:26,490][root][INFO] - Training Epoch: 1/2, step 7971/23838 completed (loss: 0.6052590012550354, acc: 0.8199999928474426)
[2025-02-16 12:25:26,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:26,958][root][INFO] - Training Epoch: 1/2, step 7972/23838 completed (loss: 0.15486307442188263, acc: 0.9658119678497314)
[2025-02-16 12:25:27,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:27,510][root][INFO] - Training Epoch: 1/2, step 7973/23838 completed (loss: 0.20266756415367126, acc: 0.9473684430122375)
[2025-02-16 12:25:27,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:28,201][root][INFO] - Training Epoch: 1/2, step 7974/23838 completed (loss: 0.19303636252880096, acc: 0.9408602118492126)
[2025-02-16 12:25:28,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:28,889][root][INFO] - Training Epoch: 1/2, step 7975/23838 completed (loss: 0.4832988679409027, acc: 0.867132842540741)
[2025-02-16 12:25:29,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:29,317][root][INFO] - Training Epoch: 1/2, step 7976/23838 completed (loss: 0.6320968270301819, acc: 0.7857142686843872)
[2025-02-16 12:25:29,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:29,735][root][INFO] - Training Epoch: 1/2, step 7977/23838 completed (loss: 0.29301613569259644, acc: 0.9193548560142517)
[2025-02-16 12:25:29,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:30,174][root][INFO] - Training Epoch: 1/2, step 7978/23838 completed (loss: 0.3118447959423065, acc: 0.9375)
[2025-02-16 12:25:30,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:30,590][root][INFO] - Training Epoch: 1/2, step 7979/23838 completed (loss: 0.5338279008865356, acc: 0.843478262424469)
[2025-02-16 12:25:30,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:31,005][root][INFO] - Training Epoch: 1/2, step 7980/23838 completed (loss: 0.5913974046707153, acc: 0.8461538553237915)
[2025-02-16 12:25:31,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:31,447][root][INFO] - Training Epoch: 1/2, step 7981/23838 completed (loss: 0.4700735807418823, acc: 0.8695651888847351)
[2025-02-16 12:25:31,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:31,933][root][INFO] - Training Epoch: 1/2, step 7982/23838 completed (loss: 0.09164363890886307, acc: 0.9866666793823242)
[2025-02-16 12:25:32,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:32,402][root][INFO] - Training Epoch: 1/2, step 7983/23838 completed (loss: 0.16851748526096344, acc: 0.9425287246704102)
[2025-02-16 12:25:32,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:32,844][root][INFO] - Training Epoch: 1/2, step 7984/23838 completed (loss: 0.14779187738895416, acc: 0.9708737730979919)
[2025-02-16 12:25:33,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:33,273][root][INFO] - Training Epoch: 1/2, step 7985/23838 completed (loss: 0.34050050377845764, acc: 0.9047619104385376)
[2025-02-16 12:25:33,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:33,940][root][INFO] - Training Epoch: 1/2, step 7986/23838 completed (loss: 0.3090154826641083, acc: 0.9134615659713745)
[2025-02-16 12:25:34,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:34,352][root][INFO] - Training Epoch: 1/2, step 7987/23838 completed (loss: 0.5302660465240479, acc: 0.8333333134651184)
[2025-02-16 12:25:34,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:34,711][root][INFO] - Training Epoch: 1/2, step 7988/23838 completed (loss: 0.8058391213417053, acc: 0.7857142686843872)
[2025-02-16 12:25:34,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:35,125][root][INFO] - Training Epoch: 1/2, step 7989/23838 completed (loss: 0.39204102754592896, acc: 0.8846153616905212)
[2025-02-16 12:25:35,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:35,549][root][INFO] - Training Epoch: 1/2, step 7990/23838 completed (loss: 0.47305887937545776, acc: 0.8888888955116272)
[2025-02-16 12:25:35,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:36,075][root][INFO] - Training Epoch: 1/2, step 7991/23838 completed (loss: 0.13580380380153656, acc: 0.951724112033844)
[2025-02-16 12:25:36,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:36,530][root][INFO] - Training Epoch: 1/2, step 7992/23838 completed (loss: 0.2752315402030945, acc: 0.9454545378684998)
[2025-02-16 12:25:36,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:37,062][root][INFO] - Training Epoch: 1/2, step 7993/23838 completed (loss: 0.10797479748725891, acc: 0.9700854420661926)
[2025-02-16 12:25:37,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:37,556][root][INFO] - Training Epoch: 1/2, step 7994/23838 completed (loss: 0.1790761798620224, acc: 0.9512194991111755)
[2025-02-16 12:25:37,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:37,976][root][INFO] - Training Epoch: 1/2, step 7995/23838 completed (loss: 0.16024969518184662, acc: 0.9674796462059021)
[2025-02-16 12:25:38,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:38,325][root][INFO] - Training Epoch: 1/2, step 7996/23838 completed (loss: 0.17047029733657837, acc: 0.9652174115180969)
[2025-02-16 12:25:38,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:38,766][root][INFO] - Training Epoch: 1/2, step 7997/23838 completed (loss: 0.18809771537780762, acc: 0.9553571343421936)
[2025-02-16 12:25:39,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:39,365][root][INFO] - Training Epoch: 1/2, step 7998/23838 completed (loss: 0.4638756811618805, acc: 0.8962264060974121)
[2025-02-16 12:25:39,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:39,770][root][INFO] - Training Epoch: 1/2, step 7999/23838 completed (loss: 0.6115130186080933, acc: 0.8695651888847351)
[2025-02-16 12:25:39,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:40,208][root][INFO] - Training Epoch: 1/2, step 8000/23838 completed (loss: 0.10778048634529114, acc: 0.978723406791687)
[2025-02-16 12:25:40,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:40,595][root][INFO] - Training Epoch: 1/2, step 8001/23838 completed (loss: 0.7730157375335693, acc: 0.760869562625885)
[2025-02-16 12:25:40,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:41,048][root][INFO] - Training Epoch: 1/2, step 8002/23838 completed (loss: 0.45932552218437195, acc: 0.839195966720581)
[2025-02-16 12:25:41,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:41,485][root][INFO] - Training Epoch: 1/2, step 8003/23838 completed (loss: 0.8239234685897827, acc: 0.7657657861709595)
[2025-02-16 12:25:41,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:41,929][root][INFO] - Training Epoch: 1/2, step 8004/23838 completed (loss: 0.5417486429214478, acc: 0.8591549396514893)
[2025-02-16 12:25:42,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:42,326][root][INFO] - Training Epoch: 1/2, step 8005/23838 completed (loss: 0.4373217225074768, acc: 0.9120879173278809)
[2025-02-16 12:25:42,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:42,761][root][INFO] - Training Epoch: 1/2, step 8006/23838 completed (loss: 1.0239970684051514, acc: 0.7400000095367432)
[2025-02-16 12:25:42,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:43,158][root][INFO] - Training Epoch: 1/2, step 8007/23838 completed (loss: 0.5377004742622375, acc: 0.8604651093482971)
[2025-02-16 12:25:43,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:43,603][root][INFO] - Training Epoch: 1/2, step 8008/23838 completed (loss: 0.7967799305915833, acc: 0.7767857313156128)
[2025-02-16 12:25:43,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:44,001][root][INFO] - Training Epoch: 1/2, step 8009/23838 completed (loss: 0.7805696129798889, acc: 0.767123281955719)
[2025-02-16 12:25:44,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:44,426][root][INFO] - Training Epoch: 1/2, step 8010/23838 completed (loss: 0.6007181406021118, acc: 0.8285714387893677)
[2025-02-16 12:25:44,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:45,030][root][INFO] - Training Epoch: 1/2, step 8011/23838 completed (loss: 0.9315435290336609, acc: 0.75)
[2025-02-16 12:25:45,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:45,415][root][INFO] - Training Epoch: 1/2, step 8012/23838 completed (loss: 0.2707698941230774, acc: 0.9368420839309692)
[2025-02-16 12:25:45,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:45,805][root][INFO] - Training Epoch: 1/2, step 8013/23838 completed (loss: 0.508765697479248, acc: 0.8799999952316284)
[2025-02-16 12:25:45,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:46,235][root][INFO] - Training Epoch: 1/2, step 8014/23838 completed (loss: 0.807486891746521, acc: 0.7894737124443054)
[2025-02-16 12:25:46,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:46,641][root][INFO] - Training Epoch: 1/2, step 8015/23838 completed (loss: 1.2944130897521973, acc: 0.5909090638160706)
[2025-02-16 12:25:46,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:47,090][root][INFO] - Training Epoch: 1/2, step 8016/23838 completed (loss: 0.8488031625747681, acc: 0.76106196641922)
[2025-02-16 12:25:47,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:47,475][root][INFO] - Training Epoch: 1/2, step 8017/23838 completed (loss: 0.5978007912635803, acc: 0.8349514603614807)
[2025-02-16 12:25:47,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:47,835][root][INFO] - Training Epoch: 1/2, step 8018/23838 completed (loss: 0.5182426571846008, acc: 0.8369565010070801)
[2025-02-16 12:25:48,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:48,288][root][INFO] - Training Epoch: 1/2, step 8019/23838 completed (loss: 0.831272304058075, acc: 0.8048780560493469)
[2025-02-16 12:25:48,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:48,759][root][INFO] - Training Epoch: 1/2, step 8020/23838 completed (loss: 1.0620194673538208, acc: 0.75)
[2025-02-16 12:25:48,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:49,187][root][INFO] - Training Epoch: 1/2, step 8021/23838 completed (loss: 0.8629883527755737, acc: 0.7398374080657959)
[2025-02-16 12:25:49,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:49,662][root][INFO] - Training Epoch: 1/2, step 8022/23838 completed (loss: 0.622143030166626, acc: 0.8382353186607361)
[2025-02-16 12:25:49,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:50,108][root][INFO] - Training Epoch: 1/2, step 8023/23838 completed (loss: 0.4422640800476074, acc: 0.8571428656578064)
[2025-02-16 12:25:50,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:50,491][root][INFO] - Training Epoch: 1/2, step 8024/23838 completed (loss: 0.7963857650756836, acc: 0.7747747898101807)
[2025-02-16 12:25:50,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:50,921][root][INFO] - Training Epoch: 1/2, step 8025/23838 completed (loss: 0.5665137767791748, acc: 0.8165137767791748)
[2025-02-16 12:25:51,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:51,355][root][INFO] - Training Epoch: 1/2, step 8026/23838 completed (loss: 0.6877581477165222, acc: 0.7894737124443054)
[2025-02-16 12:25:51,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:51,754][root][INFO] - Training Epoch: 1/2, step 8027/23838 completed (loss: 0.2729952037334442, acc: 0.9047619104385376)
[2025-02-16 12:25:52,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:52,248][root][INFO] - Training Epoch: 1/2, step 8028/23838 completed (loss: 0.25714513659477234, acc: 0.9230769276618958)
[2025-02-16 12:25:52,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:52,866][root][INFO] - Training Epoch: 1/2, step 8029/23838 completed (loss: 0.28940507769584656, acc: 0.9152542352676392)
[2025-02-16 12:25:53,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:53,355][root][INFO] - Training Epoch: 1/2, step 8030/23838 completed (loss: 0.6774420142173767, acc: 0.7777777910232544)
[2025-02-16 12:25:53,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:53,767][root][INFO] - Training Epoch: 1/2, step 8031/23838 completed (loss: 0.763037383556366, acc: 0.7808219194412231)
[2025-02-16 12:25:54,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:54,306][root][INFO] - Training Epoch: 1/2, step 8032/23838 completed (loss: 0.48467350006103516, acc: 0.8837209343910217)
[2025-02-16 12:25:54,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:55,070][root][INFO] - Training Epoch: 1/2, step 8033/23838 completed (loss: 0.6139848232269287, acc: 0.875)
[2025-02-16 12:25:55,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:56,083][root][INFO] - Training Epoch: 1/2, step 8034/23838 completed (loss: 0.2675653398036957, acc: 0.935251772403717)
[2025-02-16 12:25:56,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:56,528][root][INFO] - Training Epoch: 1/2, step 8035/23838 completed (loss: 0.6792019009590149, acc: 0.7804877758026123)
[2025-02-16 12:25:56,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:56,982][root][INFO] - Training Epoch: 1/2, step 8036/23838 completed (loss: 0.7194194793701172, acc: 0.8333333134651184)
[2025-02-16 12:25:57,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:57,524][root][INFO] - Training Epoch: 1/2, step 8037/23838 completed (loss: 0.7433281540870667, acc: 0.7820512652397156)
[2025-02-16 12:25:57,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:57,998][root][INFO] - Training Epoch: 1/2, step 8038/23838 completed (loss: 0.473772257566452, acc: 0.9069767594337463)
[2025-02-16 12:25:58,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:58,598][root][INFO] - Training Epoch: 1/2, step 8039/23838 completed (loss: 0.5531429052352905, acc: 0.8709677457809448)
[2025-02-16 12:25:58,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:59,053][root][INFO] - Training Epoch: 1/2, step 8040/23838 completed (loss: 0.3775559663772583, acc: 0.9318181872367859)
[2025-02-16 12:25:59,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:59,489][root][INFO] - Training Epoch: 1/2, step 8041/23838 completed (loss: 0.6373146772384644, acc: 0.8240740895271301)
[2025-02-16 12:25:59,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:25:59,972][root][INFO] - Training Epoch: 1/2, step 8042/23838 completed (loss: 0.2589736580848694, acc: 0.9166666865348816)
[2025-02-16 12:26:00,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:00,368][root][INFO] - Training Epoch: 1/2, step 8043/23838 completed (loss: 0.7074174284934998, acc: 0.7788461446762085)
[2025-02-16 12:26:00,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:00,892][root][INFO] - Training Epoch: 1/2, step 8044/23838 completed (loss: 0.44293874502182007, acc: 0.8636363744735718)
[2025-02-16 12:26:01,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:01,241][root][INFO] - Training Epoch: 1/2, step 8045/23838 completed (loss: 0.7274685502052307, acc: 0.796875)
[2025-02-16 12:26:01,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:01,610][root][INFO] - Training Epoch: 1/2, step 8046/23838 completed (loss: 0.7450933456420898, acc: 0.7727272510528564)
[2025-02-16 12:26:01,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:02,146][root][INFO] - Training Epoch: 1/2, step 8047/23838 completed (loss: 1.02361261844635, acc: 0.695652186870575)
[2025-02-16 12:26:02,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:02,612][root][INFO] - Training Epoch: 1/2, step 8048/23838 completed (loss: 0.5958853363990784, acc: 0.841269850730896)
[2025-02-16 12:26:02,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:03,036][root][INFO] - Training Epoch: 1/2, step 8049/23838 completed (loss: 0.757452666759491, acc: 0.7983871102333069)
[2025-02-16 12:26:03,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:03,709][root][INFO] - Training Epoch: 1/2, step 8050/23838 completed (loss: 0.45538413524627686, acc: 0.8873239159584045)
[2025-02-16 12:26:03,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:04,119][root][INFO] - Training Epoch: 1/2, step 8051/23838 completed (loss: 0.5812715888023376, acc: 0.8032786846160889)
[2025-02-16 12:26:04,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:04,653][root][INFO] - Training Epoch: 1/2, step 8052/23838 completed (loss: 0.19106915593147278, acc: 0.9368420839309692)
[2025-02-16 12:26:05,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:05,431][root][INFO] - Training Epoch: 1/2, step 8053/23838 completed (loss: 0.5527043342590332, acc: 0.802395224571228)
[2025-02-16 12:26:05,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:05,898][root][INFO] - Training Epoch: 1/2, step 8054/23838 completed (loss: 0.42680394649505615, acc: 0.8947368264198303)
[2025-02-16 12:26:06,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:06,309][root][INFO] - Training Epoch: 1/2, step 8055/23838 completed (loss: 0.3478888273239136, acc: 0.9047619104385376)
[2025-02-16 12:26:06,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:06,743][root][INFO] - Training Epoch: 1/2, step 8056/23838 completed (loss: 0.32257258892059326, acc: 0.8987341523170471)
[2025-02-16 12:26:07,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:07,268][root][INFO] - Training Epoch: 1/2, step 8057/23838 completed (loss: 0.40220484137535095, acc: 0.8740741014480591)
[2025-02-16 12:26:07,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:08,064][root][INFO] - Training Epoch: 1/2, step 8058/23838 completed (loss: 0.4949234127998352, acc: 0.8652482032775879)
[2025-02-16 12:26:08,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:08,458][root][INFO] - Training Epoch: 1/2, step 8059/23838 completed (loss: 0.14595894515514374, acc: 0.9428571462631226)
[2025-02-16 12:26:08,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:08,884][root][INFO] - Training Epoch: 1/2, step 8060/23838 completed (loss: 0.27510935068130493, acc: 0.9124087691307068)
[2025-02-16 12:26:09,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:09,382][root][INFO] - Training Epoch: 1/2, step 8061/23838 completed (loss: 0.3460698425769806, acc: 0.9179487228393555)
[2025-02-16 12:26:09,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:09,826][root][INFO] - Training Epoch: 1/2, step 8062/23838 completed (loss: 0.4696104824542999, acc: 0.8648648858070374)
[2025-02-16 12:26:10,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:10,211][root][INFO] - Training Epoch: 1/2, step 8063/23838 completed (loss: 0.45345133543014526, acc: 0.8653846383094788)
[2025-02-16 12:26:10,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:10,707][root][INFO] - Training Epoch: 1/2, step 8064/23838 completed (loss: 0.4038873016834259, acc: 0.9103448390960693)
[2025-02-16 12:26:10,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:11,121][root][INFO] - Training Epoch: 1/2, step 8065/23838 completed (loss: 0.32704436779022217, acc: 0.9139785170555115)
[2025-02-16 12:26:11,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:11,523][root][INFO] - Training Epoch: 1/2, step 8066/23838 completed (loss: 1.0629069805145264, acc: 0.6721311211585999)
[2025-02-16 12:26:11,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:11,959][root][INFO] - Training Epoch: 1/2, step 8067/23838 completed (loss: 0.2724193334579468, acc: 0.9264705777168274)
[2025-02-16 12:26:12,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:12,407][root][INFO] - Training Epoch: 1/2, step 8068/23838 completed (loss: 0.20493103563785553, acc: 0.9367088675498962)
[2025-02-16 12:26:12,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:12,777][root][INFO] - Training Epoch: 1/2, step 8069/23838 completed (loss: 0.46246346831321716, acc: 0.8658536672592163)
[2025-02-16 12:26:12,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:13,211][root][INFO] - Training Epoch: 1/2, step 8070/23838 completed (loss: 0.3665274679660797, acc: 0.918181836605072)
[2025-02-16 12:26:13,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:13,561][root][INFO] - Training Epoch: 1/2, step 8071/23838 completed (loss: 0.8706754446029663, acc: 0.8133333325386047)
[2025-02-16 12:26:13,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:13,994][root][INFO] - Training Epoch: 1/2, step 8072/23838 completed (loss: 0.40967002511024475, acc: 0.8736842274665833)
[2025-02-16 12:26:14,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:14,400][root][INFO] - Training Epoch: 1/2, step 8073/23838 completed (loss: 1.153313159942627, acc: 0.6388888955116272)
[2025-02-16 12:26:14,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:14,849][root][INFO] - Training Epoch: 1/2, step 8074/23838 completed (loss: 0.5743712782859802, acc: 0.8285714387893677)
[2025-02-16 12:26:15,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:15,708][root][INFO] - Training Epoch: 1/2, step 8075/23838 completed (loss: 0.5090562105178833, acc: 0.8872180581092834)
[2025-02-16 12:26:15,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:16,077][root][INFO] - Training Epoch: 1/2, step 8076/23838 completed (loss: 0.8527678847312927, acc: 0.75)
[2025-02-16 12:26:16,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:16,481][root][INFO] - Training Epoch: 1/2, step 8077/23838 completed (loss: 1.1600826978683472, acc: 0.6153846383094788)
[2025-02-16 12:26:16,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:16,892][root][INFO] - Training Epoch: 1/2, step 8078/23838 completed (loss: 0.48945486545562744, acc: 0.8387096524238586)
[2025-02-16 12:26:17,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:17,380][root][INFO] - Training Epoch: 1/2, step 8079/23838 completed (loss: 0.6662512421607971, acc: 0.7831325531005859)
[2025-02-16 12:26:17,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:18,023][root][INFO] - Training Epoch: 1/2, step 8080/23838 completed (loss: 0.3365384340286255, acc: 0.9006211161613464)
[2025-02-16 12:26:18,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:18,482][root][INFO] - Training Epoch: 1/2, step 8081/23838 completed (loss: 0.22940713167190552, acc: 0.93388432264328)
[2025-02-16 12:26:18,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:18,941][root][INFO] - Training Epoch: 1/2, step 8082/23838 completed (loss: 0.4640055298805237, acc: 0.8571428656578064)
[2025-02-16 12:26:19,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:19,701][root][INFO] - Training Epoch: 1/2, step 8083/23838 completed (loss: 0.3929000794887543, acc: 0.8846153616905212)
[2025-02-16 12:26:19,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:20,190][root][INFO] - Training Epoch: 1/2, step 8084/23838 completed (loss: 0.3490496575832367, acc: 0.8999999761581421)
[2025-02-16 12:26:20,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:20,635][root][INFO] - Training Epoch: 1/2, step 8085/23838 completed (loss: 0.43306511640548706, acc: 0.866310179233551)
[2025-02-16 12:26:20,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:21,048][root][INFO] - Training Epoch: 1/2, step 8086/23838 completed (loss: 0.5261527299880981, acc: 0.8404255509376526)
[2025-02-16 12:26:21,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:21,455][root][INFO] - Training Epoch: 1/2, step 8087/23838 completed (loss: 0.43636927008628845, acc: 0.8661971688270569)
[2025-02-16 12:26:21,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:22,109][root][INFO] - Training Epoch: 1/2, step 8088/23838 completed (loss: 0.5461266040802002, acc: 0.8545454740524292)
[2025-02-16 12:26:22,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:22,549][root][INFO] - Training Epoch: 1/2, step 8089/23838 completed (loss: 0.936988353729248, acc: 0.7627118825912476)
[2025-02-16 12:26:22,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:23,021][root][INFO] - Training Epoch: 1/2, step 8090/23838 completed (loss: 0.5428500771522522, acc: 0.8646616339683533)
[2025-02-16 12:26:23,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:23,507][root][INFO] - Training Epoch: 1/2, step 8091/23838 completed (loss: 0.3140462338924408, acc: 0.8989899158477783)
[2025-02-16 12:26:23,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:23,922][root][INFO] - Training Epoch: 1/2, step 8092/23838 completed (loss: 0.5313504934310913, acc: 0.8387096524238586)
[2025-02-16 12:26:24,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:24,332][root][INFO] - Training Epoch: 1/2, step 8093/23838 completed (loss: 0.8846490979194641, acc: 0.7352941036224365)
[2025-02-16 12:26:24,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:24,767][root][INFO] - Training Epoch: 1/2, step 8094/23838 completed (loss: 1.1463690996170044, acc: 0.6617646813392639)
[2025-02-16 12:26:24,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:25,202][root][INFO] - Training Epoch: 1/2, step 8095/23838 completed (loss: 0.8309551477432251, acc: 0.800000011920929)
[2025-02-16 12:26:25,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:25,681][root][INFO] - Training Epoch: 1/2, step 8096/23838 completed (loss: 0.5554311275482178, acc: 0.8137931227684021)
[2025-02-16 12:26:25,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:26,104][root][INFO] - Training Epoch: 1/2, step 8097/23838 completed (loss: 0.6848829388618469, acc: 0.7945205569267273)
[2025-02-16 12:26:26,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:26,543][root][INFO] - Training Epoch: 1/2, step 8098/23838 completed (loss: 0.4827224910259247, acc: 0.8837209343910217)
[2025-02-16 12:26:26,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:27,117][root][INFO] - Training Epoch: 1/2, step 8099/23838 completed (loss: 0.16309404373168945, acc: 0.9559471607208252)
[2025-02-16 12:26:27,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:27,539][root][INFO] - Training Epoch: 1/2, step 8100/23838 completed (loss: 1.0546753406524658, acc: 0.6811594367027283)
[2025-02-16 12:26:27,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:28,408][root][INFO] - Training Epoch: 1/2, step 8101/23838 completed (loss: 0.5064182877540588, acc: 0.8717948794364929)
[2025-02-16 12:26:28,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:28,822][root][INFO] - Training Epoch: 1/2, step 8102/23838 completed (loss: 0.35994529724121094, acc: 0.9090909361839294)
[2025-02-16 12:26:29,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:29,321][root][INFO] - Training Epoch: 1/2, step 8103/23838 completed (loss: 0.25988173484802246, acc: 0.9237288236618042)
[2025-02-16 12:26:29,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:29,754][root][INFO] - Training Epoch: 1/2, step 8104/23838 completed (loss: 0.6533840298652649, acc: 0.7976190447807312)
[2025-02-16 12:26:29,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:30,178][root][INFO] - Training Epoch: 1/2, step 8105/23838 completed (loss: 0.13751797378063202, acc: 0.978723406791687)
[2025-02-16 12:26:30,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:30,570][root][INFO] - Training Epoch: 1/2, step 8106/23838 completed (loss: 0.3250906467437744, acc: 0.8897637724876404)
[2025-02-16 12:26:30,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:31,002][root][INFO] - Training Epoch: 1/2, step 8107/23838 completed (loss: 0.3954058289527893, acc: 0.8903225660324097)
[2025-02-16 12:26:31,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:31,380][root][INFO] - Training Epoch: 1/2, step 8108/23838 completed (loss: 0.3159707486629486, acc: 0.886956512928009)
[2025-02-16 12:26:31,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:31,871][root][INFO] - Training Epoch: 1/2, step 8109/23838 completed (loss: 0.5068830251693726, acc: 0.9032257795333862)
[2025-02-16 12:26:32,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:32,256][root][INFO] - Training Epoch: 1/2, step 8110/23838 completed (loss: 0.4415022134780884, acc: 0.875)
[2025-02-16 12:26:32,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:32,637][root][INFO] - Training Epoch: 1/2, step 8111/23838 completed (loss: 0.37612053751945496, acc: 0.9130434989929199)
[2025-02-16 12:26:32,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:33,067][root][INFO] - Training Epoch: 1/2, step 8112/23838 completed (loss: 0.46351730823516846, acc: 0.8659793734550476)
[2025-02-16 12:26:33,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:33,540][root][INFO] - Training Epoch: 1/2, step 8113/23838 completed (loss: 0.5553979873657227, acc: 0.8695651888847351)
[2025-02-16 12:26:33,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:33,982][root][INFO] - Training Epoch: 1/2, step 8114/23838 completed (loss: 0.6520172953605652, acc: 0.8433734774589539)
[2025-02-16 12:26:34,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:34,353][root][INFO] - Training Epoch: 1/2, step 8115/23838 completed (loss: 0.6031554937362671, acc: 0.8467742204666138)
[2025-02-16 12:26:34,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:34,810][root][INFO] - Training Epoch: 1/2, step 8116/23838 completed (loss: 0.20404300093650818, acc: 0.9450549483299255)
[2025-02-16 12:26:35,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:35,240][root][INFO] - Training Epoch: 1/2, step 8117/23838 completed (loss: 0.21475961804389954, acc: 0.9285714030265808)
[2025-02-16 12:26:35,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:35,625][root][INFO] - Training Epoch: 1/2, step 8118/23838 completed (loss: 0.1346885859966278, acc: 0.9583333134651184)
[2025-02-16 12:26:35,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:36,054][root][INFO] - Training Epoch: 1/2, step 8119/23838 completed (loss: 0.24411849677562714, acc: 0.9289340376853943)
[2025-02-16 12:26:36,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:36,490][root][INFO] - Training Epoch: 1/2, step 8120/23838 completed (loss: 0.275745689868927, acc: 0.9444444179534912)
[2025-02-16 12:26:36,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:36,877][root][INFO] - Training Epoch: 1/2, step 8121/23838 completed (loss: 0.34785178303718567, acc: 0.9024389982223511)
[2025-02-16 12:26:37,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:37,315][root][INFO] - Training Epoch: 1/2, step 8122/23838 completed (loss: 0.28181639313697815, acc: 0.9263157844543457)
[2025-02-16 12:26:37,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:37,744][root][INFO] - Training Epoch: 1/2, step 8123/23838 completed (loss: 0.7635408639907837, acc: 0.7373737096786499)
[2025-02-16 12:26:37,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:38,177][root][INFO] - Training Epoch: 1/2, step 8124/23838 completed (loss: 0.3718213140964508, acc: 0.9108911156654358)
[2025-02-16 12:26:38,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:38,550][root][INFO] - Training Epoch: 1/2, step 8125/23838 completed (loss: 0.46680405735969543, acc: 0.874015748500824)
[2025-02-16 12:26:38,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:39,003][root][INFO] - Training Epoch: 1/2, step 8126/23838 completed (loss: 0.5832715034484863, acc: 0.8780487775802612)
[2025-02-16 12:26:39,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:39,398][root][INFO] - Training Epoch: 1/2, step 8127/23838 completed (loss: 0.607369065284729, acc: 0.8333333134651184)
[2025-02-16 12:26:39,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:39,905][root][INFO] - Training Epoch: 1/2, step 8128/23838 completed (loss: 0.2779220640659332, acc: 0.9253731369972229)
[2025-02-16 12:26:40,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:40,303][root][INFO] - Training Epoch: 1/2, step 8129/23838 completed (loss: 0.5079085826873779, acc: 0.8863636255264282)
[2025-02-16 12:26:40,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:40,670][root][INFO] - Training Epoch: 1/2, step 8130/23838 completed (loss: 0.2668570876121521, acc: 0.8888888955116272)
[2025-02-16 12:26:40,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:41,089][root][INFO] - Training Epoch: 1/2, step 8131/23838 completed (loss: 0.4913528561592102, acc: 0.8888888955116272)
[2025-02-16 12:26:41,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:41,495][root][INFO] - Training Epoch: 1/2, step 8132/23838 completed (loss: 0.24856439232826233, acc: 0.9240506291389465)
[2025-02-16 12:26:41,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:41,920][root][INFO] - Training Epoch: 1/2, step 8133/23838 completed (loss: 0.34106093645095825, acc: 0.931506872177124)
[2025-02-16 12:26:42,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:42,291][root][INFO] - Training Epoch: 1/2, step 8134/23838 completed (loss: 0.415698379278183, acc: 0.8924731016159058)
[2025-02-16 12:26:42,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:42,717][root][INFO] - Training Epoch: 1/2, step 8135/23838 completed (loss: 0.17617659270763397, acc: 0.961904764175415)
[2025-02-16 12:26:42,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:43,165][root][INFO] - Training Epoch: 1/2, step 8136/23838 completed (loss: 0.4252624809741974, acc: 0.886956512928009)
[2025-02-16 12:26:43,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:43,550][root][INFO] - Training Epoch: 1/2, step 8137/23838 completed (loss: 0.47592097520828247, acc: 0.895348846912384)
[2025-02-16 12:26:43,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:43,987][root][INFO] - Training Epoch: 1/2, step 8138/23838 completed (loss: 0.16847914457321167, acc: 0.9469026327133179)
[2025-02-16 12:26:44,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:44,426][root][INFO] - Training Epoch: 1/2, step 8139/23838 completed (loss: 0.3026863634586334, acc: 0.8999999761581421)
[2025-02-16 12:26:44,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:44,815][root][INFO] - Training Epoch: 1/2, step 8140/23838 completed (loss: 0.409259557723999, acc: 0.8636363744735718)
[2025-02-16 12:26:44,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:45,217][root][INFO] - Training Epoch: 1/2, step 8141/23838 completed (loss: 0.2847502529621124, acc: 0.9189189076423645)
[2025-02-16 12:26:45,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:45,702][root][INFO] - Training Epoch: 1/2, step 8142/23838 completed (loss: 0.22416862845420837, acc: 0.9379310607910156)
[2025-02-16 12:26:45,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:46,185][root][INFO] - Training Epoch: 1/2, step 8143/23838 completed (loss: 0.17361237108707428, acc: 0.9399999976158142)
[2025-02-16 12:26:46,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:46,560][root][INFO] - Training Epoch: 1/2, step 8144/23838 completed (loss: 0.046366892755031586, acc: 0.9900990128517151)
[2025-02-16 12:26:46,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:47,005][root][INFO] - Training Epoch: 1/2, step 8145/23838 completed (loss: 0.18835707008838654, acc: 0.949367105960846)
[2025-02-16 12:26:47,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:47,401][root][INFO] - Training Epoch: 1/2, step 8146/23838 completed (loss: 0.07443511486053467, acc: 0.9870129823684692)
[2025-02-16 12:26:47,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:47,821][root][INFO] - Training Epoch: 1/2, step 8147/23838 completed (loss: 0.14776086807250977, acc: 0.9570552110671997)
[2025-02-16 12:26:48,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:48,221][root][INFO] - Training Epoch: 1/2, step 8148/23838 completed (loss: 0.3257303535938263, acc: 0.9459459185600281)
[2025-02-16 12:26:48,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:48,654][root][INFO] - Training Epoch: 1/2, step 8149/23838 completed (loss: 0.0957232266664505, acc: 0.9739130139350891)
[2025-02-16 12:26:48,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:49,096][root][INFO] - Training Epoch: 1/2, step 8150/23838 completed (loss: 0.11284530907869339, acc: 0.9829059839248657)
[2025-02-16 12:26:49,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:49,567][root][INFO] - Training Epoch: 1/2, step 8151/23838 completed (loss: 0.05614950880408287, acc: 0.9887640476226807)
[2025-02-16 12:26:49,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:49,950][root][INFO] - Training Epoch: 1/2, step 8152/23838 completed (loss: 0.18320734798908234, acc: 0.9568965435028076)
[2025-02-16 12:26:50,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:50,390][root][INFO] - Training Epoch: 1/2, step 8153/23838 completed (loss: 0.1686142086982727, acc: 0.9603174328804016)
[2025-02-16 12:26:50,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:51,090][root][INFO] - Training Epoch: 1/2, step 8154/23838 completed (loss: 0.16005519032478333, acc: 0.9513513445854187)
[2025-02-16 12:26:51,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:51,588][root][INFO] - Training Epoch: 1/2, step 8155/23838 completed (loss: 0.22735929489135742, acc: 0.9216867685317993)
[2025-02-16 12:26:51,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:52,025][root][INFO] - Training Epoch: 1/2, step 8156/23838 completed (loss: 0.21049600839614868, acc: 0.948051929473877)
[2025-02-16 12:26:52,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:52,430][root][INFO] - Training Epoch: 1/2, step 8157/23838 completed (loss: 0.0296183954924345, acc: 1.0)
[2025-02-16 12:26:52,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:52,865][root][INFO] - Training Epoch: 1/2, step 8158/23838 completed (loss: 0.6793975234031677, acc: 0.8461538553237915)
[2025-02-16 12:26:53,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:53,241][root][INFO] - Training Epoch: 1/2, step 8159/23838 completed (loss: 0.7656605839729309, acc: 0.7864077687263489)
[2025-02-16 12:26:53,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:53,691][root][INFO] - Training Epoch: 1/2, step 8160/23838 completed (loss: 0.39569321274757385, acc: 0.9036144614219666)
[2025-02-16 12:26:53,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:54,061][root][INFO] - Training Epoch: 1/2, step 8161/23838 completed (loss: 1.069047451019287, acc: 0.7627118825912476)
[2025-02-16 12:26:54,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:54,426][root][INFO] - Training Epoch: 1/2, step 8162/23838 completed (loss: 0.35880666971206665, acc: 0.8785046935081482)
[2025-02-16 12:26:54,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:54,869][root][INFO] - Training Epoch: 1/2, step 8163/23838 completed (loss: 0.46994760632514954, acc: 0.8709677457809448)
[2025-02-16 12:26:55,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:55,306][root][INFO] - Training Epoch: 1/2, step 8164/23838 completed (loss: 0.4654172658920288, acc: 0.8983050584793091)
[2025-02-16 12:26:55,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:55,683][root][INFO] - Training Epoch: 1/2, step 8165/23838 completed (loss: 0.7253188490867615, acc: 0.7407407164573669)
[2025-02-16 12:26:55,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:56,089][root][INFO] - Training Epoch: 1/2, step 8166/23838 completed (loss: 0.6519575119018555, acc: 0.8235294222831726)
[2025-02-16 12:26:56,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:56,490][root][INFO] - Training Epoch: 1/2, step 8167/23838 completed (loss: 0.7279062271118164, acc: 0.8202247023582458)
[2025-02-16 12:26:56,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:56,907][root][INFO] - Training Epoch: 1/2, step 8168/23838 completed (loss: 0.346431165933609, acc: 0.9078947305679321)
[2025-02-16 12:26:57,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:57,271][root][INFO] - Training Epoch: 1/2, step 8169/23838 completed (loss: 0.81926029920578, acc: 0.8113207817077637)
[2025-02-16 12:26:57,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:57,691][root][INFO] - Training Epoch: 1/2, step 8170/23838 completed (loss: 0.589698314666748, acc: 0.8307692408561707)
[2025-02-16 12:26:57,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:58,085][root][INFO] - Training Epoch: 1/2, step 8171/23838 completed (loss: 1.3008555173873901, acc: 0.6415094137191772)
[2025-02-16 12:26:58,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:58,519][root][INFO] - Training Epoch: 1/2, step 8172/23838 completed (loss: 0.6544152498245239, acc: 0.7586206793785095)
[2025-02-16 12:26:58,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:58,918][root][INFO] - Training Epoch: 1/2, step 8173/23838 completed (loss: 0.38301411271095276, acc: 0.8999999761581421)
[2025-02-16 12:26:59,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:59,363][root][INFO] - Training Epoch: 1/2, step 8174/23838 completed (loss: 0.18259435892105103, acc: 0.9300000071525574)
[2025-02-16 12:26:59,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:26:59,794][root][INFO] - Training Epoch: 1/2, step 8175/23838 completed (loss: 0.9859375357627869, acc: 0.7627118825912476)
[2025-02-16 12:27:00,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:00,239][root][INFO] - Training Epoch: 1/2, step 8176/23838 completed (loss: 0.9646634459495544, acc: 0.707317054271698)
[2025-02-16 12:27:00,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:00,656][root][INFO] - Training Epoch: 1/2, step 8177/23838 completed (loss: 0.4795038104057312, acc: 0.818965494632721)
[2025-02-16 12:27:00,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:01,072][root][INFO] - Training Epoch: 1/2, step 8178/23838 completed (loss: 0.9518712162971497, acc: 0.7884615659713745)
[2025-02-16 12:27:01,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:01,537][root][INFO] - Training Epoch: 1/2, step 8179/23838 completed (loss: 0.9299758672714233, acc: 0.75)
[2025-02-16 12:27:01,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:01,967][root][INFO] - Training Epoch: 1/2, step 8180/23838 completed (loss: 0.6838012933731079, acc: 0.7886179089546204)
[2025-02-16 12:27:02,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:02,402][root][INFO] - Training Epoch: 1/2, step 8181/23838 completed (loss: 0.5191690921783447, acc: 0.8494623899459839)
[2025-02-16 12:27:02,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:02,792][root][INFO] - Training Epoch: 1/2, step 8182/23838 completed (loss: 0.684149444103241, acc: 0.8142856955528259)
[2025-02-16 12:27:02,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:03,214][root][INFO] - Training Epoch: 1/2, step 8183/23838 completed (loss: 0.8125611543655396, acc: 0.7142857313156128)
[2025-02-16 12:27:03,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:03,595][root][INFO] - Training Epoch: 1/2, step 8184/23838 completed (loss: 0.7582740783691406, acc: 0.782608687877655)
[2025-02-16 12:27:03,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:04,004][root][INFO] - Training Epoch: 1/2, step 8185/23838 completed (loss: 0.6591607332229614, acc: 0.8181818127632141)
[2025-02-16 12:27:04,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:04,439][root][INFO] - Training Epoch: 1/2, step 8186/23838 completed (loss: 0.8130688071250916, acc: 0.7460317611694336)
[2025-02-16 12:27:04,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:04,874][root][INFO] - Training Epoch: 1/2, step 8187/23838 completed (loss: 0.43483734130859375, acc: 0.875)
[2025-02-16 12:27:05,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:05,226][root][INFO] - Training Epoch: 1/2, step 8188/23838 completed (loss: 0.9595246911048889, acc: 0.7164179086685181)
[2025-02-16 12:27:05,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:05,672][root][INFO] - Training Epoch: 1/2, step 8189/23838 completed (loss: 0.7325116991996765, acc: 0.8016529083251953)
[2025-02-16 12:27:05,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:06,143][root][INFO] - Training Epoch: 1/2, step 8190/23838 completed (loss: 0.6072999238967896, acc: 0.8235294222831726)
[2025-02-16 12:27:06,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:06,523][root][INFO] - Training Epoch: 1/2, step 8191/23838 completed (loss: 0.5386620759963989, acc: 0.8600000143051147)
[2025-02-16 12:27:06,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:06,985][root][INFO] - Training Epoch: 1/2, step 8192/23838 completed (loss: 0.7932708859443665, acc: 0.7821782231330872)
[2025-02-16 12:27:07,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:07,391][root][INFO] - Training Epoch: 1/2, step 8193/23838 completed (loss: 0.3583633303642273, acc: 0.8500000238418579)
[2025-02-16 12:27:07,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:07,823][root][INFO] - Training Epoch: 1/2, step 8194/23838 completed (loss: 0.891789436340332, acc: 0.765625)
[2025-02-16 12:27:08,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:08,255][root][INFO] - Training Epoch: 1/2, step 8195/23838 completed (loss: 0.8033152222633362, acc: 0.7922077775001526)
[2025-02-16 12:27:08,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:08,674][root][INFO] - Training Epoch: 1/2, step 8196/23838 completed (loss: 0.4530779719352722, acc: 0.8804348111152649)
[2025-02-16 12:27:08,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:09,096][root][INFO] - Training Epoch: 1/2, step 8197/23838 completed (loss: 1.0379328727722168, acc: 0.7083333134651184)
[2025-02-16 12:27:09,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:09,528][root][INFO] - Training Epoch: 1/2, step 8198/23838 completed (loss: 0.9687497615814209, acc: 0.7333333492279053)
[2025-02-16 12:27:09,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:09,992][root][INFO] - Training Epoch: 1/2, step 8199/23838 completed (loss: 0.24150867760181427, acc: 0.9523809552192688)
[2025-02-16 12:27:10,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:10,393][root][INFO] - Training Epoch: 1/2, step 8200/23838 completed (loss: 0.4519810080528259, acc: 0.8448275923728943)
[2025-02-16 12:27:10,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:10,795][root][INFO] - Training Epoch: 1/2, step 8201/23838 completed (loss: 0.5875338912010193, acc: 0.8586956262588501)
[2025-02-16 12:27:10,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:11,195][root][INFO] - Training Epoch: 1/2, step 8202/23838 completed (loss: 0.5173287987709045, acc: 0.8679245114326477)
[2025-02-16 12:27:11,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:11,611][root][INFO] - Training Epoch: 1/2, step 8203/23838 completed (loss: 0.4119890630245209, acc: 0.8780487775802612)
[2025-02-16 12:27:11,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:12,036][root][INFO] - Training Epoch: 1/2, step 8204/23838 completed (loss: 0.5092704892158508, acc: 0.8556700944900513)
[2025-02-16 12:27:12,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:12,476][root][INFO] - Training Epoch: 1/2, step 8205/23838 completed (loss: 0.1983327567577362, acc: 0.9350649118423462)
[2025-02-16 12:27:12,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:12,873][root][INFO] - Training Epoch: 1/2, step 8206/23838 completed (loss: 0.45160844922065735, acc: 0.8795180916786194)
[2025-02-16 12:27:13,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:13,227][root][INFO] - Training Epoch: 1/2, step 8207/23838 completed (loss: 0.5484389662742615, acc: 0.8738738894462585)
[2025-02-16 12:27:13,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:13,676][root][INFO] - Training Epoch: 1/2, step 8208/23838 completed (loss: 0.5216757655143738, acc: 0.9074074029922485)
[2025-02-16 12:27:13,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:14,104][root][INFO] - Training Epoch: 1/2, step 8209/23838 completed (loss: 0.4503403306007385, acc: 0.8536585569381714)
[2025-02-16 12:27:14,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:14,552][root][INFO] - Training Epoch: 1/2, step 8210/23838 completed (loss: 1.1902499198913574, acc: 0.6181818246841431)
[2025-02-16 12:27:14,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:14,980][root][INFO] - Training Epoch: 1/2, step 8211/23838 completed (loss: 0.6678316593170166, acc: 0.8095238208770752)
[2025-02-16 12:27:15,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:15,415][root][INFO] - Training Epoch: 1/2, step 8212/23838 completed (loss: 0.23166756331920624, acc: 0.9279279112815857)
[2025-02-16 12:27:15,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:15,850][root][INFO] - Training Epoch: 1/2, step 8213/23838 completed (loss: 0.5226410627365112, acc: 0.8153846263885498)
[2025-02-16 12:27:16,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:16,281][root][INFO] - Training Epoch: 1/2, step 8214/23838 completed (loss: 0.4392460584640503, acc: 0.8939393758773804)
[2025-02-16 12:27:16,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:16,692][root][INFO] - Training Epoch: 1/2, step 8215/23838 completed (loss: 0.8463543653488159, acc: 0.761904776096344)
[2025-02-16 12:27:16,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:17,115][root][INFO] - Training Epoch: 1/2, step 8216/23838 completed (loss: 0.5035712122917175, acc: 0.8656716346740723)
[2025-02-16 12:27:17,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:17,576][root][INFO] - Training Epoch: 1/2, step 8217/23838 completed (loss: 0.308152437210083, acc: 0.9200000166893005)
[2025-02-16 12:27:17,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:17,989][root][INFO] - Training Epoch: 1/2, step 8218/23838 completed (loss: 0.512917697429657, acc: 0.8214285969734192)
[2025-02-16 12:27:18,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:18,407][root][INFO] - Training Epoch: 1/2, step 8219/23838 completed (loss: 1.2599519491195679, acc: 0.5833333134651184)
[2025-02-16 12:27:18,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:18,906][root][INFO] - Training Epoch: 1/2, step 8220/23838 completed (loss: 0.6631391644477844, acc: 0.8426966071128845)
[2025-02-16 12:27:19,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:19,335][root][INFO] - Training Epoch: 1/2, step 8221/23838 completed (loss: 0.7242931127548218, acc: 0.8227847814559937)
[2025-02-16 12:27:19,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:19,708][root][INFO] - Training Epoch: 1/2, step 8222/23838 completed (loss: 0.5441443920135498, acc: 0.8735632300376892)
[2025-02-16 12:27:19,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:20,094][root][INFO] - Training Epoch: 1/2, step 8223/23838 completed (loss: 0.30016428232192993, acc: 0.9255319237709045)
[2025-02-16 12:27:20,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:20,484][root][INFO] - Training Epoch: 1/2, step 8224/23838 completed (loss: 0.498020201921463, acc: 0.8571428656578064)
[2025-02-16 12:27:20,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:20,886][root][INFO] - Training Epoch: 1/2, step 8225/23838 completed (loss: 0.49457043409347534, acc: 0.8865979313850403)
[2025-02-16 12:27:21,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:21,358][root][INFO] - Training Epoch: 1/2, step 8226/23838 completed (loss: 0.26831212639808655, acc: 0.9078947305679321)
[2025-02-16 12:27:21,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:21,762][root][INFO] - Training Epoch: 1/2, step 8227/23838 completed (loss: 0.5784112215042114, acc: 0.8166666626930237)
[2025-02-16 12:27:21,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:22,228][root][INFO] - Training Epoch: 1/2, step 8228/23838 completed (loss: 0.5121443867683411, acc: 0.8652482032775879)
[2025-02-16 12:27:22,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:22,697][root][INFO] - Training Epoch: 1/2, step 8229/23838 completed (loss: 0.7431250214576721, acc: 0.7701149582862854)
[2025-02-16 12:27:22,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:23,119][root][INFO] - Training Epoch: 1/2, step 8230/23838 completed (loss: 0.7335042953491211, acc: 0.7945205569267273)
[2025-02-16 12:27:23,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:23,698][root][INFO] - Training Epoch: 1/2, step 8231/23838 completed (loss: 0.365047812461853, acc: 0.8992248177528381)
[2025-02-16 12:27:23,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:24,077][root][INFO] - Training Epoch: 1/2, step 8232/23838 completed (loss: 1.2766125202178955, acc: 0.6071428656578064)
[2025-02-16 12:27:24,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:24,606][root][INFO] - Training Epoch: 1/2, step 8233/23838 completed (loss: 1.0134282112121582, acc: 0.7525773048400879)
[2025-02-16 12:27:24,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:25,022][root][INFO] - Training Epoch: 1/2, step 8234/23838 completed (loss: 1.011969804763794, acc: 0.738095223903656)
[2025-02-16 12:27:25,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:25,476][root][INFO] - Training Epoch: 1/2, step 8235/23838 completed (loss: 1.033197045326233, acc: 0.75)
[2025-02-16 12:27:25,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:25,933][root][INFO] - Training Epoch: 1/2, step 8236/23838 completed (loss: 0.7878850698471069, acc: 0.7727272510528564)
[2025-02-16 12:27:26,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:26,391][root][INFO] - Training Epoch: 1/2, step 8237/23838 completed (loss: 0.44646018743515015, acc: 0.8613138794898987)
[2025-02-16 12:27:26,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:26,829][root][INFO] - Training Epoch: 1/2, step 8238/23838 completed (loss: 0.6217071413993835, acc: 0.7916666865348816)
[2025-02-16 12:27:27,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:27,274][root][INFO] - Training Epoch: 1/2, step 8239/23838 completed (loss: 0.48224443197250366, acc: 0.844660222530365)
[2025-02-16 12:27:27,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:27,690][root][INFO] - Training Epoch: 1/2, step 8240/23838 completed (loss: 0.4751870334148407, acc: 0.8472222089767456)
[2025-02-16 12:27:27,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:28,108][root][INFO] - Training Epoch: 1/2, step 8241/23838 completed (loss: 0.8403890132904053, acc: 0.7848101258277893)
[2025-02-16 12:27:28,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:28,572][root][INFO] - Training Epoch: 1/2, step 8242/23838 completed (loss: 0.68669593334198, acc: 0.8133333325386047)
[2025-02-16 12:27:28,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:29,025][root][INFO] - Training Epoch: 1/2, step 8243/23838 completed (loss: 0.36258867383003235, acc: 0.9142857193946838)
[2025-02-16 12:27:29,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:29,423][root][INFO] - Training Epoch: 1/2, step 8244/23838 completed (loss: 0.6493420004844666, acc: 0.8053691387176514)
[2025-02-16 12:27:29,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:29,810][root][INFO] - Training Epoch: 1/2, step 8245/23838 completed (loss: 0.783283531665802, acc: 0.7745097875595093)
[2025-02-16 12:27:29,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:30,241][root][INFO] - Training Epoch: 1/2, step 8246/23838 completed (loss: 0.7139558792114258, acc: 0.8656716346740723)
[2025-02-16 12:27:30,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:30,647][root][INFO] - Training Epoch: 1/2, step 8247/23838 completed (loss: 0.5795576572418213, acc: 0.8333333134651184)
[2025-02-16 12:27:30,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:31,065][root][INFO] - Training Epoch: 1/2, step 8248/23838 completed (loss: 0.5156950950622559, acc: 0.8550724387168884)
[2025-02-16 12:27:31,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:31,484][root][INFO] - Training Epoch: 1/2, step 8249/23838 completed (loss: 0.6263774037361145, acc: 0.8117647171020508)
[2025-02-16 12:27:31,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:31,866][root][INFO] - Training Epoch: 1/2, step 8250/23838 completed (loss: 0.8854620456695557, acc: 0.7142857313156128)
[2025-02-16 12:27:32,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:32,314][root][INFO] - Training Epoch: 1/2, step 8251/23838 completed (loss: 0.5337350368499756, acc: 0.8571428656578064)
[2025-02-16 12:27:32,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:32,716][root][INFO] - Training Epoch: 1/2, step 8252/23838 completed (loss: 0.7455301284790039, acc: 0.8333333134651184)
[2025-02-16 12:27:32,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:33,163][root][INFO] - Training Epoch: 1/2, step 8253/23838 completed (loss: 0.3535928428173065, acc: 0.9126213788986206)
[2025-02-16 12:27:33,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:33,624][root][INFO] - Training Epoch: 1/2, step 8254/23838 completed (loss: 0.4816446900367737, acc: 0.859649121761322)
[2025-02-16 12:27:33,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:34,029][root][INFO] - Training Epoch: 1/2, step 8255/23838 completed (loss: 1.1900449991226196, acc: 0.6666666865348816)
[2025-02-16 12:27:34,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:34,460][root][INFO] - Training Epoch: 1/2, step 8256/23838 completed (loss: 0.7264989614486694, acc: 0.8255813717842102)
[2025-02-16 12:27:34,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:34,873][root][INFO] - Training Epoch: 1/2, step 8257/23838 completed (loss: 0.4408828616142273, acc: 0.8904109597206116)
[2025-02-16 12:27:35,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:35,320][root][INFO] - Training Epoch: 1/2, step 8258/23838 completed (loss: 0.4546082615852356, acc: 0.8536585569381714)
[2025-02-16 12:27:35,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:35,718][root][INFO] - Training Epoch: 1/2, step 8259/23838 completed (loss: 0.639713704586029, acc: 0.8615384697914124)
[2025-02-16 12:27:35,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:36,110][root][INFO] - Training Epoch: 1/2, step 8260/23838 completed (loss: 0.2819175124168396, acc: 0.9285714030265808)
[2025-02-16 12:27:36,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:36,483][root][INFO] - Training Epoch: 1/2, step 8261/23838 completed (loss: 0.5361997485160828, acc: 0.807692289352417)
[2025-02-16 12:27:36,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:36,922][root][INFO] - Training Epoch: 1/2, step 8262/23838 completed (loss: 0.8203311562538147, acc: 0.796875)
[2025-02-16 12:27:37,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:37,309][root][INFO] - Training Epoch: 1/2, step 8263/23838 completed (loss: 0.34018179774284363, acc: 0.8999999761581421)
[2025-02-16 12:27:37,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:37,793][root][INFO] - Training Epoch: 1/2, step 8264/23838 completed (loss: 0.4354344606399536, acc: 0.8600000143051147)
[2025-02-16 12:27:37,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:38,187][root][INFO] - Training Epoch: 1/2, step 8265/23838 completed (loss: 0.5015326738357544, acc: 0.8679245114326477)
[2025-02-16 12:27:38,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:38,548][root][INFO] - Training Epoch: 1/2, step 8266/23838 completed (loss: 0.35518258810043335, acc: 0.8999999761581421)
[2025-02-16 12:27:38,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:38,925][root][INFO] - Training Epoch: 1/2, step 8267/23838 completed (loss: 0.21885274350643158, acc: 0.9473684430122375)
[2025-02-16 12:27:39,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:39,278][root][INFO] - Training Epoch: 1/2, step 8268/23838 completed (loss: 0.8388897180557251, acc: 0.790123462677002)
[2025-02-16 12:27:39,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:39,637][root][INFO] - Training Epoch: 1/2, step 8269/23838 completed (loss: 0.699129045009613, acc: 0.7777777910232544)
[2025-02-16 12:27:39,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:40,049][root][INFO] - Training Epoch: 1/2, step 8270/23838 completed (loss: 0.6373903751373291, acc: 0.8032786846160889)
[2025-02-16 12:27:40,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:40,430][root][INFO] - Training Epoch: 1/2, step 8271/23838 completed (loss: 0.4475068151950836, acc: 0.8807339668273926)
[2025-02-16 12:27:40,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:40,824][root][INFO] - Training Epoch: 1/2, step 8272/23838 completed (loss: 0.5195692181587219, acc: 0.8349514603614807)
[2025-02-16 12:27:41,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:41,250][root][INFO] - Training Epoch: 1/2, step 8273/23838 completed (loss: 0.3333338797092438, acc: 0.9322034120559692)
[2025-02-16 12:27:41,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:41,721][root][INFO] - Training Epoch: 1/2, step 8274/23838 completed (loss: 0.4883400499820709, acc: 0.8958333134651184)
[2025-02-16 12:27:41,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:42,160][root][INFO] - Training Epoch: 1/2, step 8275/23838 completed (loss: 0.6661184430122375, acc: 0.8275862336158752)
[2025-02-16 12:27:42,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:42,586][root][INFO] - Training Epoch: 1/2, step 8276/23838 completed (loss: 0.7073233127593994, acc: 0.8194444179534912)
[2025-02-16 12:27:42,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:43,031][root][INFO] - Training Epoch: 1/2, step 8277/23838 completed (loss: 0.35802316665649414, acc: 0.875)
[2025-02-16 12:27:43,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:43,413][root][INFO] - Training Epoch: 1/2, step 8278/23838 completed (loss: 0.8844084143638611, acc: 0.8199999928474426)
[2025-02-16 12:27:43,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:43,794][root][INFO] - Training Epoch: 1/2, step 8279/23838 completed (loss: 0.4192098379135132, acc: 0.9215686321258545)
[2025-02-16 12:27:43,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:44,234][root][INFO] - Training Epoch: 1/2, step 8280/23838 completed (loss: 0.5470463633537292, acc: 0.8701298832893372)
[2025-02-16 12:27:44,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:44,641][root][INFO] - Training Epoch: 1/2, step 8281/23838 completed (loss: 0.4262763559818268, acc: 0.8809523582458496)
[2025-02-16 12:27:44,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:45,043][root][INFO] - Training Epoch: 1/2, step 8282/23838 completed (loss: 0.33254337310791016, acc: 0.9158878326416016)
[2025-02-16 12:27:45,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:45,467][root][INFO] - Training Epoch: 1/2, step 8283/23838 completed (loss: 0.2311510294675827, acc: 0.9333333373069763)
[2025-02-16 12:27:45,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:45,872][root][INFO] - Training Epoch: 1/2, step 8284/23838 completed (loss: 0.20543614029884338, acc: 0.9615384340286255)
[2025-02-16 12:27:46,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:46,254][root][INFO] - Training Epoch: 1/2, step 8285/23838 completed (loss: 0.3348206877708435, acc: 0.9175257682800293)
[2025-02-16 12:27:46,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:46,655][root][INFO] - Training Epoch: 1/2, step 8286/23838 completed (loss: 0.5507654547691345, acc: 0.8636363744735718)
[2025-02-16 12:27:46,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:47,075][root][INFO] - Training Epoch: 1/2, step 8287/23838 completed (loss: 0.3957098126411438, acc: 0.8888888955116272)
[2025-02-16 12:27:47,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:47,528][root][INFO] - Training Epoch: 1/2, step 8288/23838 completed (loss: 0.2812654674053192, acc: 0.9545454382896423)
[2025-02-16 12:27:47,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:47,991][root][INFO] - Training Epoch: 1/2, step 8289/23838 completed (loss: 0.34266048669815063, acc: 0.8999999761581421)
[2025-02-16 12:27:48,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:48,437][root][INFO] - Training Epoch: 1/2, step 8290/23838 completed (loss: 0.43870827555656433, acc: 0.8494623899459839)
[2025-02-16 12:27:48,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:48,807][root][INFO] - Training Epoch: 1/2, step 8291/23838 completed (loss: 0.5019150972366333, acc: 0.8500000238418579)
[2025-02-16 12:27:48,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:49,187][root][INFO] - Training Epoch: 1/2, step 8292/23838 completed (loss: 0.6605290174484253, acc: 0.7833333611488342)
[2025-02-16 12:27:49,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:49,606][root][INFO] - Training Epoch: 1/2, step 8293/23838 completed (loss: 0.46128249168395996, acc: 0.8947368264198303)
[2025-02-16 12:27:49,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:50,064][root][INFO] - Training Epoch: 1/2, step 8294/23838 completed (loss: 0.3626837730407715, acc: 0.9174311757087708)
[2025-02-16 12:27:50,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:50,474][root][INFO] - Training Epoch: 1/2, step 8295/23838 completed (loss: 0.3712896704673767, acc: 0.9120000004768372)
[2025-02-16 12:27:50,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:50,896][root][INFO] - Training Epoch: 1/2, step 8296/23838 completed (loss: 0.3428763449192047, acc: 0.9019607901573181)
[2025-02-16 12:27:51,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:51,371][root][INFO] - Training Epoch: 1/2, step 8297/23838 completed (loss: 0.3827208876609802, acc: 0.9026548862457275)
[2025-02-16 12:27:51,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:51,806][root][INFO] - Training Epoch: 1/2, step 8298/23838 completed (loss: 0.378805547952652, acc: 0.8931297659873962)
[2025-02-16 12:27:51,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:52,253][root][INFO] - Training Epoch: 1/2, step 8299/23838 completed (loss: 0.2428622990846634, acc: 0.9444444179534912)
[2025-02-16 12:27:52,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:52,661][root][INFO] - Training Epoch: 1/2, step 8300/23838 completed (loss: 0.39344894886016846, acc: 0.9142857193946838)
[2025-02-16 12:27:52,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:53,050][root][INFO] - Training Epoch: 1/2, step 8301/23838 completed (loss: 0.16092783212661743, acc: 0.9604519605636597)
[2025-02-16 12:27:53,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:53,487][root][INFO] - Training Epoch: 1/2, step 8302/23838 completed (loss: 0.41883212327957153, acc: 0.891566276550293)
[2025-02-16 12:27:53,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:53,858][root][INFO] - Training Epoch: 1/2, step 8303/23838 completed (loss: 0.598068356513977, acc: 0.862500011920929)
[2025-02-16 12:27:54,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:54,265][root][INFO] - Training Epoch: 1/2, step 8304/23838 completed (loss: 0.4466819763183594, acc: 0.8941176533699036)
[2025-02-16 12:27:54,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:54,666][root][INFO] - Training Epoch: 1/2, step 8305/23838 completed (loss: 0.553522527217865, acc: 0.8701298832893372)
[2025-02-16 12:27:54,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:55,018][root][INFO] - Training Epoch: 1/2, step 8306/23838 completed (loss: 0.20144854485988617, acc: 0.9324324131011963)
[2025-02-16 12:27:55,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:55,425][root][INFO] - Training Epoch: 1/2, step 8307/23838 completed (loss: 0.15265855193138123, acc: 0.9624999761581421)
[2025-02-16 12:27:55,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:55,832][root][INFO] - Training Epoch: 1/2, step 8308/23838 completed (loss: 0.8129706978797913, acc: 0.804347813129425)
[2025-02-16 12:27:56,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:56,303][root][INFO] - Training Epoch: 1/2, step 8309/23838 completed (loss: 0.4860778748989105, acc: 0.8811880946159363)
[2025-02-16 12:27:56,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:56,687][root][INFO] - Training Epoch: 1/2, step 8310/23838 completed (loss: 0.22479471564292908, acc: 0.9468085169792175)
[2025-02-16 12:27:56,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:57,140][root][INFO] - Training Epoch: 1/2, step 8311/23838 completed (loss: 0.299823135137558, acc: 0.8783783912658691)
[2025-02-16 12:27:57,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:57,546][root][INFO] - Training Epoch: 1/2, step 8312/23838 completed (loss: 0.5458008646965027, acc: 0.8571428656578064)
[2025-02-16 12:27:57,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:57,951][root][INFO] - Training Epoch: 1/2, step 8313/23838 completed (loss: 0.4167977273464203, acc: 0.8522727489471436)
[2025-02-16 12:27:58,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:58,377][root][INFO] - Training Epoch: 1/2, step 8314/23838 completed (loss: 0.2946619987487793, acc: 0.868852436542511)
[2025-02-16 12:27:58,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:58,764][root][INFO] - Training Epoch: 1/2, step 8315/23838 completed (loss: 0.38257846236228943, acc: 0.9054054021835327)
[2025-02-16 12:27:58,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:59,200][root][INFO] - Training Epoch: 1/2, step 8316/23838 completed (loss: 0.5672299265861511, acc: 0.8292682766914368)
[2025-02-16 12:27:59,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:59,575][root][INFO] - Training Epoch: 1/2, step 8317/23838 completed (loss: 0.3116704523563385, acc: 0.9230769276618958)
[2025-02-16 12:27:59,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:27:59,936][root][INFO] - Training Epoch: 1/2, step 8318/23838 completed (loss: 0.6157315373420715, acc: 0.859649121761322)
[2025-02-16 12:28:00,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:00,366][root][INFO] - Training Epoch: 1/2, step 8319/23838 completed (loss: 0.5275351405143738, acc: 0.8461538553237915)
[2025-02-16 12:28:00,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:00,817][root][INFO] - Training Epoch: 1/2, step 8320/23838 completed (loss: 0.2222997397184372, acc: 0.9304347634315491)
[2025-02-16 12:28:00,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:01,238][root][INFO] - Training Epoch: 1/2, step 8321/23838 completed (loss: 0.4252305030822754, acc: 0.8759689927101135)
[2025-02-16 12:28:01,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:01,678][root][INFO] - Training Epoch: 1/2, step 8322/23838 completed (loss: 0.09870869666337967, acc: 0.9634146094322205)
[2025-02-16 12:28:01,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:02,056][root][INFO] - Training Epoch: 1/2, step 8323/23838 completed (loss: 0.2539668381214142, acc: 0.9431818127632141)
[2025-02-16 12:28:02,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:02,495][root][INFO] - Training Epoch: 1/2, step 8324/23838 completed (loss: 0.3059839606285095, acc: 0.9111111164093018)
[2025-02-16 12:28:02,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:02,916][root][INFO] - Training Epoch: 1/2, step 8325/23838 completed (loss: 0.5116720199584961, acc: 0.8977272510528564)
[2025-02-16 12:28:03,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:03,386][root][INFO] - Training Epoch: 1/2, step 8326/23838 completed (loss: 0.3796410858631134, acc: 0.8983050584793091)
[2025-02-16 12:28:03,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:03,782][root][INFO] - Training Epoch: 1/2, step 8327/23838 completed (loss: 0.6594475507736206, acc: 0.8717948794364929)
[2025-02-16 12:28:03,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:04,225][root][INFO] - Training Epoch: 1/2, step 8328/23838 completed (loss: 0.37898048758506775, acc: 0.859375)
[2025-02-16 12:28:04,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:04,638][root][INFO] - Training Epoch: 1/2, step 8329/23838 completed (loss: 0.44729775190353394, acc: 0.8620689511299133)
[2025-02-16 12:28:04,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:05,020][root][INFO] - Training Epoch: 1/2, step 8330/23838 completed (loss: 0.28194862604141235, acc: 0.9861111044883728)
[2025-02-16 12:28:05,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:05,456][root][INFO] - Training Epoch: 1/2, step 8331/23838 completed (loss: 0.296425461769104, acc: 0.9200000166893005)
[2025-02-16 12:28:05,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:05,930][root][INFO] - Training Epoch: 1/2, step 8332/23838 completed (loss: 0.3488727807998657, acc: 0.8782608509063721)
[2025-02-16 12:28:06,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:06,353][root][INFO] - Training Epoch: 1/2, step 8333/23838 completed (loss: 0.7178598046302795, acc: 0.8292682766914368)
[2025-02-16 12:28:06,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:06,730][root][INFO] - Training Epoch: 1/2, step 8334/23838 completed (loss: 0.4036650061607361, acc: 0.8870967626571655)
[2025-02-16 12:28:06,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:07,187][root][INFO] - Training Epoch: 1/2, step 8335/23838 completed (loss: 0.7689000964164734, acc: 0.853210985660553)
[2025-02-16 12:28:07,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:07,775][root][INFO] - Training Epoch: 1/2, step 8336/23838 completed (loss: 0.33995217084884644, acc: 0.9032257795333862)
[2025-02-16 12:28:08,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:08,274][root][INFO] - Training Epoch: 1/2, step 8337/23838 completed (loss: 0.42414024472236633, acc: 0.8554216623306274)
[2025-02-16 12:28:08,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:08,740][root][INFO] - Training Epoch: 1/2, step 8338/23838 completed (loss: 0.28113293647766113, acc: 0.918181836605072)
[2025-02-16 12:28:08,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:09,199][root][INFO] - Training Epoch: 1/2, step 8339/23838 completed (loss: 0.3637513816356659, acc: 0.8888888955116272)
[2025-02-16 12:28:09,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:09,635][root][INFO] - Training Epoch: 1/2, step 8340/23838 completed (loss: 0.32725808024406433, acc: 0.8823529481887817)
[2025-02-16 12:28:09,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:10,057][root][INFO] - Training Epoch: 1/2, step 8341/23838 completed (loss: 0.3354932367801666, acc: 0.9156626462936401)
[2025-02-16 12:28:10,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:10,488][root][INFO] - Training Epoch: 1/2, step 8342/23838 completed (loss: 0.236337348818779, acc: 0.932692289352417)
[2025-02-16 12:28:10,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:10,903][root][INFO] - Training Epoch: 1/2, step 8343/23838 completed (loss: 0.36599111557006836, acc: 0.9204545617103577)
[2025-02-16 12:28:11,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:11,279][root][INFO] - Training Epoch: 1/2, step 8344/23838 completed (loss: 0.6877413988113403, acc: 0.797468364238739)
[2025-02-16 12:28:11,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:11,675][root][INFO] - Training Epoch: 1/2, step 8345/23838 completed (loss: 0.41257670521736145, acc: 0.8674699068069458)
[2025-02-16 12:28:11,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:12,131][root][INFO] - Training Epoch: 1/2, step 8346/23838 completed (loss: 0.6090973019599915, acc: 0.8352941274642944)
[2025-02-16 12:28:12,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:12,541][root][INFO] - Training Epoch: 1/2, step 8347/23838 completed (loss: 0.9691848158836365, acc: 0.7450980544090271)
[2025-02-16 12:28:12,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:12,968][root][INFO] - Training Epoch: 1/2, step 8348/23838 completed (loss: 0.2507953643798828, acc: 0.9240506291389465)
[2025-02-16 12:28:13,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:13,388][root][INFO] - Training Epoch: 1/2, step 8349/23838 completed (loss: 0.3293692171573639, acc: 0.9262295365333557)
[2025-02-16 12:28:13,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:13,815][root][INFO] - Training Epoch: 1/2, step 8350/23838 completed (loss: 0.4992457926273346, acc: 0.9027777910232544)
[2025-02-16 12:28:13,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:14,201][root][INFO] - Training Epoch: 1/2, step 8351/23838 completed (loss: 0.29715341329574585, acc: 0.9166666865348816)
[2025-02-16 12:28:14,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:14,605][root][INFO] - Training Epoch: 1/2, step 8352/23838 completed (loss: 0.5421257019042969, acc: 0.8360655903816223)
[2025-02-16 12:28:14,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:15,046][root][INFO] - Training Epoch: 1/2, step 8353/23838 completed (loss: 0.23828928172588348, acc: 0.9369369149208069)
[2025-02-16 12:28:15,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:15,491][root][INFO] - Training Epoch: 1/2, step 8354/23838 completed (loss: 0.8295889496803284, acc: 0.7758620977401733)
[2025-02-16 12:28:15,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:15,911][root][INFO] - Training Epoch: 1/2, step 8355/23838 completed (loss: 0.9012055993080139, acc: 0.875)
[2025-02-16 12:28:16,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:16,372][root][INFO] - Training Epoch: 1/2, step 8356/23838 completed (loss: 0.3128104507923126, acc: 0.9230769276618958)
[2025-02-16 12:28:16,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:16,775][root][INFO] - Training Epoch: 1/2, step 8357/23838 completed (loss: 0.5825048685073853, acc: 0.8360655903816223)
[2025-02-16 12:28:16,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:17,204][root][INFO] - Training Epoch: 1/2, step 8358/23838 completed (loss: 0.580437183380127, acc: 0.8157894611358643)
[2025-02-16 12:28:17,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:17,613][root][INFO] - Training Epoch: 1/2, step 8359/23838 completed (loss: 0.24377860128879547, acc: 0.9375)
[2025-02-16 12:28:17,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:18,055][root][INFO] - Training Epoch: 1/2, step 8360/23838 completed (loss: 0.34586864709854126, acc: 0.9213483333587646)
[2025-02-16 12:28:18,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:18,481][root][INFO] - Training Epoch: 1/2, step 8361/23838 completed (loss: 0.7079353332519531, acc: 0.7924528121948242)
[2025-02-16 12:28:18,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:18,920][root][INFO] - Training Epoch: 1/2, step 8362/23838 completed (loss: 0.4243414103984833, acc: 0.8666666746139526)
[2025-02-16 12:28:19,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:19,365][root][INFO] - Training Epoch: 1/2, step 8363/23838 completed (loss: 0.2038966864347458, acc: 0.9611650705337524)
[2025-02-16 12:28:19,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:19,732][root][INFO] - Training Epoch: 1/2, step 8364/23838 completed (loss: 0.454618901014328, acc: 0.869918704032898)
[2025-02-16 12:28:19,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:20,128][root][INFO] - Training Epoch: 1/2, step 8365/23838 completed (loss: 0.47811785340309143, acc: 0.8888888955116272)
[2025-02-16 12:28:20,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:20,528][root][INFO] - Training Epoch: 1/2, step 8366/23838 completed (loss: 0.3474162518978119, acc: 0.9220778942108154)
[2025-02-16 12:28:20,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:20,973][root][INFO] - Training Epoch: 1/2, step 8367/23838 completed (loss: 0.4658593535423279, acc: 0.8392857313156128)
[2025-02-16 12:28:21,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:21,379][root][INFO] - Training Epoch: 1/2, step 8368/23838 completed (loss: 0.42751580476760864, acc: 0.9019607901573181)
[2025-02-16 12:28:21,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:21,778][root][INFO] - Training Epoch: 1/2, step 8369/23838 completed (loss: 0.3759574890136719, acc: 0.90625)
[2025-02-16 12:28:21,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:22,202][root][INFO] - Training Epoch: 1/2, step 8370/23838 completed (loss: 0.44017353653907776, acc: 0.9027777910232544)
[2025-02-16 12:28:22,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:22,589][root][INFO] - Training Epoch: 1/2, step 8371/23838 completed (loss: 0.4426088035106659, acc: 0.8863636255264282)
[2025-02-16 12:28:22,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:23,084][root][INFO] - Training Epoch: 1/2, step 8372/23838 completed (loss: 0.24147894978523254, acc: 0.921875)
[2025-02-16 12:28:23,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:23,529][root][INFO] - Training Epoch: 1/2, step 8373/23838 completed (loss: 0.2825350761413574, acc: 0.9200000166893005)
[2025-02-16 12:28:23,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:23,983][root][INFO] - Training Epoch: 1/2, step 8374/23838 completed (loss: 0.25124090909957886, acc: 0.949367105960846)
[2025-02-16 12:28:24,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:24,433][root][INFO] - Training Epoch: 1/2, step 8375/23838 completed (loss: 0.13808009028434753, acc: 0.9541984796524048)
[2025-02-16 12:28:24,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:24,873][root][INFO] - Training Epoch: 1/2, step 8376/23838 completed (loss: 0.3540436923503876, acc: 0.8999999761581421)
[2025-02-16 12:28:25,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:25,290][root][INFO] - Training Epoch: 1/2, step 8377/23838 completed (loss: 0.1831461787223816, acc: 0.9636363387107849)
[2025-02-16 12:28:25,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:25,669][root][INFO] - Training Epoch: 1/2, step 8378/23838 completed (loss: 0.3684590756893158, acc: 0.9263157844543457)
[2025-02-16 12:28:25,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:26,080][root][INFO] - Training Epoch: 1/2, step 8379/23838 completed (loss: 0.21189992129802704, acc: 0.9420289993286133)
[2025-02-16 12:28:26,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:26,535][root][INFO] - Training Epoch: 1/2, step 8380/23838 completed (loss: 0.3311395049095154, acc: 0.9097744226455688)
[2025-02-16 12:28:26,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:26,976][root][INFO] - Training Epoch: 1/2, step 8381/23838 completed (loss: 0.4608567953109741, acc: 0.9081632494926453)
[2025-02-16 12:28:27,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:27,439][root][INFO] - Training Epoch: 1/2, step 8382/23838 completed (loss: 0.6715819835662842, acc: 0.8500000238418579)
[2025-02-16 12:28:28,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:28,541][root][INFO] - Training Epoch: 1/2, step 8383/23838 completed (loss: 0.32556620240211487, acc: 0.9365079402923584)
[2025-02-16 12:28:28,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:29,054][root][INFO] - Training Epoch: 1/2, step 8384/23838 completed (loss: 0.3005882501602173, acc: 0.9181286692619324)
[2025-02-16 12:28:29,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:29,505][root][INFO] - Training Epoch: 1/2, step 8385/23838 completed (loss: 0.4052543640136719, acc: 0.8735632300376892)
[2025-02-16 12:28:29,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:29,926][root][INFO] - Training Epoch: 1/2, step 8386/23838 completed (loss: 0.2674919664859772, acc: 0.9189189076423645)
[2025-02-16 12:28:30,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:30,430][root][INFO] - Training Epoch: 1/2, step 8387/23838 completed (loss: 0.45951518416404724, acc: 0.881118893623352)
[2025-02-16 12:28:30,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:30,865][root][INFO] - Training Epoch: 1/2, step 8388/23838 completed (loss: 0.47159409523010254, acc: 0.8933333158493042)
[2025-02-16 12:28:31,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:31,333][root][INFO] - Training Epoch: 1/2, step 8389/23838 completed (loss: 0.4613441228866577, acc: 0.9035087823867798)
[2025-02-16 12:28:31,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:31,747][root][INFO] - Training Epoch: 1/2, step 8390/23838 completed (loss: 0.08726415038108826, acc: 0.9846153855323792)
[2025-02-16 12:28:31,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:32,152][root][INFO] - Training Epoch: 1/2, step 8391/23838 completed (loss: 0.16748879849910736, acc: 0.9722222089767456)
[2025-02-16 12:28:32,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:32,610][root][INFO] - Training Epoch: 1/2, step 8392/23838 completed (loss: 1.6873608827590942, acc: 0.5357142686843872)
[2025-02-16 12:28:32,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:33,030][root][INFO] - Training Epoch: 1/2, step 8393/23838 completed (loss: 0.42185279726982117, acc: 0.875)
[2025-02-16 12:28:33,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:33,457][root][INFO] - Training Epoch: 1/2, step 8394/23838 completed (loss: 0.1788187175989151, acc: 0.942148745059967)
[2025-02-16 12:28:33,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:33,878][root][INFO] - Training Epoch: 1/2, step 8395/23838 completed (loss: 0.36128950119018555, acc: 0.9102563858032227)
[2025-02-16 12:28:34,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:34,296][root][INFO] - Training Epoch: 1/2, step 8396/23838 completed (loss: 0.6271766424179077, acc: 0.8181818127632141)
[2025-02-16 12:28:34,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:34,729][root][INFO] - Training Epoch: 1/2, step 8397/23838 completed (loss: 0.4031681418418884, acc: 0.9055117964744568)
[2025-02-16 12:28:34,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:35,126][root][INFO] - Training Epoch: 1/2, step 8398/23838 completed (loss: 0.4608430862426758, acc: 0.8961039185523987)
[2025-02-16 12:28:35,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:35,576][root][INFO] - Training Epoch: 1/2, step 8399/23838 completed (loss: 0.4108780324459076, acc: 0.8796296119689941)
[2025-02-16 12:28:35,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:35,988][root][INFO] - Training Epoch: 1/2, step 8400/23838 completed (loss: 0.29084616899490356, acc: 0.8999999761581421)
[2025-02-16 12:28:36,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:36,412][root][INFO] - Training Epoch: 1/2, step 8401/23838 completed (loss: 0.6234598159790039, acc: 0.8309859037399292)
[2025-02-16 12:28:36,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:36,868][root][INFO] - Training Epoch: 1/2, step 8402/23838 completed (loss: 0.6182287931442261, acc: 0.8055555820465088)
[2025-02-16 12:28:37,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:37,302][root][INFO] - Training Epoch: 1/2, step 8403/23838 completed (loss: 0.35687917470932007, acc: 0.9191918969154358)
[2025-02-16 12:28:37,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:37,671][root][INFO] - Training Epoch: 1/2, step 8404/23838 completed (loss: 0.7037680745124817, acc: 0.7878788113594055)
[2025-02-16 12:28:37,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:38,109][root][INFO] - Training Epoch: 1/2, step 8405/23838 completed (loss: 0.2035689651966095, acc: 0.9338235259056091)
[2025-02-16 12:28:38,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:38,542][root][INFO] - Training Epoch: 1/2, step 8406/23838 completed (loss: 0.5445589423179626, acc: 0.8918918967247009)
[2025-02-16 12:28:38,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:38,936][root][INFO] - Training Epoch: 1/2, step 8407/23838 completed (loss: 0.5323743224143982, acc: 0.8630136847496033)
[2025-02-16 12:28:39,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:39,434][root][INFO] - Training Epoch: 1/2, step 8408/23838 completed (loss: 0.33195847272872925, acc: 0.9277108311653137)
[2025-02-16 12:28:39,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:39,926][root][INFO] - Training Epoch: 1/2, step 8409/23838 completed (loss: 0.1895170360803604, acc: 0.9435483813285828)
[2025-02-16 12:28:40,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:40,333][root][INFO] - Training Epoch: 1/2, step 8410/23838 completed (loss: 0.2930513024330139, acc: 0.9263157844543457)
[2025-02-16 12:28:40,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:40,793][root][INFO] - Training Epoch: 1/2, step 8411/23838 completed (loss: 0.2981225550174713, acc: 0.9270833134651184)
[2025-02-16 12:28:40,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:41,219][root][INFO] - Training Epoch: 1/2, step 8412/23838 completed (loss: 0.40695154666900635, acc: 0.8817204236984253)
[2025-02-16 12:28:41,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:41,686][root][INFO] - Training Epoch: 1/2, step 8413/23838 completed (loss: 0.4379650056362152, acc: 0.9009901285171509)
[2025-02-16 12:28:41,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:42,036][root][INFO] - Training Epoch: 1/2, step 8414/23838 completed (loss: 0.32964271306991577, acc: 0.9152542352676392)
[2025-02-16 12:28:42,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:42,487][root][INFO] - Training Epoch: 1/2, step 8415/23838 completed (loss: 0.6109139919281006, acc: 0.8526315689086914)
[2025-02-16 12:28:42,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:42,954][root][INFO] - Training Epoch: 1/2, step 8416/23838 completed (loss: 0.3320023715496063, acc: 0.9327731132507324)
[2025-02-16 12:28:43,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:43,388][root][INFO] - Training Epoch: 1/2, step 8417/23838 completed (loss: 0.2598525583744049, acc: 0.9099099040031433)
[2025-02-16 12:28:43,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:43,781][root][INFO] - Training Epoch: 1/2, step 8418/23838 completed (loss: 0.3415206968784332, acc: 0.8931297659873962)
[2025-02-16 12:28:44,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:44,624][root][INFO] - Training Epoch: 1/2, step 8419/23838 completed (loss: 0.5275366902351379, acc: 0.8658536672592163)
[2025-02-16 12:28:44,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:45,078][root][INFO] - Training Epoch: 1/2, step 8420/23838 completed (loss: 0.4868921935558319, acc: 0.8103448152542114)
[2025-02-16 12:28:45,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:45,538][root][INFO] - Training Epoch: 1/2, step 8421/23838 completed (loss: 0.3580182194709778, acc: 0.891566276550293)
[2025-02-16 12:28:45,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:45,951][root][INFO] - Training Epoch: 1/2, step 8422/23838 completed (loss: 0.6113227009773254, acc: 0.8405796885490417)
[2025-02-16 12:28:46,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:46,460][root][INFO] - Training Epoch: 1/2, step 8423/23838 completed (loss: 0.3784688413143158, acc: 0.891566276550293)
[2025-02-16 12:28:46,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:46,896][root][INFO] - Training Epoch: 1/2, step 8424/23838 completed (loss: 0.26286181807518005, acc: 0.9363636374473572)
[2025-02-16 12:28:47,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:47,283][root][INFO] - Training Epoch: 1/2, step 8425/23838 completed (loss: 0.2781513035297394, acc: 0.9279279112815857)
[2025-02-16 12:28:47,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:47,763][root][INFO] - Training Epoch: 1/2, step 8426/23838 completed (loss: 0.8693196773529053, acc: 0.795918345451355)
[2025-02-16 12:28:47,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:48,206][root][INFO] - Training Epoch: 1/2, step 8427/23838 completed (loss: 0.1343882977962494, acc: 0.96875)
[2025-02-16 12:28:48,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:48,690][root][INFO] - Training Epoch: 1/2, step 8428/23838 completed (loss: 0.31093209981918335, acc: 0.9404761791229248)
[2025-02-16 12:28:48,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:49,135][root][INFO] - Training Epoch: 1/2, step 8429/23838 completed (loss: 0.22612813115119934, acc: 0.9404761791229248)
[2025-02-16 12:28:49,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:49,574][root][INFO] - Training Epoch: 1/2, step 8430/23838 completed (loss: 0.45548608899116516, acc: 0.8846153616905212)
[2025-02-16 12:28:49,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:50,012][root][INFO] - Training Epoch: 1/2, step 8431/23838 completed (loss: 0.9186248183250427, acc: 0.7890625)
[2025-02-16 12:28:50,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:50,456][root][INFO] - Training Epoch: 1/2, step 8432/23838 completed (loss: 0.2554641664028168, acc: 0.9270833134651184)
[2025-02-16 12:28:50,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:50,999][root][INFO] - Training Epoch: 1/2, step 8433/23838 completed (loss: 0.7942690849304199, acc: 0.8061224222183228)
[2025-02-16 12:28:51,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:51,383][root][INFO] - Training Epoch: 1/2, step 8434/23838 completed (loss: 0.24496488273143768, acc: 0.932584285736084)
[2025-02-16 12:28:51,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:51,867][root][INFO] - Training Epoch: 1/2, step 8435/23838 completed (loss: 0.6628577709197998, acc: 0.8225806355476379)
[2025-02-16 12:28:52,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:52,324][root][INFO] - Training Epoch: 1/2, step 8436/23838 completed (loss: 0.33955642580986023, acc: 0.8958333134651184)
[2025-02-16 12:28:52,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:52,808][root][INFO] - Training Epoch: 1/2, step 8437/23838 completed (loss: 0.2465672791004181, acc: 0.9482758641242981)
[2025-02-16 12:28:53,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:53,256][root][INFO] - Training Epoch: 1/2, step 8438/23838 completed (loss: 0.35916680097579956, acc: 0.8947368264198303)
[2025-02-16 12:28:53,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:53,673][root][INFO] - Training Epoch: 1/2, step 8439/23838 completed (loss: 0.39931297302246094, acc: 0.8653846383094788)
[2025-02-16 12:28:53,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:54,096][root][INFO] - Training Epoch: 1/2, step 8440/23838 completed (loss: 0.12563639879226685, acc: 0.9696969985961914)
[2025-02-16 12:28:54,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:54,637][root][INFO] - Training Epoch: 1/2, step 8441/23838 completed (loss: 0.9829917550086975, acc: 0.7431192398071289)
[2025-02-16 12:28:54,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:55,032][root][INFO] - Training Epoch: 1/2, step 8442/23838 completed (loss: 0.3682389557361603, acc: 0.9038461446762085)
[2025-02-16 12:28:55,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:55,580][root][INFO] - Training Epoch: 1/2, step 8443/23838 completed (loss: 0.5074238181114197, acc: 0.8873239159584045)
[2025-02-16 12:28:55,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:56,045][root][INFO] - Training Epoch: 1/2, step 8444/23838 completed (loss: 1.0763317346572876, acc: 0.7317073345184326)
[2025-02-16 12:28:56,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:56,533][root][INFO] - Training Epoch: 1/2, step 8445/23838 completed (loss: 0.3301682770252228, acc: 0.9166666865348816)
[2025-02-16 12:28:56,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:57,063][root][INFO] - Training Epoch: 1/2, step 8446/23838 completed (loss: 0.42625531554222107, acc: 0.859649121761322)
[2025-02-16 12:28:57,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:57,497][root][INFO] - Training Epoch: 1/2, step 8447/23838 completed (loss: 0.5773534178733826, acc: 0.8269230723381042)
[2025-02-16 12:28:57,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:58,018][root][INFO] - Training Epoch: 1/2, step 8448/23838 completed (loss: 1.1750003099441528, acc: 0.6744186282157898)
[2025-02-16 12:28:58,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:58,450][root][INFO] - Training Epoch: 1/2, step 8449/23838 completed (loss: 0.41700130701065063, acc: 0.8817204236984253)
[2025-02-16 12:28:58,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:58,877][root][INFO] - Training Epoch: 1/2, step 8450/23838 completed (loss: 0.22515453398227692, acc: 0.9340659379959106)
[2025-02-16 12:28:59,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:59,371][root][INFO] - Training Epoch: 1/2, step 8451/23838 completed (loss: 0.4151555895805359, acc: 0.9090909361839294)
[2025-02-16 12:28:59,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:28:59,767][root][INFO] - Training Epoch: 1/2, step 8452/23838 completed (loss: 0.37221813201904297, acc: 0.8999999761581421)
[2025-02-16 12:28:59,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:00,202][root][INFO] - Training Epoch: 1/2, step 8453/23838 completed (loss: 0.711891770362854, acc: 0.7978723645210266)
[2025-02-16 12:29:00,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:00,678][root][INFO] - Training Epoch: 1/2, step 8454/23838 completed (loss: 0.3669402301311493, acc: 0.890625)
[2025-02-16 12:29:00,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:01,110][root][INFO] - Training Epoch: 1/2, step 8455/23838 completed (loss: 0.2596130967140198, acc: 0.9220778942108154)
[2025-02-16 12:29:01,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:01,572][root][INFO] - Training Epoch: 1/2, step 8456/23838 completed (loss: 0.24017733335494995, acc: 0.9274193644523621)
[2025-02-16 12:29:01,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:02,009][root][INFO] - Training Epoch: 1/2, step 8457/23838 completed (loss: 0.40893372893333435, acc: 0.8767123222351074)
[2025-02-16 12:29:02,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:02,410][root][INFO] - Training Epoch: 1/2, step 8458/23838 completed (loss: 0.40565821528434753, acc: 0.9090909361839294)
[2025-02-16 12:29:02,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:02,913][root][INFO] - Training Epoch: 1/2, step 8459/23838 completed (loss: 0.5397080779075623, acc: 0.8295454382896423)
[2025-02-16 12:29:03,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:03,295][root][INFO] - Training Epoch: 1/2, step 8460/23838 completed (loss: 0.8421644568443298, acc: 0.7599999904632568)
[2025-02-16 12:29:03,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:03,739][root][INFO] - Training Epoch: 1/2, step 8461/23838 completed (loss: 0.936347484588623, acc: 0.707317054271698)
[2025-02-16 12:29:03,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:04,219][root][INFO] - Training Epoch: 1/2, step 8462/23838 completed (loss: 0.9095934629440308, acc: 0.7142857313156128)
[2025-02-16 12:29:04,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:04,706][root][INFO] - Training Epoch: 1/2, step 8463/23838 completed (loss: 0.7536459565162659, acc: 0.7540983557701111)
[2025-02-16 12:29:04,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:05,142][root][INFO] - Training Epoch: 1/2, step 8464/23838 completed (loss: 0.6431230306625366, acc: 0.8235294222831726)
[2025-02-16 12:29:05,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:05,533][root][INFO] - Training Epoch: 1/2, step 8465/23838 completed (loss: 0.43468108773231506, acc: 0.9210526347160339)
[2025-02-16 12:29:05,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:06,028][root][INFO] - Training Epoch: 1/2, step 8466/23838 completed (loss: 0.4795077443122864, acc: 0.8613861203193665)
[2025-02-16 12:29:06,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:06,462][root][INFO] - Training Epoch: 1/2, step 8467/23838 completed (loss: 0.7961757183074951, acc: 0.7764706015586853)
[2025-02-16 12:29:06,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:06,931][root][INFO] - Training Epoch: 1/2, step 8468/23838 completed (loss: 0.5512346625328064, acc: 0.8888888955116272)
[2025-02-16 12:29:07,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:07,316][root][INFO] - Training Epoch: 1/2, step 8469/23838 completed (loss: 0.23470066487789154, acc: 0.9482758641242981)
[2025-02-16 12:29:07,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:07,791][root][INFO] - Training Epoch: 1/2, step 8470/23838 completed (loss: 0.5712018609046936, acc: 0.8405796885490417)
[2025-02-16 12:29:07,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:08,220][root][INFO] - Training Epoch: 1/2, step 8471/23838 completed (loss: 0.6132532358169556, acc: 0.8207547068595886)
[2025-02-16 12:29:08,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:08,873][root][INFO] - Training Epoch: 1/2, step 8472/23838 completed (loss: 0.773400068283081, acc: 0.8134328126907349)
[2025-02-16 12:29:09,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:09,287][root][INFO] - Training Epoch: 1/2, step 8473/23838 completed (loss: 0.9812058806419373, acc: 0.7802197933197021)
[2025-02-16 12:29:09,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:09,657][root][INFO] - Training Epoch: 1/2, step 8474/23838 completed (loss: 0.7379558682441711, acc: 0.7706422209739685)
[2025-02-16 12:29:09,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:10,043][root][INFO] - Training Epoch: 1/2, step 8475/23838 completed (loss: 0.3564327657222748, acc: 0.9333333373069763)
[2025-02-16 12:29:10,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:10,466][root][INFO] - Training Epoch: 1/2, step 8476/23838 completed (loss: 0.3365154564380646, acc: 0.9157894849777222)
[2025-02-16 12:29:10,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:10,865][root][INFO] - Training Epoch: 1/2, step 8477/23838 completed (loss: 0.6885940432548523, acc: 0.875)
[2025-02-16 12:29:11,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:11,278][root][INFO] - Training Epoch: 1/2, step 8478/23838 completed (loss: 0.7857122421264648, acc: 0.7971014380455017)
[2025-02-16 12:29:11,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:11,712][root][INFO] - Training Epoch: 1/2, step 8479/23838 completed (loss: 0.560602068901062, acc: 0.8214285969734192)
[2025-02-16 12:29:11,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:12,116][root][INFO] - Training Epoch: 1/2, step 8480/23838 completed (loss: 0.6786699295043945, acc: 0.8082191944122314)
[2025-02-16 12:29:12,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:12,512][root][INFO] - Training Epoch: 1/2, step 8481/23838 completed (loss: 0.5848528146743774, acc: 0.8333333134651184)
[2025-02-16 12:29:12,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:12,965][root][INFO] - Training Epoch: 1/2, step 8482/23838 completed (loss: 0.7714342474937439, acc: 0.7894737124443054)
[2025-02-16 12:29:13,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:13,443][root][INFO] - Training Epoch: 1/2, step 8483/23838 completed (loss: 0.600614607334137, acc: 0.7761194109916687)
[2025-02-16 12:29:13,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:13,884][root][INFO] - Training Epoch: 1/2, step 8484/23838 completed (loss: 0.8259749412536621, acc: 0.8023256063461304)
[2025-02-16 12:29:14,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:14,331][root][INFO] - Training Epoch: 1/2, step 8485/23838 completed (loss: 0.404630571603775, acc: 0.8617021441459656)
[2025-02-16 12:29:14,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:14,777][root][INFO] - Training Epoch: 1/2, step 8486/23838 completed (loss: 0.6085208654403687, acc: 0.8415841460227966)
[2025-02-16 12:29:14,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:15,148][root][INFO] - Training Epoch: 1/2, step 8487/23838 completed (loss: 0.40216338634490967, acc: 0.8999999761581421)
[2025-02-16 12:29:15,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:15,759][root][INFO] - Training Epoch: 1/2, step 8488/23838 completed (loss: 0.6900515556335449, acc: 0.805031418800354)
[2025-02-16 12:29:15,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:16,174][root][INFO] - Training Epoch: 1/2, step 8489/23838 completed (loss: 1.403691291809082, acc: 0.650602400302887)
[2025-02-16 12:29:16,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:16,602][root][INFO] - Training Epoch: 1/2, step 8490/23838 completed (loss: 0.5377538204193115, acc: 0.8548387289047241)
[2025-02-16 12:29:16,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:16,964][root][INFO] - Training Epoch: 1/2, step 8491/23838 completed (loss: 0.8495931029319763, acc: 0.800000011920929)
[2025-02-16 12:29:17,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:17,450][root][INFO] - Training Epoch: 1/2, step 8492/23838 completed (loss: 0.36240535974502563, acc: 0.8875739574432373)
[2025-02-16 12:29:17,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:17,877][root][INFO] - Training Epoch: 1/2, step 8493/23838 completed (loss: 0.6988581418991089, acc: 0.8333333134651184)
[2025-02-16 12:29:18,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:18,260][root][INFO] - Training Epoch: 1/2, step 8494/23838 completed (loss: 0.39279603958129883, acc: 0.859375)
[2025-02-16 12:29:18,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:18,665][root][INFO] - Training Epoch: 1/2, step 8495/23838 completed (loss: 0.27007412910461426, acc: 0.9411764740943909)
[2025-02-16 12:29:18,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:19,107][root][INFO] - Training Epoch: 1/2, step 8496/23838 completed (loss: 0.4912503659725189, acc: 0.8695651888847351)
[2025-02-16 12:29:19,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:19,492][root][INFO] - Training Epoch: 1/2, step 8497/23838 completed (loss: 0.21146544814109802, acc: 0.9615384340286255)
[2025-02-16 12:29:19,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:19,992][root][INFO] - Training Epoch: 1/2, step 8498/23838 completed (loss: 0.6792392730712891, acc: 0.8658536672592163)
[2025-02-16 12:29:20,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:20,424][root][INFO] - Training Epoch: 1/2, step 8499/23838 completed (loss: 0.6001361608505249, acc: 0.8524590134620667)
[2025-02-16 12:29:20,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:20,856][root][INFO] - Training Epoch: 1/2, step 8500/23838 completed (loss: 0.529243528842926, acc: 0.8823529481887817)
[2025-02-16 12:29:21,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:21,331][root][INFO] - Training Epoch: 1/2, step 8501/23838 completed (loss: 0.791154146194458, acc: 0.7446808218955994)
[2025-02-16 12:29:21,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:21,714][root][INFO] - Training Epoch: 1/2, step 8502/23838 completed (loss: 0.2441389262676239, acc: 0.8888888955116272)
[2025-02-16 12:29:21,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:22,172][root][INFO] - Training Epoch: 1/2, step 8503/23838 completed (loss: 0.5931653380393982, acc: 0.8815789222717285)
[2025-02-16 12:29:22,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:22,520][root][INFO] - Training Epoch: 1/2, step 8504/23838 completed (loss: 0.4728408455848694, acc: 0.8873239159584045)
[2025-02-16 12:29:22,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:22,952][root][INFO] - Training Epoch: 1/2, step 8505/23838 completed (loss: 0.4869137406349182, acc: 0.8965517282485962)
[2025-02-16 12:29:23,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:23,424][root][INFO] - Training Epoch: 1/2, step 8506/23838 completed (loss: 0.17566430568695068, acc: 0.9722222089767456)
[2025-02-16 12:29:23,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:23,958][root][INFO] - Training Epoch: 1/2, step 8507/23838 completed (loss: 0.19611667096614838, acc: 0.9157894849777222)
[2025-02-16 12:29:24,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:24,609][root][INFO] - Training Epoch: 1/2, step 8508/23838 completed (loss: 0.7475079298019409, acc: 0.7731092572212219)
[2025-02-16 12:29:24,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:25,065][root][INFO] - Training Epoch: 1/2, step 8509/23838 completed (loss: 0.20342548191547394, acc: 0.9363636374473572)
[2025-02-16 12:29:25,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:25,452][root][INFO] - Training Epoch: 1/2, step 8510/23838 completed (loss: 0.27052706480026245, acc: 0.9340659379959106)
[2025-02-16 12:29:25,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:25,983][root][INFO] - Training Epoch: 1/2, step 8511/23838 completed (loss: 0.31532055139541626, acc: 0.9247311949729919)
[2025-02-16 12:29:26,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:26,479][root][INFO] - Training Epoch: 1/2, step 8512/23838 completed (loss: 0.37613150477409363, acc: 0.9042553305625916)
[2025-02-16 12:29:26,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:26,885][root][INFO] - Training Epoch: 1/2, step 8513/23838 completed (loss: 1.2146357297897339, acc: 0.698113203048706)
[2025-02-16 12:29:27,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:27,321][root][INFO] - Training Epoch: 1/2, step 8514/23838 completed (loss: 0.6525119543075562, acc: 0.7857142686843872)
[2025-02-16 12:29:27,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:27,720][root][INFO] - Training Epoch: 1/2, step 8515/23838 completed (loss: 0.3226337432861328, acc: 0.9529411792755127)
[2025-02-16 12:29:27,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:28,147][root][INFO] - Training Epoch: 1/2, step 8516/23838 completed (loss: 0.7319638133049011, acc: 0.7967479825019836)
[2025-02-16 12:29:28,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:28,610][root][INFO] - Training Epoch: 1/2, step 8517/23838 completed (loss: 0.5070329308509827, acc: 0.8793103694915771)
[2025-02-16 12:29:28,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:29,088][root][INFO] - Training Epoch: 1/2, step 8518/23838 completed (loss: 0.3733508884906769, acc: 0.9176470637321472)
[2025-02-16 12:29:29,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:29,494][root][INFO] - Training Epoch: 1/2, step 8519/23838 completed (loss: 0.5459803938865662, acc: 0.8275862336158752)
[2025-02-16 12:29:29,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:29,951][root][INFO] - Training Epoch: 1/2, step 8520/23838 completed (loss: 0.5077574849128723, acc: 0.8773584961891174)
[2025-02-16 12:29:30,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:30,305][root][INFO] - Training Epoch: 1/2, step 8521/23838 completed (loss: 0.7810110449790955, acc: 0.8055555820465088)
[2025-02-16 12:29:30,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:30,648][root][INFO] - Training Epoch: 1/2, step 8522/23838 completed (loss: 0.6749756932258606, acc: 0.8159999847412109)
[2025-02-16 12:29:30,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:31,035][root][INFO] - Training Epoch: 1/2, step 8523/23838 completed (loss: 1.0130701065063477, acc: 0.7196261882781982)
[2025-02-16 12:29:31,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:31,461][root][INFO] - Training Epoch: 1/2, step 8524/23838 completed (loss: 0.3794368505477905, acc: 0.8780487775802612)
[2025-02-16 12:29:31,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:31,981][root][INFO] - Training Epoch: 1/2, step 8525/23838 completed (loss: 1.2434812784194946, acc: 0.6507936716079712)
[2025-02-16 12:29:32,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:32,383][root][INFO] - Training Epoch: 1/2, step 8526/23838 completed (loss: 0.49377647042274475, acc: 0.8470588326454163)
[2025-02-16 12:29:32,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:32,903][root][INFO] - Training Epoch: 1/2, step 8527/23838 completed (loss: 0.6473239660263062, acc: 0.8041958212852478)
[2025-02-16 12:29:33,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:33,375][root][INFO] - Training Epoch: 1/2, step 8528/23838 completed (loss: 0.6010674834251404, acc: 0.8527131676673889)
[2025-02-16 12:29:33,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:33,830][root][INFO] - Training Epoch: 1/2, step 8529/23838 completed (loss: 0.5218948721885681, acc: 0.8496240377426147)
[2025-02-16 12:29:34,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:34,235][root][INFO] - Training Epoch: 1/2, step 8530/23838 completed (loss: 0.542264461517334, acc: 0.8152173757553101)
[2025-02-16 12:29:34,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:34,653][root][INFO] - Training Epoch: 1/2, step 8531/23838 completed (loss: 0.5082770586013794, acc: 0.8382353186607361)
[2025-02-16 12:29:34,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:35,100][root][INFO] - Training Epoch: 1/2, step 8532/23838 completed (loss: 0.31015124917030334, acc: 0.8999999761581421)
[2025-02-16 12:29:35,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:35,564][root][INFO] - Training Epoch: 1/2, step 8533/23838 completed (loss: 0.5127429366111755, acc: 0.837837815284729)
[2025-02-16 12:29:35,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:36,008][root][INFO] - Training Epoch: 1/2, step 8534/23838 completed (loss: 0.6389938592910767, acc: 0.8783783912658691)
[2025-02-16 12:29:36,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:36,477][root][INFO] - Training Epoch: 1/2, step 8535/23838 completed (loss: 0.5960772633552551, acc: 0.7951807379722595)
[2025-02-16 12:29:36,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:36,970][root][INFO] - Training Epoch: 1/2, step 8536/23838 completed (loss: 0.4381057024002075, acc: 0.9041095972061157)
[2025-02-16 12:29:37,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:37,607][root][INFO] - Training Epoch: 1/2, step 8537/23838 completed (loss: 0.4164845943450928, acc: 0.8976377844810486)
[2025-02-16 12:29:37,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:38,053][root][INFO] - Training Epoch: 1/2, step 8538/23838 completed (loss: 0.7568539977073669, acc: 0.734375)
[2025-02-16 12:29:38,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:38,455][root][INFO] - Training Epoch: 1/2, step 8539/23838 completed (loss: 0.6594748497009277, acc: 0.8245614171028137)
[2025-02-16 12:29:38,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:38,899][root][INFO] - Training Epoch: 1/2, step 8540/23838 completed (loss: 0.5100443959236145, acc: 0.8135592937469482)
[2025-02-16 12:29:39,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:39,363][root][INFO] - Training Epoch: 1/2, step 8541/23838 completed (loss: 0.520593523979187, acc: 0.8695651888847351)
[2025-02-16 12:29:39,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:39,778][root][INFO] - Training Epoch: 1/2, step 8542/23838 completed (loss: 0.4638517200946808, acc: 0.9032257795333862)
[2025-02-16 12:29:40,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:40,249][root][INFO] - Training Epoch: 1/2, step 8543/23838 completed (loss: 0.48111650347709656, acc: 0.8717948794364929)
[2025-02-16 12:29:40,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:40,613][root][INFO] - Training Epoch: 1/2, step 8544/23838 completed (loss: 0.5473528504371643, acc: 0.8461538553237915)
[2025-02-16 12:29:40,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:41,307][root][INFO] - Training Epoch: 1/2, step 8545/23838 completed (loss: 0.5665643215179443, acc: 0.8374999761581421)
[2025-02-16 12:29:41,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:41,762][root][INFO] - Training Epoch: 1/2, step 8546/23838 completed (loss: 0.4302384555339813, acc: 0.9007092118263245)
[2025-02-16 12:29:41,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:42,206][root][INFO] - Training Epoch: 1/2, step 8547/23838 completed (loss: 1.049521803855896, acc: 0.7164179086685181)
[2025-02-16 12:29:42,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:42,859][root][INFO] - Training Epoch: 1/2, step 8548/23838 completed (loss: 0.5134143233299255, acc: 0.8367347121238708)
[2025-02-16 12:29:43,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:43,356][root][INFO] - Training Epoch: 1/2, step 8549/23838 completed (loss: 0.28977566957473755, acc: 0.8943089246749878)
[2025-02-16 12:29:43,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:43,821][root][INFO] - Training Epoch: 1/2, step 8550/23838 completed (loss: 0.12229550629854202, acc: 0.9700000286102295)
[2025-02-16 12:29:44,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:44,317][root][INFO] - Training Epoch: 1/2, step 8551/23838 completed (loss: 0.6131070256233215, acc: 0.8452380895614624)
[2025-02-16 12:29:44,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:44,790][root][INFO] - Training Epoch: 1/2, step 8552/23838 completed (loss: 0.4606116712093353, acc: 0.876288652420044)
[2025-02-16 12:29:44,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:45,186][root][INFO] - Training Epoch: 1/2, step 8553/23838 completed (loss: 0.35176166892051697, acc: 0.8818181753158569)
[2025-02-16 12:29:45,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:45,742][root][INFO] - Training Epoch: 1/2, step 8554/23838 completed (loss: 0.36632969975471497, acc: 0.9101123809814453)
[2025-02-16 12:29:45,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:46,184][root][INFO] - Training Epoch: 1/2, step 8555/23838 completed (loss: 0.45130711793899536, acc: 0.8571428656578064)
[2025-02-16 12:29:46,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:46,567][root][INFO] - Training Epoch: 1/2, step 8556/23838 completed (loss: 0.6330416202545166, acc: 0.824999988079071)
[2025-02-16 12:29:46,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:47,032][root][INFO] - Training Epoch: 1/2, step 8557/23838 completed (loss: 1.1476269960403442, acc: 0.7115384340286255)
[2025-02-16 12:29:47,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:47,476][root][INFO] - Training Epoch: 1/2, step 8558/23838 completed (loss: 0.6050183773040771, acc: 0.8409090638160706)
[2025-02-16 12:29:47,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:47,934][root][INFO] - Training Epoch: 1/2, step 8559/23838 completed (loss: 0.4464195668697357, acc: 0.868686854839325)
[2025-02-16 12:29:48,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:48,356][root][INFO] - Training Epoch: 1/2, step 8560/23838 completed (loss: 0.8343796133995056, acc: 0.72826087474823)
[2025-02-16 12:29:48,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:48,770][root][INFO] - Training Epoch: 1/2, step 8561/23838 completed (loss: 0.7407005429267883, acc: 0.7894737124443054)
[2025-02-16 12:29:48,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:49,180][root][INFO] - Training Epoch: 1/2, step 8562/23838 completed (loss: 0.9361721873283386, acc: 0.7916666865348816)
[2025-02-16 12:29:49,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:49,624][root][INFO] - Training Epoch: 1/2, step 8563/23838 completed (loss: 0.2857877016067505, acc: 0.9069767594337463)
[2025-02-16 12:29:49,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:50,055][root][INFO] - Training Epoch: 1/2, step 8564/23838 completed (loss: 1.1143945455551147, acc: 0.7215189933776855)
[2025-02-16 12:29:50,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:50,570][root][INFO] - Training Epoch: 1/2, step 8565/23838 completed (loss: 0.5377393960952759, acc: 0.8448275923728943)
[2025-02-16 12:29:50,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:50,972][root][INFO] - Training Epoch: 1/2, step 8566/23838 completed (loss: 0.2782309353351593, acc: 0.9397590160369873)
[2025-02-16 12:29:51,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:51,424][root][INFO] - Training Epoch: 1/2, step 8567/23838 completed (loss: 0.34020188450813293, acc: 0.9142857193946838)
[2025-02-16 12:29:51,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:51,854][root][INFO] - Training Epoch: 1/2, step 8568/23838 completed (loss: 0.5590626001358032, acc: 0.8217821717262268)
[2025-02-16 12:29:52,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:52,308][root][INFO] - Training Epoch: 1/2, step 8569/23838 completed (loss: 0.7898993492126465, acc: 0.7698412537574768)
[2025-02-16 12:29:52,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:52,730][root][INFO] - Training Epoch: 1/2, step 8570/23838 completed (loss: 0.8572189211845398, acc: 0.7241379022598267)
[2025-02-16 12:29:52,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:53,157][root][INFO] - Training Epoch: 1/2, step 8571/23838 completed (loss: 0.9193428158760071, acc: 0.7857142686843872)
[2025-02-16 12:29:53,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:53,629][root][INFO] - Training Epoch: 1/2, step 8572/23838 completed (loss: 0.3692081570625305, acc: 0.8866666555404663)
[2025-02-16 12:29:53,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:54,073][root][INFO] - Training Epoch: 1/2, step 8573/23838 completed (loss: 0.404646098613739, acc: 0.9029850959777832)
[2025-02-16 12:29:54,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:54,520][root][INFO] - Training Epoch: 1/2, step 8574/23838 completed (loss: 1.068158745765686, acc: 0.6428571343421936)
[2025-02-16 12:29:54,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:54,967][root][INFO] - Training Epoch: 1/2, step 8575/23838 completed (loss: 0.37662604451179504, acc: 0.9166666865348816)
[2025-02-16 12:29:55,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:55,375][root][INFO] - Training Epoch: 1/2, step 8576/23838 completed (loss: 0.4349135458469391, acc: 0.875)
[2025-02-16 12:29:55,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:55,761][root][INFO] - Training Epoch: 1/2, step 8577/23838 completed (loss: 0.32027265429496765, acc: 0.9102563858032227)
[2025-02-16 12:29:55,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:56,200][root][INFO] - Training Epoch: 1/2, step 8578/23838 completed (loss: 0.616970956325531, acc: 0.8133333325386047)
[2025-02-16 12:29:56,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:56,581][root][INFO] - Training Epoch: 1/2, step 8579/23838 completed (loss: 0.2538549602031708, acc: 0.957446813583374)
[2025-02-16 12:29:56,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:57,021][root][INFO] - Training Epoch: 1/2, step 8580/23838 completed (loss: 0.6073916554450989, acc: 0.8348624110221863)
[2025-02-16 12:29:57,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:57,417][root][INFO] - Training Epoch: 1/2, step 8581/23838 completed (loss: 0.3750877380371094, acc: 0.8771929740905762)
[2025-02-16 12:29:57,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:57,923][root][INFO] - Training Epoch: 1/2, step 8582/23838 completed (loss: 0.46469831466674805, acc: 0.8531073331832886)
[2025-02-16 12:29:58,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:58,427][root][INFO] - Training Epoch: 1/2, step 8583/23838 completed (loss: 0.6776907444000244, acc: 0.813725471496582)
[2025-02-16 12:29:58,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:58,915][root][INFO] - Training Epoch: 1/2, step 8584/23838 completed (loss: 0.4912393391132355, acc: 0.8717948794364929)
[2025-02-16 12:29:59,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:59,407][root][INFO] - Training Epoch: 1/2, step 8585/23838 completed (loss: 0.4934150278568268, acc: 0.859649121761322)
[2025-02-16 12:29:59,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:29:59,781][root][INFO] - Training Epoch: 1/2, step 8586/23838 completed (loss: 0.37176573276519775, acc: 0.9059829115867615)
[2025-02-16 12:29:59,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:00,166][root][INFO] - Training Epoch: 1/2, step 8587/23838 completed (loss: 0.2974599599838257, acc: 0.9448819160461426)
[2025-02-16 12:30:00,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:00,616][root][INFO] - Training Epoch: 1/2, step 8588/23838 completed (loss: 0.20870277285575867, acc: 0.9508196711540222)
[2025-02-16 12:30:00,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:01,083][root][INFO] - Training Epoch: 1/2, step 8589/23838 completed (loss: 0.22126427292823792, acc: 0.9441624283790588)
[2025-02-16 12:30:01,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:01,532][root][INFO] - Training Epoch: 1/2, step 8590/23838 completed (loss: 0.5502509474754333, acc: 0.8571428656578064)
[2025-02-16 12:30:01,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:01,975][root][INFO] - Training Epoch: 1/2, step 8591/23838 completed (loss: 0.9186604022979736, acc: 0.7362637519836426)
[2025-02-16 12:30:02,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:02,422][root][INFO] - Training Epoch: 1/2, step 8592/23838 completed (loss: 0.5678372979164124, acc: 0.8761904835700989)
[2025-02-16 12:30:02,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:02,806][root][INFO] - Training Epoch: 1/2, step 8593/23838 completed (loss: 0.34666693210601807, acc: 0.9019607901573181)
[2025-02-16 12:30:03,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:03,270][root][INFO] - Training Epoch: 1/2, step 8594/23838 completed (loss: 0.18424488604068756, acc: 0.9642857313156128)
[2025-02-16 12:30:03,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:03,680][root][INFO] - Training Epoch: 1/2, step 8595/23838 completed (loss: 0.6608807444572449, acc: 0.8035714030265808)
[2025-02-16 12:30:03,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:04,088][root][INFO] - Training Epoch: 1/2, step 8596/23838 completed (loss: 0.21187883615493774, acc: 0.9230769276618958)
[2025-02-16 12:30:04,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:04,438][root][INFO] - Training Epoch: 1/2, step 8597/23838 completed (loss: 0.30860501527786255, acc: 0.9117646813392639)
[2025-02-16 12:30:04,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:04,887][root][INFO] - Training Epoch: 1/2, step 8598/23838 completed (loss: 0.4033697843551636, acc: 0.8793103694915771)
[2025-02-16 12:30:05,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:05,275][root][INFO] - Training Epoch: 1/2, step 8599/23838 completed (loss: 0.4077991247177124, acc: 0.8799999952316284)
[2025-02-16 12:30:05,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:05,745][root][INFO] - Training Epoch: 1/2, step 8600/23838 completed (loss: 0.33229002356529236, acc: 0.8742138147354126)
[2025-02-16 12:30:05,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:06,142][root][INFO] - Training Epoch: 1/2, step 8601/23838 completed (loss: 0.5402798056602478, acc: 0.8595041036605835)
[2025-02-16 12:30:06,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:06,593][root][INFO] - Training Epoch: 1/2, step 8602/23838 completed (loss: 0.34589216113090515, acc: 0.9152542352676392)
[2025-02-16 12:30:06,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:06,979][root][INFO] - Training Epoch: 1/2, step 8603/23838 completed (loss: 0.24363739788532257, acc: 0.9519230723381042)
[2025-02-16 12:30:07,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:07,442][root][INFO] - Training Epoch: 1/2, step 8604/23838 completed (loss: 0.3363698422908783, acc: 0.9032257795333862)
[2025-02-16 12:30:07,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:07,851][root][INFO] - Training Epoch: 1/2, step 8605/23838 completed (loss: 0.4093290865421295, acc: 0.8720930218696594)
[2025-02-16 12:30:07,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:08,224][root][INFO] - Training Epoch: 1/2, step 8606/23838 completed (loss: 0.22281961143016815, acc: 0.9359999895095825)
[2025-02-16 12:30:08,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:08,684][root][INFO] - Training Epoch: 1/2, step 8607/23838 completed (loss: 0.3522779643535614, acc: 0.8672566413879395)
[2025-02-16 12:30:08,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:09,108][root][INFO] - Training Epoch: 1/2, step 8608/23838 completed (loss: 0.17452780902385712, acc: 0.9431818127632141)
[2025-02-16 12:30:09,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:09,500][root][INFO] - Training Epoch: 1/2, step 8609/23838 completed (loss: 0.32303673028945923, acc: 0.903553307056427)
[2025-02-16 12:30:09,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:09,904][root][INFO] - Training Epoch: 1/2, step 8610/23838 completed (loss: 0.5921348929405212, acc: 0.8448275923728943)
[2025-02-16 12:30:10,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:10,333][root][INFO] - Training Epoch: 1/2, step 8611/23838 completed (loss: 0.43967223167419434, acc: 0.8913043737411499)
[2025-02-16 12:30:10,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:10,773][root][INFO] - Training Epoch: 1/2, step 8612/23838 completed (loss: 0.2700203061103821, acc: 0.913385808467865)
[2025-02-16 12:30:10,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:11,213][root][INFO] - Training Epoch: 1/2, step 8613/23838 completed (loss: 0.19210144877433777, acc: 0.9620253443717957)
[2025-02-16 12:30:11,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:11,669][root][INFO] - Training Epoch: 1/2, step 8614/23838 completed (loss: 0.5953037738800049, acc: 0.8399999737739563)
[2025-02-16 12:30:11,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:12,068][root][INFO] - Training Epoch: 1/2, step 8615/23838 completed (loss: 0.3482063114643097, acc: 0.901098906993866)
[2025-02-16 12:30:12,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:12,489][root][INFO] - Training Epoch: 1/2, step 8616/23838 completed (loss: 0.4137486517429352, acc: 0.8538461327552795)
[2025-02-16 12:30:12,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:12,955][root][INFO] - Training Epoch: 1/2, step 8617/23838 completed (loss: 0.23118191957473755, acc: 0.9425287246704102)
[2025-02-16 12:30:13,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:13,401][root][INFO] - Training Epoch: 1/2, step 8618/23838 completed (loss: 0.3899376690387726, acc: 0.8879310488700867)
[2025-02-16 12:30:13,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:13,781][root][INFO] - Training Epoch: 1/2, step 8619/23838 completed (loss: 0.37850648164749146, acc: 0.8695651888847351)
[2025-02-16 12:30:13,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:14,194][root][INFO] - Training Epoch: 1/2, step 8620/23838 completed (loss: 0.4847266376018524, acc: 0.8529411554336548)
[2025-02-16 12:30:14,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:14,665][root][INFO] - Training Epoch: 1/2, step 8621/23838 completed (loss: 0.37970587611198425, acc: 0.8539325594902039)
[2025-02-16 12:30:14,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:15,122][root][INFO] - Training Epoch: 1/2, step 8622/23838 completed (loss: 0.41087502241134644, acc: 0.8409090638160706)
[2025-02-16 12:30:15,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:15,577][root][INFO] - Training Epoch: 1/2, step 8623/23838 completed (loss: 0.29835325479507446, acc: 0.9204545617103577)
[2025-02-16 12:30:15,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:16,199][root][INFO] - Training Epoch: 1/2, step 8624/23838 completed (loss: 0.2537786066532135, acc: 0.8904109597206116)
[2025-02-16 12:30:16,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:16,643][root][INFO] - Training Epoch: 1/2, step 8625/23838 completed (loss: 0.29122963547706604, acc: 0.8961039185523987)
[2025-02-16 12:30:16,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:17,020][root][INFO] - Training Epoch: 1/2, step 8626/23838 completed (loss: 0.7902441620826721, acc: 0.7246376872062683)
[2025-02-16 12:30:17,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:17,378][root][INFO] - Training Epoch: 1/2, step 8627/23838 completed (loss: 0.45030155777931213, acc: 0.9060402512550354)
[2025-02-16 12:30:17,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:17,759][root][INFO] - Training Epoch: 1/2, step 8628/23838 completed (loss: 0.4793880879878998, acc: 0.8241758346557617)
[2025-02-16 12:30:17,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:18,121][root][INFO] - Training Epoch: 1/2, step 8629/23838 completed (loss: 0.39522454142570496, acc: 0.8999999761581421)
[2025-02-16 12:30:18,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:18,500][root][INFO] - Training Epoch: 1/2, step 8630/23838 completed (loss: 0.4043431878089905, acc: 0.8660714030265808)
[2025-02-16 12:30:18,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:18,924][root][INFO] - Training Epoch: 1/2, step 8631/23838 completed (loss: 1.1510225534439087, acc: 0.699999988079071)
[2025-02-16 12:30:19,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:19,358][root][INFO] - Training Epoch: 1/2, step 8632/23838 completed (loss: 0.5856508612632751, acc: 0.8247422575950623)
[2025-02-16 12:30:19,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:19,782][root][INFO] - Training Epoch: 1/2, step 8633/23838 completed (loss: 0.4070102572441101, acc: 0.8888888955116272)
[2025-02-16 12:30:19,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:20,187][root][INFO] - Training Epoch: 1/2, step 8634/23838 completed (loss: 0.5843460559844971, acc: 0.8859649300575256)
[2025-02-16 12:30:20,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:20,625][root][INFO] - Training Epoch: 1/2, step 8635/23838 completed (loss: 0.5691004395484924, acc: 0.8113207817077637)
[2025-02-16 12:30:20,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:21,084][root][INFO] - Training Epoch: 1/2, step 8636/23838 completed (loss: 0.26636162400245667, acc: 0.9111111164093018)
[2025-02-16 12:30:21,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:21,489][root][INFO] - Training Epoch: 1/2, step 8637/23838 completed (loss: 0.5126909613609314, acc: 0.8872180581092834)
[2025-02-16 12:30:21,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:21,986][root][INFO] - Training Epoch: 1/2, step 8638/23838 completed (loss: 0.3361823260784149, acc: 0.9090909361839294)
[2025-02-16 12:30:22,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:22,412][root][INFO] - Training Epoch: 1/2, step 8639/23838 completed (loss: 0.4585547149181366, acc: 0.8692307472229004)
[2025-02-16 12:30:22,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:22,893][root][INFO] - Training Epoch: 1/2, step 8640/23838 completed (loss: 0.5243704915046692, acc: 0.8636363744735718)
[2025-02-16 12:30:23,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:23,257][root][INFO] - Training Epoch: 1/2, step 8641/23838 completed (loss: 0.6204105615615845, acc: 0.8333333134651184)
[2025-02-16 12:30:23,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:23,717][root][INFO] - Training Epoch: 1/2, step 8642/23838 completed (loss: 0.2910287082195282, acc: 0.9278350472450256)
[2025-02-16 12:30:23,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:24,266][root][INFO] - Training Epoch: 1/2, step 8643/23838 completed (loss: 0.22485189139842987, acc: 0.936170220375061)
[2025-02-16 12:30:24,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:24,701][root][INFO] - Training Epoch: 1/2, step 8644/23838 completed (loss: 0.14189541339874268, acc: 0.9637305736541748)
[2025-02-16 12:30:24,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:25,147][root][INFO] - Training Epoch: 1/2, step 8645/23838 completed (loss: 0.3246833086013794, acc: 0.9153439402580261)
[2025-02-16 12:30:25,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:25,598][root][INFO] - Training Epoch: 1/2, step 8646/23838 completed (loss: 0.19719253480434418, acc: 0.9354838728904724)
[2025-02-16 12:30:25,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:26,109][root][INFO] - Training Epoch: 1/2, step 8647/23838 completed (loss: 0.17390699684619904, acc: 0.9508196711540222)
[2025-02-16 12:30:26,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:26,516][root][INFO] - Training Epoch: 1/2, step 8648/23838 completed (loss: 0.5270776152610779, acc: 0.8850574493408203)
[2025-02-16 12:30:26,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:26,886][root][INFO] - Training Epoch: 1/2, step 8649/23838 completed (loss: 0.29089653491973877, acc: 0.9126983880996704)
[2025-02-16 12:30:27,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:27,321][root][INFO] - Training Epoch: 1/2, step 8650/23838 completed (loss: 0.4625450670719147, acc: 0.8784530162811279)
[2025-02-16 12:30:27,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:27,718][root][INFO] - Training Epoch: 1/2, step 8651/23838 completed (loss: 0.46451735496520996, acc: 0.8709677457809448)
[2025-02-16 12:30:27,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:28,177][root][INFO] - Training Epoch: 1/2, step 8652/23838 completed (loss: 0.3617025315761566, acc: 0.904411792755127)
[2025-02-16 12:30:28,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:28,613][root][INFO] - Training Epoch: 1/2, step 8653/23838 completed (loss: 0.6279895305633545, acc: 0.8297872543334961)
[2025-02-16 12:30:28,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:29,051][root][INFO] - Training Epoch: 1/2, step 8654/23838 completed (loss: 0.27534011006355286, acc: 0.9369369149208069)
[2025-02-16 12:30:29,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:29,482][root][INFO] - Training Epoch: 1/2, step 8655/23838 completed (loss: 0.4371620714664459, acc: 0.8793103694915771)
[2025-02-16 12:30:29,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:29,904][root][INFO] - Training Epoch: 1/2, step 8656/23838 completed (loss: 0.23950324952602386, acc: 0.9285714030265808)
[2025-02-16 12:30:30,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:30,331][root][INFO] - Training Epoch: 1/2, step 8657/23838 completed (loss: 0.6998447179794312, acc: 0.7945205569267273)
[2025-02-16 12:30:30,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:30,761][root][INFO] - Training Epoch: 1/2, step 8658/23838 completed (loss: 0.3756980299949646, acc: 0.8771929740905762)
[2025-02-16 12:30:30,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:31,178][root][INFO] - Training Epoch: 1/2, step 8659/23838 completed (loss: 0.16797350347042084, acc: 0.9603960514068604)
[2025-02-16 12:30:31,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:31,676][root][INFO] - Training Epoch: 1/2, step 8660/23838 completed (loss: 0.2577621042728424, acc: 0.9268292784690857)
[2025-02-16 12:30:31,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:32,105][root][INFO] - Training Epoch: 1/2, step 8661/23838 completed (loss: 0.3663676679134369, acc: 0.9027777910232544)
[2025-02-16 12:30:32,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:32,517][root][INFO] - Training Epoch: 1/2, step 8662/23838 completed (loss: 0.4002256989479065, acc: 0.8901098966598511)
[2025-02-16 12:30:32,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:32,944][root][INFO] - Training Epoch: 1/2, step 8663/23838 completed (loss: 0.33109724521636963, acc: 0.9259259104728699)
[2025-02-16 12:30:33,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:33,384][root][INFO] - Training Epoch: 1/2, step 8664/23838 completed (loss: 0.13381926715373993, acc: 0.9722222089767456)
[2025-02-16 12:30:33,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:33,813][root][INFO] - Training Epoch: 1/2, step 8665/23838 completed (loss: 0.17914198338985443, acc: 0.9698795080184937)
[2025-02-16 12:30:34,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:34,274][root][INFO] - Training Epoch: 1/2, step 8666/23838 completed (loss: 0.28121334314346313, acc: 0.9438202381134033)
[2025-02-16 12:30:34,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:34,672][root][INFO] - Training Epoch: 1/2, step 8667/23838 completed (loss: 0.530620276927948, acc: 0.8690476417541504)
[2025-02-16 12:30:34,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:35,128][root][INFO] - Training Epoch: 1/2, step 8668/23838 completed (loss: 0.3397674858570099, acc: 0.8897058963775635)
[2025-02-16 12:30:35,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:35,611][root][INFO] - Training Epoch: 1/2, step 8669/23838 completed (loss: 0.24431438744068146, acc: 0.918367326259613)
[2025-02-16 12:30:35,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:36,184][root][INFO] - Training Epoch: 1/2, step 8670/23838 completed (loss: 0.29341766238212585, acc: 0.8870967626571655)
[2025-02-16 12:30:36,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:36,671][root][INFO] - Training Epoch: 1/2, step 8671/23838 completed (loss: 0.3838895261287689, acc: 0.8832116723060608)
[2025-02-16 12:30:36,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:37,089][root][INFO] - Training Epoch: 1/2, step 8672/23838 completed (loss: 0.3362548351287842, acc: 0.9017857313156128)
[2025-02-16 12:30:37,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:37,462][root][INFO] - Training Epoch: 1/2, step 8673/23838 completed (loss: 0.3555305302143097, acc: 0.9125000238418579)
[2025-02-16 12:30:37,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:37,964][root][INFO] - Training Epoch: 1/2, step 8674/23838 completed (loss: 0.308552086353302, acc: 0.9090909361839294)
[2025-02-16 12:30:38,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:38,347][root][INFO] - Training Epoch: 1/2, step 8675/23838 completed (loss: 0.5530697703361511, acc: 0.8941176533699036)
[2025-02-16 12:30:38,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:38,788][root][INFO] - Training Epoch: 1/2, step 8676/23838 completed (loss: 0.2977103590965271, acc: 0.9291338324546814)
[2025-02-16 12:30:39,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:39,263][root][INFO] - Training Epoch: 1/2, step 8677/23838 completed (loss: 0.2741169035434723, acc: 0.9190751314163208)
[2025-02-16 12:30:39,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:39,657][root][INFO] - Training Epoch: 1/2, step 8678/23838 completed (loss: 0.21584521234035492, acc: 0.9343065619468689)
[2025-02-16 12:30:39,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:40,079][root][INFO] - Training Epoch: 1/2, step 8679/23838 completed (loss: 0.24429814517498016, acc: 0.9450549483299255)
[2025-02-16 12:30:40,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:40,518][root][INFO] - Training Epoch: 1/2, step 8680/23838 completed (loss: 0.1968667358160019, acc: 0.9496402740478516)
[2025-02-16 12:30:40,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:40,946][root][INFO] - Training Epoch: 1/2, step 8681/23838 completed (loss: 0.2571471929550171, acc: 0.9096385836601257)
[2025-02-16 12:30:41,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:41,417][root][INFO] - Training Epoch: 1/2, step 8682/23838 completed (loss: 0.4661046862602234, acc: 0.8809523582458496)
[2025-02-16 12:30:41,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:41,887][root][INFO] - Training Epoch: 1/2, step 8683/23838 completed (loss: 0.458974689245224, acc: 0.8592592477798462)
[2025-02-16 12:30:42,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:42,366][root][INFO] - Training Epoch: 1/2, step 8684/23838 completed (loss: 0.27577006816864014, acc: 0.9282511472702026)
[2025-02-16 12:30:42,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:42,864][root][INFO] - Training Epoch: 1/2, step 8685/23838 completed (loss: 0.3738205134868622, acc: 0.875)
[2025-02-16 12:30:43,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:43,343][root][INFO] - Training Epoch: 1/2, step 8686/23838 completed (loss: 0.3519922196865082, acc: 0.9147287011146545)
[2025-02-16 12:30:43,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:43,837][root][INFO] - Training Epoch: 1/2, step 8687/23838 completed (loss: 0.4527788460254669, acc: 0.8716216087341309)
[2025-02-16 12:30:44,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:44,317][root][INFO] - Training Epoch: 1/2, step 8688/23838 completed (loss: 0.32194554805755615, acc: 0.9230769276618958)
[2025-02-16 12:30:44,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:44,737][root][INFO] - Training Epoch: 1/2, step 8689/23838 completed (loss: 0.31606215238571167, acc: 0.9081632494926453)
[2025-02-16 12:30:44,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:45,110][root][INFO] - Training Epoch: 1/2, step 8690/23838 completed (loss: 0.31335899233818054, acc: 0.9090909361839294)
[2025-02-16 12:30:45,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:45,535][root][INFO] - Training Epoch: 1/2, step 8691/23838 completed (loss: 0.5395948886871338, acc: 0.8547008633613586)
[2025-02-16 12:30:45,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:45,979][root][INFO] - Training Epoch: 1/2, step 8692/23838 completed (loss: 0.5921372771263123, acc: 0.8636363744735718)
[2025-02-16 12:30:46,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:46,371][root][INFO] - Training Epoch: 1/2, step 8693/23838 completed (loss: 0.5554957389831543, acc: 0.8623188138008118)
[2025-02-16 12:30:46,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:46,815][root][INFO] - Training Epoch: 1/2, step 8694/23838 completed (loss: 0.5379830002784729, acc: 0.8348624110221863)
[2025-02-16 12:30:46,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:47,199][root][INFO] - Training Epoch: 1/2, step 8695/23838 completed (loss: 0.44013074040412903, acc: 0.8783783912658691)
[2025-02-16 12:30:47,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:47,571][root][INFO] - Training Epoch: 1/2, step 8696/23838 completed (loss: 0.21851864457130432, acc: 0.9402984976768494)
[2025-02-16 12:30:47,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:48,024][root][INFO] - Training Epoch: 1/2, step 8697/23838 completed (loss: 0.27502718567848206, acc: 0.9256198406219482)
[2025-02-16 12:30:48,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:48,415][root][INFO] - Training Epoch: 1/2, step 8698/23838 completed (loss: 0.3926960527896881, acc: 0.8730158805847168)
[2025-02-16 12:30:48,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:48,846][root][INFO] - Training Epoch: 1/2, step 8699/23838 completed (loss: 0.6753841638565063, acc: 0.779411792755127)
[2025-02-16 12:30:49,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:49,307][root][INFO] - Training Epoch: 1/2, step 8700/23838 completed (loss: 0.33717089891433716, acc: 0.9115646481513977)
[2025-02-16 12:30:49,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:49,750][root][INFO] - Training Epoch: 1/2, step 8701/23838 completed (loss: 0.3219451606273651, acc: 0.9112903475761414)
[2025-02-16 12:30:49,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:50,204][root][INFO] - Training Epoch: 1/2, step 8702/23838 completed (loss: 0.14889845252037048, acc: 0.9672130942344666)
[2025-02-16 12:30:50,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:50,628][root][INFO] - Training Epoch: 1/2, step 8703/23838 completed (loss: 0.2340199500322342, acc: 0.9173553586006165)
[2025-02-16 12:30:50,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:51,063][root][INFO] - Training Epoch: 1/2, step 8704/23838 completed (loss: 0.37341511249542236, acc: 0.9081632494926453)
[2025-02-16 12:30:51,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:51,500][root][INFO] - Training Epoch: 1/2, step 8705/23838 completed (loss: 0.5325983166694641, acc: 0.8088235259056091)
[2025-02-16 12:30:51,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:51,969][root][INFO] - Training Epoch: 1/2, step 8706/23838 completed (loss: 0.5051859617233276, acc: 0.8365384340286255)
[2025-02-16 12:30:52,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:52,320][root][INFO] - Training Epoch: 1/2, step 8707/23838 completed (loss: 0.5833609104156494, acc: 0.8727272748947144)
[2025-02-16 12:30:52,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:52,761][root][INFO] - Training Epoch: 1/2, step 8708/23838 completed (loss: 0.40531885623931885, acc: 0.8717948794364929)
[2025-02-16 12:30:52,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:53,218][root][INFO] - Training Epoch: 1/2, step 8709/23838 completed (loss: 0.2843916118144989, acc: 0.9256756901741028)
[2025-02-16 12:30:53,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:53,719][root][INFO] - Training Epoch: 1/2, step 8710/23838 completed (loss: 0.381391704082489, acc: 0.8732394576072693)
[2025-02-16 12:30:53,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:54,204][root][INFO] - Training Epoch: 1/2, step 8711/23838 completed (loss: 0.22575338184833527, acc: 0.9271523356437683)
[2025-02-16 12:30:54,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:54,610][root][INFO] - Training Epoch: 1/2, step 8712/23838 completed (loss: 0.4098453223705292, acc: 0.8557692170143127)
[2025-02-16 12:30:54,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:55,119][root][INFO] - Training Epoch: 1/2, step 8713/23838 completed (loss: 0.7482192516326904, acc: 0.7540983557701111)
[2025-02-16 12:30:55,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:55,588][root][INFO] - Training Epoch: 1/2, step 8714/23838 completed (loss: 0.15545357763767242, acc: 0.9463087320327759)
[2025-02-16 12:30:55,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:56,019][root][INFO] - Training Epoch: 1/2, step 8715/23838 completed (loss: 0.5222164988517761, acc: 0.851190447807312)
[2025-02-16 12:30:56,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:56,420][root][INFO] - Training Epoch: 1/2, step 8716/23838 completed (loss: 0.7248803377151489, acc: 0.8181818127632141)
[2025-02-16 12:30:56,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:56,833][root][INFO] - Training Epoch: 1/2, step 8717/23838 completed (loss: 0.3801717758178711, acc: 0.8767123222351074)
[2025-02-16 12:30:57,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:57,268][root][INFO] - Training Epoch: 1/2, step 8718/23838 completed (loss: 0.33120062947273254, acc: 0.9074074029922485)
[2025-02-16 12:30:57,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:57,764][root][INFO] - Training Epoch: 1/2, step 8719/23838 completed (loss: 0.24820683896541595, acc: 0.939393937587738)
[2025-02-16 12:30:57,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:58,215][root][INFO] - Training Epoch: 1/2, step 8720/23838 completed (loss: 0.2826470136642456, acc: 0.93388432264328)
[2025-02-16 12:30:58,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:58,651][root][INFO] - Training Epoch: 1/2, step 8721/23838 completed (loss: 0.23288781940937042, acc: 0.9473684430122375)
[2025-02-16 12:30:58,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:59,069][root][INFO] - Training Epoch: 1/2, step 8722/23838 completed (loss: 0.2436988651752472, acc: 0.9347826242446899)
[2025-02-16 12:30:59,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:59,466][root][INFO] - Training Epoch: 1/2, step 8723/23838 completed (loss: 0.24154627323150635, acc: 0.9316770434379578)
[2025-02-16 12:30:59,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:30:59,916][root][INFO] - Training Epoch: 1/2, step 8724/23838 completed (loss: 0.4603561758995056, acc: 0.904411792755127)
[2025-02-16 12:31:00,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:00,357][root][INFO] - Training Epoch: 1/2, step 8725/23838 completed (loss: 0.7472981214523315, acc: 0.807692289352417)
[2025-02-16 12:31:00,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:00,813][root][INFO] - Training Epoch: 1/2, step 8726/23838 completed (loss: 0.101067915558815, acc: 0.970588207244873)
[2025-02-16 12:31:01,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:01,285][root][INFO] - Training Epoch: 1/2, step 8727/23838 completed (loss: 0.5728849172592163, acc: 0.8510638475418091)
[2025-02-16 12:31:01,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:01,625][root][INFO] - Training Epoch: 1/2, step 8728/23838 completed (loss: 0.21091042459011078, acc: 0.9586206674575806)
[2025-02-16 12:31:01,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:02,055][root][INFO] - Training Epoch: 1/2, step 8729/23838 completed (loss: 0.5898109674453735, acc: 0.8815789222717285)
[2025-02-16 12:31:02,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:02,534][root][INFO] - Training Epoch: 1/2, step 8730/23838 completed (loss: 0.2939474284648895, acc: 0.9224806427955627)
[2025-02-16 12:31:02,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:02,936][root][INFO] - Training Epoch: 1/2, step 8731/23838 completed (loss: 0.36779648065567017, acc: 0.9047619104385376)
[2025-02-16 12:31:03,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:03,378][root][INFO] - Training Epoch: 1/2, step 8732/23838 completed (loss: 0.16145184636116028, acc: 0.9526627063751221)
[2025-02-16 12:31:03,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:03,856][root][INFO] - Training Epoch: 1/2, step 8733/23838 completed (loss: 0.4950345754623413, acc: 0.8495575189590454)
[2025-02-16 12:31:04,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:04,306][root][INFO] - Training Epoch: 1/2, step 8734/23838 completed (loss: 0.26474249362945557, acc: 0.9204545617103577)
[2025-02-16 12:31:04,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:04,744][root][INFO] - Training Epoch: 1/2, step 8735/23838 completed (loss: 0.24757714569568634, acc: 0.9345238208770752)
[2025-02-16 12:31:04,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:05,154][root][INFO] - Training Epoch: 1/2, step 8736/23838 completed (loss: 0.3954269289970398, acc: 0.8804348111152649)
[2025-02-16 12:31:05,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:05,637][root][INFO] - Training Epoch: 1/2, step 8737/23838 completed (loss: 0.3452282249927521, acc: 0.9235668778419495)
[2025-02-16 12:31:05,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:06,087][root][INFO] - Training Epoch: 1/2, step 8738/23838 completed (loss: 0.6091870069503784, acc: 0.8442623019218445)
[2025-02-16 12:31:06,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:06,569][root][INFO] - Training Epoch: 1/2, step 8739/23838 completed (loss: 0.23808884620666504, acc: 0.9281437397003174)
[2025-02-16 12:31:06,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:06,975][root][INFO] - Training Epoch: 1/2, step 8740/23838 completed (loss: 0.42887622117996216, acc: 0.8783783912658691)
[2025-02-16 12:31:07,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:07,354][root][INFO] - Training Epoch: 1/2, step 8741/23838 completed (loss: 0.2713901996612549, acc: 0.9280575513839722)
[2025-02-16 12:31:07,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:07,724][root][INFO] - Training Epoch: 1/2, step 8742/23838 completed (loss: 0.9838581085205078, acc: 0.7352941036224365)
[2025-02-16 12:31:07,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:08,097][root][INFO] - Training Epoch: 1/2, step 8743/23838 completed (loss: 0.42390552163124084, acc: 0.8666666746139526)
[2025-02-16 12:31:08,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:08,502][root][INFO] - Training Epoch: 1/2, step 8744/23838 completed (loss: 0.6021533608436584, acc: 0.8396226167678833)
[2025-02-16 12:31:08,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:08,936][root][INFO] - Training Epoch: 1/2, step 8745/23838 completed (loss: 0.5751841068267822, acc: 0.8615384697914124)
[2025-02-16 12:31:09,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:09,400][root][INFO] - Training Epoch: 1/2, step 8746/23838 completed (loss: 0.18822209537029266, acc: 0.9444444179534912)
[2025-02-16 12:31:09,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:09,857][root][INFO] - Training Epoch: 1/2, step 8747/23838 completed (loss: 0.2556706368923187, acc: 0.9032257795333862)
[2025-02-16 12:31:10,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:10,285][root][INFO] - Training Epoch: 1/2, step 8748/23838 completed (loss: 0.1528780460357666, acc: 0.9636363387107849)
[2025-02-16 12:31:10,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:10,629][root][INFO] - Training Epoch: 1/2, step 8749/23838 completed (loss: 0.5856012105941772, acc: 0.8928571343421936)
[2025-02-16 12:31:10,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:10,997][root][INFO] - Training Epoch: 1/2, step 8750/23838 completed (loss: 0.13629716634750366, acc: 0.9425287246704102)
[2025-02-16 12:31:11,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:11,432][root][INFO] - Training Epoch: 1/2, step 8751/23838 completed (loss: 0.20355229079723358, acc: 0.9354838728904724)
[2025-02-16 12:31:11,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:11,794][root][INFO] - Training Epoch: 1/2, step 8752/23838 completed (loss: 0.2028873711824417, acc: 0.9519230723381042)
[2025-02-16 12:31:11,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:12,171][root][INFO] - Training Epoch: 1/2, step 8753/23838 completed (loss: 0.35389527678489685, acc: 0.9100000262260437)
[2025-02-16 12:31:12,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:12,540][root][INFO] - Training Epoch: 1/2, step 8754/23838 completed (loss: 0.9146149158477783, acc: 0.720588207244873)
[2025-02-16 12:31:12,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:13,004][root][INFO] - Training Epoch: 1/2, step 8755/23838 completed (loss: 0.12235128879547119, acc: 0.9662162065505981)
[2025-02-16 12:31:13,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:13,475][root][INFO] - Training Epoch: 1/2, step 8756/23838 completed (loss: 0.2239099144935608, acc: 0.951724112033844)
[2025-02-16 12:31:13,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:13,971][root][INFO] - Training Epoch: 1/2, step 8757/23838 completed (loss: 0.260602742433548, acc: 0.9324324131011963)
[2025-02-16 12:31:14,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:14,400][root][INFO] - Training Epoch: 1/2, step 8758/23838 completed (loss: 0.2886587083339691, acc: 0.929347813129425)
[2025-02-16 12:31:14,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:14,861][root][INFO] - Training Epoch: 1/2, step 8759/23838 completed (loss: 0.13257244229316711, acc: 0.9589743614196777)
[2025-02-16 12:31:15,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:15,276][root][INFO] - Training Epoch: 1/2, step 8760/23838 completed (loss: 0.6049795150756836, acc: 0.8269230723381042)
[2025-02-16 12:31:15,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:15,732][root][INFO] - Training Epoch: 1/2, step 8761/23838 completed (loss: 0.8102630972862244, acc: 0.84112149477005)
[2025-02-16 12:31:15,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:16,173][root][INFO] - Training Epoch: 1/2, step 8762/23838 completed (loss: 0.32436394691467285, acc: 0.9268292784690857)
[2025-02-16 12:31:16,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:16,820][root][INFO] - Training Epoch: 1/2, step 8763/23838 completed (loss: 0.24746666848659515, acc: 0.915032684803009)
[2025-02-16 12:31:17,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:17,288][root][INFO] - Training Epoch: 1/2, step 8764/23838 completed (loss: 0.14501258730888367, acc: 0.9557521939277649)
[2025-02-16 12:31:17,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:17,726][root][INFO] - Training Epoch: 1/2, step 8765/23838 completed (loss: 0.37110093235969543, acc: 0.9126213788986206)
[2025-02-16 12:31:17,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:18,168][root][INFO] - Training Epoch: 1/2, step 8766/23838 completed (loss: 0.2434377521276474, acc: 0.9052631855010986)
[2025-02-16 12:31:18,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:18,595][root][INFO] - Training Epoch: 1/2, step 8767/23838 completed (loss: 0.18848255276679993, acc: 0.9359999895095825)
[2025-02-16 12:31:18,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:19,045][root][INFO] - Training Epoch: 1/2, step 8768/23838 completed (loss: 0.42134302854537964, acc: 0.8769230842590332)
[2025-02-16 12:31:19,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:19,444][root][INFO] - Training Epoch: 1/2, step 8769/23838 completed (loss: 0.4020196795463562, acc: 0.8848921060562134)
[2025-02-16 12:31:19,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:19,880][root][INFO] - Training Epoch: 1/2, step 8770/23838 completed (loss: 0.18713119626045227, acc: 0.9518716335296631)
[2025-02-16 12:31:20,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:20,298][root][INFO] - Training Epoch: 1/2, step 8771/23838 completed (loss: 0.23916611075401306, acc: 0.9354838728904724)
[2025-02-16 12:31:20,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:20,710][root][INFO] - Training Epoch: 1/2, step 8772/23838 completed (loss: 0.19828203320503235, acc: 0.932692289352417)
[2025-02-16 12:31:20,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:21,081][root][INFO] - Training Epoch: 1/2, step 8773/23838 completed (loss: 0.37666383385658264, acc: 0.8823529481887817)
[2025-02-16 12:31:21,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:21,530][root][INFO] - Training Epoch: 1/2, step 8774/23838 completed (loss: 0.41820546984672546, acc: 0.8817204236984253)
[2025-02-16 12:31:21,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:21,930][root][INFO] - Training Epoch: 1/2, step 8775/23838 completed (loss: 0.36582180857658386, acc: 0.9013158082962036)
[2025-02-16 12:31:22,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:22,395][root][INFO] - Training Epoch: 1/2, step 8776/23838 completed (loss: 0.2106635421514511, acc: 0.9259259104728699)
[2025-02-16 12:31:22,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:22,850][root][INFO] - Training Epoch: 1/2, step 8777/23838 completed (loss: 0.47294798493385315, acc: 0.8828828930854797)
[2025-02-16 12:31:22,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:23,220][root][INFO] - Training Epoch: 1/2, step 8778/23838 completed (loss: 0.5437411665916443, acc: 0.8117647171020508)
[2025-02-16 12:31:23,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:23,618][root][INFO] - Training Epoch: 1/2, step 8779/23838 completed (loss: 0.5490558743476868, acc: 0.8372092843055725)
[2025-02-16 12:31:23,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:24,060][root][INFO] - Training Epoch: 1/2, step 8780/23838 completed (loss: 0.39902400970458984, acc: 0.8972602486610413)
[2025-02-16 12:31:24,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:24,454][root][INFO] - Training Epoch: 1/2, step 8781/23838 completed (loss: 0.5606927275657654, acc: 0.8600000143051147)
[2025-02-16 12:31:24,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:24,851][root][INFO] - Training Epoch: 1/2, step 8782/23838 completed (loss: 0.28580242395401, acc: 0.9137930870056152)
[2025-02-16 12:31:25,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:25,259][root][INFO] - Training Epoch: 1/2, step 8783/23838 completed (loss: 0.2666204571723938, acc: 0.93388432264328)
[2025-02-16 12:31:25,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:25,723][root][INFO] - Training Epoch: 1/2, step 8784/23838 completed (loss: 0.4901266098022461, acc: 0.9020978808403015)
[2025-02-16 12:31:25,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:26,169][root][INFO] - Training Epoch: 1/2, step 8785/23838 completed (loss: 0.4434337913990021, acc: 0.8725489974021912)
[2025-02-16 12:31:26,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:26,634][root][INFO] - Training Epoch: 1/2, step 8786/23838 completed (loss: 0.38601914048194885, acc: 0.8764705657958984)
[2025-02-16 12:31:26,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:27,062][root][INFO] - Training Epoch: 1/2, step 8787/23838 completed (loss: 0.4493992328643799, acc: 0.9005848169326782)
[2025-02-16 12:31:27,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:27,430][root][INFO] - Training Epoch: 1/2, step 8788/23838 completed (loss: 0.2936640977859497, acc: 0.939130425453186)
[2025-02-16 12:31:27,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:27,864][root][INFO] - Training Epoch: 1/2, step 8789/23838 completed (loss: 0.428780198097229, acc: 0.8601398468017578)
[2025-02-16 12:31:28,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:28,367][root][INFO] - Training Epoch: 1/2, step 8790/23838 completed (loss: 0.13767960667610168, acc: 0.96484375)
[2025-02-16 12:31:28,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:28,823][root][INFO] - Training Epoch: 1/2, step 8791/23838 completed (loss: 0.5383671522140503, acc: 0.8514851331710815)
[2025-02-16 12:31:29,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:29,248][root][INFO] - Training Epoch: 1/2, step 8792/23838 completed (loss: 0.18207964301109314, acc: 0.9898989796638489)
[2025-02-16 12:31:29,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:29,818][root][INFO] - Training Epoch: 1/2, step 8793/23838 completed (loss: 0.18408869206905365, acc: 0.95686274766922)
[2025-02-16 12:31:30,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:30,289][root][INFO] - Training Epoch: 1/2, step 8794/23838 completed (loss: 0.8106886148452759, acc: 0.7906976938247681)
[2025-02-16 12:31:30,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:30,720][root][INFO] - Training Epoch: 1/2, step 8795/23838 completed (loss: 0.29191794991493225, acc: 0.9263157844543457)
[2025-02-16 12:31:30,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:31,152][root][INFO] - Training Epoch: 1/2, step 8796/23838 completed (loss: 0.5521443486213684, acc: 0.8803418874740601)
[2025-02-16 12:31:31,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:31,656][root][INFO] - Training Epoch: 1/2, step 8797/23838 completed (loss: 0.29174038767814636, acc: 0.9144384860992432)
[2025-02-16 12:31:31,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:32,118][root][INFO] - Training Epoch: 1/2, step 8798/23838 completed (loss: 0.281191885471344, acc: 0.8846153616905212)
[2025-02-16 12:31:32,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:32,589][root][INFO] - Training Epoch: 1/2, step 8799/23838 completed (loss: 0.8941969871520996, acc: 0.791304349899292)
[2025-02-16 12:31:32,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:33,019][root][INFO] - Training Epoch: 1/2, step 8800/23838 completed (loss: 0.5898571610450745, acc: 0.8712871074676514)
[2025-02-16 12:31:33,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:33,450][root][INFO] - Training Epoch: 1/2, step 8801/23838 completed (loss: 0.899055540561676, acc: 0.7659574747085571)
[2025-02-16 12:31:33,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:33,948][root][INFO] - Training Epoch: 1/2, step 8802/23838 completed (loss: 0.3805318772792816, acc: 0.8854166865348816)
[2025-02-16 12:31:34,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:34,507][root][INFO] - Training Epoch: 1/2, step 8803/23838 completed (loss: 0.6815553903579712, acc: 0.8305084705352783)
[2025-02-16 12:31:34,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:34,949][root][INFO] - Training Epoch: 1/2, step 8804/23838 completed (loss: 0.2446509599685669, acc: 0.931034505367279)
[2025-02-16 12:31:35,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:35,426][root][INFO] - Training Epoch: 1/2, step 8805/23838 completed (loss: 0.3615676164627075, acc: 0.8999999761581421)
[2025-02-16 12:31:35,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:35,851][root][INFO] - Training Epoch: 1/2, step 8806/23838 completed (loss: 0.53971928358078, acc: 0.8474576473236084)
[2025-02-16 12:31:36,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:36,268][root][INFO] - Training Epoch: 1/2, step 8807/23838 completed (loss: 0.379109263420105, acc: 0.9139072895050049)
[2025-02-16 12:31:36,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:36,711][root][INFO] - Training Epoch: 1/2, step 8808/23838 completed (loss: 0.5659836530685425, acc: 0.8493150472640991)
[2025-02-16 12:31:36,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:37,182][root][INFO] - Training Epoch: 1/2, step 8809/23838 completed (loss: 0.7068201303482056, acc: 0.7307692170143127)
[2025-02-16 12:31:37,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:37,659][root][INFO] - Training Epoch: 1/2, step 8810/23838 completed (loss: 0.3069009482860565, acc: 0.90625)
[2025-02-16 12:31:37,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:38,061][root][INFO] - Training Epoch: 1/2, step 8811/23838 completed (loss: 0.07574653625488281, acc: 0.9847328066825867)
[2025-02-16 12:31:38,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:38,427][root][INFO] - Training Epoch: 1/2, step 8812/23838 completed (loss: 0.40319114923477173, acc: 0.9032257795333862)
[2025-02-16 12:31:38,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:38,849][root][INFO] - Training Epoch: 1/2, step 8813/23838 completed (loss: 0.2638918161392212, acc: 0.9385964870452881)
[2025-02-16 12:31:39,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:39,326][root][INFO] - Training Epoch: 1/2, step 8814/23838 completed (loss: 0.2762121558189392, acc: 0.9357798099517822)
[2025-02-16 12:31:39,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:39,747][root][INFO] - Training Epoch: 1/2, step 8815/23838 completed (loss: 0.9727059006690979, acc: 0.7049180269241333)
[2025-02-16 12:31:39,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:40,139][root][INFO] - Training Epoch: 1/2, step 8816/23838 completed (loss: 0.40039777755737305, acc: 0.8714285492897034)
[2025-02-16 12:31:40,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:40,578][root][INFO] - Training Epoch: 1/2, step 8817/23838 completed (loss: 0.3725062310695648, acc: 0.8999999761581421)
[2025-02-16 12:31:40,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:41,081][root][INFO] - Training Epoch: 1/2, step 8818/23838 completed (loss: 0.32805001735687256, acc: 0.8895348906517029)
[2025-02-16 12:31:41,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:41,543][root][INFO] - Training Epoch: 1/2, step 8819/23838 completed (loss: 0.24390441179275513, acc: 0.9166666865348816)
[2025-02-16 12:31:41,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:41,942][root][INFO] - Training Epoch: 1/2, step 8820/23838 completed (loss: 0.22087281942367554, acc: 0.9370629191398621)
[2025-02-16 12:31:42,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:42,350][root][INFO] - Training Epoch: 1/2, step 8821/23838 completed (loss: 0.5506787896156311, acc: 0.843137264251709)
[2025-02-16 12:31:42,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:42,801][root][INFO] - Training Epoch: 1/2, step 8822/23838 completed (loss: 0.8756141662597656, acc: 0.75)
[2025-02-16 12:31:42,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:43,241][root][INFO] - Training Epoch: 1/2, step 8823/23838 completed (loss: 0.6433136463165283, acc: 0.8256880640983582)
[2025-02-16 12:31:43,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:43,677][root][INFO] - Training Epoch: 1/2, step 8824/23838 completed (loss: 0.6138964295387268, acc: 0.8207547068595886)
[2025-02-16 12:31:43,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:44,077][root][INFO] - Training Epoch: 1/2, step 8825/23838 completed (loss: 0.6802218556404114, acc: 0.8365384340286255)
[2025-02-16 12:31:44,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:44,522][root][INFO] - Training Epoch: 1/2, step 8826/23838 completed (loss: 0.8549168705940247, acc: 0.8148148059844971)
[2025-02-16 12:31:44,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:44,918][root][INFO] - Training Epoch: 1/2, step 8827/23838 completed (loss: 0.6812820434570312, acc: 0.841269850730896)
[2025-02-16 12:31:45,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:45,346][root][INFO] - Training Epoch: 1/2, step 8828/23838 completed (loss: 0.3005087375640869, acc: 0.9402984976768494)
[2025-02-16 12:31:45,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:45,787][root][INFO] - Training Epoch: 1/2, step 8829/23838 completed (loss: 0.3818822503089905, acc: 0.8944099545478821)
[2025-02-16 12:31:45,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:46,250][root][INFO] - Training Epoch: 1/2, step 8830/23838 completed (loss: 0.6085023880004883, acc: 0.818791925907135)
[2025-02-16 12:31:46,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:46,700][root][INFO] - Training Epoch: 1/2, step 8831/23838 completed (loss: 0.3803359568119049, acc: 0.8888888955116272)
[2025-02-16 12:31:46,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:47,179][root][INFO] - Training Epoch: 1/2, step 8832/23838 completed (loss: 0.5951928496360779, acc: 0.8389261960983276)
[2025-02-16 12:31:47,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:47,641][root][INFO] - Training Epoch: 1/2, step 8833/23838 completed (loss: 0.7369092702865601, acc: 0.8354430198669434)
[2025-02-16 12:31:47,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:48,113][root][INFO] - Training Epoch: 1/2, step 8834/23838 completed (loss: 0.41923588514328003, acc: 0.8888888955116272)
[2025-02-16 12:31:48,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:48,584][root][INFO] - Training Epoch: 1/2, step 8835/23838 completed (loss: 0.31760910153388977, acc: 0.9077669978141785)
[2025-02-16 12:31:48,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:49,006][root][INFO] - Training Epoch: 1/2, step 8836/23838 completed (loss: 0.4301983416080475, acc: 0.8563535809516907)
[2025-02-16 12:31:49,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:49,424][root][INFO] - Training Epoch: 1/2, step 8837/23838 completed (loss: 0.28344422578811646, acc: 0.8992248177528381)
[2025-02-16 12:31:49,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:49,901][root][INFO] - Training Epoch: 1/2, step 8838/23838 completed (loss: 0.7677054405212402, acc: 0.8055555820465088)
[2025-02-16 12:31:50,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:50,371][root][INFO] - Training Epoch: 1/2, step 8839/23838 completed (loss: 0.5231048464775085, acc: 0.8512820601463318)
[2025-02-16 12:31:50,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:50,790][root][INFO] - Training Epoch: 1/2, step 8840/23838 completed (loss: 0.5805014967918396, acc: 0.8659217953681946)
[2025-02-16 12:31:50,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:51,229][root][INFO] - Training Epoch: 1/2, step 8841/23838 completed (loss: 0.9486163854598999, acc: 0.7171717286109924)
[2025-02-16 12:31:51,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:51,641][root][INFO] - Training Epoch: 1/2, step 8842/23838 completed (loss: 0.7728136777877808, acc: 0.8039215803146362)
[2025-02-16 12:31:51,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:52,069][root][INFO] - Training Epoch: 1/2, step 8843/23838 completed (loss: 0.9071827530860901, acc: 0.7727272510528564)
[2025-02-16 12:31:52,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:52,519][root][INFO] - Training Epoch: 1/2, step 8844/23838 completed (loss: 0.9546817541122437, acc: 0.773809552192688)
[2025-02-16 12:31:52,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:52,972][root][INFO] - Training Epoch: 1/2, step 8845/23838 completed (loss: 0.45028555393218994, acc: 0.8999999761581421)
[2025-02-16 12:31:53,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:53,419][root][INFO] - Training Epoch: 1/2, step 8846/23838 completed (loss: 1.0040746927261353, acc: 0.7681159377098083)
[2025-02-16 12:31:53,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:53,862][root][INFO] - Training Epoch: 1/2, step 8847/23838 completed (loss: 0.3925769031047821, acc: 0.9176470637321472)
[2025-02-16 12:31:54,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:54,302][root][INFO] - Training Epoch: 1/2, step 8848/23838 completed (loss: 0.9938114881515503, acc: 0.7397260069847107)
[2025-02-16 12:31:54,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:54,758][root][INFO] - Training Epoch: 1/2, step 8849/23838 completed (loss: 0.6338031888008118, acc: 0.8081395626068115)
[2025-02-16 12:31:54,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:55,194][root][INFO] - Training Epoch: 1/2, step 8850/23838 completed (loss: 0.24442528188228607, acc: 0.9330143332481384)
[2025-02-16 12:31:55,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:55,660][root][INFO] - Training Epoch: 1/2, step 8851/23838 completed (loss: 0.33755651116371155, acc: 0.9103448390960693)
[2025-02-16 12:31:55,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:56,093][root][INFO] - Training Epoch: 1/2, step 8852/23838 completed (loss: 0.3861410617828369, acc: 0.8888888955116272)
[2025-02-16 12:31:56,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:56,533][root][INFO] - Training Epoch: 1/2, step 8853/23838 completed (loss: 0.6110431551933289, acc: 0.8666666746139526)
[2025-02-16 12:31:56,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:56,971][root][INFO] - Training Epoch: 1/2, step 8854/23838 completed (loss: 0.42274755239486694, acc: 0.8611111044883728)
[2025-02-16 12:31:57,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:57,420][root][INFO] - Training Epoch: 1/2, step 8855/23838 completed (loss: 0.2786996066570282, acc: 0.9512194991111755)
[2025-02-16 12:31:57,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:57,917][root][INFO] - Training Epoch: 1/2, step 8856/23838 completed (loss: 0.17802157998085022, acc: 0.9263803958892822)
[2025-02-16 12:31:58,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:58,367][root][INFO] - Training Epoch: 1/2, step 8857/23838 completed (loss: 0.15693990886211395, acc: 0.9384615421295166)
[2025-02-16 12:31:58,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:58,891][root][INFO] - Training Epoch: 1/2, step 8858/23838 completed (loss: 0.34601110219955444, acc: 0.8983957171440125)
[2025-02-16 12:31:59,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:59,323][root][INFO] - Training Epoch: 1/2, step 8859/23838 completed (loss: 0.25278711318969727, acc: 0.9102563858032227)
[2025-02-16 12:31:59,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:31:59,811][root][INFO] - Training Epoch: 1/2, step 8860/23838 completed (loss: 0.25063812732696533, acc: 0.9113923907279968)
[2025-02-16 12:31:59,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:00,187][root][INFO] - Training Epoch: 1/2, step 8861/23838 completed (loss: 0.48819267749786377, acc: 0.9012345671653748)
[2025-02-16 12:32:00,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:00,617][root][INFO] - Training Epoch: 1/2, step 8862/23838 completed (loss: 0.23051953315734863, acc: 0.9459459185600281)
[2025-02-16 12:32:00,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:01,029][root][INFO] - Training Epoch: 1/2, step 8863/23838 completed (loss: 0.23799313604831696, acc: 0.9406779408454895)
[2025-02-16 12:32:01,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:01,466][root][INFO] - Training Epoch: 1/2, step 8864/23838 completed (loss: 0.49468186497688293, acc: 0.8533333539962769)
[2025-02-16 12:32:01,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:01,943][root][INFO] - Training Epoch: 1/2, step 8865/23838 completed (loss: 0.3211649954319, acc: 0.9186992049217224)
[2025-02-16 12:32:02,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:02,303][root][INFO] - Training Epoch: 1/2, step 8866/23838 completed (loss: 0.4404515326023102, acc: 0.875)
[2025-02-16 12:32:02,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:02,730][root][INFO] - Training Epoch: 1/2, step 8867/23838 completed (loss: 0.667783796787262, acc: 0.8333333134651184)
[2025-02-16 12:32:02,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:03,112][root][INFO] - Training Epoch: 1/2, step 8868/23838 completed (loss: 0.5063919425010681, acc: 0.8469387888908386)
[2025-02-16 12:32:03,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:03,535][root][INFO] - Training Epoch: 1/2, step 8869/23838 completed (loss: 0.6144830584526062, acc: 0.8292682766914368)
[2025-02-16 12:32:03,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:04,079][root][INFO] - Training Epoch: 1/2, step 8870/23838 completed (loss: 0.456885427236557, acc: 0.8666666746139526)
[2025-02-16 12:32:04,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:04,490][root][INFO] - Training Epoch: 1/2, step 8871/23838 completed (loss: 0.2698410153388977, acc: 0.9200000166893005)
[2025-02-16 12:32:04,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:04,926][root][INFO] - Training Epoch: 1/2, step 8872/23838 completed (loss: 0.42734652757644653, acc: 0.9275362491607666)
[2025-02-16 12:32:05,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:05,349][root][INFO] - Training Epoch: 1/2, step 8873/23838 completed (loss: 0.320794939994812, acc: 0.9166666865348816)
[2025-02-16 12:32:05,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:05,719][root][INFO] - Training Epoch: 1/2, step 8874/23838 completed (loss: 0.29041141271591187, acc: 0.9487179517745972)
[2025-02-16 12:32:05,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:06,152][root][INFO] - Training Epoch: 1/2, step 8875/23838 completed (loss: 0.5257195234298706, acc: 0.795918345451355)
[2025-02-16 12:32:06,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:06,609][root][INFO] - Training Epoch: 1/2, step 8876/23838 completed (loss: 0.76420658826828, acc: 0.6875)
[2025-02-16 12:32:06,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:07,076][root][INFO] - Training Epoch: 1/2, step 8877/23838 completed (loss: 0.9026523232460022, acc: 0.7906976938247681)
[2025-02-16 12:32:07,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:07,530][root][INFO] - Training Epoch: 1/2, step 8878/23838 completed (loss: 0.8496996760368347, acc: 0.7368420958518982)
[2025-02-16 12:32:07,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:07,926][root][INFO] - Training Epoch: 1/2, step 8879/23838 completed (loss: 0.26329872012138367, acc: 0.9382715821266174)
[2025-02-16 12:32:08,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:08,378][root][INFO] - Training Epoch: 1/2, step 8880/23838 completed (loss: 0.13180546462535858, acc: 0.9518072009086609)
[2025-02-16 12:32:08,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:08,769][root][INFO] - Training Epoch: 1/2, step 8881/23838 completed (loss: 0.7079628109931946, acc: 0.8225806355476379)
[2025-02-16 12:32:08,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:09,204][root][INFO] - Training Epoch: 1/2, step 8882/23838 completed (loss: 0.6603572964668274, acc: 0.8695651888847351)
[2025-02-16 12:32:09,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:09,648][root][INFO] - Training Epoch: 1/2, step 8883/23838 completed (loss: 0.14341431856155396, acc: 0.9382715821266174)
[2025-02-16 12:32:09,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:10,097][root][INFO] - Training Epoch: 1/2, step 8884/23838 completed (loss: 0.22497518360614777, acc: 0.9444444179534912)
[2025-02-16 12:32:10,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:10,494][root][INFO] - Training Epoch: 1/2, step 8885/23838 completed (loss: 0.9459713101387024, acc: 0.7647058963775635)
[2025-02-16 12:32:10,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:10,931][root][INFO] - Training Epoch: 1/2, step 8886/23838 completed (loss: 0.7217239737510681, acc: 0.8208954930305481)
[2025-02-16 12:32:11,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:11,373][root][INFO] - Training Epoch: 1/2, step 8887/23838 completed (loss: 0.13093072175979614, acc: 0.9659090638160706)
[2025-02-16 12:32:11,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:11,824][root][INFO] - Training Epoch: 1/2, step 8888/23838 completed (loss: 0.26695939898490906, acc: 0.8936170339584351)
[2025-02-16 12:32:12,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:12,262][root][INFO] - Training Epoch: 1/2, step 8889/23838 completed (loss: 0.4852636456489563, acc: 0.8653846383094788)
[2025-02-16 12:32:12,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:12,693][root][INFO] - Training Epoch: 1/2, step 8890/23838 completed (loss: 0.22457948327064514, acc: 0.9491525292396545)
[2025-02-16 12:32:12,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:13,130][root][INFO] - Training Epoch: 1/2, step 8891/23838 completed (loss: 1.062980055809021, acc: 0.7346938848495483)
[2025-02-16 12:32:13,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:13,518][root][INFO] - Training Epoch: 1/2, step 8892/23838 completed (loss: 0.9906952381134033, acc: 0.7083333134651184)
[2025-02-16 12:32:13,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:13,956][root][INFO] - Training Epoch: 1/2, step 8893/23838 completed (loss: 0.8232367634773254, acc: 0.8518518805503845)
[2025-02-16 12:32:14,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:14,372][root][INFO] - Training Epoch: 1/2, step 8894/23838 completed (loss: 0.17305240035057068, acc: 0.9800000190734863)
[2025-02-16 12:32:14,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:14,756][root][INFO] - Training Epoch: 1/2, step 8895/23838 completed (loss: 0.43889519572257996, acc: 0.9019607901573181)
[2025-02-16 12:32:14,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:15,193][root][INFO] - Training Epoch: 1/2, step 8896/23838 completed (loss: 0.26034262776374817, acc: 0.9482758641242981)
[2025-02-16 12:32:15,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:15,661][root][INFO] - Training Epoch: 1/2, step 8897/23838 completed (loss: 0.636659562587738, acc: 0.843137264251709)
[2025-02-16 12:32:15,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:16,112][root][INFO] - Training Epoch: 1/2, step 8898/23838 completed (loss: 0.4752538800239563, acc: 0.8392857313156128)
[2025-02-16 12:32:16,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:16,523][root][INFO] - Training Epoch: 1/2, step 8899/23838 completed (loss: 0.48557159304618835, acc: 0.8873239159584045)
[2025-02-16 12:32:16,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:16,945][root][INFO] - Training Epoch: 1/2, step 8900/23838 completed (loss: 0.31923219561576843, acc: 0.930232584476471)
[2025-02-16 12:32:17,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:17,376][root][INFO] - Training Epoch: 1/2, step 8901/23838 completed (loss: 0.3333859443664551, acc: 0.9047619104385376)
[2025-02-16 12:32:17,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:17,851][root][INFO] - Training Epoch: 1/2, step 8902/23838 completed (loss: 0.287122517824173, acc: 0.8974359035491943)
[2025-02-16 12:32:18,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:18,329][root][INFO] - Training Epoch: 1/2, step 8903/23838 completed (loss: 0.526039719581604, acc: 0.8727272748947144)
[2025-02-16 12:32:18,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:18,919][root][INFO] - Training Epoch: 1/2, step 8904/23838 completed (loss: 0.1016036719083786, acc: 0.9789473414421082)
[2025-02-16 12:32:19,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:19,344][root][INFO] - Training Epoch: 1/2, step 8905/23838 completed (loss: 0.31886109709739685, acc: 0.9239130616188049)
[2025-02-16 12:32:19,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:19,740][root][INFO] - Training Epoch: 1/2, step 8906/23838 completed (loss: 0.16638179123401642, acc: 0.9367088675498962)
[2025-02-16 12:32:19,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:20,104][root][INFO] - Training Epoch: 1/2, step 8907/23838 completed (loss: 0.39540034532546997, acc: 0.8421052694320679)
[2025-02-16 12:32:20,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:20,459][root][INFO] - Training Epoch: 1/2, step 8908/23838 completed (loss: 1.360609769821167, acc: 0.6595744490623474)
[2025-02-16 12:32:20,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:20,876][root][INFO] - Training Epoch: 1/2, step 8909/23838 completed (loss: 0.5258985757827759, acc: 0.7692307829856873)
[2025-02-16 12:32:21,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:21,281][root][INFO] - Training Epoch: 1/2, step 8910/23838 completed (loss: 0.7938898205757141, acc: 0.75)
[2025-02-16 12:32:21,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:21,733][root][INFO] - Training Epoch: 1/2, step 8911/23838 completed (loss: 0.3539784550666809, acc: 0.9056603908538818)
[2025-02-16 12:32:21,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:22,168][root][INFO] - Training Epoch: 1/2, step 8912/23838 completed (loss: 0.38075563311576843, acc: 0.8947368264198303)
[2025-02-16 12:32:22,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:22,634][root][INFO] - Training Epoch: 1/2, step 8913/23838 completed (loss: 0.2519363760948181, acc: 0.9615384340286255)
[2025-02-16 12:32:22,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:23,079][root][INFO] - Training Epoch: 1/2, step 8914/23838 completed (loss: 0.25146234035491943, acc: 0.9142857193946838)
[2025-02-16 12:32:23,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:23,504][root][INFO] - Training Epoch: 1/2, step 8915/23838 completed (loss: 0.35948729515075684, acc: 0.8775510191917419)
[2025-02-16 12:32:23,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:23,865][root][INFO] - Training Epoch: 1/2, step 8916/23838 completed (loss: 0.2920204699039459, acc: 0.8837209343910217)
[2025-02-16 12:32:24,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:24,293][root][INFO] - Training Epoch: 1/2, step 8917/23838 completed (loss: 0.522691547870636, acc: 0.8604651093482971)
[2025-02-16 12:32:24,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:24,771][root][INFO] - Training Epoch: 1/2, step 8918/23838 completed (loss: 0.397487998008728, acc: 0.8620689511299133)
[2025-02-16 12:32:24,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:25,192][root][INFO] - Training Epoch: 1/2, step 8919/23838 completed (loss: 0.15099644660949707, acc: 0.9599999785423279)
[2025-02-16 12:32:25,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:25,706][root][INFO] - Training Epoch: 1/2, step 8920/23838 completed (loss: 0.1906430870294571, acc: 0.9504950642585754)
[2025-02-16 12:32:25,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:26,141][root][INFO] - Training Epoch: 1/2, step 8921/23838 completed (loss: 0.17437313497066498, acc: 0.9772727489471436)
[2025-02-16 12:32:26,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:26,600][root][INFO] - Training Epoch: 1/2, step 8922/23838 completed (loss: 0.4277612566947937, acc: 0.8809523582458496)
[2025-02-16 12:32:26,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:27,024][root][INFO] - Training Epoch: 1/2, step 8923/23838 completed (loss: 0.5167288780212402, acc: 0.8181818127632141)
[2025-02-16 12:32:27,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:27,474][root][INFO] - Training Epoch: 1/2, step 8924/23838 completed (loss: 0.14943300187587738, acc: 0.9642857313156128)
[2025-02-16 12:32:27,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:27,906][root][INFO] - Training Epoch: 1/2, step 8925/23838 completed (loss: 0.43601229786872864, acc: 0.9154929518699646)
[2025-02-16 12:32:28,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:28,279][root][INFO] - Training Epoch: 1/2, step 8926/23838 completed (loss: 0.23401471972465515, acc: 0.9130434989929199)
[2025-02-16 12:32:28,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:28,719][root][INFO] - Training Epoch: 1/2, step 8927/23838 completed (loss: 0.3838432729244232, acc: 0.8902438879013062)
[2025-02-16 12:32:28,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:29,143][root][INFO] - Training Epoch: 1/2, step 8928/23838 completed (loss: 0.38852953910827637, acc: 0.8888888955116272)
[2025-02-16 12:32:29,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:29,533][root][INFO] - Training Epoch: 1/2, step 8929/23838 completed (loss: 0.696779727935791, acc: 0.8275862336158752)
[2025-02-16 12:32:29,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:29,989][root][INFO] - Training Epoch: 1/2, step 8930/23838 completed (loss: 0.2598170042037964, acc: 0.8936170339584351)
[2025-02-16 12:32:30,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:30,452][root][INFO] - Training Epoch: 1/2, step 8931/23838 completed (loss: 0.4707472324371338, acc: 0.8780487775802612)
[2025-02-16 12:32:30,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:30,873][root][INFO] - Training Epoch: 1/2, step 8932/23838 completed (loss: 0.3405030071735382, acc: 0.9365079402923584)
[2025-02-16 12:32:31,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:31,242][root][INFO] - Training Epoch: 1/2, step 8933/23838 completed (loss: 0.5020936727523804, acc: 0.8870967626571655)
[2025-02-16 12:32:31,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:31,618][root][INFO] - Training Epoch: 1/2, step 8934/23838 completed (loss: 0.32799115777015686, acc: 0.9215686321258545)
[2025-02-16 12:32:31,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:32,071][root][INFO] - Training Epoch: 1/2, step 8935/23838 completed (loss: 0.38620230555534363, acc: 0.8684210777282715)
[2025-02-16 12:32:32,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:32,496][root][INFO] - Training Epoch: 1/2, step 8936/23838 completed (loss: 0.16897740960121155, acc: 0.9743589758872986)
[2025-02-16 12:32:32,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:32,985][root][INFO] - Training Epoch: 1/2, step 8937/23838 completed (loss: 0.23756906390190125, acc: 0.9387755393981934)
[2025-02-16 12:32:33,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:33,364][root][INFO] - Training Epoch: 1/2, step 8938/23838 completed (loss: 0.6053740978240967, acc: 0.828125)
[2025-02-16 12:32:33,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:33,822][root][INFO] - Training Epoch: 1/2, step 8939/23838 completed (loss: 0.4454576075077057, acc: 0.875)
[2025-02-16 12:32:34,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:34,197][root][INFO] - Training Epoch: 1/2, step 8940/23838 completed (loss: 0.28108200430870056, acc: 0.890625)
[2025-02-16 12:32:34,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:34,612][root][INFO] - Training Epoch: 1/2, step 8941/23838 completed (loss: 0.3719317615032196, acc: 0.868852436542511)
[2025-02-16 12:32:34,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:35,011][root][INFO] - Training Epoch: 1/2, step 8942/23838 completed (loss: 0.34177151322364807, acc: 0.9253731369972229)
[2025-02-16 12:32:35,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:35,405][root][INFO] - Training Epoch: 1/2, step 8943/23838 completed (loss: 0.30152955651283264, acc: 0.875)
[2025-02-16 12:32:35,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:35,865][root][INFO] - Training Epoch: 1/2, step 8944/23838 completed (loss: 0.1485099047422409, acc: 0.9692307710647583)
[2025-02-16 12:32:36,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:36,339][root][INFO] - Training Epoch: 1/2, step 8945/23838 completed (loss: 0.7155705094337463, acc: 0.84375)
[2025-02-16 12:32:36,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:36,808][root][INFO] - Training Epoch: 1/2, step 8946/23838 completed (loss: 0.52435302734375, acc: 0.8604651093482971)
[2025-02-16 12:32:37,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:37,243][root][INFO] - Training Epoch: 1/2, step 8947/23838 completed (loss: 0.3367723226547241, acc: 0.9016393423080444)
[2025-02-16 12:32:37,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:37,656][root][INFO] - Training Epoch: 1/2, step 8948/23838 completed (loss: 0.36193203926086426, acc: 0.9027777910232544)
[2025-02-16 12:32:37,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:38,072][root][INFO] - Training Epoch: 1/2, step 8949/23838 completed (loss: 1.2038359642028809, acc: 0.7209302186965942)
[2025-02-16 12:32:38,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:38,505][root][INFO] - Training Epoch: 1/2, step 8950/23838 completed (loss: 0.5760931968688965, acc: 0.8199999928474426)
[2025-02-16 12:32:38,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:38,914][root][INFO] - Training Epoch: 1/2, step 8951/23838 completed (loss: 0.5629104375839233, acc: 0.8390804529190063)
[2025-02-16 12:32:39,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:39,286][root][INFO] - Training Epoch: 1/2, step 8952/23838 completed (loss: 0.444637656211853, acc: 0.8799999952316284)
[2025-02-16 12:32:39,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:39,717][root][INFO] - Training Epoch: 1/2, step 8953/23838 completed (loss: 0.4606892168521881, acc: 0.9056603908538818)
[2025-02-16 12:32:39,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:40,124][root][INFO] - Training Epoch: 1/2, step 8954/23838 completed (loss: 0.28118881583213806, acc: 0.9285714030265808)
[2025-02-16 12:32:40,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:40,534][root][INFO] - Training Epoch: 1/2, step 8955/23838 completed (loss: 0.4053089916706085, acc: 0.8846153616905212)
[2025-02-16 12:32:40,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:40,914][root][INFO] - Training Epoch: 1/2, step 8956/23838 completed (loss: 0.8770986199378967, acc: 0.6764705777168274)
[2025-02-16 12:32:41,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:41,328][root][INFO] - Training Epoch: 1/2, step 8957/23838 completed (loss: 0.8118310570716858, acc: 0.8205128312110901)
[2025-02-16 12:32:41,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:41,746][root][INFO] - Training Epoch: 1/2, step 8958/23838 completed (loss: 0.3638319969177246, acc: 0.8947368264198303)
[2025-02-16 12:32:41,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:42,107][root][INFO] - Training Epoch: 1/2, step 8959/23838 completed (loss: 0.18084456026554108, acc: 0.9512194991111755)
[2025-02-16 12:32:42,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:42,486][root][INFO] - Training Epoch: 1/2, step 8960/23838 completed (loss: 1.0028467178344727, acc: 0.7878788113594055)
[2025-02-16 12:32:42,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:42,916][root][INFO] - Training Epoch: 1/2, step 8961/23838 completed (loss: 0.8827401995658875, acc: 0.7575757503509521)
[2025-02-16 12:32:43,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:43,321][root][INFO] - Training Epoch: 1/2, step 8962/23838 completed (loss: 0.48801201581954956, acc: 0.8648648858070374)
[2025-02-16 12:32:43,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:43,693][root][INFO] - Training Epoch: 1/2, step 8963/23838 completed (loss: 0.5140405297279358, acc: 0.8363636136054993)
[2025-02-16 12:32:43,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:44,143][root][INFO] - Training Epoch: 1/2, step 8964/23838 completed (loss: 0.15296649932861328, acc: 0.9599999785423279)
[2025-02-16 12:32:44,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:44,631][root][INFO] - Training Epoch: 1/2, step 8965/23838 completed (loss: 0.5161851644515991, acc: 0.8135592937469482)
[2025-02-16 12:32:44,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:45,040][root][INFO] - Training Epoch: 1/2, step 8966/23838 completed (loss: 0.7192506194114685, acc: 0.8157894611358643)
[2025-02-16 12:32:45,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:45,471][root][INFO] - Training Epoch: 1/2, step 8967/23838 completed (loss: 0.13260869681835175, acc: 0.9661017060279846)
[2025-02-16 12:32:45,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:45,921][root][INFO] - Training Epoch: 1/2, step 8968/23838 completed (loss: 0.3890664577484131, acc: 0.8695651888847351)
[2025-02-16 12:32:46,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:46,373][root][INFO] - Training Epoch: 1/2, step 8969/23838 completed (loss: 0.6029584407806396, acc: 0.8399999737739563)
[2025-02-16 12:32:46,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:46,863][root][INFO] - Training Epoch: 1/2, step 8970/23838 completed (loss: 0.5488442778587341, acc: 0.8421052694320679)
[2025-02-16 12:32:47,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:47,225][root][INFO] - Training Epoch: 1/2, step 8971/23838 completed (loss: 0.6381897330284119, acc: 0.7971014380455017)
[2025-02-16 12:32:47,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:47,608][root][INFO] - Training Epoch: 1/2, step 8972/23838 completed (loss: 0.12939845025539398, acc: 0.953125)
[2025-02-16 12:32:47,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:48,044][root][INFO] - Training Epoch: 1/2, step 8973/23838 completed (loss: 0.30323728919029236, acc: 0.9583333134651184)
[2025-02-16 12:32:48,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:48,486][root][INFO] - Training Epoch: 1/2, step 8974/23838 completed (loss: 0.31629616022109985, acc: 0.8571428656578064)
[2025-02-16 12:32:48,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:48,916][root][INFO] - Training Epoch: 1/2, step 8975/23838 completed (loss: 0.14953134953975677, acc: 0.9230769276618958)
[2025-02-16 12:32:49,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:49,341][root][INFO] - Training Epoch: 1/2, step 8976/23838 completed (loss: 0.2835528552532196, acc: 0.9420289993286133)
[2025-02-16 12:32:49,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:49,718][root][INFO] - Training Epoch: 1/2, step 8977/23838 completed (loss: 0.2710324823856354, acc: 0.9210526347160339)
[2025-02-16 12:32:49,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:50,108][root][INFO] - Training Epoch: 1/2, step 8978/23838 completed (loss: 0.46794116497039795, acc: 0.8235294222831726)
[2025-02-16 12:32:50,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:50,543][root][INFO] - Training Epoch: 1/2, step 8979/23838 completed (loss: 0.29988792538642883, acc: 0.8918918967247009)
[2025-02-16 12:32:50,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:50,932][root][INFO] - Training Epoch: 1/2, step 8980/23838 completed (loss: 0.4305725395679474, acc: 0.8888888955116272)
[2025-02-16 12:32:51,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:51,297][root][INFO] - Training Epoch: 1/2, step 8981/23838 completed (loss: 0.4490892291069031, acc: 0.8461538553237915)
[2025-02-16 12:32:51,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:51,748][root][INFO] - Training Epoch: 1/2, step 8982/23838 completed (loss: 1.218000054359436, acc: 0.7400000095367432)
[2025-02-16 12:32:51,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:52,194][root][INFO] - Training Epoch: 1/2, step 8983/23838 completed (loss: 0.43316730856895447, acc: 0.8857142925262451)
[2025-02-16 12:32:52,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:52,599][root][INFO] - Training Epoch: 1/2, step 8984/23838 completed (loss: 0.43332788348197937, acc: 0.9090909361839294)
[2025-02-16 12:32:52,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:53,036][root][INFO] - Training Epoch: 1/2, step 8985/23838 completed (loss: 0.3883667290210724, acc: 0.942307710647583)
[2025-02-16 12:32:53,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:53,438][root][INFO] - Training Epoch: 1/2, step 8986/23838 completed (loss: 0.20172913372516632, acc: 0.9589040875434875)
[2025-02-16 12:32:53,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:53,906][root][INFO] - Training Epoch: 1/2, step 8987/23838 completed (loss: 0.4311172366142273, acc: 0.891566276550293)
[2025-02-16 12:32:54,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:54,295][root][INFO] - Training Epoch: 1/2, step 8988/23838 completed (loss: 0.4915485680103302, acc: 0.9090909361839294)
[2025-02-16 12:32:54,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:54,758][root][INFO] - Training Epoch: 1/2, step 8989/23838 completed (loss: 0.4740045368671417, acc: 0.9016393423080444)
[2025-02-16 12:32:54,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:55,246][root][INFO] - Training Epoch: 1/2, step 8990/23838 completed (loss: 0.5172984004020691, acc: 0.8421052694320679)
[2025-02-16 12:32:55,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:55,662][root][INFO] - Training Epoch: 1/2, step 8991/23838 completed (loss: 0.9077366590499878, acc: 0.7534246444702148)
[2025-02-16 12:32:55,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:56,103][root][INFO] - Training Epoch: 1/2, step 8992/23838 completed (loss: 0.6611663699150085, acc: 0.8399999737739563)
[2025-02-16 12:32:56,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:56,575][root][INFO] - Training Epoch: 1/2, step 8993/23838 completed (loss: 0.650439441204071, acc: 0.7931034564971924)
[2025-02-16 12:32:56,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:56,993][root][INFO] - Training Epoch: 1/2, step 8994/23838 completed (loss: 0.1700018346309662, acc: 0.9714285731315613)
[2025-02-16 12:32:57,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:57,419][root][INFO] - Training Epoch: 1/2, step 8995/23838 completed (loss: 0.28552043437957764, acc: 0.9135802388191223)
[2025-02-16 12:32:57,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:57,839][root][INFO] - Training Epoch: 1/2, step 8996/23838 completed (loss: 0.25333163142204285, acc: 0.9139785170555115)
[2025-02-16 12:32:58,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:58,342][root][INFO] - Training Epoch: 1/2, step 8997/23838 completed (loss: 0.3467590808868408, acc: 0.8857142925262451)
[2025-02-16 12:32:58,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:58,812][root][INFO] - Training Epoch: 1/2, step 8998/23838 completed (loss: 0.22342504560947418, acc: 0.9166666865348816)
[2025-02-16 12:32:59,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:59,333][root][INFO] - Training Epoch: 1/2, step 8999/23838 completed (loss: 0.5408478379249573, acc: 0.8405796885490417)
[2025-02-16 12:32:59,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:32:59,730][root][INFO] - Training Epoch: 1/2, step 9000/23838 completed (loss: 0.27680933475494385, acc: 0.9090909361839294)
[2025-02-16 12:32:59,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:00,260][root][INFO] - Training Epoch: 1/2, step 9001/23838 completed (loss: 0.2741720676422119, acc: 0.9024389982223511)
[2025-02-16 12:33:00,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:00,688][root][INFO] - Training Epoch: 1/2, step 9002/23838 completed (loss: 0.35976678133010864, acc: 0.925000011920929)
[2025-02-16 12:33:00,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:01,180][root][INFO] - Training Epoch: 1/2, step 9003/23838 completed (loss: 0.3552778959274292, acc: 0.9071428775787354)
[2025-02-16 12:33:01,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:01,589][root][INFO] - Training Epoch: 1/2, step 9004/23838 completed (loss: 0.1391844004392624, acc: 0.9736841917037964)
[2025-02-16 12:33:01,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:02,009][root][INFO] - Training Epoch: 1/2, step 9005/23838 completed (loss: 0.19840680062770844, acc: 0.9090909361839294)
[2025-02-16 12:33:02,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:02,396][root][INFO] - Training Epoch: 1/2, step 9006/23838 completed (loss: 0.40300124883651733, acc: 0.8500000238418579)
[2025-02-16 12:33:02,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:02,829][root][INFO] - Training Epoch: 1/2, step 9007/23838 completed (loss: 0.1234176903963089, acc: 0.9634146094322205)
[2025-02-16 12:33:03,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:03,306][root][INFO] - Training Epoch: 1/2, step 9008/23838 completed (loss: 0.18687938153743744, acc: 0.9552238583564758)
[2025-02-16 12:33:03,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:03,769][root][INFO] - Training Epoch: 1/2, step 9009/23838 completed (loss: 1.0363295078277588, acc: 0.7241379022598267)
[2025-02-16 12:33:03,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:04,208][root][INFO] - Training Epoch: 1/2, step 9010/23838 completed (loss: 0.21061500906944275, acc: 0.9591836929321289)
[2025-02-16 12:33:04,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:04,580][root][INFO] - Training Epoch: 1/2, step 9011/23838 completed (loss: 0.36238500475883484, acc: 0.9223300814628601)
[2025-02-16 12:33:04,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:04,947][root][INFO] - Training Epoch: 1/2, step 9012/23838 completed (loss: 0.30644309520721436, acc: 0.9076923131942749)
[2025-02-16 12:33:05,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:05,417][root][INFO] - Training Epoch: 1/2, step 9013/23838 completed (loss: 0.4997507929801941, acc: 0.8837209343910217)
[2025-02-16 12:33:05,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:05,865][root][INFO] - Training Epoch: 1/2, step 9014/23838 completed (loss: 0.27104267477989197, acc: 0.8780487775802612)
[2025-02-16 12:33:06,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:06,287][root][INFO] - Training Epoch: 1/2, step 9015/23838 completed (loss: 0.2629203796386719, acc: 0.9344262480735779)
[2025-02-16 12:33:06,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:06,760][root][INFO] - Training Epoch: 1/2, step 9016/23838 completed (loss: 0.2921851575374603, acc: 0.9189189076423645)
[2025-02-16 12:33:06,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:07,158][root][INFO] - Training Epoch: 1/2, step 9017/23838 completed (loss: 0.6559818387031555, acc: 0.8600000143051147)
[2025-02-16 12:33:07,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:07,630][root][INFO] - Training Epoch: 1/2, step 9018/23838 completed (loss: 0.46171119809150696, acc: 0.8947368264198303)
[2025-02-16 12:33:07,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:08,050][root][INFO] - Training Epoch: 1/2, step 9019/23838 completed (loss: 0.33501124382019043, acc: 0.9375)
[2025-02-16 12:33:08,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:08,458][root][INFO] - Training Epoch: 1/2, step 9020/23838 completed (loss: 0.12663304805755615, acc: 0.9607843160629272)
[2025-02-16 12:33:08,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:08,830][root][INFO] - Training Epoch: 1/2, step 9021/23838 completed (loss: 0.707338809967041, acc: 0.8311688303947449)
[2025-02-16 12:33:09,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:09,281][root][INFO] - Training Epoch: 1/2, step 9022/23838 completed (loss: 0.43225398659706116, acc: 0.8829787373542786)
[2025-02-16 12:33:09,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:09,807][root][INFO] - Training Epoch: 1/2, step 9023/23838 completed (loss: 0.4449276030063629, acc: 0.8612716794013977)
[2025-02-16 12:33:09,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:10,233][root][INFO] - Training Epoch: 1/2, step 9024/23838 completed (loss: 0.5907049775123596, acc: 0.8482758402824402)
[2025-02-16 12:33:10,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:10,726][root][INFO] - Training Epoch: 1/2, step 9025/23838 completed (loss: 0.6570971608161926, acc: 0.8230088353157043)
[2025-02-16 12:33:10,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:11,212][root][INFO] - Training Epoch: 1/2, step 9026/23838 completed (loss: 0.5672339200973511, acc: 0.8297872543334961)
[2025-02-16 12:33:11,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:11,694][root][INFO] - Training Epoch: 1/2, step 9027/23838 completed (loss: 0.4871661067008972, acc: 0.8778625726699829)
[2025-02-16 12:33:11,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:12,094][root][INFO] - Training Epoch: 1/2, step 9028/23838 completed (loss: 0.41450875997543335, acc: 0.8860759735107422)
[2025-02-16 12:33:12,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:12,476][root][INFO] - Training Epoch: 1/2, step 9029/23838 completed (loss: 0.27496713399887085, acc: 0.9215686321258545)
[2025-02-16 12:33:12,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:12,908][root][INFO] - Training Epoch: 1/2, step 9030/23838 completed (loss: 0.25860610604286194, acc: 0.931034505367279)
[2025-02-16 12:33:13,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:13,322][root][INFO] - Training Epoch: 1/2, step 9031/23838 completed (loss: 0.3084682822227478, acc: 0.9120000004768372)
[2025-02-16 12:33:13,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:13,690][root][INFO] - Training Epoch: 1/2, step 9032/23838 completed (loss: 0.47578859329223633, acc: 0.8849557638168335)
[2025-02-16 12:33:13,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:14,221][root][INFO] - Training Epoch: 1/2, step 9033/23838 completed (loss: 0.2740873396396637, acc: 0.9289617538452148)
[2025-02-16 12:33:14,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:14,636][root][INFO] - Training Epoch: 1/2, step 9034/23838 completed (loss: 0.23019076883792877, acc: 0.9174311757087708)
[2025-02-16 12:33:14,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:15,039][root][INFO] - Training Epoch: 1/2, step 9035/23838 completed (loss: 0.21436236798763275, acc: 0.9672130942344666)
[2025-02-16 12:33:15,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:15,457][root][INFO] - Training Epoch: 1/2, step 9036/23838 completed (loss: 0.31805115938186646, acc: 0.9066666960716248)
[2025-02-16 12:33:15,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:15,910][root][INFO] - Training Epoch: 1/2, step 9037/23838 completed (loss: 0.30790984630584717, acc: 0.9354838728904724)
[2025-02-16 12:33:16,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:16,364][root][INFO] - Training Epoch: 1/2, step 9038/23838 completed (loss: 0.2524756193161011, acc: 0.9337748289108276)
[2025-02-16 12:33:16,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:16,724][root][INFO] - Training Epoch: 1/2, step 9039/23838 completed (loss: 0.5184945464134216, acc: 0.8990825414657593)
[2025-02-16 12:33:16,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:17,141][root][INFO] - Training Epoch: 1/2, step 9040/23838 completed (loss: 0.29999488592147827, acc: 0.9281045794487)
[2025-02-16 12:33:17,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:17,667][root][INFO] - Training Epoch: 1/2, step 9041/23838 completed (loss: 0.24691760540008545, acc: 0.9150943160057068)
[2025-02-16 12:33:17,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:18,116][root][INFO] - Training Epoch: 1/2, step 9042/23838 completed (loss: 0.34436529874801636, acc: 0.8928571343421936)
[2025-02-16 12:33:18,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:18,537][root][INFO] - Training Epoch: 1/2, step 9043/23838 completed (loss: 0.44101276993751526, acc: 0.8783783912658691)
[2025-02-16 12:33:18,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:18,961][root][INFO] - Training Epoch: 1/2, step 9044/23838 completed (loss: 0.2618074417114258, acc: 0.939130425453186)
[2025-02-16 12:33:19,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:19,427][root][INFO] - Training Epoch: 1/2, step 9045/23838 completed (loss: 0.5033823847770691, acc: 0.8931297659873962)
[2025-02-16 12:33:19,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:19,817][root][INFO] - Training Epoch: 1/2, step 9046/23838 completed (loss: 0.27344319224357605, acc: 0.9234972596168518)
[2025-02-16 12:33:19,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:20,210][root][INFO] - Training Epoch: 1/2, step 9047/23838 completed (loss: 0.49612560868263245, acc: 0.8333333134651184)
[2025-02-16 12:33:20,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:20,597][root][INFO] - Training Epoch: 1/2, step 9048/23838 completed (loss: 0.5084059238433838, acc: 0.8558558821678162)
[2025-02-16 12:33:20,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:20,984][root][INFO] - Training Epoch: 1/2, step 9049/23838 completed (loss: 0.14961688220500946, acc: 0.949999988079071)
[2025-02-16 12:33:21,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:21,370][root][INFO] - Training Epoch: 1/2, step 9050/23838 completed (loss: 0.2396164983510971, acc: 0.9375)
[2025-02-16 12:33:21,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:21,861][root][INFO] - Training Epoch: 1/2, step 9051/23838 completed (loss: 0.2248900830745697, acc: 0.9276315569877625)
[2025-02-16 12:33:22,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:22,341][root][INFO] - Training Epoch: 1/2, step 9052/23838 completed (loss: 0.2187054455280304, acc: 0.9469026327133179)
[2025-02-16 12:33:22,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:22,743][root][INFO] - Training Epoch: 1/2, step 9053/23838 completed (loss: 0.24488919973373413, acc: 0.925000011920929)
[2025-02-16 12:33:22,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:23,184][root][INFO] - Training Epoch: 1/2, step 9054/23838 completed (loss: 0.1989608108997345, acc: 0.9508196711540222)
[2025-02-16 12:33:23,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:23,679][root][INFO] - Training Epoch: 1/2, step 9055/23838 completed (loss: 0.12987561523914337, acc: 0.9767441749572754)
[2025-02-16 12:33:23,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:24,055][root][INFO] - Training Epoch: 1/2, step 9056/23838 completed (loss: 0.2980101704597473, acc: 0.9147287011146545)
[2025-02-16 12:33:24,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:24,417][root][INFO] - Training Epoch: 1/2, step 9057/23838 completed (loss: 0.1562490314245224, acc: 0.9530201554298401)
[2025-02-16 12:33:24,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:24,829][root][INFO] - Training Epoch: 1/2, step 9058/23838 completed (loss: 0.488151490688324, acc: 0.8399999737739563)
[2025-02-16 12:33:25,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:25,254][root][INFO] - Training Epoch: 1/2, step 9059/23838 completed (loss: 0.37779784202575684, acc: 0.9130434989929199)
[2025-02-16 12:33:25,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:25,714][root][INFO] - Training Epoch: 1/2, step 9060/23838 completed (loss: 0.2607977092266083, acc: 0.9382022619247437)
[2025-02-16 12:33:25,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:26,169][root][INFO] - Training Epoch: 1/2, step 9061/23838 completed (loss: 0.27811533212661743, acc: 0.921875)
[2025-02-16 12:33:26,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:26,605][root][INFO] - Training Epoch: 1/2, step 9062/23838 completed (loss: 0.8107040524482727, acc: 0.7802197933197021)
[2025-02-16 12:33:26,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:27,085][root][INFO] - Training Epoch: 1/2, step 9063/23838 completed (loss: 0.6658695340156555, acc: 0.8461538553237915)
[2025-02-16 12:33:27,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:27,467][root][INFO] - Training Epoch: 1/2, step 9064/23838 completed (loss: 0.6762351393699646, acc: 0.8144329786300659)
[2025-02-16 12:33:27,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:27,825][root][INFO] - Training Epoch: 1/2, step 9065/23838 completed (loss: 0.37282416224479675, acc: 0.9125000238418579)
[2025-02-16 12:33:28,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:28,249][root][INFO] - Training Epoch: 1/2, step 9066/23838 completed (loss: 0.4394657015800476, acc: 0.9100000262260437)
[2025-02-16 12:33:28,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:28,676][root][INFO] - Training Epoch: 1/2, step 9067/23838 completed (loss: 0.3544543385505676, acc: 0.8899999856948853)
[2025-02-16 12:33:28,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:29,163][root][INFO] - Training Epoch: 1/2, step 9068/23838 completed (loss: 0.3192535936832428, acc: 0.8947368264198303)
[2025-02-16 12:33:29,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:29,610][root][INFO] - Training Epoch: 1/2, step 9069/23838 completed (loss: 0.16988493502140045, acc: 0.975806474685669)
[2025-02-16 12:33:29,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:30,028][root][INFO] - Training Epoch: 1/2, step 9070/23838 completed (loss: 0.3355987071990967, acc: 0.9039999842643738)
[2025-02-16 12:33:30,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:30,551][root][INFO] - Training Epoch: 1/2, step 9071/23838 completed (loss: 0.4382334053516388, acc: 0.9230769276618958)
[2025-02-16 12:33:30,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:30,966][root][INFO] - Training Epoch: 1/2, step 9072/23838 completed (loss: 0.17003385722637177, acc: 0.9495798349380493)
[2025-02-16 12:33:31,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:31,409][root][INFO] - Training Epoch: 1/2, step 9073/23838 completed (loss: 0.24462243914604187, acc: 0.936170220375061)
[2025-02-16 12:33:31,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:31,857][root][INFO] - Training Epoch: 1/2, step 9074/23838 completed (loss: 0.19113871455192566, acc: 0.95333331823349)
[2025-02-16 12:33:32,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:32,329][root][INFO] - Training Epoch: 1/2, step 9075/23838 completed (loss: 0.1472463607788086, acc: 0.9650349617004395)
[2025-02-16 12:33:32,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:32,748][root][INFO] - Training Epoch: 1/2, step 9076/23838 completed (loss: 0.44862526655197144, acc: 0.8732394576072693)
[2025-02-16 12:33:32,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:33,222][root][INFO] - Training Epoch: 1/2, step 9077/23838 completed (loss: 0.32939407229423523, acc: 0.8943089246749878)
[2025-02-16 12:33:33,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:33,692][root][INFO] - Training Epoch: 1/2, step 9078/23838 completed (loss: 0.2349535971879959, acc: 0.9397590160369873)
[2025-02-16 12:33:33,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:34,155][root][INFO] - Training Epoch: 1/2, step 9079/23838 completed (loss: 0.1868334859609604, acc: 0.9520000219345093)
[2025-02-16 12:33:34,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:34,533][root][INFO] - Training Epoch: 1/2, step 9080/23838 completed (loss: 0.1316542774438858, acc: 0.9426229596138)
[2025-02-16 12:33:34,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:35,007][root][INFO] - Training Epoch: 1/2, step 9081/23838 completed (loss: 0.2733103036880493, acc: 0.9351851940155029)
[2025-02-16 12:33:35,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:35,448][root][INFO] - Training Epoch: 1/2, step 9082/23838 completed (loss: 0.15999376773834229, acc: 0.9450549483299255)
[2025-02-16 12:33:35,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:35,929][root][INFO] - Training Epoch: 1/2, step 9083/23838 completed (loss: 0.2411692589521408, acc: 0.931034505367279)
[2025-02-16 12:33:36,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:36,386][root][INFO] - Training Epoch: 1/2, step 9084/23838 completed (loss: 0.5480672121047974, acc: 0.8602150678634644)
[2025-02-16 12:33:36,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:36,811][root][INFO] - Training Epoch: 1/2, step 9085/23838 completed (loss: 0.22847327589988708, acc: 0.9352940917015076)
[2025-02-16 12:33:37,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:37,258][root][INFO] - Training Epoch: 1/2, step 9086/23838 completed (loss: 0.511048436164856, acc: 0.8333333134651184)
[2025-02-16 12:33:37,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:37,649][root][INFO] - Training Epoch: 1/2, step 9087/23838 completed (loss: 0.10876455157995224, acc: 0.9732142686843872)
[2025-02-16 12:33:37,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:38,093][root][INFO] - Training Epoch: 1/2, step 9088/23838 completed (loss: 0.5619966387748718, acc: 0.875)
[2025-02-16 12:33:38,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:38,516][root][INFO] - Training Epoch: 1/2, step 9089/23838 completed (loss: 0.19812582433223724, acc: 0.9599999785423279)
[2025-02-16 12:33:38,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:38,916][root][INFO] - Training Epoch: 1/2, step 9090/23838 completed (loss: 0.4457211196422577, acc: 0.8852459192276001)
[2025-02-16 12:33:39,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:39,382][root][INFO] - Training Epoch: 1/2, step 9091/23838 completed (loss: 0.22251927852630615, acc: 0.9603960514068604)
[2025-02-16 12:33:39,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:39,869][root][INFO] - Training Epoch: 1/2, step 9092/23838 completed (loss: 0.42246633768081665, acc: 0.8802816867828369)
[2025-02-16 12:33:40,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:40,291][root][INFO] - Training Epoch: 1/2, step 9093/23838 completed (loss: 0.2542509436607361, acc: 0.9386503100395203)
[2025-02-16 12:33:40,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:40,695][root][INFO] - Training Epoch: 1/2, step 9094/23838 completed (loss: 0.156808003783226, acc: 0.9765625)
[2025-02-16 12:33:40,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:41,093][root][INFO] - Training Epoch: 1/2, step 9095/23838 completed (loss: 0.23716776072978973, acc: 0.9408283829689026)
[2025-02-16 12:33:41,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:41,482][root][INFO] - Training Epoch: 1/2, step 9096/23838 completed (loss: 0.12586763501167297, acc: 0.949999988079071)
[2025-02-16 12:33:41,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:41,871][root][INFO] - Training Epoch: 1/2, step 9097/23838 completed (loss: 0.2882842719554901, acc: 0.9142857193946838)
[2025-02-16 12:33:42,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:42,324][root][INFO] - Training Epoch: 1/2, step 9098/23838 completed (loss: 0.5439477562904358, acc: 0.8782608509063721)
[2025-02-16 12:33:42,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:42,688][root][INFO] - Training Epoch: 1/2, step 9099/23838 completed (loss: 0.8089697360992432, acc: 0.7924528121948242)
[2025-02-16 12:33:42,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:43,109][root][INFO] - Training Epoch: 1/2, step 9100/23838 completed (loss: 0.4639425277709961, acc: 0.8990825414657593)
[2025-02-16 12:33:43,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:43,471][root][INFO] - Training Epoch: 1/2, step 9101/23838 completed (loss: 0.43981805443763733, acc: 0.871999979019165)
[2025-02-16 12:33:43,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:43,845][root][INFO] - Training Epoch: 1/2, step 9102/23838 completed (loss: 0.21749769151210785, acc: 0.9247311949729919)
[2025-02-16 12:33:43,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:44,228][root][INFO] - Training Epoch: 1/2, step 9103/23838 completed (loss: 0.7733949422836304, acc: 0.7956989407539368)
[2025-02-16 12:33:44,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:44,647][root][INFO] - Training Epoch: 1/2, step 9104/23838 completed (loss: 0.19903385639190674, acc: 0.9516128897666931)
[2025-02-16 12:33:44,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:45,096][root][INFO] - Training Epoch: 1/2, step 9105/23838 completed (loss: 0.3314104378223419, acc: 0.9090909361839294)
[2025-02-16 12:33:45,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:45,486][root][INFO] - Training Epoch: 1/2, step 9106/23838 completed (loss: 0.4466698169708252, acc: 0.8421052694320679)
[2025-02-16 12:33:45,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:45,917][root][INFO] - Training Epoch: 1/2, step 9107/23838 completed (loss: 0.7171452641487122, acc: 0.8214285969734192)
[2025-02-16 12:33:46,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:46,378][root][INFO] - Training Epoch: 1/2, step 9108/23838 completed (loss: 0.17136184871196747, acc: 0.970588207244873)
[2025-02-16 12:33:46,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:46,849][root][INFO] - Training Epoch: 1/2, step 9109/23838 completed (loss: 0.4883747100830078, acc: 0.8823529481887817)
[2025-02-16 12:33:47,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:47,339][root][INFO] - Training Epoch: 1/2, step 9110/23838 completed (loss: 0.35364577174186707, acc: 0.9230769276618958)
[2025-02-16 12:33:47,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:47,838][root][INFO] - Training Epoch: 1/2, step 9111/23838 completed (loss: 0.3361872434616089, acc: 0.9041095972061157)
[2025-02-16 12:33:48,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:48,321][root][INFO] - Training Epoch: 1/2, step 9112/23838 completed (loss: 0.3229915499687195, acc: 0.9251337051391602)
[2025-02-16 12:33:48,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:48,742][root][INFO] - Training Epoch: 1/2, step 9113/23838 completed (loss: 0.5162680745124817, acc: 0.8615384697914124)
[2025-02-16 12:33:48,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:49,203][root][INFO] - Training Epoch: 1/2, step 9114/23838 completed (loss: 0.3855586647987366, acc: 0.8794326186180115)
[2025-02-16 12:33:49,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:49,624][root][INFO] - Training Epoch: 1/2, step 9115/23838 completed (loss: 0.3553285002708435, acc: 0.8904761672019958)
[2025-02-16 12:33:49,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:50,073][root][INFO] - Training Epoch: 1/2, step 9116/23838 completed (loss: 0.7997919917106628, acc: 0.7540983557701111)
[2025-02-16 12:33:50,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:50,439][root][INFO] - Training Epoch: 1/2, step 9117/23838 completed (loss: 0.4000754654407501, acc: 0.9104477763175964)
[2025-02-16 12:33:50,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:50,897][root][INFO] - Training Epoch: 1/2, step 9118/23838 completed (loss: 0.49663475155830383, acc: 0.8977272510528564)
[2025-02-16 12:33:51,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:51,324][root][INFO] - Training Epoch: 1/2, step 9119/23838 completed (loss: 0.31747838854789734, acc: 0.9210526347160339)
[2025-02-16 12:33:51,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:51,730][root][INFO] - Training Epoch: 1/2, step 9120/23838 completed (loss: 0.4552468955516815, acc: 0.9100000262260437)
[2025-02-16 12:33:51,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:52,190][root][INFO] - Training Epoch: 1/2, step 9121/23838 completed (loss: 0.332278847694397, acc: 0.893203854560852)
[2025-02-16 12:33:52,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:52,500][root][INFO] - Training Epoch: 1/2, step 9122/23838 completed (loss: 0.2732903063297272, acc: 0.939130425453186)
[2025-02-16 12:33:52,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:52,958][root][INFO] - Training Epoch: 1/2, step 9123/23838 completed (loss: 0.1770258992910385, acc: 0.9491525292396545)
[2025-02-16 12:33:53,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:53,447][root][INFO] - Training Epoch: 1/2, step 9124/23838 completed (loss: 0.8217893838882446, acc: 0.8095238208770752)
[2025-02-16 12:33:53,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:53,922][root][INFO] - Training Epoch: 1/2, step 9125/23838 completed (loss: 0.13995495438575745, acc: 0.949999988079071)
[2025-02-16 12:33:54,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:54,387][root][INFO] - Training Epoch: 1/2, step 9126/23838 completed (loss: 0.31086498498916626, acc: 0.9175257682800293)
[2025-02-16 12:33:54,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:54,836][root][INFO] - Training Epoch: 1/2, step 9127/23838 completed (loss: 0.16259941458702087, acc: 0.9572192430496216)
[2025-02-16 12:33:55,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:55,301][root][INFO] - Training Epoch: 1/2, step 9128/23838 completed (loss: 0.23050470650196075, acc: 0.9212121367454529)
[2025-02-16 12:33:55,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:55,803][root][INFO] - Training Epoch: 1/2, step 9129/23838 completed (loss: 0.4739159941673279, acc: 0.8545454740524292)
[2025-02-16 12:33:55,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:56,249][root][INFO] - Training Epoch: 1/2, step 9130/23838 completed (loss: 0.3879545331001282, acc: 0.8849557638168335)
[2025-02-16 12:33:56,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:56,675][root][INFO] - Training Epoch: 1/2, step 9131/23838 completed (loss: 0.3147182762622833, acc: 0.9244186282157898)
[2025-02-16 12:33:56,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:57,094][root][INFO] - Training Epoch: 1/2, step 9132/23838 completed (loss: 0.32098186016082764, acc: 0.9304347634315491)
[2025-02-16 12:33:57,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:57,545][root][INFO] - Training Epoch: 1/2, step 9133/23838 completed (loss: 0.4023344814777374, acc: 0.9101123809814453)
[2025-02-16 12:33:57,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:58,025][root][INFO] - Training Epoch: 1/2, step 9134/23838 completed (loss: 0.13789750635623932, acc: 0.9661017060279846)
[2025-02-16 12:33:58,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:58,411][root][INFO] - Training Epoch: 1/2, step 9135/23838 completed (loss: 0.7654891610145569, acc: 0.7627118825912476)
[2025-02-16 12:33:58,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:58,810][root][INFO] - Training Epoch: 1/2, step 9136/23838 completed (loss: 0.9232702851295471, acc: 0.7457627058029175)
[2025-02-16 12:33:59,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:59,267][root][INFO] - Training Epoch: 1/2, step 9137/23838 completed (loss: 0.22663387656211853, acc: 0.9642857313156128)
[2025-02-16 12:33:59,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:33:59,658][root][INFO] - Training Epoch: 1/2, step 9138/23838 completed (loss: 0.3665505647659302, acc: 0.9375)
[2025-02-16 12:33:59,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:00,121][root][INFO] - Training Epoch: 1/2, step 9139/23838 completed (loss: 0.28133097290992737, acc: 0.9408602118492126)
[2025-02-16 12:34:00,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:00,569][root][INFO] - Training Epoch: 1/2, step 9140/23838 completed (loss: 0.2257697433233261, acc: 0.9428571462631226)
[2025-02-16 12:34:00,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:00,978][root][INFO] - Training Epoch: 1/2, step 9141/23838 completed (loss: 0.2737075090408325, acc: 0.9411764740943909)
[2025-02-16 12:34:01,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:01,336][root][INFO] - Training Epoch: 1/2, step 9142/23838 completed (loss: 0.4535520672798157, acc: 0.8681318759918213)
[2025-02-16 12:34:01,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:01,758][root][INFO] - Training Epoch: 1/2, step 9143/23838 completed (loss: 0.1609557569026947, acc: 0.9430894255638123)
[2025-02-16 12:34:01,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:02,190][root][INFO] - Training Epoch: 1/2, step 9144/23838 completed (loss: 0.2999306917190552, acc: 0.8823529481887817)
[2025-02-16 12:34:02,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:02,668][root][INFO] - Training Epoch: 1/2, step 9145/23838 completed (loss: 0.1774483621120453, acc: 0.939393937587738)
[2025-02-16 12:34:02,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:03,100][root][INFO] - Training Epoch: 1/2, step 9146/23838 completed (loss: 0.27530133724212646, acc: 0.9047619104385376)
[2025-02-16 12:34:03,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:03,500][root][INFO] - Training Epoch: 1/2, step 9147/23838 completed (loss: 0.17458924651145935, acc: 0.9426229596138)
[2025-02-16 12:34:03,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:03,882][root][INFO] - Training Epoch: 1/2, step 9148/23838 completed (loss: 0.4517119526863098, acc: 0.8627451062202454)
[2025-02-16 12:34:04,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:04,262][root][INFO] - Training Epoch: 1/2, step 9149/23838 completed (loss: 0.17725950479507446, acc: 0.9595959782600403)
[2025-02-16 12:34:04,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:04,652][root][INFO] - Training Epoch: 1/2, step 9150/23838 completed (loss: 0.20186425745487213, acc: 0.9503546357154846)
[2025-02-16 12:34:04,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:05,034][root][INFO] - Training Epoch: 1/2, step 9151/23838 completed (loss: 0.10683268308639526, acc: 0.9767441749572754)
[2025-02-16 12:34:05,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:05,500][root][INFO] - Training Epoch: 1/2, step 9152/23838 completed (loss: 0.18768973648548126, acc: 0.9285714030265808)
[2025-02-16 12:34:05,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:05,900][root][INFO] - Training Epoch: 1/2, step 9153/23838 completed (loss: 0.13835328817367554, acc: 0.9693251252174377)
[2025-02-16 12:34:06,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:06,466][root][INFO] - Training Epoch: 1/2, step 9154/23838 completed (loss: 0.3851706087589264, acc: 0.8777777552604675)
[2025-02-16 12:34:06,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:06,893][root][INFO] - Training Epoch: 1/2, step 9155/23838 completed (loss: 0.2683265209197998, acc: 0.9255319237709045)
[2025-02-16 12:34:07,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:07,319][root][INFO] - Training Epoch: 1/2, step 9156/23838 completed (loss: 0.401172399520874, acc: 0.8962963223457336)
[2025-02-16 12:34:07,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:07,772][root][INFO] - Training Epoch: 1/2, step 9157/23838 completed (loss: 0.5149311423301697, acc: 0.8888888955116272)
[2025-02-16 12:34:07,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:08,220][root][INFO] - Training Epoch: 1/2, step 9158/23838 completed (loss: 0.15662288665771484, acc: 0.9622641801834106)
[2025-02-16 12:34:08,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:08,765][root][INFO] - Training Epoch: 1/2, step 9159/23838 completed (loss: 0.2449803203344345, acc: 0.9481865167617798)
[2025-02-16 12:34:08,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:09,203][root][INFO] - Training Epoch: 1/2, step 9160/23838 completed (loss: 0.20134472846984863, acc: 0.9359999895095825)
[2025-02-16 12:34:09,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:09,602][root][INFO] - Training Epoch: 1/2, step 9161/23838 completed (loss: 0.16472257673740387, acc: 0.9779411554336548)
[2025-02-16 12:34:09,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:10,035][root][INFO] - Training Epoch: 1/2, step 9162/23838 completed (loss: 0.2716907262802124, acc: 0.9116021990776062)
[2025-02-16 12:34:10,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:10,521][root][INFO] - Training Epoch: 1/2, step 9163/23838 completed (loss: 0.45393118262290955, acc: 0.8707864880561829)
[2025-02-16 12:34:10,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:10,976][root][INFO] - Training Epoch: 1/2, step 9164/23838 completed (loss: 0.3069201707839966, acc: 0.9312977194786072)
[2025-02-16 12:34:11,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:11,442][root][INFO] - Training Epoch: 1/2, step 9165/23838 completed (loss: 0.19303925335407257, acc: 0.9454545378684998)
[2025-02-16 12:34:11,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:11,829][root][INFO] - Training Epoch: 1/2, step 9166/23838 completed (loss: 0.5208605527877808, acc: 0.8709677457809448)
[2025-02-16 12:34:12,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:12,274][root][INFO] - Training Epoch: 1/2, step 9167/23838 completed (loss: 0.526276171207428, acc: 0.859375)
[2025-02-16 12:34:12,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:12,691][root][INFO] - Training Epoch: 1/2, step 9168/23838 completed (loss: 0.35454297065734863, acc: 0.9047619104385376)
[2025-02-16 12:34:12,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:13,157][root][INFO] - Training Epoch: 1/2, step 9169/23838 completed (loss: 0.7117635607719421, acc: 0.84375)
[2025-02-16 12:34:13,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:13,561][root][INFO] - Training Epoch: 1/2, step 9170/23838 completed (loss: 0.37589457631111145, acc: 0.9067796468734741)
[2025-02-16 12:34:13,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:13,998][root][INFO] - Training Epoch: 1/2, step 9171/23838 completed (loss: 0.32798707485198975, acc: 0.9059829115867615)
[2025-02-16 12:34:14,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:14,425][root][INFO] - Training Epoch: 1/2, step 9172/23838 completed (loss: 0.20585046708583832, acc: 0.9509803652763367)
[2025-02-16 12:34:14,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:14,879][root][INFO] - Training Epoch: 1/2, step 9173/23838 completed (loss: 0.36115512251853943, acc: 0.8787878751754761)
[2025-02-16 12:34:15,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:15,297][root][INFO] - Training Epoch: 1/2, step 9174/23838 completed (loss: 0.3509456515312195, acc: 0.9387755393981934)
[2025-02-16 12:34:15,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:15,708][root][INFO] - Training Epoch: 1/2, step 9175/23838 completed (loss: 0.3110412061214447, acc: 0.8913043737411499)
[2025-02-16 12:34:15,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:16,149][root][INFO] - Training Epoch: 1/2, step 9176/23838 completed (loss: 0.26185208559036255, acc: 0.9246575236320496)
[2025-02-16 12:34:16,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:16,590][root][INFO] - Training Epoch: 1/2, step 9177/23838 completed (loss: 0.3247827887535095, acc: 0.8882978558540344)
[2025-02-16 12:34:16,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:16,988][root][INFO] - Training Epoch: 1/2, step 9178/23838 completed (loss: 0.6808285117149353, acc: 0.7870370149612427)
[2025-02-16 12:34:17,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:17,382][root][INFO] - Training Epoch: 1/2, step 9179/23838 completed (loss: 0.1887296587228775, acc: 0.9491525292396545)
[2025-02-16 12:34:17,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:17,790][root][INFO] - Training Epoch: 1/2, step 9180/23838 completed (loss: 0.2499481737613678, acc: 0.9157894849777222)
[2025-02-16 12:34:18,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:18,230][root][INFO] - Training Epoch: 1/2, step 9181/23838 completed (loss: 0.43529167771339417, acc: 0.875)
[2025-02-16 12:34:18,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:18,703][root][INFO] - Training Epoch: 1/2, step 9182/23838 completed (loss: 0.36528778076171875, acc: 0.8834356069564819)
[2025-02-16 12:34:18,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:19,159][root][INFO] - Training Epoch: 1/2, step 9183/23838 completed (loss: 0.3284226655960083, acc: 0.9178082346916199)
[2025-02-16 12:34:19,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:19,664][root][INFO] - Training Epoch: 1/2, step 9184/23838 completed (loss: 0.2395206093788147, acc: 0.918367326259613)
[2025-02-16 12:34:19,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:20,125][root][INFO] - Training Epoch: 1/2, step 9185/23838 completed (loss: 0.3600809872150421, acc: 0.9171270728111267)
[2025-02-16 12:34:20,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:20,561][root][INFO] - Training Epoch: 1/2, step 9186/23838 completed (loss: 0.126103475689888, acc: 0.953125)
[2025-02-16 12:34:20,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:20,918][root][INFO] - Training Epoch: 1/2, step 9187/23838 completed (loss: 0.45531412959098816, acc: 0.8857142925262451)
[2025-02-16 12:34:21,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:21,283][root][INFO] - Training Epoch: 1/2, step 9188/23838 completed (loss: 0.4326973259449005, acc: 0.898876428604126)
[2025-02-16 12:34:21,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:21,718][root][INFO] - Training Epoch: 1/2, step 9189/23838 completed (loss: 0.15134620666503906, acc: 0.957317054271698)
[2025-02-16 12:34:21,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:22,104][root][INFO] - Training Epoch: 1/2, step 9190/23838 completed (loss: 0.23387186229228973, acc: 0.9212598204612732)
[2025-02-16 12:34:22,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:22,570][root][INFO] - Training Epoch: 1/2, step 9191/23838 completed (loss: 0.2340368926525116, acc: 0.9425287246704102)
[2025-02-16 12:34:22,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:22,984][root][INFO] - Training Epoch: 1/2, step 9192/23838 completed (loss: 0.29105475544929504, acc: 0.9203979969024658)
[2025-02-16 12:34:23,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:23,387][root][INFO] - Training Epoch: 1/2, step 9193/23838 completed (loss: 0.18211062252521515, acc: 0.9469026327133179)
[2025-02-16 12:34:23,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:23,775][root][INFO] - Training Epoch: 1/2, step 9194/23838 completed (loss: 0.12353472411632538, acc: 0.970588207244873)
[2025-02-16 12:34:23,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:24,174][root][INFO] - Training Epoch: 1/2, step 9195/23838 completed (loss: 0.2016260176897049, acc: 0.949999988079071)
[2025-02-16 12:34:24,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:24,617][root][INFO] - Training Epoch: 1/2, step 9196/23838 completed (loss: 0.3084934651851654, acc: 0.9252336621284485)
[2025-02-16 12:34:24,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:25,050][root][INFO] - Training Epoch: 1/2, step 9197/23838 completed (loss: 0.21575893461704254, acc: 0.9383561611175537)
[2025-02-16 12:34:25,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:25,471][root][INFO] - Training Epoch: 1/2, step 9198/23838 completed (loss: 0.09876421093940735, acc: 0.9743589758872986)
[2025-02-16 12:34:25,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:25,860][root][INFO] - Training Epoch: 1/2, step 9199/23838 completed (loss: 0.26208797097206116, acc: 0.925000011920929)
[2025-02-16 12:34:26,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:26,342][root][INFO] - Training Epoch: 1/2, step 9200/23838 completed (loss: 0.44205930829048157, acc: 0.8910890817642212)
[2025-02-16 12:34:26,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:26,809][root][INFO] - Training Epoch: 1/2, step 9201/23838 completed (loss: 0.30568763613700867, acc: 0.9166666865348816)
[2025-02-16 12:34:27,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:27,255][root][INFO] - Training Epoch: 1/2, step 9202/23838 completed (loss: 0.19843289256095886, acc: 0.934959352016449)
[2025-02-16 12:34:27,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:27,697][root][INFO] - Training Epoch: 1/2, step 9203/23838 completed (loss: 0.21998244524002075, acc: 0.9536423683166504)
[2025-02-16 12:34:27,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:28,055][root][INFO] - Training Epoch: 1/2, step 9204/23838 completed (loss: 0.4094541668891907, acc: 0.9074074029922485)
[2025-02-16 12:34:28,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:28,521][root][INFO] - Training Epoch: 1/2, step 9205/23838 completed (loss: 0.320235013961792, acc: 0.9223300814628601)
[2025-02-16 12:34:28,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:28,979][root][INFO] - Training Epoch: 1/2, step 9206/23838 completed (loss: 0.2364177107810974, acc: 0.932692289352417)
[2025-02-16 12:34:29,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:29,485][root][INFO] - Training Epoch: 1/2, step 9207/23838 completed (loss: 0.37308329343795776, acc: 0.9247311949729919)
[2025-02-16 12:34:29,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:29,892][root][INFO] - Training Epoch: 1/2, step 9208/23838 completed (loss: 0.39372536540031433, acc: 0.8793103694915771)
[2025-02-16 12:34:30,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:30,292][root][INFO] - Training Epoch: 1/2, step 9209/23838 completed (loss: 0.46697232127189636, acc: 0.859375)
[2025-02-16 12:34:30,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:30,722][root][INFO] - Training Epoch: 1/2, step 9210/23838 completed (loss: 0.20438328385353088, acc: 0.95652174949646)
[2025-02-16 12:34:30,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:31,133][root][INFO] - Training Epoch: 1/2, step 9211/23838 completed (loss: 0.20930014550685883, acc: 0.9444444179534912)
[2025-02-16 12:34:31,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:31,568][root][INFO] - Training Epoch: 1/2, step 9212/23838 completed (loss: 0.3671659529209137, acc: 0.8977272510528564)
[2025-02-16 12:34:31,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:32,011][root][INFO] - Training Epoch: 1/2, step 9213/23838 completed (loss: 0.20272931456565857, acc: 0.9541984796524048)
[2025-02-16 12:34:32,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:32,420][root][INFO] - Training Epoch: 1/2, step 9214/23838 completed (loss: 0.27544984221458435, acc: 0.932584285736084)
[2025-02-16 12:34:32,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:32,839][root][INFO] - Training Epoch: 1/2, step 9215/23838 completed (loss: 0.37284502387046814, acc: 0.9064327478408813)
[2025-02-16 12:34:33,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:33,325][root][INFO] - Training Epoch: 1/2, step 9216/23838 completed (loss: 0.21040497720241547, acc: 0.9430052042007446)
[2025-02-16 12:34:33,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:33,730][root][INFO] - Training Epoch: 1/2, step 9217/23838 completed (loss: 0.14919069409370422, acc: 0.9482758641242981)
[2025-02-16 12:34:33,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:34,121][root][INFO] - Training Epoch: 1/2, step 9218/23838 completed (loss: 0.23906929790973663, acc: 0.939393937587738)
[2025-02-16 12:34:34,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:34,498][root][INFO] - Training Epoch: 1/2, step 9219/23838 completed (loss: 0.5571639537811279, acc: 0.8720930218696594)
[2025-02-16 12:34:34,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:35,091][root][INFO] - Training Epoch: 1/2, step 9220/23838 completed (loss: 0.47448766231536865, acc: 0.9047619104385376)
[2025-02-16 12:34:35,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:35,519][root][INFO] - Training Epoch: 1/2, step 9221/23838 completed (loss: 0.33917099237442017, acc: 0.8979591727256775)
[2025-02-16 12:34:35,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:35,961][root][INFO] - Training Epoch: 1/2, step 9222/23838 completed (loss: 0.24784314632415771, acc: 0.912162184715271)
[2025-02-16 12:34:36,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:36,430][root][INFO] - Training Epoch: 1/2, step 9223/23838 completed (loss: 0.07960045337677002, acc: 0.9759036302566528)
[2025-02-16 12:34:36,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:36,870][root][INFO] - Training Epoch: 1/2, step 9224/23838 completed (loss: 0.29199501872062683, acc: 0.9160839319229126)
[2025-02-16 12:34:37,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:37,350][root][INFO] - Training Epoch: 1/2, step 9225/23838 completed (loss: 0.33713486790657043, acc: 0.9230769276618958)
[2025-02-16 12:34:37,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:37,805][root][INFO] - Training Epoch: 1/2, step 9226/23838 completed (loss: 0.3213444650173187, acc: 0.9133333563804626)
[2025-02-16 12:34:37,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:38,224][root][INFO] - Training Epoch: 1/2, step 9227/23838 completed (loss: 0.4337443709373474, acc: 0.8867924809455872)
[2025-02-16 12:34:38,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:38,674][root][INFO] - Training Epoch: 1/2, step 9228/23838 completed (loss: 0.3359895646572113, acc: 0.9006622433662415)
[2025-02-16 12:34:38,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:39,082][root][INFO] - Training Epoch: 1/2, step 9229/23838 completed (loss: 0.6105092763900757, acc: 0.8682170510292053)
[2025-02-16 12:34:39,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:39,481][root][INFO] - Training Epoch: 1/2, step 9230/23838 completed (loss: 0.4046458899974823, acc: 0.9380530714988708)
[2025-02-16 12:34:39,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:39,895][root][INFO] - Training Epoch: 1/2, step 9231/23838 completed (loss: 0.399713933467865, acc: 0.8907103538513184)
[2025-02-16 12:34:40,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:40,354][root][INFO] - Training Epoch: 1/2, step 9232/23838 completed (loss: 0.2583671510219574, acc: 0.9402173757553101)
[2025-02-16 12:34:40,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:40,832][root][INFO] - Training Epoch: 1/2, step 9233/23838 completed (loss: 0.16018934547901154, acc: 0.9411764740943909)
[2025-02-16 12:34:41,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:41,261][root][INFO] - Training Epoch: 1/2, step 9234/23838 completed (loss: 0.2925143241882324, acc: 0.9292035102844238)
[2025-02-16 12:34:41,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:41,675][root][INFO] - Training Epoch: 1/2, step 9235/23838 completed (loss: 0.31929531693458557, acc: 0.9230769276618958)
[2025-02-16 12:34:41,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:42,193][root][INFO] - Training Epoch: 1/2, step 9236/23838 completed (loss: 0.32553666830062866, acc: 0.9011628031730652)
[2025-02-16 12:34:42,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:42,648][root][INFO] - Training Epoch: 1/2, step 9237/23838 completed (loss: 0.26515722274780273, acc: 0.9047619104385376)
[2025-02-16 12:34:42,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:43,066][root][INFO] - Training Epoch: 1/2, step 9238/23838 completed (loss: 0.12457611411809921, acc: 0.9596773982048035)
[2025-02-16 12:34:43,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:43,488][root][INFO] - Training Epoch: 1/2, step 9239/23838 completed (loss: 0.14477944374084473, acc: 0.9459459185600281)
[2025-02-16 12:34:43,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:43,904][root][INFO] - Training Epoch: 1/2, step 9240/23838 completed (loss: 0.37565627694129944, acc: 0.8842105269432068)
[2025-02-16 12:34:44,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:44,999][root][INFO] - Training Epoch: 1/2, step 9241/23838 completed (loss: 0.5666369199752808, acc: 0.8666666746139526)
[2025-02-16 12:34:45,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:45,364][root][INFO] - Training Epoch: 1/2, step 9242/23838 completed (loss: 0.3588837683200836, acc: 0.875)
[2025-02-16 12:34:45,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:45,844][root][INFO] - Training Epoch: 1/2, step 9243/23838 completed (loss: 1.0218063592910767, acc: 0.7323943376541138)
[2025-02-16 12:34:46,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:46,377][root][INFO] - Training Epoch: 1/2, step 9244/23838 completed (loss: 0.4057096838951111, acc: 0.8850574493408203)
[2025-02-16 12:34:46,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:46,847][root][INFO] - Training Epoch: 1/2, step 9245/23838 completed (loss: 0.45835331082344055, acc: 0.8846153616905212)
[2025-02-16 12:34:47,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:47,834][root][INFO] - Training Epoch: 1/2, step 9246/23838 completed (loss: 0.34060022234916687, acc: 0.9150943160057068)
[2025-02-16 12:34:48,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:48,644][root][INFO] - Training Epoch: 1/2, step 9247/23838 completed (loss: 0.2762313485145569, acc: 0.9082568883895874)
[2025-02-16 12:34:48,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:49,222][root][INFO] - Training Epoch: 1/2, step 9248/23838 completed (loss: 0.6964911222457886, acc: 0.8414633870124817)
[2025-02-16 12:34:49,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:49,723][root][INFO] - Training Epoch: 1/2, step 9249/23838 completed (loss: 0.26415643095970154, acc: 0.9125000238418579)
[2025-02-16 12:34:49,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:50,229][root][INFO] - Training Epoch: 1/2, step 9250/23838 completed (loss: 0.3704843819141388, acc: 0.8846153616905212)
[2025-02-16 12:34:50,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:51,133][root][INFO] - Training Epoch: 1/2, step 9251/23838 completed (loss: 0.252659410238266, acc: 0.9236947894096375)
[2025-02-16 12:34:51,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:51,812][root][INFO] - Training Epoch: 1/2, step 9252/23838 completed (loss: 0.24184919893741608, acc: 0.8904109597206116)
[2025-02-16 12:34:52,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:52,472][root][INFO] - Training Epoch: 1/2, step 9253/23838 completed (loss: 0.7901783585548401, acc: 0.7777777910232544)
[2025-02-16 12:34:52,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:52,967][root][INFO] - Training Epoch: 1/2, step 9254/23838 completed (loss: 0.7160680294036865, acc: 0.8444444537162781)
[2025-02-16 12:34:53,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:53,430][root][INFO] - Training Epoch: 1/2, step 9255/23838 completed (loss: 0.6149476766586304, acc: 0.8292682766914368)
[2025-02-16 12:34:53,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:54,113][root][INFO] - Training Epoch: 1/2, step 9256/23838 completed (loss: 0.5758519768714905, acc: 0.8468468189239502)
[2025-02-16 12:34:54,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:55,176][root][INFO] - Training Epoch: 1/2, step 9257/23838 completed (loss: 0.47443416714668274, acc: 0.8655462265014648)
[2025-02-16 12:34:55,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:55,626][root][INFO] - Training Epoch: 1/2, step 9258/23838 completed (loss: 0.354899525642395, acc: 0.8904109597206116)
[2025-02-16 12:34:55,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:56,075][root][INFO] - Training Epoch: 1/2, step 9259/23838 completed (loss: 0.25052303075790405, acc: 0.9107142686843872)
[2025-02-16 12:34:56,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:56,541][root][INFO] - Training Epoch: 1/2, step 9260/23838 completed (loss: 0.1759711503982544, acc: 0.9670329689979553)
[2025-02-16 12:34:56,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:57,225][root][INFO] - Training Epoch: 1/2, step 9261/23838 completed (loss: 0.3117999732494354, acc: 0.9032257795333862)
[2025-02-16 12:34:57,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:57,956][root][INFO] - Training Epoch: 1/2, step 9262/23838 completed (loss: 0.6704107522964478, acc: 0.75)
[2025-02-16 12:34:58,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:58,370][root][INFO] - Training Epoch: 1/2, step 9263/23838 completed (loss: 0.5824060440063477, acc: 0.8055555820465088)
[2025-02-16 12:34:58,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:58,951][root][INFO] - Training Epoch: 1/2, step 9264/23838 completed (loss: 0.40789031982421875, acc: 0.9032257795333862)
[2025-02-16 12:34:59,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:59,409][root][INFO] - Training Epoch: 1/2, step 9265/23838 completed (loss: 0.859869122505188, acc: 0.800000011920929)
[2025-02-16 12:34:59,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:34:59,896][root][INFO] - Training Epoch: 1/2, step 9266/23838 completed (loss: 0.9067840576171875, acc: 0.7272727489471436)
[2025-02-16 12:35:00,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:00,971][root][INFO] - Training Epoch: 1/2, step 9267/23838 completed (loss: 1.0958976745605469, acc: 0.7394366264343262)
[2025-02-16 12:35:01,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:01,632][root][INFO] - Training Epoch: 1/2, step 9268/23838 completed (loss: 0.51395583152771, acc: 0.8607594966888428)
[2025-02-16 12:35:01,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:02,098][root][INFO] - Training Epoch: 1/2, step 9269/23838 completed (loss: 0.5291816592216492, acc: 0.8627451062202454)
[2025-02-16 12:35:02,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:02,566][root][INFO] - Training Epoch: 1/2, step 9270/23838 completed (loss: 0.6606597304344177, acc: 0.7931034564971924)
[2025-02-16 12:35:02,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:03,086][root][INFO] - Training Epoch: 1/2, step 9271/23838 completed (loss: 0.7440328598022461, acc: 0.8307692408561707)
[2025-02-16 12:35:03,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:04,147][root][INFO] - Training Epoch: 1/2, step 9272/23838 completed (loss: 0.6101614236831665, acc: 0.8187500238418579)
[2025-02-16 12:35:04,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:04,523][root][INFO] - Training Epoch: 1/2, step 9273/23838 completed (loss: 0.5149214863777161, acc: 0.8552631735801697)
[2025-02-16 12:35:04,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:05,278][root][INFO] - Training Epoch: 1/2, step 9274/23838 completed (loss: 0.5871316194534302, acc: 0.8363636136054993)
[2025-02-16 12:35:05,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:05,721][root][INFO] - Training Epoch: 1/2, step 9275/23838 completed (loss: 0.2694089412689209, acc: 0.9130434989929199)
[2025-02-16 12:35:06,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:06,289][root][INFO] - Training Epoch: 1/2, step 9276/23838 completed (loss: 0.6710448265075684, acc: 0.8235294222831726)
[2025-02-16 12:35:06,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:06,731][root][INFO] - Training Epoch: 1/2, step 9277/23838 completed (loss: 0.420466810464859, acc: 0.8867924809455872)
[2025-02-16 12:35:06,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:07,203][root][INFO] - Training Epoch: 1/2, step 9278/23838 completed (loss: 0.7020425796508789, acc: 0.7209302186965942)
[2025-02-16 12:35:07,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:08,260][root][INFO] - Training Epoch: 1/2, step 9279/23838 completed (loss: 1.1912343502044678, acc: 0.6391752362251282)
[2025-02-16 12:35:08,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:08,733][root][INFO] - Training Epoch: 1/2, step 9280/23838 completed (loss: 0.7159619927406311, acc: 0.7941176295280457)
[2025-02-16 12:35:08,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:09,106][root][INFO] - Training Epoch: 1/2, step 9281/23838 completed (loss: 0.6860687732696533, acc: 0.8148148059844971)
[2025-02-16 12:35:09,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:09,599][root][INFO] - Training Epoch: 1/2, step 9282/23838 completed (loss: 0.6193419098854065, acc: 0.8592965006828308)
[2025-02-16 12:35:09,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:10,052][root][INFO] - Training Epoch: 1/2, step 9283/23838 completed (loss: 0.38929829001426697, acc: 0.8536585569381714)
[2025-02-16 12:35:10,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:10,584][root][INFO] - Training Epoch: 1/2, step 9284/23838 completed (loss: 0.6192249655723572, acc: 0.8181818127632141)
[2025-02-16 12:35:10,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:11,227][root][INFO] - Training Epoch: 1/2, step 9285/23838 completed (loss: 0.594917893409729, acc: 0.8028169274330139)
[2025-02-16 12:35:11,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:11,832][root][INFO] - Training Epoch: 1/2, step 9286/23838 completed (loss: 0.23901715874671936, acc: 0.9277108311653137)
[2025-02-16 12:35:12,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:12,237][root][INFO] - Training Epoch: 1/2, step 9287/23838 completed (loss: 0.8419817090034485, acc: 0.7977527976036072)
[2025-02-16 12:35:12,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:12,780][root][INFO] - Training Epoch: 1/2, step 9288/23838 completed (loss: 0.08557949960231781, acc: 0.9743589758872986)
[2025-02-16 12:35:12,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:13,202][root][INFO] - Training Epoch: 1/2, step 9289/23838 completed (loss: 0.4013655185699463, acc: 0.9230769276618958)
[2025-02-16 12:35:13,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:13,674][root][INFO] - Training Epoch: 1/2, step 9290/23838 completed (loss: 0.5655900835990906, acc: 0.8723404407501221)
[2025-02-16 12:35:13,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:14,191][root][INFO] - Training Epoch: 1/2, step 9291/23838 completed (loss: 0.5178412795066833, acc: 0.8428571224212646)
[2025-02-16 12:35:14,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:14,804][root][INFO] - Training Epoch: 1/2, step 9292/23838 completed (loss: 0.5370002388954163, acc: 0.8421052694320679)
[2025-02-16 12:35:15,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:15,389][root][INFO] - Training Epoch: 1/2, step 9293/23838 completed (loss: 0.5791242718696594, acc: 0.8333333134651184)
[2025-02-16 12:35:15,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:15,811][root][INFO] - Training Epoch: 1/2, step 9294/23838 completed (loss: 0.7535823583602905, acc: 0.7910447716712952)
[2025-02-16 12:35:16,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:16,714][root][INFO] - Training Epoch: 1/2, step 9295/23838 completed (loss: 0.785465657711029, acc: 0.8139534592628479)
[2025-02-16 12:35:17,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:17,758][root][INFO] - Training Epoch: 1/2, step 9296/23838 completed (loss: 1.103635549545288, acc: 0.7383177280426025)
[2025-02-16 12:35:18,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:18,821][root][INFO] - Training Epoch: 1/2, step 9297/23838 completed (loss: 0.6851783990859985, acc: 0.811475396156311)
[2025-02-16 12:35:19,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:19,319][root][INFO] - Training Epoch: 1/2, step 9298/23838 completed (loss: 0.6140263676643372, acc: 0.8285714387893677)
[2025-02-16 12:35:19,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:20,201][root][INFO] - Training Epoch: 1/2, step 9299/23838 completed (loss: 0.5101060271263123, acc: 0.8421052694320679)
[2025-02-16 12:35:20,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:20,832][root][INFO] - Training Epoch: 1/2, step 9300/23838 completed (loss: 0.5898452401161194, acc: 0.8289473652839661)
[2025-02-16 12:35:21,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:21,877][root][INFO] - Training Epoch: 1/2, step 9301/23838 completed (loss: 1.333092212677002, acc: 0.6666666865348816)
[2025-02-16 12:35:22,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:22,296][root][INFO] - Training Epoch: 1/2, step 9302/23838 completed (loss: 0.5097395181655884, acc: 0.8707482814788818)
[2025-02-16 12:35:22,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:22,822][root][INFO] - Training Epoch: 1/2, step 9303/23838 completed (loss: 0.8487330079078674, acc: 0.7894737124443054)
[2025-02-16 12:35:23,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:23,868][root][INFO] - Training Epoch: 1/2, step 9304/23838 completed (loss: 1.013482928276062, acc: 0.761904776096344)
[2025-02-16 12:35:24,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:24,580][root][INFO] - Training Epoch: 1/2, step 9305/23838 completed (loss: 0.6752066016197205, acc: 0.8048780560493469)
[2025-02-16 12:35:24,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:24,958][root][INFO] - Training Epoch: 1/2, step 9306/23838 completed (loss: 0.5391640067100525, acc: 0.828125)
[2025-02-16 12:35:25,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:25,397][root][INFO] - Training Epoch: 1/2, step 9307/23838 completed (loss: 0.7189776301383972, acc: 0.8048780560493469)
[2025-02-16 12:35:25,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:25,919][root][INFO] - Training Epoch: 1/2, step 9308/23838 completed (loss: 0.788307785987854, acc: 0.800000011920929)
[2025-02-16 12:35:26,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:26,678][root][INFO] - Training Epoch: 1/2, step 9309/23838 completed (loss: 0.6306227445602417, acc: 0.8101266026496887)
[2025-02-16 12:35:26,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:27,104][root][INFO] - Training Epoch: 1/2, step 9310/23838 completed (loss: 0.8509638905525208, acc: 0.8235294222831726)
[2025-02-16 12:35:27,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:28,184][root][INFO] - Training Epoch: 1/2, step 9311/23838 completed (loss: 0.9025214314460754, acc: 0.7426470518112183)
[2025-02-16 12:35:28,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:28,747][root][INFO] - Training Epoch: 1/2, step 9312/23838 completed (loss: 0.4914860427379608, acc: 0.8653846383094788)
[2025-02-16 12:35:28,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:29,231][root][INFO] - Training Epoch: 1/2, step 9313/23838 completed (loss: 0.5749896764755249, acc: 0.8314606547355652)
[2025-02-16 12:35:29,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:29,716][root][INFO] - Training Epoch: 1/2, step 9314/23838 completed (loss: 1.090461015701294, acc: 0.6888889074325562)
[2025-02-16 12:35:30,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:30,842][root][INFO] - Training Epoch: 1/2, step 9315/23838 completed (loss: 0.7714623808860779, acc: 0.8060606122016907)
[2025-02-16 12:35:31,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:31,730][root][INFO] - Training Epoch: 1/2, step 9316/23838 completed (loss: 0.5733734965324402, acc: 0.8454545736312866)
[2025-02-16 12:35:31,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:32,185][root][INFO] - Training Epoch: 1/2, step 9317/23838 completed (loss: 0.5488650798797607, acc: 0.8780487775802612)
[2025-02-16 12:35:32,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:32,575][root][INFO] - Training Epoch: 1/2, step 9318/23838 completed (loss: 0.4698891043663025, acc: 0.824999988079071)
[2025-02-16 12:35:32,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:33,203][root][INFO] - Training Epoch: 1/2, step 9319/23838 completed (loss: 0.2821246087551117, acc: 0.9078947305679321)
[2025-02-16 12:35:33,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:33,679][root][INFO] - Training Epoch: 1/2, step 9320/23838 completed (loss: 0.7176761031150818, acc: 0.8367347121238708)
[2025-02-16 12:35:34,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:34,678][root][INFO] - Training Epoch: 1/2, step 9321/23838 completed (loss: 0.81514573097229, acc: 0.7839999794960022)
[2025-02-16 12:35:35,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:35,758][root][INFO] - Training Epoch: 1/2, step 9322/23838 completed (loss: 0.5664080381393433, acc: 0.8535031676292419)
[2025-02-16 12:35:36,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:36,845][root][INFO] - Training Epoch: 1/2, step 9323/23838 completed (loss: 0.8371422290802002, acc: 0.7852349281311035)
[2025-02-16 12:35:37,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:37,621][root][INFO] - Training Epoch: 1/2, step 9324/23838 completed (loss: 0.6448699831962585, acc: 0.8395061492919922)
[2025-02-16 12:35:38,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:38,682][root][INFO] - Training Epoch: 1/2, step 9325/23838 completed (loss: 1.3700783252716064, acc: 0.6379310488700867)
[2025-02-16 12:35:39,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:39,735][root][INFO] - Training Epoch: 1/2, step 9326/23838 completed (loss: 0.6759893894195557, acc: 0.7850467562675476)
[2025-02-16 12:35:39,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:40,252][root][INFO] - Training Epoch: 1/2, step 9327/23838 completed (loss: 0.5635539889335632, acc: 0.8364779949188232)
[2025-02-16 12:35:40,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:40,795][root][INFO] - Training Epoch: 1/2, step 9328/23838 completed (loss: 0.40358173847198486, acc: 0.8852459192276001)
[2025-02-16 12:35:41,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:41,867][root][INFO] - Training Epoch: 1/2, step 9329/23838 completed (loss: 0.8611559271812439, acc: 0.7416666746139526)
[2025-02-16 12:35:42,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:42,906][root][INFO] - Training Epoch: 1/2, step 9330/23838 completed (loss: 1.254765510559082, acc: 0.6320754885673523)
[2025-02-16 12:35:43,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:43,832][root][INFO] - Training Epoch: 1/2, step 9331/23838 completed (loss: 0.7682723999023438, acc: 0.7692307829856873)
[2025-02-16 12:35:44,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:44,384][root][INFO] - Training Epoch: 1/2, step 9332/23838 completed (loss: 0.4628888964653015, acc: 0.8725489974021912)
[2025-02-16 12:35:44,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:45,146][root][INFO] - Training Epoch: 1/2, step 9333/23838 completed (loss: 0.6127012968063354, acc: 0.801886796951294)
[2025-02-16 12:35:45,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:46,017][root][INFO] - Training Epoch: 1/2, step 9334/23838 completed (loss: 0.4670327305793762, acc: 0.8768116235733032)
[2025-02-16 12:35:46,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:47,105][root][INFO] - Training Epoch: 1/2, step 9335/23838 completed (loss: 0.5592287182807922, acc: 0.8607594966888428)
[2025-02-16 12:35:47,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:47,541][root][INFO] - Training Epoch: 1/2, step 9336/23838 completed (loss: 0.9524182677268982, acc: 0.737864077091217)
[2025-02-16 12:35:47,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:47,997][root][INFO] - Training Epoch: 1/2, step 9337/23838 completed (loss: 0.500930666923523, acc: 0.8529411554336548)
[2025-02-16 12:35:48,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:48,468][root][INFO] - Training Epoch: 1/2, step 9338/23838 completed (loss: 0.471309095621109, acc: 0.875)
[2025-02-16 12:35:48,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:49,052][root][INFO] - Training Epoch: 1/2, step 9339/23838 completed (loss: 0.5222715735435486, acc: 0.843137264251709)
[2025-02-16 12:35:49,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:49,527][root][INFO] - Training Epoch: 1/2, step 9340/23838 completed (loss: 0.7494162321090698, acc: 0.782608687877655)
[2025-02-16 12:35:49,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:50,263][root][INFO] - Training Epoch: 1/2, step 9341/23838 completed (loss: 0.9278839826583862, acc: 0.7837837934494019)
[2025-02-16 12:35:50,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:51,254][root][INFO] - Training Epoch: 1/2, step 9342/23838 completed (loss: 0.5539129376411438, acc: 0.837837815284729)
[2025-02-16 12:35:51,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:51,697][root][INFO] - Training Epoch: 1/2, step 9343/23838 completed (loss: 0.6858420968055725, acc: 0.824999988079071)
[2025-02-16 12:35:51,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:52,180][root][INFO] - Training Epoch: 1/2, step 9344/23838 completed (loss: 1.00008225440979, acc: 0.7200000286102295)
[2025-02-16 12:35:52,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:52,743][root][INFO] - Training Epoch: 1/2, step 9345/23838 completed (loss: 0.5415888428688049, acc: 0.8522727489471436)
[2025-02-16 12:35:53,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:53,817][root][INFO] - Training Epoch: 1/2, step 9346/23838 completed (loss: 0.8215101957321167, acc: 0.7666666507720947)
[2025-02-16 12:35:54,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:54,659][root][INFO] - Training Epoch: 1/2, step 9347/23838 completed (loss: 0.4072553515434265, acc: 0.8613861203193665)
[2025-02-16 12:35:54,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:55,057][root][INFO] - Training Epoch: 1/2, step 9348/23838 completed (loss: 0.3143754303455353, acc: 0.9160839319229126)
[2025-02-16 12:35:55,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:55,531][root][INFO] - Training Epoch: 1/2, step 9349/23838 completed (loss: 0.15883734822273254, acc: 0.9586777091026306)
[2025-02-16 12:35:55,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:55,988][root][INFO] - Training Epoch: 1/2, step 9350/23838 completed (loss: 0.5190623998641968, acc: 0.8230088353157043)
[2025-02-16 12:35:56,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:56,423][root][INFO] - Training Epoch: 1/2, step 9351/23838 completed (loss: 0.35358279943466187, acc: 0.9030836820602417)
[2025-02-16 12:35:56,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:56,854][root][INFO] - Training Epoch: 1/2, step 9352/23838 completed (loss: 1.0986014604568481, acc: 0.7529411911964417)
[2025-02-16 12:35:57,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:57,368][root][INFO] - Training Epoch: 1/2, step 9353/23838 completed (loss: 0.5703267455101013, acc: 0.8819444179534912)
[2025-02-16 12:35:57,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:57,958][root][INFO] - Training Epoch: 1/2, step 9354/23838 completed (loss: 0.3264027535915375, acc: 0.916201114654541)
[2025-02-16 12:35:58,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:58,456][root][INFO] - Training Epoch: 1/2, step 9355/23838 completed (loss: 0.4816785156726837, acc: 0.8707482814788818)
[2025-02-16 12:35:58,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:58,864][root][INFO] - Training Epoch: 1/2, step 9356/23838 completed (loss: 0.29493725299835205, acc: 0.9117646813392639)
[2025-02-16 12:35:59,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:59,567][root][INFO] - Training Epoch: 1/2, step 9357/23838 completed (loss: 0.5665026903152466, acc: 0.8465608358383179)
[2025-02-16 12:35:59,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:35:59,941][root][INFO] - Training Epoch: 1/2, step 9358/23838 completed (loss: 0.471610426902771, acc: 0.8909090757369995)
[2025-02-16 12:36:00,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:00,430][root][INFO] - Training Epoch: 1/2, step 9359/23838 completed (loss: 0.38660097122192383, acc: 0.8936170339584351)
[2025-02-16 12:36:00,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:00,892][root][INFO] - Training Epoch: 1/2, step 9360/23838 completed (loss: 0.519374430179596, acc: 0.8333333134651184)
[2025-02-16 12:36:01,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:01,356][root][INFO] - Training Epoch: 1/2, step 9361/23838 completed (loss: 0.4242704212665558, acc: 0.9056603908538818)
[2025-02-16 12:36:01,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:01,782][root][INFO] - Training Epoch: 1/2, step 9362/23838 completed (loss: 1.0010223388671875, acc: 0.7789473533630371)
[2025-02-16 12:36:02,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:02,288][root][INFO] - Training Epoch: 1/2, step 9363/23838 completed (loss: 0.3326963782310486, acc: 0.8897637724876404)
[2025-02-16 12:36:02,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:02,790][root][INFO] - Training Epoch: 1/2, step 9364/23838 completed (loss: 0.4326551556587219, acc: 0.868852436542511)
[2025-02-16 12:36:03,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:03,258][root][INFO] - Training Epoch: 1/2, step 9365/23838 completed (loss: 0.21424761414527893, acc: 0.9285714030265808)
[2025-02-16 12:36:03,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:03,839][root][INFO] - Training Epoch: 1/2, step 9366/23838 completed (loss: 0.9251773953437805, acc: 0.7941176295280457)
[2025-02-16 12:36:03,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:04,277][root][INFO] - Training Epoch: 1/2, step 9367/23838 completed (loss: 0.583857536315918, acc: 0.8320000171661377)
[2025-02-16 12:36:04,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:04,743][root][INFO] - Training Epoch: 1/2, step 9368/23838 completed (loss: 0.408814013004303, acc: 0.8584070801734924)
[2025-02-16 12:36:05,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:05,305][root][INFO] - Training Epoch: 1/2, step 9369/23838 completed (loss: 0.4538930654525757, acc: 0.8776595592498779)
[2025-02-16 12:36:05,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:05,682][root][INFO] - Training Epoch: 1/2, step 9370/23838 completed (loss: 0.38981178402900696, acc: 0.8823529481887817)
[2025-02-16 12:36:05,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:06,134][root][INFO] - Training Epoch: 1/2, step 9371/23838 completed (loss: 0.49263429641723633, acc: 0.8455284833908081)
[2025-02-16 12:36:06,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:06,603][root][INFO] - Training Epoch: 1/2, step 9372/23838 completed (loss: 0.3314204812049866, acc: 0.924369752407074)
[2025-02-16 12:36:06,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:07,092][root][INFO] - Training Epoch: 1/2, step 9373/23838 completed (loss: 0.2084420919418335, acc: 0.9416058659553528)
[2025-02-16 12:36:07,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:07,542][root][INFO] - Training Epoch: 1/2, step 9374/23838 completed (loss: 0.7371887564659119, acc: 0.8266666531562805)
[2025-02-16 12:36:07,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:08,008][root][INFO] - Training Epoch: 1/2, step 9375/23838 completed (loss: 0.6075645089149475, acc: 0.8695651888847351)
[2025-02-16 12:36:08,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:08,449][root][INFO] - Training Epoch: 1/2, step 9376/23838 completed (loss: 0.33825403451919556, acc: 0.9160305261611938)
[2025-02-16 12:36:08,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:08,986][root][INFO] - Training Epoch: 1/2, step 9377/23838 completed (loss: 0.2436598390340805, acc: 0.942105233669281)
[2025-02-16 12:36:09,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:09,398][root][INFO] - Training Epoch: 1/2, step 9378/23838 completed (loss: 0.49330633878707886, acc: 0.8561151027679443)
[2025-02-16 12:36:09,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:09,844][root][INFO] - Training Epoch: 1/2, step 9379/23838 completed (loss: 0.41004857420921326, acc: 0.8888888955116272)
[2025-02-16 12:36:10,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:10,349][root][INFO] - Training Epoch: 1/2, step 9380/23838 completed (loss: 0.31999731063842773, acc: 0.9057591557502747)
[2025-02-16 12:36:10,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:10,806][root][INFO] - Training Epoch: 1/2, step 9381/23838 completed (loss: 0.4566173851490021, acc: 0.8877550959587097)
[2025-02-16 12:36:11,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:11,398][root][INFO] - Training Epoch: 1/2, step 9382/23838 completed (loss: 0.26684263348579407, acc: 0.9261083602905273)
[2025-02-16 12:36:11,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:12,337][root][INFO] - Training Epoch: 1/2, step 9383/23838 completed (loss: 0.23831593990325928, acc: 0.9285714030265808)
[2025-02-16 12:36:12,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:12,748][root][INFO] - Training Epoch: 1/2, step 9384/23838 completed (loss: 0.2456440031528473, acc: 0.9396551847457886)
[2025-02-16 12:36:13,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:13,607][root][INFO] - Training Epoch: 1/2, step 9385/23838 completed (loss: 0.4053219258785248, acc: 0.8947368264198303)
[2025-02-16 12:36:13,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:14,128][root][INFO] - Training Epoch: 1/2, step 9386/23838 completed (loss: 0.22568561136722565, acc: 0.9462365508079529)
[2025-02-16 12:36:14,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:14,590][root][INFO] - Training Epoch: 1/2, step 9387/23838 completed (loss: 0.433186799287796, acc: 0.8943089246749878)
[2025-02-16 12:36:14,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:15,058][root][INFO] - Training Epoch: 1/2, step 9388/23838 completed (loss: 0.8710811734199524, acc: 0.7857142686843872)
[2025-02-16 12:36:15,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:15,503][root][INFO] - Training Epoch: 1/2, step 9389/23838 completed (loss: 0.46991726756095886, acc: 0.8971962332725525)
[2025-02-16 12:36:15,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:16,239][root][INFO] - Training Epoch: 1/2, step 9390/23838 completed (loss: 0.3900900185108185, acc: 0.9099099040031433)
[2025-02-16 12:36:16,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:16,700][root][INFO] - Training Epoch: 1/2, step 9391/23838 completed (loss: 0.37676554918289185, acc: 0.8602150678634644)
[2025-02-16 12:36:17,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:17,510][root][INFO] - Training Epoch: 1/2, step 9392/23838 completed (loss: 0.25167056918144226, acc: 0.935251772403717)
[2025-02-16 12:36:17,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:17,954][root][INFO] - Training Epoch: 1/2, step 9393/23838 completed (loss: 1.2368173599243164, acc: 0.6938775777816772)
[2025-02-16 12:36:18,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:18,425][root][INFO] - Training Epoch: 1/2, step 9394/23838 completed (loss: 0.191849023103714, acc: 0.9615384340286255)
[2025-02-16 12:36:18,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:18,844][root][INFO] - Training Epoch: 1/2, step 9395/23838 completed (loss: 0.2618650794029236, acc: 0.9230769276618958)
[2025-02-16 12:36:19,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:19,696][root][INFO] - Training Epoch: 1/2, step 9396/23838 completed (loss: 0.3896418809890747, acc: 0.915730357170105)
[2025-02-16 12:36:19,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:20,180][root][INFO] - Training Epoch: 1/2, step 9397/23838 completed (loss: 0.5028970837593079, acc: 0.8584070801734924)
[2025-02-16 12:36:20,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:20,586][root][INFO] - Training Epoch: 1/2, step 9398/23838 completed (loss: 0.6216403245925903, acc: 0.8644067645072937)
[2025-02-16 12:36:20,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:21,118][root][INFO] - Training Epoch: 1/2, step 9399/23838 completed (loss: 0.14310605823993683, acc: 0.9532710313796997)
[2025-02-16 12:36:21,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:21,861][root][INFO] - Training Epoch: 1/2, step 9400/23838 completed (loss: 0.2935154438018799, acc: 0.957317054271698)
[2025-02-16 12:36:22,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:22,342][root][INFO] - Training Epoch: 1/2, step 9401/23838 completed (loss: 0.26323145627975464, acc: 0.9230769276618958)
[2025-02-16 12:36:22,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:22,806][root][INFO] - Training Epoch: 1/2, step 9402/23838 completed (loss: 0.24175816774368286, acc: 0.932692289352417)
[2025-02-16 12:36:23,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:23,278][root][INFO] - Training Epoch: 1/2, step 9403/23838 completed (loss: 0.4339250922203064, acc: 0.8703703880310059)
[2025-02-16 12:36:23,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:23,712][root][INFO] - Training Epoch: 1/2, step 9404/23838 completed (loss: 0.28960031270980835, acc: 0.948051929473877)
[2025-02-16 12:36:23,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:24,089][root][INFO] - Training Epoch: 1/2, step 9405/23838 completed (loss: 0.40226832032203674, acc: 0.8709677457809448)
[2025-02-16 12:36:24,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:24,547][root][INFO] - Training Epoch: 1/2, step 9406/23838 completed (loss: 0.4064198434352875, acc: 0.8790322542190552)
[2025-02-16 12:36:24,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:24,943][root][INFO] - Training Epoch: 1/2, step 9407/23838 completed (loss: 0.23479588329792023, acc: 0.9101123809814453)
[2025-02-16 12:36:25,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:25,386][root][INFO] - Training Epoch: 1/2, step 9408/23838 completed (loss: 0.5427588224411011, acc: 0.8560606241226196)
[2025-02-16 12:36:25,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:25,784][root][INFO] - Training Epoch: 1/2, step 9409/23838 completed (loss: 0.20794670283794403, acc: 0.9523809552192688)
[2025-02-16 12:36:25,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:26,211][root][INFO] - Training Epoch: 1/2, step 9410/23838 completed (loss: 0.44379329681396484, acc: 0.8791208863258362)
[2025-02-16 12:36:26,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:26,764][root][INFO] - Training Epoch: 1/2, step 9411/23838 completed (loss: 0.2288414090871811, acc: 0.9545454382896423)
[2025-02-16 12:36:26,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:27,156][root][INFO] - Training Epoch: 1/2, step 9412/23838 completed (loss: 0.5557252168655396, acc: 0.8433734774589539)
[2025-02-16 12:36:27,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:27,611][root][INFO] - Training Epoch: 1/2, step 9413/23838 completed (loss: 0.20581991970539093, acc: 0.9444444179534912)
[2025-02-16 12:36:27,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:28,049][root][INFO] - Training Epoch: 1/2, step 9414/23838 completed (loss: 0.23576132953166962, acc: 0.949999988079071)
[2025-02-16 12:36:28,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:28,571][root][INFO] - Training Epoch: 1/2, step 9415/23838 completed (loss: 0.2581540048122406, acc: 0.939130425453186)
[2025-02-16 12:36:28,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:28,948][root][INFO] - Training Epoch: 1/2, step 9416/23838 completed (loss: 0.43420112133026123, acc: 0.9090909361839294)
[2025-02-16 12:36:29,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:29,451][root][INFO] - Training Epoch: 1/2, step 9417/23838 completed (loss: 0.2755156457424164, acc: 0.9215686321258545)
[2025-02-16 12:36:29,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:30,090][root][INFO] - Training Epoch: 1/2, step 9418/23838 completed (loss: 0.050646740943193436, acc: 0.9851852059364319)
[2025-02-16 12:36:30,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:30,567][root][INFO] - Training Epoch: 1/2, step 9419/23838 completed (loss: 0.8640365600585938, acc: 0.7714285850524902)
[2025-02-16 12:36:30,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:31,048][root][INFO] - Training Epoch: 1/2, step 9420/23838 completed (loss: 0.4371395409107208, acc: 0.8828828930854797)
[2025-02-16 12:36:31,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:31,532][root][INFO] - Training Epoch: 1/2, step 9421/23838 completed (loss: 0.3222733736038208, acc: 0.9160839319229126)
[2025-02-16 12:36:31,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:32,419][root][INFO] - Training Epoch: 1/2, step 9422/23838 completed (loss: 0.18275249004364014, acc: 0.9455445408821106)
[2025-02-16 12:36:32,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:32,915][root][INFO] - Training Epoch: 1/2, step 9423/23838 completed (loss: 0.2694590985774994, acc: 0.9473684430122375)
[2025-02-16 12:36:33,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:33,299][root][INFO] - Training Epoch: 1/2, step 9424/23838 completed (loss: 0.5188025236129761, acc: 0.8461538553237915)
[2025-02-16 12:36:33,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:33,762][root][INFO] - Training Epoch: 1/2, step 9425/23838 completed (loss: 0.2853541076183319, acc: 0.9430894255638123)
[2025-02-16 12:36:33,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:34,178][root][INFO] - Training Epoch: 1/2, step 9426/23838 completed (loss: 0.34209588170051575, acc: 0.926174521446228)
[2025-02-16 12:36:34,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:34,784][root][INFO] - Training Epoch: 1/2, step 9427/23838 completed (loss: 0.29038307070732117, acc: 0.9319371581077576)
[2025-02-16 12:36:34,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:35,193][root][INFO] - Training Epoch: 1/2, step 9428/23838 completed (loss: 0.2492302507162094, acc: 0.9512194991111755)
[2025-02-16 12:36:35,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:36,187][root][INFO] - Training Epoch: 1/2, step 9429/23838 completed (loss: 0.46218228340148926, acc: 0.871345043182373)
[2025-02-16 12:36:36,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:36,589][root][INFO] - Training Epoch: 1/2, step 9430/23838 completed (loss: 0.1512409895658493, acc: 0.9370078444480896)
[2025-02-16 12:36:36,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:37,033][root][INFO] - Training Epoch: 1/2, step 9431/23838 completed (loss: 0.6074944734573364, acc: 0.843137264251709)
[2025-02-16 12:36:37,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:37,463][root][INFO] - Training Epoch: 1/2, step 9432/23838 completed (loss: 0.32047751545906067, acc: 0.9140625)
[2025-02-16 12:36:37,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:37,922][root][INFO] - Training Epoch: 1/2, step 9433/23838 completed (loss: 0.7243491411209106, acc: 0.7727272510528564)
[2025-02-16 12:36:38,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:38,338][root][INFO] - Training Epoch: 1/2, step 9434/23838 completed (loss: 0.3806751072406769, acc: 0.9038461446762085)
[2025-02-16 12:36:38,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:39,077][root][INFO] - Training Epoch: 1/2, step 9435/23838 completed (loss: 0.43945741653442383, acc: 0.8882681727409363)
[2025-02-16 12:36:39,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:39,621][root][INFO] - Training Epoch: 1/2, step 9436/23838 completed (loss: 0.5291408896446228, acc: 0.8292682766914368)
[2025-02-16 12:36:39,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:40,246][root][INFO] - Training Epoch: 1/2, step 9437/23838 completed (loss: 0.42310547828674316, acc: 0.902255654335022)
[2025-02-16 12:36:40,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:40,808][root][INFO] - Training Epoch: 1/2, step 9438/23838 completed (loss: 0.5708064436912537, acc: 0.8686131238937378)
[2025-02-16 12:36:40,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:41,173][root][INFO] - Training Epoch: 1/2, step 9439/23838 completed (loss: 0.6041605472564697, acc: 0.8390804529190063)
[2025-02-16 12:36:41,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:41,611][root][INFO] - Training Epoch: 1/2, step 9440/23838 completed (loss: 0.6685066223144531, acc: 0.8265306353569031)
[2025-02-16 12:36:41,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:42,071][root][INFO] - Training Epoch: 1/2, step 9441/23838 completed (loss: 0.04782679304480553, acc: 0.9870129823684692)
[2025-02-16 12:36:42,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:42,459][root][INFO] - Training Epoch: 1/2, step 9442/23838 completed (loss: 0.8630273342132568, acc: 0.7200000286102295)
[2025-02-16 12:36:42,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:42,963][root][INFO] - Training Epoch: 1/2, step 9443/23838 completed (loss: 0.47445324063301086, acc: 0.8770492076873779)
[2025-02-16 12:36:43,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:43,401][root][INFO] - Training Epoch: 1/2, step 9444/23838 completed (loss: 0.18679660558700562, acc: 0.9444444179534912)
[2025-02-16 12:36:43,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:43,917][root][INFO] - Training Epoch: 1/2, step 9445/23838 completed (loss: 0.3355347812175751, acc: 0.9024389982223511)
[2025-02-16 12:36:44,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:44,350][root][INFO] - Training Epoch: 1/2, step 9446/23838 completed (loss: 0.5487638115882874, acc: 0.8765432238578796)
[2025-02-16 12:36:44,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:44,805][root][INFO] - Training Epoch: 1/2, step 9447/23838 completed (loss: 0.34056979417800903, acc: 0.918181836605072)
[2025-02-16 12:36:45,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:45,261][root][INFO] - Training Epoch: 1/2, step 9448/23838 completed (loss: 0.10142073780298233, acc: 0.9818181991577148)
[2025-02-16 12:36:45,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:45,690][root][INFO] - Training Epoch: 1/2, step 9449/23838 completed (loss: 0.416088342666626, acc: 0.8769230842590332)
[2025-02-16 12:36:45,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:46,165][root][INFO] - Training Epoch: 1/2, step 9450/23838 completed (loss: 0.4685129225254059, acc: 0.8767123222351074)
[2025-02-16 12:36:46,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:46,676][root][INFO] - Training Epoch: 1/2, step 9451/23838 completed (loss: 0.2712145745754242, acc: 0.9230769276618958)
[2025-02-16 12:36:46,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:47,097][root][INFO] - Training Epoch: 1/2, step 9452/23838 completed (loss: 0.7017876505851746, acc: 0.8199999928474426)
[2025-02-16 12:36:47,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:47,623][root][INFO] - Training Epoch: 1/2, step 9453/23838 completed (loss: 0.43902623653411865, acc: 0.904347836971283)
[2025-02-16 12:36:47,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:48,100][root][INFO] - Training Epoch: 1/2, step 9454/23838 completed (loss: 0.4877362847328186, acc: 0.8661971688270569)
[2025-02-16 12:36:48,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:48,673][root][INFO] - Training Epoch: 1/2, step 9455/23838 completed (loss: 0.5648351311683655, acc: 0.8503937125205994)
[2025-02-16 12:36:48,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:49,072][root][INFO] - Training Epoch: 1/2, step 9456/23838 completed (loss: 0.2635412812232971, acc: 0.970588207244873)
[2025-02-16 12:36:49,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:49,554][root][INFO] - Training Epoch: 1/2, step 9457/23838 completed (loss: 0.5759823322296143, acc: 0.8676470518112183)
[2025-02-16 12:36:49,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:50,052][root][INFO] - Training Epoch: 1/2, step 9458/23838 completed (loss: 0.4801117777824402, acc: 0.8848484754562378)
[2025-02-16 12:36:50,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:50,514][root][INFO] - Training Epoch: 1/2, step 9459/23838 completed (loss: 0.3177119493484497, acc: 0.8938053250312805)
[2025-02-16 12:36:50,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:50,950][root][INFO] - Training Epoch: 1/2, step 9460/23838 completed (loss: 0.41155532002449036, acc: 0.8709677457809448)
[2025-02-16 12:36:51,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:51,404][root][INFO] - Training Epoch: 1/2, step 9461/23838 completed (loss: 0.29396575689315796, acc: 0.9279279112815857)
[2025-02-16 12:36:51,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:51,885][root][INFO] - Training Epoch: 1/2, step 9462/23838 completed (loss: 0.21659652888774872, acc: 0.9263803958892822)
[2025-02-16 12:36:52,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:52,337][root][INFO] - Training Epoch: 1/2, step 9463/23838 completed (loss: 0.28793618083000183, acc: 0.9172932505607605)
[2025-02-16 12:36:52,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:52,733][root][INFO] - Training Epoch: 1/2, step 9464/23838 completed (loss: 0.32070639729499817, acc: 0.8999999761581421)
[2025-02-16 12:36:52,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:53,184][root][INFO] - Training Epoch: 1/2, step 9465/23838 completed (loss: 0.31611546874046326, acc: 0.936170220375061)
[2025-02-16 12:36:53,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:53,793][root][INFO] - Training Epoch: 1/2, step 9466/23838 completed (loss: 0.23727969825267792, acc: 0.9283154010772705)
[2025-02-16 12:36:54,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:54,654][root][INFO] - Training Epoch: 1/2, step 9467/23838 completed (loss: 0.2535722553730011, acc: 0.9102563858032227)
[2025-02-16 12:36:54,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:55,134][root][INFO] - Training Epoch: 1/2, step 9468/23838 completed (loss: 0.2238427996635437, acc: 0.925000011920929)
[2025-02-16 12:36:55,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:55,535][root][INFO] - Training Epoch: 1/2, step 9469/23838 completed (loss: 0.17113594710826874, acc: 0.9432623982429504)
[2025-02-16 12:36:55,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:55,997][root][INFO] - Training Epoch: 1/2, step 9470/23838 completed (loss: 0.2542504072189331, acc: 0.9289940595626831)
[2025-02-16 12:36:56,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:56,471][root][INFO] - Training Epoch: 1/2, step 9471/23838 completed (loss: 0.3640357553958893, acc: 0.9124087691307068)
[2025-02-16 12:36:56,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:57,016][root][INFO] - Training Epoch: 1/2, step 9472/23838 completed (loss: 0.3268965184688568, acc: 0.934959352016449)
[2025-02-16 12:36:57,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:57,484][root][INFO] - Training Epoch: 1/2, step 9473/23838 completed (loss: 0.10100744664669037, acc: 0.9774436354637146)
[2025-02-16 12:36:57,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:58,102][root][INFO] - Training Epoch: 1/2, step 9474/23838 completed (loss: 0.3179386556148529, acc: 0.8999999761581421)
[2025-02-16 12:36:58,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:58,560][root][INFO] - Training Epoch: 1/2, step 9475/23838 completed (loss: 0.347972571849823, acc: 0.9239130616188049)
[2025-02-16 12:36:58,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:59,203][root][INFO] - Training Epoch: 1/2, step 9476/23838 completed (loss: 0.26036337018013, acc: 0.9200000166893005)
[2025-02-16 12:36:59,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:36:59,595][root][INFO] - Training Epoch: 1/2, step 9477/23838 completed (loss: 0.1817985475063324, acc: 0.9538461565971375)
[2025-02-16 12:36:59,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:00,311][root][INFO] - Training Epoch: 1/2, step 9478/23838 completed (loss: 0.3023666739463806, acc: 0.9299065470695496)
[2025-02-16 12:37:00,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:00,873][root][INFO] - Training Epoch: 1/2, step 9479/23838 completed (loss: 0.3186369240283966, acc: 0.9272727370262146)
[2025-02-16 12:37:01,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:01,349][root][INFO] - Training Epoch: 1/2, step 9480/23838 completed (loss: 0.20362678170204163, acc: 0.9371069073677063)
[2025-02-16 12:37:01,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:01,796][root][INFO] - Training Epoch: 1/2, step 9481/23838 completed (loss: 0.49847039580345154, acc: 0.8681318759918213)
[2025-02-16 12:37:02,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:02,301][root][INFO] - Training Epoch: 1/2, step 9482/23838 completed (loss: 0.23658105731010437, acc: 0.9425287246704102)
[2025-02-16 12:37:02,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:02,718][root][INFO] - Training Epoch: 1/2, step 9483/23838 completed (loss: 0.2423594444990158, acc: 0.9327731132507324)
[2025-02-16 12:37:02,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:03,200][root][INFO] - Training Epoch: 1/2, step 9484/23838 completed (loss: 0.5208833813667297, acc: 0.8899999856948853)
[2025-02-16 12:37:03,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:03,646][root][INFO] - Training Epoch: 1/2, step 9485/23838 completed (loss: 0.23542103171348572, acc: 0.9369369149208069)
[2025-02-16 12:37:03,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:04,051][root][INFO] - Training Epoch: 1/2, step 9486/23838 completed (loss: 0.2530829906463623, acc: 0.9019607901573181)
[2025-02-16 12:37:04,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:04,532][root][INFO] - Training Epoch: 1/2, step 9487/23838 completed (loss: 0.19137880206108093, acc: 0.9492753744125366)
[2025-02-16 12:37:04,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:05,009][root][INFO] - Training Epoch: 1/2, step 9488/23838 completed (loss: 0.36593666672706604, acc: 0.8888888955116272)
[2025-02-16 12:37:05,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:05,418][root][INFO] - Training Epoch: 1/2, step 9489/23838 completed (loss: 0.46255531907081604, acc: 0.8823529481887817)
[2025-02-16 12:37:05,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:05,865][root][INFO] - Training Epoch: 1/2, step 9490/23838 completed (loss: 0.1393810510635376, acc: 0.9770992398262024)
[2025-02-16 12:37:06,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:06,334][root][INFO] - Training Epoch: 1/2, step 9491/23838 completed (loss: 0.24065762758255005, acc: 0.9411764740943909)
[2025-02-16 12:37:06,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:06,805][root][INFO] - Training Epoch: 1/2, step 9492/23838 completed (loss: 0.15299028158187866, acc: 0.96875)
[2025-02-16 12:37:07,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:07,303][root][INFO] - Training Epoch: 1/2, step 9493/23838 completed (loss: 0.34171098470687866, acc: 0.9272727370262146)
[2025-02-16 12:37:07,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:07,757][root][INFO] - Training Epoch: 1/2, step 9494/23838 completed (loss: 0.34358978271484375, acc: 0.8979591727256775)
[2025-02-16 12:37:07,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:08,168][root][INFO] - Training Epoch: 1/2, step 9495/23838 completed (loss: 0.26685383915901184, acc: 0.9319728016853333)
[2025-02-16 12:37:08,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:08,538][root][INFO] - Training Epoch: 1/2, step 9496/23838 completed (loss: 0.29699334502220154, acc: 0.9368420839309692)
[2025-02-16 12:37:08,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:08,916][root][INFO] - Training Epoch: 1/2, step 9497/23838 completed (loss: 0.4923281967639923, acc: 0.8421052694320679)
[2025-02-16 12:37:09,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:09,318][root][INFO] - Training Epoch: 1/2, step 9498/23838 completed (loss: 0.48661789298057556, acc: 0.875)
[2025-02-16 12:37:09,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:09,871][root][INFO] - Training Epoch: 1/2, step 9499/23838 completed (loss: 0.1354892998933792, acc: 0.9561403393745422)
[2025-02-16 12:37:10,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:10,315][root][INFO] - Training Epoch: 1/2, step 9500/23838 completed (loss: 0.2279697209596634, acc: 0.9230769276618958)
[2025-02-16 12:37:10,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:10,744][root][INFO] - Training Epoch: 1/2, step 9501/23838 completed (loss: 0.1558150053024292, acc: 0.9726027250289917)
[2025-02-16 12:37:10,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:11,139][root][INFO] - Training Epoch: 1/2, step 9502/23838 completed (loss: 0.4870193302631378, acc: 0.8571428656578064)
[2025-02-16 12:37:11,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:11,540][root][INFO] - Training Epoch: 1/2, step 9503/23838 completed (loss: 0.3913951516151428, acc: 0.9009901285171509)
[2025-02-16 12:37:11,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:12,013][root][INFO] - Training Epoch: 1/2, step 9504/23838 completed (loss: 0.2993294596672058, acc: 0.9333333373069763)
[2025-02-16 12:37:12,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:12,590][root][INFO] - Training Epoch: 1/2, step 9505/23838 completed (loss: 0.10802566260099411, acc: 0.9816513657569885)
[2025-02-16 12:37:12,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:13,035][root][INFO] - Training Epoch: 1/2, step 9506/23838 completed (loss: 0.18569989502429962, acc: 0.9615384340286255)
[2025-02-16 12:37:13,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:13,421][root][INFO] - Training Epoch: 1/2, step 9507/23838 completed (loss: 0.026638951152563095, acc: 1.0)
[2025-02-16 12:37:13,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:13,872][root][INFO] - Training Epoch: 1/2, step 9508/23838 completed (loss: 0.20937971770763397, acc: 0.9333333373069763)
[2025-02-16 12:37:14,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:14,606][root][INFO] - Training Epoch: 1/2, step 9509/23838 completed (loss: 0.5384361147880554, acc: 0.8791208863258362)
[2025-02-16 12:37:14,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:14,949][root][INFO] - Training Epoch: 1/2, step 9510/23838 completed (loss: 0.22931692004203796, acc: 0.9369369149208069)
[2025-02-16 12:37:15,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:15,643][root][INFO] - Training Epoch: 1/2, step 9511/23838 completed (loss: 0.288470983505249, acc: 0.9181286692619324)
[2025-02-16 12:37:15,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:16,065][root][INFO] - Training Epoch: 1/2, step 9512/23838 completed (loss: 0.15569044649600983, acc: 0.9411764740943909)
[2025-02-16 12:37:16,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:16,459][root][INFO] - Training Epoch: 1/2, step 9513/23838 completed (loss: 0.3674381971359253, acc: 0.8876404762268066)
[2025-02-16 12:37:16,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:16,879][root][INFO] - Training Epoch: 1/2, step 9514/23838 completed (loss: 0.15487036108970642, acc: 0.9569892287254333)
[2025-02-16 12:37:17,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:17,365][root][INFO] - Training Epoch: 1/2, step 9515/23838 completed (loss: 0.35849758982658386, acc: 0.9139072895050049)
[2025-02-16 12:37:17,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:17,818][root][INFO] - Training Epoch: 1/2, step 9516/23838 completed (loss: 0.15425437688827515, acc: 0.9416058659553528)
[2025-02-16 12:37:18,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:18,268][root][INFO] - Training Epoch: 1/2, step 9517/23838 completed (loss: 0.18310494720935822, acc: 0.9604519605636597)
[2025-02-16 12:37:18,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:18,747][root][INFO] - Training Epoch: 1/2, step 9518/23838 completed (loss: 0.23976212739944458, acc: 0.9399999976158142)
[2025-02-16 12:37:19,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:19,236][root][INFO] - Training Epoch: 1/2, step 9519/23838 completed (loss: 0.39305856823921204, acc: 0.9320388436317444)
[2025-02-16 12:37:19,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:19,686][root][INFO] - Training Epoch: 1/2, step 9520/23838 completed (loss: 0.1299125999212265, acc: 0.938144326210022)
[2025-02-16 12:37:19,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:20,055][root][INFO] - Training Epoch: 1/2, step 9521/23838 completed (loss: 0.13242092728614807, acc: 0.9520547986030579)
[2025-02-16 12:37:20,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:20,585][root][INFO] - Training Epoch: 1/2, step 9522/23838 completed (loss: 0.5309073328971863, acc: 0.8999999761581421)
[2025-02-16 12:37:20,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:21,031][root][INFO] - Training Epoch: 1/2, step 9523/23838 completed (loss: 0.36102211475372314, acc: 0.9375)
[2025-02-16 12:37:21,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:21,454][root][INFO] - Training Epoch: 1/2, step 9524/23838 completed (loss: 0.45137616991996765, acc: 0.8777777552604675)
[2025-02-16 12:37:21,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:21,892][root][INFO] - Training Epoch: 1/2, step 9525/23838 completed (loss: 0.5972996354103088, acc: 0.8709677457809448)
[2025-02-16 12:37:22,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:22,298][root][INFO] - Training Epoch: 1/2, step 9526/23838 completed (loss: 0.23446710407733917, acc: 0.9322034120559692)
[2025-02-16 12:37:22,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:22,710][root][INFO] - Training Epoch: 1/2, step 9527/23838 completed (loss: 0.9415938258171082, acc: 0.75)
[2025-02-16 12:37:22,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:23,119][root][INFO] - Training Epoch: 1/2, step 9528/23838 completed (loss: 0.19051361083984375, acc: 0.9529411792755127)
[2025-02-16 12:37:23,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:23,551][root][INFO] - Training Epoch: 1/2, step 9529/23838 completed (loss: 0.4378742277622223, acc: 0.9090909361839294)
[2025-02-16 12:37:23,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:23,957][root][INFO] - Training Epoch: 1/2, step 9530/23838 completed (loss: 0.12177745252847672, acc: 0.984375)
[2025-02-16 12:37:24,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:24,365][root][INFO] - Training Epoch: 1/2, step 9531/23838 completed (loss: 0.468475878238678, acc: 0.8974359035491943)
[2025-02-16 12:37:24,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:24,947][root][INFO] - Training Epoch: 1/2, step 9532/23838 completed (loss: 0.3022308945655823, acc: 0.9321267008781433)
[2025-02-16 12:37:25,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:25,335][root][INFO] - Training Epoch: 1/2, step 9533/23838 completed (loss: 0.19884687662124634, acc: 0.961904764175415)
[2025-02-16 12:37:25,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:25,811][root][INFO] - Training Epoch: 1/2, step 9534/23838 completed (loss: 0.03657745569944382, acc: 0.9879518151283264)
[2025-02-16 12:37:26,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:26,259][root][INFO] - Training Epoch: 1/2, step 9535/23838 completed (loss: 0.20333169400691986, acc: 0.9519230723381042)
[2025-02-16 12:37:26,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:26,738][root][INFO] - Training Epoch: 1/2, step 9536/23838 completed (loss: 0.15198542177677155, acc: 0.9444444179534912)
[2025-02-16 12:37:26,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:27,162][root][INFO] - Training Epoch: 1/2, step 9537/23838 completed (loss: 0.5315736532211304, acc: 0.8571428656578064)
[2025-02-16 12:37:27,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:28,101][root][INFO] - Training Epoch: 1/2, step 9538/23838 completed (loss: 0.3330298960208893, acc: 0.8982036113739014)
[2025-02-16 12:37:28,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:28,591][root][INFO] - Training Epoch: 1/2, step 9539/23838 completed (loss: 0.3158930242061615, acc: 0.8979591727256775)
[2025-02-16 12:37:28,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:29,064][root][INFO] - Training Epoch: 1/2, step 9540/23838 completed (loss: 0.26301297545433044, acc: 0.9396551847457886)
[2025-02-16 12:37:29,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:29,512][root][INFO] - Training Epoch: 1/2, step 9541/23838 completed (loss: 0.4405692219734192, acc: 0.8741258978843689)
[2025-02-16 12:37:29,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:29,945][root][INFO] - Training Epoch: 1/2, step 9542/23838 completed (loss: 0.29397881031036377, acc: 0.9255319237709045)
[2025-02-16 12:37:30,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:30,395][root][INFO] - Training Epoch: 1/2, step 9543/23838 completed (loss: 0.31973886489868164, acc: 0.9370078444480896)
[2025-02-16 12:37:30,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:30,867][root][INFO] - Training Epoch: 1/2, step 9544/23838 completed (loss: 0.30871620774269104, acc: 0.9210526347160339)
[2025-02-16 12:37:31,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:31,338][root][INFO] - Training Epoch: 1/2, step 9545/23838 completed (loss: 0.23777121305465698, acc: 0.9285714030265808)
[2025-02-16 12:37:31,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:31,789][root][INFO] - Training Epoch: 1/2, step 9546/23838 completed (loss: 0.21224656701087952, acc: 0.9324324131011963)
[2025-02-16 12:37:32,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:32,340][root][INFO] - Training Epoch: 1/2, step 9547/23838 completed (loss: 0.38779422640800476, acc: 0.9090909361839294)
[2025-02-16 12:37:32,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:32,724][root][INFO] - Training Epoch: 1/2, step 9548/23838 completed (loss: 0.20298869907855988, acc: 0.9435483813285828)
[2025-02-16 12:37:32,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:33,130][root][INFO] - Training Epoch: 1/2, step 9549/23838 completed (loss: 0.20354583859443665, acc: 0.9333333373069763)
[2025-02-16 12:37:33,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:33,604][root][INFO] - Training Epoch: 1/2, step 9550/23838 completed (loss: 0.2519446015357971, acc: 0.9328358173370361)
[2025-02-16 12:37:33,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:34,217][root][INFO] - Training Epoch: 1/2, step 9551/23838 completed (loss: 0.27924516797065735, acc: 0.9133333563804626)
[2025-02-16 12:37:34,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:34,694][root][INFO] - Training Epoch: 1/2, step 9552/23838 completed (loss: 0.15937528014183044, acc: 0.9482758641242981)
[2025-02-16 12:37:34,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:35,151][root][INFO] - Training Epoch: 1/2, step 9553/23838 completed (loss: 0.2243976891040802, acc: 0.9379844665527344)
[2025-02-16 12:37:35,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:35,842][root][INFO] - Training Epoch: 1/2, step 9554/23838 completed (loss: 0.20728306472301483, acc: 0.9555555582046509)
[2025-02-16 12:37:36,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:36,261][root][INFO] - Training Epoch: 1/2, step 9555/23838 completed (loss: 0.4757719337940216, acc: 0.8860759735107422)
[2025-02-16 12:37:36,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:36,697][root][INFO] - Training Epoch: 1/2, step 9556/23838 completed (loss: 0.8730100393295288, acc: 0.8139534592628479)
[2025-02-16 12:37:36,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:37,174][root][INFO] - Training Epoch: 1/2, step 9557/23838 completed (loss: 0.34284520149230957, acc: 0.8787878751754761)
[2025-02-16 12:37:37,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:37,610][root][INFO] - Training Epoch: 1/2, step 9558/23838 completed (loss: 0.6509538292884827, acc: 0.859375)
[2025-02-16 12:37:37,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:38,039][root][INFO] - Training Epoch: 1/2, step 9559/23838 completed (loss: 0.3898446559906006, acc: 0.8941176533699036)
[2025-02-16 12:37:38,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:38,508][root][INFO] - Training Epoch: 1/2, step 9560/23838 completed (loss: 0.5172880291938782, acc: 0.8387096524238586)
[2025-02-16 12:37:38,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:39,202][root][INFO] - Training Epoch: 1/2, step 9561/23838 completed (loss: 0.39809972047805786, acc: 0.9026548862457275)
[2025-02-16 12:37:39,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:39,581][root][INFO] - Training Epoch: 1/2, step 9562/23838 completed (loss: 0.26295676827430725, acc: 0.918367326259613)
[2025-02-16 12:37:39,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:40,020][root][INFO] - Training Epoch: 1/2, step 9563/23838 completed (loss: 0.6529691219329834, acc: 0.795918345451355)
[2025-02-16 12:37:40,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:40,502][root][INFO] - Training Epoch: 1/2, step 9564/23838 completed (loss: 0.7209778428077698, acc: 0.800000011920929)
[2025-02-16 12:37:40,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:41,132][root][INFO] - Training Epoch: 1/2, step 9565/23838 completed (loss: 0.344843327999115, acc: 0.9222221970558167)
[2025-02-16 12:37:41,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:41,578][root][INFO] - Training Epoch: 1/2, step 9566/23838 completed (loss: 0.6751875877380371, acc: 0.8103448152542114)
[2025-02-16 12:37:41,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:42,049][root][INFO] - Training Epoch: 1/2, step 9567/23838 completed (loss: 0.27466994524002075, acc: 0.9038461446762085)
[2025-02-16 12:37:42,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:42,521][root][INFO] - Training Epoch: 1/2, step 9568/23838 completed (loss: 0.29954856634140015, acc: 0.9285714030265808)
[2025-02-16 12:37:42,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:43,046][root][INFO] - Training Epoch: 1/2, step 9569/23838 completed (loss: 0.21924807131290436, acc: 0.9237288236618042)
[2025-02-16 12:37:43,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:43,504][root][INFO] - Training Epoch: 1/2, step 9570/23838 completed (loss: 0.2780304253101349, acc: 0.9203540086746216)
[2025-02-16 12:37:43,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:44,010][root][INFO] - Training Epoch: 1/2, step 9571/23838 completed (loss: 0.7056536674499512, acc: 0.8309859037399292)
[2025-02-16 12:37:44,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:44,426][root][INFO] - Training Epoch: 1/2, step 9572/23838 completed (loss: 0.16361281275749207, acc: 0.9509803652763367)
[2025-02-16 12:37:44,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:45,004][root][INFO] - Training Epoch: 1/2, step 9573/23838 completed (loss: 0.53719562292099, acc: 0.8630136847496033)
[2025-02-16 12:37:45,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:45,462][root][INFO] - Training Epoch: 1/2, step 9574/23838 completed (loss: 0.5653488636016846, acc: 0.8292682766914368)
[2025-02-16 12:37:45,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:45,989][root][INFO] - Training Epoch: 1/2, step 9575/23838 completed (loss: 0.46488216519355774, acc: 0.8591549396514893)
[2025-02-16 12:37:46,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:46,377][root][INFO] - Training Epoch: 1/2, step 9576/23838 completed (loss: 0.1864292323589325, acc: 0.9629629850387573)
[2025-02-16 12:37:46,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:46,794][root][INFO] - Training Epoch: 1/2, step 9577/23838 completed (loss: 0.1366768628358841, acc: 0.9599999785423279)
[2025-02-16 12:37:47,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:47,287][root][INFO] - Training Epoch: 1/2, step 9578/23838 completed (loss: 0.24521133303642273, acc: 0.920634925365448)
[2025-02-16 12:37:47,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:47,721][root][INFO] - Training Epoch: 1/2, step 9579/23838 completed (loss: 0.34829017519950867, acc: 0.890625)
[2025-02-16 12:37:48,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:48,349][root][INFO] - Training Epoch: 1/2, step 9580/23838 completed (loss: 0.4458869695663452, acc: 0.8407079577445984)
[2025-02-16 12:37:48,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:49,025][root][INFO] - Training Epoch: 1/2, step 9581/23838 completed (loss: 0.5565884113311768, acc: 0.875)
[2025-02-16 12:37:49,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:49,505][root][INFO] - Training Epoch: 1/2, step 9582/23838 completed (loss: 0.24130390584468842, acc: 0.9561403393745422)
[2025-02-16 12:37:49,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:50,038][root][INFO] - Training Epoch: 1/2, step 9583/23838 completed (loss: 0.24467629194259644, acc: 0.9181286692619324)
[2025-02-16 12:37:50,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:50,509][root][INFO] - Training Epoch: 1/2, step 9584/23838 completed (loss: 0.22853900492191315, acc: 0.9257425665855408)
[2025-02-16 12:37:50,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:50,899][root][INFO] - Training Epoch: 1/2, step 9585/23838 completed (loss: 0.390970379114151, acc: 0.8936170339584351)
[2025-02-16 12:37:51,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:51,290][root][INFO] - Training Epoch: 1/2, step 9586/23838 completed (loss: 0.2100493609905243, acc: 0.9428571462631226)
[2025-02-16 12:37:51,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:51,828][root][INFO] - Training Epoch: 1/2, step 9587/23838 completed (loss: 0.2516297698020935, acc: 0.9316239356994629)
[2025-02-16 12:37:51,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:52,241][root][INFO] - Training Epoch: 1/2, step 9588/23838 completed (loss: 0.3914710283279419, acc: 0.8736842274665833)
[2025-02-16 12:37:52,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:52,771][root][INFO] - Training Epoch: 1/2, step 9589/23838 completed (loss: 0.3162117600440979, acc: 0.8938053250312805)
[2025-02-16 12:37:53,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:53,238][root][INFO] - Training Epoch: 1/2, step 9590/23838 completed (loss: 0.3001652657985687, acc: 0.9154929518699646)
[2025-02-16 12:37:53,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:53,655][root][INFO] - Training Epoch: 1/2, step 9591/23838 completed (loss: 0.4170663058757782, acc: 0.8961039185523987)
[2025-02-16 12:37:53,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:54,215][root][INFO] - Training Epoch: 1/2, step 9592/23838 completed (loss: 0.5706161260604858, acc: 0.8608247637748718)
[2025-02-16 12:37:54,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:54,596][root][INFO] - Training Epoch: 1/2, step 9593/23838 completed (loss: 0.24877867102622986, acc: 0.9268292784690857)
[2025-02-16 12:37:54,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:55,108][root][INFO] - Training Epoch: 1/2, step 9594/23838 completed (loss: 0.724589467048645, acc: 0.7654321193695068)
[2025-02-16 12:37:55,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:55,603][root][INFO] - Training Epoch: 1/2, step 9595/23838 completed (loss: 0.4406699538230896, acc: 0.8773584961891174)
[2025-02-16 12:37:55,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:56,169][root][INFO] - Training Epoch: 1/2, step 9596/23838 completed (loss: 0.6173835396766663, acc: 0.8547008633613586)
[2025-02-16 12:37:56,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:56,552][root][INFO] - Training Epoch: 1/2, step 9597/23838 completed (loss: 0.13475260138511658, acc: 0.949999988079071)
[2025-02-16 12:37:56,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:56,948][root][INFO] - Training Epoch: 1/2, step 9598/23838 completed (loss: 0.5870311260223389, acc: 0.8453608155250549)
[2025-02-16 12:37:57,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:57,526][root][INFO] - Training Epoch: 1/2, step 9599/23838 completed (loss: 0.3259546458721161, acc: 0.9294871687889099)
[2025-02-16 12:37:57,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:58,022][root][INFO] - Training Epoch: 1/2, step 9600/23838 completed (loss: 0.17577415704727173, acc: 0.9444444179534912)
[2025-02-16 12:37:58,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:58,482][root][INFO] - Training Epoch: 1/2, step 9601/23838 completed (loss: 0.3487369120121002, acc: 0.9354838728904724)
[2025-02-16 12:37:58,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:58,924][root][INFO] - Training Epoch: 1/2, step 9602/23838 completed (loss: 0.4071189761161804, acc: 0.875)
[2025-02-16 12:37:59,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:59,400][root][INFO] - Training Epoch: 1/2, step 9603/23838 completed (loss: 0.5213219523429871, acc: 0.8783783912658691)
[2025-02-16 12:37:59,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:37:59,822][root][INFO] - Training Epoch: 1/2, step 9604/23838 completed (loss: 0.38622456789016724, acc: 0.9111111164093018)
[2025-02-16 12:38:00,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:00,239][root][INFO] - Training Epoch: 1/2, step 9605/23838 completed (loss: 0.5188519358634949, acc: 0.8352941274642944)
[2025-02-16 12:38:00,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:00,716][root][INFO] - Training Epoch: 1/2, step 9606/23838 completed (loss: 0.1619088500738144, acc: 0.9655172228813171)
[2025-02-16 12:38:00,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:01,186][root][INFO] - Training Epoch: 1/2, step 9607/23838 completed (loss: 0.45171236991882324, acc: 0.8771929740905762)
[2025-02-16 12:38:01,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:01,669][root][INFO] - Training Epoch: 1/2, step 9608/23838 completed (loss: 0.08484354615211487, acc: 0.9793103337287903)
[2025-02-16 12:38:01,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:02,105][root][INFO] - Training Epoch: 1/2, step 9609/23838 completed (loss: 0.04595225304365158, acc: 1.0)
[2025-02-16 12:38:02,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:02,459][root][INFO] - Training Epoch: 1/2, step 9610/23838 completed (loss: 0.2936515212059021, acc: 0.9375)
[2025-02-16 12:38:02,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:02,934][root][INFO] - Training Epoch: 1/2, step 9611/23838 completed (loss: 0.6564311981201172, acc: 0.7910447716712952)
[2025-02-16 12:38:03,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:03,510][root][INFO] - Training Epoch: 1/2, step 9612/23838 completed (loss: 0.3489212989807129, acc: 0.9160839319229126)
[2025-02-16 12:38:03,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:03,929][root][INFO] - Training Epoch: 1/2, step 9613/23838 completed (loss: 0.12776833772659302, acc: 0.9677419066429138)
[2025-02-16 12:38:04,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:04,302][root][INFO] - Training Epoch: 1/2, step 9614/23838 completed (loss: 0.28421440720558167, acc: 0.9270833134651184)
[2025-02-16 12:38:04,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:04,701][root][INFO] - Training Epoch: 1/2, step 9615/23838 completed (loss: 0.2820727229118347, acc: 0.9268292784690857)
[2025-02-16 12:38:04,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:05,166][root][INFO] - Training Epoch: 1/2, step 9616/23838 completed (loss: 0.4092446267604828, acc: 0.8588235378265381)
[2025-02-16 12:38:05,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:05,572][root][INFO] - Training Epoch: 1/2, step 9617/23838 completed (loss: 0.165894016623497, acc: 0.9622641801834106)
[2025-02-16 12:38:05,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:06,001][root][INFO] - Training Epoch: 1/2, step 9618/23838 completed (loss: 0.5977877378463745, acc: 0.8518518805503845)
[2025-02-16 12:38:06,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:06,452][root][INFO] - Training Epoch: 1/2, step 9619/23838 completed (loss: 0.2050834447145462, acc: 0.9636363387107849)
[2025-02-16 12:38:06,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:06,926][root][INFO] - Training Epoch: 1/2, step 9620/23838 completed (loss: 0.8801520466804504, acc: 0.8314606547355652)
[2025-02-16 12:38:07,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:07,355][root][INFO] - Training Epoch: 1/2, step 9621/23838 completed (loss: 0.5703763365745544, acc: 0.8500000238418579)
[2025-02-16 12:38:07,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:07,847][root][INFO] - Training Epoch: 1/2, step 9622/23838 completed (loss: 0.6023443341255188, acc: 0.8644067645072937)
[2025-02-16 12:38:08,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:08,262][root][INFO] - Training Epoch: 1/2, step 9623/23838 completed (loss: 1.0043585300445557, acc: 0.75)
[2025-02-16 12:38:08,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:08,634][root][INFO] - Training Epoch: 1/2, step 9624/23838 completed (loss: 1.145320177078247, acc: 0.701298713684082)
[2025-02-16 12:38:08,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:09,064][root][INFO] - Training Epoch: 1/2, step 9625/23838 completed (loss: 0.9719843864440918, acc: 0.7604166865348816)
[2025-02-16 12:38:09,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:09,507][root][INFO] - Training Epoch: 1/2, step 9626/23838 completed (loss: 0.4482385516166687, acc: 0.9090909361839294)
[2025-02-16 12:38:09,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:09,974][root][INFO] - Training Epoch: 1/2, step 9627/23838 completed (loss: 1.1609265804290771, acc: 0.738095223903656)
[2025-02-16 12:38:10,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:10,443][root][INFO] - Training Epoch: 1/2, step 9628/23838 completed (loss: 1.0001198053359985, acc: 0.7435897588729858)
[2025-02-16 12:38:10,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:10,894][root][INFO] - Training Epoch: 1/2, step 9629/23838 completed (loss: 0.4491550028324127, acc: 0.920634925365448)
[2025-02-16 12:38:11,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:11,320][root][INFO] - Training Epoch: 1/2, step 9630/23838 completed (loss: 1.1305474042892456, acc: 0.6666666865348816)
[2025-02-16 12:38:11,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:11,738][root][INFO] - Training Epoch: 1/2, step 9631/23838 completed (loss: 1.0281184911727905, acc: 0.7394366264343262)
[2025-02-16 12:38:11,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:12,163][root][INFO] - Training Epoch: 1/2, step 9632/23838 completed (loss: 0.7081367373466492, acc: 0.7882353067398071)
[2025-02-16 12:38:12,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:12,587][root][INFO] - Training Epoch: 1/2, step 9633/23838 completed (loss: 1.1772847175598145, acc: 0.6666666865348816)
[2025-02-16 12:38:12,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:13,011][root][INFO] - Training Epoch: 1/2, step 9634/23838 completed (loss: 0.8150309324264526, acc: 0.7751938104629517)
[2025-02-16 12:38:13,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:13,418][root][INFO] - Training Epoch: 1/2, step 9635/23838 completed (loss: 0.751652717590332, acc: 0.8068181872367859)
[2025-02-16 12:38:13,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:13,875][root][INFO] - Training Epoch: 1/2, step 9636/23838 completed (loss: 0.6156759858131409, acc: 0.8295454382896423)
[2025-02-16 12:38:14,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:14,322][root][INFO] - Training Epoch: 1/2, step 9637/23838 completed (loss: 0.27546313405036926, acc: 0.9154929518699646)
[2025-02-16 12:38:14,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:14,772][root][INFO] - Training Epoch: 1/2, step 9638/23838 completed (loss: 0.46767154335975647, acc: 0.8771929740905762)
[2025-02-16 12:38:14,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:15,214][root][INFO] - Training Epoch: 1/2, step 9639/23838 completed (loss: 0.5555834174156189, acc: 0.7931034564971924)
[2025-02-16 12:38:15,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:15,699][root][INFO] - Training Epoch: 1/2, step 9640/23838 completed (loss: 0.9984728693962097, acc: 0.6824324131011963)
[2025-02-16 12:38:15,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:16,140][root][INFO] - Training Epoch: 1/2, step 9641/23838 completed (loss: 0.8529130220413208, acc: 0.7719298005104065)
[2025-02-16 12:38:16,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:16,607][root][INFO] - Training Epoch: 1/2, step 9642/23838 completed (loss: 0.4589257836341858, acc: 0.918367326259613)
[2025-02-16 12:38:16,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:17,075][root][INFO] - Training Epoch: 1/2, step 9643/23838 completed (loss: 1.0561814308166504, acc: 0.6363636255264282)
[2025-02-16 12:38:17,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:17,515][root][INFO] - Training Epoch: 1/2, step 9644/23838 completed (loss: 1.0417425632476807, acc: 0.7333333492279053)
[2025-02-16 12:38:17,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:17,930][root][INFO] - Training Epoch: 1/2, step 9645/23838 completed (loss: 0.42975929379463196, acc: 0.9100000262260437)
[2025-02-16 12:38:18,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:18,356][root][INFO] - Training Epoch: 1/2, step 9646/23838 completed (loss: 0.3452366292476654, acc: 0.9268292784690857)
[2025-02-16 12:38:18,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:18,839][root][INFO] - Training Epoch: 1/2, step 9647/23838 completed (loss: 0.48773422837257385, acc: 0.8679245114326477)
[2025-02-16 12:38:19,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:19,230][root][INFO] - Training Epoch: 1/2, step 9648/23838 completed (loss: 0.9896358847618103, acc: 0.7441860437393188)
[2025-02-16 12:38:19,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:19,711][root][INFO] - Training Epoch: 1/2, step 9649/23838 completed (loss: 0.30801618099212646, acc: 0.895652174949646)
[2025-02-16 12:38:19,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:20,152][root][INFO] - Training Epoch: 1/2, step 9650/23838 completed (loss: 0.6659008264541626, acc: 0.8206896781921387)
[2025-02-16 12:38:20,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:20,526][root][INFO] - Training Epoch: 1/2, step 9651/23838 completed (loss: 0.8702089786529541, acc: 0.779411792755127)
[2025-02-16 12:38:20,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:20,916][root][INFO] - Training Epoch: 1/2, step 9652/23838 completed (loss: 0.43926942348480225, acc: 0.8674699068069458)
[2025-02-16 12:38:21,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:21,335][root][INFO] - Training Epoch: 1/2, step 9653/23838 completed (loss: 0.9725855588912964, acc: 0.75)
[2025-02-16 12:38:21,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:21,712][root][INFO] - Training Epoch: 1/2, step 9654/23838 completed (loss: 1.048126220703125, acc: 0.7362637519836426)
[2025-02-16 12:38:21,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:22,127][root][INFO] - Training Epoch: 1/2, step 9655/23838 completed (loss: 0.20821000635623932, acc: 0.9365079402923584)
[2025-02-16 12:38:22,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:22,591][root][INFO] - Training Epoch: 1/2, step 9656/23838 completed (loss: 0.39365217089653015, acc: 0.90625)
[2025-02-16 12:38:22,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:23,006][root][INFO] - Training Epoch: 1/2, step 9657/23838 completed (loss: 0.4828365445137024, acc: 0.8653846383094788)
[2025-02-16 12:38:23,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:23,439][root][INFO] - Training Epoch: 1/2, step 9658/23838 completed (loss: 0.6329958438873291, acc: 0.8214285969734192)
[2025-02-16 12:38:23,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:23,865][root][INFO] - Training Epoch: 1/2, step 9659/23838 completed (loss: 0.6653572916984558, acc: 0.8030303120613098)
[2025-02-16 12:38:24,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:24,333][root][INFO] - Training Epoch: 1/2, step 9660/23838 completed (loss: 0.5417718291282654, acc: 0.8782608509063721)
[2025-02-16 12:38:24,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:24,819][root][INFO] - Training Epoch: 1/2, step 9661/23838 completed (loss: 0.7827327251434326, acc: 0.7727272510528564)
[2025-02-16 12:38:24,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:25,197][root][INFO] - Training Epoch: 1/2, step 9662/23838 completed (loss: 0.5967459678649902, acc: 0.8552631735801697)
[2025-02-16 12:38:25,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:25,640][root][INFO] - Training Epoch: 1/2, step 9663/23838 completed (loss: 0.9308885335922241, acc: 0.7605633735656738)
[2025-02-16 12:38:25,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:26,119][root][INFO] - Training Epoch: 1/2, step 9664/23838 completed (loss: 0.5575830340385437, acc: 0.8333333134651184)
[2025-02-16 12:38:26,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:26,525][root][INFO] - Training Epoch: 1/2, step 9665/23838 completed (loss: 0.6919991970062256, acc: 0.8333333134651184)
[2025-02-16 12:38:26,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:26,961][root][INFO] - Training Epoch: 1/2, step 9666/23838 completed (loss: 0.6741256713867188, acc: 0.837837815284729)
[2025-02-16 12:38:27,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:27,441][root][INFO] - Training Epoch: 1/2, step 9667/23838 completed (loss: 0.41267669200897217, acc: 0.8909090757369995)
[2025-02-16 12:38:27,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:27,885][root][INFO] - Training Epoch: 1/2, step 9668/23838 completed (loss: 0.8169558644294739, acc: 0.7882353067398071)
[2025-02-16 12:38:28,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:28,303][root][INFO] - Training Epoch: 1/2, step 9669/23838 completed (loss: 0.46485206484794617, acc: 0.8644067645072937)
[2025-02-16 12:38:28,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:28,727][root][INFO] - Training Epoch: 1/2, step 9670/23838 completed (loss: 0.7522590160369873, acc: 0.7882353067398071)
[2025-02-16 12:38:28,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:29,184][root][INFO] - Training Epoch: 1/2, step 9671/23838 completed (loss: 0.4732232391834259, acc: 0.8513513803482056)
[2025-02-16 12:38:29,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:29,609][root][INFO] - Training Epoch: 1/2, step 9672/23838 completed (loss: 0.6731207966804504, acc: 0.800000011920929)
[2025-02-16 12:38:29,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:30,040][root][INFO] - Training Epoch: 1/2, step 9673/23838 completed (loss: 0.9148533344268799, acc: 0.7413793206214905)
[2025-02-16 12:38:30,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:30,415][root][INFO] - Training Epoch: 1/2, step 9674/23838 completed (loss: 0.6244447231292725, acc: 0.8352941274642944)
[2025-02-16 12:38:30,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:30,971][root][INFO] - Training Epoch: 1/2, step 9675/23838 completed (loss: 0.8675405979156494, acc: 0.7479674816131592)
[2025-02-16 12:38:31,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:31,353][root][INFO] - Training Epoch: 1/2, step 9676/23838 completed (loss: 0.44188880920410156, acc: 0.8600000143051147)
[2025-02-16 12:38:31,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:31,831][root][INFO] - Training Epoch: 1/2, step 9677/23838 completed (loss: 0.41969794034957886, acc: 0.8829787373542786)
[2025-02-16 12:38:32,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:32,248][root][INFO] - Training Epoch: 1/2, step 9678/23838 completed (loss: 0.5523101687431335, acc: 0.8604651093482971)
[2025-02-16 12:38:32,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:32,705][root][INFO] - Training Epoch: 1/2, step 9679/23838 completed (loss: 0.507319986820221, acc: 0.841269850730896)
[2025-02-16 12:38:32,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:33,150][root][INFO] - Training Epoch: 1/2, step 9680/23838 completed (loss: 0.5024776458740234, acc: 0.8433734774589539)
[2025-02-16 12:38:33,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:33,612][root][INFO] - Training Epoch: 1/2, step 9681/23838 completed (loss: 0.7985902428627014, acc: 0.7761194109916687)
[2025-02-16 12:38:33,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:34,065][root][INFO] - Training Epoch: 1/2, step 9682/23838 completed (loss: 0.5612949132919312, acc: 0.8225806355476379)
[2025-02-16 12:38:34,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:34,501][root][INFO] - Training Epoch: 1/2, step 9683/23838 completed (loss: 0.6704499125480652, acc: 0.7777777910232544)
[2025-02-16 12:38:34,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:34,898][root][INFO] - Training Epoch: 1/2, step 9684/23838 completed (loss: 1.5021991729736328, acc: 0.5714285969734192)
[2025-02-16 12:38:35,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:35,378][root][INFO] - Training Epoch: 1/2, step 9685/23838 completed (loss: 1.1779552698135376, acc: 0.7346938848495483)
[2025-02-16 12:38:35,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:35,811][root][INFO] - Training Epoch: 1/2, step 9686/23838 completed (loss: 1.0816117525100708, acc: 0.7592592835426331)
[2025-02-16 12:38:35,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:36,209][root][INFO] - Training Epoch: 1/2, step 9687/23838 completed (loss: 0.9983822703361511, acc: 0.7594936490058899)
[2025-02-16 12:38:36,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:36,633][root][INFO] - Training Epoch: 1/2, step 9688/23838 completed (loss: 0.9761952757835388, acc: 0.7475728392601013)
[2025-02-16 12:38:36,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:37,111][root][INFO] - Training Epoch: 1/2, step 9689/23838 completed (loss: 0.40391865372657776, acc: 0.8795180916786194)
[2025-02-16 12:38:37,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:37,565][root][INFO] - Training Epoch: 1/2, step 9690/23838 completed (loss: 0.7815788984298706, acc: 0.774193525314331)
[2025-02-16 12:38:37,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:37,950][root][INFO] - Training Epoch: 1/2, step 9691/23838 completed (loss: 0.7759273648262024, acc: 0.8333333134651184)
[2025-02-16 12:38:38,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:38,421][root][INFO] - Training Epoch: 1/2, step 9692/23838 completed (loss: 0.6246261596679688, acc: 0.8068181872367859)
[2025-02-16 12:38:38,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:38,836][root][INFO] - Training Epoch: 1/2, step 9693/23838 completed (loss: 0.4418037533760071, acc: 0.8987341523170471)
[2025-02-16 12:38:39,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:39,284][root][INFO] - Training Epoch: 1/2, step 9694/23838 completed (loss: 0.2571030259132385, acc: 0.931506872177124)
[2025-02-16 12:38:39,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:39,720][root][INFO] - Training Epoch: 1/2, step 9695/23838 completed (loss: 0.6309981942176819, acc: 0.7708333134651184)
[2025-02-16 12:38:39,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:40,233][root][INFO] - Training Epoch: 1/2, step 9696/23838 completed (loss: 0.7883774638175964, acc: 0.8181818127632141)
[2025-02-16 12:38:40,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:40,723][root][INFO] - Training Epoch: 1/2, step 9697/23838 completed (loss: 0.18314886093139648, acc: 0.9646017551422119)
[2025-02-16 12:38:40,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:41,113][root][INFO] - Training Epoch: 1/2, step 9698/23838 completed (loss: 0.6566419005393982, acc: 0.8080000281333923)
[2025-02-16 12:38:41,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:42,129][root][INFO] - Training Epoch: 1/2, step 9699/23838 completed (loss: 0.9286820292472839, acc: 0.773755669593811)
[2025-02-16 12:38:42,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:42,618][root][INFO] - Training Epoch: 1/2, step 9700/23838 completed (loss: 0.5062171220779419, acc: 0.85628741979599)
[2025-02-16 12:38:42,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:43,073][root][INFO] - Training Epoch: 1/2, step 9701/23838 completed (loss: 0.6617493629455566, acc: 0.8159999847412109)
[2025-02-16 12:38:43,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:43,513][root][INFO] - Training Epoch: 1/2, step 9702/23838 completed (loss: 0.5089985132217407, acc: 0.8888888955116272)
[2025-02-16 12:38:43,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:43,941][root][INFO] - Training Epoch: 1/2, step 9703/23838 completed (loss: 1.1288236379623413, acc: 0.7051281929016113)
[2025-02-16 12:38:44,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:44,393][root][INFO] - Training Epoch: 1/2, step 9704/23838 completed (loss: 0.7829681038856506, acc: 0.8037382960319519)
[2025-02-16 12:38:44,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:44,882][root][INFO] - Training Epoch: 1/2, step 9705/23838 completed (loss: 0.3442888557910919, acc: 0.8909090757369995)
[2025-02-16 12:38:45,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:45,402][root][INFO] - Training Epoch: 1/2, step 9706/23838 completed (loss: 0.48080265522003174, acc: 0.8724831938743591)
[2025-02-16 12:38:45,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:45,852][root][INFO] - Training Epoch: 1/2, step 9707/23838 completed (loss: 0.7501989006996155, acc: 0.7864077687263489)
[2025-02-16 12:38:46,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:46,322][root][INFO] - Training Epoch: 1/2, step 9708/23838 completed (loss: 0.7975387573242188, acc: 0.7560975551605225)
[2025-02-16 12:38:46,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:46,701][root][INFO] - Training Epoch: 1/2, step 9709/23838 completed (loss: 0.7015345096588135, acc: 0.800000011920929)
[2025-02-16 12:38:46,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:47,152][root][INFO] - Training Epoch: 1/2, step 9710/23838 completed (loss: 0.5457867383956909, acc: 0.8780487775802612)
[2025-02-16 12:38:47,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:47,560][root][INFO] - Training Epoch: 1/2, step 9711/23838 completed (loss: 0.28295913338661194, acc: 0.9285714030265808)
[2025-02-16 12:38:47,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:47,999][root][INFO] - Training Epoch: 1/2, step 9712/23838 completed (loss: 0.6131318211555481, acc: 0.8389830589294434)
[2025-02-16 12:38:48,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:48,424][root][INFO] - Training Epoch: 1/2, step 9713/23838 completed (loss: 0.7287331223487854, acc: 0.8571428656578064)
[2025-02-16 12:38:48,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:48,776][root][INFO] - Training Epoch: 1/2, step 9714/23838 completed (loss: 1.0221574306488037, acc: 0.7647058963775635)
[2025-02-16 12:38:48,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:49,141][root][INFO] - Training Epoch: 1/2, step 9715/23838 completed (loss: 1.161232352256775, acc: 0.7076923251152039)
[2025-02-16 12:38:49,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:49,610][root][INFO] - Training Epoch: 1/2, step 9716/23838 completed (loss: 0.4679450988769531, acc: 0.8870967626571655)
[2025-02-16 12:38:49,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:50,020][root][INFO] - Training Epoch: 1/2, step 9717/23838 completed (loss: 0.378890722990036, acc: 0.8888888955116272)
[2025-02-16 12:38:50,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:50,529][root][INFO] - Training Epoch: 1/2, step 9718/23838 completed (loss: 0.6575946807861328, acc: 0.8098591566085815)
[2025-02-16 12:38:50,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:50,956][root][INFO] - Training Epoch: 1/2, step 9719/23838 completed (loss: 0.5241910815238953, acc: 0.8389830589294434)
[2025-02-16 12:38:51,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:51,432][root][INFO] - Training Epoch: 1/2, step 9720/23838 completed (loss: 0.4603690803050995, acc: 0.8496240377426147)
[2025-02-16 12:38:51,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:51,928][root][INFO] - Training Epoch: 1/2, step 9721/23838 completed (loss: 0.5694553852081299, acc: 0.8611111044883728)
[2025-02-16 12:38:52,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:52,394][root][INFO] - Training Epoch: 1/2, step 9722/23838 completed (loss: 0.3548613488674164, acc: 0.8904109597206116)
[2025-02-16 12:38:52,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:52,814][root][INFO] - Training Epoch: 1/2, step 9723/23838 completed (loss: 0.6758258938789368, acc: 0.8333333134651184)
[2025-02-16 12:38:53,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:53,243][root][INFO] - Training Epoch: 1/2, step 9724/23838 completed (loss: 0.2509812116622925, acc: 0.8846153616905212)
[2025-02-16 12:38:53,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:53,751][root][INFO] - Training Epoch: 1/2, step 9725/23838 completed (loss: 0.5082809925079346, acc: 0.869918704032898)
[2025-02-16 12:38:53,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:54,224][root][INFO] - Training Epoch: 1/2, step 9726/23838 completed (loss: 1.0653537511825562, acc: 0.7441860437393188)
[2025-02-16 12:38:54,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:54,718][root][INFO] - Training Epoch: 1/2, step 9727/23838 completed (loss: 0.915644645690918, acc: 0.7358490824699402)
[2025-02-16 12:38:54,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:55,175][root][INFO] - Training Epoch: 1/2, step 9728/23838 completed (loss: 0.7441682815551758, acc: 0.8333333134651184)
[2025-02-16 12:38:55,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:55,668][root][INFO] - Training Epoch: 1/2, step 9729/23838 completed (loss: 0.4331730902194977, acc: 0.9215686321258545)
[2025-02-16 12:38:55,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:56,107][root][INFO] - Training Epoch: 1/2, step 9730/23838 completed (loss: 0.39888995885849, acc: 0.8736842274665833)
[2025-02-16 12:38:56,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:56,479][root][INFO] - Training Epoch: 1/2, step 9731/23838 completed (loss: 0.3225141167640686, acc: 0.8901098966598511)
[2025-02-16 12:38:56,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:56,918][root][INFO] - Training Epoch: 1/2, step 9732/23838 completed (loss: 0.6488339304924011, acc: 0.8285714387893677)
[2025-02-16 12:38:57,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:57,388][root][INFO] - Training Epoch: 1/2, step 9733/23838 completed (loss: 0.389053076505661, acc: 0.9024389982223511)
[2025-02-16 12:38:57,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:57,807][root][INFO] - Training Epoch: 1/2, step 9734/23838 completed (loss: 0.41143661737442017, acc: 0.8735632300376892)
[2025-02-16 12:38:58,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:58,275][root][INFO] - Training Epoch: 1/2, step 9735/23838 completed (loss: 0.6215267181396484, acc: 0.8181818127632141)
[2025-02-16 12:38:58,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:58,734][root][INFO] - Training Epoch: 1/2, step 9736/23838 completed (loss: 1.4545890092849731, acc: 0.5975610017776489)
[2025-02-16 12:38:58,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:59,186][root][INFO] - Training Epoch: 1/2, step 9737/23838 completed (loss: 0.6666855216026306, acc: 0.8373983502388)
[2025-02-16 12:38:59,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:38:59,607][root][INFO] - Training Epoch: 1/2, step 9738/23838 completed (loss: 0.5482620596885681, acc: 0.8382353186607361)
[2025-02-16 12:38:59,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:00,043][root][INFO] - Training Epoch: 1/2, step 9739/23838 completed (loss: 0.6370252966880798, acc: 0.8028169274330139)
[2025-02-16 12:39:00,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:00,472][root][INFO] - Training Epoch: 1/2, step 9740/23838 completed (loss: 0.5036335587501526, acc: 0.835616409778595)
[2025-02-16 12:39:00,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:00,904][root][INFO] - Training Epoch: 1/2, step 9741/23838 completed (loss: 0.8081912994384766, acc: 0.8275862336158752)
[2025-02-16 12:39:01,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:01,350][root][INFO] - Training Epoch: 1/2, step 9742/23838 completed (loss: 0.6273552179336548, acc: 0.8059701323509216)
[2025-02-16 12:39:01,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:01,846][root][INFO] - Training Epoch: 1/2, step 9743/23838 completed (loss: 0.6456506252288818, acc: 0.8315789699554443)
[2025-02-16 12:39:02,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:02,300][root][INFO] - Training Epoch: 1/2, step 9744/23838 completed (loss: 0.9481889009475708, acc: 0.6984127163887024)
[2025-02-16 12:39:02,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:02,750][root][INFO] - Training Epoch: 1/2, step 9745/23838 completed (loss: 0.36353370547294617, acc: 0.8846153616905212)
[2025-02-16 12:39:02,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:03,128][root][INFO] - Training Epoch: 1/2, step 9746/23838 completed (loss: 0.3566363751888275, acc: 0.8846153616905212)
[2025-02-16 12:39:03,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:03,558][root][INFO] - Training Epoch: 1/2, step 9747/23838 completed (loss: 0.5929591059684753, acc: 0.8571428656578064)
[2025-02-16 12:39:03,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:03,910][root][INFO] - Training Epoch: 1/2, step 9748/23838 completed (loss: 0.49142324924468994, acc: 0.8717948794364929)
[2025-02-16 12:39:04,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:04,318][root][INFO] - Training Epoch: 1/2, step 9749/23838 completed (loss: 0.534239649772644, acc: 0.8500000238418579)
[2025-02-16 12:39:04,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:04,798][root][INFO] - Training Epoch: 1/2, step 9750/23838 completed (loss: 0.5660263895988464, acc: 0.831932783126831)
[2025-02-16 12:39:04,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:05,205][root][INFO] - Training Epoch: 1/2, step 9751/23838 completed (loss: 1.3761873245239258, acc: 0.686274528503418)
[2025-02-16 12:39:05,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:05,603][root][INFO] - Training Epoch: 1/2, step 9752/23838 completed (loss: 1.3277685642242432, acc: 0.6617646813392639)
[2025-02-16 12:39:05,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:06,026][root][INFO] - Training Epoch: 1/2, step 9753/23838 completed (loss: 0.8780989050865173, acc: 0.7857142686843872)
[2025-02-16 12:39:06,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:06,442][root][INFO] - Training Epoch: 1/2, step 9754/23838 completed (loss: 0.376979798078537, acc: 0.8703703880310059)
[2025-02-16 12:39:06,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:06,886][root][INFO] - Training Epoch: 1/2, step 9755/23838 completed (loss: 0.5859070420265198, acc: 0.8269230723381042)
[2025-02-16 12:39:07,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:07,362][root][INFO] - Training Epoch: 1/2, step 9756/23838 completed (loss: 1.3341071605682373, acc: 0.6296296119689941)
[2025-02-16 12:39:07,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:07,817][root][INFO] - Training Epoch: 1/2, step 9757/23838 completed (loss: 0.8059635162353516, acc: 0.7837837934494019)
[2025-02-16 12:39:08,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:08,275][root][INFO] - Training Epoch: 1/2, step 9758/23838 completed (loss: 0.8116648197174072, acc: 0.7808219194412231)
[2025-02-16 12:39:08,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:08,869][root][INFO] - Training Epoch: 1/2, step 9759/23838 completed (loss: 0.9909906983375549, acc: 0.7599999904632568)
[2025-02-16 12:39:09,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:09,276][root][INFO] - Training Epoch: 1/2, step 9760/23838 completed (loss: 0.34004276990890503, acc: 0.9324324131011963)
[2025-02-16 12:39:09,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:09,807][root][INFO] - Training Epoch: 1/2, step 9761/23838 completed (loss: 0.5998262763023376, acc: 0.8251748085021973)
[2025-02-16 12:39:10,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:10,301][root][INFO] - Training Epoch: 1/2, step 9762/23838 completed (loss: 0.41622209548950195, acc: 0.876288652420044)
[2025-02-16 12:39:10,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:10,721][root][INFO] - Training Epoch: 1/2, step 9763/23838 completed (loss: 0.5645621418952942, acc: 0.8666666746139526)
[2025-02-16 12:39:10,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:11,174][root][INFO] - Training Epoch: 1/2, step 9764/23838 completed (loss: 1.1113497018814087, acc: 0.6703296899795532)
[2025-02-16 12:39:11,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:11,648][root][INFO] - Training Epoch: 1/2, step 9765/23838 completed (loss: 0.6054242253303528, acc: 0.8640776872634888)
[2025-02-16 12:39:11,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:12,162][root][INFO] - Training Epoch: 1/2, step 9766/23838 completed (loss: 0.7355379462242126, acc: 0.8064516186714172)
[2025-02-16 12:39:12,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:12,548][root][INFO] - Training Epoch: 1/2, step 9767/23838 completed (loss: 0.5478274822235107, acc: 0.8374999761581421)
[2025-02-16 12:39:12,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:12,932][root][INFO] - Training Epoch: 1/2, step 9768/23838 completed (loss: 0.2613249719142914, acc: 0.9009901285171509)
[2025-02-16 12:39:13,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:13,359][root][INFO] - Training Epoch: 1/2, step 9769/23838 completed (loss: 0.675215482711792, acc: 0.8409090638160706)
[2025-02-16 12:39:13,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:13,849][root][INFO] - Training Epoch: 1/2, step 9770/23838 completed (loss: 0.4173635244369507, acc: 0.8965517282485962)
[2025-02-16 12:39:14,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:14,299][root][INFO] - Training Epoch: 1/2, step 9771/23838 completed (loss: 0.3675754964351654, acc: 0.9126213788986206)
[2025-02-16 12:39:14,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:14,764][root][INFO] - Training Epoch: 1/2, step 9772/23838 completed (loss: 0.31169626116752625, acc: 0.9166666865348816)
[2025-02-16 12:39:14,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:15,203][root][INFO] - Training Epoch: 1/2, step 9773/23838 completed (loss: 0.6572578549385071, acc: 0.8333333134651184)
[2025-02-16 12:39:15,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:15,887][root][INFO] - Training Epoch: 1/2, step 9774/23838 completed (loss: 0.4459858238697052, acc: 0.8737373948097229)
[2025-02-16 12:39:16,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:16,337][root][INFO] - Training Epoch: 1/2, step 9775/23838 completed (loss: 0.8869569301605225, acc: 0.7599999904632568)
[2025-02-16 12:39:16,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:16,784][root][INFO] - Training Epoch: 1/2, step 9776/23838 completed (loss: 0.6102014183998108, acc: 0.8165137767791748)
[2025-02-16 12:39:17,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:17,244][root][INFO] - Training Epoch: 1/2, step 9777/23838 completed (loss: 1.6492172479629517, acc: 0.5681818127632141)
[2025-02-16 12:39:17,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:17,767][root][INFO] - Training Epoch: 1/2, step 9778/23838 completed (loss: 0.4646540880203247, acc: 0.8582677245140076)
[2025-02-16 12:39:18,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:18,369][root][INFO] - Training Epoch: 1/2, step 9779/23838 completed (loss: 0.5426804423332214, acc: 0.8484848737716675)
[2025-02-16 12:39:18,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:18,844][root][INFO] - Training Epoch: 1/2, step 9780/23838 completed (loss: 1.2000316381454468, acc: 0.6625000238418579)
[2025-02-16 12:39:18,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:19,227][root][INFO] - Training Epoch: 1/2, step 9781/23838 completed (loss: 1.0293723344802856, acc: 0.7272727489471436)
[2025-02-16 12:39:19,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:19,597][root][INFO] - Training Epoch: 1/2, step 9782/23838 completed (loss: 0.7386215329170227, acc: 0.8260869383811951)
[2025-02-16 12:39:19,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:20,014][root][INFO] - Training Epoch: 1/2, step 9783/23838 completed (loss: 0.783089280128479, acc: 0.7613636255264282)
[2025-02-16 12:39:20,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:20,459][root][INFO] - Training Epoch: 1/2, step 9784/23838 completed (loss: 0.42270609736442566, acc: 0.9027777910232544)
[2025-02-16 12:39:20,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:20,920][root][INFO] - Training Epoch: 1/2, step 9785/23838 completed (loss: 0.9199260473251343, acc: 0.774193525314331)
[2025-02-16 12:39:21,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:21,410][root][INFO] - Training Epoch: 1/2, step 9786/23838 completed (loss: 0.4708787500858307, acc: 0.8630136847496033)
[2025-02-16 12:39:21,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:21,861][root][INFO] - Training Epoch: 1/2, step 9787/23838 completed (loss: 0.6074443459510803, acc: 0.8333333134651184)
[2025-02-16 12:39:22,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:22,327][root][INFO] - Training Epoch: 1/2, step 9788/23838 completed (loss: 0.6862073540687561, acc: 0.7727272510528564)
[2025-02-16 12:39:22,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:22,723][root][INFO] - Training Epoch: 1/2, step 9789/23838 completed (loss: 0.41029539704322815, acc: 0.8676470518112183)
[2025-02-16 12:39:22,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:23,187][root][INFO] - Training Epoch: 1/2, step 9790/23838 completed (loss: 0.36470022797584534, acc: 0.9102563858032227)
[2025-02-16 12:39:23,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:23,668][root][INFO] - Training Epoch: 1/2, step 9791/23838 completed (loss: 0.8478258848190308, acc: 0.7809523940086365)
[2025-02-16 12:39:23,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:24,137][root][INFO] - Training Epoch: 1/2, step 9792/23838 completed (loss: 0.7800591588020325, acc: 0.7702702879905701)
[2025-02-16 12:39:24,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:24,553][root][INFO] - Training Epoch: 1/2, step 9793/23838 completed (loss: 0.7550485134124756, acc: 0.8333333134651184)
[2025-02-16 12:39:24,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:25,003][root][INFO] - Training Epoch: 1/2, step 9794/23838 completed (loss: 1.1774544715881348, acc: 0.7236841917037964)
[2025-02-16 12:39:25,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:25,406][root][INFO] - Training Epoch: 1/2, step 9795/23838 completed (loss: 0.7662411332130432, acc: 0.7638888955116272)
[2025-02-16 12:39:25,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:25,857][root][INFO] - Training Epoch: 1/2, step 9796/23838 completed (loss: 0.959197461605072, acc: 0.719298243522644)
[2025-02-16 12:39:26,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:26,289][root][INFO] - Training Epoch: 1/2, step 9797/23838 completed (loss: 0.4192744791507721, acc: 0.9016393423080444)
[2025-02-16 12:39:26,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:26,718][root][INFO] - Training Epoch: 1/2, step 9798/23838 completed (loss: 0.25702670216560364, acc: 0.9279279112815857)
[2025-02-16 12:39:26,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:27,156][root][INFO] - Training Epoch: 1/2, step 9799/23838 completed (loss: 0.7473393678665161, acc: 0.7605633735656738)
[2025-02-16 12:39:27,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:27,640][root][INFO] - Training Epoch: 1/2, step 9800/23838 completed (loss: 0.613871693611145, acc: 0.8493150472640991)
[2025-02-16 12:39:27,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:28,083][root][INFO] - Training Epoch: 1/2, step 9801/23838 completed (loss: 0.501899242401123, acc: 0.9130434989929199)
[2025-02-16 12:39:28,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:28,548][root][INFO] - Training Epoch: 1/2, step 9802/23838 completed (loss: 0.590262770652771, acc: 0.8269230723381042)
[2025-02-16 12:39:28,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:29,071][root][INFO] - Training Epoch: 1/2, step 9803/23838 completed (loss: 0.8878293037414551, acc: 0.7766990065574646)
[2025-02-16 12:39:29,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:29,545][root][INFO] - Training Epoch: 1/2, step 9804/23838 completed (loss: 0.45397111773490906, acc: 0.8113207817077637)
[2025-02-16 12:39:29,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:29,975][root][INFO] - Training Epoch: 1/2, step 9805/23838 completed (loss: 0.5875812768936157, acc: 0.8604651093482971)
[2025-02-16 12:39:30,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:30,411][root][INFO] - Training Epoch: 1/2, step 9806/23838 completed (loss: 0.30723652243614197, acc: 0.914893627166748)
[2025-02-16 12:39:30,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:30,894][root][INFO] - Training Epoch: 1/2, step 9807/23838 completed (loss: 0.550094485282898, acc: 0.8582677245140076)
[2025-02-16 12:39:31,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:31,353][root][INFO] - Training Epoch: 1/2, step 9808/23838 completed (loss: 0.251631498336792, acc: 0.9166666865348816)
[2025-02-16 12:39:31,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:31,745][root][INFO] - Training Epoch: 1/2, step 9809/23838 completed (loss: 0.42922911047935486, acc: 0.8870967626571655)
[2025-02-16 12:39:31,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:32,195][root][INFO] - Training Epoch: 1/2, step 9810/23838 completed (loss: 0.7013171911239624, acc: 0.8222222328186035)
[2025-02-16 12:39:32,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:32,583][root][INFO] - Training Epoch: 1/2, step 9811/23838 completed (loss: 0.8026720881462097, acc: 0.7547169923782349)
[2025-02-16 12:39:32,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:33,022][root][INFO] - Training Epoch: 1/2, step 9812/23838 completed (loss: 0.8984795212745667, acc: 0.7272727489471436)
[2025-02-16 12:39:33,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:33,488][root][INFO] - Training Epoch: 1/2, step 9813/23838 completed (loss: 0.28883153200149536, acc: 0.9195402264595032)
[2025-02-16 12:39:33,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:33,894][root][INFO] - Training Epoch: 1/2, step 9814/23838 completed (loss: 0.9286537170410156, acc: 0.7543859481811523)
[2025-02-16 12:39:34,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:34,331][root][INFO] - Training Epoch: 1/2, step 9815/23838 completed (loss: 0.8884656429290771, acc: 0.7790697813034058)
[2025-02-16 12:39:34,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:34,847][root][INFO] - Training Epoch: 1/2, step 9816/23838 completed (loss: 0.45942553877830505, acc: 0.8725489974021912)
[2025-02-16 12:39:35,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:35,268][root][INFO] - Training Epoch: 1/2, step 9817/23838 completed (loss: 0.820723831653595, acc: 0.8024691343307495)
[2025-02-16 12:39:35,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:35,704][root][INFO] - Training Epoch: 1/2, step 9818/23838 completed (loss: 0.550010085105896, acc: 0.8591549396514893)
[2025-02-16 12:39:35,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:36,188][root][INFO] - Training Epoch: 1/2, step 9819/23838 completed (loss: 0.8610295057296753, acc: 0.7627118825912476)
[2025-02-16 12:39:36,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:36,651][root][INFO] - Training Epoch: 1/2, step 9820/23838 completed (loss: 0.5322279334068298, acc: 0.8730158805847168)
[2025-02-16 12:39:36,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:37,092][root][INFO] - Training Epoch: 1/2, step 9821/23838 completed (loss: 0.6747020483016968, acc: 0.7575757503509521)
[2025-02-16 12:39:37,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:37,534][root][INFO] - Training Epoch: 1/2, step 9822/23838 completed (loss: 0.6657261252403259, acc: 0.8305084705352783)
[2025-02-16 12:39:37,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:38,012][root][INFO] - Training Epoch: 1/2, step 9823/23838 completed (loss: 0.6895644068717957, acc: 0.797468364238739)
[2025-02-16 12:39:38,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:38,420][root][INFO] - Training Epoch: 1/2, step 9824/23838 completed (loss: 1.2775782346725464, acc: 0.7164179086685181)
[2025-02-16 12:39:38,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:38,887][root][INFO] - Training Epoch: 1/2, step 9825/23838 completed (loss: 0.7598430514335632, acc: 0.8072289228439331)
[2025-02-16 12:39:39,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:39,259][root][INFO] - Training Epoch: 1/2, step 9826/23838 completed (loss: 0.46790435910224915, acc: 0.8902438879013062)
[2025-02-16 12:39:39,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:39,650][root][INFO] - Training Epoch: 1/2, step 9827/23838 completed (loss: 0.6162906885147095, acc: 0.800000011920929)
[2025-02-16 12:39:39,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:40,069][root][INFO] - Training Epoch: 1/2, step 9828/23838 completed (loss: 0.379726380109787, acc: 0.875)
[2025-02-16 12:39:40,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:40,442][root][INFO] - Training Epoch: 1/2, step 9829/23838 completed (loss: 0.7817549109458923, acc: 0.7222222089767456)
[2025-02-16 12:39:40,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:40,869][root][INFO] - Training Epoch: 1/2, step 9830/23838 completed (loss: 0.7165546417236328, acc: 0.8481012582778931)
[2025-02-16 12:39:41,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:41,315][root][INFO] - Training Epoch: 1/2, step 9831/23838 completed (loss: 0.8294817209243774, acc: 0.7674418687820435)
[2025-02-16 12:39:41,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:41,687][root][INFO] - Training Epoch: 1/2, step 9832/23838 completed (loss: 0.3577844798564911, acc: 0.9113923907279968)
[2025-02-16 12:39:41,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:42,068][root][INFO] - Training Epoch: 1/2, step 9833/23838 completed (loss: 1.545221209526062, acc: 0.523809552192688)
[2025-02-16 12:39:42,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:42,482][root][INFO] - Training Epoch: 1/2, step 9834/23838 completed (loss: 0.6181772947311401, acc: 0.8125)
[2025-02-16 12:39:42,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:42,939][root][INFO] - Training Epoch: 1/2, step 9835/23838 completed (loss: 0.5568523406982422, acc: 0.800000011920929)
[2025-02-16 12:39:43,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:43,369][root][INFO] - Training Epoch: 1/2, step 9836/23838 completed (loss: 0.6230306029319763, acc: 0.9076923131942749)
[2025-02-16 12:39:43,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:43,808][root][INFO] - Training Epoch: 1/2, step 9837/23838 completed (loss: 0.7050303220748901, acc: 0.8145161271095276)
[2025-02-16 12:39:44,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:44,282][root][INFO] - Training Epoch: 1/2, step 9838/23838 completed (loss: 0.5939207673072815, acc: 0.8556700944900513)
[2025-02-16 12:39:44,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:44,717][root][INFO] - Training Epoch: 1/2, step 9839/23838 completed (loss: 0.5892512202262878, acc: 0.8285714387893677)
[2025-02-16 12:39:45,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:45,350][root][INFO] - Training Epoch: 1/2, step 9840/23838 completed (loss: 0.611650824546814, acc: 0.8296296000480652)
[2025-02-16 12:39:45,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:45,762][root][INFO] - Training Epoch: 1/2, step 9841/23838 completed (loss: 0.4837210774421692, acc: 0.8840579986572266)
[2025-02-16 12:39:45,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:46,231][root][INFO] - Training Epoch: 1/2, step 9842/23838 completed (loss: 0.6012611985206604, acc: 0.8333333134651184)
[2025-02-16 12:39:46,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:46,676][root][INFO] - Training Epoch: 1/2, step 9843/23838 completed (loss: 0.2674139142036438, acc: 0.9038461446762085)
[2025-02-16 12:39:46,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:47,135][root][INFO] - Training Epoch: 1/2, step 9844/23838 completed (loss: 0.23710967600345612, acc: 0.8977272510528564)
[2025-02-16 12:39:47,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:47,598][root][INFO] - Training Epoch: 1/2, step 9845/23838 completed (loss: 0.6397354602813721, acc: 0.8207547068595886)
[2025-02-16 12:39:47,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:48,104][root][INFO] - Training Epoch: 1/2, step 9846/23838 completed (loss: 0.3281581401824951, acc: 0.8834951519966125)
[2025-02-16 12:39:48,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:48,471][root][INFO] - Training Epoch: 1/2, step 9847/23838 completed (loss: 0.5458923578262329, acc: 0.8272727131843567)
[2025-02-16 12:39:48,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:48,842][root][INFO] - Training Epoch: 1/2, step 9848/23838 completed (loss: 0.2593162953853607, acc: 0.9047619104385376)
[2025-02-16 12:39:49,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:49,268][root][INFO] - Training Epoch: 1/2, step 9849/23838 completed (loss: 0.47937166690826416, acc: 0.9014084339141846)
[2025-02-16 12:39:49,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:49,983][root][INFO] - Training Epoch: 1/2, step 9850/23838 completed (loss: 0.19932229816913605, acc: 0.9387755393981934)
[2025-02-16 12:39:50,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:50,421][root][INFO] - Training Epoch: 1/2, step 9851/23838 completed (loss: 0.22704356908798218, acc: 0.9054054021835327)
[2025-02-16 12:39:50,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:50,904][root][INFO] - Training Epoch: 1/2, step 9852/23838 completed (loss: 0.5205883383750916, acc: 0.8151260614395142)
[2025-02-16 12:39:51,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:51,409][root][INFO] - Training Epoch: 1/2, step 9853/23838 completed (loss: 0.5330772399902344, acc: 0.8604651093482971)
[2025-02-16 12:39:51,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:51,842][root][INFO] - Training Epoch: 1/2, step 9854/23838 completed (loss: 0.5579102039337158, acc: 0.8425197005271912)
[2025-02-16 12:39:51,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:52,232][root][INFO] - Training Epoch: 1/2, step 9855/23838 completed (loss: 0.5554199814796448, acc: 0.8461538553237915)
[2025-02-16 12:39:52,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:52,749][root][INFO] - Training Epoch: 1/2, step 9856/23838 completed (loss: 0.3116271495819092, acc: 0.9044585824012756)
[2025-02-16 12:39:53,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:53,488][root][INFO] - Training Epoch: 1/2, step 9857/23838 completed (loss: 0.33809608221054077, acc: 0.9024389982223511)
[2025-02-16 12:39:53,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:53,895][root][INFO] - Training Epoch: 1/2, step 9858/23838 completed (loss: 0.5512143969535828, acc: 0.8199999928474426)
[2025-02-16 12:39:54,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:54,367][root][INFO] - Training Epoch: 1/2, step 9859/23838 completed (loss: 0.2818616032600403, acc: 0.9242424368858337)
[2025-02-16 12:39:54,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:54,838][root][INFO] - Training Epoch: 1/2, step 9860/23838 completed (loss: 0.31384190917015076, acc: 0.8991596698760986)
[2025-02-16 12:39:55,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:55,403][root][INFO] - Training Epoch: 1/2, step 9861/23838 completed (loss: 0.17456384003162384, acc: 0.9406779408454895)
[2025-02-16 12:39:55,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:55,868][root][INFO] - Training Epoch: 1/2, step 9862/23838 completed (loss: 0.09517153352499008, acc: 0.9680851101875305)
[2025-02-16 12:39:56,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:56,420][root][INFO] - Training Epoch: 1/2, step 9863/23838 completed (loss: 0.3792886435985565, acc: 0.8818181753158569)
[2025-02-16 12:39:56,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:56,886][root][INFO] - Training Epoch: 1/2, step 9864/23838 completed (loss: 0.19867056608200073, acc: 0.9491525292396545)
[2025-02-16 12:39:57,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:57,557][root][INFO] - Training Epoch: 1/2, step 9865/23838 completed (loss: 0.41520893573760986, acc: 0.8877550959587097)
[2025-02-16 12:39:57,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:58,022][root][INFO] - Training Epoch: 1/2, step 9866/23838 completed (loss: 0.29368671774864197, acc: 0.918181836605072)
[2025-02-16 12:39:58,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:58,507][root][INFO] - Training Epoch: 1/2, step 9867/23838 completed (loss: 0.4191228151321411, acc: 0.8645161390304565)
[2025-02-16 12:39:58,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:58,977][root][INFO] - Training Epoch: 1/2, step 9868/23838 completed (loss: 0.5650951862335205, acc: 0.859649121761322)
[2025-02-16 12:39:59,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:59,430][root][INFO] - Training Epoch: 1/2, step 9869/23838 completed (loss: 0.7557635307312012, acc: 0.8333333134651184)
[2025-02-16 12:39:59,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:39:59,850][root][INFO] - Training Epoch: 1/2, step 9870/23838 completed (loss: 0.5636487007141113, acc: 0.8651685118675232)
[2025-02-16 12:40:00,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:00,234][root][INFO] - Training Epoch: 1/2, step 9871/23838 completed (loss: 0.4500998556613922, acc: 0.8787878751754761)
[2025-02-16 12:40:00,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:00,675][root][INFO] - Training Epoch: 1/2, step 9872/23838 completed (loss: 0.27049246430397034, acc: 0.9090909361839294)
[2025-02-16 12:40:00,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:01,141][root][INFO] - Training Epoch: 1/2, step 9873/23838 completed (loss: 0.6355765461921692, acc: 0.800000011920929)
[2025-02-16 12:40:01,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:01,549][root][INFO] - Training Epoch: 1/2, step 9874/23838 completed (loss: 0.514805018901825, acc: 0.8529411554336548)
[2025-02-16 12:40:01,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:02,152][root][INFO] - Training Epoch: 1/2, step 9875/23838 completed (loss: 0.7567652463912964, acc: 0.8251748085021973)
[2025-02-16 12:40:02,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:02,662][root][INFO] - Training Epoch: 1/2, step 9876/23838 completed (loss: 0.381789892911911, acc: 0.907216489315033)
[2025-02-16 12:40:02,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:03,343][root][INFO] - Training Epoch: 1/2, step 9877/23838 completed (loss: 0.6436502933502197, acc: 0.8399999737739563)
[2025-02-16 12:40:03,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:03,738][root][INFO] - Training Epoch: 1/2, step 9878/23838 completed (loss: 0.14189256727695465, acc: 0.9718309640884399)
[2025-02-16 12:40:03,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:04,168][root][INFO] - Training Epoch: 1/2, step 9879/23838 completed (loss: 0.3036440312862396, acc: 0.8907563090324402)
[2025-02-16 12:40:04,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:04,652][root][INFO] - Training Epoch: 1/2, step 9880/23838 completed (loss: 0.2691391408443451, acc: 0.9214285612106323)
[2025-02-16 12:40:04,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:05,093][root][INFO] - Training Epoch: 1/2, step 9881/23838 completed (loss: 0.6410637497901917, acc: 0.8157894611358643)
[2025-02-16 12:40:05,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:05,609][root][INFO] - Training Epoch: 1/2, step 9882/23838 completed (loss: 0.46323341131210327, acc: 0.8678160905838013)
[2025-02-16 12:40:05,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:06,079][root][INFO] - Training Epoch: 1/2, step 9883/23838 completed (loss: 1.015526533126831, acc: 0.7111111283302307)
[2025-02-16 12:40:06,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:06,575][root][INFO] - Training Epoch: 1/2, step 9884/23838 completed (loss: 0.5831961631774902, acc: 0.8899999856948853)
[2025-02-16 12:40:06,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:07,081][root][INFO] - Training Epoch: 1/2, step 9885/23838 completed (loss: 0.5344858765602112, acc: 0.8448275923728943)
[2025-02-16 12:40:07,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:07,598][root][INFO] - Training Epoch: 1/2, step 9886/23838 completed (loss: 0.5538744926452637, acc: 0.8444444537162781)
[2025-02-16 12:40:07,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:08,054][root][INFO] - Training Epoch: 1/2, step 9887/23838 completed (loss: 0.7819184064865112, acc: 0.765625)
[2025-02-16 12:40:08,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:08,540][root][INFO] - Training Epoch: 1/2, step 9888/23838 completed (loss: 0.4521487057209015, acc: 0.9038461446762085)
[2025-02-16 12:40:08,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:09,065][root][INFO] - Training Epoch: 1/2, step 9889/23838 completed (loss: 0.9448195695877075, acc: 0.7416666746139526)
[2025-02-16 12:40:09,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:09,617][root][INFO] - Training Epoch: 1/2, step 9890/23838 completed (loss: 0.39937224984169006, acc: 0.8931297659873962)
[2025-02-16 12:40:09,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:10,021][root][INFO] - Training Epoch: 1/2, step 9891/23838 completed (loss: 0.620724618434906, acc: 0.8399999737739563)
[2025-02-16 12:40:10,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:10,479][root][INFO] - Training Epoch: 1/2, step 9892/23838 completed (loss: 1.059221863746643, acc: 0.7564102411270142)
[2025-02-16 12:40:10,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:10,899][root][INFO] - Training Epoch: 1/2, step 9893/23838 completed (loss: 0.8775890469551086, acc: 0.7719298005104065)
[2025-02-16 12:40:11,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:11,387][root][INFO] - Training Epoch: 1/2, step 9894/23838 completed (loss: 0.959209144115448, acc: 0.759036123752594)
[2025-02-16 12:40:11,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:11,834][root][INFO] - Training Epoch: 1/2, step 9895/23838 completed (loss: 0.5651739239692688, acc: 0.8608695864677429)
[2025-02-16 12:40:11,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:12,276][root][INFO] - Training Epoch: 1/2, step 9896/23838 completed (loss: 0.41397228837013245, acc: 0.8807339668273926)
[2025-02-16 12:40:12,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:12,725][root][INFO] - Training Epoch: 1/2, step 9897/23838 completed (loss: 0.7213526368141174, acc: 0.8041236996650696)
[2025-02-16 12:40:12,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:13,193][root][INFO] - Training Epoch: 1/2, step 9898/23838 completed (loss: 1.0999928712844849, acc: 0.7415730357170105)
[2025-02-16 12:40:13,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:13,656][root][INFO] - Training Epoch: 1/2, step 9899/23838 completed (loss: 0.8360501527786255, acc: 0.7473683953285217)
[2025-02-16 12:40:13,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:14,083][root][INFO] - Training Epoch: 1/2, step 9900/23838 completed (loss: 0.8658167123794556, acc: 0.7764706015586853)
[2025-02-16 12:40:14,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:14,560][root][INFO] - Training Epoch: 1/2, step 9901/23838 completed (loss: 0.49377745389938354, acc: 0.904411792755127)
[2025-02-16 12:40:14,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:15,082][root][INFO] - Training Epoch: 1/2, step 9902/23838 completed (loss: 0.5977071523666382, acc: 0.8508771657943726)
[2025-02-16 12:40:15,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:15,510][root][INFO] - Training Epoch: 1/2, step 9903/23838 completed (loss: 0.8935637474060059, acc: 0.762499988079071)
[2025-02-16 12:40:15,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:16,195][root][INFO] - Training Epoch: 1/2, step 9904/23838 completed (loss: 0.7037619352340698, acc: 0.8176795840263367)
[2025-02-16 12:40:16,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:16,621][root][INFO] - Training Epoch: 1/2, step 9905/23838 completed (loss: 0.21280594170093536, acc: 0.9784946441650391)
[2025-02-16 12:40:16,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:17,105][root][INFO] - Training Epoch: 1/2, step 9906/23838 completed (loss: 0.2716881036758423, acc: 0.9006211161613464)
[2025-02-16 12:40:17,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:17,638][root][INFO] - Training Epoch: 1/2, step 9907/23838 completed (loss: 0.614923894405365, acc: 0.8439716100692749)
[2025-02-16 12:40:17,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:18,083][root][INFO] - Training Epoch: 1/2, step 9908/23838 completed (loss: 0.7326266169548035, acc: 0.8333333134651184)
[2025-02-16 12:40:18,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:18,520][root][INFO] - Training Epoch: 1/2, step 9909/23838 completed (loss: 0.5602412223815918, acc: 0.8396226167678833)
[2025-02-16 12:40:18,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:19,012][root][INFO] - Training Epoch: 1/2, step 9910/23838 completed (loss: 0.6128017902374268, acc: 0.8545454740524292)
[2025-02-16 12:40:19,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:19,490][root][INFO] - Training Epoch: 1/2, step 9911/23838 completed (loss: 0.34997135400772095, acc: 0.9074074029922485)
[2025-02-16 12:40:19,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:20,103][root][INFO] - Training Epoch: 1/2, step 9912/23838 completed (loss: 0.45695286989212036, acc: 0.8772727251052856)
[2025-02-16 12:40:20,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:20,631][root][INFO] - Training Epoch: 1/2, step 9913/23838 completed (loss: 0.4961833655834198, acc: 0.89552241563797)
[2025-02-16 12:40:20,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:21,144][root][INFO] - Training Epoch: 1/2, step 9914/23838 completed (loss: 0.33395522832870483, acc: 0.9186046719551086)
[2025-02-16 12:40:21,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:21,620][root][INFO] - Training Epoch: 1/2, step 9915/23838 completed (loss: 0.4776376485824585, acc: 0.8877550959587097)
[2025-02-16 12:40:21,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:22,065][root][INFO] - Training Epoch: 1/2, step 9916/23838 completed (loss: 0.437228262424469, acc: 0.8592592477798462)
[2025-02-16 12:40:22,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:22,495][root][INFO] - Training Epoch: 1/2, step 9917/23838 completed (loss: 0.8501822352409363, acc: 0.8072289228439331)
[2025-02-16 12:40:22,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:22,962][root][INFO] - Training Epoch: 1/2, step 9918/23838 completed (loss: 0.4382762014865875, acc: 0.8818181753158569)
[2025-02-16 12:40:23,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:23,377][root][INFO] - Training Epoch: 1/2, step 9919/23838 completed (loss: 0.7056599855422974, acc: 0.7547169923782349)
[2025-02-16 12:40:23,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:23,866][root][INFO] - Training Epoch: 1/2, step 9920/23838 completed (loss: 0.651710569858551, acc: 0.8269230723381042)
[2025-02-16 12:40:24,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:24,342][root][INFO] - Training Epoch: 1/2, step 9921/23838 completed (loss: 0.5844057202339172, acc: 0.8629032373428345)
[2025-02-16 12:40:24,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:24,816][root][INFO] - Training Epoch: 1/2, step 9922/23838 completed (loss: 0.7474536895751953, acc: 0.7928571701049805)
[2025-02-16 12:40:25,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:25,209][root][INFO] - Training Epoch: 1/2, step 9923/23838 completed (loss: 0.31969013810157776, acc: 0.9210526347160339)
[2025-02-16 12:40:25,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:25,657][root][INFO] - Training Epoch: 1/2, step 9924/23838 completed (loss: 0.3039575517177582, acc: 0.9173553586006165)
[2025-02-16 12:40:25,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:26,054][root][INFO] - Training Epoch: 1/2, step 9925/23838 completed (loss: 0.6194739937782288, acc: 0.8278145790100098)
[2025-02-16 12:40:26,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:26,555][root][INFO] - Training Epoch: 1/2, step 9926/23838 completed (loss: 0.8553423881530762, acc: 0.800000011920929)
[2025-02-16 12:40:26,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:26,937][root][INFO] - Training Epoch: 1/2, step 9927/23838 completed (loss: 0.42127832770347595, acc: 0.8823529481887817)
[2025-02-16 12:40:27,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:27,402][root][INFO] - Training Epoch: 1/2, step 9928/23838 completed (loss: 0.5362389087677002, acc: 0.8454545736312866)
[2025-02-16 12:40:27,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:27,871][root][INFO] - Training Epoch: 1/2, step 9929/23838 completed (loss: 0.8536675572395325, acc: 0.7222222089767456)
[2025-02-16 12:40:28,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:28,309][root][INFO] - Training Epoch: 1/2, step 9930/23838 completed (loss: 0.6177999973297119, acc: 0.8441558480262756)
[2025-02-16 12:40:28,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:28,784][root][INFO] - Training Epoch: 1/2, step 9931/23838 completed (loss: 0.2154303342103958, acc: 0.9462365508079529)
[2025-02-16 12:40:28,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:29,219][root][INFO] - Training Epoch: 1/2, step 9932/23838 completed (loss: 0.26948702335357666, acc: 0.9411764740943909)
[2025-02-16 12:40:29,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:29,660][root][INFO] - Training Epoch: 1/2, step 9933/23838 completed (loss: 0.376110702753067, acc: 0.8793103694915771)
[2025-02-16 12:40:29,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:30,104][root][INFO] - Training Epoch: 1/2, step 9934/23838 completed (loss: 0.6236459612846375, acc: 0.8474576473236084)
[2025-02-16 12:40:30,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:30,576][root][INFO] - Training Epoch: 1/2, step 9935/23838 completed (loss: 0.3856658637523651, acc: 0.8947368264198303)
[2025-02-16 12:40:30,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:31,046][root][INFO] - Training Epoch: 1/2, step 9936/23838 completed (loss: 0.5755270719528198, acc: 0.84112149477005)
[2025-02-16 12:40:31,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:31,573][root][INFO] - Training Epoch: 1/2, step 9937/23838 completed (loss: 0.4993203580379486, acc: 0.8290598392486572)
[2025-02-16 12:40:31,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:32,041][root][INFO] - Training Epoch: 1/2, step 9938/23838 completed (loss: 0.5173381567001343, acc: 0.8403361439704895)
[2025-02-16 12:40:32,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:32,507][root][INFO] - Training Epoch: 1/2, step 9939/23838 completed (loss: 0.3067212700843811, acc: 0.8993710875511169)
[2025-02-16 12:40:32,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:32,986][root][INFO] - Training Epoch: 1/2, step 9940/23838 completed (loss: 0.3089110255241394, acc: 0.8902438879013062)
[2025-02-16 12:40:33,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:33,385][root][INFO] - Training Epoch: 1/2, step 9941/23838 completed (loss: 0.5237634181976318, acc: 0.875)
[2025-02-16 12:40:33,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:33,821][root][INFO] - Training Epoch: 1/2, step 9942/23838 completed (loss: 0.781227171421051, acc: 0.7407407164573669)
[2025-02-16 12:40:33,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:34,277][root][INFO] - Training Epoch: 1/2, step 9943/23838 completed (loss: 0.8830705285072327, acc: 0.7638888955116272)
[2025-02-16 12:40:34,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:34,733][root][INFO] - Training Epoch: 1/2, step 9944/23838 completed (loss: 0.3173716366291046, acc: 0.9059829115867615)
[2025-02-16 12:40:34,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:35,167][root][INFO] - Training Epoch: 1/2, step 9945/23838 completed (loss: 0.7380677461624146, acc: 0.8333333134651184)
[2025-02-16 12:40:35,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:35,620][root][INFO] - Training Epoch: 1/2, step 9946/23838 completed (loss: 0.6612496972084045, acc: 0.8333333134651184)
[2025-02-16 12:40:35,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:36,026][root][INFO] - Training Epoch: 1/2, step 9947/23838 completed (loss: 0.756269097328186, acc: 0.7727272510528564)
[2025-02-16 12:40:36,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:36,367][root][INFO] - Training Epoch: 1/2, step 9948/23838 completed (loss: 0.9057279825210571, acc: 0.6888889074325562)
[2025-02-16 12:40:36,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:36,738][root][INFO] - Training Epoch: 1/2, step 9949/23838 completed (loss: 1.0297480821609497, acc: 0.6875)
[2025-02-16 12:40:36,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:37,133][root][INFO] - Training Epoch: 1/2, step 9950/23838 completed (loss: 0.7015199661254883, acc: 0.7943925261497498)
[2025-02-16 12:40:37,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:37,625][root][INFO] - Training Epoch: 1/2, step 9951/23838 completed (loss: 0.4216221272945404, acc: 0.8799999952316284)
[2025-02-16 12:40:37,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:38,078][root][INFO] - Training Epoch: 1/2, step 9952/23838 completed (loss: 0.667715847492218, acc: 0.8292682766914368)
[2025-02-16 12:40:38,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:38,489][root][INFO] - Training Epoch: 1/2, step 9953/23838 completed (loss: 0.7310203313827515, acc: 0.800000011920929)
[2025-02-16 12:40:38,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:38,935][root][INFO] - Training Epoch: 1/2, step 9954/23838 completed (loss: 1.1164741516113281, acc: 0.698113203048706)
[2025-02-16 12:40:39,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:39,346][root][INFO] - Training Epoch: 1/2, step 9955/23838 completed (loss: 0.8868822455406189, acc: 0.7681159377098083)
[2025-02-16 12:40:39,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:39,790][root][INFO] - Training Epoch: 1/2, step 9956/23838 completed (loss: 0.5029424428939819, acc: 0.8640776872634888)
[2025-02-16 12:40:39,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:40,244][root][INFO] - Training Epoch: 1/2, step 9957/23838 completed (loss: 0.7075608372688293, acc: 0.7640449404716492)
[2025-02-16 12:40:40,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:40,720][root][INFO] - Training Epoch: 1/2, step 9958/23838 completed (loss: 0.5146254897117615, acc: 0.8666666746139526)
[2025-02-16 12:40:40,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:41,111][root][INFO] - Training Epoch: 1/2, step 9959/23838 completed (loss: 0.41870200634002686, acc: 0.8846153616905212)
[2025-02-16 12:40:41,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:41,589][root][INFO] - Training Epoch: 1/2, step 9960/23838 completed (loss: 0.34602469205856323, acc: 0.8971962332725525)
[2025-02-16 12:40:41,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:42,015][root][INFO] - Training Epoch: 1/2, step 9961/23838 completed (loss: 0.5159364342689514, acc: 0.875)
[2025-02-16 12:40:42,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:42,416][root][INFO] - Training Epoch: 1/2, step 9962/23838 completed (loss: 0.4493647813796997, acc: 0.8539325594902039)
[2025-02-16 12:40:42,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:42,842][root][INFO] - Training Epoch: 1/2, step 9963/23838 completed (loss: 0.24327953159809113, acc: 0.925000011920929)
[2025-02-16 12:40:42,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:43,286][root][INFO] - Training Epoch: 1/2, step 9964/23838 completed (loss: 1.1959428787231445, acc: 0.7297297120094299)
[2025-02-16 12:40:43,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:43,782][root][INFO] - Training Epoch: 1/2, step 9965/23838 completed (loss: 0.3312159478664398, acc: 0.8875739574432373)
[2025-02-16 12:40:44,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:44,270][root][INFO] - Training Epoch: 1/2, step 9966/23838 completed (loss: 0.3739660084247589, acc: 0.9028571248054504)
[2025-02-16 12:40:44,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:44,752][root][INFO] - Training Epoch: 1/2, step 9967/23838 completed (loss: 0.5250699520111084, acc: 0.8631578683853149)
[2025-02-16 12:40:44,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:45,291][root][INFO] - Training Epoch: 1/2, step 9968/23838 completed (loss: 0.42886900901794434, acc: 0.8711656332015991)
[2025-02-16 12:40:45,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:45,780][root][INFO] - Training Epoch: 1/2, step 9969/23838 completed (loss: 0.40774083137512207, acc: 0.8735632300376892)
[2025-02-16 12:40:46,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:46,274][root][INFO] - Training Epoch: 1/2, step 9970/23838 completed (loss: 0.5453827381134033, acc: 0.8264462947845459)
[2025-02-16 12:40:46,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:46,717][root][INFO] - Training Epoch: 1/2, step 9971/23838 completed (loss: 0.42901238799095154, acc: 0.8928571343421936)
[2025-02-16 12:40:46,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:47,182][root][INFO] - Training Epoch: 1/2, step 9972/23838 completed (loss: 0.26889553666114807, acc: 0.936170220375061)
[2025-02-16 12:40:47,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:47,618][root][INFO] - Training Epoch: 1/2, step 9973/23838 completed (loss: 0.489642858505249, acc: 0.8787878751754761)
[2025-02-16 12:40:47,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:48,017][root][INFO] - Training Epoch: 1/2, step 9974/23838 completed (loss: 0.2755879759788513, acc: 0.9305555820465088)
[2025-02-16 12:40:48,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:48,796][root][INFO] - Training Epoch: 1/2, step 9975/23838 completed (loss: 0.2565056085586548, acc: 0.9337349534034729)
[2025-02-16 12:40:49,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:49,254][root][INFO] - Training Epoch: 1/2, step 9976/23838 completed (loss: 0.23612384498119354, acc: 0.9375)
[2025-02-16 12:40:49,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:49,673][root][INFO] - Training Epoch: 1/2, step 9977/23838 completed (loss: 0.2921053469181061, acc: 0.8999999761581421)
[2025-02-16 12:40:49,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:50,148][root][INFO] - Training Epoch: 1/2, step 9978/23838 completed (loss: 0.2041322886943817, acc: 0.9203540086746216)
[2025-02-16 12:40:50,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:50,543][root][INFO] - Training Epoch: 1/2, step 9979/23838 completed (loss: 0.48090192675590515, acc: 0.8679245114326477)
[2025-02-16 12:40:50,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:50,979][root][INFO] - Training Epoch: 1/2, step 9980/23838 completed (loss: 0.2824425995349884, acc: 0.932584285736084)
[2025-02-16 12:40:51,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:51,404][root][INFO] - Training Epoch: 1/2, step 9981/23838 completed (loss: 0.2924588620662689, acc: 0.9125000238418579)
[2025-02-16 12:40:51,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:51,835][root][INFO] - Training Epoch: 1/2, step 9982/23838 completed (loss: 0.2635606825351715, acc: 0.9339622855186462)
[2025-02-16 12:40:52,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:52,268][root][INFO] - Training Epoch: 1/2, step 9983/23838 completed (loss: 0.5424880385398865, acc: 0.8068181872367859)
[2025-02-16 12:40:52,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:52,797][root][INFO] - Training Epoch: 1/2, step 9984/23838 completed (loss: 0.5499654412269592, acc: 0.844660222530365)
[2025-02-16 12:40:53,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:53,256][root][INFO] - Training Epoch: 1/2, step 9985/23838 completed (loss: 0.5463227033615112, acc: 0.8588235378265381)
[2025-02-16 12:40:53,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:53,918][root][INFO] - Training Epoch: 1/2, step 9986/23838 completed (loss: 0.1683746576309204, acc: 0.9407894611358643)
[2025-02-16 12:40:54,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:54,383][root][INFO] - Training Epoch: 1/2, step 9987/23838 completed (loss: 0.8718109130859375, acc: 0.7777777910232544)
[2025-02-16 12:40:54,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:54,849][root][INFO] - Training Epoch: 1/2, step 9988/23838 completed (loss: 0.9312589764595032, acc: 0.720588207244873)
[2025-02-16 12:40:55,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:55,253][root][INFO] - Training Epoch: 1/2, step 9989/23838 completed (loss: 0.5650821924209595, acc: 0.8543046116828918)
[2025-02-16 12:40:55,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:55,683][root][INFO] - Training Epoch: 1/2, step 9990/23838 completed (loss: 0.7407577037811279, acc: 0.7752808928489685)
[2025-02-16 12:40:55,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:56,135][root][INFO] - Training Epoch: 1/2, step 9991/23838 completed (loss: 1.0840208530426025, acc: 0.7291666865348816)
[2025-02-16 12:40:56,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:56,565][root][INFO] - Training Epoch: 1/2, step 9992/23838 completed (loss: 1.1119996309280396, acc: 0.6491228342056274)
[2025-02-16 12:40:56,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:56,989][root][INFO] - Training Epoch: 1/2, step 9993/23838 completed (loss: 0.6950216293334961, acc: 0.800000011920929)
[2025-02-16 12:40:57,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:57,455][root][INFO] - Training Epoch: 1/2, step 9994/23838 completed (loss: 0.7327122688293457, acc: 0.8055555820465088)
[2025-02-16 12:40:57,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:57,935][root][INFO] - Training Epoch: 1/2, step 9995/23838 completed (loss: 0.4877735674381256, acc: 0.8571428656578064)
[2025-02-16 12:40:58,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:58,388][root][INFO] - Training Epoch: 1/2, step 9996/23838 completed (loss: 0.36886632442474365, acc: 0.9047619104385376)
[2025-02-16 12:40:58,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:58,807][root][INFO] - Training Epoch: 1/2, step 9997/23838 completed (loss: 0.2940887212753296, acc: 0.9236111044883728)
[2025-02-16 12:40:59,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:59,239][root][INFO] - Training Epoch: 1/2, step 9998/23838 completed (loss: 0.32466012239456177, acc: 0.8842975497245789)
[2025-02-16 12:40:59,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:40:59,705][root][INFO] - Training Epoch: 1/2, step 9999/23838 completed (loss: 0.40798500180244446, acc: 0.8813559412956238)
[2025-02-16 12:40:59,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:00,122][root][INFO] - Training Epoch: 1/2, step 10000/23838 completed (loss: 0.26422375440597534, acc: 0.9144384860992432)
[2025-02-16 12:41:00,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:00,545][root][INFO] - Training Epoch: 1/2, step 10001/23838 completed (loss: 0.5454269647598267, acc: 0.8350515365600586)
[2025-02-16 12:41:00,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:01,055][root][INFO] - Training Epoch: 1/2, step 10002/23838 completed (loss: 0.6514310240745544, acc: 0.8235294222831726)
[2025-02-16 12:41:01,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:01,543][root][INFO] - Training Epoch: 1/2, step 10003/23838 completed (loss: 0.6981362104415894, acc: 0.7746478915214539)
[2025-02-16 12:41:01,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:01,982][root][INFO] - Training Epoch: 1/2, step 10004/23838 completed (loss: 0.5985718965530396, acc: 0.8345864415168762)
[2025-02-16 12:41:02,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:02,466][root][INFO] - Training Epoch: 1/2, step 10005/23838 completed (loss: 0.26551392674446106, acc: 0.9337349534034729)
[2025-02-16 12:41:02,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:03,264][root][INFO] - Training Epoch: 1/2, step 10006/23838 completed (loss: 0.3352915048599243, acc: 0.9225806593894958)
[2025-02-16 12:41:03,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:03,680][root][INFO] - Training Epoch: 1/2, step 10007/23838 completed (loss: 0.6073873043060303, acc: 0.8333333134651184)
[2025-02-16 12:41:04,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:04,398][root][INFO] - Training Epoch: 1/2, step 10008/23838 completed (loss: 0.3304045498371124, acc: 0.9155555367469788)
[2025-02-16 12:41:04,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:04,924][root][INFO] - Training Epoch: 1/2, step 10009/23838 completed (loss: 0.42726805806159973, acc: 0.8399999737739563)
[2025-02-16 12:41:05,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:05,455][root][INFO] - Training Epoch: 1/2, step 10010/23838 completed (loss: 0.47489210963249207, acc: 0.8815789222717285)
[2025-02-16 12:41:05,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:05,979][root][INFO] - Training Epoch: 1/2, step 10011/23838 completed (loss: 0.5746480822563171, acc: 0.8357142806053162)
[2025-02-16 12:41:06,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:06,560][root][INFO] - Training Epoch: 1/2, step 10012/23838 completed (loss: 0.5109399557113647, acc: 0.8310810923576355)
[2025-02-16 12:41:06,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:07,104][root][INFO] - Training Epoch: 1/2, step 10013/23838 completed (loss: 0.5892400741577148, acc: 0.8151260614395142)
[2025-02-16 12:41:07,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:07,863][root][INFO] - Training Epoch: 1/2, step 10014/23838 completed (loss: 0.3256317973136902, acc: 0.897849440574646)
[2025-02-16 12:41:08,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:08,669][root][INFO] - Training Epoch: 1/2, step 10015/23838 completed (loss: 0.32223424315452576, acc: 0.9185185432434082)
[2025-02-16 12:41:08,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:09,125][root][INFO] - Training Epoch: 1/2, step 10016/23838 completed (loss: 0.2915616035461426, acc: 0.9124087691307068)
[2025-02-16 12:41:09,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:09,596][root][INFO] - Training Epoch: 1/2, step 10017/23838 completed (loss: 0.6368139386177063, acc: 0.8433734774589539)
[2025-02-16 12:41:09,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:10,172][root][INFO] - Training Epoch: 1/2, step 10018/23838 completed (loss: 0.33564484119415283, acc: 0.9230769276618958)
[2025-02-16 12:41:10,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:10,650][root][INFO] - Training Epoch: 1/2, step 10019/23838 completed (loss: 0.4037254750728607, acc: 0.9042553305625916)
[2025-02-16 12:41:11,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:11,592][root][INFO] - Training Epoch: 1/2, step 10020/23838 completed (loss: 0.527141273021698, acc: 0.8461538553237915)
[2025-02-16 12:41:11,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:12,321][root][INFO] - Training Epoch: 1/2, step 10021/23838 completed (loss: 0.4253225326538086, acc: 0.8832116723060608)
[2025-02-16 12:41:12,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:13,072][root][INFO] - Training Epoch: 1/2, step 10022/23838 completed (loss: 0.316428542137146, acc: 0.9235668778419495)
[2025-02-16 12:41:13,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:13,976][root][INFO] - Training Epoch: 1/2, step 10023/23838 completed (loss: 0.31190046668052673, acc: 0.9166666865348816)
[2025-02-16 12:41:14,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:14,600][root][INFO] - Training Epoch: 1/2, step 10024/23838 completed (loss: 0.6148232221603394, acc: 0.8134328126907349)
[2025-02-16 12:41:14,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:14,930][root][INFO] - Training Epoch: 1/2, step 10025/23838 completed (loss: 0.3367612063884735, acc: 0.8983050584793091)
[2025-02-16 12:41:15,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:15,350][root][INFO] - Training Epoch: 1/2, step 10026/23838 completed (loss: 0.4784477949142456, acc: 0.8811880946159363)
[2025-02-16 12:41:15,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:15,923][root][INFO] - Training Epoch: 1/2, step 10027/23838 completed (loss: 0.3437764644622803, acc: 0.8863636255264282)
[2025-02-16 12:41:16,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:16,451][root][INFO] - Training Epoch: 1/2, step 10028/23838 completed (loss: 0.2246352881193161, acc: 0.939393937587738)
[2025-02-16 12:41:16,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:16,977][root][INFO] - Training Epoch: 1/2, step 10029/23838 completed (loss: 0.3535406291484833, acc: 0.9141104221343994)
[2025-02-16 12:41:17,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:17,486][root][INFO] - Training Epoch: 1/2, step 10030/23838 completed (loss: 0.384969562292099, acc: 0.9047619104385376)
[2025-02-16 12:41:17,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:18,371][root][INFO] - Training Epoch: 1/2, step 10031/23838 completed (loss: 0.13101017475128174, acc: 0.9638009071350098)
[2025-02-16 12:41:18,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:18,906][root][INFO] - Training Epoch: 1/2, step 10032/23838 completed (loss: 0.5296290516853333, acc: 0.8833333253860474)
[2025-02-16 12:41:19,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:19,457][root][INFO] - Training Epoch: 1/2, step 10033/23838 completed (loss: 0.23605114221572876, acc: 0.9191918969154358)
[2025-02-16 12:41:19,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:20,069][root][INFO] - Training Epoch: 1/2, step 10034/23838 completed (loss: 0.31303870677948, acc: 0.9047619104385376)
[2025-02-16 12:41:20,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:20,613][root][INFO] - Training Epoch: 1/2, step 10035/23838 completed (loss: 0.44746634364128113, acc: 0.8608695864677429)
[2025-02-16 12:41:20,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:21,281][root][INFO] - Training Epoch: 1/2, step 10036/23838 completed (loss: 0.4600035846233368, acc: 0.8521126508712769)
[2025-02-16 12:41:21,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:22,062][root][INFO] - Training Epoch: 1/2, step 10037/23838 completed (loss: 0.356507271528244, acc: 0.8878923654556274)
[2025-02-16 12:41:22,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:22,530][root][INFO] - Training Epoch: 1/2, step 10038/23838 completed (loss: 0.33408790826797485, acc: 0.9090909361839294)
[2025-02-16 12:41:22,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:23,286][root][INFO] - Training Epoch: 1/2, step 10039/23838 completed (loss: 0.3826788365840912, acc: 0.8865979313850403)
[2025-02-16 12:41:23,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:23,695][root][INFO] - Training Epoch: 1/2, step 10040/23838 completed (loss: 0.4547104239463806, acc: 0.8701298832893372)
[2025-02-16 12:41:23,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:24,217][root][INFO] - Training Epoch: 1/2, step 10041/23838 completed (loss: 0.34956562519073486, acc: 0.9055117964744568)
[2025-02-16 12:41:24,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:24,664][root][INFO] - Training Epoch: 1/2, step 10042/23838 completed (loss: 0.7203434109687805, acc: 0.8315789699554443)
[2025-02-16 12:41:24,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:25,302][root][INFO] - Training Epoch: 1/2, step 10043/23838 completed (loss: 0.2631049156188965, acc: 0.9194630980491638)
[2025-02-16 12:41:25,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:25,773][root][INFO] - Training Epoch: 1/2, step 10044/23838 completed (loss: 0.24840031564235687, acc: 0.9430379867553711)
[2025-02-16 12:41:25,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:26,270][root][INFO] - Training Epoch: 1/2, step 10045/23838 completed (loss: 0.47685757279396057, acc: 0.8774193525314331)
[2025-02-16 12:41:26,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:27,162][root][INFO] - Training Epoch: 1/2, step 10046/23838 completed (loss: 0.3692852258682251, acc: 0.8936170339584351)
[2025-02-16 12:41:27,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:28,081][root][INFO] - Training Epoch: 1/2, step 10047/23838 completed (loss: 0.33997032046318054, acc: 0.9252336621284485)
[2025-02-16 12:41:28,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:28,679][root][INFO] - Training Epoch: 1/2, step 10048/23838 completed (loss: 0.48977798223495483, acc: 0.8888888955116272)
[2025-02-16 12:41:28,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:29,118][root][INFO] - Training Epoch: 1/2, step 10049/23838 completed (loss: 0.6822492480278015, acc: 0.8611111044883728)
[2025-02-16 12:41:29,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:29,561][root][INFO] - Training Epoch: 1/2, step 10050/23838 completed (loss: 0.3285028338432312, acc: 0.9367088675498962)
[2025-02-16 12:41:29,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:30,103][root][INFO] - Training Epoch: 1/2, step 10051/23838 completed (loss: 0.182907372713089, acc: 0.957446813583374)
[2025-02-16 12:41:30,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:30,669][root][INFO] - Training Epoch: 1/2, step 10052/23838 completed (loss: 0.5014348030090332, acc: 0.8500000238418579)
[2025-02-16 12:41:30,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:31,109][root][INFO] - Training Epoch: 1/2, step 10053/23838 completed (loss: 0.3031221628189087, acc: 0.8936170339584351)
[2025-02-16 12:41:31,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:31,539][root][INFO] - Training Epoch: 1/2, step 10054/23838 completed (loss: 0.17729441821575165, acc: 0.9629629850387573)
[2025-02-16 12:41:31,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:31,917][root][INFO] - Training Epoch: 1/2, step 10055/23838 completed (loss: 0.25672557950019836, acc: 0.9047619104385376)
[2025-02-16 12:41:32,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:32,389][root][INFO] - Training Epoch: 1/2, step 10056/23838 completed (loss: 0.24009233713150024, acc: 0.9328858852386475)
[2025-02-16 12:41:32,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:32,801][root][INFO] - Training Epoch: 1/2, step 10057/23838 completed (loss: 0.4368766248226166, acc: 0.8661417365074158)
[2025-02-16 12:41:33,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:33,307][root][INFO] - Training Epoch: 1/2, step 10058/23838 completed (loss: 0.2692060172557831, acc: 0.9217391014099121)
[2025-02-16 12:41:33,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:33,917][root][INFO] - Training Epoch: 1/2, step 10059/23838 completed (loss: 0.3438846170902252, acc: 0.9105691313743591)
[2025-02-16 12:41:34,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:34,363][root][INFO] - Training Epoch: 1/2, step 10060/23838 completed (loss: 0.4902925491333008, acc: 0.8778625726699829)
[2025-02-16 12:41:34,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:34,878][root][INFO] - Training Epoch: 1/2, step 10061/23838 completed (loss: 0.2959017753601074, acc: 0.9152542352676392)
[2025-02-16 12:41:35,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:35,277][root][INFO] - Training Epoch: 1/2, step 10062/23838 completed (loss: 0.5738076567649841, acc: 0.8508771657943726)
[2025-02-16 12:41:35,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:35,694][root][INFO] - Training Epoch: 1/2, step 10063/23838 completed (loss: 0.7069083452224731, acc: 0.8494623899459839)
[2025-02-16 12:41:35,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:36,120][root][INFO] - Training Epoch: 1/2, step 10064/23838 completed (loss: 0.2990758419036865, acc: 0.8859649300575256)
[2025-02-16 12:41:36,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:36,556][root][INFO] - Training Epoch: 1/2, step 10065/23838 completed (loss: 0.44292402267456055, acc: 0.8846153616905212)
[2025-02-16 12:41:36,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:37,015][root][INFO] - Training Epoch: 1/2, step 10066/23838 completed (loss: 0.4595930576324463, acc: 0.8791208863258362)
[2025-02-16 12:41:37,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:37,428][root][INFO] - Training Epoch: 1/2, step 10067/23838 completed (loss: 0.1955321580171585, acc: 0.970370352268219)
[2025-02-16 12:41:37,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:37,916][root][INFO] - Training Epoch: 1/2, step 10068/23838 completed (loss: 0.2425851821899414, acc: 0.9308176040649414)
[2025-02-16 12:41:38,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:38,425][root][INFO] - Training Epoch: 1/2, step 10069/23838 completed (loss: 0.25002342462539673, acc: 0.9285714030265808)
[2025-02-16 12:41:38,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:38,928][root][INFO] - Training Epoch: 1/2, step 10070/23838 completed (loss: 0.12463781982660294, acc: 0.9731543660163879)
[2025-02-16 12:41:39,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:39,394][root][INFO] - Training Epoch: 1/2, step 10071/23838 completed (loss: 0.40875983238220215, acc: 0.8965517282485962)
[2025-02-16 12:41:39,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:39,998][root][INFO] - Training Epoch: 1/2, step 10072/23838 completed (loss: 0.22076913714408875, acc: 0.9464285969734192)
[2025-02-16 12:41:40,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:40,360][root][INFO] - Training Epoch: 1/2, step 10073/23838 completed (loss: 0.40537911653518677, acc: 0.8767123222351074)
[2025-02-16 12:41:40,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:40,917][root][INFO] - Training Epoch: 1/2, step 10074/23838 completed (loss: 0.3672998547554016, acc: 0.8999999761581421)
[2025-02-16 12:41:41,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:41,760][root][INFO] - Training Epoch: 1/2, step 10075/23838 completed (loss: 0.41162043809890747, acc: 0.8965517282485962)
[2025-02-16 12:41:41,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:42,216][root][INFO] - Training Epoch: 1/2, step 10076/23838 completed (loss: 0.7613164782524109, acc: 0.7916666865348816)
[2025-02-16 12:41:42,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:43,290][root][INFO] - Training Epoch: 1/2, step 10077/23838 completed (loss: 0.29314881563186646, acc: 0.8947368264198303)
[2025-02-16 12:41:43,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:43,786][root][INFO] - Training Epoch: 1/2, step 10078/23838 completed (loss: 0.11475912481546402, acc: 0.9847328066825867)
[2025-02-16 12:41:43,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:44,222][root][INFO] - Training Epoch: 1/2, step 10079/23838 completed (loss: 0.32684895396232605, acc: 0.90625)
[2025-02-16 12:41:44,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:44,942][root][INFO] - Training Epoch: 1/2, step 10080/23838 completed (loss: 0.15407545864582062, acc: 0.9523809552192688)
[2025-02-16 12:41:45,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:45,395][root][INFO] - Training Epoch: 1/2, step 10081/23838 completed (loss: 0.5360188484191895, acc: 0.8571428656578064)
[2025-02-16 12:41:45,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:45,938][root][INFO] - Training Epoch: 1/2, step 10082/23838 completed (loss: 0.17518529295921326, acc: 0.966292142868042)
[2025-02-16 12:41:46,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:46,356][root][INFO] - Training Epoch: 1/2, step 10083/23838 completed (loss: 0.3460066318511963, acc: 0.8405796885490417)
[2025-02-16 12:41:46,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:46,866][root][INFO] - Training Epoch: 1/2, step 10084/23838 completed (loss: 0.23695431649684906, acc: 0.9451219439506531)
[2025-02-16 12:41:47,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:47,494][root][INFO] - Training Epoch: 1/2, step 10085/23838 completed (loss: 0.18771564960479736, acc: 0.95652174949646)
[2025-02-16 12:41:47,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:48,095][root][INFO] - Training Epoch: 1/2, step 10086/23838 completed (loss: 0.29584017395973206, acc: 0.9189189076423645)
[2025-02-16 12:41:48,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:48,499][root][INFO] - Training Epoch: 1/2, step 10087/23838 completed (loss: 1.3197102546691895, acc: 0.6714285612106323)
[2025-02-16 12:41:49,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:49,509][root][INFO] - Training Epoch: 1/2, step 10088/23838 completed (loss: 1.4706000089645386, acc: 0.5849056839942932)
[2025-02-16 12:41:49,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:49,949][root][INFO] - Training Epoch: 1/2, step 10089/23838 completed (loss: 1.6676127910614014, acc: 0.5714285969734192)
[2025-02-16 12:41:50,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:50,379][root][INFO] - Training Epoch: 1/2, step 10090/23838 completed (loss: 1.9431078433990479, acc: 0.4193548262119293)
[2025-02-16 12:41:50,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:50,856][root][INFO] - Training Epoch: 1/2, step 10091/23838 completed (loss: 0.8299950361251831, acc: 0.7777777910232544)
[2025-02-16 12:41:51,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:51,294][root][INFO] - Training Epoch: 1/2, step 10092/23838 completed (loss: 0.44137635827064514, acc: 0.9137930870056152)
[2025-02-16 12:41:51,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:51,747][root][INFO] - Training Epoch: 1/2, step 10093/23838 completed (loss: 0.38757455348968506, acc: 0.8974359035491943)
[2025-02-16 12:41:51,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:52,140][root][INFO] - Training Epoch: 1/2, step 10094/23838 completed (loss: 1.668346881866455, acc: 0.40625)
[2025-02-16 12:41:52,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:52,600][root][INFO] - Training Epoch: 1/2, step 10095/23838 completed (loss: 1.466894507408142, acc: 0.5714285969734192)
[2025-02-16 12:41:52,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:53,011][root][INFO] - Training Epoch: 1/2, step 10096/23838 completed (loss: 1.3275729417800903, acc: 0.625)
[2025-02-16 12:41:53,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:53,462][root][INFO] - Training Epoch: 1/2, step 10097/23838 completed (loss: 0.9037005305290222, acc: 0.694915235042572)
[2025-02-16 12:41:53,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:53,936][root][INFO] - Training Epoch: 1/2, step 10098/23838 completed (loss: 0.8409510254859924, acc: 0.7291666865348816)
[2025-02-16 12:41:54,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:54,430][root][INFO] - Training Epoch: 1/2, step 10099/23838 completed (loss: 1.1246910095214844, acc: 0.6265060305595398)
[2025-02-16 12:41:54,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:54,918][root][INFO] - Training Epoch: 1/2, step 10100/23838 completed (loss: 0.7243863344192505, acc: 0.7692307829856873)
[2025-02-16 12:41:55,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:55,548][root][INFO] - Training Epoch: 1/2, step 10101/23838 completed (loss: 0.9563119411468506, acc: 0.7179487347602844)
[2025-02-16 12:41:55,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:56,178][root][INFO] - Training Epoch: 1/2, step 10102/23838 completed (loss: 0.6411964297294617, acc: 0.8048780560493469)
[2025-02-16 12:41:56,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:56,629][root][INFO] - Training Epoch: 1/2, step 10103/23838 completed (loss: 0.5526134967803955, acc: 0.8372092843055725)
[2025-02-16 12:41:56,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:57,009][root][INFO] - Training Epoch: 1/2, step 10104/23838 completed (loss: 1.0068943500518799, acc: 0.6800000071525574)
[2025-02-16 12:41:57,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:57,508][root][INFO] - Training Epoch: 1/2, step 10105/23838 completed (loss: 1.4212703704833984, acc: 0.6388888955116272)
[2025-02-16 12:41:57,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:57,932][root][INFO] - Training Epoch: 1/2, step 10106/23838 completed (loss: 1.0509456396102905, acc: 0.695652186870575)
[2025-02-16 12:41:58,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:58,427][root][INFO] - Training Epoch: 1/2, step 10107/23838 completed (loss: 0.9184725880622864, acc: 0.746268630027771)
[2025-02-16 12:41:58,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:58,944][root][INFO] - Training Epoch: 1/2, step 10108/23838 completed (loss: 0.6315605044364929, acc: 0.800000011920929)
[2025-02-16 12:41:59,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:41:59,624][root][INFO] - Training Epoch: 1/2, step 10109/23838 completed (loss: 0.8292049169540405, acc: 0.7623762488365173)
[2025-02-16 12:41:59,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:00,104][root][INFO] - Training Epoch: 1/2, step 10110/23838 completed (loss: 0.9527191519737244, acc: 0.7931034564971924)
[2025-02-16 12:42:00,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:00,542][root][INFO] - Training Epoch: 1/2, step 10111/23838 completed (loss: 0.6525710225105286, acc: 0.8205128312110901)
[2025-02-16 12:42:00,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:01,054][root][INFO] - Training Epoch: 1/2, step 10112/23838 completed (loss: 0.9630138874053955, acc: 0.7631579041481018)
[2025-02-16 12:42:01,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:01,585][root][INFO] - Training Epoch: 1/2, step 10113/23838 completed (loss: 1.0340218544006348, acc: 0.7115384340286255)
[2025-02-16 12:42:01,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:02,157][root][INFO] - Training Epoch: 1/2, step 10114/23838 completed (loss: 1.2528983354568481, acc: 0.6438356041908264)
[2025-02-16 12:42:02,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:02,577][root][INFO] - Training Epoch: 1/2, step 10115/23838 completed (loss: 0.46234381198883057, acc: 0.8780487775802612)
[2025-02-16 12:42:02,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:02,998][root][INFO] - Training Epoch: 1/2, step 10116/23838 completed (loss: 0.5936923623085022, acc: 0.7843137383460999)
[2025-02-16 12:42:03,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:03,468][root][INFO] - Training Epoch: 1/2, step 10117/23838 completed (loss: 0.8477693796157837, acc: 0.760869562625885)
[2025-02-16 12:42:03,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:04,093][root][INFO] - Training Epoch: 1/2, step 10118/23838 completed (loss: 1.0865732431411743, acc: 0.6666666865348816)
[2025-02-16 12:42:04,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:04,587][root][INFO] - Training Epoch: 1/2, step 10119/23838 completed (loss: 1.1839336156845093, acc: 0.6388888955116272)
[2025-02-16 12:42:04,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:05,292][root][INFO] - Training Epoch: 1/2, step 10120/23838 completed (loss: 1.0401266813278198, acc: 0.7123287916183472)
[2025-02-16 12:42:05,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:05,757][root][INFO] - Training Epoch: 1/2, step 10121/23838 completed (loss: 1.221992015838623, acc: 0.6451612710952759)
[2025-02-16 12:42:06,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:06,376][root][INFO] - Training Epoch: 1/2, step 10122/23838 completed (loss: 0.9346941709518433, acc: 0.7735849022865295)
[2025-02-16 12:42:06,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:06,877][root][INFO] - Training Epoch: 1/2, step 10123/23838 completed (loss: 0.8229631185531616, acc: 0.7761194109916687)
[2025-02-16 12:42:07,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:07,392][root][INFO] - Training Epoch: 1/2, step 10124/23838 completed (loss: 1.1679731607437134, acc: 0.6831682920455933)
[2025-02-16 12:42:07,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:07,807][root][INFO] - Training Epoch: 1/2, step 10125/23838 completed (loss: 0.5031577348709106, acc: 0.8627451062202454)
[2025-02-16 12:42:08,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:08,301][root][INFO] - Training Epoch: 1/2, step 10126/23838 completed (loss: 1.0696868896484375, acc: 0.6666666865348816)
[2025-02-16 12:42:08,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:08,757][root][INFO] - Training Epoch: 1/2, step 10127/23838 completed (loss: 1.0827175378799438, acc: 0.6976743936538696)
[2025-02-16 12:42:09,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:09,298][root][INFO] - Training Epoch: 1/2, step 10128/23838 completed (loss: 1.1470221281051636, acc: 0.6969696879386902)
[2025-02-16 12:42:09,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:09,887][root][INFO] - Training Epoch: 1/2, step 10129/23838 completed (loss: 1.1251941919326782, acc: 0.7250000238418579)
[2025-02-16 12:42:10,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:10,300][root][INFO] - Training Epoch: 1/2, step 10130/23838 completed (loss: 0.7728167772293091, acc: 0.7857142686843872)
[2025-02-16 12:42:10,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:10,700][root][INFO] - Training Epoch: 1/2, step 10131/23838 completed (loss: 1.1057319641113281, acc: 0.7272727489471436)
[2025-02-16 12:42:10,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:11,191][root][INFO] - Training Epoch: 1/2, step 10132/23838 completed (loss: 0.7337636947631836, acc: 0.8139534592628479)
[2025-02-16 12:42:11,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:11,797][root][INFO] - Training Epoch: 1/2, step 10133/23838 completed (loss: 0.6429706811904907, acc: 0.8199999928474426)
[2025-02-16 12:42:12,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:12,788][root][INFO] - Training Epoch: 1/2, step 10134/23838 completed (loss: 1.5563673973083496, acc: 0.5)
[2025-02-16 12:42:12,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:13,256][root][INFO] - Training Epoch: 1/2, step 10135/23838 completed (loss: 0.6364836692810059, acc: 0.8260869383811951)
[2025-02-16 12:42:13,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:13,714][root][INFO] - Training Epoch: 1/2, step 10136/23838 completed (loss: 1.6199467182159424, acc: 0.5483871102333069)
[2025-02-16 12:42:13,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:14,165][root][INFO] - Training Epoch: 1/2, step 10137/23838 completed (loss: 0.6597535610198975, acc: 0.8260869383811951)
[2025-02-16 12:42:14,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:14,597][root][INFO] - Training Epoch: 1/2, step 10138/23838 completed (loss: 0.8004693388938904, acc: 0.7142857313156128)
[2025-02-16 12:42:14,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:15,049][root][INFO] - Training Epoch: 1/2, step 10139/23838 completed (loss: 1.0104550123214722, acc: 0.7460317611694336)
[2025-02-16 12:42:15,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:15,833][root][INFO] - Training Epoch: 1/2, step 10140/23838 completed (loss: 1.1598199605941772, acc: 0.6333333253860474)
[2025-02-16 12:42:16,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:16,256][root][INFO] - Training Epoch: 1/2, step 10141/23838 completed (loss: 1.8929582834243774, acc: 0.5319148898124695)
[2025-02-16 12:42:16,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:16,663][root][INFO] - Training Epoch: 1/2, step 10142/23838 completed (loss: 0.8146634101867676, acc: 0.7454545497894287)
[2025-02-16 12:42:16,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:17,110][root][INFO] - Training Epoch: 1/2, step 10143/23838 completed (loss: 0.943265974521637, acc: 0.7358490824699402)
[2025-02-16 12:42:17,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:17,507][root][INFO] - Training Epoch: 1/2, step 10144/23838 completed (loss: 1.075928807258606, acc: 0.7142857313156128)
[2025-02-16 12:42:17,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:17,919][root][INFO] - Training Epoch: 1/2, step 10145/23838 completed (loss: 1.0357134342193604, acc: 0.6190476417541504)
[2025-02-16 12:42:18,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:18,392][root][INFO] - Training Epoch: 1/2, step 10146/23838 completed (loss: 0.9445604681968689, acc: 0.7307692170143127)
[2025-02-16 12:42:18,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:18,843][root][INFO] - Training Epoch: 1/2, step 10147/23838 completed (loss: 0.5489897131919861, acc: 0.8611111044883728)
[2025-02-16 12:42:19,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:19,310][root][INFO] - Training Epoch: 1/2, step 10148/23838 completed (loss: 0.8868589997291565, acc: 0.7692307829856873)
[2025-02-16 12:42:19,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:19,817][root][INFO] - Training Epoch: 1/2, step 10149/23838 completed (loss: 0.9817466139793396, acc: 0.7068965435028076)
[2025-02-16 12:42:20,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:20,256][root][INFO] - Training Epoch: 1/2, step 10150/23838 completed (loss: 0.6501991152763367, acc: 0.8181818127632141)
[2025-02-16 12:42:20,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:20,820][root][INFO] - Training Epoch: 1/2, step 10151/23838 completed (loss: 1.1683579683303833, acc: 0.6979166865348816)
[2025-02-16 12:42:21,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:21,411][root][INFO] - Training Epoch: 1/2, step 10152/23838 completed (loss: 1.102730631828308, acc: 0.7096773982048035)
[2025-02-16 12:42:21,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:21,825][root][INFO] - Training Epoch: 1/2, step 10153/23838 completed (loss: 1.5490126609802246, acc: 0.5714285969734192)
[2025-02-16 12:42:22,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:22,327][root][INFO] - Training Epoch: 1/2, step 10154/23838 completed (loss: 0.7037795186042786, acc: 0.7872340679168701)
[2025-02-16 12:42:22,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:22,773][root][INFO] - Training Epoch: 1/2, step 10155/23838 completed (loss: 1.373709797859192, acc: 0.5555555820465088)
[2025-02-16 12:42:22,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:23,159][root][INFO] - Training Epoch: 1/2, step 10156/23838 completed (loss: 0.617275059223175, acc: 0.8472222089767456)
[2025-02-16 12:42:23,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:23,599][root][INFO] - Training Epoch: 1/2, step 10157/23838 completed (loss: 1.4302928447723389, acc: 0.5714285969734192)
[2025-02-16 12:42:23,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:24,070][root][INFO] - Training Epoch: 1/2, step 10158/23838 completed (loss: 0.7054178714752197, acc: 0.8275862336158752)
[2025-02-16 12:42:24,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:24,473][root][INFO] - Training Epoch: 1/2, step 10159/23838 completed (loss: 1.1584159135818481, acc: 0.6804123520851135)
[2025-02-16 12:42:24,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:24,890][root][INFO] - Training Epoch: 1/2, step 10160/23838 completed (loss: 0.9734429121017456, acc: 0.75)
[2025-02-16 12:42:25,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:25,301][root][INFO] - Training Epoch: 1/2, step 10161/23838 completed (loss: 0.43789875507354736, acc: 0.8735632300376892)
[2025-02-16 12:42:25,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:25,866][root][INFO] - Training Epoch: 1/2, step 10162/23838 completed (loss: 1.1013305187225342, acc: 0.692307710647583)
[2025-02-16 12:42:26,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:26,347][root][INFO] - Training Epoch: 1/2, step 10163/23838 completed (loss: 0.5514288544654846, acc: 0.8333333134651184)
[2025-02-16 12:42:26,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:26,810][root][INFO] - Training Epoch: 1/2, step 10164/23838 completed (loss: 1.2102131843566895, acc: 0.695652186870575)
[2025-02-16 12:42:27,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:27,253][root][INFO] - Training Epoch: 1/2, step 10165/23838 completed (loss: 1.0235775709152222, acc: 0.7115384340286255)
[2025-02-16 12:42:27,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:27,661][root][INFO] - Training Epoch: 1/2, step 10166/23838 completed (loss: 1.1013439893722534, acc: 0.6851851940155029)
[2025-02-16 12:42:27,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:28,093][root][INFO] - Training Epoch: 1/2, step 10167/23838 completed (loss: 0.8677682280540466, acc: 0.717391312122345)
[2025-02-16 12:42:28,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:28,545][root][INFO] - Training Epoch: 1/2, step 10168/23838 completed (loss: 1.3216798305511475, acc: 0.6851851940155029)
[2025-02-16 12:42:28,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:29,006][root][INFO] - Training Epoch: 1/2, step 10169/23838 completed (loss: 2.0026376247406006, acc: 0.5151515007019043)
[2025-02-16 12:42:29,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:29,979][root][INFO] - Training Epoch: 1/2, step 10170/23838 completed (loss: 1.2520910501480103, acc: 0.6279069781303406)
[2025-02-16 12:42:30,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:30,400][root][INFO] - Training Epoch: 1/2, step 10171/23838 completed (loss: 0.8883634805679321, acc: 0.6842105388641357)
[2025-02-16 12:42:30,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:30,968][root][INFO] - Training Epoch: 1/2, step 10172/23838 completed (loss: 0.9499247074127197, acc: 0.7882353067398071)
[2025-02-16 12:42:31,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:31,510][root][INFO] - Training Epoch: 1/2, step 10173/23838 completed (loss: 1.100584626197815, acc: 0.6764705777168274)
[2025-02-16 12:42:31,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:32,003][root][INFO] - Training Epoch: 1/2, step 10174/23838 completed (loss: 0.796743631362915, acc: 0.7796609997749329)
[2025-02-16 12:42:32,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:32,471][root][INFO] - Training Epoch: 1/2, step 10175/23838 completed (loss: 0.893446147441864, acc: 0.7415730357170105)
[2025-02-16 12:42:32,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:33,007][root][INFO] - Training Epoch: 1/2, step 10176/23838 completed (loss: 0.5655291080474854, acc: 0.8169013857841492)
[2025-02-16 12:42:33,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:33,947][root][INFO] - Training Epoch: 1/2, step 10177/23838 completed (loss: 1.1031640768051147, acc: 0.6615384817123413)
[2025-02-16 12:42:34,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:34,713][root][INFO] - Training Epoch: 1/2, step 10178/23838 completed (loss: 1.1551504135131836, acc: 0.6909090876579285)
[2025-02-16 12:42:34,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:35,174][root][INFO] - Training Epoch: 1/2, step 10179/23838 completed (loss: 1.067920446395874, acc: 0.760869562625885)
[2025-02-16 12:42:35,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:36,236][root][INFO] - Training Epoch: 1/2, step 10180/23838 completed (loss: 1.0007978677749634, acc: 0.7346938848495483)
[2025-02-16 12:42:36,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:36,773][root][INFO] - Training Epoch: 1/2, step 10181/23838 completed (loss: 1.1132701635360718, acc: 0.6818181872367859)
[2025-02-16 12:42:37,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:37,318][root][INFO] - Training Epoch: 1/2, step 10182/23838 completed (loss: 1.091399908065796, acc: 0.6849315166473389)
[2025-02-16 12:42:37,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:37,867][root][INFO] - Training Epoch: 1/2, step 10183/23838 completed (loss: 0.6598471403121948, acc: 0.8275862336158752)
[2025-02-16 12:42:38,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:38,358][root][INFO] - Training Epoch: 1/2, step 10184/23838 completed (loss: 1.226864218711853, acc: 0.7027027010917664)
[2025-02-16 12:42:38,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:39,116][root][INFO] - Training Epoch: 1/2, step 10185/23838 completed (loss: 0.6356661915779114, acc: 0.8486486673355103)
[2025-02-16 12:42:39,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:39,907][root][INFO] - Training Epoch: 1/2, step 10186/23838 completed (loss: 0.6119688153266907, acc: 0.8214285969734192)
[2025-02-16 12:42:40,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:40,519][root][INFO] - Training Epoch: 1/2, step 10187/23838 completed (loss: 0.5962924957275391, acc: 0.8031495809555054)
[2025-02-16 12:42:40,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:40,959][root][INFO] - Training Epoch: 1/2, step 10188/23838 completed (loss: 1.0112483501434326, acc: 0.72826087474823)
[2025-02-16 12:42:41,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:41,396][root][INFO] - Training Epoch: 1/2, step 10189/23838 completed (loss: 0.7271732091903687, acc: 0.8260869383811951)
[2025-02-16 12:42:41,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:41,832][root][INFO] - Training Epoch: 1/2, step 10190/23838 completed (loss: 0.48289522528648376, acc: 0.8650793433189392)
[2025-02-16 12:42:42,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:42,325][root][INFO] - Training Epoch: 1/2, step 10191/23838 completed (loss: 0.58344966173172, acc: 0.8507462739944458)
[2025-02-16 12:42:42,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:42,762][root][INFO] - Training Epoch: 1/2, step 10192/23838 completed (loss: 0.45072945952415466, acc: 0.8604651093482971)
[2025-02-16 12:42:42,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:43,240][root][INFO] - Training Epoch: 1/2, step 10193/23838 completed (loss: 0.4522032141685486, acc: 0.8793103694915771)
[2025-02-16 12:42:43,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:43,656][root][INFO] - Training Epoch: 1/2, step 10194/23838 completed (loss: 0.5671424269676208, acc: 0.8421052694320679)
[2025-02-16 12:42:43,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:44,153][root][INFO] - Training Epoch: 1/2, step 10195/23838 completed (loss: 0.4440195858478546, acc: 0.8536585569381714)
[2025-02-16 12:42:44,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:44,608][root][INFO] - Training Epoch: 1/2, step 10196/23838 completed (loss: 0.47859278321266174, acc: 0.8673469424247742)
[2025-02-16 12:42:45,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:45,499][root][INFO] - Training Epoch: 1/2, step 10197/23838 completed (loss: 0.943882405757904, acc: 0.7592592835426331)
[2025-02-16 12:42:45,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:45,888][root][INFO] - Training Epoch: 1/2, step 10198/23838 completed (loss: 0.5371628999710083, acc: 0.875)
[2025-02-16 12:42:46,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:46,353][root][INFO] - Training Epoch: 1/2, step 10199/23838 completed (loss: 0.26493656635284424, acc: 0.9448819160461426)
[2025-02-16 12:42:46,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:46,761][root][INFO] - Training Epoch: 1/2, step 10200/23838 completed (loss: 0.32721567153930664, acc: 0.9180327653884888)
[2025-02-16 12:42:47,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:47,350][root][INFO] - Training Epoch: 1/2, step 10201/23838 completed (loss: 0.4541802704334259, acc: 0.864130437374115)
[2025-02-16 12:42:47,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:48,230][root][INFO] - Training Epoch: 1/2, step 10202/23838 completed (loss: 0.5837656855583191, acc: 0.848739504814148)
[2025-02-16 12:42:48,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:48,660][root][INFO] - Training Epoch: 1/2, step 10203/23838 completed (loss: 0.3531578779220581, acc: 0.8943662047386169)
[2025-02-16 12:42:48,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:49,172][root][INFO] - Training Epoch: 1/2, step 10204/23838 completed (loss: 0.4859268367290497, acc: 0.8793103694915771)
[2025-02-16 12:42:49,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:49,583][root][INFO] - Training Epoch: 1/2, step 10205/23838 completed (loss: 0.6988736391067505, acc: 0.8333333134651184)
[2025-02-16 12:42:49,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:50,264][root][INFO] - Training Epoch: 1/2, step 10206/23838 completed (loss: 0.5350501537322998, acc: 0.8263157606124878)
[2025-02-16 12:42:50,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:50,739][root][INFO] - Training Epoch: 1/2, step 10207/23838 completed (loss: 0.5602614283561707, acc: 0.8320000171661377)
[2025-02-16 12:42:50,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:51,202][root][INFO] - Training Epoch: 1/2, step 10208/23838 completed (loss: 0.4124026894569397, acc: 0.8799999952316284)
[2025-02-16 12:42:51,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:51,599][root][INFO] - Training Epoch: 1/2, step 10209/23838 completed (loss: 0.6343403458595276, acc: 0.8541666865348816)
[2025-02-16 12:42:51,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:52,234][root][INFO] - Training Epoch: 1/2, step 10210/23838 completed (loss: 0.7264242768287659, acc: 0.8021978139877319)
[2025-02-16 12:42:52,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:52,760][root][INFO] - Training Epoch: 1/2, step 10211/23838 completed (loss: 0.7090039849281311, acc: 0.7721518874168396)
[2025-02-16 12:42:52,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:53,201][root][INFO] - Training Epoch: 1/2, step 10212/23838 completed (loss: 0.37220534682273865, acc: 0.8853503465652466)
[2025-02-16 12:42:53,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:53,671][root][INFO] - Training Epoch: 1/2, step 10213/23838 completed (loss: 0.5191068053245544, acc: 0.8539325594902039)
[2025-02-16 12:42:53,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:54,094][root][INFO] - Training Epoch: 1/2, step 10214/23838 completed (loss: 0.36061546206474304, acc: 0.8791208863258362)
[2025-02-16 12:42:54,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:54,467][root][INFO] - Training Epoch: 1/2, step 10215/23838 completed (loss: 0.7328886389732361, acc: 0.8227847814559937)
[2025-02-16 12:42:54,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:54,920][root][INFO] - Training Epoch: 1/2, step 10216/23838 completed (loss: 0.6476516723632812, acc: 0.8229166865348816)
[2025-02-16 12:42:55,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:55,695][root][INFO] - Training Epoch: 1/2, step 10217/23838 completed (loss: 0.6196717619895935, acc: 0.8674699068069458)
[2025-02-16 12:42:55,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:56,136][root][INFO] - Training Epoch: 1/2, step 10218/23838 completed (loss: 0.34831005334854126, acc: 0.9113923907279968)
[2025-02-16 12:42:56,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:56,876][root][INFO] - Training Epoch: 1/2, step 10219/23838 completed (loss: 0.4584362804889679, acc: 0.8695651888847351)
[2025-02-16 12:42:57,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:57,305][root][INFO] - Training Epoch: 1/2, step 10220/23838 completed (loss: 0.5116172432899475, acc: 0.8271604776382446)
[2025-02-16 12:42:57,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:57,801][root][INFO] - Training Epoch: 1/2, step 10221/23838 completed (loss: 0.24688328802585602, acc: 0.9346405267715454)
[2025-02-16 12:42:58,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:58,669][root][INFO] - Training Epoch: 1/2, step 10222/23838 completed (loss: 0.47897985577583313, acc: 0.8702290058135986)
[2025-02-16 12:42:58,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:59,330][root][INFO] - Training Epoch: 1/2, step 10223/23838 completed (loss: 0.4963180124759674, acc: 0.8679245114326477)
[2025-02-16 12:42:59,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:42:59,813][root][INFO] - Training Epoch: 1/2, step 10224/23838 completed (loss: 0.31013956665992737, acc: 0.9436619877815247)
[2025-02-16 12:43:00,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:00,270][root][INFO] - Training Epoch: 1/2, step 10225/23838 completed (loss: 0.3937794268131256, acc: 0.8898305296897888)
[2025-02-16 12:43:00,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:00,966][root][INFO] - Training Epoch: 1/2, step 10226/23838 completed (loss: 0.4323104918003082, acc: 0.8971962332725525)
[2025-02-16 12:43:01,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:01,390][root][INFO] - Training Epoch: 1/2, step 10227/23838 completed (loss: 0.890876054763794, acc: 0.752212405204773)
[2025-02-16 12:43:01,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:01,887][root][INFO] - Training Epoch: 1/2, step 10228/23838 completed (loss: 0.6439173817634583, acc: 0.8211382031440735)
[2025-02-16 12:43:02,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:02,514][root][INFO] - Training Epoch: 1/2, step 10229/23838 completed (loss: 0.5056080222129822, acc: 0.8661417365074158)
[2025-02-16 12:43:02,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:03,054][root][INFO] - Training Epoch: 1/2, step 10230/23838 completed (loss: 0.37933024764060974, acc: 0.8947368264198303)
[2025-02-16 12:43:03,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:03,721][root][INFO] - Training Epoch: 1/2, step 10231/23838 completed (loss: 0.8119003772735596, acc: 0.761904776096344)
[2025-02-16 12:43:03,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:04,207][root][INFO] - Training Epoch: 1/2, step 10232/23838 completed (loss: 0.9563998579978943, acc: 0.7444444298744202)
[2025-02-16 12:43:04,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:04,669][root][INFO] - Training Epoch: 1/2, step 10233/23838 completed (loss: 0.8224897980690002, acc: 0.7272727489471436)
[2025-02-16 12:43:04,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:05,194][root][INFO] - Training Epoch: 1/2, step 10234/23838 completed (loss: 0.39564934372901917, acc: 0.8684210777282715)
[2025-02-16 12:43:05,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:05,648][root][INFO] - Training Epoch: 1/2, step 10235/23838 completed (loss: 0.3753754496574402, acc: 0.8904109597206116)
[2025-02-16 12:43:05,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:06,256][root][INFO] - Training Epoch: 1/2, step 10236/23838 completed (loss: 0.6969484686851501, acc: 0.8170731663703918)
[2025-02-16 12:43:06,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:06,715][root][INFO] - Training Epoch: 1/2, step 10237/23838 completed (loss: 0.4925440549850464, acc: 0.867132842540741)
[2025-02-16 12:43:06,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:07,270][root][INFO] - Training Epoch: 1/2, step 10238/23838 completed (loss: 0.2934378981590271, acc: 0.9259259104728699)
[2025-02-16 12:43:07,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:08,148][root][INFO] - Training Epoch: 1/2, step 10239/23838 completed (loss: 0.9582884311676025, acc: 0.7635135054588318)
[2025-02-16 12:43:08,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:08,591][root][INFO] - Training Epoch: 1/2, step 10240/23838 completed (loss: 0.5834360718727112, acc: 0.8421052694320679)
[2025-02-16 12:43:08,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:08,993][root][INFO] - Training Epoch: 1/2, step 10241/23838 completed (loss: 0.3742890954017639, acc: 0.893203854560852)
[2025-02-16 12:43:09,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:09,376][root][INFO] - Training Epoch: 1/2, step 10242/23838 completed (loss: 0.3632558584213257, acc: 0.8823529481887817)
[2025-02-16 12:43:09,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:09,895][root][INFO] - Training Epoch: 1/2, step 10243/23838 completed (loss: 0.6122353672981262, acc: 0.8220338821411133)
[2025-02-16 12:43:10,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:10,344][root][INFO] - Training Epoch: 1/2, step 10244/23838 completed (loss: 0.3402799963951111, acc: 0.9098360538482666)
[2025-02-16 12:43:10,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:10,817][root][INFO] - Training Epoch: 1/2, step 10245/23838 completed (loss: 0.4562804698944092, acc: 0.8970588445663452)
[2025-02-16 12:43:11,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:11,230][root][INFO] - Training Epoch: 1/2, step 10246/23838 completed (loss: 0.13724638521671295, acc: 0.9626168012619019)
[2025-02-16 12:43:11,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:11,612][root][INFO] - Training Epoch: 1/2, step 10247/23838 completed (loss: 0.24373823404312134, acc: 0.9210526347160339)
[2025-02-16 12:43:11,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:12,057][root][INFO] - Training Epoch: 1/2, step 10248/23838 completed (loss: 0.4411912262439728, acc: 0.855555534362793)
[2025-02-16 12:43:12,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:12,533][root][INFO] - Training Epoch: 1/2, step 10249/23838 completed (loss: 0.5246060490608215, acc: 0.8461538553237915)
[2025-02-16 12:43:12,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:13,001][root][INFO] - Training Epoch: 1/2, step 10250/23838 completed (loss: 0.39314597845077515, acc: 0.8790322542190552)
[2025-02-16 12:43:13,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:13,486][root][INFO] - Training Epoch: 1/2, step 10251/23838 completed (loss: 0.33493396639823914, acc: 0.8783783912658691)
[2025-02-16 12:43:13,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:13,935][root][INFO] - Training Epoch: 1/2, step 10252/23838 completed (loss: 0.44756075739860535, acc: 0.8695651888847351)
[2025-02-16 12:43:14,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:14,432][root][INFO] - Training Epoch: 1/2, step 10253/23838 completed (loss: 0.3583003878593445, acc: 0.8999999761581421)
[2025-02-16 12:43:14,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:14,944][root][INFO] - Training Epoch: 1/2, step 10254/23838 completed (loss: 0.6128482222557068, acc: 0.849056601524353)
[2025-02-16 12:43:15,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:15,344][root][INFO] - Training Epoch: 1/2, step 10255/23838 completed (loss: 0.43381112813949585, acc: 0.8803418874740601)
[2025-02-16 12:43:15,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:15,792][root][INFO] - Training Epoch: 1/2, step 10256/23838 completed (loss: 0.48491808772087097, acc: 0.8783783912658691)
[2025-02-16 12:43:16,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:16,320][root][INFO] - Training Epoch: 1/2, step 10257/23838 completed (loss: 0.38067081570625305, acc: 0.881118893623352)
[2025-02-16 12:43:16,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:16,888][root][INFO] - Training Epoch: 1/2, step 10258/23838 completed (loss: 0.416543573141098, acc: 0.8695651888847351)
[2025-02-16 12:43:17,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:17,385][root][INFO] - Training Epoch: 1/2, step 10259/23838 completed (loss: 0.249801367521286, acc: 0.9266055226325989)
[2025-02-16 12:43:17,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:17,876][root][INFO] - Training Epoch: 1/2, step 10260/23838 completed (loss: 0.23113711178302765, acc: 0.9389312863349915)
[2025-02-16 12:43:18,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:18,361][root][INFO] - Training Epoch: 1/2, step 10261/23838 completed (loss: 0.32867953181266785, acc: 0.8934426307678223)
[2025-02-16 12:43:18,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:18,923][root][INFO] - Training Epoch: 1/2, step 10262/23838 completed (loss: 0.374166339635849, acc: 0.8979591727256775)
[2025-02-16 12:43:19,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:19,512][root][INFO] - Training Epoch: 1/2, step 10263/23838 completed (loss: 0.46196773648262024, acc: 0.8442623019218445)
[2025-02-16 12:43:19,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:20,232][root][INFO] - Training Epoch: 1/2, step 10264/23838 completed (loss: 0.3759796619415283, acc: 0.9085714221000671)
[2025-02-16 12:43:20,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:20,783][root][INFO] - Training Epoch: 1/2, step 10265/23838 completed (loss: 0.5205450057983398, acc: 0.8547008633613586)
[2025-02-16 12:43:21,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:21,776][root][INFO] - Training Epoch: 1/2, step 10266/23838 completed (loss: 0.5292243361473083, acc: 0.8666666746139526)
[2025-02-16 12:43:22,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:22,305][root][INFO] - Training Epoch: 1/2, step 10267/23838 completed (loss: 0.33478257060050964, acc: 0.8928571343421936)
[2025-02-16 12:43:22,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:22,906][root][INFO] - Training Epoch: 1/2, step 10268/23838 completed (loss: 0.522820770740509, acc: 0.8425197005271912)
[2025-02-16 12:43:23,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:23,359][root][INFO] - Training Epoch: 1/2, step 10269/23838 completed (loss: 0.7284071445465088, acc: 0.808080792427063)
[2025-02-16 12:43:23,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:23,964][root][INFO] - Training Epoch: 1/2, step 10270/23838 completed (loss: 0.2823779881000519, acc: 0.9117646813392639)
[2025-02-16 12:43:24,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:24,471][root][INFO] - Training Epoch: 1/2, step 10271/23838 completed (loss: 0.5317913293838501, acc: 0.8758620619773865)
[2025-02-16 12:43:24,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:24,962][root][INFO] - Training Epoch: 1/2, step 10272/23838 completed (loss: 0.2982025444507599, acc: 0.916201114654541)
[2025-02-16 12:43:25,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:25,387][root][INFO] - Training Epoch: 1/2, step 10273/23838 completed (loss: 0.4839208722114563, acc: 0.8539325594902039)
[2025-02-16 12:43:25,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:25,928][root][INFO] - Training Epoch: 1/2, step 10274/23838 completed (loss: 0.45586827397346497, acc: 0.8878504633903503)
[2025-02-16 12:43:26,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:26,418][root][INFO] - Training Epoch: 1/2, step 10275/23838 completed (loss: 0.6964302062988281, acc: 0.8240740895271301)
[2025-02-16 12:43:26,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:26,905][root][INFO] - Training Epoch: 1/2, step 10276/23838 completed (loss: 0.8034355044364929, acc: 0.75)
[2025-02-16 12:43:27,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:27,339][root][INFO] - Training Epoch: 1/2, step 10277/23838 completed (loss: 0.354682981967926, acc: 0.886956512928009)
[2025-02-16 12:43:27,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:27,798][root][INFO] - Training Epoch: 1/2, step 10278/23838 completed (loss: 0.21987777948379517, acc: 0.9217877388000488)
[2025-02-16 12:43:28,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:28,278][root][INFO] - Training Epoch: 1/2, step 10279/23838 completed (loss: 0.4370032250881195, acc: 0.8852459192276001)
[2025-02-16 12:43:28,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:28,633][root][INFO] - Training Epoch: 1/2, step 10280/23838 completed (loss: 0.11470316350460052, acc: 0.9605262875556946)
[2025-02-16 12:43:28,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:29,071][root][INFO] - Training Epoch: 1/2, step 10281/23838 completed (loss: 0.4358861446380615, acc: 0.8761467933654785)
[2025-02-16 12:43:29,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:29,483][root][INFO] - Training Epoch: 1/2, step 10282/23838 completed (loss: 0.15924304723739624, acc: 0.938144326210022)
[2025-02-16 12:43:29,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:30,051][root][INFO] - Training Epoch: 1/2, step 10283/23838 completed (loss: 0.40473246574401855, acc: 0.8778625726699829)
[2025-02-16 12:43:30,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:30,501][root][INFO] - Training Epoch: 1/2, step 10284/23838 completed (loss: 0.6121275424957275, acc: 0.8256880640983582)
[2025-02-16 12:43:30,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:30,983][root][INFO] - Training Epoch: 1/2, step 10285/23838 completed (loss: 0.45010849833488464, acc: 0.8823529481887817)
[2025-02-16 12:43:31,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:31,398][root][INFO] - Training Epoch: 1/2, step 10286/23838 completed (loss: 0.17050959169864655, acc: 0.955974817276001)
[2025-02-16 12:43:31,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:31,836][root][INFO] - Training Epoch: 1/2, step 10287/23838 completed (loss: 0.16130100190639496, acc: 0.9552238583564758)
[2025-02-16 12:43:32,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:32,272][root][INFO] - Training Epoch: 1/2, step 10288/23838 completed (loss: 0.32103708386421204, acc: 0.918181836605072)
[2025-02-16 12:43:32,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:32,707][root][INFO] - Training Epoch: 1/2, step 10289/23838 completed (loss: 0.557731032371521, acc: 0.8787878751754761)
[2025-02-16 12:43:32,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:33,127][root][INFO] - Training Epoch: 1/2, step 10290/23838 completed (loss: 0.1523769199848175, acc: 0.9485294222831726)
[2025-02-16 12:43:33,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:33,526][root][INFO] - Training Epoch: 1/2, step 10291/23838 completed (loss: 0.41998955607414246, acc: 0.9016393423080444)
[2025-02-16 12:43:33,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:33,976][root][INFO] - Training Epoch: 1/2, step 10292/23838 completed (loss: 0.44242334365844727, acc: 0.8861788511276245)
[2025-02-16 12:43:34,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:34,396][root][INFO] - Training Epoch: 1/2, step 10293/23838 completed (loss: 0.1394643485546112, acc: 0.9576271176338196)
[2025-02-16 12:43:34,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:34,871][root][INFO] - Training Epoch: 1/2, step 10294/23838 completed (loss: 0.2449546754360199, acc: 0.9338521361351013)
[2025-02-16 12:43:35,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:35,357][root][INFO] - Training Epoch: 1/2, step 10295/23838 completed (loss: 0.27718105912208557, acc: 0.9375)
[2025-02-16 12:43:35,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:35,910][root][INFO] - Training Epoch: 1/2, step 10296/23838 completed (loss: 0.5462446212768555, acc: 0.8705036044120789)
[2025-02-16 12:43:36,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:36,317][root][INFO] - Training Epoch: 1/2, step 10297/23838 completed (loss: 0.315593421459198, acc: 0.8989899158477783)
[2025-02-16 12:43:36,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:36,774][root][INFO] - Training Epoch: 1/2, step 10298/23838 completed (loss: 0.2164122611284256, acc: 0.9439252614974976)
[2025-02-16 12:43:36,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:37,221][root][INFO] - Training Epoch: 1/2, step 10299/23838 completed (loss: 0.47543105483055115, acc: 0.8604651093482971)
[2025-02-16 12:43:37,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:37,743][root][INFO] - Training Epoch: 1/2, step 10300/23838 completed (loss: 0.5824332237243652, acc: 0.8208954930305481)
[2025-02-16 12:43:37,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:38,147][root][INFO] - Training Epoch: 1/2, step 10301/23838 completed (loss: 0.33388638496398926, acc: 0.8980582356452942)
[2025-02-16 12:43:38,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:38,576][root][INFO] - Training Epoch: 1/2, step 10302/23838 completed (loss: 0.1842607855796814, acc: 0.9537037014961243)
[2025-02-16 12:43:38,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:39,009][root][INFO] - Training Epoch: 1/2, step 10303/23838 completed (loss: 0.2443525195121765, acc: 0.9444444179534912)
[2025-02-16 12:43:39,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:39,463][root][INFO] - Training Epoch: 1/2, step 10304/23838 completed (loss: 0.3788197934627533, acc: 0.913385808467865)
[2025-02-16 12:43:39,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:39,910][root][INFO] - Training Epoch: 1/2, step 10305/23838 completed (loss: 0.5443345308303833, acc: 0.8225806355476379)
[2025-02-16 12:43:40,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:40,320][root][INFO] - Training Epoch: 1/2, step 10306/23838 completed (loss: 0.3757341206073761, acc: 0.8648648858070374)
[2025-02-16 12:43:40,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:40,741][root][INFO] - Training Epoch: 1/2, step 10307/23838 completed (loss: 0.6797780394554138, acc: 0.7961165308952332)
[2025-02-16 12:43:40,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:41,187][root][INFO] - Training Epoch: 1/2, step 10308/23838 completed (loss: 0.385240375995636, acc: 0.8823529481887817)
[2025-02-16 12:43:41,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:41,613][root][INFO] - Training Epoch: 1/2, step 10309/23838 completed (loss: 0.3002447485923767, acc: 0.9259259104728699)
[2025-02-16 12:43:41,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:42,050][root][INFO] - Training Epoch: 1/2, step 10310/23838 completed (loss: 0.4121344983577728, acc: 0.875)
[2025-02-16 12:43:42,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:42,448][root][INFO] - Training Epoch: 1/2, step 10311/23838 completed (loss: 0.5179643630981445, acc: 0.8939393758773804)
[2025-02-16 12:43:42,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:42,911][root][INFO] - Training Epoch: 1/2, step 10312/23838 completed (loss: 0.6019510626792908, acc: 0.875)
[2025-02-16 12:43:43,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:43,316][root][INFO] - Training Epoch: 1/2, step 10313/23838 completed (loss: 0.3422413766384125, acc: 0.913385808467865)
[2025-02-16 12:43:43,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:43,874][root][INFO] - Training Epoch: 1/2, step 10314/23838 completed (loss: 0.5238796472549438, acc: 0.8729507923126221)
[2025-02-16 12:43:44,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:44,304][root][INFO] - Training Epoch: 1/2, step 10315/23838 completed (loss: 0.9315416216850281, acc: 0.7837837934494019)
[2025-02-16 12:43:44,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:44,759][root][INFO] - Training Epoch: 1/2, step 10316/23838 completed (loss: 0.08033782988786697, acc: 0.9873417615890503)
[2025-02-16 12:43:45,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:45,363][root][INFO] - Training Epoch: 1/2, step 10317/23838 completed (loss: 0.28843069076538086, acc: 0.9107142686843872)
[2025-02-16 12:43:45,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:45,802][root][INFO] - Training Epoch: 1/2, step 10318/23838 completed (loss: 0.6643240451812744, acc: 0.8205128312110901)
[2025-02-16 12:43:46,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:46,270][root][INFO] - Training Epoch: 1/2, step 10319/23838 completed (loss: 0.12584859132766724, acc: 0.970059871673584)
[2025-02-16 12:43:46,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:46,751][root][INFO] - Training Epoch: 1/2, step 10320/23838 completed (loss: 0.2781461775302887, acc: 0.9128440618515015)
[2025-02-16 12:43:46,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:47,226][root][INFO] - Training Epoch: 1/2, step 10321/23838 completed (loss: 0.9333583116531372, acc: 0.7234042286872864)
[2025-02-16 12:43:47,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:47,704][root][INFO] - Training Epoch: 1/2, step 10322/23838 completed (loss: 0.4542292058467865, acc: 0.8562091588973999)
[2025-02-16 12:43:47,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:48,187][root][INFO] - Training Epoch: 1/2, step 10323/23838 completed (loss: 0.23512324690818787, acc: 0.9424083828926086)
[2025-02-16 12:43:48,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:48,655][root][INFO] - Training Epoch: 1/2, step 10324/23838 completed (loss: 0.24195009469985962, acc: 0.9371428489685059)
[2025-02-16 12:43:49,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:49,533][root][INFO] - Training Epoch: 1/2, step 10325/23838 completed (loss: 0.24064481258392334, acc: 0.9466192126274109)
[2025-02-16 12:43:49,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:49,999][root][INFO] - Training Epoch: 1/2, step 10326/23838 completed (loss: 0.4303278625011444, acc: 0.8763440847396851)
[2025-02-16 12:43:50,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:50,489][root][INFO] - Training Epoch: 1/2, step 10327/23838 completed (loss: 0.23988516628742218, acc: 0.9082125425338745)
[2025-02-16 12:43:50,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:51,004][root][INFO] - Training Epoch: 1/2, step 10328/23838 completed (loss: 0.24376162886619568, acc: 0.9399999976158142)
[2025-02-16 12:43:51,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:51,487][root][INFO] - Training Epoch: 1/2, step 10329/23838 completed (loss: 0.3302801847457886, acc: 0.8901098966598511)
[2025-02-16 12:43:51,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:51,882][root][INFO] - Training Epoch: 1/2, step 10330/23838 completed (loss: 0.3436253070831299, acc: 0.9142857193946838)
[2025-02-16 12:43:52,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:52,256][root][INFO] - Training Epoch: 1/2, step 10331/23838 completed (loss: 0.42037954926490784, acc: 0.8852459192276001)
[2025-02-16 12:43:52,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:52,701][root][INFO] - Training Epoch: 1/2, step 10332/23838 completed (loss: 0.25500965118408203, acc: 0.9368420839309692)
[2025-02-16 12:43:52,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:53,141][root][INFO] - Training Epoch: 1/2, step 10333/23838 completed (loss: 0.2563951015472412, acc: 0.9115646481513977)
[2025-02-16 12:43:53,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:53,557][root][INFO] - Training Epoch: 1/2, step 10334/23838 completed (loss: 0.31442999839782715, acc: 0.9108280539512634)
[2025-02-16 12:43:53,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:53,938][root][INFO] - Training Epoch: 1/2, step 10335/23838 completed (loss: 0.27520909905433655, acc: 0.9113923907279968)
[2025-02-16 12:43:54,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:54,336][root][INFO] - Training Epoch: 1/2, step 10336/23838 completed (loss: 0.5222795605659485, acc: 0.8629032373428345)
[2025-02-16 12:43:54,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:54,754][root][INFO] - Training Epoch: 1/2, step 10337/23838 completed (loss: 0.23381398618221283, acc: 0.9444444179534912)
[2025-02-16 12:43:54,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:55,164][root][INFO] - Training Epoch: 1/2, step 10338/23838 completed (loss: 0.5395037531852722, acc: 0.8421052694320679)
[2025-02-16 12:43:55,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:55,587][root][INFO] - Training Epoch: 1/2, step 10339/23838 completed (loss: 0.21072451770305634, acc: 0.9459459185600281)
[2025-02-16 12:43:55,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:56,034][root][INFO] - Training Epoch: 1/2, step 10340/23838 completed (loss: 0.2274472415447235, acc: 0.9473684430122375)
[2025-02-16 12:43:56,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:56,446][root][INFO] - Training Epoch: 1/2, step 10341/23838 completed (loss: 0.1631077378988266, acc: 0.9594594836235046)
[2025-02-16 12:43:56,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:56,865][root][INFO] - Training Epoch: 1/2, step 10342/23838 completed (loss: 0.24199426174163818, acc: 0.9270073175430298)
[2025-02-16 12:43:57,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:57,365][root][INFO] - Training Epoch: 1/2, step 10343/23838 completed (loss: 0.22603310644626617, acc: 0.9468598961830139)
[2025-02-16 12:43:57,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:57,815][root][INFO] - Training Epoch: 1/2, step 10344/23838 completed (loss: 0.34077298641204834, acc: 0.8714285492897034)
[2025-02-16 12:43:57,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:58,191][root][INFO] - Training Epoch: 1/2, step 10345/23838 completed (loss: 0.2741048038005829, acc: 0.8962963223457336)
[2025-02-16 12:43:58,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:58,681][root][INFO] - Training Epoch: 1/2, step 10346/23838 completed (loss: 0.3590012490749359, acc: 0.9054054021835327)
[2025-02-16 12:43:58,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:59,126][root][INFO] - Training Epoch: 1/2, step 10347/23838 completed (loss: 0.14745329320430756, acc: 0.9479768872261047)
[2025-02-16 12:43:59,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:59,479][root][INFO] - Training Epoch: 1/2, step 10348/23838 completed (loss: 0.33756494522094727, acc: 0.9278350472450256)
[2025-02-16 12:43:59,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:43:59,930][root][INFO] - Training Epoch: 1/2, step 10349/23838 completed (loss: 0.2997003197669983, acc: 0.9444444179534912)
[2025-02-16 12:44:00,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:00,381][root][INFO] - Training Epoch: 1/2, step 10350/23838 completed (loss: 0.2350672334432602, acc: 0.949999988079071)
[2025-02-16 12:44:00,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:00,836][root][INFO] - Training Epoch: 1/2, step 10351/23838 completed (loss: 0.26989665627479553, acc: 0.9217391014099121)
[2025-02-16 12:44:01,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:01,391][root][INFO] - Training Epoch: 1/2, step 10352/23838 completed (loss: 0.15823641419410706, acc: 0.9545454382896423)
[2025-02-16 12:44:01,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:01,866][root][INFO] - Training Epoch: 1/2, step 10353/23838 completed (loss: 0.27288490533828735, acc: 0.9435483813285828)
[2025-02-16 12:44:02,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:02,408][root][INFO] - Training Epoch: 1/2, step 10354/23838 completed (loss: 0.3397503197193146, acc: 0.9256756901741028)
[2025-02-16 12:44:02,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:03,282][root][INFO] - Training Epoch: 1/2, step 10355/23838 completed (loss: 0.31893855333328247, acc: 0.9176470637321472)
[2025-02-16 12:44:03,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:03,712][root][INFO] - Training Epoch: 1/2, step 10356/23838 completed (loss: 0.3187023997306824, acc: 0.9193548560142517)
[2025-02-16 12:44:03,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:04,140][root][INFO] - Training Epoch: 1/2, step 10357/23838 completed (loss: 0.1635507047176361, acc: 0.9669421315193176)
[2025-02-16 12:44:04,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:04,533][root][INFO] - Training Epoch: 1/2, step 10358/23838 completed (loss: 0.3015058934688568, acc: 0.9345238208770752)
[2025-02-16 12:44:04,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:04,990][root][INFO] - Training Epoch: 1/2, step 10359/23838 completed (loss: 0.6567962765693665, acc: 0.8039215803146362)
[2025-02-16 12:44:05,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:05,941][root][INFO] - Training Epoch: 1/2, step 10360/23838 completed (loss: 0.22482648491859436, acc: 0.9514563083648682)
[2025-02-16 12:44:06,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:06,408][root][INFO] - Training Epoch: 1/2, step 10361/23838 completed (loss: 0.3942381739616394, acc: 0.9189189076423645)
[2025-02-16 12:44:06,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:06,852][root][INFO] - Training Epoch: 1/2, step 10362/23838 completed (loss: 0.3024035394191742, acc: 0.9113923907279968)
[2025-02-16 12:44:07,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:07,237][root][INFO] - Training Epoch: 1/2, step 10363/23838 completed (loss: 0.42500734329223633, acc: 0.9157894849777222)
[2025-02-16 12:44:07,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:07,626][root][INFO] - Training Epoch: 1/2, step 10364/23838 completed (loss: 0.16174103319644928, acc: 0.9629629850387573)
[2025-02-16 12:44:07,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:08,087][root][INFO] - Training Epoch: 1/2, step 10365/23838 completed (loss: 0.19163450598716736, acc: 0.939393937587738)
[2025-02-16 12:44:08,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:08,551][root][INFO] - Training Epoch: 1/2, step 10366/23838 completed (loss: 0.4684370458126068, acc: 0.8787878751754761)
[2025-02-16 12:44:08,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:09,009][root][INFO] - Training Epoch: 1/2, step 10367/23838 completed (loss: 0.3281664550304413, acc: 0.9039999842643738)
[2025-02-16 12:44:09,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:09,443][root][INFO] - Training Epoch: 1/2, step 10368/23838 completed (loss: 0.5878727436065674, acc: 0.8684210777282715)
[2025-02-16 12:44:09,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:09,886][root][INFO] - Training Epoch: 1/2, step 10369/23838 completed (loss: 0.2559325397014618, acc: 0.9203540086746216)
[2025-02-16 12:44:10,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:10,282][root][INFO] - Training Epoch: 1/2, step 10370/23838 completed (loss: 0.31641823053359985, acc: 0.8859060406684875)
[2025-02-16 12:44:10,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:10,734][root][INFO] - Training Epoch: 1/2, step 10371/23838 completed (loss: 0.46922460198402405, acc: 0.8928571343421936)
[2025-02-16 12:44:10,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:11,162][root][INFO] - Training Epoch: 1/2, step 10372/23838 completed (loss: 0.6354824900627136, acc: 0.8453608155250549)
[2025-02-16 12:44:11,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:11,595][root][INFO] - Training Epoch: 1/2, step 10373/23838 completed (loss: 0.3221706449985504, acc: 0.8941798806190491)
[2025-02-16 12:44:11,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:12,025][root][INFO] - Training Epoch: 1/2, step 10374/23838 completed (loss: 0.4279497265815735, acc: 0.8804348111152649)
[2025-02-16 12:44:12,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:12,465][root][INFO] - Training Epoch: 1/2, step 10375/23838 completed (loss: 0.325361430644989, acc: 0.915032684803009)
[2025-02-16 12:44:12,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:12,921][root][INFO] - Training Epoch: 1/2, step 10376/23838 completed (loss: 0.6499266028404236, acc: 0.8347107172012329)
[2025-02-16 12:44:13,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:13,345][root][INFO] - Training Epoch: 1/2, step 10377/23838 completed (loss: 0.3099852204322815, acc: 0.9024389982223511)
[2025-02-16 12:44:13,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:13,796][root][INFO] - Training Epoch: 1/2, step 10378/23838 completed (loss: 0.2843245267868042, acc: 0.929729700088501)
[2025-02-16 12:44:14,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:14,250][root][INFO] - Training Epoch: 1/2, step 10379/23838 completed (loss: 0.4108445346355438, acc: 0.8888888955116272)
[2025-02-16 12:44:14,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:14,678][root][INFO] - Training Epoch: 1/2, step 10380/23838 completed (loss: 0.22903817892074585, acc: 0.9411764740943909)
[2025-02-16 12:44:14,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:15,134][root][INFO] - Training Epoch: 1/2, step 10381/23838 completed (loss: 0.11834615468978882, acc: 0.9710982441902161)
[2025-02-16 12:44:15,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:15,532][root][INFO] - Training Epoch: 1/2, step 10382/23838 completed (loss: 0.23737779259681702, acc: 0.9280575513839722)
[2025-02-16 12:44:15,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:15,995][root][INFO] - Training Epoch: 1/2, step 10383/23838 completed (loss: 0.5186086297035217, acc: 0.8802816867828369)
[2025-02-16 12:44:16,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:16,654][root][INFO] - Training Epoch: 1/2, step 10384/23838 completed (loss: 0.3352693021297455, acc: 0.9030836820602417)
[2025-02-16 12:44:16,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:17,114][root][INFO] - Training Epoch: 1/2, step 10385/23838 completed (loss: 0.4052770733833313, acc: 0.8944099545478821)
[2025-02-16 12:44:17,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:17,470][root][INFO] - Training Epoch: 1/2, step 10386/23838 completed (loss: 0.4155385494232178, acc: 0.8552631735801697)
[2025-02-16 12:44:17,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:17,904][root][INFO] - Training Epoch: 1/2, step 10387/23838 completed (loss: 0.48969000577926636, acc: 0.8510638475418091)
[2025-02-16 12:44:18,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:18,377][root][INFO] - Training Epoch: 1/2, step 10388/23838 completed (loss: 0.20361775159835815, acc: 0.9425287246704102)
[2025-02-16 12:44:18,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:18,792][root][INFO] - Training Epoch: 1/2, step 10389/23838 completed (loss: 0.3824608027935028, acc: 0.8655462265014648)
[2025-02-16 12:44:18,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:19,194][root][INFO] - Training Epoch: 1/2, step 10390/23838 completed (loss: 0.46242502331733704, acc: 0.8947368264198303)
[2025-02-16 12:44:19,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:19,624][root][INFO] - Training Epoch: 1/2, step 10391/23838 completed (loss: 0.41788801550865173, acc: 0.9047619104385376)
[2025-02-16 12:44:19,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:20,083][root][INFO] - Training Epoch: 1/2, step 10392/23838 completed (loss: 0.40323734283447266, acc: 0.875)
[2025-02-16 12:44:20,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:20,516][root][INFO] - Training Epoch: 1/2, step 10393/23838 completed (loss: 0.9559558629989624, acc: 0.7304964661598206)
[2025-02-16 12:44:20,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:20,983][root][INFO] - Training Epoch: 1/2, step 10394/23838 completed (loss: 0.4406171441078186, acc: 0.8548387289047241)
[2025-02-16 12:44:21,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:21,510][root][INFO] - Training Epoch: 1/2, step 10395/23838 completed (loss: 0.22105149924755096, acc: 0.9479768872261047)
[2025-02-16 12:44:21,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:21,960][root][INFO] - Training Epoch: 1/2, step 10396/23838 completed (loss: 0.4066936671733856, acc: 0.8951612710952759)
[2025-02-16 12:44:22,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:22,397][root][INFO] - Training Epoch: 1/2, step 10397/23838 completed (loss: 0.3820889890193939, acc: 0.8684210777282715)
[2025-02-16 12:44:22,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:22,862][root][INFO] - Training Epoch: 1/2, step 10398/23838 completed (loss: 0.25588807463645935, acc: 0.9300699234008789)
[2025-02-16 12:44:23,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:23,353][root][INFO] - Training Epoch: 1/2, step 10399/23838 completed (loss: 0.5298514366149902, acc: 0.854651153087616)
[2025-02-16 12:44:23,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:23,849][root][INFO] - Training Epoch: 1/2, step 10400/23838 completed (loss: 0.33926406502723694, acc: 0.9064039587974548)
[2025-02-16 12:44:24,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:24,265][root][INFO] - Training Epoch: 1/2, step 10401/23838 completed (loss: 0.6306165456771851, acc: 0.8500000238418579)
[2025-02-16 12:44:24,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:24,761][root][INFO] - Training Epoch: 1/2, step 10402/23838 completed (loss: 0.2779131531715393, acc: 0.9428571462631226)
[2025-02-16 12:44:25,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:25,279][root][INFO] - Training Epoch: 1/2, step 10403/23838 completed (loss: 0.35767996311187744, acc: 0.9024389982223511)
[2025-02-16 12:44:25,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:25,726][root][INFO] - Training Epoch: 1/2, step 10404/23838 completed (loss: 0.2962326407432556, acc: 0.9416666626930237)
[2025-02-16 12:44:25,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:26,137][root][INFO] - Training Epoch: 1/2, step 10405/23838 completed (loss: 0.5056091547012329, acc: 0.8799999952316284)
[2025-02-16 12:44:26,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:26,575][root][INFO] - Training Epoch: 1/2, step 10406/23838 completed (loss: 0.2920306921005249, acc: 0.9047619104385376)
[2025-02-16 12:44:26,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:27,015][root][INFO] - Training Epoch: 1/2, step 10407/23838 completed (loss: 0.20734648406505585, acc: 0.9391891956329346)
[2025-02-16 12:44:27,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:27,503][root][INFO] - Training Epoch: 1/2, step 10408/23838 completed (loss: 0.20229847729206085, acc: 0.9316239356994629)
[2025-02-16 12:44:27,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:28,017][root][INFO] - Training Epoch: 1/2, step 10409/23838 completed (loss: 0.4604698717594147, acc: 0.8838174343109131)
[2025-02-16 12:44:28,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:28,803][root][INFO] - Training Epoch: 1/2, step 10410/23838 completed (loss: 0.15355896949768066, acc: 0.9552238583564758)
[2025-02-16 12:44:28,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:29,212][root][INFO] - Training Epoch: 1/2, step 10411/23838 completed (loss: 0.19400061666965485, acc: 0.9601989984512329)
[2025-02-16 12:44:29,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:29,591][root][INFO] - Training Epoch: 1/2, step 10412/23838 completed (loss: 0.1784111112356186, acc: 0.9427480697631836)
[2025-02-16 12:44:29,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:30,006][root][INFO] - Training Epoch: 1/2, step 10413/23838 completed (loss: 0.253520667552948, acc: 0.920634925365448)
[2025-02-16 12:44:30,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:30,431][root][INFO] - Training Epoch: 1/2, step 10414/23838 completed (loss: 0.2515760362148285, acc: 0.9205297827720642)
[2025-02-16 12:44:30,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:30,852][root][INFO] - Training Epoch: 1/2, step 10415/23838 completed (loss: 0.3169950544834137, acc: 0.9312977194786072)
[2025-02-16 12:44:31,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:31,296][root][INFO] - Training Epoch: 1/2, step 10416/23838 completed (loss: 0.15552310645580292, acc: 0.9586777091026306)
[2025-02-16 12:44:31,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:31,754][root][INFO] - Training Epoch: 1/2, step 10417/23838 completed (loss: 0.3537813127040863, acc: 0.9266055226325989)
[2025-02-16 12:44:32,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:32,338][root][INFO] - Training Epoch: 1/2, step 10418/23838 completed (loss: 0.24175798892974854, acc: 0.929648220539093)
[2025-02-16 12:44:32,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:32,806][root][INFO] - Training Epoch: 1/2, step 10419/23838 completed (loss: 0.29520976543426514, acc: 0.9365079402923584)
[2025-02-16 12:44:33,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:33,259][root][INFO] - Training Epoch: 1/2, step 10420/23838 completed (loss: 0.27092501521110535, acc: 0.9059829115867615)
[2025-02-16 12:44:33,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:33,692][root][INFO] - Training Epoch: 1/2, step 10421/23838 completed (loss: 0.24424950778484344, acc: 0.9115044474601746)
[2025-02-16 12:44:33,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:34,109][root][INFO] - Training Epoch: 1/2, step 10422/23838 completed (loss: 0.12280500680208206, acc: 0.967391312122345)
[2025-02-16 12:44:34,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:34,468][root][INFO] - Training Epoch: 1/2, step 10423/23838 completed (loss: 0.4103659987449646, acc: 0.8970588445663452)
[2025-02-16 12:44:34,687][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:34,938][root][INFO] - Training Epoch: 1/2, step 10424/23838 completed (loss: 0.09650533646345139, acc: 0.966292142868042)
[2025-02-16 12:44:35,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:35,353][root][INFO] - Training Epoch: 1/2, step 10425/23838 completed (loss: 0.1639757603406906, acc: 0.9468085169792175)
[2025-02-16 12:44:35,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:35,756][root][INFO] - Training Epoch: 1/2, step 10426/23838 completed (loss: 0.32660922408103943, acc: 0.9159663915634155)
[2025-02-16 12:44:35,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:36,218][root][INFO] - Training Epoch: 1/2, step 10427/23838 completed (loss: 0.33568742871284485, acc: 0.8850574493408203)
[2025-02-16 12:44:36,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:36,681][root][INFO] - Training Epoch: 1/2, step 10428/23838 completed (loss: 0.2876335680484772, acc: 0.9140625)
[2025-02-16 12:44:36,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:37,138][root][INFO] - Training Epoch: 1/2, step 10429/23838 completed (loss: 0.14643561840057373, acc: 0.9689922332763672)
[2025-02-16 12:44:37,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:37,651][root][INFO] - Training Epoch: 1/2, step 10430/23838 completed (loss: 0.426138699054718, acc: 0.8863636255264282)
[2025-02-16 12:44:37,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:38,108][root][INFO] - Training Epoch: 1/2, step 10431/23838 completed (loss: 0.2992609143257141, acc: 0.9056603908538818)
[2025-02-16 12:44:38,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:38,555][root][INFO] - Training Epoch: 1/2, step 10432/23838 completed (loss: 0.0928332656621933, acc: 0.9741379022598267)
[2025-02-16 12:44:38,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:38,979][root][INFO] - Training Epoch: 1/2, step 10433/23838 completed (loss: 0.09804403781890869, acc: 0.9801324605941772)
[2025-02-16 12:44:39,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:39,399][root][INFO] - Training Epoch: 1/2, step 10434/23838 completed (loss: 0.40242886543273926, acc: 0.8865979313850403)
[2025-02-16 12:44:39,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:39,893][root][INFO] - Training Epoch: 1/2, step 10435/23838 completed (loss: 0.2409207969903946, acc: 0.9174311757087708)
[2025-02-16 12:44:40,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:40,354][root][INFO] - Training Epoch: 1/2, step 10436/23838 completed (loss: 0.1281833052635193, acc: 0.9558011293411255)
[2025-02-16 12:44:40,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:40,766][root][INFO] - Training Epoch: 1/2, step 10437/23838 completed (loss: 0.35930830240249634, acc: 0.9285714030265808)
[2025-02-16 12:44:41,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:41,254][root][INFO] - Training Epoch: 1/2, step 10438/23838 completed (loss: 0.2670249938964844, acc: 0.925000011920929)
[2025-02-16 12:44:41,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:41,708][root][INFO] - Training Epoch: 1/2, step 10439/23838 completed (loss: 0.38153722882270813, acc: 0.9411764740943909)
[2025-02-16 12:44:41,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:42,125][root][INFO] - Training Epoch: 1/2, step 10440/23838 completed (loss: 0.13178128004074097, acc: 0.970059871673584)
[2025-02-16 12:44:42,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:42,534][root][INFO] - Training Epoch: 1/2, step 10441/23838 completed (loss: 0.2399689108133316, acc: 0.9399999976158142)
[2025-02-16 12:44:42,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:42,970][root][INFO] - Training Epoch: 1/2, step 10442/23838 completed (loss: 0.06300484389066696, acc: 0.9803921580314636)
[2025-02-16 12:44:43,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:43,449][root][INFO] - Training Epoch: 1/2, step 10443/23838 completed (loss: 0.23486176133155823, acc: 0.9396551847457886)
[2025-02-16 12:44:43,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:43,912][root][INFO] - Training Epoch: 1/2, step 10444/23838 completed (loss: 0.23641324043273926, acc: 0.9292929172515869)
[2025-02-16 12:44:44,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:44,334][root][INFO] - Training Epoch: 1/2, step 10445/23838 completed (loss: 0.297162264585495, acc: 0.9120879173278809)
[2025-02-16 12:44:44,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:45,011][root][INFO] - Training Epoch: 1/2, step 10446/23838 completed (loss: 0.121036097407341, acc: 0.9653679728507996)
[2025-02-16 12:44:45,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:45,478][root][INFO] - Training Epoch: 1/2, step 10447/23838 completed (loss: 0.23137269914150238, acc: 0.9316239356994629)
[2025-02-16 12:44:45,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:45,839][root][INFO] - Training Epoch: 1/2, step 10448/23838 completed (loss: 0.287762850522995, acc: 0.931506872177124)
[2025-02-16 12:44:46,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:46,241][root][INFO] - Training Epoch: 1/2, step 10449/23838 completed (loss: 0.42127057909965515, acc: 0.9150943160057068)
[2025-02-16 12:44:46,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:46,774][root][INFO] - Training Epoch: 1/2, step 10450/23838 completed (loss: 0.2562039792537689, acc: 0.9222797751426697)
[2025-02-16 12:44:47,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:47,254][root][INFO] - Training Epoch: 1/2, step 10451/23838 completed (loss: 0.07072204351425171, acc: 0.9835164546966553)
[2025-02-16 12:44:47,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:47,667][root][INFO] - Training Epoch: 1/2, step 10452/23838 completed (loss: 0.18009108304977417, acc: 0.9626865386962891)
[2025-02-16 12:44:47,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:48,070][root][INFO] - Training Epoch: 1/2, step 10453/23838 completed (loss: 0.26034361124038696, acc: 0.956204354763031)
[2025-02-16 12:44:48,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:48,505][root][INFO] - Training Epoch: 1/2, step 10454/23838 completed (loss: 0.32890695333480835, acc: 0.9126983880996704)
[2025-02-16 12:44:49,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:49,498][root][INFO] - Training Epoch: 1/2, step 10455/23838 completed (loss: 0.18326343595981598, acc: 0.9526627063751221)
[2025-02-16 12:44:49,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:49,927][root][INFO] - Training Epoch: 1/2, step 10456/23838 completed (loss: 0.5294339656829834, acc: 0.852173924446106)
[2025-02-16 12:44:50,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:50,392][root][INFO] - Training Epoch: 1/2, step 10457/23838 completed (loss: 0.25127336382865906, acc: 0.9065420627593994)
[2025-02-16 12:44:50,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:50,803][root][INFO] - Training Epoch: 1/2, step 10458/23838 completed (loss: 0.16909396648406982, acc: 0.9370629191398621)
[2025-02-16 12:44:51,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:51,288][root][INFO] - Training Epoch: 1/2, step 10459/23838 completed (loss: 0.9000218510627747, acc: 0.7796609997749329)
[2025-02-16 12:44:51,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:51,734][root][INFO] - Training Epoch: 1/2, step 10460/23838 completed (loss: 0.40189921855926514, acc: 0.8829787373542786)
[2025-02-16 12:44:51,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:52,192][root][INFO] - Training Epoch: 1/2, step 10461/23838 completed (loss: 0.1483069360256195, acc: 0.9569892287254333)
[2025-02-16 12:44:52,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:52,598][root][INFO] - Training Epoch: 1/2, step 10462/23838 completed (loss: 0.45277145504951477, acc: 0.8666666746139526)
[2025-02-16 12:44:52,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:53,048][root][INFO] - Training Epoch: 1/2, step 10463/23838 completed (loss: 0.12369679659605026, acc: 0.9672130942344666)
[2025-02-16 12:44:53,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:53,509][root][INFO] - Training Epoch: 1/2, step 10464/23838 completed (loss: 0.7470329403877258, acc: 0.7358490824699402)
[2025-02-16 12:44:53,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:53,953][root][INFO] - Training Epoch: 1/2, step 10465/23838 completed (loss: 0.2617246210575104, acc: 0.9223300814628601)
[2025-02-16 12:44:54,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:54,470][root][INFO] - Training Epoch: 1/2, step 10466/23838 completed (loss: 0.3383234441280365, acc: 0.8999999761581421)
[2025-02-16 12:44:54,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:54,898][root][INFO] - Training Epoch: 1/2, step 10467/23838 completed (loss: 0.3358895480632782, acc: 0.9459459185600281)
[2025-02-16 12:44:55,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:55,341][root][INFO] - Training Epoch: 1/2, step 10468/23838 completed (loss: 0.4420083165168762, acc: 0.8796992301940918)
[2025-02-16 12:44:55,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:55,795][root][INFO] - Training Epoch: 1/2, step 10469/23838 completed (loss: 0.24405695497989655, acc: 0.9150943160057068)
[2025-02-16 12:44:55,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:56,191][root][INFO] - Training Epoch: 1/2, step 10470/23838 completed (loss: 0.05729715898633003, acc: 0.9800000190734863)
[2025-02-16 12:44:56,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:56,586][root][INFO] - Training Epoch: 1/2, step 10471/23838 completed (loss: 0.18034681677818298, acc: 0.9586777091026306)
[2025-02-16 12:44:56,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:57,035][root][INFO] - Training Epoch: 1/2, step 10472/23838 completed (loss: 0.12133931368589401, acc: 0.9795918464660645)
[2025-02-16 12:44:57,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:57,459][root][INFO] - Training Epoch: 1/2, step 10473/23838 completed (loss: 0.22407428920269012, acc: 0.9433962106704712)
[2025-02-16 12:44:57,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:57,879][root][INFO] - Training Epoch: 1/2, step 10474/23838 completed (loss: 0.0625731348991394, acc: 0.9834710955619812)
[2025-02-16 12:44:58,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:58,374][root][INFO] - Training Epoch: 1/2, step 10475/23838 completed (loss: 0.18410974740982056, acc: 0.948051929473877)
[2025-02-16 12:44:58,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:58,772][root][INFO] - Training Epoch: 1/2, step 10476/23838 completed (loss: 0.14145976305007935, acc: 0.9569892287254333)
[2025-02-16 12:44:59,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:59,336][root][INFO] - Training Epoch: 1/2, step 10477/23838 completed (loss: 0.11131446808576584, acc: 0.9597315192222595)
[2025-02-16 12:44:59,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:44:59,762][root][INFO] - Training Epoch: 1/2, step 10478/23838 completed (loss: 0.1691068708896637, acc: 0.9189189076423645)
[2025-02-16 12:44:59,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:00,238][root][INFO] - Training Epoch: 1/2, step 10479/23838 completed (loss: 0.2147802859544754, acc: 0.9372549057006836)
[2025-02-16 12:45:00,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:00,841][root][INFO] - Training Epoch: 1/2, step 10480/23838 completed (loss: 0.22717353701591492, acc: 0.9428571462631226)
[2025-02-16 12:45:01,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:01,520][root][INFO] - Training Epoch: 1/2, step 10481/23838 completed (loss: 0.29852238297462463, acc: 0.9204545617103577)
[2025-02-16 12:45:01,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:02,061][root][INFO] - Training Epoch: 1/2, step 10482/23838 completed (loss: 0.36276760697364807, acc: 0.9210526347160339)
[2025-02-16 12:45:02,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:02,470][root][INFO] - Training Epoch: 1/2, step 10483/23838 completed (loss: 0.18832440674304962, acc: 0.9405940771102905)
[2025-02-16 12:45:02,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:02,905][root][INFO] - Training Epoch: 1/2, step 10484/23838 completed (loss: 0.16800273954868317, acc: 0.9631901979446411)
[2025-02-16 12:45:03,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:03,333][root][INFO] - Training Epoch: 1/2, step 10485/23838 completed (loss: 0.3395668864250183, acc: 0.8829787373542786)
[2025-02-16 12:45:03,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:03,770][root][INFO] - Training Epoch: 1/2, step 10486/23838 completed (loss: 0.22136670351028442, acc: 0.9406779408454895)
[2025-02-16 12:45:03,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:04,233][root][INFO] - Training Epoch: 1/2, step 10487/23838 completed (loss: 0.334276407957077, acc: 0.8775510191917419)
[2025-02-16 12:45:04,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:04,649][root][INFO] - Training Epoch: 1/2, step 10488/23838 completed (loss: 0.37327080965042114, acc: 0.8842105269432068)
[2025-02-16 12:45:04,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:05,132][root][INFO] - Training Epoch: 1/2, step 10489/23838 completed (loss: 0.553161084651947, acc: 0.828125)
[2025-02-16 12:45:05,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:06,187][root][INFO] - Training Epoch: 1/2, step 10490/23838 completed (loss: 0.36072710156440735, acc: 0.898809552192688)
[2025-02-16 12:45:06,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:06,623][root][INFO] - Training Epoch: 1/2, step 10491/23838 completed (loss: 0.4451245069503784, acc: 0.8543689250946045)
[2025-02-16 12:45:07,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:07,519][root][INFO] - Training Epoch: 1/2, step 10492/23838 completed (loss: 0.16613994538784027, acc: 0.9458128213882446)
[2025-02-16 12:45:07,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:07,930][root][INFO] - Training Epoch: 1/2, step 10493/23838 completed (loss: 0.18929068744182587, acc: 0.949999988079071)
[2025-02-16 12:45:08,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:08,375][root][INFO] - Training Epoch: 1/2, step 10494/23838 completed (loss: 0.2179221361875534, acc: 0.9299362897872925)
[2025-02-16 12:45:08,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:08,833][root][INFO] - Training Epoch: 1/2, step 10495/23838 completed (loss: 0.3229738175868988, acc: 0.9223300814628601)
[2025-02-16 12:45:09,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:09,393][root][INFO] - Training Epoch: 1/2, step 10496/23838 completed (loss: 0.2654917538166046, acc: 0.9357143044471741)
[2025-02-16 12:45:09,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:09,823][root][INFO] - Training Epoch: 1/2, step 10497/23838 completed (loss: 0.9517826437950134, acc: 0.7307692170143127)
[2025-02-16 12:45:10,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:10,271][root][INFO] - Training Epoch: 1/2, step 10498/23838 completed (loss: 0.8129870891571045, acc: 0.8129496574401855)
[2025-02-16 12:45:10,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:10,797][root][INFO] - Training Epoch: 1/2, step 10499/23838 completed (loss: 0.3731948435306549, acc: 0.875)
[2025-02-16 12:45:10,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:11,207][root][INFO] - Training Epoch: 1/2, step 10500/23838 completed (loss: 0.5912395119667053, acc: 0.800000011920929)
[2025-02-16 12:45:11,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:11,638][root][INFO] - Training Epoch: 1/2, step 10501/23838 completed (loss: 0.22549881041049957, acc: 0.9407407641410828)
[2025-02-16 12:45:11,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:12,044][root][INFO] - Training Epoch: 1/2, step 10502/23838 completed (loss: 0.15228582918643951, acc: 0.9726775884628296)
[2025-02-16 12:45:12,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:12,456][root][INFO] - Training Epoch: 1/2, step 10503/23838 completed (loss: 0.14336630702018738, acc: 0.9463414549827576)
[2025-02-16 12:45:12,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:13,386][root][INFO] - Training Epoch: 1/2, step 10504/23838 completed (loss: 0.20629914104938507, acc: 0.9599999785423279)
[2025-02-16 12:45:13,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:13,861][root][INFO] - Training Epoch: 1/2, step 10505/23838 completed (loss: 0.2594618499279022, acc: 0.9433962106704712)
[2025-02-16 12:45:14,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:14,585][root][INFO] - Training Epoch: 1/2, step 10506/23838 completed (loss: 0.19115598499774933, acc: 0.9535714387893677)
[2025-02-16 12:45:14,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:15,016][root][INFO] - Training Epoch: 1/2, step 10507/23838 completed (loss: 0.16081079840660095, acc: 0.9385964870452881)
[2025-02-16 12:45:15,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:15,982][root][INFO] - Training Epoch: 1/2, step 10508/23838 completed (loss: 0.2218884378671646, acc: 0.9340101480484009)
[2025-02-16 12:45:16,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:16,467][root][INFO] - Training Epoch: 1/2, step 10509/23838 completed (loss: 0.24457161128520966, acc: 0.9130434989929199)
[2025-02-16 12:45:16,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:16,897][root][INFO] - Training Epoch: 1/2, step 10510/23838 completed (loss: 0.19795915484428406, acc: 0.9634146094322205)
[2025-02-16 12:45:17,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:17,310][root][INFO] - Training Epoch: 1/2, step 10511/23838 completed (loss: 0.591941773891449, acc: 0.859375)
[2025-02-16 12:45:17,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:17,717][root][INFO] - Training Epoch: 1/2, step 10512/23838 completed (loss: 0.2827593982219696, acc: 0.9082568883895874)
[2025-02-16 12:45:17,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:18,142][root][INFO] - Training Epoch: 1/2, step 10513/23838 completed (loss: 0.14583538472652435, acc: 0.9504950642585754)
[2025-02-16 12:45:18,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:18,568][root][INFO] - Training Epoch: 1/2, step 10514/23838 completed (loss: 0.2840995788574219, acc: 0.9120879173278809)
[2025-02-16 12:45:18,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:18,987][root][INFO] - Training Epoch: 1/2, step 10515/23838 completed (loss: 0.17622612416744232, acc: 0.9489051103591919)
[2025-02-16 12:45:19,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:19,453][root][INFO] - Training Epoch: 1/2, step 10516/23838 completed (loss: 0.6212979555130005, acc: 0.8333333134651184)
[2025-02-16 12:45:19,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:19,959][root][INFO] - Training Epoch: 1/2, step 10517/23838 completed (loss: 0.27942779660224915, acc: 0.9090909361839294)
[2025-02-16 12:45:20,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:20,387][root][INFO] - Training Epoch: 1/2, step 10518/23838 completed (loss: 0.5613588690757751, acc: 0.8627451062202454)
[2025-02-16 12:45:20,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:20,838][root][INFO] - Training Epoch: 1/2, step 10519/23838 completed (loss: 0.814124345779419, acc: 0.8015872836112976)
[2025-02-16 12:45:21,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:21,303][root][INFO] - Training Epoch: 1/2, step 10520/23838 completed (loss: 0.5134075880050659, acc: 0.8461538553237915)
[2025-02-16 12:45:21,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:22,052][root][INFO] - Training Epoch: 1/2, step 10521/23838 completed (loss: 0.8002942204475403, acc: 0.8023256063461304)
[2025-02-16 12:45:22,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:22,501][root][INFO] - Training Epoch: 1/2, step 10522/23838 completed (loss: 0.4754015803337097, acc: 0.8478260636329651)
[2025-02-16 12:45:22,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:22,962][root][INFO] - Training Epoch: 1/2, step 10523/23838 completed (loss: 0.39049017429351807, acc: 0.892307698726654)
[2025-02-16 12:45:23,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:23,386][root][INFO] - Training Epoch: 1/2, step 10524/23838 completed (loss: 1.220252513885498, acc: 0.6363636255264282)
[2025-02-16 12:45:23,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:23,880][root][INFO] - Training Epoch: 1/2, step 10525/23838 completed (loss: 0.5243545770645142, acc: 0.7843137383460999)
[2025-02-16 12:45:24,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:24,304][root][INFO] - Training Epoch: 1/2, step 10526/23838 completed (loss: 1.295722484588623, acc: 0.6551724076271057)
[2025-02-16 12:45:24,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:24,729][root][INFO] - Training Epoch: 1/2, step 10527/23838 completed (loss: 1.0644780397415161, acc: 0.7428571581840515)
[2025-02-16 12:45:24,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:25,166][root][INFO] - Training Epoch: 1/2, step 10528/23838 completed (loss: 1.108817219734192, acc: 0.6666666865348816)
[2025-02-16 12:45:25,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:25,637][root][INFO] - Training Epoch: 1/2, step 10529/23838 completed (loss: 0.5238672494888306, acc: 0.7777777910232544)
[2025-02-16 12:45:25,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:26,125][root][INFO] - Training Epoch: 1/2, step 10530/23838 completed (loss: 0.34659087657928467, acc: 0.8644067645072937)
[2025-02-16 12:45:26,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:26,595][root][INFO] - Training Epoch: 1/2, step 10531/23838 completed (loss: 0.9646957516670227, acc: 0.7411764860153198)
[2025-02-16 12:45:26,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:27,059][root][INFO] - Training Epoch: 1/2, step 10532/23838 completed (loss: 0.4797895848751068, acc: 0.828125)
[2025-02-16 12:45:27,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:27,517][root][INFO] - Training Epoch: 1/2, step 10533/23838 completed (loss: 0.35226407647132874, acc: 0.9090909361839294)
[2025-02-16 12:45:27,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:27,936][root][INFO] - Training Epoch: 1/2, step 10534/23838 completed (loss: 0.23708294332027435, acc: 0.936170220375061)
[2025-02-16 12:45:28,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:28,441][root][INFO] - Training Epoch: 1/2, step 10535/23838 completed (loss: 0.35386812686920166, acc: 0.8333333134651184)
[2025-02-16 12:45:28,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:28,992][root][INFO] - Training Epoch: 1/2, step 10536/23838 completed (loss: 0.8787966370582581, acc: 0.7708333134651184)
[2025-02-16 12:45:29,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:29,836][root][INFO] - Training Epoch: 1/2, step 10537/23838 completed (loss: 0.5495587587356567, acc: 0.8695651888847351)
[2025-02-16 12:45:30,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:30,239][root][INFO] - Training Epoch: 1/2, step 10538/23838 completed (loss: 0.5996747016906738, acc: 0.8125)
[2025-02-16 12:45:30,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:30,680][root][INFO] - Training Epoch: 1/2, step 10539/23838 completed (loss: 0.35125795006752014, acc: 0.9104477763175964)
[2025-02-16 12:45:30,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:31,193][root][INFO] - Training Epoch: 1/2, step 10540/23838 completed (loss: 0.7641630172729492, acc: 0.8181818127632141)
[2025-02-16 12:45:31,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:31,671][root][INFO] - Training Epoch: 1/2, step 10541/23838 completed (loss: 0.318725049495697, acc: 0.8928571343421936)
[2025-02-16 12:45:31,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:32,110][root][INFO] - Training Epoch: 1/2, step 10542/23838 completed (loss: 0.7948538064956665, acc: 0.8064516186714172)
[2025-02-16 12:45:32,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:32,553][root][INFO] - Training Epoch: 1/2, step 10543/23838 completed (loss: 0.42964673042297363, acc: 0.8823529481887817)
[2025-02-16 12:45:32,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:32,980][root][INFO] - Training Epoch: 1/2, step 10544/23838 completed (loss: 1.4415793418884277, acc: 0.5263158082962036)
[2025-02-16 12:45:33,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:33,553][root][INFO] - Training Epoch: 1/2, step 10545/23838 completed (loss: 0.6694266200065613, acc: 0.75)
[2025-02-16 12:45:33,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:33,980][root][INFO] - Training Epoch: 1/2, step 10546/23838 completed (loss: 0.5791465044021606, acc: 0.8196721076965332)
[2025-02-16 12:45:34,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:34,389][root][INFO] - Training Epoch: 1/2, step 10547/23838 completed (loss: 0.4709641635417938, acc: 0.8108108043670654)
[2025-02-16 12:45:34,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:35,036][root][INFO] - Training Epoch: 1/2, step 10548/23838 completed (loss: 0.30236756801605225, acc: 0.9230769276618958)
[2025-02-16 12:45:35,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:35,474][root][INFO] - Training Epoch: 1/2, step 10549/23838 completed (loss: 0.5858756303787231, acc: 0.8500000238418579)
[2025-02-16 12:45:35,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:35,985][root][INFO] - Training Epoch: 1/2, step 10550/23838 completed (loss: 0.3071245551109314, acc: 0.9166666865348816)
[2025-02-16 12:45:36,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:36,791][root][INFO] - Training Epoch: 1/2, step 10551/23838 completed (loss: 0.5882039070129395, acc: 0.8163265585899353)
[2025-02-16 12:45:36,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:37,196][root][INFO] - Training Epoch: 1/2, step 10552/23838 completed (loss: 1.2190805673599243, acc: 0.7200000286102295)
[2025-02-16 12:45:37,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:38,032][root][INFO] - Training Epoch: 1/2, step 10553/23838 completed (loss: 0.948587954044342, acc: 0.7647058963775635)
[2025-02-16 12:45:38,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:38,503][root][INFO] - Training Epoch: 1/2, step 10554/23838 completed (loss: 1.0536094903945923, acc: 0.6744186282157898)
[2025-02-16 12:45:39,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:39,601][root][INFO] - Training Epoch: 1/2, step 10555/23838 completed (loss: 0.8444271683692932, acc: 0.7164179086685181)
[2025-02-16 12:45:39,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:40,140][root][INFO] - Training Epoch: 1/2, step 10556/23838 completed (loss: 0.4199868142604828, acc: 0.8636363744735718)
[2025-02-16 12:45:40,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:40,675][root][INFO] - Training Epoch: 1/2, step 10557/23838 completed (loss: 0.9424687623977661, acc: 0.7735849022865295)
[2025-02-16 12:45:41,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:41,502][root][INFO] - Training Epoch: 1/2, step 10558/23838 completed (loss: 1.0752383470535278, acc: 0.7272727489471436)
[2025-02-16 12:45:41,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:41,974][root][INFO] - Training Epoch: 1/2, step 10559/23838 completed (loss: 0.5291045308113098, acc: 0.8888888955116272)
[2025-02-16 12:45:42,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:42,405][root][INFO] - Training Epoch: 1/2, step 10560/23838 completed (loss: 0.474732369184494, acc: 0.8666666746139526)
[2025-02-16 12:45:42,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:42,975][root][INFO] - Training Epoch: 1/2, step 10561/23838 completed (loss: 0.34522074460983276, acc: 0.8965517282485962)
[2025-02-16 12:45:43,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:43,435][root][INFO] - Training Epoch: 1/2, step 10562/23838 completed (loss: 0.3782741129398346, acc: 0.8536585569381714)
[2025-02-16 12:45:43,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:43,888][root][INFO] - Training Epoch: 1/2, step 10563/23838 completed (loss: 0.6820955872535706, acc: 0.7058823704719543)
[2025-02-16 12:45:44,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:44,777][root][INFO] - Training Epoch: 1/2, step 10564/23838 completed (loss: 0.6774574518203735, acc: 0.7592592835426331)
[2025-02-16 12:45:45,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:45,640][root][INFO] - Training Epoch: 1/2, step 10565/23838 completed (loss: 0.46696382761001587, acc: 0.875)
[2025-02-16 12:45:45,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:46,136][root][INFO] - Training Epoch: 1/2, step 10566/23838 completed (loss: 0.05067848414182663, acc: 1.0)
[2025-02-16 12:45:46,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:46,933][root][INFO] - Training Epoch: 1/2, step 10567/23838 completed (loss: 0.6598206162452698, acc: 0.8055555820465088)
[2025-02-16 12:45:47,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:47,677][root][INFO] - Training Epoch: 1/2, step 10568/23838 completed (loss: 1.0633189678192139, acc: 0.699999988079071)
[2025-02-16 12:45:48,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:48,325][root][INFO] - Training Epoch: 1/2, step 10569/23838 completed (loss: 1.228853464126587, acc: 0.7209302186965942)
[2025-02-16 12:45:48,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:48,815][root][INFO] - Training Epoch: 1/2, step 10570/23838 completed (loss: 0.203631192445755, acc: 0.9444444179534912)
[2025-02-16 12:45:48,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:49,219][root][INFO] - Training Epoch: 1/2, step 10571/23838 completed (loss: 1.702828049659729, acc: 0.6428571343421936)
[2025-02-16 12:45:49,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:49,684][root][INFO] - Training Epoch: 1/2, step 10572/23838 completed (loss: 0.5996057987213135, acc: 0.7599999904632568)
[2025-02-16 12:45:49,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:50,085][root][INFO] - Training Epoch: 1/2, step 10573/23838 completed (loss: 0.7527626752853394, acc: 0.8035714030265808)
[2025-02-16 12:45:50,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:50,906][root][INFO] - Training Epoch: 1/2, step 10574/23838 completed (loss: 0.8456441760063171, acc: 0.7647058963775635)
[2025-02-16 12:45:51,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:51,372][root][INFO] - Training Epoch: 1/2, step 10575/23838 completed (loss: 0.4290476143360138, acc: 0.9032257795333862)
[2025-02-16 12:45:51,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:52,458][root][INFO] - Training Epoch: 1/2, step 10576/23838 completed (loss: 0.9682069420814514, acc: 0.7333333492279053)
[2025-02-16 12:45:52,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:53,218][root][INFO] - Training Epoch: 1/2, step 10577/23838 completed (loss: 0.4744766056537628, acc: 0.8717948794364929)
[2025-02-16 12:45:53,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:54,118][root][INFO] - Training Epoch: 1/2, step 10578/23838 completed (loss: 0.6746296882629395, acc: 0.8524590134620667)
[2025-02-16 12:45:54,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:54,538][root][INFO] - Training Epoch: 1/2, step 10579/23838 completed (loss: 0.9347405433654785, acc: 0.7746478915214539)
[2025-02-16 12:45:54,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:55,327][root][INFO] - Training Epoch: 1/2, step 10580/23838 completed (loss: 0.4926351010799408, acc: 0.8452380895614624)
[2025-02-16 12:45:55,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:55,800][root][INFO] - Training Epoch: 1/2, step 10581/23838 completed (loss: 0.4434521198272705, acc: 0.8399999737739563)
[2025-02-16 12:45:56,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:56,794][root][INFO] - Training Epoch: 1/2, step 10582/23838 completed (loss: 0.9207373857498169, acc: 0.7678571343421936)
[2025-02-16 12:45:57,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:57,331][root][INFO] - Training Epoch: 1/2, step 10583/23838 completed (loss: 0.5029533505439758, acc: 0.8363636136054993)
[2025-02-16 12:45:57,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:57,791][root][INFO] - Training Epoch: 1/2, step 10584/23838 completed (loss: 0.6752702593803406, acc: 0.8372092843055725)
[2025-02-16 12:45:57,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:58,243][root][INFO] - Training Epoch: 1/2, step 10585/23838 completed (loss: 0.5963197946548462, acc: 0.849056601524353)
[2025-02-16 12:45:58,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:58,900][root][INFO] - Training Epoch: 1/2, step 10586/23838 completed (loss: 0.4622175693511963, acc: 0.9166666865348816)
[2025-02-16 12:45:59,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:59,471][root][INFO] - Training Epoch: 1/2, step 10587/23838 completed (loss: 0.7136470675468445, acc: 0.7894737124443054)
[2025-02-16 12:45:59,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:45:59,900][root][INFO] - Training Epoch: 1/2, step 10588/23838 completed (loss: 0.8059759736061096, acc: 0.7843137383460999)
[2025-02-16 12:46:00,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:00,389][root][INFO] - Training Epoch: 1/2, step 10589/23838 completed (loss: 1.214935064315796, acc: 0.65625)
[2025-02-16 12:46:00,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:01,245][root][INFO] - Training Epoch: 1/2, step 10590/23838 completed (loss: 0.35517966747283936, acc: 0.8730158805847168)
[2025-02-16 12:46:01,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:02,041][root][INFO] - Training Epoch: 1/2, step 10591/23838 completed (loss: 0.7430399656295776, acc: 0.762499988079071)
[2025-02-16 12:46:02,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:02,437][root][INFO] - Training Epoch: 1/2, step 10592/23838 completed (loss: 1.2804875373840332, acc: 0.6666666865348816)
[2025-02-16 12:46:02,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:02,862][root][INFO] - Training Epoch: 1/2, step 10593/23838 completed (loss: 1.0272098779678345, acc: 0.7200000286102295)
[2025-02-16 12:46:03,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:03,537][root][INFO] - Training Epoch: 1/2, step 10594/23838 completed (loss: 0.9303398132324219, acc: 0.7714285850524902)
[2025-02-16 12:46:03,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:03,918][root][INFO] - Training Epoch: 1/2, step 10595/23838 completed (loss: 1.356714129447937, acc: 0.625)
[2025-02-16 12:46:04,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:04,474][root][INFO] - Training Epoch: 1/2, step 10596/23838 completed (loss: 0.3975505530834198, acc: 0.8591549396514893)
[2025-02-16 12:46:04,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:04,905][root][INFO] - Training Epoch: 1/2, step 10597/23838 completed (loss: 0.12564648687839508, acc: 1.0)
[2025-02-16 12:46:05,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:05,413][root][INFO] - Training Epoch: 1/2, step 10598/23838 completed (loss: 0.6868573427200317, acc: 0.8032786846160889)
[2025-02-16 12:46:05,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:06,051][root][INFO] - Training Epoch: 1/2, step 10599/23838 completed (loss: 0.5243140459060669, acc: 0.8815789222717285)
[2025-02-16 12:46:06,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:06,445][root][INFO] - Training Epoch: 1/2, step 10600/23838 completed (loss: 0.29667726159095764, acc: 0.9583333134651184)
[2025-02-16 12:46:06,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:06,886][root][INFO] - Training Epoch: 1/2, step 10601/23838 completed (loss: 0.7754833102226257, acc: 0.75)
[2025-02-16 12:46:07,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:07,491][root][INFO] - Training Epoch: 1/2, step 10602/23838 completed (loss: 1.1887038946151733, acc: 0.6666666865348816)
[2025-02-16 12:46:07,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:07,975][root][INFO] - Training Epoch: 1/2, step 10603/23838 completed (loss: 0.9925111532211304, acc: 0.7307692170143127)
[2025-02-16 12:46:08,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:08,477][root][INFO] - Training Epoch: 1/2, step 10604/23838 completed (loss: 0.5666046738624573, acc: 0.8823529481887817)
[2025-02-16 12:46:08,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:08,933][root][INFO] - Training Epoch: 1/2, step 10605/23838 completed (loss: 0.614070475101471, acc: 0.8333333134651184)
[2025-02-16 12:46:09,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:09,307][root][INFO] - Training Epoch: 1/2, step 10606/23838 completed (loss: 0.16949592530727386, acc: 0.9285714030265808)
[2025-02-16 12:46:09,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:09,716][root][INFO] - Training Epoch: 1/2, step 10607/23838 completed (loss: 1.4335030317306519, acc: 0.5641025900840759)
[2025-02-16 12:46:09,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:10,210][root][INFO] - Training Epoch: 1/2, step 10608/23838 completed (loss: 0.9843125343322754, acc: 0.739130437374115)
[2025-02-16 12:46:10,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:10,634][root][INFO] - Training Epoch: 1/2, step 10609/23838 completed (loss: 0.4743570387363434, acc: 0.9090909361839294)
[2025-02-16 12:46:10,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:11,062][root][INFO] - Training Epoch: 1/2, step 10610/23838 completed (loss: 0.49130013585090637, acc: 0.8133333325386047)
[2025-02-16 12:46:11,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:11,607][root][INFO] - Training Epoch: 1/2, step 10611/23838 completed (loss: 0.7805412411689758, acc: 0.7454545497894287)
[2025-02-16 12:46:11,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:12,113][root][INFO] - Training Epoch: 1/2, step 10612/23838 completed (loss: 0.42640984058380127, acc: 0.9347826242446899)
[2025-02-16 12:46:12,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:12,583][root][INFO] - Training Epoch: 1/2, step 10613/23838 completed (loss: 0.8696508407592773, acc: 0.6363636255264282)
[2025-02-16 12:46:12,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:13,040][root][INFO] - Training Epoch: 1/2, step 10614/23838 completed (loss: 0.54746013879776, acc: 0.8529411554336548)
[2025-02-16 12:46:13,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:13,469][root][INFO] - Training Epoch: 1/2, step 10615/23838 completed (loss: 0.4397628605365753, acc: 0.8518518805503845)
[2025-02-16 12:46:13,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:13,914][root][INFO] - Training Epoch: 1/2, step 10616/23838 completed (loss: 0.8690400719642639, acc: 0.7727272510528564)
[2025-02-16 12:46:14,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:14,356][root][INFO] - Training Epoch: 1/2, step 10617/23838 completed (loss: 0.5490834712982178, acc: 0.8799999952316284)
[2025-02-16 12:46:14,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:14,816][root][INFO] - Training Epoch: 1/2, step 10618/23838 completed (loss: 0.3484412729740143, acc: 0.8837209343910217)
[2025-02-16 12:46:15,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:15,284][root][INFO] - Training Epoch: 1/2, step 10619/23838 completed (loss: 0.756473958492279, acc: 0.8125)
[2025-02-16 12:46:15,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:15,787][root][INFO] - Training Epoch: 1/2, step 10620/23838 completed (loss: 0.7997275590896606, acc: 0.7586206793785095)
[2025-02-16 12:46:15,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:16,222][root][INFO] - Training Epoch: 1/2, step 10621/23838 completed (loss: 1.5730056762695312, acc: 0.6315789222717285)
[2025-02-16 12:46:16,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:17,297][root][INFO] - Training Epoch: 1/2, step 10622/23838 completed (loss: 0.5820392966270447, acc: 0.7906976938247681)
[2025-02-16 12:46:17,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:18,303][root][INFO] - Training Epoch: 1/2, step 10623/23838 completed (loss: 1.2429955005645752, acc: 0.7254902124404907)
[2025-02-16 12:46:18,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:18,880][root][INFO] - Training Epoch: 1/2, step 10624/23838 completed (loss: 0.07855191081762314, acc: 1.0)
[2025-02-16 12:46:19,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:19,426][root][INFO] - Training Epoch: 1/2, step 10625/23838 completed (loss: 0.3496090769767761, acc: 0.875)
[2025-02-16 12:46:19,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:20,327][root][INFO] - Training Epoch: 1/2, step 10626/23838 completed (loss: 0.6635947227478027, acc: 0.800000011920929)
[2025-02-16 12:46:20,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:20,932][root][INFO] - Training Epoch: 1/2, step 10627/23838 completed (loss: 0.3326260447502136, acc: 0.9264705777168274)
[2025-02-16 12:46:21,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:21,574][root][INFO] - Training Epoch: 1/2, step 10628/23838 completed (loss: 0.2949960231781006, acc: 0.9090909361839294)
[2025-02-16 12:46:21,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:22,362][root][INFO] - Training Epoch: 1/2, step 10629/23838 completed (loss: 0.3708125054836273, acc: 0.9411764740943909)
[2025-02-16 12:46:22,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:22,760][root][INFO] - Training Epoch: 1/2, step 10630/23838 completed (loss: 0.29454588890075684, acc: 0.9166666865348816)
[2025-02-16 12:46:22,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:23,220][root][INFO] - Training Epoch: 1/2, step 10631/23838 completed (loss: 0.9355195760726929, acc: 0.739130437374115)
[2025-02-16 12:46:23,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:23,618][root][INFO] - Training Epoch: 1/2, step 10632/23838 completed (loss: 0.37986844778060913, acc: 0.8999999761581421)
[2025-02-16 12:46:24,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:24,361][root][INFO] - Training Epoch: 1/2, step 10633/23838 completed (loss: 0.6048281192779541, acc: 0.8421052694320679)
[2025-02-16 12:46:24,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:25,126][root][INFO] - Training Epoch: 1/2, step 10634/23838 completed (loss: 0.30073001980781555, acc: 0.9473684430122375)
[2025-02-16 12:46:25,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:25,783][root][INFO] - Training Epoch: 1/2, step 10635/23838 completed (loss: 0.7770205736160278, acc: 0.7884615659713745)
[2025-02-16 12:46:26,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:26,477][root][INFO] - Training Epoch: 1/2, step 10636/23838 completed (loss: 0.5433247089385986, acc: 0.8414633870124817)
[2025-02-16 12:46:26,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:26,992][root][INFO] - Training Epoch: 1/2, step 10637/23838 completed (loss: 0.7645573019981384, acc: 0.7735849022865295)
[2025-02-16 12:46:27,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:27,697][root][INFO] - Training Epoch: 1/2, step 10638/23838 completed (loss: 0.492698609828949, acc: 0.8787878751754761)
[2025-02-16 12:46:28,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:28,724][root][INFO] - Training Epoch: 1/2, step 10639/23838 completed (loss: 0.5690299868583679, acc: 0.8229166865348816)
[2025-02-16 12:46:28,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:29,233][root][INFO] - Training Epoch: 1/2, step 10640/23838 completed (loss: 0.3737117052078247, acc: 0.9019607901573181)
[2025-02-16 12:46:29,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:30,001][root][INFO] - Training Epoch: 1/2, step 10641/23838 completed (loss: 0.44833824038505554, acc: 0.8571428656578064)
[2025-02-16 12:46:30,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:30,542][root][INFO] - Training Epoch: 1/2, step 10642/23838 completed (loss: 0.37831658124923706, acc: 0.930232584476471)
[2025-02-16 12:46:31,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:31,467][root][INFO] - Training Epoch: 1/2, step 10643/23838 completed (loss: 0.9897300004959106, acc: 0.692307710647583)
[2025-02-16 12:46:31,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:31,984][root][INFO] - Training Epoch: 1/2, step 10644/23838 completed (loss: 0.46248412132263184, acc: 0.9166666865348816)
[2025-02-16 12:46:32,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:32,569][root][INFO] - Training Epoch: 1/2, step 10645/23838 completed (loss: 0.47116124629974365, acc: 0.8524590134620667)
[2025-02-16 12:46:32,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:32,932][root][INFO] - Training Epoch: 1/2, step 10646/23838 completed (loss: 1.0840249061584473, acc: 0.7777777910232544)
[2025-02-16 12:46:33,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:33,441][root][INFO] - Training Epoch: 1/2, step 10647/23838 completed (loss: 0.8013633489608765, acc: 0.7384615540504456)
[2025-02-16 12:46:33,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:33,934][root][INFO] - Training Epoch: 1/2, step 10648/23838 completed (loss: 0.5589515566825867, acc: 0.8648648858070374)
[2025-02-16 12:46:34,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:34,783][root][INFO] - Training Epoch: 1/2, step 10649/23838 completed (loss: 0.4673363268375397, acc: 0.8793103694915771)
[2025-02-16 12:46:35,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:35,266][root][INFO] - Training Epoch: 1/2, step 10650/23838 completed (loss: 0.8526177406311035, acc: 0.7627118825912476)
[2025-02-16 12:46:35,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:35,997][root][INFO] - Training Epoch: 1/2, step 10651/23838 completed (loss: 0.5979470610618591, acc: 0.8059701323509216)
[2025-02-16 12:46:36,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:36,544][root][INFO] - Training Epoch: 1/2, step 10652/23838 completed (loss: 1.3688524961471558, acc: 0.6875)
[2025-02-16 12:46:36,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:37,263][root][INFO] - Training Epoch: 1/2, step 10653/23838 completed (loss: 1.0421048402786255, acc: 0.7407407164573669)
[2025-02-16 12:46:37,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:37,662][root][INFO] - Training Epoch: 1/2, step 10654/23838 completed (loss: 1.2189667224884033, acc: 0.7037037014961243)
[2025-02-16 12:46:37,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:38,203][root][INFO] - Training Epoch: 1/2, step 10655/23838 completed (loss: 0.5338948965072632, acc: 0.837837815284729)
[2025-02-16 12:46:38,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:38,684][root][INFO] - Training Epoch: 1/2, step 10656/23838 completed (loss: 1.0885778665542603, acc: 0.6896551847457886)
[2025-02-16 12:46:38,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:39,062][root][INFO] - Training Epoch: 1/2, step 10657/23838 completed (loss: 1.0848397016525269, acc: 0.6666666865348816)
[2025-02-16 12:46:39,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:39,468][root][INFO] - Training Epoch: 1/2, step 10658/23838 completed (loss: 0.3008568286895752, acc: 0.8571428656578064)
[2025-02-16 12:46:39,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:39,875][root][INFO] - Training Epoch: 1/2, step 10659/23838 completed (loss: 1.371535301208496, acc: 0.5909090638160706)
[2025-02-16 12:46:40,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:40,380][root][INFO] - Training Epoch: 1/2, step 10660/23838 completed (loss: 0.3277806341648102, acc: 0.8947368264198303)
[2025-02-16 12:46:40,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:40,832][root][INFO] - Training Epoch: 1/2, step 10661/23838 completed (loss: 1.118901252746582, acc: 0.7230769395828247)
[2025-02-16 12:46:41,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:41,308][root][INFO] - Training Epoch: 1/2, step 10662/23838 completed (loss: 0.506013810634613, acc: 0.8571428656578064)
[2025-02-16 12:46:41,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:41,690][root][INFO] - Training Epoch: 1/2, step 10663/23838 completed (loss: 0.5632284879684448, acc: 0.78125)
[2025-02-16 12:46:41,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:42,082][root][INFO] - Training Epoch: 1/2, step 10664/23838 completed (loss: 0.8769651055335999, acc: 0.75)
[2025-02-16 12:46:42,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:42,458][root][INFO] - Training Epoch: 1/2, step 10665/23838 completed (loss: 1.3030205965042114, acc: 0.653333306312561)
[2025-02-16 12:46:42,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:42,926][root][INFO] - Training Epoch: 1/2, step 10666/23838 completed (loss: 1.2323726415634155, acc: 0.6454545259475708)
[2025-02-16 12:46:43,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:43,375][root][INFO] - Training Epoch: 1/2, step 10667/23838 completed (loss: 1.100064754486084, acc: 0.6590909361839294)
[2025-02-16 12:46:43,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:43,798][root][INFO] - Training Epoch: 1/2, step 10668/23838 completed (loss: 0.8379557132720947, acc: 0.7603305578231812)
[2025-02-16 12:46:44,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:44,291][root][INFO] - Training Epoch: 1/2, step 10669/23838 completed (loss: 0.8925522565841675, acc: 0.7137096524238586)
[2025-02-16 12:46:44,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:44,739][root][INFO] - Training Epoch: 1/2, step 10670/23838 completed (loss: 0.9582271575927734, acc: 0.7045454382896423)
[2025-02-16 12:46:44,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:45,167][root][INFO] - Training Epoch: 1/2, step 10671/23838 completed (loss: 0.8850336670875549, acc: 0.77173912525177)
[2025-02-16 12:46:45,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:45,633][root][INFO] - Training Epoch: 1/2, step 10672/23838 completed (loss: 0.8712801337242126, acc: 0.7457627058029175)
[2025-02-16 12:46:45,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:46,040][root][INFO] - Training Epoch: 1/2, step 10673/23838 completed (loss: 0.5718534588813782, acc: 0.8512396812438965)
[2025-02-16 12:46:46,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:46,578][root][INFO] - Training Epoch: 1/2, step 10674/23838 completed (loss: 0.6326206922531128, acc: 0.8238095045089722)
[2025-02-16 12:46:46,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:46,987][root][INFO] - Training Epoch: 1/2, step 10675/23838 completed (loss: 0.847042441368103, acc: 0.707317054271698)
[2025-02-16 12:46:47,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:47,566][root][INFO] - Training Epoch: 1/2, step 10676/23838 completed (loss: 0.9998543858528137, acc: 0.7368420958518982)
[2025-02-16 12:46:47,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:48,017][root][INFO] - Training Epoch: 1/2, step 10677/23838 completed (loss: 0.8284817934036255, acc: 0.7843137383460999)
[2025-02-16 12:46:48,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:48,422][root][INFO] - Training Epoch: 1/2, step 10678/23838 completed (loss: 0.7107473611831665, acc: 0.8220338821411133)
[2025-02-16 12:46:48,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:48,798][root][INFO] - Training Epoch: 1/2, step 10679/23838 completed (loss: 0.8218491673469543, acc: 0.7933884263038635)
[2025-02-16 12:46:49,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:49,342][root][INFO] - Training Epoch: 1/2, step 10680/23838 completed (loss: 0.6186233162879944, acc: 0.8305084705352783)
[2025-02-16 12:46:49,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:49,839][root][INFO] - Training Epoch: 1/2, step 10681/23838 completed (loss: 0.6203609108924866, acc: 0.8333333134651184)
[2025-02-16 12:46:50,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:50,332][root][INFO] - Training Epoch: 1/2, step 10682/23838 completed (loss: 0.4530627727508545, acc: 0.8571428656578064)
[2025-02-16 12:46:50,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:50,825][root][INFO] - Training Epoch: 1/2, step 10683/23838 completed (loss: 0.7841541767120361, acc: 0.8269230723381042)
[2025-02-16 12:46:51,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:51,322][root][INFO] - Training Epoch: 1/2, step 10684/23838 completed (loss: 0.6442463397979736, acc: 0.7976878881454468)
[2025-02-16 12:46:51,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:52,196][root][INFO] - Training Epoch: 1/2, step 10685/23838 completed (loss: 0.8259982466697693, acc: 0.7833333611488342)
[2025-02-16 12:46:52,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:52,712][root][INFO] - Training Epoch: 1/2, step 10686/23838 completed (loss: 0.4423760175704956, acc: 0.8636363744735718)
[2025-02-16 12:46:52,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:53,116][root][INFO] - Training Epoch: 1/2, step 10687/23838 completed (loss: 0.6879973411560059, acc: 0.7799999713897705)
[2025-02-16 12:46:53,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:53,557][root][INFO] - Training Epoch: 1/2, step 10688/23838 completed (loss: 0.5560948848724365, acc: 0.8461538553237915)
[2025-02-16 12:46:53,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:53,984][root][INFO] - Training Epoch: 1/2, step 10689/23838 completed (loss: 0.6723310947418213, acc: 0.8032786846160889)
[2025-02-16 12:46:54,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:54,360][root][INFO] - Training Epoch: 1/2, step 10690/23838 completed (loss: 1.1546785831451416, acc: 0.719298243522644)
[2025-02-16 12:46:54,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:54,793][root][INFO] - Training Epoch: 1/2, step 10691/23838 completed (loss: 0.4499983787536621, acc: 0.8510638475418091)
[2025-02-16 12:46:55,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:55,234][root][INFO] - Training Epoch: 1/2, step 10692/23838 completed (loss: 0.36017149686813354, acc: 0.8818181753158569)
[2025-02-16 12:46:55,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:55,638][root][INFO] - Training Epoch: 1/2, step 10693/23838 completed (loss: 0.7948000431060791, acc: 0.7848101258277893)
[2025-02-16 12:46:55,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:56,058][root][INFO] - Training Epoch: 1/2, step 10694/23838 completed (loss: 0.5167799592018127, acc: 0.8380952477455139)
[2025-02-16 12:46:56,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:56,457][root][INFO] - Training Epoch: 1/2, step 10695/23838 completed (loss: 0.47086626291275024, acc: 0.874015748500824)
[2025-02-16 12:46:56,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:56,906][root][INFO] - Training Epoch: 1/2, step 10696/23838 completed (loss: 0.5612398982048035, acc: 0.8695651888847351)
[2025-02-16 12:46:57,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:57,370][root][INFO] - Training Epoch: 1/2, step 10697/23838 completed (loss: 0.6327533721923828, acc: 0.8148148059844971)
[2025-02-16 12:46:57,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:58,100][root][INFO] - Training Epoch: 1/2, step 10698/23838 completed (loss: 0.4997118413448334, acc: 0.8407643437385559)
[2025-02-16 12:46:58,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:58,786][root][INFO] - Training Epoch: 1/2, step 10699/23838 completed (loss: 0.5158183574676514, acc: 0.8474576473236084)
[2025-02-16 12:46:58,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:59,158][root][INFO] - Training Epoch: 1/2, step 10700/23838 completed (loss: 0.558855414390564, acc: 0.8125)
[2025-02-16 12:46:59,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:46:59,582][root][INFO] - Training Epoch: 1/2, step 10701/23838 completed (loss: 0.5937464237213135, acc: 0.8265306353569031)
[2025-02-16 12:46:59,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:00,021][root][INFO] - Training Epoch: 1/2, step 10702/23838 completed (loss: 0.9942578673362732, acc: 0.7037037014961243)
[2025-02-16 12:47:00,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:00,514][root][INFO] - Training Epoch: 1/2, step 10703/23838 completed (loss: 1.1407482624053955, acc: 0.698924720287323)
[2025-02-16 12:47:00,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:00,976][root][INFO] - Training Epoch: 1/2, step 10704/23838 completed (loss: 0.5946708917617798, acc: 0.8111110925674438)
[2025-02-16 12:47:01,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:01,416][root][INFO] - Training Epoch: 1/2, step 10705/23838 completed (loss: 1.1648532152175903, acc: 0.7090908885002136)
[2025-02-16 12:47:01,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:01,903][root][INFO] - Training Epoch: 1/2, step 10706/23838 completed (loss: 0.48214593529701233, acc: 0.8584905862808228)
[2025-02-16 12:47:02,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:02,392][root][INFO] - Training Epoch: 1/2, step 10707/23838 completed (loss: 0.9340698719024658, acc: 0.7291666865348816)
[2025-02-16 12:47:02,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:02,816][root][INFO] - Training Epoch: 1/2, step 10708/23838 completed (loss: 0.8510580658912659, acc: 0.7636363506317139)
[2025-02-16 12:47:03,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:03,262][root][INFO] - Training Epoch: 1/2, step 10709/23838 completed (loss: 0.6490508317947388, acc: 0.8048780560493469)
[2025-02-16 12:47:03,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:03,746][root][INFO] - Training Epoch: 1/2, step 10710/23838 completed (loss: 0.6944513320922852, acc: 0.7727272510528564)
[2025-02-16 12:47:03,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:04,203][root][INFO] - Training Epoch: 1/2, step 10711/23838 completed (loss: 0.9416233897209167, acc: 0.7176470756530762)
[2025-02-16 12:47:04,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:04,669][root][INFO] - Training Epoch: 1/2, step 10712/23838 completed (loss: 1.0608574151992798, acc: 0.7272727489471436)
[2025-02-16 12:47:04,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:05,101][root][INFO] - Training Epoch: 1/2, step 10713/23838 completed (loss: 0.7380156517028809, acc: 0.7818182110786438)
[2025-02-16 12:47:05,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:05,496][root][INFO] - Training Epoch: 1/2, step 10714/23838 completed (loss: 0.3643795847892761, acc: 0.8952381014823914)
[2025-02-16 12:47:05,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:05,983][root][INFO] - Training Epoch: 1/2, step 10715/23838 completed (loss: 0.8817840814590454, acc: 0.7534246444702148)
[2025-02-16 12:47:06,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:06,464][root][INFO] - Training Epoch: 1/2, step 10716/23838 completed (loss: 0.6120207905769348, acc: 0.8674699068069458)
[2025-02-16 12:47:06,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:06,928][root][INFO] - Training Epoch: 1/2, step 10717/23838 completed (loss: 0.6906643509864807, acc: 0.8086956739425659)
[2025-02-16 12:47:07,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:07,421][root][INFO] - Training Epoch: 1/2, step 10718/23838 completed (loss: 0.5321998000144958, acc: 0.8770492076873779)
[2025-02-16 12:47:07,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:07,834][root][INFO] - Training Epoch: 1/2, step 10719/23838 completed (loss: 0.5861088037490845, acc: 0.8550724387168884)
[2025-02-16 12:47:08,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:08,341][root][INFO] - Training Epoch: 1/2, step 10720/23838 completed (loss: 0.5641882419586182, acc: 0.8152173757553101)
[2025-02-16 12:47:08,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:08,871][root][INFO] - Training Epoch: 1/2, step 10721/23838 completed (loss: 0.5638542771339417, acc: 0.8507462739944458)
[2025-02-16 12:47:09,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:09,340][root][INFO] - Training Epoch: 1/2, step 10722/23838 completed (loss: 0.6304059028625488, acc: 0.800000011920929)
[2025-02-16 12:47:09,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:09,813][root][INFO] - Training Epoch: 1/2, step 10723/23838 completed (loss: 1.0699472427368164, acc: 0.7169811129570007)
[2025-02-16 12:47:10,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:10,274][root][INFO] - Training Epoch: 1/2, step 10724/23838 completed (loss: 0.4934714138507843, acc: 0.8601398468017578)
[2025-02-16 12:47:10,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:10,744][root][INFO] - Training Epoch: 1/2, step 10725/23838 completed (loss: 0.8646103143692017, acc: 0.7471264600753784)
[2025-02-16 12:47:11,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:11,240][root][INFO] - Training Epoch: 1/2, step 10726/23838 completed (loss: 0.6435194611549377, acc: 0.8275862336158752)
[2025-02-16 12:47:11,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:11,626][root][INFO] - Training Epoch: 1/2, step 10727/23838 completed (loss: 0.40755507349967957, acc: 0.8731343150138855)
[2025-02-16 12:47:11,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:12,192][root][INFO] - Training Epoch: 1/2, step 10728/23838 completed (loss: 0.5218663811683655, acc: 0.8515284061431885)
[2025-02-16 12:47:12,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:12,630][root][INFO] - Training Epoch: 1/2, step 10729/23838 completed (loss: 0.7213852405548096, acc: 0.75)
[2025-02-16 12:47:12,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:13,086][root][INFO] - Training Epoch: 1/2, step 10730/23838 completed (loss: 0.4713911712169647, acc: 0.9044585824012756)
[2025-02-16 12:47:13,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:13,554][root][INFO] - Training Epoch: 1/2, step 10731/23838 completed (loss: 0.45048218965530396, acc: 0.8653846383094788)
[2025-02-16 12:47:13,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:14,006][root][INFO] - Training Epoch: 1/2, step 10732/23838 completed (loss: 0.7852814197540283, acc: 0.7868852615356445)
[2025-02-16 12:47:14,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:14,410][root][INFO] - Training Epoch: 1/2, step 10733/23838 completed (loss: 0.46391841769218445, acc: 0.8770492076873779)
[2025-02-16 12:47:14,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:14,981][root][INFO] - Training Epoch: 1/2, step 10734/23838 completed (loss: 0.7054311633110046, acc: 0.800000011920929)
[2025-02-16 12:47:15,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:15,419][root][INFO] - Training Epoch: 1/2, step 10735/23838 completed (loss: 0.3353961706161499, acc: 0.8969072103500366)
[2025-02-16 12:47:15,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:15,878][root][INFO] - Training Epoch: 1/2, step 10736/23838 completed (loss: 0.5978600382804871, acc: 0.8324324488639832)
[2025-02-16 12:47:16,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:16,461][root][INFO] - Training Epoch: 1/2, step 10737/23838 completed (loss: 0.7530749440193176, acc: 0.8058823347091675)
[2025-02-16 12:47:16,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:16,870][root][INFO] - Training Epoch: 1/2, step 10738/23838 completed (loss: 0.878270149230957, acc: 0.7557252049446106)
[2025-02-16 12:47:17,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:17,235][root][INFO] - Training Epoch: 1/2, step 10739/23838 completed (loss: 1.0401060581207275, acc: 0.6790123581886292)
[2025-02-16 12:47:17,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:17,655][root][INFO] - Training Epoch: 1/2, step 10740/23838 completed (loss: 0.9254183769226074, acc: 0.7280701994895935)
[2025-02-16 12:47:17,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:18,074][root][INFO] - Training Epoch: 1/2, step 10741/23838 completed (loss: 0.6852299571037292, acc: 0.7833333611488342)
[2025-02-16 12:47:18,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:18,615][root][INFO] - Training Epoch: 1/2, step 10742/23838 completed (loss: 0.7868421077728271, acc: 0.7803030014038086)
[2025-02-16 12:47:18,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:19,025][root][INFO] - Training Epoch: 1/2, step 10743/23838 completed (loss: 0.4067195951938629, acc: 0.8978102207183838)
[2025-02-16 12:47:19,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:19,489][root][INFO] - Training Epoch: 1/2, step 10744/23838 completed (loss: 0.5765681266784668, acc: 0.8513513803482056)
[2025-02-16 12:47:19,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:19,947][root][INFO] - Training Epoch: 1/2, step 10745/23838 completed (loss: 0.5487690567970276, acc: 0.801886796951294)
[2025-02-16 12:47:20,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:20,402][root][INFO] - Training Epoch: 1/2, step 10746/23838 completed (loss: 0.49787524342536926, acc: 0.8510638475418091)
[2025-02-16 12:47:20,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:20,884][root][INFO] - Training Epoch: 1/2, step 10747/23838 completed (loss: 0.37806829810142517, acc: 0.8961039185523987)
[2025-02-16 12:47:21,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:21,292][root][INFO] - Training Epoch: 1/2, step 10748/23838 completed (loss: 0.5061882734298706, acc: 0.8518518805503845)
[2025-02-16 12:47:21,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:21,680][root][INFO] - Training Epoch: 1/2, step 10749/23838 completed (loss: 0.7706034779548645, acc: 0.8125)
[2025-02-16 12:47:21,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:22,046][root][INFO] - Training Epoch: 1/2, step 10750/23838 completed (loss: 0.45974302291870117, acc: 0.8604651093482971)
[2025-02-16 12:47:22,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:22,449][root][INFO] - Training Epoch: 1/2, step 10751/23838 completed (loss: 0.4801064431667328, acc: 0.8333333134651184)
[2025-02-16 12:47:22,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:22,862][root][INFO] - Training Epoch: 1/2, step 10752/23838 completed (loss: 0.45018476247787476, acc: 0.8707482814788818)
[2025-02-16 12:47:23,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:23,686][root][INFO] - Training Epoch: 1/2, step 10753/23838 completed (loss: 0.44995060563087463, acc: 0.8849557638168335)
[2025-02-16 12:47:24,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:24,326][root][INFO] - Training Epoch: 1/2, step 10754/23838 completed (loss: 0.32838690280914307, acc: 0.9054054021835327)
[2025-02-16 12:47:24,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:24,806][root][INFO] - Training Epoch: 1/2, step 10755/23838 completed (loss: 0.6015454530715942, acc: 0.852173924446106)
[2025-02-16 12:47:25,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:25,295][root][INFO] - Training Epoch: 1/2, step 10756/23838 completed (loss: 0.798919141292572, acc: 0.8252426981925964)
[2025-02-16 12:47:25,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:25,702][root][INFO] - Training Epoch: 1/2, step 10757/23838 completed (loss: 0.7361168265342712, acc: 0.78125)
[2025-02-16 12:47:25,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:26,146][root][INFO] - Training Epoch: 1/2, step 10758/23838 completed (loss: 0.5076475143432617, acc: 0.8877550959587097)
[2025-02-16 12:47:26,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:26,597][root][INFO] - Training Epoch: 1/2, step 10759/23838 completed (loss: 0.6266991496086121, acc: 0.8220859169960022)
[2025-02-16 12:47:26,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:26,988][root][INFO] - Training Epoch: 1/2, step 10760/23838 completed (loss: 0.8326978087425232, acc: 0.7410714030265808)
[2025-02-16 12:47:27,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:27,431][root][INFO] - Training Epoch: 1/2, step 10761/23838 completed (loss: 0.6976978778839111, acc: 0.8187500238418579)
[2025-02-16 12:47:27,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:27,805][root][INFO] - Training Epoch: 1/2, step 10762/23838 completed (loss: 0.7210454940795898, acc: 0.8295454382896423)
[2025-02-16 12:47:28,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:28,247][root][INFO] - Training Epoch: 1/2, step 10763/23838 completed (loss: 1.0998369455337524, acc: 0.734375)
[2025-02-16 12:47:28,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:28,638][root][INFO] - Training Epoch: 1/2, step 10764/23838 completed (loss: 1.4255430698394775, acc: 0.65625)
[2025-02-16 12:47:28,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:29,033][root][INFO] - Training Epoch: 1/2, step 10765/23838 completed (loss: 1.0216008424758911, acc: 0.6399999856948853)
[2025-02-16 12:47:29,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:29,404][root][INFO] - Training Epoch: 1/2, step 10766/23838 completed (loss: 1.0940433740615845, acc: 0.7195122241973877)
[2025-02-16 12:47:29,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:29,791][root][INFO] - Training Epoch: 1/2, step 10767/23838 completed (loss: 1.3001011610031128, acc: 0.6538461446762085)
[2025-02-16 12:47:29,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:30,217][root][INFO] - Training Epoch: 1/2, step 10768/23838 completed (loss: 1.0686665773391724, acc: 0.7303370833396912)
[2025-02-16 12:47:30,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:30,598][root][INFO] - Training Epoch: 1/2, step 10769/23838 completed (loss: 0.5330522656440735, acc: 0.8571428656578064)
[2025-02-16 12:47:30,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:31,055][root][INFO] - Training Epoch: 1/2, step 10770/23838 completed (loss: 0.48901739716529846, acc: 0.8736842274665833)
[2025-02-16 12:47:31,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:31,501][root][INFO] - Training Epoch: 1/2, step 10771/23838 completed (loss: 0.3496136963367462, acc: 0.8960000276565552)
[2025-02-16 12:47:31,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:31,926][root][INFO] - Training Epoch: 1/2, step 10772/23838 completed (loss: 1.3550561666488647, acc: 0.6197183132171631)
[2025-02-16 12:47:32,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:32,405][root][INFO] - Training Epoch: 1/2, step 10773/23838 completed (loss: 0.5264251828193665, acc: 0.8767123222351074)
[2025-02-16 12:47:32,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:32,878][root][INFO] - Training Epoch: 1/2, step 10774/23838 completed (loss: 0.47530773282051086, acc: 0.8999999761581421)
[2025-02-16 12:47:33,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:33,346][root][INFO] - Training Epoch: 1/2, step 10775/23838 completed (loss: 0.4840017259120941, acc: 0.837837815284729)
[2025-02-16 12:47:33,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:33,750][root][INFO] - Training Epoch: 1/2, step 10776/23838 completed (loss: 0.31624916195869446, acc: 0.9191918969154358)
[2025-02-16 12:47:33,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:34,193][root][INFO] - Training Epoch: 1/2, step 10777/23838 completed (loss: 0.8558239340782166, acc: 0.796875)
[2025-02-16 12:47:34,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:34,664][root][INFO] - Training Epoch: 1/2, step 10778/23838 completed (loss: 0.5466110706329346, acc: 0.8214285969734192)
[2025-02-16 12:47:34,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:35,158][root][INFO] - Training Epoch: 1/2, step 10779/23838 completed (loss: 0.8141636848449707, acc: 0.8088235259056091)
[2025-02-16 12:47:35,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:35,597][root][INFO] - Training Epoch: 1/2, step 10780/23838 completed (loss: 0.5142644643783569, acc: 0.8914728760719299)
[2025-02-16 12:47:35,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:36,035][root][INFO] - Training Epoch: 1/2, step 10781/23838 completed (loss: 0.4000781178474426, acc: 0.8513513803482056)
[2025-02-16 12:47:36,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:36,496][root][INFO] - Training Epoch: 1/2, step 10782/23838 completed (loss: 0.3513067960739136, acc: 0.8987341523170471)
[2025-02-16 12:47:36,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:36,928][root][INFO] - Training Epoch: 1/2, step 10783/23838 completed (loss: 0.20685502886772156, acc: 0.9285714030265808)
[2025-02-16 12:47:37,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:37,388][root][INFO] - Training Epoch: 1/2, step 10784/23838 completed (loss: 0.504710853099823, acc: 0.8865979313850403)
[2025-02-16 12:47:37,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:37,804][root][INFO] - Training Epoch: 1/2, step 10785/23838 completed (loss: 0.3087741434574127, acc: 0.9197080135345459)
[2025-02-16 12:47:38,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:38,256][root][INFO] - Training Epoch: 1/2, step 10786/23838 completed (loss: 0.43843576312065125, acc: 0.8852459192276001)
[2025-02-16 12:47:38,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:38,702][root][INFO] - Training Epoch: 1/2, step 10787/23838 completed (loss: 0.47406530380249023, acc: 0.8333333134651184)
[2025-02-16 12:47:38,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:39,145][root][INFO] - Training Epoch: 1/2, step 10788/23838 completed (loss: 0.5389483571052551, acc: 0.8333333134651184)
[2025-02-16 12:47:39,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:39,584][root][INFO] - Training Epoch: 1/2, step 10789/23838 completed (loss: 0.5107125639915466, acc: 0.8125)
[2025-02-16 12:47:39,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:40,067][root][INFO] - Training Epoch: 1/2, step 10790/23838 completed (loss: 0.30995506048202515, acc: 0.9179104566574097)
[2025-02-16 12:47:40,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:40,511][root][INFO] - Training Epoch: 1/2, step 10791/23838 completed (loss: 0.22453399002552032, acc: 0.9452054500579834)
[2025-02-16 12:47:40,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:40,907][root][INFO] - Training Epoch: 1/2, step 10792/23838 completed (loss: 0.6684693098068237, acc: 0.8545454740524292)
[2025-02-16 12:47:41,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:41,349][root][INFO] - Training Epoch: 1/2, step 10793/23838 completed (loss: 0.38327768445014954, acc: 0.8802816867828369)
[2025-02-16 12:47:41,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:41,747][root][INFO] - Training Epoch: 1/2, step 10794/23838 completed (loss: 0.8575167655944824, acc: 0.7317073345184326)
[2025-02-16 12:47:41,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:42,188][root][INFO] - Training Epoch: 1/2, step 10795/23838 completed (loss: 0.7872388958930969, acc: 0.8139534592628479)
[2025-02-16 12:47:42,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:42,641][root][INFO] - Training Epoch: 1/2, step 10796/23838 completed (loss: 0.3272586464881897, acc: 0.9166666865348816)
[2025-02-16 12:47:42,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:43,072][root][INFO] - Training Epoch: 1/2, step 10797/23838 completed (loss: 0.3169682025909424, acc: 0.8947368264198303)
[2025-02-16 12:47:43,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:43,453][root][INFO] - Training Epoch: 1/2, step 10798/23838 completed (loss: 0.3407052457332611, acc: 0.9111111164093018)
[2025-02-16 12:47:43,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:43,955][root][INFO] - Training Epoch: 1/2, step 10799/23838 completed (loss: 0.3247913718223572, acc: 0.9136363863945007)
[2025-02-16 12:47:44,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:44,524][root][INFO] - Training Epoch: 1/2, step 10800/23838 completed (loss: 0.19746746122837067, acc: 0.9455252885818481)
[2025-02-16 12:47:44,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:45,007][root][INFO] - Training Epoch: 1/2, step 10801/23838 completed (loss: 0.18309685587882996, acc: 0.9642857313156128)
[2025-02-16 12:47:45,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:45,435][root][INFO] - Training Epoch: 1/2, step 10802/23838 completed (loss: 0.2963380217552185, acc: 0.9009901285171509)
[2025-02-16 12:47:45,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:45,874][root][INFO] - Training Epoch: 1/2, step 10803/23838 completed (loss: 0.1946188062429428, acc: 0.9473684430122375)
[2025-02-16 12:47:46,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:46,338][root][INFO] - Training Epoch: 1/2, step 10804/23838 completed (loss: 0.3113500475883484, acc: 0.9370078444480896)
[2025-02-16 12:47:46,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:46,822][root][INFO] - Training Epoch: 1/2, step 10805/23838 completed (loss: 0.36947306990623474, acc: 0.9126213788986206)
[2025-02-16 12:47:47,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:47,253][root][INFO] - Training Epoch: 1/2, step 10806/23838 completed (loss: 0.19190660119056702, acc: 0.9252336621284485)
[2025-02-16 12:47:47,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:47,779][root][INFO] - Training Epoch: 1/2, step 10807/23838 completed (loss: 0.38955965638160706, acc: 0.875)
[2025-02-16 12:47:48,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:48,327][root][INFO] - Training Epoch: 1/2, step 10808/23838 completed (loss: 0.1776389479637146, acc: 0.9677419066429138)
[2025-02-16 12:47:48,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:48,777][root][INFO] - Training Epoch: 1/2, step 10809/23838 completed (loss: 0.5524238348007202, acc: 0.8421052694320679)
[2025-02-16 12:47:49,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:49,340][root][INFO] - Training Epoch: 1/2, step 10810/23838 completed (loss: 0.9128854274749756, acc: 0.778761088848114)
[2025-02-16 12:47:49,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:50,065][root][INFO] - Training Epoch: 1/2, step 10811/23838 completed (loss: 0.15337079763412476, acc: 0.9482758641242981)
[2025-02-16 12:47:50,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:50,489][root][INFO] - Training Epoch: 1/2, step 10812/23838 completed (loss: 0.2882346212863922, acc: 0.9247311949729919)
[2025-02-16 12:47:50,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:51,125][root][INFO] - Training Epoch: 1/2, step 10813/23838 completed (loss: 0.22429011762142181, acc: 0.9428571462631226)
[2025-02-16 12:47:51,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:51,496][root][INFO] - Training Epoch: 1/2, step 10814/23838 completed (loss: 0.9529557824134827, acc: 0.738095223903656)
[2025-02-16 12:47:51,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:51,914][root][INFO] - Training Epoch: 1/2, step 10815/23838 completed (loss: 0.5050368905067444, acc: 0.9178082346916199)
[2025-02-16 12:47:52,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:52,795][root][INFO] - Training Epoch: 1/2, step 10816/23838 completed (loss: 0.22844387590885162, acc: 0.9562841653823853)
[2025-02-16 12:47:53,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:53,291][root][INFO] - Training Epoch: 1/2, step 10817/23838 completed (loss: 0.4244438707828522, acc: 0.8944099545478821)
[2025-02-16 12:47:53,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:53,750][root][INFO] - Training Epoch: 1/2, step 10818/23838 completed (loss: 0.5603699088096619, acc: 0.8585858345031738)
[2025-02-16 12:47:54,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:54,272][root][INFO] - Training Epoch: 1/2, step 10819/23838 completed (loss: 0.6550593972206116, acc: 0.8201438784599304)
[2025-02-16 12:47:54,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:54,810][root][INFO] - Training Epoch: 1/2, step 10820/23838 completed (loss: 0.6271235346794128, acc: 0.8172042965888977)
[2025-02-16 12:47:55,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:55,279][root][INFO] - Training Epoch: 1/2, step 10821/23838 completed (loss: 0.7321481704711914, acc: 0.7777777910232544)
[2025-02-16 12:47:55,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:55,706][root][INFO] - Training Epoch: 1/2, step 10822/23838 completed (loss: 0.37341439723968506, acc: 0.8873239159584045)
[2025-02-16 12:47:55,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:56,078][root][INFO] - Training Epoch: 1/2, step 10823/23838 completed (loss: 0.6082125306129456, acc: 0.8395061492919922)
[2025-02-16 12:47:56,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:56,512][root][INFO] - Training Epoch: 1/2, step 10824/23838 completed (loss: 0.7388468980789185, acc: 0.8082191944122314)
[2025-02-16 12:47:56,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:56,974][root][INFO] - Training Epoch: 1/2, step 10825/23838 completed (loss: 0.4625925123691559, acc: 0.8671875)
[2025-02-16 12:47:57,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:57,454][root][INFO] - Training Epoch: 1/2, step 10826/23838 completed (loss: 0.4154264032840729, acc: 0.8917197585105896)
[2025-02-16 12:47:57,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:57,904][root][INFO] - Training Epoch: 1/2, step 10827/23838 completed (loss: 0.46423959732055664, acc: 0.8604651093482971)
[2025-02-16 12:47:58,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:58,356][root][INFO] - Training Epoch: 1/2, step 10828/23838 completed (loss: 0.13872577250003815, acc: 0.9629629850387573)
[2025-02-16 12:47:58,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:58,816][root][INFO] - Training Epoch: 1/2, step 10829/23838 completed (loss: 0.6175872087478638, acc: 0.8539325594902039)
[2025-02-16 12:47:59,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:59,219][root][INFO] - Training Epoch: 1/2, step 10830/23838 completed (loss: 0.43827518820762634, acc: 0.8617021441459656)
[2025-02-16 12:47:59,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:47:59,657][root][INFO] - Training Epoch: 1/2, step 10831/23838 completed (loss: 0.4196125864982605, acc: 0.8817204236984253)
[2025-02-16 12:47:59,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:00,078][root][INFO] - Training Epoch: 1/2, step 10832/23838 completed (loss: 0.30857422947883606, acc: 0.9024389982223511)
[2025-02-16 12:48:00,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:00,486][root][INFO] - Training Epoch: 1/2, step 10833/23838 completed (loss: 0.619891881942749, acc: 0.8571428656578064)
[2025-02-16 12:48:00,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:00,993][root][INFO] - Training Epoch: 1/2, step 10834/23838 completed (loss: 0.39487549662590027, acc: 0.9032257795333862)
[2025-02-16 12:48:01,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:01,458][root][INFO] - Training Epoch: 1/2, step 10835/23838 completed (loss: 0.6377631425857544, acc: 0.8514851331710815)
[2025-02-16 12:48:01,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:01,926][root][INFO] - Training Epoch: 1/2, step 10836/23838 completed (loss: 0.2930126190185547, acc: 0.8791208863258362)
[2025-02-16 12:48:02,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:02,377][root][INFO] - Training Epoch: 1/2, step 10837/23838 completed (loss: 0.3015701174736023, acc: 0.9399999976158142)
[2025-02-16 12:48:02,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:02,783][root][INFO] - Training Epoch: 1/2, step 10838/23838 completed (loss: 0.4425548017024994, acc: 0.9166666865348816)
[2025-02-16 12:48:02,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:03,226][root][INFO] - Training Epoch: 1/2, step 10839/23838 completed (loss: 0.33863070607185364, acc: 0.9082568883895874)
[2025-02-16 12:48:03,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:03,808][root][INFO] - Training Epoch: 1/2, step 10840/23838 completed (loss: 0.5318976640701294, acc: 0.8791208863258362)
[2025-02-16 12:48:04,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:04,228][root][INFO] - Training Epoch: 1/2, step 10841/23838 completed (loss: 0.5367366075515747, acc: 0.8448275923728943)
[2025-02-16 12:48:04,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:04,672][root][INFO] - Training Epoch: 1/2, step 10842/23838 completed (loss: 0.2948859632015228, acc: 0.9237288236618042)
[2025-02-16 12:48:04,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:05,205][root][INFO] - Training Epoch: 1/2, step 10843/23838 completed (loss: 0.24507290124893188, acc: 0.9482758641242981)
[2025-02-16 12:48:05,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:05,697][root][INFO] - Training Epoch: 1/2, step 10844/23838 completed (loss: 0.29355984926223755, acc: 0.8916666507720947)
[2025-02-16 12:48:05,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:06,263][root][INFO] - Training Epoch: 1/2, step 10845/23838 completed (loss: 0.8096519708633423, acc: 0.769784152507782)
[2025-02-16 12:48:06,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:06,715][root][INFO] - Training Epoch: 1/2, step 10846/23838 completed (loss: 0.35094812512397766, acc: 0.9186046719551086)
[2025-02-16 12:48:06,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:07,195][root][INFO] - Training Epoch: 1/2, step 10847/23838 completed (loss: 0.5009697675704956, acc: 0.8515625)
[2025-02-16 12:48:07,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:07,654][root][INFO] - Training Epoch: 1/2, step 10848/23838 completed (loss: 0.17972666025161743, acc: 0.9481481313705444)
[2025-02-16 12:48:07,907][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:08,140][root][INFO] - Training Epoch: 1/2, step 10849/23838 completed (loss: 0.2251320630311966, acc: 0.9189189076423645)
[2025-02-16 12:48:08,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:08,595][root][INFO] - Training Epoch: 1/2, step 10850/23838 completed (loss: 0.5936124920845032, acc: 0.8476190567016602)
[2025-02-16 12:48:08,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:09,025][root][INFO] - Training Epoch: 1/2, step 10851/23838 completed (loss: 0.6138917803764343, acc: 0.8260869383811951)
[2025-02-16 12:48:09,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:09,452][root][INFO] - Training Epoch: 1/2, step 10852/23838 completed (loss: 0.568994402885437, acc: 0.8403361439704895)
[2025-02-16 12:48:09,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:09,983][root][INFO] - Training Epoch: 1/2, step 10853/23838 completed (loss: 0.19183599948883057, acc: 0.9433962106704712)
[2025-02-16 12:48:10,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:10,385][root][INFO] - Training Epoch: 1/2, step 10854/23838 completed (loss: 0.5463995933532715, acc: 0.8636363744735718)
[2025-02-16 12:48:10,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:10,839][root][INFO] - Training Epoch: 1/2, step 10855/23838 completed (loss: 0.2249726504087448, acc: 0.9213483333587646)
[2025-02-16 12:48:11,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:11,345][root][INFO] - Training Epoch: 1/2, step 10856/23838 completed (loss: 0.27723315358161926, acc: 0.9159663915634155)
[2025-02-16 12:48:11,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:11,771][root][INFO] - Training Epoch: 1/2, step 10857/23838 completed (loss: 0.18806986510753632, acc: 0.939130425453186)
[2025-02-16 12:48:11,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:12,222][root][INFO] - Training Epoch: 1/2, step 10858/23838 completed (loss: 0.21347537636756897, acc: 0.9365079402923584)
[2025-02-16 12:48:12,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:12,641][root][INFO] - Training Epoch: 1/2, step 10859/23838 completed (loss: 0.3483949303627014, acc: 0.9039999842643738)
[2025-02-16 12:48:12,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:13,166][root][INFO] - Training Epoch: 1/2, step 10860/23838 completed (loss: 0.6958184242248535, acc: 0.8306451439857483)
[2025-02-16 12:48:13,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:13,618][root][INFO] - Training Epoch: 1/2, step 10861/23838 completed (loss: 0.4495988190174103, acc: 0.8920454382896423)
[2025-02-16 12:48:13,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:14,026][root][INFO] - Training Epoch: 1/2, step 10862/23838 completed (loss: 0.1297643929719925, acc: 0.969924807548523)
[2025-02-16 12:48:14,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:14,406][root][INFO] - Training Epoch: 1/2, step 10863/23838 completed (loss: 0.34404900670051575, acc: 0.8761904835700989)
[2025-02-16 12:48:14,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:14,854][root][INFO] - Training Epoch: 1/2, step 10864/23838 completed (loss: 0.37554800510406494, acc: 0.8951048851013184)
[2025-02-16 12:48:15,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:15,274][root][INFO] - Training Epoch: 1/2, step 10865/23838 completed (loss: 0.3554668724536896, acc: 0.9090909361839294)
[2025-02-16 12:48:15,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:15,729][root][INFO] - Training Epoch: 1/2, step 10866/23838 completed (loss: 0.462202250957489, acc: 0.8571428656578064)
[2025-02-16 12:48:15,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:16,172][root][INFO] - Training Epoch: 1/2, step 10867/23838 completed (loss: 0.6637968420982361, acc: 0.800000011920929)
[2025-02-16 12:48:16,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:16,599][root][INFO] - Training Epoch: 1/2, step 10868/23838 completed (loss: 0.8043325543403625, acc: 0.7605633735656738)
[2025-02-16 12:48:16,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:17,044][root][INFO] - Training Epoch: 1/2, step 10869/23838 completed (loss: 0.30573469400405884, acc: 0.9230769276618958)
[2025-02-16 12:48:17,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:17,449][root][INFO] - Training Epoch: 1/2, step 10870/23838 completed (loss: 0.14175309240818024, acc: 0.9587156176567078)
[2025-02-16 12:48:17,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:17,866][root][INFO] - Training Epoch: 1/2, step 10871/23838 completed (loss: 0.1284133493900299, acc: 0.9772727489471436)
[2025-02-16 12:48:18,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:18,342][root][INFO] - Training Epoch: 1/2, step 10872/23838 completed (loss: 0.07268847525119781, acc: 0.9806451797485352)
[2025-02-16 12:48:18,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:18,749][root][INFO] - Training Epoch: 1/2, step 10873/23838 completed (loss: 0.13499398529529572, acc: 0.9571428298950195)
[2025-02-16 12:48:18,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:19,222][root][INFO] - Training Epoch: 1/2, step 10874/23838 completed (loss: 0.1340939998626709, acc: 0.9629629850387573)
[2025-02-16 12:48:19,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:19,808][root][INFO] - Training Epoch: 1/2, step 10875/23838 completed (loss: 0.14334149658679962, acc: 0.9677419066429138)
[2025-02-16 12:48:20,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:20,409][root][INFO] - Training Epoch: 1/2, step 10876/23838 completed (loss: 0.12085525691509247, acc: 0.9659863710403442)
[2025-02-16 12:48:20,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:21,019][root][INFO] - Training Epoch: 1/2, step 10877/23838 completed (loss: 0.569364607334137, acc: 0.8661417365074158)
[2025-02-16 12:48:21,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:21,898][root][INFO] - Training Epoch: 1/2, step 10878/23838 completed (loss: 0.403056800365448, acc: 0.9016393423080444)
[2025-02-16 12:48:22,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:22,399][root][INFO] - Training Epoch: 1/2, step 10879/23838 completed (loss: 0.29062017798423767, acc: 0.9473684430122375)
[2025-02-16 12:48:22,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:22,993][root][INFO] - Training Epoch: 1/2, step 10880/23838 completed (loss: 0.2643774151802063, acc: 0.9085714221000671)
[2025-02-16 12:48:23,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:23,429][root][INFO] - Training Epoch: 1/2, step 10881/23838 completed (loss: 0.39114266633987427, acc: 0.9358974099159241)
[2025-02-16 12:48:23,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:23,897][root][INFO] - Training Epoch: 1/2, step 10882/23838 completed (loss: 0.06952028721570969, acc: 0.9807692170143127)
[2025-02-16 12:48:24,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:24,418][root][INFO] - Training Epoch: 1/2, step 10883/23838 completed (loss: 0.6929649710655212, acc: 0.8163265585899353)
[2025-02-16 12:48:24,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:24,894][root][INFO] - Training Epoch: 1/2, step 10884/23838 completed (loss: 0.5812814235687256, acc: 0.8450704216957092)
[2025-02-16 12:48:25,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:25,380][root][INFO] - Training Epoch: 1/2, step 10885/23838 completed (loss: 0.475596159696579, acc: 0.8768116235733032)
[2025-02-16 12:48:25,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:25,824][root][INFO] - Training Epoch: 1/2, step 10886/23838 completed (loss: 0.3415586054325104, acc: 0.8909090757369995)
[2025-02-16 12:48:26,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:26,445][root][INFO] - Training Epoch: 1/2, step 10887/23838 completed (loss: 0.2901046872138977, acc: 0.9236111044883728)
[2025-02-16 12:48:26,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:26,883][root][INFO] - Training Epoch: 1/2, step 10888/23838 completed (loss: 0.549964964389801, acc: 0.8627451062202454)
[2025-02-16 12:48:27,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:27,394][root][INFO] - Training Epoch: 1/2, step 10889/23838 completed (loss: 0.16756615042686462, acc: 0.9484127163887024)
[2025-02-16 12:48:27,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:27,819][root][INFO] - Training Epoch: 1/2, step 10890/23838 completed (loss: 0.3256304860115051, acc: 0.9190751314163208)
[2025-02-16 12:48:28,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:28,240][root][INFO] - Training Epoch: 1/2, step 10891/23838 completed (loss: 0.40104931592941284, acc: 0.8671875)
[2025-02-16 12:48:28,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:28,639][root][INFO] - Training Epoch: 1/2, step 10892/23838 completed (loss: 0.32456985116004944, acc: 0.9126213788986206)
[2025-02-16 12:48:28,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:29,151][root][INFO] - Training Epoch: 1/2, step 10893/23838 completed (loss: 0.23535288870334625, acc: 0.931506872177124)
[2025-02-16 12:48:29,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:29,538][root][INFO] - Training Epoch: 1/2, step 10894/23838 completed (loss: 0.351138710975647, acc: 0.893081784248352)
[2025-02-16 12:48:29,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:29,995][root][INFO] - Training Epoch: 1/2, step 10895/23838 completed (loss: 0.5064047574996948, acc: 0.8208954930305481)
[2025-02-16 12:48:30,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:30,449][root][INFO] - Training Epoch: 1/2, step 10896/23838 completed (loss: 0.46058088541030884, acc: 0.8870967626571655)
[2025-02-16 12:48:30,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:30,884][root][INFO] - Training Epoch: 1/2, step 10897/23838 completed (loss: 0.3048401474952698, acc: 0.8852459192276001)
[2025-02-16 12:48:31,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:31,340][root][INFO] - Training Epoch: 1/2, step 10898/23838 completed (loss: 0.4137776792049408, acc: 0.8500000238418579)
[2025-02-16 12:48:31,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:31,753][root][INFO] - Training Epoch: 1/2, step 10899/23838 completed (loss: 0.24146319925785065, acc: 0.969072163105011)
[2025-02-16 12:48:32,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:32,346][root][INFO] - Training Epoch: 1/2, step 10900/23838 completed (loss: 0.5674225091934204, acc: 0.8450704216957092)
[2025-02-16 12:48:32,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:32,772][root][INFO] - Training Epoch: 1/2, step 10901/23838 completed (loss: 0.3111187815666199, acc: 0.9045225977897644)
[2025-02-16 12:48:32,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:33,231][root][INFO] - Training Epoch: 1/2, step 10902/23838 completed (loss: 0.39534619450569153, acc: 0.8881118893623352)
[2025-02-16 12:48:33,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:33,735][root][INFO] - Training Epoch: 1/2, step 10903/23838 completed (loss: 0.46151256561279297, acc: 0.885496199131012)
[2025-02-16 12:48:33,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:34,216][root][INFO] - Training Epoch: 1/2, step 10904/23838 completed (loss: 0.41696545481681824, acc: 0.896774172782898)
[2025-02-16 12:48:34,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:34,694][root][INFO] - Training Epoch: 1/2, step 10905/23838 completed (loss: 0.5227446556091309, acc: 0.8736842274665833)
[2025-02-16 12:48:34,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:35,101][root][INFO] - Training Epoch: 1/2, step 10906/23838 completed (loss: 0.20629528164863586, acc: 0.9312499761581421)
[2025-02-16 12:48:35,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:35,536][root][INFO] - Training Epoch: 1/2, step 10907/23838 completed (loss: 0.2830457091331482, acc: 0.9213483333587646)
[2025-02-16 12:48:35,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:36,316][root][INFO] - Training Epoch: 1/2, step 10908/23838 completed (loss: 0.49593397974967957, acc: 0.8693467378616333)
[2025-02-16 12:48:36,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:36,827][root][INFO] - Training Epoch: 1/2, step 10909/23838 completed (loss: 0.2528028190135956, acc: 0.9175257682800293)
[2025-02-16 12:48:37,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:37,359][root][INFO] - Training Epoch: 1/2, step 10910/23838 completed (loss: 0.27593177556991577, acc: 0.929347813129425)
[2025-02-16 12:48:37,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:38,169][root][INFO] - Training Epoch: 1/2, step 10911/23838 completed (loss: 0.27513766288757324, acc: 0.9275362491607666)
[2025-02-16 12:48:38,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:38,619][root][INFO] - Training Epoch: 1/2, step 10912/23838 completed (loss: 0.17559976875782013, acc: 0.949999988079071)
[2025-02-16 12:48:38,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:39,147][root][INFO] - Training Epoch: 1/2, step 10913/23838 completed (loss: 0.22909824550151825, acc: 0.9336493015289307)
[2025-02-16 12:48:39,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:39,547][root][INFO] - Training Epoch: 1/2, step 10914/23838 completed (loss: 0.2246730923652649, acc: 0.9438202381134033)
[2025-02-16 12:48:39,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:40,014][root][INFO] - Training Epoch: 1/2, step 10915/23838 completed (loss: 0.5563498139381409, acc: 0.8823529481887817)
[2025-02-16 12:48:40,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:40,495][root][INFO] - Training Epoch: 1/2, step 10916/23838 completed (loss: 0.32067057490348816, acc: 0.9178743958473206)
[2025-02-16 12:48:40,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:40,992][root][INFO] - Training Epoch: 1/2, step 10917/23838 completed (loss: 0.25485268235206604, acc: 0.920634925365448)
[2025-02-16 12:48:41,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:41,482][root][INFO] - Training Epoch: 1/2, step 10918/23838 completed (loss: 0.23387323319911957, acc: 0.9567307829856873)
[2025-02-16 12:48:41,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:41,987][root][INFO] - Training Epoch: 1/2, step 10919/23838 completed (loss: 0.3166585862636566, acc: 0.9170507192611694)
[2025-02-16 12:48:42,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:42,408][root][INFO] - Training Epoch: 1/2, step 10920/23838 completed (loss: 0.34040284156799316, acc: 0.9068322777748108)
[2025-02-16 12:48:42,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:42,890][root][INFO] - Training Epoch: 1/2, step 10921/23838 completed (loss: 0.4208183288574219, acc: 0.8861788511276245)
[2025-02-16 12:48:43,082][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:43,331][root][INFO] - Training Epoch: 1/2, step 10922/23838 completed (loss: 0.2441519945859909, acc: 0.9153845906257629)
[2025-02-16 12:48:43,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:43,732][root][INFO] - Training Epoch: 1/2, step 10923/23838 completed (loss: 0.30812355875968933, acc: 0.9171974658966064)
[2025-02-16 12:48:43,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:44,233][root][INFO] - Training Epoch: 1/2, step 10924/23838 completed (loss: 0.3764767646789551, acc: 0.8833333253860474)
[2025-02-16 12:48:44,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:44,714][root][INFO] - Training Epoch: 1/2, step 10925/23838 completed (loss: 0.2914699614048004, acc: 0.9047619104385376)
[2025-02-16 12:48:44,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:45,181][root][INFO] - Training Epoch: 1/2, step 10926/23838 completed (loss: 0.31273719668388367, acc: 0.9107142686843872)
[2025-02-16 12:48:45,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:45,682][root][INFO] - Training Epoch: 1/2, step 10927/23838 completed (loss: 0.22726595401763916, acc: 0.9609375)
[2025-02-16 12:48:45,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:46,156][root][INFO] - Training Epoch: 1/2, step 10928/23838 completed (loss: 0.21505045890808105, acc: 0.9485294222831726)
[2025-02-16 12:48:46,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:46,564][root][INFO] - Training Epoch: 1/2, step 10929/23838 completed (loss: 0.11122483015060425, acc: 0.9555555582046509)
[2025-02-16 12:48:46,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:47,101][root][INFO] - Training Epoch: 1/2, step 10930/23838 completed (loss: 0.2522432208061218, acc: 0.9345794320106506)
[2025-02-16 12:48:47,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:47,550][root][INFO] - Training Epoch: 1/2, step 10931/23838 completed (loss: 0.41448450088500977, acc: 0.8914728760719299)
[2025-02-16 12:48:47,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:48,151][root][INFO] - Training Epoch: 1/2, step 10932/23838 completed (loss: 0.14740806818008423, acc: 0.9555555582046509)
[2025-02-16 12:48:48,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:48,675][root][INFO] - Training Epoch: 1/2, step 10933/23838 completed (loss: 0.43846452236175537, acc: 0.8842975497245789)
[2025-02-16 12:48:49,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:49,370][root][INFO] - Training Epoch: 1/2, step 10934/23838 completed (loss: 0.24951273202896118, acc: 0.9234042763710022)
[2025-02-16 12:48:49,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:49,833][root][INFO] - Training Epoch: 1/2, step 10935/23838 completed (loss: 0.204056054353714, acc: 0.940119743347168)
[2025-02-16 12:48:50,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:50,269][root][INFO] - Training Epoch: 1/2, step 10936/23838 completed (loss: 0.2543887197971344, acc: 0.9304347634315491)
[2025-02-16 12:48:50,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:50,665][root][INFO] - Training Epoch: 1/2, step 10937/23838 completed (loss: 0.21324008703231812, acc: 0.9380530714988708)
[2025-02-16 12:48:50,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:51,135][root][INFO] - Training Epoch: 1/2, step 10938/23838 completed (loss: 0.5761215090751648, acc: 0.8589743375778198)
[2025-02-16 12:48:51,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:51,556][root][INFO] - Training Epoch: 1/2, step 10939/23838 completed (loss: 0.21110765635967255, acc: 0.9468085169792175)
[2025-02-16 12:48:51,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:52,129][root][INFO] - Training Epoch: 1/2, step 10940/23838 completed (loss: 0.16868184506893158, acc: 0.939393937587738)
[2025-02-16 12:48:52,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:52,548][root][INFO] - Training Epoch: 1/2, step 10941/23838 completed (loss: 0.1819271743297577, acc: 0.9649122953414917)
[2025-02-16 12:48:52,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:52,982][root][INFO] - Training Epoch: 1/2, step 10942/23838 completed (loss: 0.4289003908634186, acc: 0.8974359035491943)
[2025-02-16 12:48:53,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:53,461][root][INFO] - Training Epoch: 1/2, step 10943/23838 completed (loss: 0.1528577208518982, acc: 0.9441340565681458)
[2025-02-16 12:48:53,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:54,022][root][INFO] - Training Epoch: 1/2, step 10944/23838 completed (loss: 0.15471172332763672, acc: 0.9668049812316895)
[2025-02-16 12:48:54,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:54,664][root][INFO] - Training Epoch: 1/2, step 10945/23838 completed (loss: 0.22361284494400024, acc: 0.9308755993843079)
[2025-02-16 12:48:54,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:55,070][root][INFO] - Training Epoch: 1/2, step 10946/23838 completed (loss: 0.37803542613983154, acc: 0.8888888955116272)
[2025-02-16 12:48:55,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:55,563][root][INFO] - Training Epoch: 1/2, step 10947/23838 completed (loss: 0.4139828383922577, acc: 0.8928571343421936)
[2025-02-16 12:48:55,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:56,069][root][INFO] - Training Epoch: 1/2, step 10948/23838 completed (loss: 0.4066212773323059, acc: 0.8947368264198303)
[2025-02-16 12:48:56,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:56,536][root][INFO] - Training Epoch: 1/2, step 10949/23838 completed (loss: 0.14895933866500854, acc: 0.9414634108543396)
[2025-02-16 12:48:56,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:57,072][root][INFO] - Training Epoch: 1/2, step 10950/23838 completed (loss: 0.18442079424858093, acc: 0.9622641801834106)
[2025-02-16 12:48:57,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:57,462][root][INFO] - Training Epoch: 1/2, step 10951/23838 completed (loss: 0.4392683506011963, acc: 0.8846153616905212)
[2025-02-16 12:48:57,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:57,939][root][INFO] - Training Epoch: 1/2, step 10952/23838 completed (loss: 0.2698070704936981, acc: 0.9124087691307068)
[2025-02-16 12:48:58,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:58,447][root][INFO] - Training Epoch: 1/2, step 10953/23838 completed (loss: 0.28070491552352905, acc: 0.90625)
[2025-02-16 12:48:58,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:58,890][root][INFO] - Training Epoch: 1/2, step 10954/23838 completed (loss: 0.24912084639072418, acc: 0.9290322661399841)
[2025-02-16 12:48:59,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:59,432][root][INFO] - Training Epoch: 1/2, step 10955/23838 completed (loss: 0.13579288125038147, acc: 0.9497206807136536)
[2025-02-16 12:48:59,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:48:59,839][root][INFO] - Training Epoch: 1/2, step 10956/23838 completed (loss: 0.2449270486831665, acc: 0.8999999761581421)
[2025-02-16 12:49:00,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:00,735][root][INFO] - Training Epoch: 1/2, step 10957/23838 completed (loss: 0.3464058041572571, acc: 0.9047619104385376)
[2025-02-16 12:49:00,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:01,182][root][INFO] - Training Epoch: 1/2, step 10958/23838 completed (loss: 0.27762722969055176, acc: 0.9166666865348816)
[2025-02-16 12:49:01,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:01,794][root][INFO] - Training Epoch: 1/2, step 10959/23838 completed (loss: 0.24360176920890808, acc: 0.9444444179534912)
[2025-02-16 12:49:02,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:02,253][root][INFO] - Training Epoch: 1/2, step 10960/23838 completed (loss: 0.2887985110282898, acc: 0.9021739363670349)
[2025-02-16 12:49:02,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:02,656][root][INFO] - Training Epoch: 1/2, step 10961/23838 completed (loss: 0.1763768345117569, acc: 0.9652777910232544)
[2025-02-16 12:49:02,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:03,100][root][INFO] - Training Epoch: 1/2, step 10962/23838 completed (loss: 0.32713600993156433, acc: 0.9217391014099121)
[2025-02-16 12:49:03,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:03,547][root][INFO] - Training Epoch: 1/2, step 10963/23838 completed (loss: 0.10214947164058685, acc: 0.9708737730979919)
[2025-02-16 12:49:03,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:04,010][root][INFO] - Training Epoch: 1/2, step 10964/23838 completed (loss: 0.24803076684474945, acc: 0.9067796468734741)
[2025-02-16 12:49:04,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:04,466][root][INFO] - Training Epoch: 1/2, step 10965/23838 completed (loss: 0.2430805116891861, acc: 0.9469026327133179)
[2025-02-16 12:49:04,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:04,942][root][INFO] - Training Epoch: 1/2, step 10966/23838 completed (loss: 0.22455275058746338, acc: 0.9577465057373047)
[2025-02-16 12:49:05,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:05,461][root][INFO] - Training Epoch: 1/2, step 10967/23838 completed (loss: 0.17298059165477753, acc: 0.9230769276618958)
[2025-02-16 12:49:05,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:05,934][root][INFO] - Training Epoch: 1/2, step 10968/23838 completed (loss: 0.07733682543039322, acc: 0.988095223903656)
[2025-02-16 12:49:06,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:06,429][root][INFO] - Training Epoch: 1/2, step 10969/23838 completed (loss: 0.062165986746549606, acc: 0.988095223903656)
[2025-02-16 12:49:06,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:07,121][root][INFO] - Training Epoch: 1/2, step 10970/23838 completed (loss: 0.3523295819759369, acc: 0.9063829779624939)
[2025-02-16 12:49:07,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:07,733][root][INFO] - Training Epoch: 1/2, step 10971/23838 completed (loss: 0.12920071184635162, acc: 0.949438214302063)
[2025-02-16 12:49:07,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:08,241][root][INFO] - Training Epoch: 1/2, step 10972/23838 completed (loss: 0.36265116930007935, acc: 0.9117646813392639)
[2025-02-16 12:49:08,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:08,956][root][INFO] - Training Epoch: 1/2, step 10973/23838 completed (loss: 0.24905569851398468, acc: 0.940397322177887)
[2025-02-16 12:49:09,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:09,445][root][INFO] - Training Epoch: 1/2, step 10974/23838 completed (loss: 0.1736096888780594, acc: 0.9408283829689026)
[2025-02-16 12:49:09,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:09,975][root][INFO] - Training Epoch: 1/2, step 10975/23838 completed (loss: 0.1765669584274292, acc: 0.942307710647583)
[2025-02-16 12:49:10,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:10,382][root][INFO] - Training Epoch: 1/2, step 10976/23838 completed (loss: 0.4537472426891327, acc: 0.890625)
[2025-02-16 12:49:10,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:10,854][root][INFO] - Training Epoch: 1/2, step 10977/23838 completed (loss: 0.38144582509994507, acc: 0.9078013896942139)
[2025-02-16 12:49:11,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:11,504][root][INFO] - Training Epoch: 1/2, step 10978/23838 completed (loss: 0.20583705604076385, acc: 0.9329897165298462)
[2025-02-16 12:49:11,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:12,265][root][INFO] - Training Epoch: 1/2, step 10979/23838 completed (loss: 0.24630405008792877, acc: 0.9308755993843079)
[2025-02-16 12:49:12,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:12,848][root][INFO] - Training Epoch: 1/2, step 10980/23838 completed (loss: 0.25882628560066223, acc: 0.9327731132507324)
[2025-02-16 12:49:13,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:13,291][root][INFO] - Training Epoch: 1/2, step 10981/23838 completed (loss: 0.2578316330909729, acc: 0.9112149477005005)
[2025-02-16 12:49:13,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:13,732][root][INFO] - Training Epoch: 1/2, step 10982/23838 completed (loss: 0.44936245679855347, acc: 0.9056603908538818)
[2025-02-16 12:49:14,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:14,719][root][INFO] - Training Epoch: 1/2, step 10983/23838 completed (loss: 0.39813941717147827, acc: 0.8940678238868713)
[2025-02-16 12:49:14,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:15,241][root][INFO] - Training Epoch: 1/2, step 10984/23838 completed (loss: 0.263519287109375, acc: 0.9378882050514221)
[2025-02-16 12:49:15,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:15,743][root][INFO] - Training Epoch: 1/2, step 10985/23838 completed (loss: 0.38826796412467957, acc: 0.8918918967247009)
[2025-02-16 12:49:16,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:16,288][root][INFO] - Training Epoch: 1/2, step 10986/23838 completed (loss: 0.16671672463417053, acc: 0.9411764740943909)
[2025-02-16 12:49:16,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:16,956][root][INFO] - Training Epoch: 1/2, step 10987/23838 completed (loss: 0.22572211921215057, acc: 0.9234042763710022)
[2025-02-16 12:49:17,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:17,407][root][INFO] - Training Epoch: 1/2, step 10988/23838 completed (loss: 0.5329000353813171, acc: 0.893203854560852)
[2025-02-16 12:49:17,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:17,897][root][INFO] - Training Epoch: 1/2, step 10989/23838 completed (loss: 0.2875039577484131, acc: 0.9008264541625977)
[2025-02-16 12:49:18,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:18,413][root][INFO] - Training Epoch: 1/2, step 10990/23838 completed (loss: 0.1465073972940445, acc: 0.9639639854431152)
[2025-02-16 12:49:18,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:18,899][root][INFO] - Training Epoch: 1/2, step 10991/23838 completed (loss: 0.2976926267147064, acc: 0.8985507488250732)
[2025-02-16 12:49:19,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:19,301][root][INFO] - Training Epoch: 1/2, step 10992/23838 completed (loss: 0.3516518771648407, acc: 0.8999999761581421)
[2025-02-16 12:49:19,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:19,724][root][INFO] - Training Epoch: 1/2, step 10993/23838 completed (loss: 0.21149414777755737, acc: 0.9285714030265808)
[2025-02-16 12:49:19,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:20,140][root][INFO] - Training Epoch: 1/2, step 10994/23838 completed (loss: 0.2228122055530548, acc: 0.9318181872367859)
[2025-02-16 12:49:20,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:20,556][root][INFO] - Training Epoch: 1/2, step 10995/23838 completed (loss: 0.21668830513954163, acc: 0.9215686321258545)
[2025-02-16 12:49:20,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:20,988][root][INFO] - Training Epoch: 1/2, step 10996/23838 completed (loss: 0.3842189311981201, acc: 0.8970588445663452)
[2025-02-16 12:49:21,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:21,410][root][INFO] - Training Epoch: 1/2, step 10997/23838 completed (loss: 0.45526322722435, acc: 0.8925619721412659)
[2025-02-16 12:49:21,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:21,785][root][INFO] - Training Epoch: 1/2, step 10998/23838 completed (loss: 0.36985358595848083, acc: 0.90625)
[2025-02-16 12:49:21,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:22,187][root][INFO] - Training Epoch: 1/2, step 10999/23838 completed (loss: 0.2375498265028, acc: 0.9358974099159241)
[2025-02-16 12:49:22,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:22,590][root][INFO] - Training Epoch: 1/2, step 11000/23838 completed (loss: 0.3367874324321747, acc: 0.9014084339141846)
[2025-02-16 12:49:22,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:23,045][root][INFO] - Training Epoch: 1/2, step 11001/23838 completed (loss: 0.0779283344745636, acc: 0.9736841917037964)
[2025-02-16 12:49:23,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:23,544][root][INFO] - Training Epoch: 1/2, step 11002/23838 completed (loss: 0.2228843718767166, acc: 0.9509803652763367)
[2025-02-16 12:49:23,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:24,010][root][INFO] - Training Epoch: 1/2, step 11003/23838 completed (loss: 0.15632182359695435, acc: 0.9555555582046509)
[2025-02-16 12:49:24,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:24,487][root][INFO] - Training Epoch: 1/2, step 11004/23838 completed (loss: 0.19615979492664337, acc: 0.9204545617103577)
[2025-02-16 12:49:24,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:24,948][root][INFO] - Training Epoch: 1/2, step 11005/23838 completed (loss: 0.30348870158195496, acc: 0.8901098966598511)
[2025-02-16 12:49:25,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:25,490][root][INFO] - Training Epoch: 1/2, step 11006/23838 completed (loss: 0.24782948195934296, acc: 0.921875)
[2025-02-16 12:49:25,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:25,955][root][INFO] - Training Epoch: 1/2, step 11007/23838 completed (loss: 0.2411116659641266, acc: 0.9080459475517273)
[2025-02-16 12:49:26,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:26,367][root][INFO] - Training Epoch: 1/2, step 11008/23838 completed (loss: 0.19516506791114807, acc: 0.956204354763031)
[2025-02-16 12:49:26,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:26,758][root][INFO] - Training Epoch: 1/2, step 11009/23838 completed (loss: 0.2124236524105072, acc: 0.9244186282157898)
[2025-02-16 12:49:27,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:27,350][root][INFO] - Training Epoch: 1/2, step 11010/23838 completed (loss: 0.2564789950847626, acc: 0.9353448152542114)
[2025-02-16 12:49:27,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:27,829][root][INFO] - Training Epoch: 1/2, step 11011/23838 completed (loss: 0.21658115088939667, acc: 0.9416058659553528)
[2025-02-16 12:49:28,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:28,246][root][INFO] - Training Epoch: 1/2, step 11012/23838 completed (loss: 0.9072152972221375, acc: 0.7653061151504517)
[2025-02-16 12:49:28,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:28,773][root][INFO] - Training Epoch: 1/2, step 11013/23838 completed (loss: 0.22887147963047028, acc: 0.9135135412216187)
[2025-02-16 12:49:28,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:29,189][root][INFO] - Training Epoch: 1/2, step 11014/23838 completed (loss: 0.14460565149784088, acc: 0.9497487545013428)
[2025-02-16 12:49:29,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:29,669][root][INFO] - Training Epoch: 1/2, step 11015/23838 completed (loss: 0.0995379239320755, acc: 0.976047933101654)
[2025-02-16 12:49:29,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:30,191][root][INFO] - Training Epoch: 1/2, step 11016/23838 completed (loss: 0.39627885818481445, acc: 0.9230769276618958)
[2025-02-16 12:49:30,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:30,681][root][INFO] - Training Epoch: 1/2, step 11017/23838 completed (loss: 0.16763140261173248, acc: 0.9567307829856873)
[2025-02-16 12:49:30,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:31,180][root][INFO] - Training Epoch: 1/2, step 11018/23838 completed (loss: 0.18237480521202087, acc: 0.9403508901596069)
[2025-02-16 12:49:31,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:31,608][root][INFO] - Training Epoch: 1/2, step 11019/23838 completed (loss: 0.3397808372974396, acc: 0.9126983880996704)
[2025-02-16 12:49:31,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:32,076][root][INFO] - Training Epoch: 1/2, step 11020/23838 completed (loss: 0.43295344710350037, acc: 0.8971428275108337)
[2025-02-16 12:49:32,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:32,636][root][INFO] - Training Epoch: 1/2, step 11021/23838 completed (loss: 0.08181793242692947, acc: 0.9767441749572754)
[2025-02-16 12:49:32,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:33,080][root][INFO] - Training Epoch: 1/2, step 11022/23838 completed (loss: 0.25904738903045654, acc: 0.95652174949646)
[2025-02-16 12:49:33,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:33,516][root][INFO] - Training Epoch: 1/2, step 11023/23838 completed (loss: 0.517629086971283, acc: 0.8409090638160706)
[2025-02-16 12:49:33,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:33,969][root][INFO] - Training Epoch: 1/2, step 11024/23838 completed (loss: 0.2744327783584595, acc: 0.9154929518699646)
[2025-02-16 12:49:34,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:34,450][root][INFO] - Training Epoch: 1/2, step 11025/23838 completed (loss: 0.21385130286216736, acc: 0.9551569223403931)
[2025-02-16 12:49:34,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:34,934][root][INFO] - Training Epoch: 1/2, step 11026/23838 completed (loss: 0.3433661460876465, acc: 0.9060773253440857)
[2025-02-16 12:49:35,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:35,395][root][INFO] - Training Epoch: 1/2, step 11027/23838 completed (loss: 0.3766128122806549, acc: 0.9038461446762085)
[2025-02-16 12:49:35,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:35,857][root][INFO] - Training Epoch: 1/2, step 11028/23838 completed (loss: 0.6542989611625671, acc: 0.8476190567016602)
[2025-02-16 12:49:36,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:36,327][root][INFO] - Training Epoch: 1/2, step 11029/23838 completed (loss: 0.2731611728668213, acc: 0.920634925365448)
[2025-02-16 12:49:36,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:36,837][root][INFO] - Training Epoch: 1/2, step 11030/23838 completed (loss: 0.433449923992157, acc: 0.8941176533699036)
[2025-02-16 12:49:37,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:37,296][root][INFO] - Training Epoch: 1/2, step 11031/23838 completed (loss: 0.29648157954216003, acc: 0.8999999761581421)
[2025-02-16 12:49:37,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:37,750][root][INFO] - Training Epoch: 1/2, step 11032/23838 completed (loss: 0.43580561876296997, acc: 0.8899999856948853)
[2025-02-16 12:49:37,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:38,200][root][INFO] - Training Epoch: 1/2, step 11033/23838 completed (loss: 0.39558711647987366, acc: 0.8962264060974121)
[2025-02-16 12:49:38,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:38,686][root][INFO] - Training Epoch: 1/2, step 11034/23838 completed (loss: 0.37907353043556213, acc: 0.9041916131973267)
[2025-02-16 12:49:38,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:39,171][root][INFO] - Training Epoch: 1/2, step 11035/23838 completed (loss: 0.17909374833106995, acc: 0.945652186870575)
[2025-02-16 12:49:39,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:39,617][root][INFO] - Training Epoch: 1/2, step 11036/23838 completed (loss: 0.6152501106262207, acc: 0.8318583965301514)
[2025-02-16 12:49:39,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:40,150][root][INFO] - Training Epoch: 1/2, step 11037/23838 completed (loss: 0.2006293088197708, acc: 0.9553571343421936)
[2025-02-16 12:49:40,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:40,662][root][INFO] - Training Epoch: 1/2, step 11038/23838 completed (loss: 0.1947217732667923, acc: 0.9666666388511658)
[2025-02-16 12:49:40,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:41,115][root][INFO] - Training Epoch: 1/2, step 11039/23838 completed (loss: 0.37251555919647217, acc: 0.8888888955116272)
[2025-02-16 12:49:41,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:41,523][root][INFO] - Training Epoch: 1/2, step 11040/23838 completed (loss: 0.2757897675037384, acc: 0.9193548560142517)
[2025-02-16 12:49:41,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:42,015][root][INFO] - Training Epoch: 1/2, step 11041/23838 completed (loss: 0.3117026686668396, acc: 0.9180327653884888)
[2025-02-16 12:49:42,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:42,465][root][INFO] - Training Epoch: 1/2, step 11042/23838 completed (loss: 0.17602229118347168, acc: 0.9454545378684998)
[2025-02-16 12:49:42,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:42,855][root][INFO] - Training Epoch: 1/2, step 11043/23838 completed (loss: 0.1945299506187439, acc: 0.9527027010917664)
[2025-02-16 12:49:42,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:43,286][root][INFO] - Training Epoch: 1/2, step 11044/23838 completed (loss: 0.34781432151794434, acc: 0.9107142686843872)
[2025-02-16 12:49:43,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:43,763][root][INFO] - Training Epoch: 1/2, step 11045/23838 completed (loss: 0.3208734393119812, acc: 0.918181836605072)
[2025-02-16 12:49:44,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:44,300][root][INFO] - Training Epoch: 1/2, step 11046/23838 completed (loss: 0.21174286305904388, acc: 0.9226519465446472)
[2025-02-16 12:49:44,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:44,727][root][INFO] - Training Epoch: 1/2, step 11047/23838 completed (loss: 0.1922486573457718, acc: 0.9473684430122375)
[2025-02-16 12:49:44,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:45,135][root][INFO] - Training Epoch: 1/2, step 11048/23838 completed (loss: 0.2439904659986496, acc: 0.9371980428695679)
[2025-02-16 12:49:45,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:45,557][root][INFO] - Training Epoch: 1/2, step 11049/23838 completed (loss: 0.30816882848739624, acc: 0.9270833134651184)
[2025-02-16 12:49:45,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:46,004][root][INFO] - Training Epoch: 1/2, step 11050/23838 completed (loss: 0.23760443925857544, acc: 0.9047619104385376)
[2025-02-16 12:49:46,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:46,535][root][INFO] - Training Epoch: 1/2, step 11051/23838 completed (loss: 0.13079197704792023, acc: 0.9655172228813171)
[2025-02-16 12:49:46,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:46,991][root][INFO] - Training Epoch: 1/2, step 11052/23838 completed (loss: 0.31363001465797424, acc: 0.9161290526390076)
[2025-02-16 12:49:47,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:47,385][root][INFO] - Training Epoch: 1/2, step 11053/23838 completed (loss: 0.23786067962646484, acc: 0.9407407641410828)
[2025-02-16 12:49:47,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:47,771][root][INFO] - Training Epoch: 1/2, step 11054/23838 completed (loss: 0.2849247455596924, acc: 0.9009901285171509)
[2025-02-16 12:49:47,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:48,237][root][INFO] - Training Epoch: 1/2, step 11055/23838 completed (loss: 0.18104799091815948, acc: 0.9438202381134033)
[2025-02-16 12:49:48,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:48,941][root][INFO] - Training Epoch: 1/2, step 11056/23838 completed (loss: 0.31352391839027405, acc: 0.9026548862457275)
[2025-02-16 12:49:49,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:49,400][root][INFO] - Training Epoch: 1/2, step 11057/23838 completed (loss: 0.1419675499200821, acc: 0.9729729890823364)
[2025-02-16 12:49:49,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:49,845][root][INFO] - Training Epoch: 1/2, step 11058/23838 completed (loss: 0.11596152186393738, acc: 0.9607843160629272)
[2025-02-16 12:49:50,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:50,272][root][INFO] - Training Epoch: 1/2, step 11059/23838 completed (loss: 0.24176917970180511, acc: 0.9366196990013123)
[2025-02-16 12:49:50,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:50,831][root][INFO] - Training Epoch: 1/2, step 11060/23838 completed (loss: 0.12372057884931564, acc: 0.9613259434700012)
[2025-02-16 12:49:51,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:51,293][root][INFO] - Training Epoch: 1/2, step 11061/23838 completed (loss: 0.18945437669754028, acc: 0.9441340565681458)
[2025-02-16 12:49:51,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:51,777][root][INFO] - Training Epoch: 1/2, step 11062/23838 completed (loss: 0.35818591713905334, acc: 0.9059829115867615)
[2025-02-16 12:49:52,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:52,330][root][INFO] - Training Epoch: 1/2, step 11063/23838 completed (loss: 0.1676321178674698, acc: 0.9433962106704712)
[2025-02-16 12:49:52,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:53,043][root][INFO] - Training Epoch: 1/2, step 11064/23838 completed (loss: 0.1666170209646225, acc: 0.9537037014961243)
[2025-02-16 12:49:53,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:53,515][root][INFO] - Training Epoch: 1/2, step 11065/23838 completed (loss: 0.11402349919080734, acc: 0.9599999785423279)
[2025-02-16 12:49:53,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:54,012][root][INFO] - Training Epoch: 1/2, step 11066/23838 completed (loss: 0.2460632026195526, acc: 0.9594594836235046)
[2025-02-16 12:49:54,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:54,429][root][INFO] - Training Epoch: 1/2, step 11067/23838 completed (loss: 0.19202885031700134, acc: 0.954023003578186)
[2025-02-16 12:49:54,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:54,909][root][INFO] - Training Epoch: 1/2, step 11068/23838 completed (loss: 0.31824639439582825, acc: 0.9108280539512634)
[2025-02-16 12:49:55,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:55,350][root][INFO] - Training Epoch: 1/2, step 11069/23838 completed (loss: 0.35404133796691895, acc: 0.9036144614219666)
[2025-02-16 12:49:55,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:55,819][root][INFO] - Training Epoch: 1/2, step 11070/23838 completed (loss: 0.25490227341651917, acc: 0.9340101480484009)
[2025-02-16 12:49:56,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:56,226][root][INFO] - Training Epoch: 1/2, step 11071/23838 completed (loss: 0.203171044588089, acc: 0.9555555582046509)
[2025-02-16 12:49:56,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:56,627][root][INFO] - Training Epoch: 1/2, step 11072/23838 completed (loss: 0.2438201755285263, acc: 0.914893627166748)
[2025-02-16 12:49:56,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:57,096][root][INFO] - Training Epoch: 1/2, step 11073/23838 completed (loss: 0.21328575909137726, acc: 0.9462365508079529)
[2025-02-16 12:49:57,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:57,653][root][INFO] - Training Epoch: 1/2, step 11074/23838 completed (loss: 0.4580238461494446, acc: 0.8676470518112183)
[2025-02-16 12:49:57,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:58,107][root][INFO] - Training Epoch: 1/2, step 11075/23838 completed (loss: 0.38753506541252136, acc: 0.8857142925262451)
[2025-02-16 12:49:58,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:58,610][root][INFO] - Training Epoch: 1/2, step 11076/23838 completed (loss: 0.16225500404834747, acc: 0.9748427867889404)
[2025-02-16 12:49:58,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:59,243][root][INFO] - Training Epoch: 1/2, step 11077/23838 completed (loss: 0.08101218938827515, acc: 0.9832402467727661)
[2025-02-16 12:49:59,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:49:59,879][root][INFO] - Training Epoch: 1/2, step 11078/23838 completed (loss: 0.24410538375377655, acc: 0.9354838728904724)
[2025-02-16 12:50:00,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:00,428][root][INFO] - Training Epoch: 1/2, step 11079/23838 completed (loss: 0.0869264006614685, acc: 0.9892473220825195)
[2025-02-16 12:50:00,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:00,940][root][INFO] - Training Epoch: 1/2, step 11080/23838 completed (loss: 0.44969406723976135, acc: 0.8834356069564819)
[2025-02-16 12:50:01,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:01,966][root][INFO] - Training Epoch: 1/2, step 11081/23838 completed (loss: 0.2694025933742523, acc: 0.9353612065315247)
[2025-02-16 12:50:02,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:02,520][root][INFO] - Training Epoch: 1/2, step 11082/23838 completed (loss: 0.14261703193187714, acc: 0.9496855139732361)
[2025-02-16 12:50:02,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:02,999][root][INFO] - Training Epoch: 1/2, step 11083/23838 completed (loss: 0.28235602378845215, acc: 0.8899082541465759)
[2025-02-16 12:50:03,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:03,459][root][INFO] - Training Epoch: 1/2, step 11084/23838 completed (loss: 0.1511686146259308, acc: 0.9627329111099243)
[2025-02-16 12:50:03,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:03,947][root][INFO] - Training Epoch: 1/2, step 11085/23838 completed (loss: 0.2837609052658081, acc: 0.9090909361839294)
[2025-02-16 12:50:04,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:04,437][root][INFO] - Training Epoch: 1/2, step 11086/23838 completed (loss: 0.3497413694858551, acc: 0.9259259104728699)
[2025-02-16 12:50:04,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:04,852][root][INFO] - Training Epoch: 1/2, step 11087/23838 completed (loss: 0.41495221853256226, acc: 0.895348846912384)
[2025-02-16 12:50:05,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:05,352][root][INFO] - Training Epoch: 1/2, step 11088/23838 completed (loss: 0.127871572971344, acc: 0.9669811129570007)
[2025-02-16 12:50:05,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:05,789][root][INFO] - Training Epoch: 1/2, step 11089/23838 completed (loss: 0.2831359803676605, acc: 0.9197530746459961)
[2025-02-16 12:50:05,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:06,211][root][INFO] - Training Epoch: 1/2, step 11090/23838 completed (loss: 0.46073150634765625, acc: 0.8450704216957092)
[2025-02-16 12:50:06,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:06,721][root][INFO] - Training Epoch: 1/2, step 11091/23838 completed (loss: 0.32217076420783997, acc: 0.9029850959777832)
[2025-02-16 12:50:06,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:07,131][root][INFO] - Training Epoch: 1/2, step 11092/23838 completed (loss: 0.25291022658348083, acc: 0.9444444179534912)
[2025-02-16 12:50:07,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:07,576][root][INFO] - Training Epoch: 1/2, step 11093/23838 completed (loss: 0.42201152443885803, acc: 0.8533333539962769)
[2025-02-16 12:50:07,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:08,178][root][INFO] - Training Epoch: 1/2, step 11094/23838 completed (loss: 0.3736690580844879, acc: 0.9158878326416016)
[2025-02-16 12:50:08,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:08,580][root][INFO] - Training Epoch: 1/2, step 11095/23838 completed (loss: 0.6250397562980652, acc: 0.849056601524353)
[2025-02-16 12:50:08,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:08,996][root][INFO] - Training Epoch: 1/2, step 11096/23838 completed (loss: 0.2314215898513794, acc: 0.9568965435028076)
[2025-02-16 12:50:09,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:09,592][root][INFO] - Training Epoch: 1/2, step 11097/23838 completed (loss: 0.3214866518974304, acc: 0.9071428775787354)
[2025-02-16 12:50:09,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:09,984][root][INFO] - Training Epoch: 1/2, step 11098/23838 completed (loss: 0.33813902735710144, acc: 0.939393937587738)
[2025-02-16 12:50:10,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:10,358][root][INFO] - Training Epoch: 1/2, step 11099/23838 completed (loss: 0.33742597699165344, acc: 0.921875)
[2025-02-16 12:50:10,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:10,806][root][INFO] - Training Epoch: 1/2, step 11100/23838 completed (loss: 0.38928329944610596, acc: 0.8833333253860474)
[2025-02-16 12:50:10,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:11,235][root][INFO] - Training Epoch: 1/2, step 11101/23838 completed (loss: 0.1323963850736618, acc: 0.9494949579238892)
[2025-02-16 12:50:11,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:11,694][root][INFO] - Training Epoch: 1/2, step 11102/23838 completed (loss: 0.38130316138267517, acc: 0.891566276550293)
[2025-02-16 12:50:11,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:12,131][root][INFO] - Training Epoch: 1/2, step 11103/23838 completed (loss: 0.26665058732032776, acc: 0.9135802388191223)
[2025-02-16 12:50:12,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:12,593][root][INFO] - Training Epoch: 1/2, step 11104/23838 completed (loss: 0.27456122636795044, acc: 0.9159663915634155)
[2025-02-16 12:50:12,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:13,065][root][INFO] - Training Epoch: 1/2, step 11105/23838 completed (loss: 0.21041783690452576, acc: 0.9430379867553711)
[2025-02-16 12:50:13,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:13,491][root][INFO] - Training Epoch: 1/2, step 11106/23838 completed (loss: 0.21486824750900269, acc: 0.9245283007621765)
[2025-02-16 12:50:13,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:14,005][root][INFO] - Training Epoch: 1/2, step 11107/23838 completed (loss: 0.22883428633213043, acc: 0.9152542352676392)
[2025-02-16 12:50:14,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:14,445][root][INFO] - Training Epoch: 1/2, step 11108/23838 completed (loss: 0.617813229560852, acc: 0.8666666746139526)
[2025-02-16 12:50:14,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:15,114][root][INFO] - Training Epoch: 1/2, step 11109/23838 completed (loss: 0.28732624650001526, acc: 0.929729700088501)
[2025-02-16 12:50:15,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:15,586][root][INFO] - Training Epoch: 1/2, step 11110/23838 completed (loss: 0.39935430884361267, acc: 0.8983050584793091)
[2025-02-16 12:50:15,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:16,180][root][INFO] - Training Epoch: 1/2, step 11111/23838 completed (loss: 0.5693566799163818, acc: 0.8589743375778198)
[2025-02-16 12:50:16,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:16,623][root][INFO] - Training Epoch: 1/2, step 11112/23838 completed (loss: 0.3283044993877411, acc: 0.9363057613372803)
[2025-02-16 12:50:16,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:17,044][root][INFO] - Training Epoch: 1/2, step 11113/23838 completed (loss: 0.34955206513404846, acc: 0.884353756904602)
[2025-02-16 12:50:17,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:17,572][root][INFO] - Training Epoch: 1/2, step 11114/23838 completed (loss: 0.23765747249126434, acc: 0.9285714030265808)
[2025-02-16 12:50:17,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:18,069][root][INFO] - Training Epoch: 1/2, step 11115/23838 completed (loss: 0.27887675166130066, acc: 0.9333333373069763)
[2025-02-16 12:50:18,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:18,457][root][INFO] - Training Epoch: 1/2, step 11116/23838 completed (loss: 0.6216482520103455, acc: 0.8505747318267822)
[2025-02-16 12:50:18,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:19,120][root][INFO] - Training Epoch: 1/2, step 11117/23838 completed (loss: 0.28582295775413513, acc: 0.9090909361839294)
[2025-02-16 12:50:19,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:19,631][root][INFO] - Training Epoch: 1/2, step 11118/23838 completed (loss: 0.33280062675476074, acc: 0.9090909361839294)
[2025-02-16 12:50:19,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:20,128][root][INFO] - Training Epoch: 1/2, step 11119/23838 completed (loss: 0.3839598000049591, acc: 0.8783783912658691)
[2025-02-16 12:50:20,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:20,579][root][INFO] - Training Epoch: 1/2, step 11120/23838 completed (loss: 0.2719975411891937, acc: 0.9275362491607666)
[2025-02-16 12:50:20,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:21,023][root][INFO] - Training Epoch: 1/2, step 11121/23838 completed (loss: 0.16822847723960876, acc: 0.9316239356994629)
[2025-02-16 12:50:21,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:21,620][root][INFO] - Training Epoch: 1/2, step 11122/23838 completed (loss: 0.20034052431583405, acc: 0.9447236061096191)
[2025-02-16 12:50:21,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:22,149][root][INFO] - Training Epoch: 1/2, step 11123/23838 completed (loss: 0.2291666865348816, acc: 0.9402984976768494)
[2025-02-16 12:50:22,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:22,608][root][INFO] - Training Epoch: 1/2, step 11124/23838 completed (loss: 0.35991185903549194, acc: 0.9238578677177429)
[2025-02-16 12:50:22,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:23,047][root][INFO] - Training Epoch: 1/2, step 11125/23838 completed (loss: 0.2170170396566391, acc: 0.9509803652763367)
[2025-02-16 12:50:23,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:23,523][root][INFO] - Training Epoch: 1/2, step 11126/23838 completed (loss: 0.40514102578163147, acc: 0.9039999842643738)
[2025-02-16 12:50:23,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:24,029][root][INFO] - Training Epoch: 1/2, step 11127/23838 completed (loss: 0.302583783864975, acc: 0.9166666865348816)
[2025-02-16 12:50:24,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:24,920][root][INFO] - Training Epoch: 1/2, step 11128/23838 completed (loss: 0.3673700988292694, acc: 0.9178743958473206)
[2025-02-16 12:50:25,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:25,425][root][INFO] - Training Epoch: 1/2, step 11129/23838 completed (loss: 0.32488134503364563, acc: 0.9147982001304626)
[2025-02-16 12:50:25,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:25,906][root][INFO] - Training Epoch: 1/2, step 11130/23838 completed (loss: 0.22561509907245636, acc: 0.9408283829689026)
[2025-02-16 12:50:26,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:26,465][root][INFO] - Training Epoch: 1/2, step 11131/23838 completed (loss: 0.15802279114723206, acc: 0.9625668525695801)
[2025-02-16 12:50:26,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:26,971][root][INFO] - Training Epoch: 1/2, step 11132/23838 completed (loss: 0.10064055025577545, acc: 0.9750000238418579)
[2025-02-16 12:50:27,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:27,822][root][INFO] - Training Epoch: 1/2, step 11133/23838 completed (loss: 0.27698853611946106, acc: 0.9175257682800293)
[2025-02-16 12:50:27,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:28,226][root][INFO] - Training Epoch: 1/2, step 11134/23838 completed (loss: 0.15844175219535828, acc: 0.9473684430122375)
[2025-02-16 12:50:28,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:28,795][root][INFO] - Training Epoch: 1/2, step 11135/23838 completed (loss: 0.22063307464122772, acc: 0.9282511472702026)
[2025-02-16 12:50:29,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:29,261][root][INFO] - Training Epoch: 1/2, step 11136/23838 completed (loss: 0.4060811996459961, acc: 0.9200000166893005)
[2025-02-16 12:50:29,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:30,355][root][INFO] - Training Epoch: 1/2, step 11137/23838 completed (loss: 0.20065928995609283, acc: 0.938095211982727)
[2025-02-16 12:50:30,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:30,850][root][INFO] - Training Epoch: 1/2, step 11138/23838 completed (loss: 0.31220191717147827, acc: 0.9042553305625916)
[2025-02-16 12:50:31,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:31,352][root][INFO] - Training Epoch: 1/2, step 11139/23838 completed (loss: 0.39860856533050537, acc: 0.8925233483314514)
[2025-02-16 12:50:31,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:32,385][root][INFO] - Training Epoch: 1/2, step 11140/23838 completed (loss: 0.48299017548561096, acc: 0.8711340427398682)
[2025-02-16 12:50:32,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:32,848][root][INFO] - Training Epoch: 1/2, step 11141/23838 completed (loss: 0.8840752840042114, acc: 0.7894737124443054)
[2025-02-16 12:50:33,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:33,317][root][INFO] - Training Epoch: 1/2, step 11142/23838 completed (loss: 0.33536213636398315, acc: 0.8977272510528564)
[2025-02-16 12:50:33,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:33,704][root][INFO] - Training Epoch: 1/2, step 11143/23838 completed (loss: 0.6637463569641113, acc: 0.8307692408561707)
[2025-02-16 12:50:33,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:34,093][root][INFO] - Training Epoch: 1/2, step 11144/23838 completed (loss: 0.49045518040657043, acc: 0.8640776872634888)
[2025-02-16 12:50:34,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:34,551][root][INFO] - Training Epoch: 1/2, step 11145/23838 completed (loss: 0.26939183473587036, acc: 0.939393937587738)
[2025-02-16 12:50:34,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:35,016][root][INFO] - Training Epoch: 1/2, step 11146/23838 completed (loss: 0.43583250045776367, acc: 0.9005848169326782)
[2025-02-16 12:50:35,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:35,463][root][INFO] - Training Epoch: 1/2, step 11147/23838 completed (loss: 0.17131146788597107, acc: 0.9557521939277649)
[2025-02-16 12:50:35,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:36,326][root][INFO] - Training Epoch: 1/2, step 11148/23838 completed (loss: 0.2530890703201294, acc: 0.9209039807319641)
[2025-02-16 12:50:36,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:36,754][root][INFO] - Training Epoch: 1/2, step 11149/23838 completed (loss: 0.5181818604469299, acc: 0.8275862336158752)
[2025-02-16 12:50:36,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:37,169][root][INFO] - Training Epoch: 1/2, step 11150/23838 completed (loss: 0.9991700053215027, acc: 0.8113207817077637)
[2025-02-16 12:50:37,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:37,700][root][INFO] - Training Epoch: 1/2, step 11151/23838 completed (loss: 0.5626978278160095, acc: 0.8240000009536743)
[2025-02-16 12:50:38,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:38,309][root][INFO] - Training Epoch: 1/2, step 11152/23838 completed (loss: 1.1662319898605347, acc: 0.7475728392601013)
[2025-02-16 12:50:38,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:38,736][root][INFO] - Training Epoch: 1/2, step 11153/23838 completed (loss: 0.6439211368560791, acc: 0.8380952477455139)
[2025-02-16 12:50:38,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:39,183][root][INFO] - Training Epoch: 1/2, step 11154/23838 completed (loss: 0.7169473767280579, acc: 0.8153846263885498)
[2025-02-16 12:50:39,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:39,689][root][INFO] - Training Epoch: 1/2, step 11155/23838 completed (loss: 0.3310055136680603, acc: 0.9411764740943909)
[2025-02-16 12:50:39,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:40,141][root][INFO] - Training Epoch: 1/2, step 11156/23838 completed (loss: 0.33607739210128784, acc: 0.8461538553237915)
[2025-02-16 12:50:40,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:40,604][root][INFO] - Training Epoch: 1/2, step 11157/23838 completed (loss: 0.5240151286125183, acc: 0.875)
[2025-02-16 12:50:40,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:41,013][root][INFO] - Training Epoch: 1/2, step 11158/23838 completed (loss: 0.4010794758796692, acc: 0.8666666746139526)
[2025-02-16 12:50:41,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:41,480][root][INFO] - Training Epoch: 1/2, step 11159/23838 completed (loss: 0.32567471265792847, acc: 0.9065420627593994)
[2025-02-16 12:50:41,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:41,979][root][INFO] - Training Epoch: 1/2, step 11160/23838 completed (loss: 0.6929513812065125, acc: 0.805084764957428)
[2025-02-16 12:50:42,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:42,525][root][INFO] - Training Epoch: 1/2, step 11161/23838 completed (loss: 0.33263230323791504, acc: 0.9057971239089966)
[2025-02-16 12:50:42,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:43,259][root][INFO] - Training Epoch: 1/2, step 11162/23838 completed (loss: 0.5602476596832275, acc: 0.8362573385238647)
[2025-02-16 12:50:43,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:43,688][root][INFO] - Training Epoch: 1/2, step 11163/23838 completed (loss: 0.4350220859050751, acc: 0.8984375)
[2025-02-16 12:50:43,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:44,170][root][INFO] - Training Epoch: 1/2, step 11164/23838 completed (loss: 0.29451993107795715, acc: 0.9220778942108154)
[2025-02-16 12:50:44,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:44,561][root][INFO] - Training Epoch: 1/2, step 11165/23838 completed (loss: 0.7480121850967407, acc: 0.7962962985038757)
[2025-02-16 12:50:44,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:44,948][root][INFO] - Training Epoch: 1/2, step 11166/23838 completed (loss: 0.7036253809928894, acc: 0.8309859037399292)
[2025-02-16 12:50:45,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:45,402][root][INFO] - Training Epoch: 1/2, step 11167/23838 completed (loss: 0.6742163300514221, acc: 0.8529411554336548)
[2025-02-16 12:50:45,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:45,840][root][INFO] - Training Epoch: 1/2, step 11168/23838 completed (loss: 0.2746223509311676, acc: 0.9333333373069763)
[2025-02-16 12:50:46,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:46,300][root][INFO] - Training Epoch: 1/2, step 11169/23838 completed (loss: 0.4449968636035919, acc: 0.8482142686843872)
[2025-02-16 12:50:46,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:46,690][root][INFO] - Training Epoch: 1/2, step 11170/23838 completed (loss: 0.42818862199783325, acc: 0.8602150678634644)
[2025-02-16 12:50:46,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:47,121][root][INFO] - Training Epoch: 1/2, step 11171/23838 completed (loss: 0.39253121614456177, acc: 0.9210526347160339)
[2025-02-16 12:50:47,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:47,557][root][INFO] - Training Epoch: 1/2, step 11172/23838 completed (loss: 0.4971436560153961, acc: 0.8548387289047241)
[2025-02-16 12:50:47,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:47,995][root][INFO] - Training Epoch: 1/2, step 11173/23838 completed (loss: 0.7834696769714355, acc: 0.7818182110786438)
[2025-02-16 12:50:48,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:48,432][root][INFO] - Training Epoch: 1/2, step 11174/23838 completed (loss: 0.26284608244895935, acc: 0.9047619104385376)
[2025-02-16 12:50:48,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:48,883][root][INFO] - Training Epoch: 1/2, step 11175/23838 completed (loss: 0.7028892040252686, acc: 0.7799999713897705)
[2025-02-16 12:50:49,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:49,336][root][INFO] - Training Epoch: 1/2, step 11176/23838 completed (loss: 0.3422204256057739, acc: 0.9125000238418579)
[2025-02-16 12:50:49,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:49,745][root][INFO] - Training Epoch: 1/2, step 11177/23838 completed (loss: 0.49572449922561646, acc: 0.8405796885490417)
[2025-02-16 12:50:49,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:50,188][root][INFO] - Training Epoch: 1/2, step 11178/23838 completed (loss: 0.5335981249809265, acc: 0.8674699068069458)
[2025-02-16 12:50:50,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:50,678][root][INFO] - Training Epoch: 1/2, step 11179/23838 completed (loss: 0.40016379952430725, acc: 0.9059829115867615)
[2025-02-16 12:50:51,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:51,425][root][INFO] - Training Epoch: 1/2, step 11180/23838 completed (loss: 0.30465176701545715, acc: 0.9047619104385376)
[2025-02-16 12:50:51,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:51,904][root][INFO] - Training Epoch: 1/2, step 11181/23838 completed (loss: 0.5955593585968018, acc: 0.8479999899864197)
[2025-02-16 12:50:52,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:52,359][root][INFO] - Training Epoch: 1/2, step 11182/23838 completed (loss: 0.42242398858070374, acc: 0.8811880946159363)
[2025-02-16 12:50:52,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:52,832][root][INFO] - Training Epoch: 1/2, step 11183/23838 completed (loss: 0.4612026512622833, acc: 0.8640000224113464)
[2025-02-16 12:50:53,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:53,295][root][INFO] - Training Epoch: 1/2, step 11184/23838 completed (loss: 0.3425964415073395, acc: 0.8969072103500366)
[2025-02-16 12:50:53,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:53,718][root][INFO] - Training Epoch: 1/2, step 11185/23838 completed (loss: 0.8635242581367493, acc: 0.7843137383460999)
[2025-02-16 12:50:53,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:54,165][root][INFO] - Training Epoch: 1/2, step 11186/23838 completed (loss: 0.5977652072906494, acc: 0.7941176295280457)
[2025-02-16 12:50:54,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:54,537][root][INFO] - Training Epoch: 1/2, step 11187/23838 completed (loss: 0.6736671328544617, acc: 0.800000011920929)
[2025-02-16 12:50:54,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:55,044][root][INFO] - Training Epoch: 1/2, step 11188/23838 completed (loss: 0.5624248385429382, acc: 0.8488371968269348)
[2025-02-16 12:50:55,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:55,438][root][INFO] - Training Epoch: 1/2, step 11189/23838 completed (loss: 0.4318011701107025, acc: 0.8691588640213013)
[2025-02-16 12:50:55,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:55,829][root][INFO] - Training Epoch: 1/2, step 11190/23838 completed (loss: 0.4657699763774872, acc: 0.8518518805503845)
[2025-02-16 12:50:56,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:56,295][root][INFO] - Training Epoch: 1/2, step 11191/23838 completed (loss: 0.3918067216873169, acc: 0.8987341523170471)
[2025-02-16 12:50:56,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:56,850][root][INFO] - Training Epoch: 1/2, step 11192/23838 completed (loss: 0.3890388309955597, acc: 0.9078947305679321)
[2025-02-16 12:50:56,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:57,221][root][INFO] - Training Epoch: 1/2, step 11193/23838 completed (loss: 0.378928542137146, acc: 0.8767123222351074)
[2025-02-16 12:50:57,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:57,697][root][INFO] - Training Epoch: 1/2, step 11194/23838 completed (loss: 0.5406004190444946, acc: 0.8543689250946045)
[2025-02-16 12:50:57,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:58,162][root][INFO] - Training Epoch: 1/2, step 11195/23838 completed (loss: 0.1901054084300995, acc: 0.9473684430122375)
[2025-02-16 12:50:58,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:58,551][root][INFO] - Training Epoch: 1/2, step 11196/23838 completed (loss: 0.9308138489723206, acc: 0.7538461685180664)
[2025-02-16 12:50:58,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:58,977][root][INFO] - Training Epoch: 1/2, step 11197/23838 completed (loss: 0.5011225938796997, acc: 0.858208954334259)
[2025-02-16 12:50:59,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:59,424][root][INFO] - Training Epoch: 1/2, step 11198/23838 completed (loss: 0.7111142873764038, acc: 0.7971014380455017)
[2025-02-16 12:50:59,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:50:59,943][root][INFO] - Training Epoch: 1/2, step 11199/23838 completed (loss: 0.33901405334472656, acc: 0.918367326259613)
[2025-02-16 12:51:00,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:00,434][root][INFO] - Training Epoch: 1/2, step 11200/23838 completed (loss: 0.4326755404472351, acc: 0.8671875)
[2025-02-16 12:51:00,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:00,868][root][INFO] - Training Epoch: 1/2, step 11201/23838 completed (loss: 0.3793279528617859, acc: 0.8804348111152649)
[2025-02-16 12:51:01,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:01,368][root][INFO] - Training Epoch: 1/2, step 11202/23838 completed (loss: 0.3436765670776367, acc: 0.9416058659553528)
[2025-02-16 12:51:01,638][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:01,945][root][INFO] - Training Epoch: 1/2, step 11203/23838 completed (loss: 0.3711816072463989, acc: 0.9139072895050049)
[2025-02-16 12:51:02,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:02,393][root][INFO] - Training Epoch: 1/2, step 11204/23838 completed (loss: 0.38972729444503784, acc: 0.9107142686843872)
[2025-02-16 12:51:02,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:02,883][root][INFO] - Training Epoch: 1/2, step 11205/23838 completed (loss: 0.4077446758747101, acc: 0.8902438879013062)
[2025-02-16 12:51:03,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:03,344][root][INFO] - Training Epoch: 1/2, step 11206/23838 completed (loss: 0.5378327965736389, acc: 0.8469387888908386)
[2025-02-16 12:51:03,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:03,819][root][INFO] - Training Epoch: 1/2, step 11207/23838 completed (loss: 0.2589515745639801, acc: 0.9194630980491638)
[2025-02-16 12:51:04,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:04,854][root][INFO] - Training Epoch: 1/2, step 11208/23838 completed (loss: 0.3135796785354614, acc: 0.9235293865203857)
[2025-02-16 12:51:05,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:05,313][root][INFO] - Training Epoch: 1/2, step 11209/23838 completed (loss: 0.47417059540748596, acc: 0.8924731016159058)
[2025-02-16 12:51:05,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:06,052][root][INFO] - Training Epoch: 1/2, step 11210/23838 completed (loss: 0.3632550835609436, acc: 0.8947368264198303)
[2025-02-16 12:51:06,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:06,448][root][INFO] - Training Epoch: 1/2, step 11211/23838 completed (loss: 0.6227266788482666, acc: 0.8421052694320679)
[2025-02-16 12:51:06,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:06,814][root][INFO] - Training Epoch: 1/2, step 11212/23838 completed (loss: 0.6268453598022461, acc: 0.8363636136054993)
[2025-02-16 12:51:06,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:07,217][root][INFO] - Training Epoch: 1/2, step 11213/23838 completed (loss: 0.4505918323993683, acc: 0.8777777552604675)
[2025-02-16 12:51:07,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:07,633][root][INFO] - Training Epoch: 1/2, step 11214/23838 completed (loss: 1.0868394374847412, acc: 0.800000011920929)
[2025-02-16 12:51:07,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:08,044][root][INFO] - Training Epoch: 1/2, step 11215/23838 completed (loss: 0.4217112064361572, acc: 0.9375)
[2025-02-16 12:51:08,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:08,488][root][INFO] - Training Epoch: 1/2, step 11216/23838 completed (loss: 0.08095691353082657, acc: 1.0)
[2025-02-16 12:51:08,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:08,879][root][INFO] - Training Epoch: 1/2, step 11217/23838 completed (loss: 0.8084633946418762, acc: 0.7666666507720947)
[2025-02-16 12:51:09,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:09,262][root][INFO] - Training Epoch: 1/2, step 11218/23838 completed (loss: 0.39213433861732483, acc: 0.8999999761581421)
[2025-02-16 12:51:09,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:09,762][root][INFO] - Training Epoch: 1/2, step 11219/23838 completed (loss: 0.25469547510147095, acc: 0.9224137663841248)
[2025-02-16 12:51:10,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:10,266][root][INFO] - Training Epoch: 1/2, step 11220/23838 completed (loss: 0.5644460320472717, acc: 0.8118811845779419)
[2025-02-16 12:51:10,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:10,669][root][INFO] - Training Epoch: 1/2, step 11221/23838 completed (loss: 0.5153336524963379, acc: 0.8809523582458496)
[2025-02-16 12:51:10,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:11,075][root][INFO] - Training Epoch: 1/2, step 11222/23838 completed (loss: 0.46798333525657654, acc: 0.8536585569381714)
[2025-02-16 12:51:11,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:11,490][root][INFO] - Training Epoch: 1/2, step 11223/23838 completed (loss: 0.21618030965328217, acc: 0.9468085169792175)
[2025-02-16 12:51:11,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:11,939][root][INFO] - Training Epoch: 1/2, step 11224/23838 completed (loss: 0.47036004066467285, acc: 0.8701298832893372)
[2025-02-16 12:51:12,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:12,397][root][INFO] - Training Epoch: 1/2, step 11225/23838 completed (loss: 0.5134615302085876, acc: 0.8181818127632141)
[2025-02-16 12:51:12,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:13,065][root][INFO] - Training Epoch: 1/2, step 11226/23838 completed (loss: 0.5912449359893799, acc: 0.8583333492279053)
[2025-02-16 12:51:13,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:13,551][root][INFO] - Training Epoch: 1/2, step 11227/23838 completed (loss: 0.5342341065406799, acc: 0.8409090638160706)
[2025-02-16 12:51:13,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:14,041][root][INFO] - Training Epoch: 1/2, step 11228/23838 completed (loss: 0.220347598195076, acc: 0.9300699234008789)
[2025-02-16 12:51:14,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:14,902][root][INFO] - Training Epoch: 1/2, step 11229/23838 completed (loss: 0.5530768632888794, acc: 0.8584905862808228)
[2025-02-16 12:51:15,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:15,351][root][INFO] - Training Epoch: 1/2, step 11230/23838 completed (loss: 0.552240252494812, acc: 0.8507462739944458)
[2025-02-16 12:51:15,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:16,055][root][INFO] - Training Epoch: 1/2, step 11231/23838 completed (loss: 0.45530328154563904, acc: 0.8815789222717285)
[2025-02-16 12:51:16,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:16,441][root][INFO] - Training Epoch: 1/2, step 11232/23838 completed (loss: 0.5363986492156982, acc: 0.8611111044883728)
[2025-02-16 12:51:17,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:17,590][root][INFO] - Training Epoch: 1/2, step 11233/23838 completed (loss: 0.4189431965351105, acc: 0.8714285492897034)
[2025-02-16 12:51:17,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:18,287][root][INFO] - Training Epoch: 1/2, step 11234/23838 completed (loss: 0.7232758402824402, acc: 0.8026315569877625)
[2025-02-16 12:51:18,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:18,730][root][INFO] - Training Epoch: 1/2, step 11235/23838 completed (loss: 0.07570360600948334, acc: 0.9814814925193787)
[2025-02-16 12:51:18,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:19,241][root][INFO] - Training Epoch: 1/2, step 11236/23838 completed (loss: 0.6263915300369263, acc: 0.8333333134651184)
[2025-02-16 12:51:19,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:19,871][root][INFO] - Training Epoch: 1/2, step 11237/23838 completed (loss: 0.6140512824058533, acc: 0.8066666722297668)
[2025-02-16 12:51:20,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:20,392][root][INFO] - Training Epoch: 1/2, step 11238/23838 completed (loss: 0.4126186966896057, acc: 0.8636363744735718)
[2025-02-16 12:51:20,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:20,849][root][INFO] - Training Epoch: 1/2, step 11239/23838 completed (loss: 0.21766746044158936, acc: 0.9090909361839294)
[2025-02-16 12:51:21,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:21,254][root][INFO] - Training Epoch: 1/2, step 11240/23838 completed (loss: 0.19167958199977875, acc: 0.9347826242446899)
[2025-02-16 12:51:21,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:21,688][root][INFO] - Training Epoch: 1/2, step 11241/23838 completed (loss: 0.29101330041885376, acc: 0.9318181872367859)
[2025-02-16 12:51:21,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:22,218][root][INFO] - Training Epoch: 1/2, step 11242/23838 completed (loss: 0.18488053977489471, acc: 0.9677419066429138)
[2025-02-16 12:51:22,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:22,634][root][INFO] - Training Epoch: 1/2, step 11243/23838 completed (loss: 0.25786277651786804, acc: 0.8999999761581421)
[2025-02-16 12:51:23,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:23,553][root][INFO] - Training Epoch: 1/2, step 11244/23838 completed (loss: 0.21686464548110962, acc: 0.9504132270812988)
[2025-02-16 12:51:23,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:23,949][root][INFO] - Training Epoch: 1/2, step 11245/23838 completed (loss: 0.40394505858421326, acc: 0.8876404762268066)
[2025-02-16 12:51:24,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:24,376][root][INFO] - Training Epoch: 1/2, step 11246/23838 completed (loss: 0.1734820306301117, acc: 0.9347826242446899)
[2025-02-16 12:51:24,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:24,986][root][INFO] - Training Epoch: 1/2, step 11247/23838 completed (loss: 0.23099224269390106, acc: 0.9441340565681458)
[2025-02-16 12:51:25,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:25,491][root][INFO] - Training Epoch: 1/2, step 11248/23838 completed (loss: 0.19416822493076324, acc: 0.9444444179534912)
[2025-02-16 12:51:25,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:25,954][root][INFO] - Training Epoch: 1/2, step 11249/23838 completed (loss: 0.5259061455726624, acc: 0.8805969953536987)
[2025-02-16 12:51:26,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:26,427][root][INFO] - Training Epoch: 1/2, step 11250/23838 completed (loss: 0.1991523653268814, acc: 0.9328358173370361)
[2025-02-16 12:51:26,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:27,127][root][INFO] - Training Epoch: 1/2, step 11251/23838 completed (loss: 0.17608769237995148, acc: 0.9548022747039795)
[2025-02-16 12:51:27,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:27,624][root][INFO] - Training Epoch: 1/2, step 11252/23838 completed (loss: 0.21349242329597473, acc: 0.9289617538452148)
[2025-02-16 12:51:28,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:28,401][root][INFO] - Training Epoch: 1/2, step 11253/23838 completed (loss: 0.42270103096961975, acc: 0.879807710647583)
[2025-02-16 12:51:28,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:29,006][root][INFO] - Training Epoch: 1/2, step 11254/23838 completed (loss: 0.2992146909236908, acc: 0.9095744490623474)
[2025-02-16 12:51:29,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:29,442][root][INFO] - Training Epoch: 1/2, step 11255/23838 completed (loss: 0.11881314963102341, acc: 0.9743589758872986)
[2025-02-16 12:51:29,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:30,090][root][INFO] - Training Epoch: 1/2, step 11256/23838 completed (loss: 0.3215908408164978, acc: 0.9128205180168152)
[2025-02-16 12:51:30,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:31,118][root][INFO] - Training Epoch: 1/2, step 11257/23838 completed (loss: 0.6025234460830688, acc: 0.8397790193557739)
[2025-02-16 12:51:31,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:31,640][root][INFO] - Training Epoch: 1/2, step 11258/23838 completed (loss: 0.10895813256502151, acc: 0.9561403393745422)
[2025-02-16 12:51:31,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:32,117][root][INFO] - Training Epoch: 1/2, step 11259/23838 completed (loss: 0.26444873213768005, acc: 0.9285714030265808)
[2025-02-16 12:51:32,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:32,557][root][INFO] - Training Epoch: 1/2, step 11260/23838 completed (loss: 0.1832062005996704, acc: 0.9484536051750183)
[2025-02-16 12:51:32,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:32,958][root][INFO] - Training Epoch: 1/2, step 11261/23838 completed (loss: 0.3387438654899597, acc: 0.9158878326416016)
[2025-02-16 12:51:33,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:33,396][root][INFO] - Training Epoch: 1/2, step 11262/23838 completed (loss: 0.19400650262832642, acc: 0.9595959782600403)
[2025-02-16 12:51:33,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:33,797][root][INFO] - Training Epoch: 1/2, step 11263/23838 completed (loss: 0.23991402983665466, acc: 0.9492753744125366)
[2025-02-16 12:51:34,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:34,353][root][INFO] - Training Epoch: 1/2, step 11264/23838 completed (loss: 0.15578730404376984, acc: 0.9637681245803833)
[2025-02-16 12:51:34,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:34,811][root][INFO] - Training Epoch: 1/2, step 11265/23838 completed (loss: 0.20252537727355957, acc: 0.9320388436317444)
[2025-02-16 12:51:35,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:35,387][root][INFO] - Training Epoch: 1/2, step 11266/23838 completed (loss: 0.17825254797935486, acc: 0.9539473652839661)
[2025-02-16 12:51:35,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:36,034][root][INFO] - Training Epoch: 1/2, step 11267/23838 completed (loss: 0.14896492660045624, acc: 0.9603174328804016)
[2025-02-16 12:51:36,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:36,927][root][INFO] - Training Epoch: 1/2, step 11268/23838 completed (loss: 0.17169761657714844, acc: 0.9603960514068604)
[2025-02-16 12:51:37,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:37,379][root][INFO] - Training Epoch: 1/2, step 11269/23838 completed (loss: 0.07434705644845963, acc: 0.9864864945411682)
[2025-02-16 12:51:37,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:37,870][root][INFO] - Training Epoch: 1/2, step 11270/23838 completed (loss: 0.16566607356071472, acc: 0.9795918464660645)
[2025-02-16 12:51:38,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:38,336][root][INFO] - Training Epoch: 1/2, step 11271/23838 completed (loss: 0.13715392351150513, acc: 0.9626168012619019)
[2025-02-16 12:51:38,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:38,835][root][INFO] - Training Epoch: 1/2, step 11272/23838 completed (loss: 0.23624444007873535, acc: 0.9389312863349915)
[2025-02-16 12:51:39,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:39,926][root][INFO] - Training Epoch: 1/2, step 11273/23838 completed (loss: 0.4933323562145233, acc: 0.8611111044883728)
[2025-02-16 12:51:40,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:40,327][root][INFO] - Training Epoch: 1/2, step 11274/23838 completed (loss: 0.20928753912448883, acc: 0.9345794320106506)
[2025-02-16 12:51:40,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:40,725][root][INFO] - Training Epoch: 1/2, step 11275/23838 completed (loss: 0.1081482395529747, acc: 0.9814814925193787)
[2025-02-16 12:51:40,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:41,184][root][INFO] - Training Epoch: 1/2, step 11276/23838 completed (loss: 0.193404421210289, acc: 0.9449541568756104)
[2025-02-16 12:51:41,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:41,636][root][INFO] - Training Epoch: 1/2, step 11277/23838 completed (loss: 0.21373826265335083, acc: 0.9523809552192688)
[2025-02-16 12:51:41,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:42,156][root][INFO] - Training Epoch: 1/2, step 11278/23838 completed (loss: 1.0802383422851562, acc: 0.7575757503509521)
[2025-02-16 12:51:42,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:42,970][root][INFO] - Training Epoch: 1/2, step 11279/23838 completed (loss: 0.5261614918708801, acc: 0.8831169009208679)
[2025-02-16 12:51:43,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:43,408][root][INFO] - Training Epoch: 1/2, step 11280/23838 completed (loss: 0.1456916183233261, acc: 0.9615384340286255)
[2025-02-16 12:51:43,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:43,863][root][INFO] - Training Epoch: 1/2, step 11281/23838 completed (loss: 0.6662106513977051, acc: 0.8620689511299133)
[2025-02-16 12:51:44,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:44,353][root][INFO] - Training Epoch: 1/2, step 11282/23838 completed (loss: 0.9278687238693237, acc: 0.7642276287078857)
[2025-02-16 12:51:44,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:44,857][root][INFO] - Training Epoch: 1/2, step 11283/23838 completed (loss: 0.12552760541439056, acc: 0.95652174949646)
[2025-02-16 12:51:45,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:45,291][root][INFO] - Training Epoch: 1/2, step 11284/23838 completed (loss: 0.3367418646812439, acc: 0.9019607901573181)
[2025-02-16 12:51:45,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:45,669][root][INFO] - Training Epoch: 1/2, step 11285/23838 completed (loss: 0.03720056638121605, acc: 0.9852941036224365)
[2025-02-16 12:51:45,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:46,121][root][INFO] - Training Epoch: 1/2, step 11286/23838 completed (loss: 0.20808395743370056, acc: 0.9396551847457886)
[2025-02-16 12:51:46,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:46,658][root][INFO] - Training Epoch: 1/2, step 11287/23838 completed (loss: 0.3147159814834595, acc: 0.9124423861503601)
[2025-02-16 12:51:46,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:47,080][root][INFO] - Training Epoch: 1/2, step 11288/23838 completed (loss: 0.03966985270380974, acc: 0.9898989796638489)
[2025-02-16 12:51:47,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:47,548][root][INFO] - Training Epoch: 1/2, step 11289/23838 completed (loss: 0.31490492820739746, acc: 0.9214285612106323)
[2025-02-16 12:51:47,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:48,308][root][INFO] - Training Epoch: 1/2, step 11290/23838 completed (loss: 0.15198691189289093, acc: 0.956250011920929)
[2025-02-16 12:51:48,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:48,736][root][INFO] - Training Epoch: 1/2, step 11291/23838 completed (loss: 0.08875064551830292, acc: 0.9673202633857727)
[2025-02-16 12:51:48,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:49,204][root][INFO] - Training Epoch: 1/2, step 11292/23838 completed (loss: 0.2555893361568451, acc: 0.9444444179534912)
[2025-02-16 12:51:49,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:49,700][root][INFO] - Training Epoch: 1/2, step 11293/23838 completed (loss: 0.7359769344329834, acc: 0.7755101919174194)
[2025-02-16 12:51:50,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:50,687][root][INFO] - Training Epoch: 1/2, step 11294/23838 completed (loss: 0.3553934097290039, acc: 0.9037036895751953)
[2025-02-16 12:51:50,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:51,179][root][INFO] - Training Epoch: 1/2, step 11295/23838 completed (loss: 0.19990937411785126, acc: 0.9583333134651184)
[2025-02-16 12:51:51,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:51,585][root][INFO] - Training Epoch: 1/2, step 11296/23838 completed (loss: 0.42542561888694763, acc: 0.8965517282485962)
[2025-02-16 12:51:51,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:51,985][root][INFO] - Training Epoch: 1/2, step 11297/23838 completed (loss: 0.11181014776229858, acc: 0.9743589758872986)
[2025-02-16 12:51:52,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:52,380][root][INFO] - Training Epoch: 1/2, step 11298/23838 completed (loss: 0.22662580013275146, acc: 0.9491525292396545)
[2025-02-16 12:51:52,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:53,317][root][INFO] - Training Epoch: 1/2, step 11299/23838 completed (loss: 0.279986709356308, acc: 0.9391891956329346)
[2025-02-16 12:51:53,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:54,302][root][INFO] - Training Epoch: 1/2, step 11300/23838 completed (loss: 0.20354746282100677, acc: 0.929411768913269)
[2025-02-16 12:51:54,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:54,771][root][INFO] - Training Epoch: 1/2, step 11301/23838 completed (loss: 0.07716083526611328, acc: 0.9831932783126831)
[2025-02-16 12:51:55,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:55,300][root][INFO] - Training Epoch: 1/2, step 11302/23838 completed (loss: 0.2373935580253601, acc: 0.9507389068603516)
[2025-02-16 12:51:55,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:56,321][root][INFO] - Training Epoch: 1/2, step 11303/23838 completed (loss: 0.2209019958972931, acc: 0.9509803652763367)
[2025-02-16 12:51:56,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:56,819][root][INFO] - Training Epoch: 1/2, step 11304/23838 completed (loss: 0.40727177262306213, acc: 0.8691588640213013)
[2025-02-16 12:51:57,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:57,779][root][INFO] - Training Epoch: 1/2, step 11305/23838 completed (loss: 0.09815346449613571, acc: 0.976190447807312)
[2025-02-16 12:51:58,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:58,297][root][INFO] - Training Epoch: 1/2, step 11306/23838 completed (loss: 0.1874203383922577, acc: 0.9516128897666931)
[2025-02-16 12:51:58,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:59,026][root][INFO] - Training Epoch: 1/2, step 11307/23838 completed (loss: 0.12651649117469788, acc: 0.9776536226272583)
[2025-02-16 12:51:59,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:51:59,663][root][INFO] - Training Epoch: 1/2, step 11308/23838 completed (loss: 0.07444154471158981, acc: 0.977011501789093)
[2025-02-16 12:51:59,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:00,121][root][INFO] - Training Epoch: 1/2, step 11309/23838 completed (loss: 0.08875647187232971, acc: 0.9759036302566528)
[2025-02-16 12:52:00,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:00,571][root][INFO] - Training Epoch: 1/2, step 11310/23838 completed (loss: 0.4389207363128662, acc: 0.8823529481887817)
[2025-02-16 12:52:00,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:01,061][root][INFO] - Training Epoch: 1/2, step 11311/23838 completed (loss: 0.12097132951021194, acc: 0.9720279574394226)
[2025-02-16 12:52:01,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:01,565][root][INFO] - Training Epoch: 1/2, step 11312/23838 completed (loss: 0.3970580995082855, acc: 0.8879310488700867)
[2025-02-16 12:52:02,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:02,621][root][INFO] - Training Epoch: 1/2, step 11313/23838 completed (loss: 0.20319975912570953, acc: 0.9469026327133179)
[2025-02-16 12:52:02,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:02,995][root][INFO] - Training Epoch: 1/2, step 11314/23838 completed (loss: 0.24796094000339508, acc: 0.8990825414657593)
[2025-02-16 12:52:03,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:03,671][root][INFO] - Training Epoch: 1/2, step 11315/23838 completed (loss: 0.2301391363143921, acc: 0.9411764740943909)
[2025-02-16 12:52:03,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:04,261][root][INFO] - Training Epoch: 1/2, step 11316/23838 completed (loss: 0.13667495548725128, acc: 0.9516128897666931)
[2025-02-16 12:52:04,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:04,726][root][INFO] - Training Epoch: 1/2, step 11317/23838 completed (loss: 0.21272751688957214, acc: 0.9463087320327759)
[2025-02-16 12:52:05,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:05,446][root][INFO] - Training Epoch: 1/2, step 11318/23838 completed (loss: 0.14134284853935242, acc: 0.956250011920929)
[2025-02-16 12:52:05,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:06,346][root][INFO] - Training Epoch: 1/2, step 11319/23838 completed (loss: 0.22733084857463837, acc: 0.9239130616188049)
[2025-02-16 12:52:06,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:06,835][root][INFO] - Training Epoch: 1/2, step 11320/23838 completed (loss: 0.11630500108003616, acc: 0.9583333134651184)
[2025-02-16 12:52:07,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:07,728][root][INFO] - Training Epoch: 1/2, step 11321/23838 completed (loss: 0.5561566948890686, acc: 0.8372092843055725)
[2025-02-16 12:52:08,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:08,338][root][INFO] - Training Epoch: 1/2, step 11322/23838 completed (loss: 0.13877925276756287, acc: 0.9734513163566589)
[2025-02-16 12:52:08,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:08,750][root][INFO] - Training Epoch: 1/2, step 11323/23838 completed (loss: 0.37775158882141113, acc: 0.8840579986572266)
[2025-02-16 12:52:09,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:09,431][root][INFO] - Training Epoch: 1/2, step 11324/23838 completed (loss: 0.5520634651184082, acc: 0.8392857313156128)
[2025-02-16 12:52:09,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:10,134][root][INFO] - Training Epoch: 1/2, step 11325/23838 completed (loss: 0.5138976573944092, acc: 0.8700000047683716)
[2025-02-16 12:52:10,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:10,646][root][INFO] - Training Epoch: 1/2, step 11326/23838 completed (loss: 0.26272088289260864, acc: 0.9279279112815857)
[2025-02-16 12:52:10,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:11,154][root][INFO] - Training Epoch: 1/2, step 11327/23838 completed (loss: 0.4736863970756531, acc: 0.8793103694915771)
[2025-02-16 12:52:11,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:11,594][root][INFO] - Training Epoch: 1/2, step 11328/23838 completed (loss: 0.2930121123790741, acc: 0.9202898740768433)
[2025-02-16 12:52:11,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:12,007][root][INFO] - Training Epoch: 1/2, step 11329/23838 completed (loss: 0.9997714757919312, acc: 0.7714285850524902)
[2025-02-16 12:52:12,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:12,426][root][INFO] - Training Epoch: 1/2, step 11330/23838 completed (loss: 0.5564050674438477, acc: 0.8604651093482971)
[2025-02-16 12:52:12,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:12,830][root][INFO] - Training Epoch: 1/2, step 11331/23838 completed (loss: 0.4160791039466858, acc: 0.8875739574432373)
[2025-02-16 12:52:13,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:13,265][root][INFO] - Training Epoch: 1/2, step 11332/23838 completed (loss: 0.17047461867332458, acc: 0.9481481313705444)
[2025-02-16 12:52:13,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:13,736][root][INFO] - Training Epoch: 1/2, step 11333/23838 completed (loss: 0.2973273694515228, acc: 0.8972602486610413)
[2025-02-16 12:52:13,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:14,191][root][INFO] - Training Epoch: 1/2, step 11334/23838 completed (loss: 0.509512722492218, acc: 0.8691588640213013)
[2025-02-16 12:52:14,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:14,604][root][INFO] - Training Epoch: 1/2, step 11335/23838 completed (loss: 0.27946004271507263, acc: 0.9166666865348816)
[2025-02-16 12:52:14,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:15,022][root][INFO] - Training Epoch: 1/2, step 11336/23838 completed (loss: 0.35808175802230835, acc: 0.8742138147354126)
[2025-02-16 12:52:15,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:15,479][root][INFO] - Training Epoch: 1/2, step 11337/23838 completed (loss: 0.26925763487815857, acc: 0.9144737124443054)
[2025-02-16 12:52:15,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:15,955][root][INFO] - Training Epoch: 1/2, step 11338/23838 completed (loss: 0.275061696767807, acc: 0.9038461446762085)
[2025-02-16 12:52:16,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:16,394][root][INFO] - Training Epoch: 1/2, step 11339/23838 completed (loss: 0.4404590427875519, acc: 0.8842105269432068)
[2025-02-16 12:52:16,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:17,367][root][INFO] - Training Epoch: 1/2, step 11340/23838 completed (loss: 0.2848220765590668, acc: 0.9305136203765869)
[2025-02-16 12:52:17,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:17,827][root][INFO] - Training Epoch: 1/2, step 11341/23838 completed (loss: 0.38168466091156006, acc: 0.921875)
[2025-02-16 12:52:18,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:18,807][root][INFO] - Training Epoch: 1/2, step 11342/23838 completed (loss: 0.6268544793128967, acc: 0.8219895362854004)
[2025-02-16 12:52:19,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:19,256][root][INFO] - Training Epoch: 1/2, step 11343/23838 completed (loss: 0.46855008602142334, acc: 0.858208954334259)
[2025-02-16 12:52:19,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:19,680][root][INFO] - Training Epoch: 1/2, step 11344/23838 completed (loss: 0.18676793575286865, acc: 0.9454545378684998)
[2025-02-16 12:52:19,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:20,211][root][INFO] - Training Epoch: 1/2, step 11345/23838 completed (loss: 0.20091669261455536, acc: 0.9320388436317444)
[2025-02-16 12:52:20,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:20,684][root][INFO] - Training Epoch: 1/2, step 11346/23838 completed (loss: 0.22318774461746216, acc: 0.9132652878761292)
[2025-02-16 12:52:20,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:21,143][root][INFO] - Training Epoch: 1/2, step 11347/23838 completed (loss: 0.36740949749946594, acc: 0.8999999761581421)
[2025-02-16 12:52:21,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:21,610][root][INFO] - Training Epoch: 1/2, step 11348/23838 completed (loss: 0.28935766220092773, acc: 0.8984771370887756)
[2025-02-16 12:52:21,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:22,112][root][INFO] - Training Epoch: 1/2, step 11349/23838 completed (loss: 0.18293462693691254, acc: 0.9605262875556946)
[2025-02-16 12:52:22,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:22,578][root][INFO] - Training Epoch: 1/2, step 11350/23838 completed (loss: 0.18363378942012787, acc: 0.9403669834136963)
[2025-02-16 12:52:22,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:23,044][root][INFO] - Training Epoch: 1/2, step 11351/23838 completed (loss: 0.14362160861492157, acc: 0.9467455744743347)
[2025-02-16 12:52:23,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:23,501][root][INFO] - Training Epoch: 1/2, step 11352/23838 completed (loss: 0.40105122327804565, acc: 0.8701298832893372)
[2025-02-16 12:52:23,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:23,991][root][INFO] - Training Epoch: 1/2, step 11353/23838 completed (loss: 0.38215476274490356, acc: 0.8952381014823914)
[2025-02-16 12:52:24,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:24,439][root][INFO] - Training Epoch: 1/2, step 11354/23838 completed (loss: 0.4133317470550537, acc: 0.8902438879013062)
[2025-02-16 12:52:24,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:24,876][root][INFO] - Training Epoch: 1/2, step 11355/23838 completed (loss: 0.22355704009532928, acc: 0.93388432264328)
[2025-02-16 12:52:25,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:25,262][root][INFO] - Training Epoch: 1/2, step 11356/23838 completed (loss: 0.2789485454559326, acc: 0.9344262480735779)
[2025-02-16 12:52:25,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:25,684][root][INFO] - Training Epoch: 1/2, step 11357/23838 completed (loss: 0.21828153729438782, acc: 0.9428571462631226)
[2025-02-16 12:52:25,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:26,178][root][INFO] - Training Epoch: 1/2, step 11358/23838 completed (loss: 0.47714072465896606, acc: 0.8652482032775879)
[2025-02-16 12:52:26,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:26,599][root][INFO] - Training Epoch: 1/2, step 11359/23838 completed (loss: 0.12585219740867615, acc: 0.9444444179534912)
[2025-02-16 12:52:26,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:27,034][root][INFO] - Training Epoch: 1/2, step 11360/23838 completed (loss: 0.1890985071659088, acc: 0.9466666579246521)
[2025-02-16 12:52:27,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:27,475][root][INFO] - Training Epoch: 1/2, step 11361/23838 completed (loss: 0.15954995155334473, acc: 0.9577465057373047)
[2025-02-16 12:52:27,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:27,912][root][INFO] - Training Epoch: 1/2, step 11362/23838 completed (loss: 0.31493622064590454, acc: 0.9239130616188049)
[2025-02-16 12:52:28,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:28,295][root][INFO] - Training Epoch: 1/2, step 11363/23838 completed (loss: 0.20391260087490082, acc: 0.9508196711540222)
[2025-02-16 12:52:28,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:28,915][root][INFO] - Training Epoch: 1/2, step 11364/23838 completed (loss: 0.24495728313922882, acc: 0.9144144058227539)
[2025-02-16 12:52:29,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:29,360][root][INFO] - Training Epoch: 1/2, step 11365/23838 completed (loss: 0.5160645246505737, acc: 0.8775510191917419)
[2025-02-16 12:52:29,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:29,939][root][INFO] - Training Epoch: 1/2, step 11366/23838 completed (loss: 0.2274141162633896, acc: 0.9307692050933838)
[2025-02-16 12:52:30,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:30,331][root][INFO] - Training Epoch: 1/2, step 11367/23838 completed (loss: 0.2956126928329468, acc: 0.9090909361839294)
[2025-02-16 12:52:30,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:30,737][root][INFO] - Training Epoch: 1/2, step 11368/23838 completed (loss: 0.22296419739723206, acc: 0.9191176295280457)
[2025-02-16 12:52:30,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:31,166][root][INFO] - Training Epoch: 1/2, step 11369/23838 completed (loss: 0.32042205333709717, acc: 0.8933333158493042)
[2025-02-16 12:52:31,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:31,624][root][INFO] - Training Epoch: 1/2, step 11370/23838 completed (loss: 0.14211788773536682, acc: 0.9532163739204407)
[2025-02-16 12:52:31,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:32,023][root][INFO] - Training Epoch: 1/2, step 11371/23838 completed (loss: 0.5721635818481445, acc: 0.8235294222831726)
[2025-02-16 12:52:32,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:32,461][root][INFO] - Training Epoch: 1/2, step 11372/23838 completed (loss: 0.29915258288383484, acc: 0.8970588445663452)
[2025-02-16 12:52:32,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:32,942][root][INFO] - Training Epoch: 1/2, step 11373/23838 completed (loss: 0.3871135711669922, acc: 0.8928571343421936)
[2025-02-16 12:52:33,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:33,375][root][INFO] - Training Epoch: 1/2, step 11374/23838 completed (loss: 0.42079487442970276, acc: 0.875)
[2025-02-16 12:52:33,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:33,870][root][INFO] - Training Epoch: 1/2, step 11375/23838 completed (loss: 0.3001145124435425, acc: 0.9215686321258545)
[2025-02-16 12:52:34,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:34,331][root][INFO] - Training Epoch: 1/2, step 11376/23838 completed (loss: 0.34248456358909607, acc: 0.8976377844810486)
[2025-02-16 12:52:34,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:34,875][root][INFO] - Training Epoch: 1/2, step 11377/23838 completed (loss: 1.013179063796997, acc: 0.6969696879386902)
[2025-02-16 12:52:35,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:35,379][root][INFO] - Training Epoch: 1/2, step 11378/23838 completed (loss: 0.6937490701675415, acc: 0.8372092843055725)
[2025-02-16 12:52:35,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:35,849][root][INFO] - Training Epoch: 1/2, step 11379/23838 completed (loss: 0.3104119300842285, acc: 0.939393937587738)
[2025-02-16 12:52:36,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:36,286][root][INFO] - Training Epoch: 1/2, step 11380/23838 completed (loss: 0.32786813378334045, acc: 0.9090909361839294)
[2025-02-16 12:52:36,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:36,788][root][INFO] - Training Epoch: 1/2, step 11381/23838 completed (loss: 0.41780006885528564, acc: 0.8770492076873779)
[2025-02-16 12:52:36,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:37,225][root][INFO] - Training Epoch: 1/2, step 11382/23838 completed (loss: 0.060452669858932495, acc: 0.9776536226272583)
[2025-02-16 12:52:37,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:37,678][root][INFO] - Training Epoch: 1/2, step 11383/23838 completed (loss: 0.33656376600265503, acc: 0.8536585569381714)
[2025-02-16 12:52:37,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:38,164][root][INFO] - Training Epoch: 1/2, step 11384/23838 completed (loss: 0.31053563952445984, acc: 0.918367326259613)
[2025-02-16 12:52:38,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:38,705][root][INFO] - Training Epoch: 1/2, step 11385/23838 completed (loss: 0.148756206035614, acc: 0.9447236061096191)
[2025-02-16 12:52:38,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:39,170][root][INFO] - Training Epoch: 1/2, step 11386/23838 completed (loss: 0.4175178110599518, acc: 0.8581560254096985)
[2025-02-16 12:52:39,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:39,589][root][INFO] - Training Epoch: 1/2, step 11387/23838 completed (loss: 0.7604259848594666, acc: 0.8130841255187988)
[2025-02-16 12:52:39,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:40,035][root][INFO] - Training Epoch: 1/2, step 11388/23838 completed (loss: 0.31352853775024414, acc: 0.9207048416137695)
[2025-02-16 12:52:40,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:40,396][root][INFO] - Training Epoch: 1/2, step 11389/23838 completed (loss: 0.32898372411727905, acc: 0.9444444179534912)
[2025-02-16 12:52:40,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:40,828][root][INFO] - Training Epoch: 1/2, step 11390/23838 completed (loss: 0.39816761016845703, acc: 0.8641975522041321)
[2025-02-16 12:52:41,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:41,249][root][INFO] - Training Epoch: 1/2, step 11391/23838 completed (loss: 0.33347025513648987, acc: 0.8913043737411499)
[2025-02-16 12:52:41,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:41,732][root][INFO] - Training Epoch: 1/2, step 11392/23838 completed (loss: 0.243455171585083, acc: 0.9468085169792175)
[2025-02-16 12:52:41,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:42,215][root][INFO] - Training Epoch: 1/2, step 11393/23838 completed (loss: 0.1643477827310562, acc: 0.9350649118423462)
[2025-02-16 12:52:42,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:42,682][root][INFO] - Training Epoch: 1/2, step 11394/23838 completed (loss: 0.49012038111686707, acc: 0.8181818127632141)
[2025-02-16 12:52:42,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:43,123][root][INFO] - Training Epoch: 1/2, step 11395/23838 completed (loss: 0.2932714819908142, acc: 0.9195402264595032)
[2025-02-16 12:52:43,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:43,557][root][INFO] - Training Epoch: 1/2, step 11396/23838 completed (loss: 0.13521091639995575, acc: 0.9611650705337524)
[2025-02-16 12:52:43,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:44,017][root][INFO] - Training Epoch: 1/2, step 11397/23838 completed (loss: 0.39503929018974304, acc: 0.8943089246749878)
[2025-02-16 12:52:44,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:44,425][root][INFO] - Training Epoch: 1/2, step 11398/23838 completed (loss: 0.1419076770544052, acc: 0.9777777791023254)
[2025-02-16 12:52:44,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:44,922][root][INFO] - Training Epoch: 1/2, step 11399/23838 completed (loss: 0.17884936928749084, acc: 0.9515151381492615)
[2025-02-16 12:52:45,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:45,392][root][INFO] - Training Epoch: 1/2, step 11400/23838 completed (loss: 0.1680845320224762, acc: 0.9545454382896423)
[2025-02-16 12:52:45,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:45,820][root][INFO] - Training Epoch: 1/2, step 11401/23838 completed (loss: 0.4430283010005951, acc: 0.9160305261611938)
[2025-02-16 12:52:46,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:46,282][root][INFO] - Training Epoch: 1/2, step 11402/23838 completed (loss: 0.28317371010780334, acc: 0.939393937587738)
[2025-02-16 12:52:46,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:46,734][root][INFO] - Training Epoch: 1/2, step 11403/23838 completed (loss: 0.28178298473358154, acc: 0.9066666960716248)
[2025-02-16 12:52:46,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:47,155][root][INFO] - Training Epoch: 1/2, step 11404/23838 completed (loss: 0.30909255146980286, acc: 0.914893627166748)
[2025-02-16 12:52:47,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:47,673][root][INFO] - Training Epoch: 1/2, step 11405/23838 completed (loss: 0.3538709282875061, acc: 0.892307698726654)
[2025-02-16 12:52:47,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:48,112][root][INFO] - Training Epoch: 1/2, step 11406/23838 completed (loss: 0.22028115391731262, acc: 0.9370629191398621)
[2025-02-16 12:52:48,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:48,575][root][INFO] - Training Epoch: 1/2, step 11407/23838 completed (loss: 0.3062838912010193, acc: 0.9240506291389465)
[2025-02-16 12:52:48,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:48,974][root][INFO] - Training Epoch: 1/2, step 11408/23838 completed (loss: 0.3056392967700958, acc: 0.8552631735801697)
[2025-02-16 12:52:49,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:49,503][root][INFO] - Training Epoch: 1/2, step 11409/23838 completed (loss: 0.454651415348053, acc: 0.8814814686775208)
[2025-02-16 12:52:49,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:49,962][root][INFO] - Training Epoch: 1/2, step 11410/23838 completed (loss: 0.2723085582256317, acc: 0.8931297659873962)
[2025-02-16 12:52:50,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:50,439][root][INFO] - Training Epoch: 1/2, step 11411/23838 completed (loss: 0.22958359122276306, acc: 0.9344262480735779)
[2025-02-16 12:52:50,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:50,875][root][INFO] - Training Epoch: 1/2, step 11412/23838 completed (loss: 0.10164047032594681, acc: 0.9797979593276978)
[2025-02-16 12:52:51,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:51,323][root][INFO] - Training Epoch: 1/2, step 11413/23838 completed (loss: 0.38888683915138245, acc: 0.9016393423080444)
[2025-02-16 12:52:51,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:51,722][root][INFO] - Training Epoch: 1/2, step 11414/23838 completed (loss: 0.4888617694377899, acc: 0.8721804618835449)
[2025-02-16 12:52:51,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:52,149][root][INFO] - Training Epoch: 1/2, step 11415/23838 completed (loss: 0.40685757994651794, acc: 0.8979591727256775)
[2025-02-16 12:52:52,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:52,632][root][INFO] - Training Epoch: 1/2, step 11416/23838 completed (loss: 0.18143263459205627, acc: 0.9503546357154846)
[2025-02-16 12:52:52,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:53,122][root][INFO] - Training Epoch: 1/2, step 11417/23838 completed (loss: 0.20965413749217987, acc: 0.9492753744125366)
[2025-02-16 12:52:53,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:53,541][root][INFO] - Training Epoch: 1/2, step 11418/23838 completed (loss: 0.2664867341518402, acc: 0.921875)
[2025-02-16 12:52:53,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:54,147][root][INFO] - Training Epoch: 1/2, step 11419/23838 completed (loss: 0.25734853744506836, acc: 0.9222797751426697)
[2025-02-16 12:52:54,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:54,603][root][INFO] - Training Epoch: 1/2, step 11420/23838 completed (loss: 0.23617105185985565, acc: 0.9270073175430298)
[2025-02-16 12:52:54,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:55,044][root][INFO] - Training Epoch: 1/2, step 11421/23838 completed (loss: 0.35921818017959595, acc: 0.8933333158493042)
[2025-02-16 12:52:55,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:55,414][root][INFO] - Training Epoch: 1/2, step 11422/23838 completed (loss: 0.20724256336688995, acc: 0.949999988079071)
[2025-02-16 12:52:55,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:55,865][root][INFO] - Training Epoch: 1/2, step 11423/23838 completed (loss: 0.2569393217563629, acc: 0.9115044474601746)
[2025-02-16 12:52:56,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:56,284][root][INFO] - Training Epoch: 1/2, step 11424/23838 completed (loss: 0.3710016906261444, acc: 0.9194630980491638)
[2025-02-16 12:52:56,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:56,737][root][INFO] - Training Epoch: 1/2, step 11425/23838 completed (loss: 0.233991339802742, acc: 0.9277108311653137)
[2025-02-16 12:52:56,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:57,124][root][INFO] - Training Epoch: 1/2, step 11426/23838 completed (loss: 0.4211008846759796, acc: 0.875)
[2025-02-16 12:52:57,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:57,552][root][INFO] - Training Epoch: 1/2, step 11427/23838 completed (loss: 0.3693678677082062, acc: 0.884393036365509)
[2025-02-16 12:52:57,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:57,909][root][INFO] - Training Epoch: 1/2, step 11428/23838 completed (loss: 0.20414793491363525, acc: 0.921875)
[2025-02-16 12:52:58,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:58,364][root][INFO] - Training Epoch: 1/2, step 11429/23838 completed (loss: 0.17350831627845764, acc: 0.9576271176338196)
[2025-02-16 12:52:58,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:58,812][root][INFO] - Training Epoch: 1/2, step 11430/23838 completed (loss: 0.2091190665960312, acc: 0.9101123809814453)
[2025-02-16 12:52:59,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:59,287][root][INFO] - Training Epoch: 1/2, step 11431/23838 completed (loss: 0.14146099984645844, acc: 0.963302731513977)
[2025-02-16 12:52:59,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:52:59,753][root][INFO] - Training Epoch: 1/2, step 11432/23838 completed (loss: 0.16712012887001038, acc: 0.9689922332763672)
[2025-02-16 12:52:59,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:00,155][root][INFO] - Training Epoch: 1/2, step 11433/23838 completed (loss: 0.30542540550231934, acc: 0.9220778942108154)
[2025-02-16 12:53:00,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:00,635][root][INFO] - Training Epoch: 1/2, step 11434/23838 completed (loss: 0.40594202280044556, acc: 0.8962264060974121)
[2025-02-16 12:53:00,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:01,089][root][INFO] - Training Epoch: 1/2, step 11435/23838 completed (loss: 0.45726823806762695, acc: 0.902255654335022)
[2025-02-16 12:53:01,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:01,544][root][INFO] - Training Epoch: 1/2, step 11436/23838 completed (loss: 0.31661710143089294, acc: 0.9034090638160706)
[2025-02-16 12:53:01,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:02,037][root][INFO] - Training Epoch: 1/2, step 11437/23838 completed (loss: 0.26806262135505676, acc: 0.9234972596168518)
[2025-02-16 12:53:02,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:02,471][root][INFO] - Training Epoch: 1/2, step 11438/23838 completed (loss: 0.2653954029083252, acc: 0.9333333373069763)
[2025-02-16 12:53:02,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:03,011][root][INFO] - Training Epoch: 1/2, step 11439/23838 completed (loss: 0.15144841372966766, acc: 0.9611111283302307)
[2025-02-16 12:53:03,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:03,543][root][INFO] - Training Epoch: 1/2, step 11440/23838 completed (loss: 0.13599060475826263, acc: 0.9497206807136536)
[2025-02-16 12:53:03,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:04,073][root][INFO] - Training Epoch: 1/2, step 11441/23838 completed (loss: 0.32406607270240784, acc: 0.9270073175430298)
[2025-02-16 12:53:04,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:04,687][root][INFO] - Training Epoch: 1/2, step 11442/23838 completed (loss: 0.16633963584899902, acc: 0.959770143032074)
[2025-02-16 12:53:04,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:05,210][root][INFO] - Training Epoch: 1/2, step 11443/23838 completed (loss: 0.29305607080459595, acc: 0.9277978539466858)
[2025-02-16 12:53:05,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:05,731][root][INFO] - Training Epoch: 1/2, step 11444/23838 completed (loss: 0.09051705151796341, acc: 0.9788732528686523)
[2025-02-16 12:53:05,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:06,198][root][INFO] - Training Epoch: 1/2, step 11445/23838 completed (loss: 0.2536759674549103, acc: 0.9426229596138)
[2025-02-16 12:53:06,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:06,603][root][INFO] - Training Epoch: 1/2, step 11446/23838 completed (loss: 0.09269784390926361, acc: 0.9672130942344666)
[2025-02-16 12:53:06,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:07,023][root][INFO] - Training Epoch: 1/2, step 11447/23838 completed (loss: 0.07887756824493408, acc: 0.9871794581413269)
[2025-02-16 12:53:07,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:07,421][root][INFO] - Training Epoch: 1/2, step 11448/23838 completed (loss: 0.09502195566892624, acc: 0.9779005646705627)
[2025-02-16 12:53:07,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:07,821][root][INFO] - Training Epoch: 1/2, step 11449/23838 completed (loss: 0.5124709606170654, acc: 0.834782600402832)
[2025-02-16 12:53:08,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:08,245][root][INFO] - Training Epoch: 1/2, step 11450/23838 completed (loss: 0.5279369354248047, acc: 0.8854166865348816)
[2025-02-16 12:53:08,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:08,671][root][INFO] - Training Epoch: 1/2, step 11451/23838 completed (loss: 0.25142064690589905, acc: 0.9368420839309692)
[2025-02-16 12:53:08,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:09,120][root][INFO] - Training Epoch: 1/2, step 11452/23838 completed (loss: 0.15597595274448395, acc: 0.9508196711540222)
[2025-02-16 12:53:09,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:09,607][root][INFO] - Training Epoch: 1/2, step 11453/23838 completed (loss: 0.2673867344856262, acc: 0.930232584476471)
[2025-02-16 12:53:09,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:10,115][root][INFO] - Training Epoch: 1/2, step 11454/23838 completed (loss: 0.526668906211853, acc: 0.8684210777282715)
[2025-02-16 12:53:10,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:10,758][root][INFO] - Training Epoch: 1/2, step 11455/23838 completed (loss: 0.3699866831302643, acc: 0.8967971801757812)
[2025-02-16 12:53:10,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:11,232][root][INFO] - Training Epoch: 1/2, step 11456/23838 completed (loss: 0.16643644869327545, acc: 0.9236640930175781)
[2025-02-16 12:53:11,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:11,642][root][INFO] - Training Epoch: 1/2, step 11457/23838 completed (loss: 0.5364392995834351, acc: 0.8482758402824402)
[2025-02-16 12:53:11,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:12,283][root][INFO] - Training Epoch: 1/2, step 11458/23838 completed (loss: 0.2497953474521637, acc: 0.9202279448509216)
[2025-02-16 12:53:12,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:12,676][root][INFO] - Training Epoch: 1/2, step 11459/23838 completed (loss: 0.3024604618549347, acc: 0.9115646481513977)
[2025-02-16 12:53:12,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:13,136][root][INFO] - Training Epoch: 1/2, step 11460/23838 completed (loss: 0.279942125082016, acc: 0.9198312163352966)
[2025-02-16 12:53:13,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:13,562][root][INFO] - Training Epoch: 1/2, step 11461/23838 completed (loss: 0.4641026258468628, acc: 0.8484848737716675)
[2025-02-16 12:53:13,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:14,004][root][INFO] - Training Epoch: 1/2, step 11462/23838 completed (loss: 0.21018201112747192, acc: 0.9482758641242981)
[2025-02-16 12:53:14,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:14,642][root][INFO] - Training Epoch: 1/2, step 11463/23838 completed (loss: 0.21960461139678955, acc: 0.9414414167404175)
[2025-02-16 12:53:14,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:15,089][root][INFO] - Training Epoch: 1/2, step 11464/23838 completed (loss: 0.24418100714683533, acc: 0.9539473652839661)
[2025-02-16 12:53:15,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:15,636][root][INFO] - Training Epoch: 1/2, step 11465/23838 completed (loss: 0.45319294929504395, acc: 0.8372092843055725)
[2025-02-16 12:53:15,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:16,070][root][INFO] - Training Epoch: 1/2, step 11466/23838 completed (loss: 0.3140709102153778, acc: 0.90625)
[2025-02-16 12:53:16,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:16,506][root][INFO] - Training Epoch: 1/2, step 11467/23838 completed (loss: 0.3944868743419647, acc: 0.9101123809814453)
[2025-02-16 12:53:16,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:16,966][root][INFO] - Training Epoch: 1/2, step 11468/23838 completed (loss: 0.33272847533226013, acc: 0.9025423526763916)
[2025-02-16 12:53:17,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:17,438][root][INFO] - Training Epoch: 1/2, step 11469/23838 completed (loss: 0.41603171825408936, acc: 0.9010416865348816)
[2025-02-16 12:53:17,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:17,894][root][INFO] - Training Epoch: 1/2, step 11470/23838 completed (loss: 0.5908253192901611, acc: 0.843137264251709)
[2025-02-16 12:53:18,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:18,374][root][INFO] - Training Epoch: 1/2, step 11471/23838 completed (loss: 0.520132839679718, acc: 0.8721804618835449)
[2025-02-16 12:53:18,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:19,055][root][INFO] - Training Epoch: 1/2, step 11472/23838 completed (loss: 0.29556208848953247, acc: 0.9278846383094788)
[2025-02-16 12:53:19,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:19,491][root][INFO] - Training Epoch: 1/2, step 11473/23838 completed (loss: 0.21805648505687714, acc: 0.9469696879386902)
[2025-02-16 12:53:19,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:19,961][root][INFO] - Training Epoch: 1/2, step 11474/23838 completed (loss: 0.14291274547576904, acc: 0.954954981803894)
[2025-02-16 12:53:20,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:20,546][root][INFO] - Training Epoch: 1/2, step 11475/23838 completed (loss: 0.1247999519109726, acc: 0.9485981464385986)
[2025-02-16 12:53:20,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:21,019][root][INFO] - Training Epoch: 1/2, step 11476/23838 completed (loss: 0.37752461433410645, acc: 0.9116021990776062)
[2025-02-16 12:53:21,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:21,478][root][INFO] - Training Epoch: 1/2, step 11477/23838 completed (loss: 0.21712715923786163, acc: 0.939393937587738)
[2025-02-16 12:53:21,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:21,877][root][INFO] - Training Epoch: 1/2, step 11478/23838 completed (loss: 0.2002367526292801, acc: 0.939393937587738)
[2025-02-16 12:53:22,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:22,442][root][INFO] - Training Epoch: 1/2, step 11479/23838 completed (loss: 0.19172634184360504, acc: 0.9507389068603516)
[2025-02-16 12:53:22,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:22,927][root][INFO] - Training Epoch: 1/2, step 11480/23838 completed (loss: 0.33969998359680176, acc: 0.893203854560852)
[2025-02-16 12:53:23,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:23,402][root][INFO] - Training Epoch: 1/2, step 11481/23838 completed (loss: 0.48966366052627563, acc: 0.8671875)
[2025-02-16 12:53:23,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:23,841][root][INFO] - Training Epoch: 1/2, step 11482/23838 completed (loss: 0.32271847128868103, acc: 0.924369752407074)
[2025-02-16 12:53:24,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:24,257][root][INFO] - Training Epoch: 1/2, step 11483/23838 completed (loss: 0.26406028866767883, acc: 0.9203540086746216)
[2025-02-16 12:53:24,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:24,758][root][INFO] - Training Epoch: 1/2, step 11484/23838 completed (loss: 0.11417719721794128, acc: 0.9689440727233887)
[2025-02-16 12:53:25,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:25,490][root][INFO] - Training Epoch: 1/2, step 11485/23838 completed (loss: 0.24659402668476105, acc: 0.9313432574272156)
[2025-02-16 12:53:25,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:25,943][root][INFO] - Training Epoch: 1/2, step 11486/23838 completed (loss: 0.31359824538230896, acc: 0.8952381014823914)
[2025-02-16 12:53:26,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:26,654][root][INFO] - Training Epoch: 1/2, step 11487/23838 completed (loss: 0.2076847106218338, acc: 0.9591836929321289)
[2025-02-16 12:53:26,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:27,105][root][INFO] - Training Epoch: 1/2, step 11488/23838 completed (loss: 0.2990622818470001, acc: 0.9101123809814453)
[2025-02-16 12:53:27,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:27,594][root][INFO] - Training Epoch: 1/2, step 11489/23838 completed (loss: 0.18931499123573303, acc: 0.9459459185600281)
[2025-02-16 12:53:27,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:28,035][root][INFO] - Training Epoch: 1/2, step 11490/23838 completed (loss: 0.3561723232269287, acc: 0.90625)
[2025-02-16 12:53:28,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:28,556][root][INFO] - Training Epoch: 1/2, step 11491/23838 completed (loss: 0.1646990180015564, acc: 0.9622641801834106)
[2025-02-16 12:53:28,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:29,167][root][INFO] - Training Epoch: 1/2, step 11492/23838 completed (loss: 0.2882499396800995, acc: 0.9329897165298462)
[2025-02-16 12:53:29,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:29,574][root][INFO] - Training Epoch: 1/2, step 11493/23838 completed (loss: 0.2785346508026123, acc: 0.8860759735107422)
[2025-02-16 12:53:29,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:30,012][root][INFO] - Training Epoch: 1/2, step 11494/23838 completed (loss: 0.2847142815589905, acc: 0.9095744490623474)
[2025-02-16 12:53:30,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:30,451][root][INFO] - Training Epoch: 1/2, step 11495/23838 completed (loss: 0.2922383248806, acc: 0.9303797483444214)
[2025-02-16 12:53:30,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:30,880][root][INFO] - Training Epoch: 1/2, step 11496/23838 completed (loss: 0.37779709696769714, acc: 0.9264705777168274)
[2025-02-16 12:53:31,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:31,300][root][INFO] - Training Epoch: 1/2, step 11497/23838 completed (loss: 0.4191977381706238, acc: 0.8947368264198303)
[2025-02-16 12:53:31,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:31,705][root][INFO] - Training Epoch: 1/2, step 11498/23838 completed (loss: 0.7206391096115112, acc: 0.7704917788505554)
[2025-02-16 12:53:31,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:32,154][root][INFO] - Training Epoch: 1/2, step 11499/23838 completed (loss: 0.7226185202598572, acc: 0.8131868243217468)
[2025-02-16 12:53:32,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:32,629][root][INFO] - Training Epoch: 1/2, step 11500/23838 completed (loss: 0.17279943823814392, acc: 0.9572649598121643)
[2025-02-16 12:53:32,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:33,063][root][INFO] - Training Epoch: 1/2, step 11501/23838 completed (loss: 0.4833413362503052, acc: 0.8333333134651184)
[2025-02-16 12:53:33,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:33,524][root][INFO] - Training Epoch: 1/2, step 11502/23838 completed (loss: 0.5889895558357239, acc: 0.8024691343307495)
[2025-02-16 12:53:33,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:34,402][root][INFO] - Training Epoch: 1/2, step 11503/23838 completed (loss: 0.20512716472148895, acc: 0.9440789222717285)
[2025-02-16 12:53:34,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:35,287][root][INFO] - Training Epoch: 1/2, step 11504/23838 completed (loss: 0.35567599534988403, acc: 0.8974359035491943)
[2025-02-16 12:53:35,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:35,797][root][INFO] - Training Epoch: 1/2, step 11505/23838 completed (loss: 0.3332187235355377, acc: 0.9047619104385376)
[2025-02-16 12:53:36,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:36,303][root][INFO] - Training Epoch: 1/2, step 11506/23838 completed (loss: 0.1681976467370987, acc: 0.9399999976158142)
[2025-02-16 12:53:36,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:36,880][root][INFO] - Training Epoch: 1/2, step 11507/23838 completed (loss: 0.4400864541530609, acc: 0.8823529481887817)
[2025-02-16 12:53:37,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:37,382][root][INFO] - Training Epoch: 1/2, step 11508/23838 completed (loss: 0.2731017470359802, acc: 0.9235293865203857)
[2025-02-16 12:53:37,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:38,155][root][INFO] - Training Epoch: 1/2, step 11509/23838 completed (loss: 0.25468534231185913, acc: 0.9408602118492126)
[2025-02-16 12:53:38,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:38,758][root][INFO] - Training Epoch: 1/2, step 11510/23838 completed (loss: 0.3744502365589142, acc: 0.9212598204612732)
[2025-02-16 12:53:38,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:39,172][root][INFO] - Training Epoch: 1/2, step 11511/23838 completed (loss: 0.16829586029052734, acc: 0.9597315192222595)
[2025-02-16 12:53:39,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:39,606][root][INFO] - Training Epoch: 1/2, step 11512/23838 completed (loss: 0.35116514563560486, acc: 0.939393937587738)
[2025-02-16 12:53:39,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:40,069][root][INFO] - Training Epoch: 1/2, step 11513/23838 completed (loss: 0.2877657115459442, acc: 0.9029850959777832)
[2025-02-16 12:53:40,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:40,536][root][INFO] - Training Epoch: 1/2, step 11514/23838 completed (loss: 0.16929681599140167, acc: 0.9609755873680115)
[2025-02-16 12:53:40,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:41,028][root][INFO] - Training Epoch: 1/2, step 11515/23838 completed (loss: 0.18263909220695496, acc: 0.9537037014961243)
[2025-02-16 12:53:41,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:41,459][root][INFO] - Training Epoch: 1/2, step 11516/23838 completed (loss: 0.3293611407279968, acc: 0.9047619104385376)
[2025-02-16 12:53:41,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:42,066][root][INFO] - Training Epoch: 1/2, step 11517/23838 completed (loss: 0.203527569770813, acc: 0.9411764740943909)
[2025-02-16 12:53:42,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:42,469][root][INFO] - Training Epoch: 1/2, step 11518/23838 completed (loss: 0.17847755551338196, acc: 0.9384615421295166)
[2025-02-16 12:53:42,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:42,930][root][INFO] - Training Epoch: 1/2, step 11519/23838 completed (loss: 0.23439639806747437, acc: 0.9328358173370361)
[2025-02-16 12:53:43,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:43,376][root][INFO] - Training Epoch: 1/2, step 11520/23838 completed (loss: 0.5541680455207825, acc: 0.8645833134651184)
[2025-02-16 12:53:43,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:43,804][root][INFO] - Training Epoch: 1/2, step 11521/23838 completed (loss: 0.5998969078063965, acc: 0.8384615182876587)
[2025-02-16 12:53:44,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:44,296][root][INFO] - Training Epoch: 1/2, step 11522/23838 completed (loss: 0.4838647246360779, acc: 0.848739504814148)
[2025-02-16 12:53:44,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:44,737][root][INFO] - Training Epoch: 1/2, step 11523/23838 completed (loss: 0.4642082750797272, acc: 0.8503937125205994)
[2025-02-16 12:53:44,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:45,192][root][INFO] - Training Epoch: 1/2, step 11524/23838 completed (loss: 0.19564245641231537, acc: 0.955974817276001)
[2025-02-16 12:53:45,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:45,666][root][INFO] - Training Epoch: 1/2, step 11525/23838 completed (loss: 0.43975481390953064, acc: 0.8690476417541504)
[2025-02-16 12:53:45,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:46,111][root][INFO] - Training Epoch: 1/2, step 11526/23838 completed (loss: 0.27586185932159424, acc: 0.9204545617103577)
[2025-02-16 12:53:46,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:46,491][root][INFO] - Training Epoch: 1/2, step 11527/23838 completed (loss: 0.33972758054733276, acc: 0.8928571343421936)
[2025-02-16 12:53:46,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:47,044][root][INFO] - Training Epoch: 1/2, step 11528/23838 completed (loss: 0.21783263981342316, acc: 0.942148745059967)
[2025-02-16 12:53:47,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:47,492][root][INFO] - Training Epoch: 1/2, step 11529/23838 completed (loss: 0.13215108215808868, acc: 0.9618320465087891)
[2025-02-16 12:53:47,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:47,941][root][INFO] - Training Epoch: 1/2, step 11530/23838 completed (loss: 0.3885034918785095, acc: 0.8850574493408203)
[2025-02-16 12:53:48,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:48,388][root][INFO] - Training Epoch: 1/2, step 11531/23838 completed (loss: 0.3176119327545166, acc: 0.9084967374801636)
[2025-02-16 12:53:48,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:48,807][root][INFO] - Training Epoch: 1/2, step 11532/23838 completed (loss: 0.3287331461906433, acc: 0.9017857313156128)
[2025-02-16 12:53:49,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:49,413][root][INFO] - Training Epoch: 1/2, step 11533/23838 completed (loss: 0.4020054340362549, acc: 0.8848484754562378)
[2025-02-16 12:53:49,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:49,897][root][INFO] - Training Epoch: 1/2, step 11534/23838 completed (loss: 0.16309861838817596, acc: 0.9746192693710327)
[2025-02-16 12:53:50,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:50,661][root][INFO] - Training Epoch: 1/2, step 11535/23838 completed (loss: 0.4079795479774475, acc: 0.8878923654556274)
[2025-02-16 12:53:50,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:51,054][root][INFO] - Training Epoch: 1/2, step 11536/23838 completed (loss: 0.1269572377204895, acc: 0.9734513163566589)
[2025-02-16 12:53:51,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:51,570][root][INFO] - Training Epoch: 1/2, step 11537/23838 completed (loss: 0.5225342512130737, acc: 0.8842592835426331)
[2025-02-16 12:53:51,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:52,054][root][INFO] - Training Epoch: 1/2, step 11538/23838 completed (loss: 0.3526606261730194, acc: 0.9146005511283875)
[2025-02-16 12:53:52,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:52,493][root][INFO] - Training Epoch: 1/2, step 11539/23838 completed (loss: 0.09355223923921585, acc: 0.9589040875434875)
[2025-02-16 12:53:52,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:52,867][root][INFO] - Training Epoch: 1/2, step 11540/23838 completed (loss: 0.5576884746551514, acc: 0.8470588326454163)
[2025-02-16 12:53:53,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:53,420][root][INFO] - Training Epoch: 1/2, step 11541/23838 completed (loss: 0.28227701783180237, acc: 0.9366515874862671)
[2025-02-16 12:53:53,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:53,883][root][INFO] - Training Epoch: 1/2, step 11542/23838 completed (loss: 0.37263017892837524, acc: 0.8773584961891174)
[2025-02-16 12:53:54,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:54,313][root][INFO] - Training Epoch: 1/2, step 11543/23838 completed (loss: 0.17139571905136108, acc: 0.9722222089767456)
[2025-02-16 12:53:54,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:54,766][root][INFO] - Training Epoch: 1/2, step 11544/23838 completed (loss: 0.15335384011268616, acc: 0.9513888955116272)
[2025-02-16 12:53:54,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:55,185][root][INFO] - Training Epoch: 1/2, step 11545/23838 completed (loss: 0.17358410358428955, acc: 0.9428571462631226)
[2025-02-16 12:53:55,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:55,589][root][INFO] - Training Epoch: 1/2, step 11546/23838 completed (loss: 0.28446704149246216, acc: 0.9195402264595032)
[2025-02-16 12:53:55,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:55,990][root][INFO] - Training Epoch: 1/2, step 11547/23838 completed (loss: 0.3469010889530182, acc: 0.8901098966598511)
[2025-02-16 12:53:56,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:56,394][root][INFO] - Training Epoch: 1/2, step 11548/23838 completed (loss: 0.18395820260047913, acc: 0.9358974099159241)
[2025-02-16 12:53:56,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:56,958][root][INFO] - Training Epoch: 1/2, step 11549/23838 completed (loss: 0.31743142008781433, acc: 0.8984771370887756)
[2025-02-16 12:53:57,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:57,369][root][INFO] - Training Epoch: 1/2, step 11550/23838 completed (loss: 0.22448298335075378, acc: 0.9411764740943909)
[2025-02-16 12:53:57,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:57,848][root][INFO] - Training Epoch: 1/2, step 11551/23838 completed (loss: 0.2954651713371277, acc: 0.954023003578186)
[2025-02-16 12:53:58,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:58,445][root][INFO] - Training Epoch: 1/2, step 11552/23838 completed (loss: 0.3075889050960541, acc: 0.9188033938407898)
[2025-02-16 12:53:58,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:58,872][root][INFO] - Training Epoch: 1/2, step 11553/23838 completed (loss: 0.3347712457180023, acc: 0.8958333134651184)
[2025-02-16 12:53:59,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:59,403][root][INFO] - Training Epoch: 1/2, step 11554/23838 completed (loss: 0.24102847278118134, acc: 0.9485294222831726)
[2025-02-16 12:53:59,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:53:59,856][root][INFO] - Training Epoch: 1/2, step 11555/23838 completed (loss: 0.2656653821468353, acc: 0.8947368264198303)
[2025-02-16 12:54:00,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:00,244][root][INFO] - Training Epoch: 1/2, step 11556/23838 completed (loss: 0.08876431733369827, acc: 1.0)
[2025-02-16 12:54:00,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:00,727][root][INFO] - Training Epoch: 1/2, step 11557/23838 completed (loss: 0.31427574157714844, acc: 0.9130434989929199)
[2025-02-16 12:54:00,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:01,144][root][INFO] - Training Epoch: 1/2, step 11558/23838 completed (loss: 0.6700826287269592, acc: 0.7948718070983887)
[2025-02-16 12:54:01,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:01,591][root][INFO] - Training Epoch: 1/2, step 11559/23838 completed (loss: 0.32742974162101746, acc: 0.8962264060974121)
[2025-02-16 12:54:01,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:02,101][root][INFO] - Training Epoch: 1/2, step 11560/23838 completed (loss: 0.2526010572910309, acc: 0.9347826242446899)
[2025-02-16 12:54:02,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:02,557][root][INFO] - Training Epoch: 1/2, step 11561/23838 completed (loss: 0.3530781865119934, acc: 0.8941176533699036)
[2025-02-16 12:54:02,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:02,926][root][INFO] - Training Epoch: 1/2, step 11562/23838 completed (loss: 0.31094253063201904, acc: 0.9032257795333862)
[2025-02-16 12:54:03,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:03,441][root][INFO] - Training Epoch: 1/2, step 11563/23838 completed (loss: 0.14706125855445862, acc: 0.9685314893722534)
[2025-02-16 12:54:03,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:03,871][root][INFO] - Training Epoch: 1/2, step 11564/23838 completed (loss: 0.2538435161113739, acc: 0.9465240836143494)
[2025-02-16 12:54:04,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:04,318][root][INFO] - Training Epoch: 1/2, step 11565/23838 completed (loss: 0.24357514083385468, acc: 0.9276315569877625)
[2025-02-16 12:54:04,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:04,828][root][INFO] - Training Epoch: 1/2, step 11566/23838 completed (loss: 0.09751236438751221, acc: 0.9708737730979919)
[2025-02-16 12:54:05,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:05,320][root][INFO] - Training Epoch: 1/2, step 11567/23838 completed (loss: 0.26075002551078796, acc: 0.9275362491607666)
[2025-02-16 12:54:05,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:05,797][root][INFO] - Training Epoch: 1/2, step 11568/23838 completed (loss: 0.36561721563339233, acc: 0.8731343150138855)
[2025-02-16 12:54:06,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:06,233][root][INFO] - Training Epoch: 1/2, step 11569/23838 completed (loss: 0.45082613825798035, acc: 0.8831169009208679)
[2025-02-16 12:54:06,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:06,692][root][INFO] - Training Epoch: 1/2, step 11570/23838 completed (loss: 0.24092237651348114, acc: 0.9329897165298462)
[2025-02-16 12:54:06,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:07,132][root][INFO] - Training Epoch: 1/2, step 11571/23838 completed (loss: 0.4013412594795227, acc: 0.9021739363670349)
[2025-02-16 12:54:07,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:07,572][root][INFO] - Training Epoch: 1/2, step 11572/23838 completed (loss: 0.5444627404212952, acc: 0.8703703880310059)
[2025-02-16 12:54:07,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:08,077][root][INFO] - Training Epoch: 1/2, step 11573/23838 completed (loss: 0.6384689807891846, acc: 0.8243243098258972)
[2025-02-16 12:54:08,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:08,538][root][INFO] - Training Epoch: 1/2, step 11574/23838 completed (loss: 0.21516910195350647, acc: 0.9289340376853943)
[2025-02-16 12:54:08,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:08,932][root][INFO] - Training Epoch: 1/2, step 11575/23838 completed (loss: 0.4506823420524597, acc: 0.8931297659873962)
[2025-02-16 12:54:09,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:09,372][root][INFO] - Training Epoch: 1/2, step 11576/23838 completed (loss: 0.4145129919052124, acc: 0.875)
[2025-02-16 12:54:09,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:09,852][root][INFO] - Training Epoch: 1/2, step 11577/23838 completed (loss: 0.29413464665412903, acc: 0.9411764740943909)
[2025-02-16 12:54:10,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:10,286][root][INFO] - Training Epoch: 1/2, step 11578/23838 completed (loss: 0.11806564033031464, acc: 0.9764705896377563)
[2025-02-16 12:54:10,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:10,735][root][INFO] - Training Epoch: 1/2, step 11579/23838 completed (loss: 0.1526661515235901, acc: 0.9554455280303955)
[2025-02-16 12:54:10,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:11,114][root][INFO] - Training Epoch: 1/2, step 11580/23838 completed (loss: 0.3379630744457245, acc: 0.8983050584793091)
[2025-02-16 12:54:11,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:11,609][root][INFO] - Training Epoch: 1/2, step 11581/23838 completed (loss: 0.39934802055358887, acc: 0.895061731338501)
[2025-02-16 12:54:11,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:12,150][root][INFO] - Training Epoch: 1/2, step 11582/23838 completed (loss: 0.23246072232723236, acc: 0.9190751314163208)
[2025-02-16 12:54:12,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:12,644][root][INFO] - Training Epoch: 1/2, step 11583/23838 completed (loss: 0.1974322646856308, acc: 0.9541984796524048)
[2025-02-16 12:54:12,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:13,182][root][INFO] - Training Epoch: 1/2, step 11584/23838 completed (loss: 0.22912612557411194, acc: 0.9409282803535461)
[2025-02-16 12:54:13,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:13,620][root][INFO] - Training Epoch: 1/2, step 11585/23838 completed (loss: 0.43971559405326843, acc: 0.8757396340370178)
[2025-02-16 12:54:13,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:14,049][root][INFO] - Training Epoch: 1/2, step 11586/23838 completed (loss: 0.36707958579063416, acc: 0.90625)
[2025-02-16 12:54:14,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:14,474][root][INFO] - Training Epoch: 1/2, step 11587/23838 completed (loss: 0.27254635095596313, acc: 0.9115384817123413)
[2025-02-16 12:54:14,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:14,884][root][INFO] - Training Epoch: 1/2, step 11588/23838 completed (loss: 0.09676053375005722, acc: 0.9714285731315613)
[2025-02-16 12:54:15,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:15,350][root][INFO] - Training Epoch: 1/2, step 11589/23838 completed (loss: 0.25915688276290894, acc: 0.8974359035491943)
[2025-02-16 12:54:15,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:15,985][root][INFO] - Training Epoch: 1/2, step 11590/23838 completed (loss: 0.16084475815296173, acc: 0.9603174328804016)
[2025-02-16 12:54:16,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:16,629][root][INFO] - Training Epoch: 1/2, step 11591/23838 completed (loss: 0.20881114900112152, acc: 0.9357326626777649)
[2025-02-16 12:54:16,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:17,071][root][INFO] - Training Epoch: 1/2, step 11592/23838 completed (loss: 0.24370262026786804, acc: 0.9300411343574524)
[2025-02-16 12:54:17,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:17,665][root][INFO] - Training Epoch: 1/2, step 11593/23838 completed (loss: 0.19847999513149261, acc: 0.9484127163887024)
[2025-02-16 12:54:17,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:18,177][root][INFO] - Training Epoch: 1/2, step 11594/23838 completed (loss: 0.36630672216415405, acc: 0.8650000095367432)
[2025-02-16 12:54:18,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:18,649][root][INFO] - Training Epoch: 1/2, step 11595/23838 completed (loss: 0.24165965616703033, acc: 0.9357143044471741)
[2025-02-16 12:54:18,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:19,130][root][INFO] - Training Epoch: 1/2, step 11596/23838 completed (loss: 0.15970906615257263, acc: 0.9642857313156128)
[2025-02-16 12:54:19,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:19,913][root][INFO] - Training Epoch: 1/2, step 11597/23838 completed (loss: 0.11040910333395004, acc: 0.9756944179534912)
[2025-02-16 12:54:20,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:20,408][root][INFO] - Training Epoch: 1/2, step 11598/23838 completed (loss: 0.2362326979637146, acc: 0.9301075339317322)
[2025-02-16 12:54:20,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:20,832][root][INFO] - Training Epoch: 1/2, step 11599/23838 completed (loss: 0.08339356631040573, acc: 0.9763033390045166)
[2025-02-16 12:54:20,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:21,239][root][INFO] - Training Epoch: 1/2, step 11600/23838 completed (loss: 0.3106936514377594, acc: 0.9223300814628601)
[2025-02-16 12:54:21,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:21,658][root][INFO] - Training Epoch: 1/2, step 11601/23838 completed (loss: 0.5533631443977356, acc: 0.8421052694320679)
[2025-02-16 12:54:21,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:22,161][root][INFO] - Training Epoch: 1/2, step 11602/23838 completed (loss: 0.2009403556585312, acc: 0.9583333134651184)
[2025-02-16 12:54:22,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:22,570][root][INFO] - Training Epoch: 1/2, step 11603/23838 completed (loss: 0.2545680105686188, acc: 0.9391891956329346)
[2025-02-16 12:54:22,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:22,954][root][INFO] - Training Epoch: 1/2, step 11604/23838 completed (loss: 0.18426081538200378, acc: 0.9490445852279663)
[2025-02-16 12:54:23,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:23,362][root][INFO] - Training Epoch: 1/2, step 11605/23838 completed (loss: 0.16726522147655487, acc: 0.949999988079071)
[2025-02-16 12:54:23,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:23,787][root][INFO] - Training Epoch: 1/2, step 11606/23838 completed (loss: 0.1798948496580124, acc: 0.9383561611175537)
[2025-02-16 12:54:23,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:24,196][root][INFO] - Training Epoch: 1/2, step 11607/23838 completed (loss: 0.32749995589256287, acc: 0.9090909361839294)
[2025-02-16 12:54:24,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:24,664][root][INFO] - Training Epoch: 1/2, step 11608/23838 completed (loss: 0.18208596110343933, acc: 0.9638554453849792)
[2025-02-16 12:54:24,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:25,107][root][INFO] - Training Epoch: 1/2, step 11609/23838 completed (loss: 0.29792818427085876, acc: 0.9103448390960693)
[2025-02-16 12:54:25,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:25,510][root][INFO] - Training Epoch: 1/2, step 11610/23838 completed (loss: 0.4039890468120575, acc: 0.8778625726699829)
[2025-02-16 12:54:25,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:25,887][root][INFO] - Training Epoch: 1/2, step 11611/23838 completed (loss: 0.08307801187038422, acc: 0.9803921580314636)
[2025-02-16 12:54:26,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:26,291][root][INFO] - Training Epoch: 1/2, step 11612/23838 completed (loss: 0.28666600584983826, acc: 0.9368420839309692)
[2025-02-16 12:54:26,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:26,716][root][INFO] - Training Epoch: 1/2, step 11613/23838 completed (loss: 0.04112465679645538, acc: 0.9933775067329407)
[2025-02-16 12:54:26,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:27,212][root][INFO] - Training Epoch: 1/2, step 11614/23838 completed (loss: 0.12453333288431168, acc: 0.9702380895614624)
[2025-02-16 12:54:27,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:28,123][root][INFO] - Training Epoch: 1/2, step 11615/23838 completed (loss: 0.08992214500904083, acc: 0.9770491719245911)
[2025-02-16 12:54:28,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:29,160][root][INFO] - Training Epoch: 1/2, step 11616/23838 completed (loss: 0.1690519005060196, acc: 0.960606038570404)
[2025-02-16 12:54:29,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:29,728][root][INFO] - Training Epoch: 1/2, step 11617/23838 completed (loss: 0.36821693181991577, acc: 0.8942307829856873)
[2025-02-16 12:54:29,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:30,173][root][INFO] - Training Epoch: 1/2, step 11618/23838 completed (loss: 0.18067492544651031, acc: 0.9507042169570923)
[2025-02-16 12:54:30,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:30,679][root][INFO] - Training Epoch: 1/2, step 11619/23838 completed (loss: 0.16179470717906952, acc: 0.9659574627876282)
[2025-02-16 12:54:30,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:31,197][root][INFO] - Training Epoch: 1/2, step 11620/23838 completed (loss: 0.29033157229423523, acc: 0.8776978254318237)
[2025-02-16 12:54:31,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:31,705][root][INFO] - Training Epoch: 1/2, step 11621/23838 completed (loss: 0.08913208544254303, acc: 0.9720279574394226)
[2025-02-16 12:54:31,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:32,170][root][INFO] - Training Epoch: 1/2, step 11622/23838 completed (loss: 0.11629467457532883, acc: 0.9615384340286255)
[2025-02-16 12:54:32,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:32,596][root][INFO] - Training Epoch: 1/2, step 11623/23838 completed (loss: 0.2952452301979065, acc: 0.918749988079071)
[2025-02-16 12:54:32,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:33,152][root][INFO] - Training Epoch: 1/2, step 11624/23838 completed (loss: 0.28407561779022217, acc: 0.9435028433799744)
[2025-02-16 12:54:33,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:33,616][root][INFO] - Training Epoch: 1/2, step 11625/23838 completed (loss: 0.13288593292236328, acc: 0.9655172228813171)
[2025-02-16 12:54:33,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:34,020][root][INFO] - Training Epoch: 1/2, step 11626/23838 completed (loss: 1.1319825649261475, acc: 0.7118644118309021)
[2025-02-16 12:54:34,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:34,509][root][INFO] - Training Epoch: 1/2, step 11627/23838 completed (loss: 0.2873360514640808, acc: 0.9272727370262146)
[2025-02-16 12:54:34,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:34,994][root][INFO] - Training Epoch: 1/2, step 11628/23838 completed (loss: 0.2532764673233032, acc: 0.9155844449996948)
[2025-02-16 12:54:35,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:35,693][root][INFO] - Training Epoch: 1/2, step 11629/23838 completed (loss: 0.22481916844844818, acc: 0.949367105960846)
[2025-02-16 12:54:35,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:36,268][root][INFO] - Training Epoch: 1/2, step 11630/23838 completed (loss: 0.2080214023590088, acc: 0.9508196711540222)
[2025-02-16 12:54:36,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:36,750][root][INFO] - Training Epoch: 1/2, step 11631/23838 completed (loss: 0.2846727967262268, acc: 0.9280575513839722)
[2025-02-16 12:54:36,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:37,177][root][INFO] - Training Epoch: 1/2, step 11632/23838 completed (loss: 0.30807173252105713, acc: 0.9271523356437683)
[2025-02-16 12:54:37,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:37,730][root][INFO] - Training Epoch: 1/2, step 11633/23838 completed (loss: 0.33252274990081787, acc: 0.9126213788986206)
[2025-02-16 12:54:37,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:38,230][root][INFO] - Training Epoch: 1/2, step 11634/23838 completed (loss: 0.24089138209819794, acc: 0.938144326210022)
[2025-02-16 12:54:38,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:38,633][root][INFO] - Training Epoch: 1/2, step 11635/23838 completed (loss: 0.2317243069410324, acc: 0.9251700639724731)
[2025-02-16 12:54:38,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:39,094][root][INFO] - Training Epoch: 1/2, step 11636/23838 completed (loss: 0.2573288381099701, acc: 0.9285714030265808)
[2025-02-16 12:54:39,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:39,511][root][INFO] - Training Epoch: 1/2, step 11637/23838 completed (loss: 0.2930736541748047, acc: 0.9107142686843872)
[2025-02-16 12:54:39,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:40,016][root][INFO] - Training Epoch: 1/2, step 11638/23838 completed (loss: 0.10022714734077454, acc: 0.967391312122345)
[2025-02-16 12:54:40,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:40,923][root][INFO] - Training Epoch: 1/2, step 11639/23838 completed (loss: 0.1967220902442932, acc: 0.94017094373703)
[2025-02-16 12:54:41,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:41,376][root][INFO] - Training Epoch: 1/2, step 11640/23838 completed (loss: 0.10705671459436417, acc: 0.9659863710403442)
[2025-02-16 12:54:41,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:41,790][root][INFO] - Training Epoch: 1/2, step 11641/23838 completed (loss: 0.43559566140174866, acc: 0.8727272748947144)
[2025-02-16 12:54:41,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:42,227][root][INFO] - Training Epoch: 1/2, step 11642/23838 completed (loss: 0.11504556238651276, acc: 0.9819276928901672)
[2025-02-16 12:54:42,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:42,660][root][INFO] - Training Epoch: 1/2, step 11643/23838 completed (loss: 0.1979294717311859, acc: 0.9602649211883545)
[2025-02-16 12:54:42,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:43,205][root][INFO] - Training Epoch: 1/2, step 11644/23838 completed (loss: 0.2901307940483093, acc: 0.9285714030265808)
[2025-02-16 12:54:43,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:43,739][root][INFO] - Training Epoch: 1/2, step 11645/23838 completed (loss: 0.04802718386054039, acc: 0.9920318722724915)
[2025-02-16 12:54:44,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:44,318][root][INFO] - Training Epoch: 1/2, step 11646/23838 completed (loss: 0.26719337701797485, acc: 0.934959352016449)
[2025-02-16 12:54:44,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:44,771][root][INFO] - Training Epoch: 1/2, step 11647/23838 completed (loss: 0.16411416232585907, acc: 0.9551281929016113)
[2025-02-16 12:54:44,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:45,156][root][INFO] - Training Epoch: 1/2, step 11648/23838 completed (loss: 0.16610071063041687, acc: 0.9512194991111755)
[2025-02-16 12:54:45,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:45,586][root][INFO] - Training Epoch: 1/2, step 11649/23838 completed (loss: 0.15403521060943604, acc: 0.9543378949165344)
[2025-02-16 12:54:45,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:46,000][root][INFO] - Training Epoch: 1/2, step 11650/23838 completed (loss: 0.17424827814102173, acc: 0.9534883499145508)
[2025-02-16 12:54:46,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:46,427][root][INFO] - Training Epoch: 1/2, step 11651/23838 completed (loss: 0.2492905557155609, acc: 0.9420289993286133)
[2025-02-16 12:54:46,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:46,906][root][INFO] - Training Epoch: 1/2, step 11652/23838 completed (loss: 0.2849988341331482, acc: 0.9166666865348816)
[2025-02-16 12:54:47,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:47,341][root][INFO] - Training Epoch: 1/2, step 11653/23838 completed (loss: 0.15792378783226013, acc: 0.9523809552192688)
[2025-02-16 12:54:47,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:47,732][root][INFO] - Training Epoch: 1/2, step 11654/23838 completed (loss: 0.3812118470668793, acc: 0.8999999761581421)
[2025-02-16 12:54:47,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:48,186][root][INFO] - Training Epoch: 1/2, step 11655/23838 completed (loss: 0.10932735353708267, acc: 0.9642857313156128)
[2025-02-16 12:54:48,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:48,821][root][INFO] - Training Epoch: 1/2, step 11656/23838 completed (loss: 0.16389741003513336, acc: 0.953125)
[2025-02-16 12:54:49,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:49,294][root][INFO] - Training Epoch: 1/2, step 11657/23838 completed (loss: 0.1307653933763504, acc: 0.9563318490982056)
[2025-02-16 12:54:49,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:49,686][root][INFO] - Training Epoch: 1/2, step 11658/23838 completed (loss: 0.1431213617324829, acc: 0.9765625)
[2025-02-16 12:54:49,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:50,132][root][INFO] - Training Epoch: 1/2, step 11659/23838 completed (loss: 0.3181507885456085, acc: 0.8846153616905212)
[2025-02-16 12:54:50,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:50,640][root][INFO] - Training Epoch: 1/2, step 11660/23838 completed (loss: 0.3219786286354065, acc: 0.9029850959777832)
[2025-02-16 12:54:50,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:51,131][root][INFO] - Training Epoch: 1/2, step 11661/23838 completed (loss: 0.1631917804479599, acc: 0.9553571343421936)
[2025-02-16 12:54:51,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:51,520][root][INFO] - Training Epoch: 1/2, step 11662/23838 completed (loss: 0.12458486109972, acc: 0.9688888788223267)
[2025-02-16 12:54:51,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:52,055][root][INFO] - Training Epoch: 1/2, step 11663/23838 completed (loss: 0.1663173884153366, acc: 0.9420289993286133)
[2025-02-16 12:54:52,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:52,541][root][INFO] - Training Epoch: 1/2, step 11664/23838 completed (loss: 0.15286816656589508, acc: 0.9479166865348816)
[2025-02-16 12:54:52,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:52,955][root][INFO] - Training Epoch: 1/2, step 11665/23838 completed (loss: 0.4609283208847046, acc: 0.8441558480262756)
[2025-02-16 12:54:53,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:53,385][root][INFO] - Training Epoch: 1/2, step 11666/23838 completed (loss: 0.11052326112985611, acc: 0.9756097793579102)
[2025-02-16 12:54:53,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:53,865][root][INFO] - Training Epoch: 1/2, step 11667/23838 completed (loss: 0.08641310036182404, acc: 0.976047933101654)
[2025-02-16 12:54:54,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:54,335][root][INFO] - Training Epoch: 1/2, step 11668/23838 completed (loss: 0.3624028265476227, acc: 0.90625)
[2025-02-16 12:54:54,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:54,863][root][INFO] - Training Epoch: 1/2, step 11669/23838 completed (loss: 0.19816677272319794, acc: 0.9390243887901306)
[2025-02-16 12:54:55,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:55,295][root][INFO] - Training Epoch: 1/2, step 11670/23838 completed (loss: 0.7417392134666443, acc: 0.8152173757553101)
[2025-02-16 12:54:55,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:55,843][root][INFO] - Training Epoch: 1/2, step 11671/23838 completed (loss: 0.13022229075431824, acc: 0.967391312122345)
[2025-02-16 12:54:56,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:56,392][root][INFO] - Training Epoch: 1/2, step 11672/23838 completed (loss: 0.16052699089050293, acc: 0.949999988079071)
[2025-02-16 12:54:56,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:56,862][root][INFO] - Training Epoch: 1/2, step 11673/23838 completed (loss: 0.1389068365097046, acc: 0.9681528806686401)
[2025-02-16 12:54:57,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:57,286][root][INFO] - Training Epoch: 1/2, step 11674/23838 completed (loss: 0.3640463650226593, acc: 0.8965517282485962)
[2025-02-16 12:54:57,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:57,701][root][INFO] - Training Epoch: 1/2, step 11675/23838 completed (loss: 0.11952731013298035, acc: 0.9685039520263672)
[2025-02-16 12:54:57,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:58,225][root][INFO] - Training Epoch: 1/2, step 11676/23838 completed (loss: 0.16068701446056366, acc: 0.9504504799842834)
[2025-02-16 12:54:58,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:58,863][root][INFO] - Training Epoch: 1/2, step 11677/23838 completed (loss: 0.10812021046876907, acc: 0.9647887349128723)
[2025-02-16 12:54:59,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:59,368][root][INFO] - Training Epoch: 1/2, step 11678/23838 completed (loss: 0.282036691904068, acc: 0.918552041053772)
[2025-02-16 12:54:59,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:54:59,785][root][INFO] - Training Epoch: 1/2, step 11679/23838 completed (loss: 0.5985763072967529, acc: 0.7866666913032532)
[2025-02-16 12:54:59,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:00,247][root][INFO] - Training Epoch: 1/2, step 11680/23838 completed (loss: 0.26828092336654663, acc: 0.9489796161651611)
[2025-02-16 12:55:00,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:00,676][root][INFO] - Training Epoch: 1/2, step 11681/23838 completed (loss: 0.10621314495801926, acc: 0.9727272987365723)
[2025-02-16 12:55:00,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:01,167][root][INFO] - Training Epoch: 1/2, step 11682/23838 completed (loss: 0.07484836876392365, acc: 0.9750000238418579)
[2025-02-16 12:55:01,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:01,639][root][INFO] - Training Epoch: 1/2, step 11683/23838 completed (loss: 0.5540869235992432, acc: 0.8203125)
[2025-02-16 12:55:01,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:02,031][root][INFO] - Training Epoch: 1/2, step 11684/23838 completed (loss: 0.7392531633377075, acc: 0.7849462628364563)
[2025-02-16 12:55:02,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:02,465][root][INFO] - Training Epoch: 1/2, step 11685/23838 completed (loss: 0.8670835494995117, acc: 0.8235294222831726)
[2025-02-16 12:55:02,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:02,931][root][INFO] - Training Epoch: 1/2, step 11686/23838 completed (loss: 0.6807460784912109, acc: 0.7638888955116272)
[2025-02-16 12:55:03,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:03,353][root][INFO] - Training Epoch: 1/2, step 11687/23838 completed (loss: 0.6419395804405212, acc: 0.8067227005958557)
[2025-02-16 12:55:03,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:03,786][root][INFO] - Training Epoch: 1/2, step 11688/23838 completed (loss: 0.5116139650344849, acc: 0.8571428656578064)
[2025-02-16 12:55:04,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:04,264][root][INFO] - Training Epoch: 1/2, step 11689/23838 completed (loss: 0.9549740552902222, acc: 0.7200000286102295)
[2025-02-16 12:55:04,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:04,746][root][INFO] - Training Epoch: 1/2, step 11690/23838 completed (loss: 0.48081904649734497, acc: 0.8367347121238708)
[2025-02-16 12:55:04,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:05,223][root][INFO] - Training Epoch: 1/2, step 11691/23838 completed (loss: 0.41797199845314026, acc: 0.8695651888847351)
[2025-02-16 12:55:05,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:05,682][root][INFO] - Training Epoch: 1/2, step 11692/23838 completed (loss: 0.5135185718536377, acc: 0.8548387289047241)
[2025-02-16 12:55:05,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:06,148][root][INFO] - Training Epoch: 1/2, step 11693/23838 completed (loss: 0.3481791615486145, acc: 0.8976377844810486)
[2025-02-16 12:55:06,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:06,579][root][INFO] - Training Epoch: 1/2, step 11694/23838 completed (loss: 0.2575482726097107, acc: 0.910614550113678)
[2025-02-16 12:55:06,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:06,960][root][INFO] - Training Epoch: 1/2, step 11695/23838 completed (loss: 0.27818745374679565, acc: 0.9008264541625977)
[2025-02-16 12:55:07,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:07,328][root][INFO] - Training Epoch: 1/2, step 11696/23838 completed (loss: 0.6810705661773682, acc: 0.8141592741012573)
[2025-02-16 12:55:07,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:07,814][root][INFO] - Training Epoch: 1/2, step 11697/23838 completed (loss: 0.6961004734039307, acc: 0.7882353067398071)
[2025-02-16 12:55:08,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:08,237][root][INFO] - Training Epoch: 1/2, step 11698/23838 completed (loss: 0.49804723262786865, acc: 0.8767123222351074)
[2025-02-16 12:55:08,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:08,746][root][INFO] - Training Epoch: 1/2, step 11699/23838 completed (loss: 0.4338226020336151, acc: 0.8361582159996033)
[2025-02-16 12:55:08,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:09,224][root][INFO] - Training Epoch: 1/2, step 11700/23838 completed (loss: 0.6377133131027222, acc: 0.8275862336158752)
[2025-02-16 12:55:09,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:09,680][root][INFO] - Training Epoch: 1/2, step 11701/23838 completed (loss: 0.5207667946815491, acc: 0.8560606241226196)
[2025-02-16 12:55:09,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:10,135][root][INFO] - Training Epoch: 1/2, step 11702/23838 completed (loss: 0.27441221475601196, acc: 0.9202898740768433)
[2025-02-16 12:55:10,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:10,547][root][INFO] - Training Epoch: 1/2, step 11703/23838 completed (loss: 0.35347992181777954, acc: 0.8795180916786194)
[2025-02-16 12:55:10,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:11,003][root][INFO] - Training Epoch: 1/2, step 11704/23838 completed (loss: 0.6539818048477173, acc: 0.8139534592628479)
[2025-02-16 12:55:11,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:11,477][root][INFO] - Training Epoch: 1/2, step 11705/23838 completed (loss: 0.32943961024284363, acc: 0.875)
[2025-02-16 12:55:11,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:11,930][root][INFO] - Training Epoch: 1/2, step 11706/23838 completed (loss: 0.7084813714027405, acc: 0.8064516186714172)
[2025-02-16 12:55:12,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:12,318][root][INFO] - Training Epoch: 1/2, step 11707/23838 completed (loss: 0.2952897846698761, acc: 0.890625)
[2025-02-16 12:55:12,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:12,711][root][INFO] - Training Epoch: 1/2, step 11708/23838 completed (loss: 0.44092506170272827, acc: 0.9020978808403015)
[2025-02-16 12:55:12,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:13,103][root][INFO] - Training Epoch: 1/2, step 11709/23838 completed (loss: 0.5673516392707825, acc: 0.8518518805503845)
[2025-02-16 12:55:13,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:13,542][root][INFO] - Training Epoch: 1/2, step 11710/23838 completed (loss: 0.4008476436138153, acc: 0.8940397500991821)
[2025-02-16 12:55:13,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:13,976][root][INFO] - Training Epoch: 1/2, step 11711/23838 completed (loss: 0.23889140784740448, acc: 0.9370078444480896)
[2025-02-16 12:55:14,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:14,457][root][INFO] - Training Epoch: 1/2, step 11712/23838 completed (loss: 0.3483206331729889, acc: 0.8962264060974121)
[2025-02-16 12:55:14,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:14,915][root][INFO] - Training Epoch: 1/2, step 11713/23838 completed (loss: 0.4127068519592285, acc: 0.8818181753158569)
[2025-02-16 12:55:15,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:15,389][root][INFO] - Training Epoch: 1/2, step 11714/23838 completed (loss: 0.4685298800468445, acc: 0.874316930770874)
[2025-02-16 12:55:15,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:15,771][root][INFO] - Training Epoch: 1/2, step 11715/23838 completed (loss: 0.2635713517665863, acc: 0.9440000057220459)
[2025-02-16 12:55:15,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:16,167][root][INFO] - Training Epoch: 1/2, step 11716/23838 completed (loss: 0.6965935826301575, acc: 0.7833333611488342)
[2025-02-16 12:55:16,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:16,568][root][INFO] - Training Epoch: 1/2, step 11717/23838 completed (loss: 0.260788232088089, acc: 0.89570552110672)
[2025-02-16 12:55:16,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:17,034][root][INFO] - Training Epoch: 1/2, step 11718/23838 completed (loss: 0.31481674313545227, acc: 0.8867924809455872)
[2025-02-16 12:55:17,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:17,496][root][INFO] - Training Epoch: 1/2, step 11719/23838 completed (loss: 0.4213455021381378, acc: 0.8795180916786194)
[2025-02-16 12:55:17,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:17,981][root][INFO] - Training Epoch: 1/2, step 11720/23838 completed (loss: 0.5117557644844055, acc: 0.852173924446106)
[2025-02-16 12:55:18,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:18,409][root][INFO] - Training Epoch: 1/2, step 11721/23838 completed (loss: 0.4577205181121826, acc: 0.8909090757369995)
[2025-02-16 12:55:18,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:18,811][root][INFO] - Training Epoch: 1/2, step 11722/23838 completed (loss: 1.3114153146743774, acc: 0.6470588445663452)
[2025-02-16 12:55:18,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:19,211][root][INFO] - Training Epoch: 1/2, step 11723/23838 completed (loss: 0.356288343667984, acc: 0.8994975090026855)
[2025-02-16 12:55:19,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:19,621][root][INFO] - Training Epoch: 1/2, step 11724/23838 completed (loss: 0.7659184336662292, acc: 0.8181818127632141)
[2025-02-16 12:55:19,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:20,012][root][INFO] - Training Epoch: 1/2, step 11725/23838 completed (loss: 0.5085916519165039, acc: 0.8333333134651184)
[2025-02-16 12:55:20,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:20,488][root][INFO] - Training Epoch: 1/2, step 11726/23838 completed (loss: 0.2519761025905609, acc: 0.9427083134651184)
[2025-02-16 12:55:20,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:20,871][root][INFO] - Training Epoch: 1/2, step 11727/23838 completed (loss: 0.43136146664619446, acc: 0.8677685856819153)
[2025-02-16 12:55:21,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:21,262][root][INFO] - Training Epoch: 1/2, step 11728/23838 completed (loss: 0.5413274168968201, acc: 0.8776978254318237)
[2025-02-16 12:55:21,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:21,691][root][INFO] - Training Epoch: 1/2, step 11729/23838 completed (loss: 0.41036149859428406, acc: 0.8628571629524231)
[2025-02-16 12:55:21,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:22,131][root][INFO] - Training Epoch: 1/2, step 11730/23838 completed (loss: 0.8213330507278442, acc: 0.75)
[2025-02-16 12:55:22,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:22,579][root][INFO] - Training Epoch: 1/2, step 11731/23838 completed (loss: 0.8534970283508301, acc: 0.732758641242981)
[2025-02-16 12:55:22,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:23,078][root][INFO] - Training Epoch: 1/2, step 11732/23838 completed (loss: 0.38855308294296265, acc: 0.8814433217048645)
[2025-02-16 12:55:23,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:23,496][root][INFO] - Training Epoch: 1/2, step 11733/23838 completed (loss: 0.7309768795967102, acc: 0.796875)
[2025-02-16 12:55:23,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:23,875][root][INFO] - Training Epoch: 1/2, step 11734/23838 completed (loss: 1.068052053451538, acc: 0.7164179086685181)
[2025-02-16 12:55:24,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:24,313][root][INFO] - Training Epoch: 1/2, step 11735/23838 completed (loss: 0.5169467329978943, acc: 0.8372092843055725)
[2025-02-16 12:55:24,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:24,780][root][INFO] - Training Epoch: 1/2, step 11736/23838 completed (loss: 0.8356043696403503, acc: 0.7808219194412231)
[2025-02-16 12:55:25,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:25,247][root][INFO] - Training Epoch: 1/2, step 11737/23838 completed (loss: 0.39563629031181335, acc: 0.8925619721412659)
[2025-02-16 12:55:25,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:25,729][root][INFO] - Training Epoch: 1/2, step 11738/23838 completed (loss: 0.280765175819397, acc: 0.9285714030265808)
[2025-02-16 12:55:25,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:26,133][root][INFO] - Training Epoch: 1/2, step 11739/23838 completed (loss: 0.34778743982315063, acc: 0.8809523582458496)
[2025-02-16 12:55:26,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:26,491][root][INFO] - Training Epoch: 1/2, step 11740/23838 completed (loss: 0.341802179813385, acc: 0.8888888955116272)
[2025-02-16 12:55:26,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:26,879][root][INFO] - Training Epoch: 1/2, step 11741/23838 completed (loss: 0.7004827260971069, acc: 0.8349514603614807)
[2025-02-16 12:55:27,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:27,318][root][INFO] - Training Epoch: 1/2, step 11742/23838 completed (loss: 0.6201441884040833, acc: 0.8170731663703918)
[2025-02-16 12:55:27,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:27,718][root][INFO] - Training Epoch: 1/2, step 11743/23838 completed (loss: 0.39629706740379333, acc: 0.8888888955116272)
[2025-02-16 12:55:27,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:28,197][root][INFO] - Training Epoch: 1/2, step 11744/23838 completed (loss: 0.9426114559173584, acc: 0.7605633735656738)
[2025-02-16 12:55:28,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:28,684][root][INFO] - Training Epoch: 1/2, step 11745/23838 completed (loss: 0.7649704813957214, acc: 0.7910447716712952)
[2025-02-16 12:55:28,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:29,126][root][INFO] - Training Epoch: 1/2, step 11746/23838 completed (loss: 0.4444889426231384, acc: 0.895652174949646)
[2025-02-16 12:55:29,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:29,581][root][INFO] - Training Epoch: 1/2, step 11747/23838 completed (loss: 0.353049099445343, acc: 0.8921568393707275)
[2025-02-16 12:55:29,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:30,019][root][INFO] - Training Epoch: 1/2, step 11748/23838 completed (loss: 0.20255227386951447, acc: 0.947826087474823)
[2025-02-16 12:55:30,213][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:30,439][root][INFO] - Training Epoch: 1/2, step 11749/23838 completed (loss: 0.3356363773345947, acc: 0.9097222089767456)
[2025-02-16 12:55:30,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:30,862][root][INFO] - Training Epoch: 1/2, step 11750/23838 completed (loss: 0.5942312479019165, acc: 0.8163265585899353)
[2025-02-16 12:55:31,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:31,293][root][INFO] - Training Epoch: 1/2, step 11751/23838 completed (loss: 0.6315076947212219, acc: 0.8322981595993042)
[2025-02-16 12:55:31,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:31,746][root][INFO] - Training Epoch: 1/2, step 11752/23838 completed (loss: 0.3836667537689209, acc: 0.897777795791626)
[2025-02-16 12:55:31,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:32,177][root][INFO] - Training Epoch: 1/2, step 11753/23838 completed (loss: 0.5175048112869263, acc: 0.8426966071128845)
[2025-02-16 12:55:32,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:32,566][root][INFO] - Training Epoch: 1/2, step 11754/23838 completed (loss: 0.39448457956314087, acc: 0.8691588640213013)
[2025-02-16 12:55:32,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:33,012][root][INFO] - Training Epoch: 1/2, step 11755/23838 completed (loss: 0.30489060282707214, acc: 0.9166666865348816)
[2025-02-16 12:55:33,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:33,481][root][INFO] - Training Epoch: 1/2, step 11756/23838 completed (loss: 0.5716043710708618, acc: 0.8644067645072937)
[2025-02-16 12:55:33,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:33,929][root][INFO] - Training Epoch: 1/2, step 11757/23838 completed (loss: 0.2886890769004822, acc: 0.8989899158477783)
[2025-02-16 12:55:34,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:34,386][root][INFO] - Training Epoch: 1/2, step 11758/23838 completed (loss: 0.30373048782348633, acc: 0.9120879173278809)
[2025-02-16 12:55:34,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:34,840][root][INFO] - Training Epoch: 1/2, step 11759/23838 completed (loss: 0.7132681012153625, acc: 0.8061224222183228)
[2025-02-16 12:55:34,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:35,205][root][INFO] - Training Epoch: 1/2, step 11760/23838 completed (loss: 0.5155376195907593, acc: 0.8454545736312866)
[2025-02-16 12:55:35,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:35,619][root][INFO] - Training Epoch: 1/2, step 11761/23838 completed (loss: 0.3402705490589142, acc: 0.8939393758773804)
[2025-02-16 12:55:35,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:36,052][root][INFO] - Training Epoch: 1/2, step 11762/23838 completed (loss: 0.36532920598983765, acc: 0.9194630980491638)
[2025-02-16 12:55:36,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:36,520][root][INFO] - Training Epoch: 1/2, step 11763/23838 completed (loss: 0.471752792596817, acc: 0.8711656332015991)
[2025-02-16 12:55:36,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:36,950][root][INFO] - Training Epoch: 1/2, step 11764/23838 completed (loss: 0.23898375034332275, acc: 0.9277777671813965)
[2025-02-16 12:55:37,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:37,436][root][INFO] - Training Epoch: 1/2, step 11765/23838 completed (loss: 0.6839923858642578, acc: 0.8266666531562805)
[2025-02-16 12:55:37,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:37,905][root][INFO] - Training Epoch: 1/2, step 11766/23838 completed (loss: 0.30173778533935547, acc: 0.9273356199264526)
[2025-02-16 12:55:38,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:38,411][root][INFO] - Training Epoch: 1/2, step 11767/23838 completed (loss: 0.505845308303833, acc: 0.8783270120620728)
[2025-02-16 12:55:38,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:38,851][root][INFO] - Training Epoch: 1/2, step 11768/23838 completed (loss: 0.5994561910629272, acc: 0.8314606547355652)
[2025-02-16 12:55:39,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:39,280][root][INFO] - Training Epoch: 1/2, step 11769/23838 completed (loss: 0.3099241852760315, acc: 0.8844221234321594)
[2025-02-16 12:55:39,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:39,752][root][INFO] - Training Epoch: 1/2, step 11770/23838 completed (loss: 0.342885822057724, acc: 0.9142857193946838)
[2025-02-16 12:55:39,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:40,232][root][INFO] - Training Epoch: 1/2, step 11771/23838 completed (loss: 0.647993803024292, acc: 0.7824074029922485)
[2025-02-16 12:55:40,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:40,743][root][INFO] - Training Epoch: 1/2, step 11772/23838 completed (loss: 0.41001853346824646, acc: 0.887499988079071)
[2025-02-16 12:55:40,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:41,211][root][INFO] - Training Epoch: 1/2, step 11773/23838 completed (loss: 0.2933759093284607, acc: 0.89552241563797)
[2025-02-16 12:55:41,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:41,617][root][INFO] - Training Epoch: 1/2, step 11774/23838 completed (loss: 0.5529020428657532, acc: 0.8191489577293396)
[2025-02-16 12:55:41,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:42,030][root][INFO] - Training Epoch: 1/2, step 11775/23838 completed (loss: 0.5471943020820618, acc: 0.8674699068069458)
[2025-02-16 12:55:42,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:42,519][root][INFO] - Training Epoch: 1/2, step 11776/23838 completed (loss: 0.25889551639556885, acc: 0.888059675693512)
[2025-02-16 12:55:42,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:43,012][root][INFO] - Training Epoch: 1/2, step 11777/23838 completed (loss: 0.23117107152938843, acc: 0.9208633303642273)
[2025-02-16 12:55:43,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:43,495][root][INFO] - Training Epoch: 1/2, step 11778/23838 completed (loss: 0.26655593514442444, acc: 0.9289617538452148)
[2025-02-16 12:55:43,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:43,909][root][INFO] - Training Epoch: 1/2, step 11779/23838 completed (loss: 0.43404731154441833, acc: 0.8939393758773804)
[2025-02-16 12:55:44,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:44,384][root][INFO] - Training Epoch: 1/2, step 11780/23838 completed (loss: 0.09317520260810852, acc: 0.9635036587715149)
[2025-02-16 12:55:44,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:44,831][root][INFO] - Training Epoch: 1/2, step 11781/23838 completed (loss: 0.24055413901805878, acc: 0.924369752407074)
[2025-02-16 12:55:45,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:45,330][root][INFO] - Training Epoch: 1/2, step 11782/23838 completed (loss: 0.9632845520973206, acc: 0.7631579041481018)
[2025-02-16 12:55:45,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:45,779][root][INFO] - Training Epoch: 1/2, step 11783/23838 completed (loss: 0.19936718046665192, acc: 0.9490445852279663)
[2025-02-16 12:55:46,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:46,255][root][INFO] - Training Epoch: 1/2, step 11784/23838 completed (loss: 0.141112819314003, acc: 0.9409090876579285)
[2025-02-16 12:55:46,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:46,743][root][INFO] - Training Epoch: 1/2, step 11785/23838 completed (loss: 0.2686677575111389, acc: 0.929648220539093)
[2025-02-16 12:55:46,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:47,177][root][INFO] - Training Epoch: 1/2, step 11786/23838 completed (loss: 0.5150454044342041, acc: 0.8627451062202454)
[2025-02-16 12:55:47,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:47,567][root][INFO] - Training Epoch: 1/2, step 11787/23838 completed (loss: 0.4079243540763855, acc: 0.8592592477798462)
[2025-02-16 12:55:47,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:48,010][root][INFO] - Training Epoch: 1/2, step 11788/23838 completed (loss: 0.27374520897865295, acc: 0.9193548560142517)
[2025-02-16 12:55:48,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:48,372][root][INFO] - Training Epoch: 1/2, step 11789/23838 completed (loss: 0.35894694924354553, acc: 0.8833333253860474)
[2025-02-16 12:55:48,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:48,763][root][INFO] - Training Epoch: 1/2, step 11790/23838 completed (loss: 0.44379839301109314, acc: 0.85628741979599)
[2025-02-16 12:55:48,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:49,232][root][INFO] - Training Epoch: 1/2, step 11791/23838 completed (loss: 0.35592207312583923, acc: 0.90625)
[2025-02-16 12:55:49,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:49,668][root][INFO] - Training Epoch: 1/2, step 11792/23838 completed (loss: 0.4751545488834381, acc: 0.8715083599090576)
[2025-02-16 12:55:49,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:50,039][root][INFO] - Training Epoch: 1/2, step 11793/23838 completed (loss: 0.45779526233673096, acc: 0.8494623899459839)
[2025-02-16 12:55:50,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:50,425][root][INFO] - Training Epoch: 1/2, step 11794/23838 completed (loss: 0.5634490251541138, acc: 0.824999988079071)
[2025-02-16 12:55:50,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:50,866][root][INFO] - Training Epoch: 1/2, step 11795/23838 completed (loss: 0.30705180764198303, acc: 0.9098360538482666)
[2025-02-16 12:55:51,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:51,378][root][INFO] - Training Epoch: 1/2, step 11796/23838 completed (loss: 0.3140362501144409, acc: 0.9126213788986206)
[2025-02-16 12:55:51,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:51,790][root][INFO] - Training Epoch: 1/2, step 11797/23838 completed (loss: 0.4414084255695343, acc: 0.8725489974021912)
[2025-02-16 12:55:52,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:52,248][root][INFO] - Training Epoch: 1/2, step 11798/23838 completed (loss: 0.3856131136417389, acc: 0.8857142925262451)
[2025-02-16 12:55:52,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:52,644][root][INFO] - Training Epoch: 1/2, step 11799/23838 completed (loss: 0.5185754895210266, acc: 0.8148148059844971)
[2025-02-16 12:55:52,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:53,089][root][INFO] - Training Epoch: 1/2, step 11800/23838 completed (loss: 0.8672622442245483, acc: 0.7534246444702148)
[2025-02-16 12:55:53,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:53,535][root][INFO] - Training Epoch: 1/2, step 11801/23838 completed (loss: 1.1652363538742065, acc: 0.6724137663841248)
[2025-02-16 12:55:53,750][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:53,987][root][INFO] - Training Epoch: 1/2, step 11802/23838 completed (loss: 0.3146117329597473, acc: 0.918749988079071)
[2025-02-16 12:55:54,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:54,395][root][INFO] - Training Epoch: 1/2, step 11803/23838 completed (loss: 0.15289808809757233, acc: 0.9649122953414917)
[2025-02-16 12:55:54,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:54,867][root][INFO] - Training Epoch: 1/2, step 11804/23838 completed (loss: 0.7722740173339844, acc: 0.7744361162185669)
[2025-02-16 12:55:55,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:55,315][root][INFO] - Training Epoch: 1/2, step 11805/23838 completed (loss: 0.9237749576568604, acc: 0.7142857313156128)
[2025-02-16 12:55:55,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:55,755][root][INFO] - Training Epoch: 1/2, step 11806/23838 completed (loss: 0.7073176503181458, acc: 0.8239436745643616)
[2025-02-16 12:55:55,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:56,226][root][INFO] - Training Epoch: 1/2, step 11807/23838 completed (loss: 0.3163206875324249, acc: 0.9075342416763306)
[2025-02-16 12:55:56,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:56,646][root][INFO] - Training Epoch: 1/2, step 11808/23838 completed (loss: 0.6434418559074402, acc: 0.7861635088920593)
[2025-02-16 12:55:56,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:57,092][root][INFO] - Training Epoch: 1/2, step 11809/23838 completed (loss: 0.5254177451133728, acc: 0.8289473652839661)
[2025-02-16 12:55:57,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:57,545][root][INFO] - Training Epoch: 1/2, step 11810/23838 completed (loss: 1.0711463689804077, acc: 0.692307710647583)
[2025-02-16 12:55:57,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:58,019][root][INFO] - Training Epoch: 1/2, step 11811/23838 completed (loss: 0.47694069147109985, acc: 0.8691588640213013)
[2025-02-16 12:55:58,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:58,445][root][INFO] - Training Epoch: 1/2, step 11812/23838 completed (loss: 0.5643503665924072, acc: 0.8512820601463318)
[2025-02-16 12:55:58,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:58,917][root][INFO] - Training Epoch: 1/2, step 11813/23838 completed (loss: 0.561203122138977, acc: 0.7936508059501648)
[2025-02-16 12:55:59,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:59,341][root][INFO] - Training Epoch: 1/2, step 11814/23838 completed (loss: 0.5089021325111389, acc: 0.8525345325469971)
[2025-02-16 12:55:59,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:55:59,742][root][INFO] - Training Epoch: 1/2, step 11815/23838 completed (loss: 0.7997259497642517, acc: 0.8030303120613098)
[2025-02-16 12:55:59,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:00,142][root][INFO] - Training Epoch: 1/2, step 11816/23838 completed (loss: 0.9733738303184509, acc: 0.7383177280426025)
[2025-02-16 12:56:00,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:00,549][root][INFO] - Training Epoch: 1/2, step 11817/23838 completed (loss: 0.778799295425415, acc: 0.782608687877655)
[2025-02-16 12:56:00,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:00,969][root][INFO] - Training Epoch: 1/2, step 11818/23838 completed (loss: 0.7062591314315796, acc: 0.7865168452262878)
[2025-02-16 12:56:01,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:01,339][root][INFO] - Training Epoch: 1/2, step 11819/23838 completed (loss: 0.5937501788139343, acc: 0.8467742204666138)
[2025-02-16 12:56:01,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:01,748][root][INFO] - Training Epoch: 1/2, step 11820/23838 completed (loss: 0.36806103587150574, acc: 0.896774172782898)
[2025-02-16 12:56:01,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:02,234][root][INFO] - Training Epoch: 1/2, step 11821/23838 completed (loss: 0.5157106518745422, acc: 0.866310179233551)
[2025-02-16 12:56:02,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:02,646][root][INFO] - Training Epoch: 1/2, step 11822/23838 completed (loss: 0.9110540151596069, acc: 0.7272727489471436)
[2025-02-16 12:56:02,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:03,129][root][INFO] - Training Epoch: 1/2, step 11823/23838 completed (loss: 0.3967113792896271, acc: 0.9024389982223511)
[2025-02-16 12:56:03,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:03,589][root][INFO] - Training Epoch: 1/2, step 11824/23838 completed (loss: 0.397590309381485, acc: 0.8979591727256775)
[2025-02-16 12:56:03,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:03,999][root][INFO] - Training Epoch: 1/2, step 11825/23838 completed (loss: 0.181543231010437, acc: 0.9363636374473572)
[2025-02-16 12:56:04,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:04,562][root][INFO] - Training Epoch: 1/2, step 11826/23838 completed (loss: 0.5092328190803528, acc: 0.8767772316932678)
[2025-02-16 12:56:04,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:04,954][root][INFO] - Training Epoch: 1/2, step 11827/23838 completed (loss: 0.33076363801956177, acc: 0.9230769276618958)
[2025-02-16 12:56:05,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:05,335][root][INFO] - Training Epoch: 1/2, step 11828/23838 completed (loss: 0.5181663632392883, acc: 0.8099173307418823)
[2025-02-16 12:56:05,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:05,756][root][INFO] - Training Epoch: 1/2, step 11829/23838 completed (loss: 0.49506816267967224, acc: 0.8981481194496155)
[2025-02-16 12:56:05,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:06,183][root][INFO] - Training Epoch: 1/2, step 11830/23838 completed (loss: 0.23801273107528687, acc: 0.9528301954269409)
[2025-02-16 12:56:06,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:06,666][root][INFO] - Training Epoch: 1/2, step 11831/23838 completed (loss: 0.2737859785556793, acc: 0.9207921028137207)
[2025-02-16 12:56:06,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:07,108][root][INFO] - Training Epoch: 1/2, step 11832/23838 completed (loss: 0.3440406620502472, acc: 0.9207921028137207)
[2025-02-16 12:56:07,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:07,536][root][INFO] - Training Epoch: 1/2, step 11833/23838 completed (loss: 0.21725620329380035, acc: 0.9289617538452148)
[2025-02-16 12:56:07,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:07,983][root][INFO] - Training Epoch: 1/2, step 11834/23838 completed (loss: 0.6807366013526917, acc: 0.8207547068595886)
[2025-02-16 12:56:08,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:08,446][root][INFO] - Training Epoch: 1/2, step 11835/23838 completed (loss: 0.45674702525138855, acc: 0.8584070801734924)
[2025-02-16 12:56:08,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:08,912][root][INFO] - Training Epoch: 1/2, step 11836/23838 completed (loss: 0.36212262511253357, acc: 0.9236640930175781)
[2025-02-16 12:56:09,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:09,340][root][INFO] - Training Epoch: 1/2, step 11837/23838 completed (loss: 0.4811060130596161, acc: 0.8666666746139526)
[2025-02-16 12:56:09,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:09,856][root][INFO] - Training Epoch: 1/2, step 11838/23838 completed (loss: 0.25029003620147705, acc: 0.9200000166893005)
[2025-02-16 12:56:10,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:10,331][root][INFO] - Training Epoch: 1/2, step 11839/23838 completed (loss: 0.5735438466072083, acc: 0.8645161390304565)
[2025-02-16 12:56:10,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:11,394][root][INFO] - Training Epoch: 1/2, step 11840/23838 completed (loss: 0.46760040521621704, acc: 0.8756476640701294)
[2025-02-16 12:56:11,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:11,838][root][INFO] - Training Epoch: 1/2, step 11841/23838 completed (loss: 0.4580286741256714, acc: 0.895348846912384)
[2025-02-16 12:56:12,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:12,317][root][INFO] - Training Epoch: 1/2, step 11842/23838 completed (loss: 0.27172887325286865, acc: 0.936170220375061)
[2025-02-16 12:56:12,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:12,720][root][INFO] - Training Epoch: 1/2, step 11843/23838 completed (loss: 0.2953488528728485, acc: 0.932584285736084)
[2025-02-16 12:56:12,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:13,110][root][INFO] - Training Epoch: 1/2, step 11844/23838 completed (loss: 0.16406814754009247, acc: 0.9255319237709045)
[2025-02-16 12:56:13,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:13,589][root][INFO] - Training Epoch: 1/2, step 11845/23838 completed (loss: 0.3740858733654022, acc: 0.9105691313743591)
[2025-02-16 12:56:13,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:14,026][root][INFO] - Training Epoch: 1/2, step 11846/23838 completed (loss: 0.35889771580696106, acc: 0.9253731369972229)
[2025-02-16 12:56:14,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:14,394][root][INFO] - Training Epoch: 1/2, step 11847/23838 completed (loss: 0.26146480441093445, acc: 0.9051724076271057)
[2025-02-16 12:56:14,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:14,859][root][INFO] - Training Epoch: 1/2, step 11848/23838 completed (loss: 0.43495821952819824, acc: 0.8837209343910217)
[2025-02-16 12:56:15,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:15,274][root][INFO] - Training Epoch: 1/2, step 11849/23838 completed (loss: 0.30337220430374146, acc: 0.9036144614219666)
[2025-02-16 12:56:15,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:15,720][root][INFO] - Training Epoch: 1/2, step 11850/23838 completed (loss: 0.5110312104225159, acc: 0.8695651888847351)
[2025-02-16 12:56:15,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:16,164][root][INFO] - Training Epoch: 1/2, step 11851/23838 completed (loss: 0.9764034748077393, acc: 0.7551020383834839)
[2025-02-16 12:56:16,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:16,672][root][INFO] - Training Epoch: 1/2, step 11852/23838 completed (loss: 0.774508535861969, acc: 0.8045976758003235)
[2025-02-16 12:56:16,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:17,185][root][INFO] - Training Epoch: 1/2, step 11853/23838 completed (loss: 0.57015061378479, acc: 0.800000011920929)
[2025-02-16 12:56:17,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:18,337][root][INFO] - Training Epoch: 1/2, step 11854/23838 completed (loss: 0.31143444776535034, acc: 0.9033613204956055)
[2025-02-16 12:56:18,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:18,786][root][INFO] - Training Epoch: 1/2, step 11855/23838 completed (loss: 0.2038353681564331, acc: 0.9406779408454895)
[2025-02-16 12:56:19,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:19,335][root][INFO] - Training Epoch: 1/2, step 11856/23838 completed (loss: 0.2745969593524933, acc: 0.9485294222831726)
[2025-02-16 12:56:19,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:19,738][root][INFO] - Training Epoch: 1/2, step 11857/23838 completed (loss: 0.26348426938056946, acc: 0.9385964870452881)
[2025-02-16 12:56:19,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:20,126][root][INFO] - Training Epoch: 1/2, step 11858/23838 completed (loss: 0.25430354475975037, acc: 0.9189189076423645)
[2025-02-16 12:56:20,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:20,512][root][INFO] - Training Epoch: 1/2, step 11859/23838 completed (loss: 0.25897642970085144, acc: 0.9120879173278809)
[2025-02-16 12:56:20,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:20,932][root][INFO] - Training Epoch: 1/2, step 11860/23838 completed (loss: 0.3358057141304016, acc: 0.9166666865348816)
[2025-02-16 12:56:21,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:21,330][root][INFO] - Training Epoch: 1/2, step 11861/23838 completed (loss: 0.2150408774614334, acc: 0.9639639854431152)
[2025-02-16 12:56:21,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:21,792][root][INFO] - Training Epoch: 1/2, step 11862/23838 completed (loss: 0.1289685219526291, acc: 0.9595959782600403)
[2025-02-16 12:56:21,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:22,203][root][INFO] - Training Epoch: 1/2, step 11863/23838 completed (loss: 0.09966551512479782, acc: 0.9615384340286255)
[2025-02-16 12:56:22,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:22,658][root][INFO] - Training Epoch: 1/2, step 11864/23838 completed (loss: 0.38755902647972107, acc: 0.9459459185600281)
[2025-02-16 12:56:22,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:23,062][root][INFO] - Training Epoch: 1/2, step 11865/23838 completed (loss: 0.22496308386325836, acc: 0.9054054021835327)
[2025-02-16 12:56:23,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:23,432][root][INFO] - Training Epoch: 1/2, step 11866/23838 completed (loss: 0.29875829815864563, acc: 0.904347836971283)
[2025-02-16 12:56:23,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:23,839][root][INFO] - Training Epoch: 1/2, step 11867/23838 completed (loss: 0.6077376008033752, acc: 0.774193525314331)
[2025-02-16 12:56:24,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:24,292][root][INFO] - Training Epoch: 1/2, step 11868/23838 completed (loss: 0.44715332984924316, acc: 0.8382353186607361)
[2025-02-16 12:56:24,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:24,744][root][INFO] - Training Epoch: 1/2, step 11869/23838 completed (loss: 0.4373561143875122, acc: 0.9166666865348816)
[2025-02-16 12:56:24,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:25,201][root][INFO] - Training Epoch: 1/2, step 11870/23838 completed (loss: 0.10296200960874557, acc: 0.9534883499145508)
[2025-02-16 12:56:25,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:25,626][root][INFO] - Training Epoch: 1/2, step 11871/23838 completed (loss: 0.29884833097457886, acc: 0.9277108311653137)
[2025-02-16 12:56:25,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:26,062][root][INFO] - Training Epoch: 1/2, step 11872/23838 completed (loss: 0.40237128734588623, acc: 0.8970588445663452)
[2025-02-16 12:56:26,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:26,475][root][INFO] - Training Epoch: 1/2, step 11873/23838 completed (loss: 0.5479074716567993, acc: 0.875)
[2025-02-16 12:56:26,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:26,887][root][INFO] - Training Epoch: 1/2, step 11874/23838 completed (loss: 0.9775006175041199, acc: 0.75)
[2025-02-16 12:56:27,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:27,345][root][INFO] - Training Epoch: 1/2, step 11875/23838 completed (loss: 0.34950071573257446, acc: 0.9213483333587646)
[2025-02-16 12:56:27,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:27,760][root][INFO] - Training Epoch: 1/2, step 11876/23838 completed (loss: 0.35537344217300415, acc: 0.8571428656578064)
[2025-02-16 12:56:27,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:28,146][root][INFO] - Training Epoch: 1/2, step 11877/23838 completed (loss: 0.15173394978046417, acc: 0.9672130942344666)
[2025-02-16 12:56:28,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:28,514][root][INFO] - Training Epoch: 1/2, step 11878/23838 completed (loss: 0.2955099642276764, acc: 0.931034505367279)
[2025-02-16 12:56:28,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:28,895][root][INFO] - Training Epoch: 1/2, step 11879/23838 completed (loss: 0.35386571288108826, acc: 0.875)
[2025-02-16 12:56:29,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:29,347][root][INFO] - Training Epoch: 1/2, step 11880/23838 completed (loss: 0.5988556742668152, acc: 0.8421052694320679)
[2025-02-16 12:56:29,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:29,787][root][INFO] - Training Epoch: 1/2, step 11881/23838 completed (loss: 0.15635055303573608, acc: 0.9387755393981934)
[2025-02-16 12:56:29,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:30,199][root][INFO] - Training Epoch: 1/2, step 11882/23838 completed (loss: 0.47909560799598694, acc: 0.8644067645072937)
[2025-02-16 12:56:30,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:30,646][root][INFO] - Training Epoch: 1/2, step 11883/23838 completed (loss: 0.24293534457683563, acc: 0.9270833134651184)
[2025-02-16 12:56:30,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:31,162][root][INFO] - Training Epoch: 1/2, step 11884/23838 completed (loss: 0.34999382495880127, acc: 0.8888888955116272)
[2025-02-16 12:56:31,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:31,577][root][INFO] - Training Epoch: 1/2, step 11885/23838 completed (loss: 0.2705019414424896, acc: 0.9268292784690857)
[2025-02-16 12:56:31,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:31,996][root][INFO] - Training Epoch: 1/2, step 11886/23838 completed (loss: 0.49429577589035034, acc: 0.8588235378265381)
[2025-02-16 12:56:32,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:32,438][root][INFO] - Training Epoch: 1/2, step 11887/23838 completed (loss: 0.3800635039806366, acc: 0.9222221970558167)
[2025-02-16 12:56:32,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:32,906][root][INFO] - Training Epoch: 1/2, step 11888/23838 completed (loss: 0.3469877243041992, acc: 0.9090909361839294)
[2025-02-16 12:56:33,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:33,400][root][INFO] - Training Epoch: 1/2, step 11889/23838 completed (loss: 0.42166808247566223, acc: 0.8947368264198303)
[2025-02-16 12:56:33,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:33,852][root][INFO] - Training Epoch: 1/2, step 11890/23838 completed (loss: 0.5571919083595276, acc: 0.8651685118675232)
[2025-02-16 12:56:34,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:34,317][root][INFO] - Training Epoch: 1/2, step 11891/23838 completed (loss: 0.7169381380081177, acc: 0.8709677457809448)
[2025-02-16 12:56:34,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:34,711][root][INFO] - Training Epoch: 1/2, step 11892/23838 completed (loss: 0.31906068325042725, acc: 0.931034505367279)
[2025-02-16 12:56:34,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:35,103][root][INFO] - Training Epoch: 1/2, step 11893/23838 completed (loss: 0.47634392976760864, acc: 0.8805969953536987)
[2025-02-16 12:56:35,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:35,506][root][INFO] - Training Epoch: 1/2, step 11894/23838 completed (loss: 0.38499438762664795, acc: 0.8876404762268066)
[2025-02-16 12:56:35,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:35,965][root][INFO] - Training Epoch: 1/2, step 11895/23838 completed (loss: 0.2819330394268036, acc: 0.9075144529342651)
[2025-02-16 12:56:36,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:36,398][root][INFO] - Training Epoch: 1/2, step 11896/23838 completed (loss: 0.54338139295578, acc: 0.8676470518112183)
[2025-02-16 12:56:36,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:37,019][root][INFO] - Training Epoch: 1/2, step 11897/23838 completed (loss: 0.42046990990638733, acc: 0.8651162981987)
[2025-02-16 12:56:37,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:37,451][root][INFO] - Training Epoch: 1/2, step 11898/23838 completed (loss: 0.23303388059139252, acc: 0.9175257682800293)
[2025-02-16 12:56:37,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:37,801][root][INFO] - Training Epoch: 1/2, step 11899/23838 completed (loss: 0.6260655522346497, acc: 0.8500000238418579)
[2025-02-16 12:56:37,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:38,169][root][INFO] - Training Epoch: 1/2, step 11900/23838 completed (loss: 0.31243887543678284, acc: 0.9139785170555115)
[2025-02-16 12:56:38,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:38,623][root][INFO] - Training Epoch: 1/2, step 11901/23838 completed (loss: 0.37671947479248047, acc: 0.9090909361839294)
[2025-02-16 12:56:38,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:38,998][root][INFO] - Training Epoch: 1/2, step 11902/23838 completed (loss: 0.400058388710022, acc: 0.8737863898277283)
[2025-02-16 12:56:39,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:39,401][root][INFO] - Training Epoch: 1/2, step 11903/23838 completed (loss: 0.54581618309021, acc: 0.8518518805503845)
[2025-02-16 12:56:39,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:39,847][root][INFO] - Training Epoch: 1/2, step 11904/23838 completed (loss: 0.32426121830940247, acc: 0.9193548560142517)
[2025-02-16 12:56:40,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:40,298][root][INFO] - Training Epoch: 1/2, step 11905/23838 completed (loss: 0.3919123709201813, acc: 0.8846153616905212)
[2025-02-16 12:56:40,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:40,698][root][INFO] - Training Epoch: 1/2, step 11906/23838 completed (loss: 0.3874718248844147, acc: 0.8987341523170471)
[2025-02-16 12:56:40,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:41,112][root][INFO] - Training Epoch: 1/2, step 11907/23838 completed (loss: 0.3992151618003845, acc: 0.8999999761581421)
[2025-02-16 12:56:41,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:41,496][root][INFO] - Training Epoch: 1/2, step 11908/23838 completed (loss: 0.3053528368473053, acc: 0.9266055226325989)
[2025-02-16 12:56:41,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:41,943][root][INFO] - Training Epoch: 1/2, step 11909/23838 completed (loss: 0.5603744387626648, acc: 0.8775510191917419)
[2025-02-16 12:56:42,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:42,374][root][INFO] - Training Epoch: 1/2, step 11910/23838 completed (loss: 0.42725110054016113, acc: 0.8526315689086914)
[2025-02-16 12:56:42,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:42,823][root][INFO] - Training Epoch: 1/2, step 11911/23838 completed (loss: 0.13732492923736572, acc: 0.9545454382896423)
[2025-02-16 12:56:42,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:43,215][root][INFO] - Training Epoch: 1/2, step 11912/23838 completed (loss: 0.6124544143676758, acc: 0.7954545617103577)
[2025-02-16 12:56:43,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:43,645][root][INFO] - Training Epoch: 1/2, step 11913/23838 completed (loss: 0.2677938938140869, acc: 0.9266666769981384)
[2025-02-16 12:56:43,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:44,058][root][INFO] - Training Epoch: 1/2, step 11914/23838 completed (loss: 0.6175767183303833, acc: 0.8059701323509216)
[2025-02-16 12:56:44,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:44,490][root][INFO] - Training Epoch: 1/2, step 11915/23838 completed (loss: 0.29363593459129333, acc: 0.9180327653884888)
[2025-02-16 12:56:44,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:44,903][root][INFO] - Training Epoch: 1/2, step 11916/23838 completed (loss: 0.5784974694252014, acc: 0.8640776872634888)
[2025-02-16 12:56:45,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:45,353][root][INFO] - Training Epoch: 1/2, step 11917/23838 completed (loss: 0.20191577076911926, acc: 0.9512194991111755)
[2025-02-16 12:56:46,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:46,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:47,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:48,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:48,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:49,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:49,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:50,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:50,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:51,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:52,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:52,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:53,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:54,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:54,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:55,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:55,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:56,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:57,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:57,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:58,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:58,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:56:59,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:00,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:00,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:01,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:01,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:02,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:02,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:03,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:03,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:04,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:04,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:05,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:05,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:06,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:07,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:07,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:08,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:08,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:09,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:09,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:10,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:10,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:11,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:11,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:12,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:12,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:13,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:13,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:14,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:14,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:15,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:15,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:16,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:16,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:17,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:17,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:18,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:18,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:19,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:19,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:20,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:20,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:21,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:21,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:22,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:22,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:23,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:23,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:24,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:24,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:25,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:25,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:25,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:26,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:27,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:27,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:28,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:28,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:29,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:29,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:30,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:30,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:31,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:31,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:32,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:32,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:33,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:33,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:34,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:35,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:35,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:35,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:36,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:36,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:37,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:37,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:38,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:38,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:39,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:39,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:40,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:40,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:41,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:41,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:42,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:42,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:43,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:43,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:44,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:44,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:45,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:45,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:46,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:46,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:47,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:48,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:48,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:49,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:49,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:50,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:50,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:51,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:51,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:52,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:52,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:53,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:53,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:54,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:54,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:55,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:55,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:56,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:56,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:57,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:57,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:58,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:58,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:59,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:57:59,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:00,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:00,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:01,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:01,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:02,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:02,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:03,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:03,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:04,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:04,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:05,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:05,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:06,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:06,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:07,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:07,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:08,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:08,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:09,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:09,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:10,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:11,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:11,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:12,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:12,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:13,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:13,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:14,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:14,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:15,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:15,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:16,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:16,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:17,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:17,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:18,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:18,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:19,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:19,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:20,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:21,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:21,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:21,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:22,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:22,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:23,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:23,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:24,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:24,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:25,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:26,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:26,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:27,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:27,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:27,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:28,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:28,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:29,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:29,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:30,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:30,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:31,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:31,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:32,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:32,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:33,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:33,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:34,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:34,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:35,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:35,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:36,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:36,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:37,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:37,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:38,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:38,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:39,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:40,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:40,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:41,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:41,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:42,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:42,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:43,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:43,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:44,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:44,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:45,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:45,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:46,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:46,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:47,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:47,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:48,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:48,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:49,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:49,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:50,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:50,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:51,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:51,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:52,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:52,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:53,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:53,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:54,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:54,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:55,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:55,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:56,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:56,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:57,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:57,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:58,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:58,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:59,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:58:59,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:00,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:00,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:01,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:01,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:02,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:02,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:03,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:03,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:04,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:04,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:05,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:05,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:06,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:06,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:06,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:07,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:07,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:08,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:08,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:09,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:09,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:10,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:10,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:11,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:11,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:12,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:12,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:13,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:13,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:14,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:14,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:15,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:15,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:16,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:16,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:17,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:17,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:18,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:18,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:19,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:19,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:20,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:20,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:21,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:21,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:22,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:22,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:23,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:23,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:24,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:25,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:25,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:26,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:26,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:27,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:27,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:28,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:28,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:29,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:29,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:30,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:30,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:30,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:31,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:31,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:32,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:32,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:33,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:33,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:34,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:34,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:35,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:35,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:36,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:36,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:36,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:37,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:37,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:38,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:38,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:39,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:39,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:40,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:40,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:41,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:41,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:41,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:42,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:43,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:43,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:44,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:44,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:45,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:46,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:46,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:47,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:47,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:48,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:48,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:49,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:49,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:50,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:50,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:51,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:51,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:52,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:52,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:53,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:53,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:53,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:54,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:54,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:55,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:56,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:56,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:56,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:57,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:57,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:58,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:58,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:59,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 12:59:59,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:00,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:00,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:01,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:01,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:02,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:02,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:03,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:03,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:03,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:04,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:04,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:05,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:05,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:06,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:06,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:07,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:07,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:08,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:08,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:08,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:09,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:09,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:10,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:10,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:11,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:11,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:12,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:13,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:13,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:13,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:14,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:15,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:15,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:15,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:16,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:16,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:17,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:17,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:18,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:18,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:19,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:20,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:20,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:21,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:21,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:22,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:22,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:23,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:23,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:24,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:24,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:25,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:25,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:26,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:26,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:27,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:27,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:28,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:28,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:29,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:29,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:30,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:30,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:31,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:31,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:32,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:32,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:33,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:33,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:34,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:35,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:35,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:35,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:36,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:37,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:37,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:38,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:38,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:39,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:39,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:40,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:40,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:41,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:41,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:42,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:42,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:43,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:43,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:44,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:44,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:45,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:45,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:46,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:46,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:47,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:47,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:48,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:48,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:49,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:49,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:50,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:50,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:51,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:51,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:52,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:52,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:52,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:53,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:53,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:54,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:54,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:55,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:55,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:56,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:56,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:57,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:57,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:58,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:58,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:58,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:59,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:00:59,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:00,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:01,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:01,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:02,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:02,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:03,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:03,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:04,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:04,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:05,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:06,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:06,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:07,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:07,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:08,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:09,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:09,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:10,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:10,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:11,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:11,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:12,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:12,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:13,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:13,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:14,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:14,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:15,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:15,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:16,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:16,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:17,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:17,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:18,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:18,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:19,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:19,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:20,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:20,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:21,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:21,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:22,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:22,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:23,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:23,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:24,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:24,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:25,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:25,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:26,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:27,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:27,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:28,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:28,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:29,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:29,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:30,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:30,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:31,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:31,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:32,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:32,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:33,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:33,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:34,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:34,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:35,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:36,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:36,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:37,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:37,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:38,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:38,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:39,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:39,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:40,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:41,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:42,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:42,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:43,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:44,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:44,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:45,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:46,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:46,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:47,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:47,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:48,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:48,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:49,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:50,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:50,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:51,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:51,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:52,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:53,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:53,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:53,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:54,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:54,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:55,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:56,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:56,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:57,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:57,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:58,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:58,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:01:59,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:00,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:00,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:01,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:01,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:01,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:02,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:02,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:03,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:03,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:04,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:04,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:05,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:05,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:06,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:06,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:07,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:07,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:08,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:08,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:09,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:10,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:10,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:11,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:11,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:12,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:12,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:13,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:13,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:14,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:15,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:15,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:15,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:16,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:16,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:17,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:18,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:18,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:19,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:19,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:20,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:20,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:21,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:21,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:21,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:22,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:22,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:23,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:24,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:24,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:25,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:25,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:26,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:26,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:27,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:27,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:28,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:28,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:29,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:30,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:30,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:31,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:31,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:32,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:32,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:33,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:34,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:34,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:35,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:35,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:36,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:36,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:37,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:37,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:38,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:39,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:39,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:40,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:40,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:40,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:41,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:42,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:42,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:42,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:43,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:43,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:44,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:45,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:45,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:46,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:47,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:47,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:48,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:48,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:49,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:49,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:50,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:50,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:51,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:51,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:52,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:53,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:53,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:54,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:54,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:55,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:55,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:56,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:56,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:57,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:57,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:58,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:58,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:02:59,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:00,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:00,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:01,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:01,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:02,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:03,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:04,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:04,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:05,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:05,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:06,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:06,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:06,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:07,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:08,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:08,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:09,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:09,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:10,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:10,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:11,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:11,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:12,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:12,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:13,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:13,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:14,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:14,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:15,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:15,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:16,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:16,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:17,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:17,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:18,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:18,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:19,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:19,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:20,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:20,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:21,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:21,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:22,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:22,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:23,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:23,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:24,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:25,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:25,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:26,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:26,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:27,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:27,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:28,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:28,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:29,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:29,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:30,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:30,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:31,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:31,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:32,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:32,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:33,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:33,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:34,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:35,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:35,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:36,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:36,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:37,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:37,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:38,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:38,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:39,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:39,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:40,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:40,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:41,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:41,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:42,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:42,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:43,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:43,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:44,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:44,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:45,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:45,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:46,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:46,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:47,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:47,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:48,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:48,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:49,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:49,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:50,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:51,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:51,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:52,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:52,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:53,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:53,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:54,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:55,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:55,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:56,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:56,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:57,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:57,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:58,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:58,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:59,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:03:59,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:00,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:00,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:01,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:01,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:02,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:02,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:03,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:03,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:04,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:04,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:05,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:06,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:06,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:07,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:07,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:08,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:08,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:09,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:09,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:10,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:10,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:11,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:11,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:12,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:12,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:13,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:13,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:14,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:14,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:15,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:16,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:16,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:17,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:17,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:18,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:18,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:18,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:19,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:19,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:20,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:20,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:21,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:21,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:22,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:22,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:23,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:23,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:24,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:24,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:25,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:25,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:26,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:26,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:27,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:27,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:28,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:28,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:29,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:29,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:30,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:30,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:31,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:31,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:32,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:32,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:33,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:33,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:34,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:34,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:35,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:35,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:36,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:36,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:37,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:37,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:38,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:38,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:39,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:40,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:40,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:41,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:41,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:42,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:42,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:42,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:43,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:44,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:44,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:45,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:45,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:46,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:46,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:47,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:47,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:48,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:48,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:49,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:49,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:50,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:50,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:51,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:51,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:51,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:52,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:52,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:53,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:53,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:54,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:55,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:55,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:55,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:56,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:56,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:57,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:57,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:58,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:58,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:59,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:04:59,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:00,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:00,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:01,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:01,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:02,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:02,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:03,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:03,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:04,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:04,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:05,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:05,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:06,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:06,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:07,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:07,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:08,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:08,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:09,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:10,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:10,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:10,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:11,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:11,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:12,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:12,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:13,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:13,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:14,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:14,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:15,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:15,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:16,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:16,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:17,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:17,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:18,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:19,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:19,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:19,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:20,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:20,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:21,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:21,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:22,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:23,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:23,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:23,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:24,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:25,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:25,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:26,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:26,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:27,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:28,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:29,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:29,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:30,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:30,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:31,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:31,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:32,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:32,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:33,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:33,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:34,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:34,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:35,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:35,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:36,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:36,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:37,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:37,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:38,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:38,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:39,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:39,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:40,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:40,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:41,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:42,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:42,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:43,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:43,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:44,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:44,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:45,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:45,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:46,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:46,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:47,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:47,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:47,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:48,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:48,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:49,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:49,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:50,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:51,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:51,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:52,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:52,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:53,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:53,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:54,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:54,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:55,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:55,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:56,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:56,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:57,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:57,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:58,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:59,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:05:59,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:00,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:00,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:01,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:01,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:02,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:02,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:03,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:03,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:03,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:04,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:05,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:05,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:06,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:06,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:07,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:08,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:08,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:09,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:09,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:10,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:10,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:11,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:11,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:12,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:12,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:13,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:13,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:14,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:14,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:15,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:15,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:16,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:16,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:17,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:17,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:18,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:18,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:18,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:19,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:20,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:20,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:21,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:21,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:22,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:22,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:23,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:23,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:24,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:24,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:25,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:25,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:26,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:26,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:27,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:27,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:28,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:28,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:29,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:29,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:30,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:30,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:31,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:31,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:32,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:32,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:33,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:33,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:34,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:34,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:35,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:35,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:36,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:36,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:37,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:37,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:38,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:38,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:39,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:39,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:40,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:41,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:41,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:42,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:42,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:43,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:43,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:44,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:44,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:45,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:45,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:46,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:46,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:47,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:47,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:48,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:49,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:49,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:50,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:50,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:50,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:51,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:51,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:52,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:52,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:53,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:53,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:54,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:54,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:55,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:55,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:56,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:56,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:57,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:57,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:58,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:58,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:59,260][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:06:59,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:00,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:00,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:01,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:01,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:02,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:02,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:03,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:03,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:03,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:04,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:05,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:05,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:06,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:06,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:07,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:07,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:08,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:08,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:09,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:09,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:10,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:10,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:11,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:11,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:12,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:12,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:13,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:13,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:13,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:14,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:14,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:15,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:15,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:16,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:16,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:17,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:17,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:18,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:18,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:19,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:19,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:20,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:20,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:21,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:21,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:22,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:22,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:23,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:23,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:24,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:24,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:25,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:25,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:26,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:26,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:27,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:27,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:28,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:28,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:29,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:30,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:30,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:31,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:32,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:32,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:33,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:33,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:34,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:34,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:35,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:35,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:36,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:37,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:37,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:38,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:38,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:39,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:39,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:40,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:41,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:41,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:42,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:43,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:43,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:44,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:44,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:45,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:45,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:46,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:47,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:47,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:48,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:48,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:49,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:49,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:50,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:50,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:51,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:51,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:52,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:52,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:53,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:53,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:54,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:54,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:55,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:56,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:56,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:57,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:58,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:58,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:59,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:07:59,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:00,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:01,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:01,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:02,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:02,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:03,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:03,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:04,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:05,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:05,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:06,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:06,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:07,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:07,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:08,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:09,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:09,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:10,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:10,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:11,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:11,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:12,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:12,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:13,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:13,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:14,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:15,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:15,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:16,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:16,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:17,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:17,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:18,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:19,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:19,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:20,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:20,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:21,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:21,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:22,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:22,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:23,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:23,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:24,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:24,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:25,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:26,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:26,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:27,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:27,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:28,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:28,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:29,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:30,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:30,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:31,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:31,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:32,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:32,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:33,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:33,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:34,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:34,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:35,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:36,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:36,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:37,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:37,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:38,302][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:38,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:39,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:39,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:40,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:40,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:41,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:41,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:42,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:42,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:43,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:44,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:44,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:44,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:45,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:46,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:46,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:47,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:48,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:48,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:49,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:50,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:50,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:51,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:51,686][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:52,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:52,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:53,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:54,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:54,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:55,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:55,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:56,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:56,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:57,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:57,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:58,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:58,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:59,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:08:59,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:00,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:00,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:01,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:02,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:02,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:03,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:03,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:04,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:04,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:05,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:05,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:06,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:07,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:07,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:08,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:08,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:09,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:09,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:10,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:10,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:11,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:12,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:12,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:13,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:14,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:14,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:15,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:15,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:16,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:16,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:17,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:18,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:18,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:19,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:19,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:20,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:20,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:21,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:21,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:22,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:22,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:23,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:23,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:24,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:24,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:25,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:25,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:26,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:27,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:27,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:28,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:28,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:29,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:29,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:30,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:30,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:31,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:31,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:32,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:32,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:33,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:34,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:34,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:35,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:35,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:36,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:37,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:37,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:38,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:38,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:39,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:39,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:40,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:41,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:41,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:42,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:42,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:43,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:43,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:44,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:45,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:45,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:45,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:46,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:47,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:47,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:48,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:48,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:49,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:49,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:50,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:51,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:51,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:51,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:52,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:52,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:53,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:54,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:54,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:55,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:55,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:56,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:56,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:57,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:58,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:58,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:59,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:09:59,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:00,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:01,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:01,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:02,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:03,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:03,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:04,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:05,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:05,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:06,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:07,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:07,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:08,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:08,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:09,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:09,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:10,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:10,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:11,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:11,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:12,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:12,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:13,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:13,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:14,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:14,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:15,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:15,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:16,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:16,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:17,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:17,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:18,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:18,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:19,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:19,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:20,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:20,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:21,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:21,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:22,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:22,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:23,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:23,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:24,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:24,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:25,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:25,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:26,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:26,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:27,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:27,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:28,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:28,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:29,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:29,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:30,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:30,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:31,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:31,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:32,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:32,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:33,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:33,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:33,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:34,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:34,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:35,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:35,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:36,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:36,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:37,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:37,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:38,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:39,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:39,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:39,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:40,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:41,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:41,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:42,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:42,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:43,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:43,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:44,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:44,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:45,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:45,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:46,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:46,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:47,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:47,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:48,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:48,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:49,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:49,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:50,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:50,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:51,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:51,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:52,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:53,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:53,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:54,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:54,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:55,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:55,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:56,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:56,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:57,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:57,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:58,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:58,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:59,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:10:59,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:00,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:00,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:01,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:01,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:02,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:02,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:02,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:03,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:03,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:04,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:04,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:05,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:06,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:06,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:07,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:07,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:07,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:08,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:09,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:09,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:09,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:10,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:10,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:11,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:12,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:12,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:12,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:13,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:13,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:14,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:14,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:15,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:15,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:16,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:16,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:17,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:17,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:18,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:19,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:19,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:20,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:20,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:21,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:21,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:22,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:22,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:23,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:23,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:23,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:24,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:24,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:25,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:26,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:26,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:27,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:27,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:28,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:28,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:29,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:29,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:30,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:30,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:31,367][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:31,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:32,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:32,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:33,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:34,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:34,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:35,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:35,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:36,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:36,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:37,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:37,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:38,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:38,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:39,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:39,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:40,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:40,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:41,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:42,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:42,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:43,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:44,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:44,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:44,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:45,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:46,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:46,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:47,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:47,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:48,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:49,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:49,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:50,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:51,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:51,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:52,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:52,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:53,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:53,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:54,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:54,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:55,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:56,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:56,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:57,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:57,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:57,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:58,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:59,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:11:59,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:00,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:00,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:01,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:01,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:02,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:02,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:03,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:03,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:04,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:04,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:05,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:05,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:06,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:07,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:07,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:08,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:08,784][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:09,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:09,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:10,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:10,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:11,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:11,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:12,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:13,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:13,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:14,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:14,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:15,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:15,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:16,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:16,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:17,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:17,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:18,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:18,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:19,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:19,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:20,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:20,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:21,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:21,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:22,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:22,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:23,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:23,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:24,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:25,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:25,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:26,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:26,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:27,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:27,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:28,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:28,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:29,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:29,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:30,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:30,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:30,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:31,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:32,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:32,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:33,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:33,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:34,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:34,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:35,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:35,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:36,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:37,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:37,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:38,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:38,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:39,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:39,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:40,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:40,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:41,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:42,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:42,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:43,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:43,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:44,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:45,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:45,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:45,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:46,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:46,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:47,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:48,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:48,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:49,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:49,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:50,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:50,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:51,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:51,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:52,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:52,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:53,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:53,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:54,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:54,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:55,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:55,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:56,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:56,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:57,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:57,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:58,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:58,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:12:59,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:00,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:00,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:01,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:01,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:02,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:02,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:03,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:03,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:04,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:04,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:05,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:05,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:06,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:06,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:07,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:07,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:08,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:08,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:09,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:09,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:10,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:10,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:11,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:11,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:11,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:12,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:12,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:13,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:13,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:14,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:15,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:15,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:15,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:16,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:17,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:17,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:17,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:18,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:18,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:19,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:20,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:20,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:21,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:21,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:22,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:22,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:23,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:23,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:24,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:24,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:24,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:25,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:25,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:26,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:26,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:27,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:27,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:28,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:28,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:29,117][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:29,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:30,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:30,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:31,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:31,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:32,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:32,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:33,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:34,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:34,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:34,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:35,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:36,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:36,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:36,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:37,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:37,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:38,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:38,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:39,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:39,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:40,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:40,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:41,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:41,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:42,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:42,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:43,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:43,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:44,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:44,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:45,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:45,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:46,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:46,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:47,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:47,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:48,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:48,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:49,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:49,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:50,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:50,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:51,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:51,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:52,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:52,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:53,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:53,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:53,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:54,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:54,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:55,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:55,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:56,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:56,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:57,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:57,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:58,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:58,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:59,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:13:59,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:00,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:00,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:01,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:02,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:02,539][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:02,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:03,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:03,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:04,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:04,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:05,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:05,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:06,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:06,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:07,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:07,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:08,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:08,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:09,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:09,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:10,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:10,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:11,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:11,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:12,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:12,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:13,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:13,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:14,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:14,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:15,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:15,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:16,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:16,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:16,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:17,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:17,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:18,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:18,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:19,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:19,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:20,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:20,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:21,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:21,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:22,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:22,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:22,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:23,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:23,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:24,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:24,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:25,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:25,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:26,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:26,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:27,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:27,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:28,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:28,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:29,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:30,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:30,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:31,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:31,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:32,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:32,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:33,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:33,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:34,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:34,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:35,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:35,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:36,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:36,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:37,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:37,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:38,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:38,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:39,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:39,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:40,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:40,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:41,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:42,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:42,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:43,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:43,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:44,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:44,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:45,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:46,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:46,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:47,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:47,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:48,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:48,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:49,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:49,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:50,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:50,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:51,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:51,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:52,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:52,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:53,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:53,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:54,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:54,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:55,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:56,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:56,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:57,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:57,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:57,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:58,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:58,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:59,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:14:59,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:00,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:00,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:01,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:01,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:02,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:02,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:03,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:03,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:04,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:04,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:05,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:05,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:06,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:06,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:07,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:07,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:07,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:08,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:08,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:09,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:09,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:10,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:10,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:11,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:11,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:12,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:12,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:13,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:13,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:14,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:14,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:15,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:15,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:16,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:17,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:17,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:18,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:18,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:18,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:19,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:19,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:20,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:20,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:21,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:21,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:22,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:22,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:23,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:23,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:24,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:24,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:25,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:25,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:26,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:26,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:27,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:27,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:28,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:28,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:29,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:29,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:30,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:30,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:31,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:31,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:32,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:32,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:33,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:34,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:34,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:35,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:35,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:36,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:36,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:37,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:37,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:38,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:38,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:39,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:39,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:40,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:40,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:41,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:41,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:42,443][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:42,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:43,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:43,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:44,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:44,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:45,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:46,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:46,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:46,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:47,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:48,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:48,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:49,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:49,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:50,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:50,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:51,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:51,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:52,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:52,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:53,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:54,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:54,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:54,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:55,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:55,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:56,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:56,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:57,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:57,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:58,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:58,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:59,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:15:59,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:00,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:00,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:01,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:01,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:02,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:02,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:03,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:04,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:05,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:05,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:06,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:06,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:07,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:07,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:08,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:08,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:09,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:10,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:10,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:11,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:12,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:12,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:13,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:13,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:14,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:14,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:15,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:15,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:16,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:17,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:17,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:18,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:18,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:19,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:19,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:20,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:20,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:21,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:21,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:22,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:23,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:23,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:24,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:24,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:25,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:25,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:26,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:26,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:27,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:27,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:28,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:28,481][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:28,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:29,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:30,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:30,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:31,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:31,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:32,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:32,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:33,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:33,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:34,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:34,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:34,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:35,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:36,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:36,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:37,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:37,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:38,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:38,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:39,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:39,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:40,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:41,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:41,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:41,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:42,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:42,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:43,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:43,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:44,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:44,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:45,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:45,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:46,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:47,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:47,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:48,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:48,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:49,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:49,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:50,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:51,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:51,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:51,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:52,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:52,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:53,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:53,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:54,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:55,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:55,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:56,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:56,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:57,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:57,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:58,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:58,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:58,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:59,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:16:59,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:00,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:00,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:01,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:01,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:02,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:02,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:03,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:03,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:04,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:04,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:05,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:05,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:06,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:06,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:07,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:07,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:08,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:08,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:09,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:09,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:10,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:11,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:11,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:12,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:12,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:13,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:14,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:14,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:15,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:15,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:16,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:16,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:17,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:17,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:17,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:18,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:19,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:19,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:19,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:20,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:20,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:21,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:21,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:22,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:22,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:23,266][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:23,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:24,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:24,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:25,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:25,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:26,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:27,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:27,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:28,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:28,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:29,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:29,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:29,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:30,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:30,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:31,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:32,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:32,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:33,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:33,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:34,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:34,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:35,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:35,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:35,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:36,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:37,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:37,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:38,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:39,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:39,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:40,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:41,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:41,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:42,065][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:42,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:43,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:44,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:45,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:45,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:46,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:47,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:48,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:48,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:49,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:49,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:50,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:50,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:51,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:51,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:51,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:52,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:52,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:53,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:53,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:54,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:54,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:55,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:55,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:55,898][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:56,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:56,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:57,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:57,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:58,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:58,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:59,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:17:59,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:00,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:00,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:01,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:01,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:02,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:02,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:03,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:03,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:04,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:05,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:05,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:06,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:06,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:07,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:07,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:08,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:08,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:09,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:09,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:10,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:10,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:11,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:11,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:12,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:12,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:12,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:13,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:13,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:14,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:14,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:15,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:15,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:16,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:17,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:17,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:17,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:18,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:19,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:19,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:20,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:20,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:21,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:21,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:22,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:22,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:23,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:23,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:24,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:24,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:25,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:25,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:26,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:26,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:27,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:27,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:28,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:28,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:29,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:30,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:30,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:31,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:31,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:32,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:32,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:32,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:33,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:33,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:34,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:35,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:35,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:35,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:36,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:36,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:37,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:37,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:38,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:38,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:39,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:39,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:40,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:40,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:41,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:41,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:42,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:42,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:43,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:43,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:44,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:45,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:45,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:46,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:46,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:47,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:47,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:48,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:48,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:49,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:49,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:50,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:50,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:51,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:51,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:52,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:53,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:53,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:53,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:54,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:54,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:55,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:55,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:56,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:56,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:57,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:57,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:58,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:58,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:59,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:18:59,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:00,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:00,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:01,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:01,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:02,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:02,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:03,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:03,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:03,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:04,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:04,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:05,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:05,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:06,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:06,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:07,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:07,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:08,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:08,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:09,405][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:09,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:10,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:10,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:11,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:12,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:12,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:13,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:13,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:14,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:14,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:15,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:15,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:16,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:16,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:17,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:17,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:18,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:18,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:19,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:19,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:20,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:20,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:21,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:21,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:22,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:22,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:23,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:24,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:24,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:24,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:25,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:25,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:26,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:26,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:27,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:27,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:28,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:29,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:29,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:30,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:31,331][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:31,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:32,619][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:33,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:33,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:34,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:34,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:35,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:35,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:36,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:36,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:37,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:37,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:38,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:39,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:39,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:40,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:41,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:41,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:42,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:42,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:42,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:43,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:43,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:44,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:44,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:45,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:45,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:46,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:46,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:47,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:47,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:47,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:48,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:49,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:49,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:49,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:50,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:50,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:51,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:51,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:52,460][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:52,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:53,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:53,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:54,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:54,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:55,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:55,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:56,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:56,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:57,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:57,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:57,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:58,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:58,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:59,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:19:59,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:00,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:00,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:01,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:01,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:01,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:02,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:02,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:03,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:03,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:04,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:04,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:05,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:05,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:06,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:06,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:06,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:07,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:07,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:08,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:08,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:09,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:09,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:10,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:10,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:11,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:11,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:12,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:13,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:13,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:13,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:14,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:14,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:15,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:15,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:16,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:16,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:17,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:17,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:18,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:18,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:19,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:19,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:20,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:20,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:21,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:21,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:21,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:22,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:22,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:23,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:24,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:24,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:25,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:25,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:25,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:26,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:26,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:27,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:27,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:28,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:28,544][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:28,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:29,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:29,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:30,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:30,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:31,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:31,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:32,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:32,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:33,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:33,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:33,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:34,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:35,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:35,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:35,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:36,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:37,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:37,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:38,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:38,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:39,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:39,472][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:39,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:40,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:40,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:41,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:41,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:42,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:42,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:43,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:43,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:44,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:44,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:45,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:45,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:46,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:46,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:47,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:47,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:48,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:48,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:48,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:49,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:49,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:50,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:50,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:51,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:51,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:52,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:52,552][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:53,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:53,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:54,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:54,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:55,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:55,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:56,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:56,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:57,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:57,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:58,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:58,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:59,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:20:59,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:00,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:00,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:01,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:01,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:02,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:02,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:03,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:03,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:04,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:05,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:05,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:06,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:06,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:07,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:07,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:08,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:08,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:09,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:09,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:10,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:10,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:11,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:11,595][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:12,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:12,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:13,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:13,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:13,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:14,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:14,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:15,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:15,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:16,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:16,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:17,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:17,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:17,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:18,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:18,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:19,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:20,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:20,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:20,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:21,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:21,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:22,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:22,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:23,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:23,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:24,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:25,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:25,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:26,064][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:26,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:27,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:27,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:27,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:28,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:28,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:29,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:29,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:30,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:30,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:31,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:31,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:32,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:33,100][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:33,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:34,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:34,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:35,090][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:35,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:36,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:36,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:37,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:37,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:38,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:38,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:39,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:39,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:40,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:41,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:41,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:42,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:43,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:43,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:44,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:44,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:45,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:45,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:46,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:46,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:47,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:47,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:48,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:49,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:49,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:50,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:50,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:51,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:51,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:52,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:52,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:53,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:53,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:54,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:54,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:55,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:55,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:56,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:56,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:57,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:58,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:58,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:59,051][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:21:59,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:00,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:00,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:01,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:02,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:02,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:03,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:03,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:04,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:04,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:05,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:05,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:06,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:07,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:07,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:08,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:09,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:09,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:10,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:10,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:11,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:12,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:12,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:13,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:13,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:14,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:14,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:15,035][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:15,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:16,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:16,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:17,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:17,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:18,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:18,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:18,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:19,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:20,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:20,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:21,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:21,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:22,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:22,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:23,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:23,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:24,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:24,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:25,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:26,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:26,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:27,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:28,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:28,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:29,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:29,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:30,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:31,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:32,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:32,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:33,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:33,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:34,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:34,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:35,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:35,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:36,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:37,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:37,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:38,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:38,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:39,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:39,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:40,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:40,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:41,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:41,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:42,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:42,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:43,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:43,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:44,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:44,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:45,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:45,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:46,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:46,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:47,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:47,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:48,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:49,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:50,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:50,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:51,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:51,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:52,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:52,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:53,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:53,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:54,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:54,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:54,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:55,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:56,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:56,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:57,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:57,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:58,067][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:58,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:58,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:59,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:22:59,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:00,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:01,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:01,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:02,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:02,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:03,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:04,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:04,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:04,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:05,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:05,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:06,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:06,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:07,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:07,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:08,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:08,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:09,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:09,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:10,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:10,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:11,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:11,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:12,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:12,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:13,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:13,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:14,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:14,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:14,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:15,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:15,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:16,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:16,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:17,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:17,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:18,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:18,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:19,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:20,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:20,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:21,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:21,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:21,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:22,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:22,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:23,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:23,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:23,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:24,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:24,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:25,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:25,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:26,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:27,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:27,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:28,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:28,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:29,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:29,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:30,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:30,845][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:31,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:31,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:32,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:32,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:33,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:33,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:34,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:34,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:35,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:35,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:36,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:36,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:37,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:37,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:38,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:39,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:39,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:40,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:40,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:41,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:41,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:42,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:42,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:43,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:43,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:44,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:44,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:45,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:45,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:46,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:46,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:47,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:47,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:48,455][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:48,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:49,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:50,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:50,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:51,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:51,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:52,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:53,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:53,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:53,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:54,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:55,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:55,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:56,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:56,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:56,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:57,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:57,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:58,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:58,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:23:59,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:00,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:00,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:01,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:01,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:02,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:02,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:03,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:03,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:04,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:04,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:05,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:05,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:06,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:06,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:07,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:07,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:08,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:08,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:09,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:09,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:10,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:10,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:11,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:11,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:12,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:12,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:13,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:13,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:13,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:14,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:14,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:15,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:15,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:16,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:16,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:17,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:17,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:18,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:18,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:19,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:19,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:20,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:20,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:21,169][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:21,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:22,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:22,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:23,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:23,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:24,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:24,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:25,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:25,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:26,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:26,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:27,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:28,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:28,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:29,030][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:29,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:30,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:30,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:31,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:31,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:32,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:32,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:33,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:33,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:34,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:34,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:35,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:35,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:36,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:36,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:36,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:37,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:37,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:38,323][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:38,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:39,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:39,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:40,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:41,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:41,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:42,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:42,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:43,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:43,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:44,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:44,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:45,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:45,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:46,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:46,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:46,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:47,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:47,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:48,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:48,760][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:49,255][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:49,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:50,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:50,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:51,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:51,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:51,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:52,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:52,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:53,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:53,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:54,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:54,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:55,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:55,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:56,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:56,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:57,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:57,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:58,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:58,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:58,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:59,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:24:59,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:00,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:01,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:01,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:02,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:02,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:03,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:03,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:04,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:04,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:05,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:05,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:06,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:06,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:07,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:07,855][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7894, device='cuda:0') eval_epoch_loss=tensor(0.5819, device='cuda:0') eval_epoch_acc=tensor(0.8412, device='cuda:0')
[2025-02-16 13:25:07,857][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-16 13:25:07,858][slam_llm.utils.checkpoint_handler][INFO] - --> saving model checkpoint...
[2025-02-16 13:25:08,835][slam_llm.utils.checkpoint_handler][INFO] - Checkpoint saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_dual_peft_seed_42/asr_epoch_1_step_11918_loss_0.5818797945976257/model.pt
[2025-02-16 13:25:08,840][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_dual_peft_seed_42 directory
[2025-02-16 13:25:08,841][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.5818797945976257
[2025-02-16 13:25:08,842][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8411897420883179
[2025-02-16 13:25:09,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:09,319][root][INFO] - Training Epoch: 1/2, step 11918/23838 completed (loss: 0.2780016362667084, acc: 0.9083333611488342)
[2025-02-16 13:25:09,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:09,742][root][INFO] - Training Epoch: 1/2, step 11919/23838 completed (loss: 0.4178379774093628, acc: 0.8854166865348816)
[2025-02-16 13:25:09,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:10,161][root][INFO] - Training Epoch: 1/2, step 11920/23838 completed (loss: 0.30974823236465454, acc: 0.9226519465446472)
[2025-02-16 13:25:10,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:10,667][root][INFO] - Training Epoch: 1/2, step 11921/23838 completed (loss: 0.32568737864494324, acc: 0.8931297659873962)
[2025-02-16 13:25:10,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:11,082][root][INFO] - Training Epoch: 1/2, step 11922/23838 completed (loss: 0.4147140085697174, acc: 0.9047619104385376)
[2025-02-16 13:25:11,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:11,523][root][INFO] - Training Epoch: 1/2, step 11923/23838 completed (loss: 0.22198763489723206, acc: 0.939393937587738)
[2025-02-16 13:25:11,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:11,978][root][INFO] - Training Epoch: 1/2, step 11924/23838 completed (loss: 0.3102859854698181, acc: 0.9370078444480896)
[2025-02-16 13:25:12,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:12,931][root][INFO] - Training Epoch: 1/2, step 11925/23838 completed (loss: 0.18619167804718018, acc: 0.9624413251876831)
[2025-02-16 13:25:13,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:13,312][root][INFO] - Training Epoch: 1/2, step 11926/23838 completed (loss: 0.24398137629032135, acc: 0.9285714030265808)
[2025-02-16 13:25:13,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:13,785][root][INFO] - Training Epoch: 1/2, step 11927/23838 completed (loss: 0.28207531571388245, acc: 0.9320388436317444)
[2025-02-16 13:25:14,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:14,234][root][INFO] - Training Epoch: 1/2, step 11928/23838 completed (loss: 0.38588571548461914, acc: 0.875)
[2025-02-16 13:25:14,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:14,618][root][INFO] - Training Epoch: 1/2, step 11929/23838 completed (loss: 0.4018935263156891, acc: 0.8695651888847351)
[2025-02-16 13:25:14,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:15,069][root][INFO] - Training Epoch: 1/2, step 11930/23838 completed (loss: 0.9516470432281494, acc: 0.6486486196517944)
[2025-02-16 13:25:15,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:15,525][root][INFO] - Training Epoch: 1/2, step 11931/23838 completed (loss: 0.18590107560157776, acc: 0.9375)
[2025-02-16 13:25:15,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:16,018][root][INFO] - Training Epoch: 1/2, step 11932/23838 completed (loss: 0.3346552550792694, acc: 0.9142857193946838)
[2025-02-16 13:25:16,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:16,519][root][INFO] - Training Epoch: 1/2, step 11933/23838 completed (loss: 0.26849114894866943, acc: 0.9294871687889099)
[2025-02-16 13:25:16,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:16,946][root][INFO] - Training Epoch: 1/2, step 11934/23838 completed (loss: 0.43794551491737366, acc: 0.885496199131012)
[2025-02-16 13:25:17,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:17,433][root][INFO] - Training Epoch: 1/2, step 11935/23838 completed (loss: 0.2731158435344696, acc: 0.9494949579238892)
[2025-02-16 13:25:17,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:17,983][root][INFO] - Training Epoch: 1/2, step 11936/23838 completed (loss: 0.4061087965965271, acc: 0.8993710875511169)
[2025-02-16 13:25:18,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:18,419][root][INFO] - Training Epoch: 1/2, step 11937/23838 completed (loss: 0.4246389865875244, acc: 0.8970588445663452)
[2025-02-16 13:25:18,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:18,823][root][INFO] - Training Epoch: 1/2, step 11938/23838 completed (loss: 0.29893070459365845, acc: 0.953125)
[2025-02-16 13:25:18,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:19,218][root][INFO] - Training Epoch: 1/2, step 11939/23838 completed (loss: 0.8535339832305908, acc: 0.7662337422370911)
[2025-02-16 13:25:19,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:19,652][root][INFO] - Training Epoch: 1/2, step 11940/23838 completed (loss: 0.9637508392333984, acc: 0.767123281955719)
[2025-02-16 13:25:19,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:20,089][root][INFO] - Training Epoch: 1/2, step 11941/23838 completed (loss: 0.272339403629303, acc: 0.9142857193946838)
[2025-02-16 13:25:20,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:20,539][root][INFO] - Training Epoch: 1/2, step 11942/23838 completed (loss: 0.19294783473014832, acc: 0.9595959782600403)
[2025-02-16 13:25:20,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:20,989][root][INFO] - Training Epoch: 1/2, step 11943/23838 completed (loss: 0.4241892993450165, acc: 0.8723404407501221)
[2025-02-16 13:25:21,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:21,423][root][INFO] - Training Epoch: 1/2, step 11944/23838 completed (loss: 0.4804348647594452, acc: 0.859375)
[2025-02-16 13:25:21,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:21,883][root][INFO] - Training Epoch: 1/2, step 11945/23838 completed (loss: 0.33333027362823486, acc: 0.9344262480735779)
[2025-02-16 13:25:22,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:22,287][root][INFO] - Training Epoch: 1/2, step 11946/23838 completed (loss: 0.34955132007598877, acc: 0.907216489315033)
[2025-02-16 13:25:22,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:22,737][root][INFO] - Training Epoch: 1/2, step 11947/23838 completed (loss: 0.28647124767303467, acc: 0.9047619104385376)
[2025-02-16 13:25:22,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:23,223][root][INFO] - Training Epoch: 1/2, step 11948/23838 completed (loss: 0.8312923312187195, acc: 0.7526881694793701)
[2025-02-16 13:25:23,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:23,708][root][INFO] - Training Epoch: 1/2, step 11949/23838 completed (loss: 0.7325318455696106, acc: 0.7924528121948242)
[2025-02-16 13:25:23,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:24,117][root][INFO] - Training Epoch: 1/2, step 11950/23838 completed (loss: 0.4722236096858978, acc: 0.8235294222831726)
[2025-02-16 13:25:24,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:24,528][root][INFO] - Training Epoch: 1/2, step 11951/23838 completed (loss: 0.4082900881767273, acc: 0.89682537317276)
[2025-02-16 13:25:24,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:24,962][root][INFO] - Training Epoch: 1/2, step 11952/23838 completed (loss: 0.7168859839439392, acc: 0.835616409778595)
[2025-02-16 13:25:25,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:25,399][root][INFO] - Training Epoch: 1/2, step 11953/23838 completed (loss: 0.5982632040977478, acc: 0.8383838534355164)
[2025-02-16 13:25:25,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:25,829][root][INFO] - Training Epoch: 1/2, step 11954/23838 completed (loss: 0.6535666584968567, acc: 0.8428571224212646)
[2025-02-16 13:25:26,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:26,318][root][INFO] - Training Epoch: 1/2, step 11955/23838 completed (loss: 0.8546931147575378, acc: 0.7719298005104065)
[2025-02-16 13:25:26,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:26,792][root][INFO] - Training Epoch: 1/2, step 11956/23838 completed (loss: 0.26137953996658325, acc: 0.9154929518699646)
[2025-02-16 13:25:26,955][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:27,202][root][INFO] - Training Epoch: 1/2, step 11957/23838 completed (loss: 0.2372124046087265, acc: 0.9360465407371521)
[2025-02-16 13:25:27,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:27,663][root][INFO] - Training Epoch: 1/2, step 11958/23838 completed (loss: 0.42453452944755554, acc: 0.9111111164093018)
[2025-02-16 13:25:27,847][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:28,078][root][INFO] - Training Epoch: 1/2, step 11959/23838 completed (loss: 0.5309527516365051, acc: 0.8823529481887817)
[2025-02-16 13:25:28,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:28,514][root][INFO] - Training Epoch: 1/2, step 11960/23838 completed (loss: 0.22122928500175476, acc: 0.9710144996643066)
[2025-02-16 13:25:28,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:28,956][root][INFO] - Training Epoch: 1/2, step 11961/23838 completed (loss: 0.41100800037384033, acc: 0.9054054021835327)
[2025-02-16 13:25:29,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:29,439][root][INFO] - Training Epoch: 1/2, step 11962/23838 completed (loss: 0.3370685279369354, acc: 0.9189189076423645)
[2025-02-16 13:25:29,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:29,857][root][INFO] - Training Epoch: 1/2, step 11963/23838 completed (loss: 0.45216938853263855, acc: 0.8888888955116272)
[2025-02-16 13:25:29,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:30,236][root][INFO] - Training Epoch: 1/2, step 11964/23838 completed (loss: 0.4667864441871643, acc: 0.8813559412956238)
[2025-02-16 13:25:30,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:30,692][root][INFO] - Training Epoch: 1/2, step 11965/23838 completed (loss: 0.9972661733627319, acc: 0.6585366129875183)
[2025-02-16 13:25:30,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:31,173][root][INFO] - Training Epoch: 1/2, step 11966/23838 completed (loss: 0.239376500248909, acc: 0.9428571462631226)
[2025-02-16 13:25:31,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:31,638][root][INFO] - Training Epoch: 1/2, step 11967/23838 completed (loss: 0.1840028464794159, acc: 0.9252336621284485)
[2025-02-16 13:25:31,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:32,116][root][INFO] - Training Epoch: 1/2, step 11968/23838 completed (loss: 0.4652295708656311, acc: 0.8503937125205994)
[2025-02-16 13:25:32,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:32,567][root][INFO] - Training Epoch: 1/2, step 11969/23838 completed (loss: 0.4047273099422455, acc: 0.8767123222351074)
[2025-02-16 13:25:32,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:32,954][root][INFO] - Training Epoch: 1/2, step 11970/23838 completed (loss: 0.308012455701828, acc: 0.9081632494926453)
[2025-02-16 13:25:33,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:33,422][root][INFO] - Training Epoch: 1/2, step 11971/23838 completed (loss: 0.3410766124725342, acc: 0.8840579986572266)
[2025-02-16 13:25:33,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:33,898][root][INFO] - Training Epoch: 1/2, step 11972/23838 completed (loss: 0.23976187407970428, acc: 0.9172932505607605)
[2025-02-16 13:25:34,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:34,388][root][INFO] - Training Epoch: 1/2, step 11973/23838 completed (loss: 0.29433199763298035, acc: 0.9159663915634155)
[2025-02-16 13:25:34,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:34,862][root][INFO] - Training Epoch: 1/2, step 11974/23838 completed (loss: 0.10564379394054413, acc: 0.9545454382896423)
[2025-02-16 13:25:35,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:35,308][root][INFO] - Training Epoch: 1/2, step 11975/23838 completed (loss: 0.40724068880081177, acc: 0.9074074029922485)
[2025-02-16 13:25:35,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:35,776][root][INFO] - Training Epoch: 1/2, step 11976/23838 completed (loss: 0.05324340984225273, acc: 0.9841269850730896)
[2025-02-16 13:25:35,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:36,194][root][INFO] - Training Epoch: 1/2, step 11977/23838 completed (loss: 0.4094255864620209, acc: 0.8925619721412659)
[2025-02-16 13:25:36,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:36,681][root][INFO] - Training Epoch: 1/2, step 11978/23838 completed (loss: 0.49775606393814087, acc: 0.8404255509376526)
[2025-02-16 13:25:36,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:37,147][root][INFO] - Training Epoch: 1/2, step 11979/23838 completed (loss: 1.0298309326171875, acc: 0.6875)
[2025-02-16 13:25:37,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:37,586][root][INFO] - Training Epoch: 1/2, step 11980/23838 completed (loss: 0.8195839524269104, acc: 0.7263157963752747)
[2025-02-16 13:25:37,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:38,026][root][INFO] - Training Epoch: 1/2, step 11981/23838 completed (loss: 0.4588386118412018, acc: 0.8695651888847351)
[2025-02-16 13:25:38,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:38,474][root][INFO] - Training Epoch: 1/2, step 11982/23838 completed (loss: 0.6030356884002686, acc: 0.8690476417541504)
[2025-02-16 13:25:38,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:38,926][root][INFO] - Training Epoch: 1/2, step 11983/23838 completed (loss: 0.20035232603549957, acc: 0.9428571462631226)
[2025-02-16 13:25:39,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:39,376][root][INFO] - Training Epoch: 1/2, step 11984/23838 completed (loss: 0.22281001508235931, acc: 0.9191176295280457)
[2025-02-16 13:25:39,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:39,747][root][INFO] - Training Epoch: 1/2, step 11985/23838 completed (loss: 0.42985114455223083, acc: 0.9090909361839294)
[2025-02-16 13:25:39,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:40,220][root][INFO] - Training Epoch: 1/2, step 11986/23838 completed (loss: 0.042917780578136444, acc: 0.9900000095367432)
[2025-02-16 13:25:40,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:40,670][root][INFO] - Training Epoch: 1/2, step 11987/23838 completed (loss: 0.34916195273399353, acc: 0.9285714030265808)
[2025-02-16 13:25:40,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:41,161][root][INFO] - Training Epoch: 1/2, step 11988/23838 completed (loss: 0.2645663917064667, acc: 0.932584285736084)
[2025-02-16 13:25:41,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:41,569][root][INFO] - Training Epoch: 1/2, step 11989/23838 completed (loss: 0.31768518686294556, acc: 0.9607843160629272)
[2025-02-16 13:25:41,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:41,937][root][INFO] - Training Epoch: 1/2, step 11990/23838 completed (loss: 0.22114989161491394, acc: 0.9142857193946838)
[2025-02-16 13:25:42,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:42,324][root][INFO] - Training Epoch: 1/2, step 11991/23838 completed (loss: 0.2931024432182312, acc: 0.9146341681480408)
[2025-02-16 13:25:42,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:42,787][root][INFO] - Training Epoch: 1/2, step 11992/23838 completed (loss: 0.2637900710105896, acc: 0.939393937587738)
[2025-02-16 13:25:42,960][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:43,190][root][INFO] - Training Epoch: 1/2, step 11993/23838 completed (loss: 0.3742159307003021, acc: 0.9208633303642273)
[2025-02-16 13:25:43,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:43,649][root][INFO] - Training Epoch: 1/2, step 11994/23838 completed (loss: 0.21945373713970184, acc: 0.9516128897666931)
[2025-02-16 13:25:43,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:44,093][root][INFO] - Training Epoch: 1/2, step 11995/23838 completed (loss: 0.17066366970539093, acc: 0.9469026327133179)
[2025-02-16 13:25:44,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:44,581][root][INFO] - Training Epoch: 1/2, step 11996/23838 completed (loss: 0.20019671320915222, acc: 0.9454545378684998)
[2025-02-16 13:25:44,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:45,052][root][INFO] - Training Epoch: 1/2, step 11997/23838 completed (loss: 0.0667911246418953, acc: 0.9900000095367432)
[2025-02-16 13:25:45,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:45,516][root][INFO] - Training Epoch: 1/2, step 11998/23838 completed (loss: 0.19359414279460907, acc: 0.9444444179534912)
[2025-02-16 13:25:45,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:45,959][root][INFO] - Training Epoch: 1/2, step 11999/23838 completed (loss: 0.09491684287786484, acc: 0.9863013625144958)
[2025-02-16 13:25:46,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:46,412][root][INFO] - Training Epoch: 1/2, step 12000/23838 completed (loss: 0.17905469238758087, acc: 0.9420289993286133)
[2025-02-16 13:25:46,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:46,869][root][INFO] - Training Epoch: 1/2, step 12001/23838 completed (loss: 0.37279781699180603, acc: 0.907216489315033)
[2025-02-16 13:25:47,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:47,413][root][INFO] - Training Epoch: 1/2, step 12002/23838 completed (loss: 0.454464852809906, acc: 0.9255319237709045)
[2025-02-16 13:25:47,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:47,945][root][INFO] - Training Epoch: 1/2, step 12003/23838 completed (loss: 0.09599417448043823, acc: 0.9836065769195557)
[2025-02-16 13:25:48,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:48,366][root][INFO] - Training Epoch: 1/2, step 12004/23838 completed (loss: 0.12058506160974503, acc: 0.9818181991577148)
[2025-02-16 13:25:48,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:49,332][root][INFO] - Training Epoch: 1/2, step 12005/23838 completed (loss: 0.29916447401046753, acc: 0.8909090757369995)
[2025-02-16 13:25:49,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:49,869][root][INFO] - Training Epoch: 1/2, step 12006/23838 completed (loss: 0.25979965925216675, acc: 0.9076923131942749)
[2025-02-16 13:25:50,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:50,825][root][INFO] - Training Epoch: 1/2, step 12007/23838 completed (loss: 0.5831418037414551, acc: 0.8354430198669434)
[2025-02-16 13:25:50,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:51,196][root][INFO] - Training Epoch: 1/2, step 12008/23838 completed (loss: 1.176615595817566, acc: 0.7058823704719543)
[2025-02-16 13:25:51,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:51,559][root][INFO] - Training Epoch: 1/2, step 12009/23838 completed (loss: 0.36831143498420715, acc: 0.8676470518112183)
[2025-02-16 13:25:51,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:51,977][root][INFO] - Training Epoch: 1/2, step 12010/23838 completed (loss: 0.13993123173713684, acc: 0.9780219793319702)
[2025-02-16 13:25:52,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:52,484][root][INFO] - Training Epoch: 1/2, step 12011/23838 completed (loss: 0.34034377336502075, acc: 0.9009009003639221)
[2025-02-16 13:25:52,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:53,076][root][INFO] - Training Epoch: 1/2, step 12012/23838 completed (loss: 0.2630082368850708, acc: 0.9461538195610046)
[2025-02-16 13:25:53,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:53,461][root][INFO] - Training Epoch: 1/2, step 12013/23838 completed (loss: 0.16106189787387848, acc: 0.9439252614974976)
[2025-02-16 13:25:53,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:54,175][root][INFO] - Training Epoch: 1/2, step 12014/23838 completed (loss: 0.12721732258796692, acc: 0.9449541568756104)
[2025-02-16 13:25:54,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:54,631][root][INFO] - Training Epoch: 1/2, step 12015/23838 completed (loss: 0.06174666807055473, acc: 0.9789473414421082)
[2025-02-16 13:25:54,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:55,069][root][INFO] - Training Epoch: 1/2, step 12016/23838 completed (loss: 0.5207287073135376, acc: 0.8705882430076599)
[2025-02-16 13:25:55,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:55,515][root][INFO] - Training Epoch: 1/2, step 12017/23838 completed (loss: 0.2528262734413147, acc: 0.931034505367279)
[2025-02-16 13:25:55,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:55,913][root][INFO] - Training Epoch: 1/2, step 12018/23838 completed (loss: 0.1748581826686859, acc: 0.9347826242446899)
[2025-02-16 13:25:56,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:56,392][root][INFO] - Training Epoch: 1/2, step 12019/23838 completed (loss: 0.6073982119560242, acc: 0.8160919547080994)
[2025-02-16 13:25:56,594][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:56,857][root][INFO] - Training Epoch: 1/2, step 12020/23838 completed (loss: 0.17355923354625702, acc: 0.9545454382896423)
[2025-02-16 13:25:57,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:57,289][root][INFO] - Training Epoch: 1/2, step 12021/23838 completed (loss: 0.24275019764900208, acc: 0.9160305261611938)
[2025-02-16 13:25:57,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:57,736][root][INFO] - Training Epoch: 1/2, step 12022/23838 completed (loss: 0.31669744849205017, acc: 0.9105691313743591)
[2025-02-16 13:25:57,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:58,253][root][INFO] - Training Epoch: 1/2, step 12023/23838 completed (loss: 0.4013251066207886, acc: 0.8999999761581421)
[2025-02-16 13:25:58,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:58,675][root][INFO] - Training Epoch: 1/2, step 12024/23838 completed (loss: 0.19878149032592773, acc: 0.9655172228813171)
[2025-02-16 13:25:58,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:59,136][root][INFO] - Training Epoch: 1/2, step 12025/23838 completed (loss: 0.20632100105285645, acc: 0.9452054500579834)
[2025-02-16 13:25:59,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:25:59,686][root][INFO] - Training Epoch: 1/2, step 12026/23838 completed (loss: 0.3538414537906647, acc: 0.8981481194496155)
[2025-02-16 13:25:59,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:00,081][root][INFO] - Training Epoch: 1/2, step 12027/23838 completed (loss: 0.990788996219635, acc: 0.791208803653717)
[2025-02-16 13:26:00,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:00,576][root][INFO] - Training Epoch: 1/2, step 12028/23838 completed (loss: 0.19360004365444183, acc: 0.9316239356994629)
[2025-02-16 13:26:00,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:00,956][root][INFO] - Training Epoch: 1/2, step 12029/23838 completed (loss: 0.23054282367229462, acc: 0.9264705777168274)
[2025-02-16 13:26:01,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:01,432][root][INFO] - Training Epoch: 1/2, step 12030/23838 completed (loss: 0.04498980566859245, acc: 0.9862068891525269)
[2025-02-16 13:26:01,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:01,845][root][INFO] - Training Epoch: 1/2, step 12031/23838 completed (loss: 0.195978045463562, acc: 0.9375)
[2025-02-16 13:26:02,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:02,279][root][INFO] - Training Epoch: 1/2, step 12032/23838 completed (loss: 0.40780362486839294, acc: 0.9189189076423645)
[2025-02-16 13:26:02,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:02,709][root][INFO] - Training Epoch: 1/2, step 12033/23838 completed (loss: 0.351680189371109, acc: 0.9026548862457275)
[2025-02-16 13:26:02,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:03,114][root][INFO] - Training Epoch: 1/2, step 12034/23838 completed (loss: 0.3416097164154053, acc: 0.9056603908538818)
[2025-02-16 13:26:03,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:03,991][root][INFO] - Training Epoch: 1/2, step 12035/23838 completed (loss: 0.5787158012390137, acc: 0.8258426785469055)
[2025-02-16 13:26:04,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:04,496][root][INFO] - Training Epoch: 1/2, step 12036/23838 completed (loss: 0.21897397935390472, acc: 0.9251700639724731)
[2025-02-16 13:26:04,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:04,985][root][INFO] - Training Epoch: 1/2, step 12037/23838 completed (loss: 0.30348560214042664, acc: 0.9017857313156128)
[2025-02-16 13:26:05,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:05,427][root][INFO] - Training Epoch: 1/2, step 12038/23838 completed (loss: 0.20015332102775574, acc: 0.942148745059967)
[2025-02-16 13:26:05,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:05,942][root][INFO] - Training Epoch: 1/2, step 12039/23838 completed (loss: 0.21984055638313293, acc: 0.9437500238418579)
[2025-02-16 13:26:06,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:06,511][root][INFO] - Training Epoch: 1/2, step 12040/23838 completed (loss: 0.4224412143230438, acc: 0.8828828930854797)
[2025-02-16 13:26:06,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:06,977][root][INFO] - Training Epoch: 1/2, step 12041/23838 completed (loss: 0.38109612464904785, acc: 0.8979591727256775)
[2025-02-16 13:26:07,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:07,432][root][INFO] - Training Epoch: 1/2, step 12042/23838 completed (loss: 0.49091070890426636, acc: 0.8785046935081482)
[2025-02-16 13:26:07,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:07,851][root][INFO] - Training Epoch: 1/2, step 12043/23838 completed (loss: 0.5978503823280334, acc: 0.8453608155250549)
[2025-02-16 13:26:08,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:08,300][root][INFO] - Training Epoch: 1/2, step 12044/23838 completed (loss: 0.3922264277935028, acc: 0.9166666865348816)
[2025-02-16 13:26:08,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:08,709][root][INFO] - Training Epoch: 1/2, step 12045/23838 completed (loss: 0.10808803141117096, acc: 0.9736841917037964)
[2025-02-16 13:26:08,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:09,157][root][INFO] - Training Epoch: 1/2, step 12046/23838 completed (loss: 0.2421594113111496, acc: 0.9523809552192688)
[2025-02-16 13:26:09,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:09,585][root][INFO] - Training Epoch: 1/2, step 12047/23838 completed (loss: 0.39191028475761414, acc: 0.9196428656578064)
[2025-02-16 13:26:09,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:10,046][root][INFO] - Training Epoch: 1/2, step 12048/23838 completed (loss: 0.17596571147441864, acc: 0.9477124214172363)
[2025-02-16 13:26:10,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:10,456][root][INFO] - Training Epoch: 1/2, step 12049/23838 completed (loss: 0.38135746121406555, acc: 0.9027777910232544)
[2025-02-16 13:26:10,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:10,891][root][INFO] - Training Epoch: 1/2, step 12050/23838 completed (loss: 0.25663164258003235, acc: 0.9438202381134033)
[2025-02-16 13:26:11,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:11,286][root][INFO] - Training Epoch: 1/2, step 12051/23838 completed (loss: 0.3209354281425476, acc: 0.887499988079071)
[2025-02-16 13:26:11,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:11,674][root][INFO] - Training Epoch: 1/2, step 12052/23838 completed (loss: 0.14665281772613525, acc: 0.976190447807312)
[2025-02-16 13:26:11,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:12,151][root][INFO] - Training Epoch: 1/2, step 12053/23838 completed (loss: 0.522180438041687, acc: 0.868686854839325)
[2025-02-16 13:26:12,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:12,580][root][INFO] - Training Epoch: 1/2, step 12054/23838 completed (loss: 0.5145004391670227, acc: 0.8712871074676514)
[2025-02-16 13:26:12,796][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:13,029][root][INFO] - Training Epoch: 1/2, step 12055/23838 completed (loss: 0.22736790776252747, acc: 0.9395973086357117)
[2025-02-16 13:26:13,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:13,461][root][INFO] - Training Epoch: 1/2, step 12056/23838 completed (loss: 0.3123798668384552, acc: 0.918367326259613)
[2025-02-16 13:26:13,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:13,920][root][INFO] - Training Epoch: 1/2, step 12057/23838 completed (loss: 0.25518184900283813, acc: 0.9333333373069763)
[2025-02-16 13:26:14,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:14,336][root][INFO] - Training Epoch: 1/2, step 12058/23838 completed (loss: 0.6201796531677246, acc: 0.796875)
[2025-02-16 13:26:14,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:14,813][root][INFO] - Training Epoch: 1/2, step 12059/23838 completed (loss: 0.8094443678855896, acc: 0.7758620977401733)
[2025-02-16 13:26:15,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:15,283][root][INFO] - Training Epoch: 1/2, step 12060/23838 completed (loss: 1.0781395435333252, acc: 0.7083333134651184)
[2025-02-16 13:26:15,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:15,764][root][INFO] - Training Epoch: 1/2, step 12061/23838 completed (loss: 0.3862837851047516, acc: 0.8961748480796814)
[2025-02-16 13:26:15,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:16,194][root][INFO] - Training Epoch: 1/2, step 12062/23838 completed (loss: 0.3702189028263092, acc: 0.9117646813392639)
[2025-02-16 13:26:16,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:16,617][root][INFO] - Training Epoch: 1/2, step 12063/23838 completed (loss: 0.8908048272132874, acc: 0.746835470199585)
[2025-02-16 13:26:16,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:17,050][root][INFO] - Training Epoch: 1/2, step 12064/23838 completed (loss: 0.5095734596252441, acc: 0.8829787373542786)
[2025-02-16 13:26:17,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:17,457][root][INFO] - Training Epoch: 1/2, step 12065/23838 completed (loss: 0.8804366588592529, acc: 0.75)
[2025-02-16 13:26:17,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:17,878][root][INFO] - Training Epoch: 1/2, step 12066/23838 completed (loss: 0.7661315202713013, acc: 0.8095238208770752)
[2025-02-16 13:26:18,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:18,331][root][INFO] - Training Epoch: 1/2, step 12067/23838 completed (loss: 0.359065979719162, acc: 0.8878504633903503)
[2025-02-16 13:26:18,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:18,794][root][INFO] - Training Epoch: 1/2, step 12068/23838 completed (loss: 0.3849320709705353, acc: 0.8787878751754761)
[2025-02-16 13:26:18,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:19,272][root][INFO] - Training Epoch: 1/2, step 12069/23838 completed (loss: 0.3464130759239197, acc: 0.9047619104385376)
[2025-02-16 13:26:19,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:19,682][root][INFO] - Training Epoch: 1/2, step 12070/23838 completed (loss: 0.24253106117248535, acc: 0.9550561904907227)
[2025-02-16 13:26:19,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:20,050][root][INFO] - Training Epoch: 1/2, step 12071/23838 completed (loss: 0.9562661051750183, acc: 0.7368420958518982)
[2025-02-16 13:26:20,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:20,474][root][INFO] - Training Epoch: 1/2, step 12072/23838 completed (loss: 0.23689071834087372, acc: 0.9255319237709045)
[2025-02-16 13:26:20,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:20,918][root][INFO] - Training Epoch: 1/2, step 12073/23838 completed (loss: 0.710140585899353, acc: 0.8333333134651184)
[2025-02-16 13:26:21,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:21,292][root][INFO] - Training Epoch: 1/2, step 12074/23838 completed (loss: 0.32908767461776733, acc: 0.9052631855010986)
[2025-02-16 13:26:21,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:21,733][root][INFO] - Training Epoch: 1/2, step 12075/23838 completed (loss: 0.5637427568435669, acc: 0.8214285969734192)
[2025-02-16 13:26:21,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:22,106][root][INFO] - Training Epoch: 1/2, step 12076/23838 completed (loss: 0.5493142008781433, acc: 0.8387096524238586)
[2025-02-16 13:26:22,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:22,544][root][INFO] - Training Epoch: 1/2, step 12077/23838 completed (loss: 0.39856964349746704, acc: 0.9220778942108154)
[2025-02-16 13:26:22,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:22,960][root][INFO] - Training Epoch: 1/2, step 12078/23838 completed (loss: 0.3979470431804657, acc: 0.8737863898277283)
[2025-02-16 13:26:23,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:23,433][root][INFO] - Training Epoch: 1/2, step 12079/23838 completed (loss: 0.07386529445648193, acc: 1.0)
[2025-02-16 13:26:23,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:23,997][root][INFO] - Training Epoch: 1/2, step 12080/23838 completed (loss: 0.39823997020721436, acc: 0.9086538553237915)
[2025-02-16 13:26:24,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:24,329][root][INFO] - Training Epoch: 1/2, step 12081/23838 completed (loss: 0.5249007344245911, acc: 0.8829787373542786)
[2025-02-16 13:26:24,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:24,764][root][INFO] - Training Epoch: 1/2, step 12082/23838 completed (loss: 0.23926445841789246, acc: 0.9324324131011963)
[2025-02-16 13:26:24,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:25,216][root][INFO] - Training Epoch: 1/2, step 12083/23838 completed (loss: 0.1072557121515274, acc: 0.9793814420700073)
[2025-02-16 13:26:25,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:25,649][root][INFO] - Training Epoch: 1/2, step 12084/23838 completed (loss: 0.29929041862487793, acc: 0.9193548560142517)
[2025-02-16 13:26:25,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:26,045][root][INFO] - Training Epoch: 1/2, step 12085/23838 completed (loss: 0.2839212715625763, acc: 0.904347836971283)
[2025-02-16 13:26:26,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:26,506][root][INFO] - Training Epoch: 1/2, step 12086/23838 completed (loss: 0.3650817573070526, acc: 0.9056603908538818)
[2025-02-16 13:26:26,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:26,943][root][INFO] - Training Epoch: 1/2, step 12087/23838 completed (loss: 0.3450554311275482, acc: 0.9180327653884888)
[2025-02-16 13:26:27,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:27,368][root][INFO] - Training Epoch: 1/2, step 12088/23838 completed (loss: 0.3261081278324127, acc: 0.918367326259613)
[2025-02-16 13:26:27,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:27,827][root][INFO] - Training Epoch: 1/2, step 12089/23838 completed (loss: 0.25555434823036194, acc: 0.9428571462631226)
[2025-02-16 13:26:27,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:28,227][root][INFO] - Training Epoch: 1/2, step 12090/23838 completed (loss: 0.32016509771347046, acc: 0.914893627166748)
[2025-02-16 13:26:28,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:28,667][root][INFO] - Training Epoch: 1/2, step 12091/23838 completed (loss: 1.0103096961975098, acc: 0.7263157963752747)
[2025-02-16 13:26:28,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:29,154][root][INFO] - Training Epoch: 1/2, step 12092/23838 completed (loss: 0.16399234533309937, acc: 0.9457364082336426)
[2025-02-16 13:26:29,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:29,583][root][INFO] - Training Epoch: 1/2, step 12093/23838 completed (loss: 0.2136816531419754, acc: 0.9375)
[2025-02-16 13:26:29,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:30,044][root][INFO] - Training Epoch: 1/2, step 12094/23838 completed (loss: 0.45952731370925903, acc: 0.8938053250312805)
[2025-02-16 13:26:30,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:30,443][root][INFO] - Training Epoch: 1/2, step 12095/23838 completed (loss: 0.5360371470451355, acc: 0.8440366983413696)
[2025-02-16 13:26:30,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:30,866][root][INFO] - Training Epoch: 1/2, step 12096/23838 completed (loss: 0.3908173143863678, acc: 0.89570552110672)
[2025-02-16 13:26:31,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:31,272][root][INFO] - Training Epoch: 1/2, step 12097/23838 completed (loss: 0.332599401473999, acc: 0.891566276550293)
[2025-02-16 13:26:31,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:31,681][root][INFO] - Training Epoch: 1/2, step 12098/23838 completed (loss: 0.930874764919281, acc: 0.7708333134651184)
[2025-02-16 13:26:31,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:32,069][root][INFO] - Training Epoch: 1/2, step 12099/23838 completed (loss: 1.0969815254211426, acc: 0.75)
[2025-02-16 13:26:32,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:32,495][root][INFO] - Training Epoch: 1/2, step 12100/23838 completed (loss: 0.38462525606155396, acc: 0.9240506291389465)
[2025-02-16 13:26:32,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:32,923][root][INFO] - Training Epoch: 1/2, step 12101/23838 completed (loss: 0.5743883848190308, acc: 0.8651685118675232)
[2025-02-16 13:26:33,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:33,360][root][INFO] - Training Epoch: 1/2, step 12102/23838 completed (loss: 1.236680507659912, acc: 0.6627907156944275)
[2025-02-16 13:26:33,580][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:33,821][root][INFO] - Training Epoch: 1/2, step 12103/23838 completed (loss: 0.29059597849845886, acc: 0.9100000262260437)
[2025-02-16 13:26:34,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:34,227][root][INFO] - Training Epoch: 1/2, step 12104/23838 completed (loss: 0.7440128326416016, acc: 0.7857142686843872)
[2025-02-16 13:26:34,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:34,661][root][INFO] - Training Epoch: 1/2, step 12105/23838 completed (loss: 0.7252562642097473, acc: 0.8541666865348816)
[2025-02-16 13:26:34,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:35,093][root][INFO] - Training Epoch: 1/2, step 12106/23838 completed (loss: 0.5181261301040649, acc: 0.8632478713989258)
[2025-02-16 13:26:35,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:35,512][root][INFO] - Training Epoch: 1/2, step 12107/23838 completed (loss: 0.3228473365306854, acc: 0.9272727370262146)
[2025-02-16 13:26:35,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:35,994][root][INFO] - Training Epoch: 1/2, step 12108/23838 completed (loss: 0.7486875653266907, acc: 0.8051947951316833)
[2025-02-16 13:26:36,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:36,465][root][INFO] - Training Epoch: 1/2, step 12109/23838 completed (loss: 0.39391249418258667, acc: 0.8872180581092834)
[2025-02-16 13:26:36,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:36,833][root][INFO] - Training Epoch: 1/2, step 12110/23838 completed (loss: 0.37922435998916626, acc: 0.9019607901573181)
[2025-02-16 13:26:36,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:37,225][root][INFO] - Training Epoch: 1/2, step 12111/23838 completed (loss: 0.3684428036212921, acc: 0.8977272510528564)
[2025-02-16 13:26:37,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:37,656][root][INFO] - Training Epoch: 1/2, step 12112/23838 completed (loss: 0.7613488435745239, acc: 0.7814569473266602)
[2025-02-16 13:26:37,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:38,074][root][INFO] - Training Epoch: 1/2, step 12113/23838 completed (loss: 1.0443370342254639, acc: 0.7377049326896667)
[2025-02-16 13:26:38,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:38,509][root][INFO] - Training Epoch: 1/2, step 12114/23838 completed (loss: 0.31437423825263977, acc: 0.9014084339141846)
[2025-02-16 13:26:38,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:38,927][root][INFO] - Training Epoch: 1/2, step 12115/23838 completed (loss: 0.1802685558795929, acc: 0.9420289993286133)
[2025-02-16 13:26:39,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:39,365][root][INFO] - Training Epoch: 1/2, step 12116/23838 completed (loss: 0.4831136465072632, acc: 0.8522727489471436)
[2025-02-16 13:26:39,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:39,859][root][INFO] - Training Epoch: 1/2, step 12117/23838 completed (loss: 0.23270434141159058, acc: 0.9449541568756104)
[2025-02-16 13:26:40,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:40,345][root][INFO] - Training Epoch: 1/2, step 12118/23838 completed (loss: 0.12873061001300812, acc: 0.9722222089767456)
[2025-02-16 13:26:40,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:40,817][root][INFO] - Training Epoch: 1/2, step 12119/23838 completed (loss: 0.38846555352211, acc: 0.9076923131942749)
[2025-02-16 13:26:41,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:41,276][root][INFO] - Training Epoch: 1/2, step 12120/23838 completed (loss: 0.14490582048892975, acc: 0.9576271176338196)
[2025-02-16 13:26:41,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:41,786][root][INFO] - Training Epoch: 1/2, step 12121/23838 completed (loss: 0.2977595627307892, acc: 0.8960000276565552)
[2025-02-16 13:26:42,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:42,237][root][INFO] - Training Epoch: 1/2, step 12122/23838 completed (loss: 0.436408132314682, acc: 0.8769230842590332)
[2025-02-16 13:26:42,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:42,671][root][INFO] - Training Epoch: 1/2, step 12123/23838 completed (loss: 0.4282839894294739, acc: 0.8947368264198303)
[2025-02-16 13:26:43,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:43,495][root][INFO] - Training Epoch: 1/2, step 12124/23838 completed (loss: 0.3105909824371338, acc: 0.9117646813392639)
[2025-02-16 13:26:43,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:43,911][root][INFO] - Training Epoch: 1/2, step 12125/23838 completed (loss: 0.22411222755908966, acc: 0.9369369149208069)
[2025-02-16 13:26:44,126][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:44,347][root][INFO] - Training Epoch: 1/2, step 12126/23838 completed (loss: 0.684525728225708, acc: 0.800000011920929)
[2025-02-16 13:26:44,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:44,771][root][INFO] - Training Epoch: 1/2, step 12127/23838 completed (loss: 0.34936514496803284, acc: 0.9078947305679321)
[2025-02-16 13:26:45,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:45,219][root][INFO] - Training Epoch: 1/2, step 12128/23838 completed (loss: 0.4416067898273468, acc: 0.8507462739944458)
[2025-02-16 13:26:45,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:45,676][root][INFO] - Training Epoch: 1/2, step 12129/23838 completed (loss: 0.5114020705223083, acc: 0.8095238208770752)
[2025-02-16 13:26:45,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:46,287][root][INFO] - Training Epoch: 1/2, step 12130/23838 completed (loss: 0.3757907450199127, acc: 0.8954248428344727)
[2025-02-16 13:26:46,511][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:46,751][root][INFO] - Training Epoch: 1/2, step 12131/23838 completed (loss: 0.6621360778808594, acc: 0.8545454740524292)
[2025-02-16 13:26:46,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:47,208][root][INFO] - Training Epoch: 1/2, step 12132/23838 completed (loss: 0.35287609696388245, acc: 0.8860759735107422)
[2025-02-16 13:26:47,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:47,676][root][INFO] - Training Epoch: 1/2, step 12133/23838 completed (loss: 0.25228771567344666, acc: 0.9279279112815857)
[2025-02-16 13:26:47,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:48,150][root][INFO] - Training Epoch: 1/2, step 12134/23838 completed (loss: 0.3644346594810486, acc: 0.9193548560142517)
[2025-02-16 13:26:48,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:48,582][root][INFO] - Training Epoch: 1/2, step 12135/23838 completed (loss: 0.5107637047767639, acc: 0.8585858345031738)
[2025-02-16 13:26:48,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:48,983][root][INFO] - Training Epoch: 1/2, step 12136/23838 completed (loss: 0.5010559558868408, acc: 0.8799999952316284)
[2025-02-16 13:26:49,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:49,418][root][INFO] - Training Epoch: 1/2, step 12137/23838 completed (loss: 0.33709320425987244, acc: 0.9154929518699646)
[2025-02-16 13:26:49,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:49,872][root][INFO] - Training Epoch: 1/2, step 12138/23838 completed (loss: 0.2671542167663574, acc: 0.9126983880996704)
[2025-02-16 13:26:50,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:50,277][root][INFO] - Training Epoch: 1/2, step 12139/23838 completed (loss: 0.3917279541492462, acc: 0.8867924809455872)
[2025-02-16 13:26:50,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:50,670][root][INFO] - Training Epoch: 1/2, step 12140/23838 completed (loss: 0.6356046199798584, acc: 0.8470588326454163)
[2025-02-16 13:26:50,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:51,151][root][INFO] - Training Epoch: 1/2, step 12141/23838 completed (loss: 0.1476733535528183, acc: 0.9714285731315613)
[2025-02-16 13:26:51,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:51,578][root][INFO] - Training Epoch: 1/2, step 12142/23838 completed (loss: 0.17982544004917145, acc: 0.939393937587738)
[2025-02-16 13:26:51,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:52,047][root][INFO] - Training Epoch: 1/2, step 12143/23838 completed (loss: 0.3343324363231659, acc: 0.9191918969154358)
[2025-02-16 13:26:52,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:52,533][root][INFO] - Training Epoch: 1/2, step 12144/23838 completed (loss: 0.5964996218681335, acc: 0.8495575189590454)
[2025-02-16 13:26:52,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:52,933][root][INFO] - Training Epoch: 1/2, step 12145/23838 completed (loss: 0.34511950612068176, acc: 0.8644067645072937)
[2025-02-16 13:26:53,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:53,476][root][INFO] - Training Epoch: 1/2, step 12146/23838 completed (loss: 0.30408284068107605, acc: 0.9256756901741028)
[2025-02-16 13:26:53,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:53,946][root][INFO] - Training Epoch: 1/2, step 12147/23838 completed (loss: 0.4380185902118683, acc: 0.8878504633903503)
[2025-02-16 13:26:54,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:54,357][root][INFO] - Training Epoch: 1/2, step 12148/23838 completed (loss: 0.277960866689682, acc: 0.918367326259613)
[2025-02-16 13:26:54,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:54,763][root][INFO] - Training Epoch: 1/2, step 12149/23838 completed (loss: 0.24492648243904114, acc: 0.9610389471054077)
[2025-02-16 13:26:54,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:55,166][root][INFO] - Training Epoch: 1/2, step 12150/23838 completed (loss: 0.19642092287540436, acc: 0.9365853667259216)
[2025-02-16 13:26:55,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:55,589][root][INFO] - Training Epoch: 1/2, step 12151/23838 completed (loss: 0.2810317575931549, acc: 0.931034505367279)
[2025-02-16 13:26:55,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:56,181][root][INFO] - Training Epoch: 1/2, step 12152/23838 completed (loss: 0.3272041380405426, acc: 0.9176470637321472)
[2025-02-16 13:26:56,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:56,580][root][INFO] - Training Epoch: 1/2, step 12153/23838 completed (loss: 0.7339351177215576, acc: 0.795918345451355)
[2025-02-16 13:26:56,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:57,027][root][INFO] - Training Epoch: 1/2, step 12154/23838 completed (loss: 0.3038519024848938, acc: 0.9230769276618958)
[2025-02-16 13:26:57,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:57,509][root][INFO] - Training Epoch: 1/2, step 12155/23838 completed (loss: 0.3705732226371765, acc: 0.8712871074676514)
[2025-02-16 13:26:57,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:57,955][root][INFO] - Training Epoch: 1/2, step 12156/23838 completed (loss: 0.3498947322368622, acc: 0.9083333611488342)
[2025-02-16 13:26:58,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:58,441][root][INFO] - Training Epoch: 1/2, step 12157/23838 completed (loss: 0.4154641926288605, acc: 0.8846153616905212)
[2025-02-16 13:26:58,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:58,866][root][INFO] - Training Epoch: 1/2, step 12158/23838 completed (loss: 0.3232802748680115, acc: 0.8909090757369995)
[2025-02-16 13:26:59,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:59,292][root][INFO] - Training Epoch: 1/2, step 12159/23838 completed (loss: 0.36217328906059265, acc: 0.9350649118423462)
[2025-02-16 13:26:59,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:26:59,712][root][INFO] - Training Epoch: 1/2, step 12160/23838 completed (loss: 0.2837721109390259, acc: 0.9230769276618958)
[2025-02-16 13:26:59,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:00,190][root][INFO] - Training Epoch: 1/2, step 12161/23838 completed (loss: 0.12816114723682404, acc: 0.970588207244873)
[2025-02-16 13:27:00,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:00,753][root][INFO] - Training Epoch: 1/2, step 12162/23838 completed (loss: 0.24760329723358154, acc: 0.9181286692619324)
[2025-02-16 13:27:00,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:01,273][root][INFO] - Training Epoch: 1/2, step 12163/23838 completed (loss: 0.25187918543815613, acc: 0.9270833134651184)
[2025-02-16 13:27:01,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:01,723][root][INFO] - Training Epoch: 1/2, step 12164/23838 completed (loss: 0.5168939232826233, acc: 0.9083333611488342)
[2025-02-16 13:27:02,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:02,426][root][INFO] - Training Epoch: 1/2, step 12165/23838 completed (loss: 0.2502855360507965, acc: 0.9141104221343994)
[2025-02-16 13:27:02,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:02,888][root][INFO] - Training Epoch: 1/2, step 12166/23838 completed (loss: 0.2975671589374542, acc: 0.9259259104728699)
[2025-02-16 13:27:03,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:03,327][root][INFO] - Training Epoch: 1/2, step 12167/23838 completed (loss: 0.47426488995552063, acc: 0.8536585569381714)
[2025-02-16 13:27:03,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:03,706][root][INFO] - Training Epoch: 1/2, step 12168/23838 completed (loss: 0.45271432399749756, acc: 0.8734177350997925)
[2025-02-16 13:27:03,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:04,075][root][INFO] - Training Epoch: 1/2, step 12169/23838 completed (loss: 0.2724650204181671, acc: 0.9160839319229126)
[2025-02-16 13:27:04,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:04,533][root][INFO] - Training Epoch: 1/2, step 12170/23838 completed (loss: 0.28102245926856995, acc: 0.926174521446228)
[2025-02-16 13:27:04,765][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:04,994][root][INFO] - Training Epoch: 1/2, step 12171/23838 completed (loss: 0.47433480620384216, acc: 0.8585858345031738)
[2025-02-16 13:27:05,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:05,375][root][INFO] - Training Epoch: 1/2, step 12172/23838 completed (loss: 0.209097221493721, acc: 0.9444444179534912)
[2025-02-16 13:27:05,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:05,805][root][INFO] - Training Epoch: 1/2, step 12173/23838 completed (loss: 0.905976414680481, acc: 0.7704917788505554)
[2025-02-16 13:27:06,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:06,234][root][INFO] - Training Epoch: 1/2, step 12174/23838 completed (loss: 0.1432800590991974, acc: 0.9663865566253662)
[2025-02-16 13:27:06,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:06,660][root][INFO] - Training Epoch: 1/2, step 12175/23838 completed (loss: 0.5995610952377319, acc: 0.8301886916160583)
[2025-02-16 13:27:06,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:07,064][root][INFO] - Training Epoch: 1/2, step 12176/23838 completed (loss: 0.40247541666030884, acc: 0.8849557638168335)
[2025-02-16 13:27:07,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:07,427][root][INFO] - Training Epoch: 1/2, step 12177/23838 completed (loss: 0.35290613770484924, acc: 0.9078947305679321)
[2025-02-16 13:27:07,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:07,864][root][INFO] - Training Epoch: 1/2, step 12178/23838 completed (loss: 0.3612789213657379, acc: 0.8787878751754761)
[2025-02-16 13:27:08,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:08,273][root][INFO] - Training Epoch: 1/2, step 12179/23838 completed (loss: 0.43595704436302185, acc: 0.8484848737716675)
[2025-02-16 13:27:08,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:08,709][root][INFO] - Training Epoch: 1/2, step 12180/23838 completed (loss: 0.6574660539627075, acc: 0.8904109597206116)
[2025-02-16 13:27:08,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:09,145][root][INFO] - Training Epoch: 1/2, step 12181/23838 completed (loss: 0.7585548162460327, acc: 0.8269230723381042)
[2025-02-16 13:27:09,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:09,617][root][INFO] - Training Epoch: 1/2, step 12182/23838 completed (loss: 0.8703822493553162, acc: 0.78125)
[2025-02-16 13:27:09,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:10,090][root][INFO] - Training Epoch: 1/2, step 12183/23838 completed (loss: 0.1574818193912506, acc: 0.9666666388511658)
[2025-02-16 13:27:10,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:10,501][root][INFO] - Training Epoch: 1/2, step 12184/23838 completed (loss: 0.4831636846065521, acc: 0.8636363744735718)
[2025-02-16 13:27:10,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:10,904][root][INFO] - Training Epoch: 1/2, step 12185/23838 completed (loss: 0.13843034207820892, acc: 0.9677419066429138)
[2025-02-16 13:27:11,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:11,318][root][INFO] - Training Epoch: 1/2, step 12186/23838 completed (loss: 0.34231919050216675, acc: 0.9037036895751953)
[2025-02-16 13:27:11,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:11,805][root][INFO] - Training Epoch: 1/2, step 12187/23838 completed (loss: 0.39032310247421265, acc: 0.9306930899620056)
[2025-02-16 13:27:12,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:12,273][root][INFO] - Training Epoch: 1/2, step 12188/23838 completed (loss: 0.5674455761909485, acc: 0.8659217953681946)
[2025-02-16 13:27:12,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:12,717][root][INFO] - Training Epoch: 1/2, step 12189/23838 completed (loss: 0.4741915464401245, acc: 0.8583333492279053)
[2025-02-16 13:27:12,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:13,164][root][INFO] - Training Epoch: 1/2, step 12190/23838 completed (loss: 0.2847690284252167, acc: 0.9069767594337463)
[2025-02-16 13:27:13,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:13,650][root][INFO] - Training Epoch: 1/2, step 12191/23838 completed (loss: 0.35038095712661743, acc: 0.9277108311653137)
[2025-02-16 13:27:13,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:14,105][root][INFO] - Training Epoch: 1/2, step 12192/23838 completed (loss: 0.5807797908782959, acc: 0.8472222089767456)
[2025-02-16 13:27:14,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:14,648][root][INFO] - Training Epoch: 1/2, step 12193/23838 completed (loss: 0.6271911859512329, acc: 0.834482729434967)
[2025-02-16 13:27:14,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:15,072][root][INFO] - Training Epoch: 1/2, step 12194/23838 completed (loss: 0.6950476169586182, acc: 0.7916666865348816)
[2025-02-16 13:27:15,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:15,438][root][INFO] - Training Epoch: 1/2, step 12195/23838 completed (loss: 0.5931949019432068, acc: 0.8105263113975525)
[2025-02-16 13:27:15,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:15,835][root][INFO] - Training Epoch: 1/2, step 12196/23838 completed (loss: 0.439181387424469, acc: 0.8818181753158569)
[2025-02-16 13:27:16,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:16,305][root][INFO] - Training Epoch: 1/2, step 12197/23838 completed (loss: 0.3763015568256378, acc: 0.8715596199035645)
[2025-02-16 13:27:16,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:16,737][root][INFO] - Training Epoch: 1/2, step 12198/23838 completed (loss: 0.10905532538890839, acc: 0.970588207244873)
[2025-02-16 13:27:16,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:17,155][root][INFO] - Training Epoch: 1/2, step 12199/23838 completed (loss: 0.9081747531890869, acc: 0.7323943376541138)
[2025-02-16 13:27:17,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:17,661][root][INFO] - Training Epoch: 1/2, step 12200/23838 completed (loss: 0.20654462277889252, acc: 0.929411768913269)
[2025-02-16 13:27:17,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:18,030][root][INFO] - Training Epoch: 1/2, step 12201/23838 completed (loss: 0.1708337813615799, acc: 0.953125)
[2025-02-16 13:27:18,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:18,413][root][INFO] - Training Epoch: 1/2, step 12202/23838 completed (loss: 0.4054829478263855, acc: 0.9173553586006165)
[2025-02-16 13:27:18,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:18,804][root][INFO] - Training Epoch: 1/2, step 12203/23838 completed (loss: 0.2303248792886734, acc: 0.9384615421295166)
[2025-02-16 13:27:18,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:19,190][root][INFO] - Training Epoch: 1/2, step 12204/23838 completed (loss: 0.5274935364723206, acc: 0.8790322542190552)
[2025-02-16 13:27:19,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:19,578][root][INFO] - Training Epoch: 1/2, step 12205/23838 completed (loss: 0.08946342766284943, acc: 0.9801324605941772)
[2025-02-16 13:27:19,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:19,962][root][INFO] - Training Epoch: 1/2, step 12206/23838 completed (loss: 0.22601884603500366, acc: 0.9175257682800293)
[2025-02-16 13:27:20,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:20,354][root][INFO] - Training Epoch: 1/2, step 12207/23838 completed (loss: 0.5035586357116699, acc: 0.8823529481887817)
[2025-02-16 13:27:20,575][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:20,826][root][INFO] - Training Epoch: 1/2, step 12208/23838 completed (loss: 0.3102949261665344, acc: 0.9385964870452881)
[2025-02-16 13:27:20,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:21,212][root][INFO] - Training Epoch: 1/2, step 12209/23838 completed (loss: 0.3933345675468445, acc: 0.8977272510528564)
[2025-02-16 13:27:21,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:21,656][root][INFO] - Training Epoch: 1/2, step 12210/23838 completed (loss: 0.42276790738105774, acc: 0.9058823585510254)
[2025-02-16 13:27:21,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:22,069][root][INFO] - Training Epoch: 1/2, step 12211/23838 completed (loss: 0.3179517686367035, acc: 0.9012345671653748)
[2025-02-16 13:27:22,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:22,559][root][INFO] - Training Epoch: 1/2, step 12212/23838 completed (loss: 0.01861513964831829, acc: 1.0)
[2025-02-16 13:27:22,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:22,994][root][INFO] - Training Epoch: 1/2, step 12213/23838 completed (loss: 0.14535172283649445, acc: 0.9696969985961914)
[2025-02-16 13:27:23,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:23,423][root][INFO] - Training Epoch: 1/2, step 12214/23838 completed (loss: 0.4572307765483856, acc: 0.8936170339584351)
[2025-02-16 13:27:23,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:23,845][root][INFO] - Training Epoch: 1/2, step 12215/23838 completed (loss: 0.48555096983909607, acc: 0.8764045238494873)
[2025-02-16 13:27:24,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:24,235][root][INFO] - Training Epoch: 1/2, step 12216/23838 completed (loss: 0.22801651060581207, acc: 0.9382715821266174)
[2025-02-16 13:27:24,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:24,618][root][INFO] - Training Epoch: 1/2, step 12217/23838 completed (loss: 0.41376233100891113, acc: 0.891566276550293)
[2025-02-16 13:27:24,782][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:25,007][root][INFO] - Training Epoch: 1/2, step 12218/23838 completed (loss: 0.4228576421737671, acc: 0.8780487775802612)
[2025-02-16 13:27:25,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:25,423][root][INFO] - Training Epoch: 1/2, step 12219/23838 completed (loss: 0.31446316838264465, acc: 0.8846153616905212)
[2025-02-16 13:27:25,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:25,926][root][INFO] - Training Epoch: 1/2, step 12220/23838 completed (loss: 0.14007866382598877, acc: 0.9586777091026306)
[2025-02-16 13:27:26,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:26,371][root][INFO] - Training Epoch: 1/2, step 12221/23838 completed (loss: 0.3274350166320801, acc: 0.9142857193946838)
[2025-02-16 13:27:26,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:26,836][root][INFO] - Training Epoch: 1/2, step 12222/23838 completed (loss: 0.25080031156539917, acc: 0.9534883499145508)
[2025-02-16 13:27:27,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:27,257][root][INFO] - Training Epoch: 1/2, step 12223/23838 completed (loss: 0.4545688033103943, acc: 0.8659793734550476)
[2025-02-16 13:27:27,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:27,641][root][INFO] - Training Epoch: 1/2, step 12224/23838 completed (loss: 0.13255637884140015, acc: 0.9626865386962891)
[2025-02-16 13:27:27,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:28,087][root][INFO] - Training Epoch: 1/2, step 12225/23838 completed (loss: 0.6161948442459106, acc: 0.8571428656578064)
[2025-02-16 13:27:28,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:28,447][root][INFO] - Training Epoch: 1/2, step 12226/23838 completed (loss: 0.6474717855453491, acc: 0.8157894611358643)
[2025-02-16 13:27:28,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:28,901][root][INFO] - Training Epoch: 1/2, step 12227/23838 completed (loss: 0.344875693321228, acc: 0.9238095283508301)
[2025-02-16 13:27:29,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:29,281][root][INFO] - Training Epoch: 1/2, step 12228/23838 completed (loss: 0.1480656862258911, acc: 0.9680851101875305)
[2025-02-16 13:27:29,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:29,725][root][INFO] - Training Epoch: 1/2, step 12229/23838 completed (loss: 0.3879529535770416, acc: 0.8761904835700989)
[2025-02-16 13:27:29,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:30,149][root][INFO] - Training Epoch: 1/2, step 12230/23838 completed (loss: 0.04227682203054428, acc: 1.0)
[2025-02-16 13:27:30,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:30,638][root][INFO] - Training Epoch: 1/2, step 12231/23838 completed (loss: 0.20446424186229706, acc: 0.9437229633331299)
[2025-02-16 13:27:30,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:31,101][root][INFO] - Training Epoch: 1/2, step 12232/23838 completed (loss: 0.4131719172000885, acc: 0.9008264541625977)
[2025-02-16 13:27:31,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:31,535][root][INFO] - Training Epoch: 1/2, step 12233/23838 completed (loss: 0.23353199660778046, acc: 0.9320987462997437)
[2025-02-16 13:27:31,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:31,979][root][INFO] - Training Epoch: 1/2, step 12234/23838 completed (loss: 0.7043022513389587, acc: 0.7701149582862854)
[2025-02-16 13:27:32,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:32,396][root][INFO] - Training Epoch: 1/2, step 12235/23838 completed (loss: 0.33491334319114685, acc: 0.8695651888847351)
[2025-02-16 13:27:32,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:32,866][root][INFO] - Training Epoch: 1/2, step 12236/23838 completed (loss: 0.44754526019096375, acc: 0.9007633328437805)
[2025-02-16 13:27:33,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:33,323][root][INFO] - Training Epoch: 1/2, step 12237/23838 completed (loss: 0.3750694692134857, acc: 0.9099099040031433)
[2025-02-16 13:27:33,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:33,823][root][INFO] - Training Epoch: 1/2, step 12238/23838 completed (loss: 0.4227837920188904, acc: 0.875)
[2025-02-16 13:27:34,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:34,274][root][INFO] - Training Epoch: 1/2, step 12239/23838 completed (loss: 0.7745966911315918, acc: 0.760869562625885)
[2025-02-16 13:27:34,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:34,733][root][INFO] - Training Epoch: 1/2, step 12240/23838 completed (loss: 0.16861160099506378, acc: 0.9588235020637512)
[2025-02-16 13:27:34,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:35,199][root][INFO] - Training Epoch: 1/2, step 12241/23838 completed (loss: 0.26649361848831177, acc: 0.9235293865203857)
[2025-02-16 13:27:35,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:35,597][root][INFO] - Training Epoch: 1/2, step 12242/23838 completed (loss: 0.3127962648868561, acc: 0.9230769276618958)
[2025-02-16 13:27:35,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:36,016][root][INFO] - Training Epoch: 1/2, step 12243/23838 completed (loss: 0.13926365971565247, acc: 0.948051929473877)
[2025-02-16 13:27:36,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:36,441][root][INFO] - Training Epoch: 1/2, step 12244/23838 completed (loss: 0.09606462717056274, acc: 0.9605262875556946)
[2025-02-16 13:27:36,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:36,877][root][INFO] - Training Epoch: 1/2, step 12245/23838 completed (loss: 0.24593494832515717, acc: 0.9256198406219482)
[2025-02-16 13:27:37,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:37,268][root][INFO] - Training Epoch: 1/2, step 12246/23838 completed (loss: 0.3346318304538727, acc: 0.9289340376853943)
[2025-02-16 13:27:37,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:37,728][root][INFO] - Training Epoch: 1/2, step 12247/23838 completed (loss: 0.3365807831287384, acc: 0.9178082346916199)
[2025-02-16 13:27:37,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:38,174][root][INFO] - Training Epoch: 1/2, step 12248/23838 completed (loss: 0.16052629053592682, acc: 0.9459459185600281)
[2025-02-16 13:27:38,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:38,620][root][INFO] - Training Epoch: 1/2, step 12249/23838 completed (loss: 0.40571799874305725, acc: 0.9102563858032227)
[2025-02-16 13:27:38,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:39,077][root][INFO] - Training Epoch: 1/2, step 12250/23838 completed (loss: 0.20831060409545898, acc: 0.9411764740943909)
[2025-02-16 13:27:39,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:39,530][root][INFO] - Training Epoch: 1/2, step 12251/23838 completed (loss: 0.2785632908344269, acc: 0.9015151262283325)
[2025-02-16 13:27:39,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:39,976][root][INFO] - Training Epoch: 1/2, step 12252/23838 completed (loss: 0.30951663851737976, acc: 0.9298245906829834)
[2025-02-16 13:27:40,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:40,414][root][INFO] - Training Epoch: 1/2, step 12253/23838 completed (loss: 0.2571641206741333, acc: 0.9312977194786072)
[2025-02-16 13:27:40,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:40,908][root][INFO] - Training Epoch: 1/2, step 12254/23838 completed (loss: 0.20125946402549744, acc: 0.9604519605636597)
[2025-02-16 13:27:41,163][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:41,395][root][INFO] - Training Epoch: 1/2, step 12255/23838 completed (loss: 0.47004184126853943, acc: 0.8615384697914124)
[2025-02-16 13:27:41,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:41,782][root][INFO] - Training Epoch: 1/2, step 12256/23838 completed (loss: 0.6605612635612488, acc: 0.8333333134651184)
[2025-02-16 13:27:41,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:42,222][root][INFO] - Training Epoch: 1/2, step 12257/23838 completed (loss: 0.14045917987823486, acc: 0.9555555582046509)
[2025-02-16 13:27:42,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:42,661][root][INFO] - Training Epoch: 1/2, step 12258/23838 completed (loss: 0.45669466257095337, acc: 0.8305084705352783)
[2025-02-16 13:27:42,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:43,044][root][INFO] - Training Epoch: 1/2, step 12259/23838 completed (loss: 0.6246325373649597, acc: 0.8266666531562805)
[2025-02-16 13:27:43,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:43,479][root][INFO] - Training Epoch: 1/2, step 12260/23838 completed (loss: 0.4008328318595886, acc: 0.8500000238418579)
[2025-02-16 13:27:43,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:43,965][root][INFO] - Training Epoch: 1/2, step 12261/23838 completed (loss: 0.3794030249118805, acc: 0.8852459192276001)
[2025-02-16 13:27:44,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:44,383][root][INFO] - Training Epoch: 1/2, step 12262/23838 completed (loss: 0.4425337314605713, acc: 0.862500011920929)
[2025-02-16 13:27:44,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:44,882][root][INFO] - Training Epoch: 1/2, step 12263/23838 completed (loss: 0.4107135236263275, acc: 0.8933333158493042)
[2025-02-16 13:27:45,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:45,306][root][INFO] - Training Epoch: 1/2, step 12264/23838 completed (loss: 0.26007765531539917, acc: 0.9649122953414917)
[2025-02-16 13:27:45,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:45,767][root][INFO] - Training Epoch: 1/2, step 12265/23838 completed (loss: 0.9478554725646973, acc: 0.7532467246055603)
[2025-02-16 13:27:45,994][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:46,228][root][INFO] - Training Epoch: 1/2, step 12266/23838 completed (loss: 0.3164459466934204, acc: 0.8859649300575256)
[2025-02-16 13:27:46,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:46,646][root][INFO] - Training Epoch: 1/2, step 12267/23838 completed (loss: 0.34131938219070435, acc: 0.8709677457809448)
[2025-02-16 13:27:46,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:47,153][root][INFO] - Training Epoch: 1/2, step 12268/23838 completed (loss: 0.18487636744976044, acc: 0.9259259104728699)
[2025-02-16 13:27:47,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:47,589][root][INFO] - Training Epoch: 1/2, step 12269/23838 completed (loss: 0.16023246943950653, acc: 0.9636363387107849)
[2025-02-16 13:27:47,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:48,093][root][INFO] - Training Epoch: 1/2, step 12270/23838 completed (loss: 0.5236125588417053, acc: 0.9016393423080444)
[2025-02-16 13:27:48,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:48,470][root][INFO] - Training Epoch: 1/2, step 12271/23838 completed (loss: 0.408123254776001, acc: 0.8860759735107422)
[2025-02-16 13:27:48,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:48,895][root][INFO] - Training Epoch: 1/2, step 12272/23838 completed (loss: 0.28128689527511597, acc: 0.9468085169792175)
[2025-02-16 13:27:49,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:49,352][root][INFO] - Training Epoch: 1/2, step 12273/23838 completed (loss: 0.6741787791252136, acc: 0.7978723645210266)
[2025-02-16 13:27:49,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:49,754][root][INFO] - Training Epoch: 1/2, step 12274/23838 completed (loss: 0.5341755747795105, acc: 0.8392857313156128)
[2025-02-16 13:27:49,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:50,185][root][INFO] - Training Epoch: 1/2, step 12275/23838 completed (loss: 0.7975625991821289, acc: 0.7941176295280457)
[2025-02-16 13:27:50,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:50,576][root][INFO] - Training Epoch: 1/2, step 12276/23838 completed (loss: 1.0066806077957153, acc: 0.761904776096344)
[2025-02-16 13:27:50,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:51,042][root][INFO] - Training Epoch: 1/2, step 12277/23838 completed (loss: 0.39363110065460205, acc: 0.8571428656578064)
[2025-02-16 13:27:51,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:51,435][root][INFO] - Training Epoch: 1/2, step 12278/23838 completed (loss: 0.1519003063440323, acc: 0.9576271176338196)
[2025-02-16 13:27:51,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:51,846][root][INFO] - Training Epoch: 1/2, step 12279/23838 completed (loss: 0.47105392813682556, acc: 0.835616409778595)
[2025-02-16 13:27:52,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:52,280][root][INFO] - Training Epoch: 1/2, step 12280/23838 completed (loss: 0.6210449934005737, acc: 0.8118811845779419)
[2025-02-16 13:27:52,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:52,715][root][INFO] - Training Epoch: 1/2, step 12281/23838 completed (loss: 0.28102460503578186, acc: 0.9105691313743591)
[2025-02-16 13:27:52,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:53,153][root][INFO] - Training Epoch: 1/2, step 12282/23838 completed (loss: 0.4281874895095825, acc: 0.8823529481887817)
[2025-02-16 13:27:53,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:53,563][root][INFO] - Training Epoch: 1/2, step 12283/23838 completed (loss: 0.12708500027656555, acc: 0.9523809552192688)
[2025-02-16 13:27:53,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:54,089][root][INFO] - Training Epoch: 1/2, step 12284/23838 completed (loss: 0.36423259973526, acc: 0.8661417365074158)
[2025-02-16 13:27:54,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:54,579][root][INFO] - Training Epoch: 1/2, step 12285/23838 completed (loss: 0.45506563782691956, acc: 0.8660714030265808)
[2025-02-16 13:27:54,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:55,015][root][INFO] - Training Epoch: 1/2, step 12286/23838 completed (loss: 0.10849864035844803, acc: 0.9666666388511658)
[2025-02-16 13:27:55,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:55,469][root][INFO] - Training Epoch: 1/2, step 12287/23838 completed (loss: 0.19324898719787598, acc: 0.9583333134651184)
[2025-02-16 13:27:55,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:55,962][root][INFO] - Training Epoch: 1/2, step 12288/23838 completed (loss: 0.28382354974746704, acc: 0.949367105960846)
[2025-02-16 13:27:56,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:56,381][root][INFO] - Training Epoch: 1/2, step 12289/23838 completed (loss: 0.5746406316757202, acc: 0.8275862336158752)
[2025-02-16 13:27:56,546][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:56,798][root][INFO] - Training Epoch: 1/2, step 12290/23838 completed (loss: 0.22361503541469574, acc: 0.9363636374473572)
[2025-02-16 13:27:57,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:57,247][root][INFO] - Training Epoch: 1/2, step 12291/23838 completed (loss: 0.36622360348701477, acc: 0.8942307829856873)
[2025-02-16 13:27:57,484][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:57,724][root][INFO] - Training Epoch: 1/2, step 12292/23838 completed (loss: 0.1514309048652649, acc: 0.9603960514068604)
[2025-02-16 13:27:57,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:58,177][root][INFO] - Training Epoch: 1/2, step 12293/23838 completed (loss: 0.5547631978988647, acc: 0.8648648858070374)
[2025-02-16 13:27:58,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:58,659][root][INFO] - Training Epoch: 1/2, step 12294/23838 completed (loss: 0.37310901284217834, acc: 0.9154929518699646)
[2025-02-16 13:27:58,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:59,076][root][INFO] - Training Epoch: 1/2, step 12295/23838 completed (loss: 0.3224879801273346, acc: 0.9078947305679321)
[2025-02-16 13:27:59,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:59,508][root][INFO] - Training Epoch: 1/2, step 12296/23838 completed (loss: 1.0158098936080933, acc: 0.7547169923782349)
[2025-02-16 13:27:59,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:27:59,920][root][INFO] - Training Epoch: 1/2, step 12297/23838 completed (loss: 0.4061394929885864, acc: 0.9090909361839294)
[2025-02-16 13:28:00,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:00,390][root][INFO] - Training Epoch: 1/2, step 12298/23838 completed (loss: 0.26554006338119507, acc: 0.9224137663841248)
[2025-02-16 13:28:00,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:00,861][root][INFO] - Training Epoch: 1/2, step 12299/23838 completed (loss: 0.09018591046333313, acc: 0.9784946441650391)
[2025-02-16 13:28:01,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:01,408][root][INFO] - Training Epoch: 1/2, step 12300/23838 completed (loss: 0.41201072931289673, acc: 0.8690476417541504)
[2025-02-16 13:28:01,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:01,864][root][INFO] - Training Epoch: 1/2, step 12301/23838 completed (loss: 0.37421080470085144, acc: 0.8842105269432068)
[2025-02-16 13:28:02,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:02,257][root][INFO] - Training Epoch: 1/2, step 12302/23838 completed (loss: 0.5775816440582275, acc: 0.8217054009437561)
[2025-02-16 13:28:02,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:02,653][root][INFO] - Training Epoch: 1/2, step 12303/23838 completed (loss: 0.14205724000930786, acc: 0.9491525292396545)
[2025-02-16 13:28:02,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:03,046][root][INFO] - Training Epoch: 1/2, step 12304/23838 completed (loss: 0.23605100810527802, acc: 0.9253731369972229)
[2025-02-16 13:28:03,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:03,438][root][INFO] - Training Epoch: 1/2, step 12305/23838 completed (loss: 0.3631174862384796, acc: 0.8275862336158752)
[2025-02-16 13:28:03,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:03,862][root][INFO] - Training Epoch: 1/2, step 12306/23838 completed (loss: 0.3052881062030792, acc: 0.9230769276618958)
[2025-02-16 13:28:04,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:04,335][root][INFO] - Training Epoch: 1/2, step 12307/23838 completed (loss: 0.23697519302368164, acc: 0.9316239356994629)
[2025-02-16 13:28:04,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:04,747][root][INFO] - Training Epoch: 1/2, step 12308/23838 completed (loss: 0.6367746591567993, acc: 0.837837815284729)
[2025-02-16 13:28:04,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:05,133][root][INFO] - Training Epoch: 1/2, step 12309/23838 completed (loss: 1.0246281623840332, acc: 0.7230769395828247)
[2025-02-16 13:28:05,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:05,613][root][INFO] - Training Epoch: 1/2, step 12310/23838 completed (loss: 0.27531278133392334, acc: 0.9263157844543457)
[2025-02-16 13:28:05,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:06,044][root][INFO] - Training Epoch: 1/2, step 12311/23838 completed (loss: 0.4245518147945404, acc: 0.9090909361839294)
[2025-02-16 13:28:06,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:06,560][root][INFO] - Training Epoch: 1/2, step 12312/23838 completed (loss: 0.2872855067253113, acc: 0.929347813129425)
[2025-02-16 13:28:06,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:06,969][root][INFO] - Training Epoch: 1/2, step 12313/23838 completed (loss: 0.4594012200832367, acc: 0.849056601524353)
[2025-02-16 13:28:07,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:07,438][root][INFO] - Training Epoch: 1/2, step 12314/23838 completed (loss: 0.3970974087715149, acc: 0.9081632494926453)
[2025-02-16 13:28:07,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:07,850][root][INFO] - Training Epoch: 1/2, step 12315/23838 completed (loss: 0.45645591616630554, acc: 0.8554216623306274)
[2025-02-16 13:28:07,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:08,218][root][INFO] - Training Epoch: 1/2, step 12316/23838 completed (loss: 0.18148550391197205, acc: 0.9450549483299255)
[2025-02-16 13:28:08,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:08,629][root][INFO] - Training Epoch: 1/2, step 12317/23838 completed (loss: 0.2977476418018341, acc: 0.9178082346916199)
[2025-02-16 13:28:08,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:09,114][root][INFO] - Training Epoch: 1/2, step 12318/23838 completed (loss: 0.36257776618003845, acc: 0.9064748287200928)
[2025-02-16 13:28:09,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:09,558][root][INFO] - Training Epoch: 1/2, step 12319/23838 completed (loss: 0.3369798958301544, acc: 0.9025974273681641)
[2025-02-16 13:28:09,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:09,961][root][INFO] - Training Epoch: 1/2, step 12320/23838 completed (loss: 1.2100218534469604, acc: 0.7397260069847107)
[2025-02-16 13:28:10,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:10,409][root][INFO] - Training Epoch: 1/2, step 12321/23838 completed (loss: 0.24954377114772797, acc: 0.9661017060279846)
[2025-02-16 13:28:10,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:10,825][root][INFO] - Training Epoch: 1/2, step 12322/23838 completed (loss: 0.2958599030971527, acc: 0.9333333373069763)
[2025-02-16 13:28:11,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:11,325][root][INFO] - Training Epoch: 1/2, step 12323/23838 completed (loss: 0.34776490926742554, acc: 0.9029126167297363)
[2025-02-16 13:28:11,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:11,797][root][INFO] - Training Epoch: 1/2, step 12324/23838 completed (loss: 0.20073437690734863, acc: 0.920634925365448)
[2025-02-16 13:28:11,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:12,220][root][INFO] - Training Epoch: 1/2, step 12325/23838 completed (loss: 0.38691839575767517, acc: 0.902255654335022)
[2025-02-16 13:28:12,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:12,657][root][INFO] - Training Epoch: 1/2, step 12326/23838 completed (loss: 0.196984201669693, acc: 0.9438202381134033)
[2025-02-16 13:28:12,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:13,056][root][INFO] - Training Epoch: 1/2, step 12327/23838 completed (loss: 0.25097596645355225, acc: 0.9518072009086609)
[2025-02-16 13:28:13,271][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:13,520][root][INFO] - Training Epoch: 1/2, step 12328/23838 completed (loss: 0.1505385935306549, acc: 0.957446813583374)
[2025-02-16 13:28:13,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:14,057][root][INFO] - Training Epoch: 1/2, step 12329/23838 completed (loss: 0.35759466886520386, acc: 0.886956512928009)
[2025-02-16 13:28:14,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:14,423][root][INFO] - Training Epoch: 1/2, step 12330/23838 completed (loss: 0.1282053291797638, acc: 0.9777777791023254)
[2025-02-16 13:28:14,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:14,909][root][INFO] - Training Epoch: 1/2, step 12331/23838 completed (loss: 0.31706976890563965, acc: 0.8999999761581421)
[2025-02-16 13:28:15,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:15,318][root][INFO] - Training Epoch: 1/2, step 12332/23838 completed (loss: 0.6278437376022339, acc: 0.8125)
[2025-02-16 13:28:15,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:15,733][root][INFO] - Training Epoch: 1/2, step 12333/23838 completed (loss: 0.21287809312343597, acc: 0.9411764740943909)
[2025-02-16 13:28:15,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:16,174][root][INFO] - Training Epoch: 1/2, step 12334/23838 completed (loss: 0.2077062577009201, acc: 0.9357798099517822)
[2025-02-16 13:28:16,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:16,623][root][INFO] - Training Epoch: 1/2, step 12335/23838 completed (loss: 0.21913637220859528, acc: 0.892307698726654)
[2025-02-16 13:28:16,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:17,021][root][INFO] - Training Epoch: 1/2, step 12336/23838 completed (loss: 0.34386444091796875, acc: 0.8888888955116272)
[2025-02-16 13:28:17,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:17,436][root][INFO] - Training Epoch: 1/2, step 12337/23838 completed (loss: 0.25615909695625305, acc: 0.9389312863349915)
[2025-02-16 13:28:17,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:17,851][root][INFO] - Training Epoch: 1/2, step 12338/23838 completed (loss: 0.2417925000190735, acc: 0.9316239356994629)
[2025-02-16 13:28:18,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:18,229][root][INFO] - Training Epoch: 1/2, step 12339/23838 completed (loss: 0.4870516061782837, acc: 0.8690476417541504)
[2025-02-16 13:28:18,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:18,575][root][INFO] - Training Epoch: 1/2, step 12340/23838 completed (loss: 0.22456148266792297, acc: 0.9516128897666931)
[2025-02-16 13:28:18,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:18,985][root][INFO] - Training Epoch: 1/2, step 12341/23838 completed (loss: 0.4195672273635864, acc: 0.9014084339141846)
[2025-02-16 13:28:19,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:19,403][root][INFO] - Training Epoch: 1/2, step 12342/23838 completed (loss: 0.271492063999176, acc: 0.9252336621284485)
[2025-02-16 13:28:19,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:19,843][root][INFO] - Training Epoch: 1/2, step 12343/23838 completed (loss: 0.5788318514823914, acc: 0.8544303774833679)
[2025-02-16 13:28:19,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:20,220][root][INFO] - Training Epoch: 1/2, step 12344/23838 completed (loss: 0.1833871603012085, acc: 0.9417475461959839)
[2025-02-16 13:28:20,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:20,647][root][INFO] - Training Epoch: 1/2, step 12345/23838 completed (loss: 0.2315348982810974, acc: 0.9264705777168274)
[2025-02-16 13:28:20,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:21,124][root][INFO] - Training Epoch: 1/2, step 12346/23838 completed (loss: 0.5561888217926025, acc: 0.8333333134651184)
[2025-02-16 13:28:21,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:21,559][root][INFO] - Training Epoch: 1/2, step 12347/23838 completed (loss: 0.28284522891044617, acc: 0.9245283007621765)
[2025-02-16 13:28:21,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:21,972][root][INFO] - Training Epoch: 1/2, step 12348/23838 completed (loss: 0.3136455714702606, acc: 0.9277108311653137)
[2025-02-16 13:28:22,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:22,360][root][INFO] - Training Epoch: 1/2, step 12349/23838 completed (loss: 0.2532896399497986, acc: 0.9339622855186462)
[2025-02-16 13:28:22,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:22,766][root][INFO] - Training Epoch: 1/2, step 12350/23838 completed (loss: 0.7249226570129395, acc: 0.8153846263885498)
[2025-02-16 13:28:22,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:23,180][root][INFO] - Training Epoch: 1/2, step 12351/23838 completed (loss: 0.3275440037250519, acc: 0.8977272510528564)
[2025-02-16 13:28:23,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:23,597][root][INFO] - Training Epoch: 1/2, step 12352/23838 completed (loss: 0.262586385011673, acc: 0.9259259104728699)
[2025-02-16 13:28:23,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:24,060][root][INFO] - Training Epoch: 1/2, step 12353/23838 completed (loss: 0.5553246140480042, acc: 0.8695651888847351)
[2025-02-16 13:28:24,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:24,526][root][INFO] - Training Epoch: 1/2, step 12354/23838 completed (loss: 0.30612972378730774, acc: 0.9230769276618958)
[2025-02-16 13:28:24,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:24,959][root][INFO] - Training Epoch: 1/2, step 12355/23838 completed (loss: 0.36012357473373413, acc: 0.9396551847457886)
[2025-02-16 13:28:25,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:25,358][root][INFO] - Training Epoch: 1/2, step 12356/23838 completed (loss: 0.4550495743751526, acc: 0.8961039185523987)
[2025-02-16 13:28:25,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:25,844][root][INFO] - Training Epoch: 1/2, step 12357/23838 completed (loss: 0.2433021366596222, acc: 0.9312169551849365)
[2025-02-16 13:28:26,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:26,225][root][INFO] - Training Epoch: 1/2, step 12358/23838 completed (loss: 0.18391363322734833, acc: 0.9491525292396545)
[2025-02-16 13:28:26,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:26,628][root][INFO] - Training Epoch: 1/2, step 12359/23838 completed (loss: 0.503844141960144, acc: 0.8550724387168884)
[2025-02-16 13:28:26,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:27,128][root][INFO] - Training Epoch: 1/2, step 12360/23838 completed (loss: 0.5327506065368652, acc: 0.875)
[2025-02-16 13:28:27,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:27,602][root][INFO] - Training Epoch: 1/2, step 12361/23838 completed (loss: 0.22042447328567505, acc: 0.946601927280426)
[2025-02-16 13:28:27,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:28,298][root][INFO] - Training Epoch: 1/2, step 12362/23838 completed (loss: 0.20092438161373138, acc: 0.9296875)
[2025-02-16 13:28:28,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:28,751][root][INFO] - Training Epoch: 1/2, step 12363/23838 completed (loss: 0.8006970286369324, acc: 0.8113207817077637)
[2025-02-16 13:28:28,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:29,229][root][INFO] - Training Epoch: 1/2, step 12364/23838 completed (loss: 0.34494301676750183, acc: 0.8924731016159058)
[2025-02-16 13:28:29,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:29,670][root][INFO] - Training Epoch: 1/2, step 12365/23838 completed (loss: 0.3701251447200775, acc: 0.904347836971283)
[2025-02-16 13:28:29,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:30,147][root][INFO] - Training Epoch: 1/2, step 12366/23838 completed (loss: 0.6320111155509949, acc: 0.8627451062202454)
[2025-02-16 13:28:30,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:30,596][root][INFO] - Training Epoch: 1/2, step 12367/23838 completed (loss: 0.40649378299713135, acc: 0.8870967626571655)
[2025-02-16 13:28:30,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:31,049][root][INFO] - Training Epoch: 1/2, step 12368/23838 completed (loss: 0.30695441365242004, acc: 0.9285714030265808)
[2025-02-16 13:28:31,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:31,442][root][INFO] - Training Epoch: 1/2, step 12369/23838 completed (loss: 0.4394034147262573, acc: 0.8859649300575256)
[2025-02-16 13:28:31,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:31,823][root][INFO] - Training Epoch: 1/2, step 12370/23838 completed (loss: 0.2521267533302307, acc: 0.8999999761581421)
[2025-02-16 13:28:32,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:32,242][root][INFO] - Training Epoch: 1/2, step 12371/23838 completed (loss: 0.35951682925224304, acc: 0.8867924809455872)
[2025-02-16 13:28:32,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:32,691][root][INFO] - Training Epoch: 1/2, step 12372/23838 completed (loss: 0.6270540952682495, acc: 0.8392857313156128)
[2025-02-16 13:28:32,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:33,108][root][INFO] - Training Epoch: 1/2, step 12373/23838 completed (loss: 0.8612037301063538, acc: 0.7540983557701111)
[2025-02-16 13:28:33,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:33,499][root][INFO] - Training Epoch: 1/2, step 12374/23838 completed (loss: 0.5838448405265808, acc: 0.8206896781921387)
[2025-02-16 13:28:33,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:33,876][root][INFO] - Training Epoch: 1/2, step 12375/23838 completed (loss: 0.20427089929580688, acc: 0.9259259104728699)
[2025-02-16 13:28:34,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:34,322][root][INFO] - Training Epoch: 1/2, step 12376/23838 completed (loss: 0.504332959651947, acc: 0.8482142686843872)
[2025-02-16 13:28:34,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:34,689][root][INFO] - Training Epoch: 1/2, step 12377/23838 completed (loss: 0.39924266934394836, acc: 0.8837209343910217)
[2025-02-16 13:28:34,834][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:35,065][root][INFO] - Training Epoch: 1/2, step 12378/23838 completed (loss: 0.13115578889846802, acc: 0.9508196711540222)
[2025-02-16 13:28:35,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:35,517][root][INFO] - Training Epoch: 1/2, step 12379/23838 completed (loss: 0.5017339587211609, acc: 0.8333333134651184)
[2025-02-16 13:28:35,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:35,947][root][INFO] - Training Epoch: 1/2, step 12380/23838 completed (loss: 0.21759290993213654, acc: 0.9342105388641357)
[2025-02-16 13:28:36,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:36,353][root][INFO] - Training Epoch: 1/2, step 12381/23838 completed (loss: 0.20686815679073334, acc: 0.9126983880996704)
[2025-02-16 13:28:36,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:36,804][root][INFO] - Training Epoch: 1/2, step 12382/23838 completed (loss: 0.2925932705402374, acc: 0.9354838728904724)
[2025-02-16 13:28:37,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:37,230][root][INFO] - Training Epoch: 1/2, step 12383/23838 completed (loss: 0.39216431975364685, acc: 0.8662420511245728)
[2025-02-16 13:28:37,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:37,625][root][INFO] - Training Epoch: 1/2, step 12384/23838 completed (loss: 1.0043509006500244, acc: 0.773809552192688)
[2025-02-16 13:28:37,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:38,069][root][INFO] - Training Epoch: 1/2, step 12385/23838 completed (loss: 1.0405079126358032, acc: 0.7169811129570007)
[2025-02-16 13:28:38,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:38,502][root][INFO] - Training Epoch: 1/2, step 12386/23838 completed (loss: 0.4389171004295349, acc: 0.8256880640983582)
[2025-02-16 13:28:38,658][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:38,955][root][INFO] - Training Epoch: 1/2, step 12387/23838 completed (loss: 0.5608933568000793, acc: 0.8857142925262451)
[2025-02-16 13:28:39,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:39,437][root][INFO] - Training Epoch: 1/2, step 12388/23838 completed (loss: 0.9649533629417419, acc: 0.746835470199585)
[2025-02-16 13:28:39,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:39,936][root][INFO] - Training Epoch: 1/2, step 12389/23838 completed (loss: 0.2937234342098236, acc: 0.9194630980491638)
[2025-02-16 13:28:40,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:40,421][root][INFO] - Training Epoch: 1/2, step 12390/23838 completed (loss: 0.25147396326065063, acc: 0.9214285612106323)
[2025-02-16 13:28:40,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:40,876][root][INFO] - Training Epoch: 1/2, step 12391/23838 completed (loss: 0.3225283622741699, acc: 0.9096774458885193)
[2025-02-16 13:28:41,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:41,349][root][INFO] - Training Epoch: 1/2, step 12392/23838 completed (loss: 0.27233558893203735, acc: 0.9230769276618958)
[2025-02-16 13:28:41,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:41,887][root][INFO] - Training Epoch: 1/2, step 12393/23838 completed (loss: 0.19095316529273987, acc: 0.9623655676841736)
[2025-02-16 13:28:42,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:42,374][root][INFO] - Training Epoch: 1/2, step 12394/23838 completed (loss: 0.21081240475177765, acc: 0.9292929172515869)
[2025-02-16 13:28:42,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:42,819][root][INFO] - Training Epoch: 1/2, step 12395/23838 completed (loss: 0.18447130918502808, acc: 0.9248120188713074)
[2025-02-16 13:28:43,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:43,252][root][INFO] - Training Epoch: 1/2, step 12396/23838 completed (loss: 0.39483413100242615, acc: 0.8730158805847168)
[2025-02-16 13:28:43,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:43,672][root][INFO] - Training Epoch: 1/2, step 12397/23838 completed (loss: 0.5881179571151733, acc: 0.8396946787834167)
[2025-02-16 13:28:43,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:44,065][root][INFO] - Training Epoch: 1/2, step 12398/23838 completed (loss: 0.11483125388622284, acc: 0.9753086566925049)
[2025-02-16 13:28:44,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:44,602][root][INFO] - Training Epoch: 1/2, step 12399/23838 completed (loss: 0.28626203536987305, acc: 0.9387755393981934)
[2025-02-16 13:28:44,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:45,059][root][INFO] - Training Epoch: 1/2, step 12400/23838 completed (loss: 0.2168274074792862, acc: 0.9375)
[2025-02-16 13:28:45,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:45,462][root][INFO] - Training Epoch: 1/2, step 12401/23838 completed (loss: 0.4245448112487793, acc: 0.8920863270759583)
[2025-02-16 13:28:45,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:46,033][root][INFO] - Training Epoch: 1/2, step 12402/23838 completed (loss: 0.2609405219554901, acc: 0.9467455744743347)
[2025-02-16 13:28:46,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:46,417][root][INFO] - Training Epoch: 1/2, step 12403/23838 completed (loss: 0.18450908362865448, acc: 0.9368420839309692)
[2025-02-16 13:28:46,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:46,885][root][INFO] - Training Epoch: 1/2, step 12404/23838 completed (loss: 0.25185060501098633, acc: 0.9193548560142517)
[2025-02-16 13:28:47,084][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:47,325][root][INFO] - Training Epoch: 1/2, step 12405/23838 completed (loss: 0.31523197889328003, acc: 0.9111111164093018)
[2025-02-16 13:28:47,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:47,736][root][INFO] - Training Epoch: 1/2, step 12406/23838 completed (loss: 0.2704620957374573, acc: 0.8961039185523987)
[2025-02-16 13:28:47,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:48,079][root][INFO] - Training Epoch: 1/2, step 12407/23838 completed (loss: 0.2475261092185974, acc: 0.9578947424888611)
[2025-02-16 13:28:48,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:48,447][root][INFO] - Training Epoch: 1/2, step 12408/23838 completed (loss: 0.5497627854347229, acc: 0.8863636255264282)
[2025-02-16 13:28:48,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:48,882][root][INFO] - Training Epoch: 1/2, step 12409/23838 completed (loss: 0.27950435876846313, acc: 0.935251772403717)
[2025-02-16 13:28:49,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:49,360][root][INFO] - Training Epoch: 1/2, step 12410/23838 completed (loss: 0.2344783991575241, acc: 0.9399999976158142)
[2025-02-16 13:28:49,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:49,796][root][INFO] - Training Epoch: 1/2, step 12411/23838 completed (loss: 0.18355382978916168, acc: 0.9541984796524048)
[2025-02-16 13:28:49,965][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:50,198][root][INFO] - Training Epoch: 1/2, step 12412/23838 completed (loss: 0.7187181115150452, acc: 0.8111110925674438)
[2025-02-16 13:28:50,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:50,564][root][INFO] - Training Epoch: 1/2, step 12413/23838 completed (loss: 0.39278197288513184, acc: 0.8656716346740723)
[2025-02-16 13:28:50,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:50,941][root][INFO] - Training Epoch: 1/2, step 12414/23838 completed (loss: 0.4910661578178406, acc: 0.8571428656578064)
[2025-02-16 13:28:51,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:51,368][root][INFO] - Training Epoch: 1/2, step 12415/23838 completed (loss: 0.3055223524570465, acc: 0.978723406791687)
[2025-02-16 13:28:51,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:52,026][root][INFO] - Training Epoch: 1/2, step 12416/23838 completed (loss: 0.4365125298500061, acc: 0.8809523582458496)
[2025-02-16 13:28:52,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:52,473][root][INFO] - Training Epoch: 1/2, step 12417/23838 completed (loss: 0.40045812726020813, acc: 0.8857142925262451)
[2025-02-16 13:28:52,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:52,927][root][INFO] - Training Epoch: 1/2, step 12418/23838 completed (loss: 0.27388206124305725, acc: 0.9349112510681152)
[2025-02-16 13:28:53,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:53,372][root][INFO] - Training Epoch: 1/2, step 12419/23838 completed (loss: 0.4408155679702759, acc: 0.8799999952316284)
[2025-02-16 13:28:53,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:53,823][root][INFO] - Training Epoch: 1/2, step 12420/23838 completed (loss: 0.19449229538440704, acc: 0.9529411792755127)
[2025-02-16 13:28:54,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:54,275][root][INFO] - Training Epoch: 1/2, step 12421/23838 completed (loss: 0.47781622409820557, acc: 0.8672566413879395)
[2025-02-16 13:28:54,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:54,722][root][INFO] - Training Epoch: 1/2, step 12422/23838 completed (loss: 0.36019352078437805, acc: 0.8809523582458496)
[2025-02-16 13:28:54,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:55,199][root][INFO] - Training Epoch: 1/2, step 12423/23838 completed (loss: 0.46757152676582336, acc: 0.8674699068069458)
[2025-02-16 13:28:55,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:55,630][root][INFO] - Training Epoch: 1/2, step 12424/23838 completed (loss: 0.24939094483852386, acc: 0.9274193644523621)
[2025-02-16 13:28:55,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:56,077][root][INFO] - Training Epoch: 1/2, step 12425/23838 completed (loss: 0.2851206362247467, acc: 0.9235668778419495)
[2025-02-16 13:28:56,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:56,562][root][INFO] - Training Epoch: 1/2, step 12426/23838 completed (loss: 0.32395145297050476, acc: 0.9417989253997803)
[2025-02-16 13:28:56,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:56,987][root][INFO] - Training Epoch: 1/2, step 12427/23838 completed (loss: 0.35895299911499023, acc: 0.9056603908538818)
[2025-02-16 13:28:57,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:57,424][root][INFO] - Training Epoch: 1/2, step 12428/23838 completed (loss: 0.4546794891357422, acc: 0.8702290058135986)
[2025-02-16 13:28:57,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:57,836][root][INFO] - Training Epoch: 1/2, step 12429/23838 completed (loss: 0.2785584330558777, acc: 0.930232584476471)
[2025-02-16 13:28:58,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:58,242][root][INFO] - Training Epoch: 1/2, step 12430/23838 completed (loss: 0.3394104242324829, acc: 0.9026548862457275)
[2025-02-16 13:28:58,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:58,676][root][INFO] - Training Epoch: 1/2, step 12431/23838 completed (loss: 0.3710657060146332, acc: 0.8985507488250732)
[2025-02-16 13:28:58,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:59,144][root][INFO] - Training Epoch: 1/2, step 12432/23838 completed (loss: 0.3035028278827667, acc: 0.9159663915634155)
[2025-02-16 13:28:59,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:28:59,596][root][INFO] - Training Epoch: 1/2, step 12433/23838 completed (loss: 0.7624133229255676, acc: 0.8051947951316833)
[2025-02-16 13:28:59,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:00,042][root][INFO] - Training Epoch: 1/2, step 12434/23838 completed (loss: 0.3615916669368744, acc: 0.8765432238578796)
[2025-02-16 13:29:00,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:00,425][root][INFO] - Training Epoch: 1/2, step 12435/23838 completed (loss: 0.6882607936859131, acc: 0.8023256063461304)
[2025-02-16 13:29:00,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:00,860][root][INFO] - Training Epoch: 1/2, step 12436/23838 completed (loss: 0.35929349064826965, acc: 0.9142857193946838)
[2025-02-16 13:29:01,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:01,352][root][INFO] - Training Epoch: 1/2, step 12437/23838 completed (loss: 0.171070858836174, acc: 0.9652174115180969)
[2025-02-16 13:29:01,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:01,734][root][INFO] - Training Epoch: 1/2, step 12438/23838 completed (loss: 0.06449219584465027, acc: 0.9906542301177979)
[2025-02-16 13:29:01,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:02,157][root][INFO] - Training Epoch: 1/2, step 12439/23838 completed (loss: 0.20311029255390167, acc: 0.9444444179534912)
[2025-02-16 13:29:02,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:02,506][root][INFO] - Training Epoch: 1/2, step 12440/23838 completed (loss: 0.20020915567874908, acc: 0.9104477763175964)
[2025-02-16 13:29:02,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:02,896][root][INFO] - Training Epoch: 1/2, step 12441/23838 completed (loss: 0.24230258166790009, acc: 0.9420289993286133)
[2025-02-16 13:29:03,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:03,281][root][INFO] - Training Epoch: 1/2, step 12442/23838 completed (loss: 0.330587774515152, acc: 0.9230769276618958)
[2025-02-16 13:29:03,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:03,747][root][INFO] - Training Epoch: 1/2, step 12443/23838 completed (loss: 0.1891806423664093, acc: 0.9484536051750183)
[2025-02-16 13:29:03,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:04,155][root][INFO] - Training Epoch: 1/2, step 12444/23838 completed (loss: 0.12983310222625732, acc: 0.9647058844566345)
[2025-02-16 13:29:04,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:04,634][root][INFO] - Training Epoch: 1/2, step 12445/23838 completed (loss: 0.08627822995185852, acc: 0.9830508232116699)
[2025-02-16 13:29:04,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:05,124][root][INFO] - Training Epoch: 1/2, step 12446/23838 completed (loss: 0.3050742447376251, acc: 0.8970588445663452)
[2025-02-16 13:29:05,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:05,613][root][INFO] - Training Epoch: 1/2, step 12447/23838 completed (loss: 0.3241594731807709, acc: 0.908450722694397)
[2025-02-16 13:29:05,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:06,077][root][INFO] - Training Epoch: 1/2, step 12448/23838 completed (loss: 0.11715181916952133, acc: 0.975806474685669)
[2025-02-16 13:29:06,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:06,548][root][INFO] - Training Epoch: 1/2, step 12449/23838 completed (loss: 0.2078849822282791, acc: 0.9290780425071716)
[2025-02-16 13:29:06,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:07,027][root][INFO] - Training Epoch: 1/2, step 12450/23838 completed (loss: 0.26730769872665405, acc: 0.9375)
[2025-02-16 13:29:07,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:07,388][root][INFO] - Training Epoch: 1/2, step 12451/23838 completed (loss: 0.3143315315246582, acc: 0.9180327653884888)
[2025-02-16 13:29:07,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:07,900][root][INFO] - Training Epoch: 1/2, step 12452/23838 completed (loss: 0.2897590696811676, acc: 0.9504132270812988)
[2025-02-16 13:29:08,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:08,484][root][INFO] - Training Epoch: 1/2, step 12453/23838 completed (loss: 0.30732613801956177, acc: 0.9346405267715454)
[2025-02-16 13:29:08,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:08,950][root][INFO] - Training Epoch: 1/2, step 12454/23838 completed (loss: 0.41855669021606445, acc: 0.9008264541625977)
[2025-02-16 13:29:09,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:09,952][root][INFO] - Training Epoch: 1/2, step 12455/23838 completed (loss: 0.16177652776241302, acc: 0.9528301954269409)
[2025-02-16 13:29:10,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:10,312][root][INFO] - Training Epoch: 1/2, step 12456/23838 completed (loss: 0.49639594554901123, acc: 0.8970588445663452)
[2025-02-16 13:29:10,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:10,805][root][INFO] - Training Epoch: 1/2, step 12457/23838 completed (loss: 0.054424237459897995, acc: 0.9821428656578064)
[2025-02-16 13:29:10,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:11,181][root][INFO] - Training Epoch: 1/2, step 12458/23838 completed (loss: 0.18154242634773254, acc: 0.9345794320106506)
[2025-02-16 13:29:11,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:11,561][root][INFO] - Training Epoch: 1/2, step 12459/23838 completed (loss: 0.2248111367225647, acc: 0.948387086391449)
[2025-02-16 13:29:11,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:12,036][root][INFO] - Training Epoch: 1/2, step 12460/23838 completed (loss: 0.16157425940036774, acc: 0.9495798349380493)
[2025-02-16 13:29:12,251][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:12,482][root][INFO] - Training Epoch: 1/2, step 12461/23838 completed (loss: 0.25623399019241333, acc: 0.9399999976158142)
[2025-02-16 13:29:12,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:13,071][root][INFO] - Training Epoch: 1/2, step 12462/23838 completed (loss: 0.12887629866600037, acc: 0.9568345546722412)
[2025-02-16 13:29:13,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:13,541][root][INFO] - Training Epoch: 1/2, step 12463/23838 completed (loss: 0.1891416311264038, acc: 0.9473684430122375)
[2025-02-16 13:29:13,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:13,999][root][INFO] - Training Epoch: 1/2, step 12464/23838 completed (loss: 0.4439975619316101, acc: 0.8777777552604675)
[2025-02-16 13:29:14,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:14,383][root][INFO] - Training Epoch: 1/2, step 12465/23838 completed (loss: 0.37668871879577637, acc: 0.8857142925262451)
[2025-02-16 13:29:14,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:14,780][root][INFO] - Training Epoch: 1/2, step 12466/23838 completed (loss: 0.1812991499900818, acc: 0.9571428298950195)
[2025-02-16 13:29:15,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:15,256][root][INFO] - Training Epoch: 1/2, step 12467/23838 completed (loss: 0.3831864297389984, acc: 0.9156626462936401)
[2025-02-16 13:29:15,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:15,714][root][INFO] - Training Epoch: 1/2, step 12468/23838 completed (loss: 0.5662036538124084, acc: 0.8297872543334961)
[2025-02-16 13:29:16,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:16,369][root][INFO] - Training Epoch: 1/2, step 12469/23838 completed (loss: 0.13531708717346191, acc: 0.9591836929321289)
[2025-02-16 13:29:16,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:16,784][root][INFO] - Training Epoch: 1/2, step 12470/23838 completed (loss: 0.3058827519416809, acc: 0.9367088675498962)
[2025-02-16 13:29:17,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:17,242][root][INFO] - Training Epoch: 1/2, step 12471/23838 completed (loss: 0.16135969758033752, acc: 0.9324324131011963)
[2025-02-16 13:29:17,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:17,669][root][INFO] - Training Epoch: 1/2, step 12472/23838 completed (loss: 0.1269802749156952, acc: 0.9618320465087891)
[2025-02-16 13:29:17,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:18,092][root][INFO] - Training Epoch: 1/2, step 12473/23838 completed (loss: 0.1585189700126648, acc: 0.9534883499145508)
[2025-02-16 13:29:18,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:18,527][root][INFO] - Training Epoch: 1/2, step 12474/23838 completed (loss: 0.12722498178482056, acc: 0.9626865386962891)
[2025-02-16 13:29:18,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:19,281][root][INFO] - Training Epoch: 1/2, step 12475/23838 completed (loss: 0.29610827565193176, acc: 0.9239130616188049)
[2025-02-16 13:29:19,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:19,866][root][INFO] - Training Epoch: 1/2, step 12476/23838 completed (loss: 0.05632684379816055, acc: 0.9774436354637146)
[2025-02-16 13:29:20,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:21,007][root][INFO] - Training Epoch: 1/2, step 12477/23838 completed (loss: 0.1120179072022438, acc: 0.9618644118309021)
[2025-02-16 13:29:21,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:21,373][root][INFO] - Training Epoch: 1/2, step 12478/23838 completed (loss: 0.30115118622779846, acc: 0.9255319237709045)
[2025-02-16 13:29:21,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:21,810][root][INFO] - Training Epoch: 1/2, step 12479/23838 completed (loss: 0.6590513586997986, acc: 0.8548387289047241)
[2025-02-16 13:29:22,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:22,265][root][INFO] - Training Epoch: 1/2, step 12480/23838 completed (loss: 0.10140406340360641, acc: 0.9793814420700073)
[2025-02-16 13:29:22,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:22,634][root][INFO] - Training Epoch: 1/2, step 12481/23838 completed (loss: 0.18940500915050507, acc: 0.9425287246704102)
[2025-02-16 13:29:22,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:23,072][root][INFO] - Training Epoch: 1/2, step 12482/23838 completed (loss: 0.3171130120754242, acc: 0.8962264060974121)
[2025-02-16 13:29:23,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:23,493][root][INFO] - Training Epoch: 1/2, step 12483/23838 completed (loss: 0.25688326358795166, acc: 0.9220778942108154)
[2025-02-16 13:29:23,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:23,953][root][INFO] - Training Epoch: 1/2, step 12484/23838 completed (loss: 0.382379949092865, acc: 0.9090909361839294)
[2025-02-16 13:29:24,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:24,490][root][INFO] - Training Epoch: 1/2, step 12485/23838 completed (loss: 0.2821460962295532, acc: 0.9312169551849365)
[2025-02-16 13:29:24,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:24,934][root][INFO] - Training Epoch: 1/2, step 12486/23838 completed (loss: 0.28462889790534973, acc: 0.9252336621284485)
[2025-02-16 13:29:25,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:25,444][root][INFO] - Training Epoch: 1/2, step 12487/23838 completed (loss: 0.26427048444747925, acc: 0.9316770434379578)
[2025-02-16 13:29:25,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:25,944][root][INFO] - Training Epoch: 1/2, step 12488/23838 completed (loss: 0.3884528875350952, acc: 0.880382776260376)
[2025-02-16 13:29:26,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:26,396][root][INFO] - Training Epoch: 1/2, step 12489/23838 completed (loss: 0.6642357707023621, acc: 0.811965823173523)
[2025-02-16 13:29:26,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:26,785][root][INFO] - Training Epoch: 1/2, step 12490/23838 completed (loss: 0.29704147577285767, acc: 0.9279999732971191)
[2025-02-16 13:29:26,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:27,229][root][INFO] - Training Epoch: 1/2, step 12491/23838 completed (loss: 0.2708432674407959, acc: 0.9200000166893005)
[2025-02-16 13:29:27,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:27,929][root][INFO] - Training Epoch: 1/2, step 12492/23838 completed (loss: 0.26960238814353943, acc: 0.9408866763114929)
[2025-02-16 13:29:28,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:28,343][root][INFO] - Training Epoch: 1/2, step 12493/23838 completed (loss: 0.37887004017829895, acc: 0.8700000047683716)
[2025-02-16 13:29:28,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:28,764][root][INFO] - Training Epoch: 1/2, step 12494/23838 completed (loss: 0.18814390897750854, acc: 0.929411768913269)
[2025-02-16 13:29:29,047][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:29,338][root][INFO] - Training Epoch: 1/2, step 12495/23838 completed (loss: 0.2438277304172516, acc: 0.9353233575820923)
[2025-02-16 13:29:29,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:29,740][root][INFO] - Training Epoch: 1/2, step 12496/23838 completed (loss: 0.27529799938201904, acc: 0.9541284441947937)
[2025-02-16 13:29:29,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:30,151][root][INFO] - Training Epoch: 1/2, step 12497/23838 completed (loss: 0.3399011492729187, acc: 0.8985507488250732)
[2025-02-16 13:29:30,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:30,771][root][INFO] - Training Epoch: 1/2, step 12498/23838 completed (loss: 0.330331951379776, acc: 0.9071038365364075)
[2025-02-16 13:29:30,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:31,196][root][INFO] - Training Epoch: 1/2, step 12499/23838 completed (loss: 0.27152952551841736, acc: 0.9512194991111755)
[2025-02-16 13:29:31,383][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:31,618][root][INFO] - Training Epoch: 1/2, step 12500/23838 completed (loss: 0.48474934697151184, acc: 0.8823529481887817)
[2025-02-16 13:29:31,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:32,094][root][INFO] - Training Epoch: 1/2, step 12501/23838 completed (loss: 0.26767826080322266, acc: 0.9069767594337463)
[2025-02-16 13:29:32,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:32,583][root][INFO] - Training Epoch: 1/2, step 12502/23838 completed (loss: 0.5181325674057007, acc: 0.8780487775802612)
[2025-02-16 13:29:32,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:32,966][root][INFO] - Training Epoch: 1/2, step 12503/23838 completed (loss: 0.5577089190483093, acc: 0.8322981595993042)
[2025-02-16 13:29:33,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:33,477][root][INFO] - Training Epoch: 1/2, step 12504/23838 completed (loss: 0.32575592398643494, acc: 0.8924731016159058)
[2025-02-16 13:29:33,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:34,087][root][INFO] - Training Epoch: 1/2, step 12505/23838 completed (loss: 0.1870003640651703, acc: 0.9491525292396545)
[2025-02-16 13:29:34,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:34,581][root][INFO] - Training Epoch: 1/2, step 12506/23838 completed (loss: 0.22818545997142792, acc: 0.9344262480735779)
[2025-02-16 13:29:34,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:35,000][root][INFO] - Training Epoch: 1/2, step 12507/23838 completed (loss: 0.6192495822906494, acc: 0.8271604776382446)
[2025-02-16 13:29:35,243][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:35,495][root][INFO] - Training Epoch: 1/2, step 12508/23838 completed (loss: 0.2720201313495636, acc: 0.9120879173278809)
[2025-02-16 13:29:35,746][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:35,973][root][INFO] - Training Epoch: 1/2, step 12509/23838 completed (loss: 0.2674565017223358, acc: 0.9411764740943909)
[2025-02-16 13:29:36,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:36,375][root][INFO] - Training Epoch: 1/2, step 12510/23838 completed (loss: 0.4179327189922333, acc: 0.914893627166748)
[2025-02-16 13:29:36,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:36,785][root][INFO] - Training Epoch: 1/2, step 12511/23838 completed (loss: 0.1350434124469757, acc: 0.9440993666648865)
[2025-02-16 13:29:36,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:37,175][root][INFO] - Training Epoch: 1/2, step 12512/23838 completed (loss: 0.11591986566781998, acc: 0.9848484992980957)
[2025-02-16 13:29:37,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:37,610][root][INFO] - Training Epoch: 1/2, step 12513/23838 completed (loss: 0.08405742049217224, acc: 0.984375)
[2025-02-16 13:29:37,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:38,044][root][INFO] - Training Epoch: 1/2, step 12514/23838 completed (loss: 0.30919593572616577, acc: 0.9212598204612732)
[2025-02-16 13:29:38,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:38,433][root][INFO] - Training Epoch: 1/2, step 12515/23838 completed (loss: 0.24934512376785278, acc: 0.8859060406684875)
[2025-02-16 13:29:38,714][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:39,034][root][INFO] - Training Epoch: 1/2, step 12516/23838 completed (loss: 0.32447877526283264, acc: 0.915032684803009)
[2025-02-16 13:29:39,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:39,702][root][INFO] - Training Epoch: 1/2, step 12517/23838 completed (loss: 0.37498465180397034, acc: 0.8999999761581421)
[2025-02-16 13:29:39,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:40,118][root][INFO] - Training Epoch: 1/2, step 12518/23838 completed (loss: 0.3989866375923157, acc: 0.8947368264198303)
[2025-02-16 13:29:40,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:40,623][root][INFO] - Training Epoch: 1/2, step 12519/23838 completed (loss: 0.6021457314491272, acc: 0.8444444537162781)
[2025-02-16 13:29:40,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:41,147][root][INFO] - Training Epoch: 1/2, step 12520/23838 completed (loss: 0.2532973289489746, acc: 0.9368932247161865)
[2025-02-16 13:29:41,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:41,615][root][INFO] - Training Epoch: 1/2, step 12521/23838 completed (loss: 0.18278925120830536, acc: 0.9420289993286133)
[2025-02-16 13:29:41,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:42,148][root][INFO] - Training Epoch: 1/2, step 12522/23838 completed (loss: 0.3136945068836212, acc: 0.9019607901573181)
[2025-02-16 13:29:42,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:42,535][root][INFO] - Training Epoch: 1/2, step 12523/23838 completed (loss: 0.5898684859275818, acc: 0.8439306616783142)
[2025-02-16 13:29:42,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:42,910][root][INFO] - Training Epoch: 1/2, step 12524/23838 completed (loss: 0.534688413143158, acc: 0.8646616339683533)
[2025-02-16 13:29:43,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:43,388][root][INFO] - Training Epoch: 1/2, step 12525/23838 completed (loss: 0.21273094415664673, acc: 0.9318181872367859)
[2025-02-16 13:29:43,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:43,831][root][INFO] - Training Epoch: 1/2, step 12526/23838 completed (loss: 0.4186517596244812, acc: 0.8541666865348816)
[2025-02-16 13:29:44,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:44,407][root][INFO] - Training Epoch: 1/2, step 12527/23838 completed (loss: 0.33103302121162415, acc: 0.9237288236618042)
[2025-02-16 13:29:44,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:44,851][root][INFO] - Training Epoch: 1/2, step 12528/23838 completed (loss: 0.6949282288551331, acc: 0.8301886916160583)
[2025-02-16 13:29:45,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:45,362][root][INFO] - Training Epoch: 1/2, step 12529/23838 completed (loss: 0.12778091430664062, acc: 0.9607843160629272)
[2025-02-16 13:29:45,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:45,752][root][INFO] - Training Epoch: 1/2, step 12530/23838 completed (loss: 0.44984593987464905, acc: 0.8588235378265381)
[2025-02-16 13:29:46,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:46,322][root][INFO] - Training Epoch: 1/2, step 12531/23838 completed (loss: 0.43929627537727356, acc: 0.9025974273681641)
[2025-02-16 13:29:46,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:46,754][root][INFO] - Training Epoch: 1/2, step 12532/23838 completed (loss: 0.38114720582962036, acc: 0.8867924809455872)
[2025-02-16 13:29:46,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:47,158][root][INFO] - Training Epoch: 1/2, step 12533/23838 completed (loss: 0.4249797761440277, acc: 0.8399999737739563)
[2025-02-16 13:29:47,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:47,601][root][INFO] - Training Epoch: 1/2, step 12534/23838 completed (loss: 0.2930440306663513, acc: 0.9083333611488342)
[2025-02-16 13:29:47,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:48,030][root][INFO] - Training Epoch: 1/2, step 12535/23838 completed (loss: 0.18013007938861847, acc: 0.93388432264328)
[2025-02-16 13:29:48,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:48,471][root][INFO] - Training Epoch: 1/2, step 12536/23838 completed (loss: 0.2393323928117752, acc: 0.9450549483299255)
[2025-02-16 13:29:48,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:48,884][root][INFO] - Training Epoch: 1/2, step 12537/23838 completed (loss: 0.19326747953891754, acc: 0.9438202381134033)
[2025-02-16 13:29:49,111][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:49,353][root][INFO] - Training Epoch: 1/2, step 12538/23838 completed (loss: 0.41980990767478943, acc: 0.90625)
[2025-02-16 13:29:49,585][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:49,849][root][INFO] - Training Epoch: 1/2, step 12539/23838 completed (loss: 0.152974933385849, acc: 0.9671052694320679)
[2025-02-16 13:29:50,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:50,284][root][INFO] - Training Epoch: 1/2, step 12540/23838 completed (loss: 0.5726879835128784, acc: 0.8602941036224365)
[2025-02-16 13:29:50,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:50,708][root][INFO] - Training Epoch: 1/2, step 12541/23838 completed (loss: 0.16177253425121307, acc: 0.9658119678497314)
[2025-02-16 13:29:50,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:51,129][root][INFO] - Training Epoch: 1/2, step 12542/23838 completed (loss: 0.22575178742408752, acc: 0.9347826242446899)
[2025-02-16 13:29:51,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:51,549][root][INFO] - Training Epoch: 1/2, step 12543/23838 completed (loss: 0.47842729091644287, acc: 0.8695651888847351)
[2025-02-16 13:29:51,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:52,038][root][INFO] - Training Epoch: 1/2, step 12544/23838 completed (loss: 0.3691179156303406, acc: 0.9247311949729919)
[2025-02-16 13:29:52,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:52,432][root][INFO] - Training Epoch: 1/2, step 12545/23838 completed (loss: 0.4596503973007202, acc: 0.8873239159584045)
[2025-02-16 13:29:52,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:52,919][root][INFO] - Training Epoch: 1/2, step 12546/23838 completed (loss: 0.25514647364616394, acc: 0.9193548560142517)
[2025-02-16 13:29:53,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:53,385][root][INFO] - Training Epoch: 1/2, step 12547/23838 completed (loss: 0.2961544990539551, acc: 0.9295774698257446)
[2025-02-16 13:29:53,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:53,822][root][INFO] - Training Epoch: 1/2, step 12548/23838 completed (loss: 0.1729399710893631, acc: 0.9440559148788452)
[2025-02-16 13:29:53,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:54,230][root][INFO] - Training Epoch: 1/2, step 12549/23838 completed (loss: 0.4212498962879181, acc: 0.9150943160057068)
[2025-02-16 13:29:54,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:54,748][root][INFO] - Training Epoch: 1/2, step 12550/23838 completed (loss: 0.4813477396965027, acc: 0.8842105269432068)
[2025-02-16 13:29:54,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:55,174][root][INFO] - Training Epoch: 1/2, step 12551/23838 completed (loss: 0.40244537591934204, acc: 0.8550724387168884)
[2025-02-16 13:29:55,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:55,553][root][INFO] - Training Epoch: 1/2, step 12552/23838 completed (loss: 0.7528354525566101, acc: 0.8260869383811951)
[2025-02-16 13:29:55,762][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:56,005][root][INFO] - Training Epoch: 1/2, step 12553/23838 completed (loss: 0.6726236939430237, acc: 0.8333333134651184)
[2025-02-16 13:29:56,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:56,414][root][INFO] - Training Epoch: 1/2, step 12554/23838 completed (loss: 0.4244128167629242, acc: 0.875)
[2025-02-16 13:29:56,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:56,863][root][INFO] - Training Epoch: 1/2, step 12555/23838 completed (loss: 0.26438790559768677, acc: 0.9350649118423462)
[2025-02-16 13:29:57,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:57,304][root][INFO] - Training Epoch: 1/2, step 12556/23838 completed (loss: 0.36698755621910095, acc: 0.9071428775787354)
[2025-02-16 13:29:57,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:57,694][root][INFO] - Training Epoch: 1/2, step 12557/23838 completed (loss: 0.41626521944999695, acc: 0.8545454740524292)
[2025-02-16 13:29:57,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:58,205][root][INFO] - Training Epoch: 1/2, step 12558/23838 completed (loss: 0.34625405073165894, acc: 0.885496199131012)
[2025-02-16 13:29:58,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:58,655][root][INFO] - Training Epoch: 1/2, step 12559/23838 completed (loss: 0.2637955844402313, acc: 0.9453551769256592)
[2025-02-16 13:29:58,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:59,063][root][INFO] - Training Epoch: 1/2, step 12560/23838 completed (loss: 0.292084276676178, acc: 0.8888888955116272)
[2025-02-16 13:29:59,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:59,438][root][INFO] - Training Epoch: 1/2, step 12561/23838 completed (loss: 0.5692856907844543, acc: 0.8392857313156128)
[2025-02-16 13:29:59,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:29:59,888][root][INFO] - Training Epoch: 1/2, step 12562/23838 completed (loss: 0.24946679174900055, acc: 0.9375)
[2025-02-16 13:30:00,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:00,389][root][INFO] - Training Epoch: 1/2, step 12563/23838 completed (loss: 0.17834347486495972, acc: 0.9508196711540222)
[2025-02-16 13:30:00,589][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:00,829][root][INFO] - Training Epoch: 1/2, step 12564/23838 completed (loss: 0.22166793048381805, acc: 0.9253731369972229)
[2025-02-16 13:30:01,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:01,286][root][INFO] - Training Epoch: 1/2, step 12565/23838 completed (loss: 0.46323153376579285, acc: 0.868852436542511)
[2025-02-16 13:30:01,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:01,719][root][INFO] - Training Epoch: 1/2, step 12566/23838 completed (loss: 0.5781710147857666, acc: 0.8709677457809448)
[2025-02-16 13:30:01,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:02,143][root][INFO] - Training Epoch: 1/2, step 12567/23838 completed (loss: 0.4043901860713959, acc: 0.8888888955116272)
[2025-02-16 13:30:02,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:02,602][root][INFO] - Training Epoch: 1/2, step 12568/23838 completed (loss: 0.06407758593559265, acc: 0.9908257126808167)
[2025-02-16 13:30:02,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:03,020][root][INFO] - Training Epoch: 1/2, step 12569/23838 completed (loss: 0.30651262402534485, acc: 0.9105691313743591)
[2025-02-16 13:30:03,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:03,397][root][INFO] - Training Epoch: 1/2, step 12570/23838 completed (loss: 0.22313463687896729, acc: 0.9508196711540222)
[2025-02-16 13:30:03,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:03,877][root][INFO] - Training Epoch: 1/2, step 12571/23838 completed (loss: 0.42777541279792786, acc: 0.8636363744735718)
[2025-02-16 13:30:04,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:04,317][root][INFO] - Training Epoch: 1/2, step 12572/23838 completed (loss: 0.4961842894554138, acc: 0.844660222530365)
[2025-02-16 13:30:04,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:04,726][root][INFO] - Training Epoch: 1/2, step 12573/23838 completed (loss: 0.43857884407043457, acc: 0.8736842274665833)
[2025-02-16 13:30:04,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:05,170][root][INFO] - Training Epoch: 1/2, step 12574/23838 completed (loss: 0.42677462100982666, acc: 0.8554216623306274)
[2025-02-16 13:30:05,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:05,574][root][INFO] - Training Epoch: 1/2, step 12575/23838 completed (loss: 0.22521792352199554, acc: 0.9375)
[2025-02-16 13:30:05,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:05,945][root][INFO] - Training Epoch: 1/2, step 12576/23838 completed (loss: 0.3068101704120636, acc: 0.9126213788986206)
[2025-02-16 13:30:06,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:06,407][root][INFO] - Training Epoch: 1/2, step 12577/23838 completed (loss: 0.2723442018032074, acc: 0.9225806593894958)
[2025-02-16 13:30:06,624][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:06,878][root][INFO] - Training Epoch: 1/2, step 12578/23838 completed (loss: 0.2144576460123062, acc: 0.9583333134651184)
[2025-02-16 13:30:07,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:07,367][root][INFO] - Training Epoch: 1/2, step 12579/23838 completed (loss: 0.34713560342788696, acc: 0.8991596698760986)
[2025-02-16 13:30:07,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:07,753][root][INFO] - Training Epoch: 1/2, step 12580/23838 completed (loss: 0.28269749879837036, acc: 0.9021739363670349)
[2025-02-16 13:30:07,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:08,191][root][INFO] - Training Epoch: 1/2, step 12581/23838 completed (loss: 0.21757014095783234, acc: 0.8901098966598511)
[2025-02-16 13:30:08,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:08,579][root][INFO] - Training Epoch: 1/2, step 12582/23838 completed (loss: 0.35830220580101013, acc: 0.8848921060562134)
[2025-02-16 13:30:08,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:09,006][root][INFO] - Training Epoch: 1/2, step 12583/23838 completed (loss: 0.4493412673473358, acc: 0.9102563858032227)
[2025-02-16 13:30:09,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:09,436][root][INFO] - Training Epoch: 1/2, step 12584/23838 completed (loss: 0.13628573715686798, acc: 0.9626168012619019)
[2025-02-16 13:30:09,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:09,917][root][INFO] - Training Epoch: 1/2, step 12585/23838 completed (loss: 0.2622682452201843, acc: 0.9384615421295166)
[2025-02-16 13:30:10,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:10,406][root][INFO] - Training Epoch: 1/2, step 12586/23838 completed (loss: 0.559727132320404, acc: 0.8584905862808228)
[2025-02-16 13:30:10,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:10,799][root][INFO] - Training Epoch: 1/2, step 12587/23838 completed (loss: 0.4761216640472412, acc: 0.8571428656578064)
[2025-02-16 13:30:10,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:11,220][root][INFO] - Training Epoch: 1/2, step 12588/23838 completed (loss: 0.3219761252403259, acc: 0.9230769276618958)
[2025-02-16 13:30:11,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:11,670][root][INFO] - Training Epoch: 1/2, step 12589/23838 completed (loss: 0.36292630434036255, acc: 0.9204545617103577)
[2025-02-16 13:30:11,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:12,124][root][INFO] - Training Epoch: 1/2, step 12590/23838 completed (loss: 0.15829512476921082, acc: 0.9601989984512329)
[2025-02-16 13:30:12,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:12,531][root][INFO] - Training Epoch: 1/2, step 12591/23838 completed (loss: 0.16735081374645233, acc: 0.9464285969734192)
[2025-02-16 13:30:12,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:12,974][root][INFO] - Training Epoch: 1/2, step 12592/23838 completed (loss: 0.2914268374443054, acc: 0.90625)
[2025-02-16 13:30:13,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:13,390][root][INFO] - Training Epoch: 1/2, step 12593/23838 completed (loss: 0.11867768317461014, acc: 0.9791666865348816)
[2025-02-16 13:30:13,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:13,815][root][INFO] - Training Epoch: 1/2, step 12594/23838 completed (loss: 0.3535920977592468, acc: 0.9431818127632141)
[2025-02-16 13:30:14,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:14,294][root][INFO] - Training Epoch: 1/2, step 12595/23838 completed (loss: 0.03510664030909538, acc: 0.9800000190734863)
[2025-02-16 13:30:14,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:14,731][root][INFO] - Training Epoch: 1/2, step 12596/23838 completed (loss: 0.13114668428897858, acc: 0.95652174949646)
[2025-02-16 13:30:14,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:15,205][root][INFO] - Training Epoch: 1/2, step 12597/23838 completed (loss: 0.9354518055915833, acc: 0.6853932738304138)
[2025-02-16 13:30:15,436][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:15,670][root][INFO] - Training Epoch: 1/2, step 12598/23838 completed (loss: 0.24873758852481842, acc: 0.9350649118423462)
[2025-02-16 13:30:15,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:16,115][root][INFO] - Training Epoch: 1/2, step 12599/23838 completed (loss: 0.3183104395866394, acc: 0.8837209343910217)
[2025-02-16 13:30:16,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:16,551][root][INFO] - Training Epoch: 1/2, step 12600/23838 completed (loss: 0.3400775194168091, acc: 0.9247311949729919)
[2025-02-16 13:30:16,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:16,948][root][INFO] - Training Epoch: 1/2, step 12601/23838 completed (loss: 0.23584599792957306, acc: 0.9375)
[2025-02-16 13:30:17,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:17,446][root][INFO] - Training Epoch: 1/2, step 12602/23838 completed (loss: 0.3472650945186615, acc: 0.8970588445663452)
[2025-02-16 13:30:17,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:17,905][root][INFO] - Training Epoch: 1/2, step 12603/23838 completed (loss: 0.27681615948677063, acc: 0.9075630307197571)
[2025-02-16 13:30:18,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:18,341][root][INFO] - Training Epoch: 1/2, step 12604/23838 completed (loss: 0.26727885007858276, acc: 0.9418604373931885)
[2025-02-16 13:30:18,630][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:18,937][root][INFO] - Training Epoch: 1/2, step 12605/23838 completed (loss: 0.3217226564884186, acc: 0.9057971239089966)
[2025-02-16 13:30:19,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:19,416][root][INFO] - Training Epoch: 1/2, step 12606/23838 completed (loss: 0.23043261468410492, acc: 0.9230769276618958)
[2025-02-16 13:30:19,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:19,877][root][INFO] - Training Epoch: 1/2, step 12607/23838 completed (loss: 0.19176949560642242, acc: 0.9617486596107483)
[2025-02-16 13:30:20,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:20,348][root][INFO] - Training Epoch: 1/2, step 12608/23838 completed (loss: 0.23809118568897247, acc: 0.9385964870452881)
[2025-02-16 13:30:20,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:20,751][root][INFO] - Training Epoch: 1/2, step 12609/23838 completed (loss: 0.30048397183418274, acc: 0.9200000166893005)
[2025-02-16 13:30:20,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:21,208][root][INFO] - Training Epoch: 1/2, step 12610/23838 completed (loss: 0.1779979020357132, acc: 0.9449541568756104)
[2025-02-16 13:30:21,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:21,642][root][INFO] - Training Epoch: 1/2, step 12611/23838 completed (loss: 0.20548732578754425, acc: 0.953125)
[2025-02-16 13:30:21,865][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:22,091][root][INFO] - Training Epoch: 1/2, step 12612/23838 completed (loss: 0.1631748527288437, acc: 0.9638554453849792)
[2025-02-16 13:30:22,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:22,480][root][INFO] - Training Epoch: 1/2, step 12613/23838 completed (loss: 0.1267845630645752, acc: 0.9702970385551453)
[2025-02-16 13:30:22,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:22,904][root][INFO] - Training Epoch: 1/2, step 12614/23838 completed (loss: 0.1304289847612381, acc: 0.9550561904907227)
[2025-02-16 13:30:23,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:23,368][root][INFO] - Training Epoch: 1/2, step 12615/23838 completed (loss: 0.431249737739563, acc: 0.9142857193946838)
[2025-02-16 13:30:23,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:23,796][root][INFO] - Training Epoch: 1/2, step 12616/23838 completed (loss: 0.412820965051651, acc: 0.898876428604126)
[2025-02-16 13:30:24,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:24,252][root][INFO] - Training Epoch: 1/2, step 12617/23838 completed (loss: 0.45468100905418396, acc: 0.9152542352676392)
[2025-02-16 13:30:24,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:25,166][root][INFO] - Training Epoch: 1/2, step 12618/23838 completed (loss: 0.17129820585250854, acc: 0.9433962106704712)
[2025-02-16 13:30:25,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:25,537][root][INFO] - Training Epoch: 1/2, step 12619/23838 completed (loss: 0.9097105860710144, acc: 0.797468364238739)
[2025-02-16 13:30:25,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:25,969][root][INFO] - Training Epoch: 1/2, step 12620/23838 completed (loss: 0.5502461791038513, acc: 0.8636363744735718)
[2025-02-16 13:30:26,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:26,378][root][INFO] - Training Epoch: 1/2, step 12621/23838 completed (loss: 0.35740429162979126, acc: 0.8965517282485962)
[2025-02-16 13:30:26,605][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:26,837][root][INFO] - Training Epoch: 1/2, step 12622/23838 completed (loss: 0.4180850684642792, acc: 0.904347836971283)
[2025-02-16 13:30:27,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:27,253][root][INFO] - Training Epoch: 1/2, step 12623/23838 completed (loss: 0.9273673892021179, acc: 0.7916666865348816)
[2025-02-16 13:30:27,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:27,683][root][INFO] - Training Epoch: 1/2, step 12624/23838 completed (loss: 0.7161127328872681, acc: 0.822429895401001)
[2025-02-16 13:30:27,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:28,140][root][INFO] - Training Epoch: 1/2, step 12625/23838 completed (loss: 0.8082154989242554, acc: 0.782608687877655)
[2025-02-16 13:30:28,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:28,617][root][INFO] - Training Epoch: 1/2, step 12626/23838 completed (loss: 0.3453100323677063, acc: 0.8938053250312805)
[2025-02-16 13:30:28,780][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:29,066][root][INFO] - Training Epoch: 1/2, step 12627/23838 completed (loss: 0.3402441740036011, acc: 0.9207921028137207)
[2025-02-16 13:30:29,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:29,519][root][INFO] - Training Epoch: 1/2, step 12628/23838 completed (loss: 0.4722571074962616, acc: 0.8536585569381714)
[2025-02-16 13:30:29,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:30,006][root][INFO] - Training Epoch: 1/2, step 12629/23838 completed (loss: 0.5691109299659729, acc: 0.8357142806053162)
[2025-02-16 13:30:30,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:30,406][root][INFO] - Training Epoch: 1/2, step 12630/23838 completed (loss: 0.7014859318733215, acc: 0.8333333134651184)
[2025-02-16 13:30:30,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:30,827][root][INFO] - Training Epoch: 1/2, step 12631/23838 completed (loss: 0.894804835319519, acc: 0.7301587462425232)
[2025-02-16 13:30:30,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:31,222][root][INFO] - Training Epoch: 1/2, step 12632/23838 completed (loss: 0.4594953656196594, acc: 0.8952381014823914)
[2025-02-16 13:30:31,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:31,830][root][INFO] - Training Epoch: 1/2, step 12633/23838 completed (loss: 0.3007486164569855, acc: 0.9155405163764954)
[2025-02-16 13:30:32,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:32,277][root][INFO] - Training Epoch: 1/2, step 12634/23838 completed (loss: 0.451980859041214, acc: 0.8735632300376892)
[2025-02-16 13:30:32,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:32,710][root][INFO] - Training Epoch: 1/2, step 12635/23838 completed (loss: 0.4303429424762726, acc: 0.8600000143051147)
[2025-02-16 13:30:32,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:33,184][root][INFO] - Training Epoch: 1/2, step 12636/23838 completed (loss: 0.3104649484157562, acc: 0.9189189076423645)
[2025-02-16 13:30:33,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:33,681][root][INFO] - Training Epoch: 1/2, step 12637/23838 completed (loss: 0.2992298901081085, acc: 0.905063271522522)
[2025-02-16 13:30:33,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:34,086][root][INFO] - Training Epoch: 1/2, step 12638/23838 completed (loss: 0.5754995942115784, acc: 0.834645688533783)
[2025-02-16 13:30:34,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:34,510][root][INFO] - Training Epoch: 1/2, step 12639/23838 completed (loss: 0.447831392288208, acc: 0.8720930218696594)
[2025-02-16 13:30:34,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:35,055][root][INFO] - Training Epoch: 1/2, step 12640/23838 completed (loss: 0.2787196636199951, acc: 0.9354838728904724)
[2025-02-16 13:30:35,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:35,525][root][INFO] - Training Epoch: 1/2, step 12641/23838 completed (loss: 0.29703906178474426, acc: 0.9076923131942749)
[2025-02-16 13:30:35,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:35,917][root][INFO] - Training Epoch: 1/2, step 12642/23838 completed (loss: 0.24048838019371033, acc: 0.9259259104728699)
[2025-02-16 13:30:36,158][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:36,452][root][INFO] - Training Epoch: 1/2, step 12643/23838 completed (loss: 0.3596750497817993, acc: 0.8990384340286255)
[2025-02-16 13:30:36,612][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:36,840][root][INFO] - Training Epoch: 1/2, step 12644/23838 completed (loss: 0.5569482445716858, acc: 0.8591549396514893)
[2025-02-16 13:30:37,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:37,257][root][INFO] - Training Epoch: 1/2, step 12645/23838 completed (loss: 0.21857960522174835, acc: 0.9469696879386902)
[2025-02-16 13:30:37,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:37,697][root][INFO] - Training Epoch: 1/2, step 12646/23838 completed (loss: 0.5040867924690247, acc: 0.8638497591018677)
[2025-02-16 13:30:37,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:38,083][root][INFO] - Training Epoch: 1/2, step 12647/23838 completed (loss: 0.335709810256958, acc: 0.9350649118423462)
[2025-02-16 13:30:38,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:38,455][root][INFO] - Training Epoch: 1/2, step 12648/23838 completed (loss: 0.5027896165847778, acc: 0.8613861203193665)
[2025-02-16 13:30:38,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:39,030][root][INFO] - Training Epoch: 1/2, step 12649/23838 completed (loss: 0.34891417622566223, acc: 0.9072847962379456)
[2025-02-16 13:30:39,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:39,523][root][INFO] - Training Epoch: 1/2, step 12650/23838 completed (loss: 0.4533885419368744, acc: 0.8758620619773865)
[2025-02-16 13:30:39,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:39,986][root][INFO] - Training Epoch: 1/2, step 12651/23838 completed (loss: 0.3284989893436432, acc: 0.9230769276618958)
[2025-02-16 13:30:40,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:40,616][root][INFO] - Training Epoch: 1/2, step 12652/23838 completed (loss: 0.1908106654882431, acc: 0.9572649598121643)
[2025-02-16 13:30:40,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:41,184][root][INFO] - Training Epoch: 1/2, step 12653/23838 completed (loss: 0.4819765090942383, acc: 0.8811880946159363)
[2025-02-16 13:30:41,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:41,636][root][INFO] - Training Epoch: 1/2, step 12654/23838 completed (loss: 0.2659846842288971, acc: 0.9108911156654358)
[2025-02-16 13:30:41,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:42,180][root][INFO] - Training Epoch: 1/2, step 12655/23838 completed (loss: 0.35540372133255005, acc: 0.9009901285171509)
[2025-02-16 13:30:42,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:42,688][root][INFO] - Training Epoch: 1/2, step 12656/23838 completed (loss: 0.3094603419303894, acc: 0.920187771320343)
[2025-02-16 13:30:42,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:43,161][root][INFO] - Training Epoch: 1/2, step 12657/23838 completed (loss: 0.4056187570095062, acc: 0.8653846383094788)
[2025-02-16 13:30:43,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:43,612][root][INFO] - Training Epoch: 1/2, step 12658/23838 completed (loss: 0.19982211291790009, acc: 0.9527027010917664)
[2025-02-16 13:30:43,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:44,035][root][INFO] - Training Epoch: 1/2, step 12659/23838 completed (loss: 0.3176284730434418, acc: 0.9166666865348816)
[2025-02-16 13:30:44,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:44,572][root][INFO] - Training Epoch: 1/2, step 12660/23838 completed (loss: 0.560500979423523, acc: 0.8705882430076599)
[2025-02-16 13:30:44,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:45,096][root][INFO] - Training Epoch: 1/2, step 12661/23838 completed (loss: 0.6517847776412964, acc: 0.8283582329750061)
[2025-02-16 13:30:45,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:45,524][root][INFO] - Training Epoch: 1/2, step 12662/23838 completed (loss: 0.5209679007530212, acc: 0.8241758346557617)
[2025-02-16 13:30:45,737][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:45,981][root][INFO] - Training Epoch: 1/2, step 12663/23838 completed (loss: 0.7997167706489563, acc: 0.75)
[2025-02-16 13:30:46,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:46,358][root][INFO] - Training Epoch: 1/2, step 12664/23838 completed (loss: 0.7544566988945007, acc: 0.7843137383460999)
[2025-02-16 13:30:46,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:46,811][root][INFO] - Training Epoch: 1/2, step 12665/23838 completed (loss: 0.3748762309551239, acc: 0.9166666865348816)
[2025-02-16 13:30:47,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:47,584][root][INFO] - Training Epoch: 1/2, step 12666/23838 completed (loss: 0.22564058005809784, acc: 0.9398906826972961)
[2025-02-16 13:30:47,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:48,105][root][INFO] - Training Epoch: 1/2, step 12667/23838 completed (loss: 0.5738911032676697, acc: 0.8484848737716675)
[2025-02-16 13:30:48,348][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:48,581][root][INFO] - Training Epoch: 1/2, step 12668/23838 completed (loss: 0.28653791546821594, acc: 0.9189189076423645)
[2025-02-16 13:30:48,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:49,026][root][INFO] - Training Epoch: 1/2, step 12669/23838 completed (loss: 0.36994805932044983, acc: 0.8984375)
[2025-02-16 13:30:49,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:49,425][root][INFO] - Training Epoch: 1/2, step 12670/23838 completed (loss: 0.30849674344062805, acc: 0.9100000262260437)
[2025-02-16 13:30:49,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:49,962][root][INFO] - Training Epoch: 1/2, step 12671/23838 completed (loss: 0.3741282522678375, acc: 0.8971428275108337)
[2025-02-16 13:30:50,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:50,403][root][INFO] - Training Epoch: 1/2, step 12672/23838 completed (loss: 0.3673931360244751, acc: 0.8196721076965332)
[2025-02-16 13:30:50,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:50,917][root][INFO] - Training Epoch: 1/2, step 12673/23838 completed (loss: 0.26044660806655884, acc: 0.9345794320106506)
[2025-02-16 13:30:51,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:51,678][root][INFO] - Training Epoch: 1/2, step 12674/23838 completed (loss: 0.25598031282424927, acc: 0.9363957643508911)
[2025-02-16 13:30:51,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:52,170][root][INFO] - Training Epoch: 1/2, step 12675/23838 completed (loss: 0.3170507252216339, acc: 0.9090909361839294)
[2025-02-16 13:30:52,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:52,863][root][INFO] - Training Epoch: 1/2, step 12676/23838 completed (loss: 0.3118651509284973, acc: 0.9209039807319641)
[2025-02-16 13:30:53,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:53,293][root][INFO] - Training Epoch: 1/2, step 12677/23838 completed (loss: 1.0006134510040283, acc: 0.7142857313156128)
[2025-02-16 13:30:53,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:53,699][root][INFO] - Training Epoch: 1/2, step 12678/23838 completed (loss: 0.7066906094551086, acc: 0.7763158082962036)
[2025-02-16 13:30:53,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:54,152][root][INFO] - Training Epoch: 1/2, step 12679/23838 completed (loss: 0.2974711060523987, acc: 0.9185185432434082)
[2025-02-16 13:30:54,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:54,575][root][INFO] - Training Epoch: 1/2, step 12680/23838 completed (loss: 0.3242990970611572, acc: 0.9241379499435425)
[2025-02-16 13:30:54,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:55,084][root][INFO] - Training Epoch: 1/2, step 12681/23838 completed (loss: 0.46190810203552246, acc: 0.8700000047683716)
[2025-02-16 13:30:55,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:55,574][root][INFO] - Training Epoch: 1/2, step 12682/23838 completed (loss: 0.47280004620552063, acc: 0.8721804618835449)
[2025-02-16 13:30:55,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:55,990][root][INFO] - Training Epoch: 1/2, step 12683/23838 completed (loss: 0.3815503418445587, acc: 0.9107142686843872)
[2025-02-16 13:30:56,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:56,376][root][INFO] - Training Epoch: 1/2, step 12684/23838 completed (loss: 0.3192063868045807, acc: 0.90625)
[2025-02-16 13:30:56,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:56,849][root][INFO] - Training Epoch: 1/2, step 12685/23838 completed (loss: 0.4811709523200989, acc: 0.8349514603614807)
[2025-02-16 13:30:57,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:57,251][root][INFO] - Training Epoch: 1/2, step 12686/23838 completed (loss: 0.5523382425308228, acc: 0.8369565010070801)
[2025-02-16 13:30:57,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:57,732][root][INFO] - Training Epoch: 1/2, step 12687/23838 completed (loss: 0.302566260099411, acc: 0.9152542352676392)
[2025-02-16 13:30:57,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:58,230][root][INFO] - Training Epoch: 1/2, step 12688/23838 completed (loss: 0.5317150354385376, acc: 0.8461538553237915)
[2025-02-16 13:30:58,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:58,721][root][INFO] - Training Epoch: 1/2, step 12689/23838 completed (loss: 0.5878265500068665, acc: 0.8290598392486572)
[2025-02-16 13:30:58,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:59,137][root][INFO] - Training Epoch: 1/2, step 12690/23838 completed (loss: 0.4303690195083618, acc: 0.8709677457809448)
[2025-02-16 13:30:59,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:30:59,663][root][INFO] - Training Epoch: 1/2, step 12691/23838 completed (loss: 0.5966980457305908, acc: 0.875)
[2025-02-16 13:30:59,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:00,157][root][INFO] - Training Epoch: 1/2, step 12692/23838 completed (loss: 0.1673537939786911, acc: 0.9655172228813171)
[2025-02-16 13:31:00,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:01,183][root][INFO] - Training Epoch: 1/2, step 12693/23838 completed (loss: 0.33762189745903015, acc: 0.893569827079773)
[2025-02-16 13:31:01,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:01,667][root][INFO] - Training Epoch: 1/2, step 12694/23838 completed (loss: 0.4189802408218384, acc: 0.8360655903816223)
[2025-02-16 13:31:01,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:02,175][root][INFO] - Training Epoch: 1/2, step 12695/23838 completed (loss: 0.624506950378418, acc: 0.8266666531562805)
[2025-02-16 13:31:02,324][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:02,539][root][INFO] - Training Epoch: 1/2, step 12696/23838 completed (loss: 0.5345793962478638, acc: 0.8522727489471436)
[2025-02-16 13:31:02,985][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:03,443][root][INFO] - Training Epoch: 1/2, step 12697/23838 completed (loss: 0.31301259994506836, acc: 0.9041916131973267)
[2025-02-16 13:31:03,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:03,865][root][INFO] - Training Epoch: 1/2, step 12698/23838 completed (loss: 0.39698824286460876, acc: 0.8841463327407837)
[2025-02-16 13:31:04,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:04,369][root][INFO] - Training Epoch: 1/2, step 12699/23838 completed (loss: 0.3194580078125, acc: 0.9179487228393555)
[2025-02-16 13:31:04,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:04,828][root][INFO] - Training Epoch: 1/2, step 12700/23838 completed (loss: 0.1969553381204605, acc: 0.9640287756919861)
[2025-02-16 13:31:04,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:05,220][root][INFO] - Training Epoch: 1/2, step 12701/23838 completed (loss: 0.6140003204345703, acc: 0.8556700944900513)
[2025-02-16 13:31:05,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:05,651][root][INFO] - Training Epoch: 1/2, step 12702/23838 completed (loss: 0.37224897742271423, acc: 0.8830409646034241)
[2025-02-16 13:31:05,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:06,089][root][INFO] - Training Epoch: 1/2, step 12703/23838 completed (loss: 0.4174462556838989, acc: 0.8588957190513611)
[2025-02-16 13:31:06,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:06,542][root][INFO] - Training Epoch: 1/2, step 12704/23838 completed (loss: 0.42834481596946716, acc: 0.8875739574432373)
[2025-02-16 13:31:06,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:06,964][root][INFO] - Training Epoch: 1/2, step 12705/23838 completed (loss: 0.46983495354652405, acc: 0.8333333134651184)
[2025-02-16 13:31:07,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:07,462][root][INFO] - Training Epoch: 1/2, step 12706/23838 completed (loss: 0.27844852209091187, acc: 0.9375)
[2025-02-16 13:31:07,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:07,887][root][INFO] - Training Epoch: 1/2, step 12707/23838 completed (loss: 0.46482715010643005, acc: 0.8435373902320862)
[2025-02-16 13:31:08,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:08,288][root][INFO] - Training Epoch: 1/2, step 12708/23838 completed (loss: 0.904160737991333, acc: 0.701298713684082)
[2025-02-16 13:31:08,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:08,860][root][INFO] - Training Epoch: 1/2, step 12709/23838 completed (loss: 0.2984764575958252, acc: 0.9133333563804626)
[2025-02-16 13:31:09,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:09,258][root][INFO] - Training Epoch: 1/2, step 12710/23838 completed (loss: 0.24299956858158112, acc: 0.9172932505607605)
[2025-02-16 13:31:09,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:09,854][root][INFO] - Training Epoch: 1/2, step 12711/23838 completed (loss: 0.3768564462661743, acc: 0.8991596698760986)
[2025-02-16 13:31:10,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:10,463][root][INFO] - Training Epoch: 1/2, step 12712/23838 completed (loss: 0.3857349157333374, acc: 0.8807339668273926)
[2025-02-16 13:31:10,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:10,933][root][INFO] - Training Epoch: 1/2, step 12713/23838 completed (loss: 0.20055806636810303, acc: 0.9444444179534912)
[2025-02-16 13:31:11,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:11,371][root][INFO] - Training Epoch: 1/2, step 12714/23838 completed (loss: 0.446000337600708, acc: 0.8897058963775635)
[2025-02-16 13:31:11,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:11,829][root][INFO] - Training Epoch: 1/2, step 12715/23838 completed (loss: 0.40745052695274353, acc: 0.8723404407501221)
[2025-02-16 13:31:12,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:12,223][root][INFO] - Training Epoch: 1/2, step 12716/23838 completed (loss: 0.5565401315689087, acc: 0.8316831588745117)
[2025-02-16 13:31:12,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:12,633][root][INFO] - Training Epoch: 1/2, step 12717/23838 completed (loss: 0.40488940477371216, acc: 0.8790322542190552)
[2025-02-16 13:31:12,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:13,082][root][INFO] - Training Epoch: 1/2, step 12718/23838 completed (loss: 0.4595228433609009, acc: 0.8540145754814148)
[2025-02-16 13:31:13,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:13,545][root][INFO] - Training Epoch: 1/2, step 12719/23838 completed (loss: 0.3595370054244995, acc: 0.9242424368858337)
[2025-02-16 13:31:13,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:13,903][root][INFO] - Training Epoch: 1/2, step 12720/23838 completed (loss: 0.5652204155921936, acc: 0.8470588326454163)
[2025-02-16 13:31:14,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:14,374][root][INFO] - Training Epoch: 1/2, step 12721/23838 completed (loss: 0.32069817185401917, acc: 0.9324324131011963)
[2025-02-16 13:31:14,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:14,798][root][INFO] - Training Epoch: 1/2, step 12722/23838 completed (loss: 0.6173892617225647, acc: 0.8247422575950623)
[2025-02-16 13:31:15,216][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:15,653][root][INFO] - Training Epoch: 1/2, step 12723/23838 completed (loss: 0.3663198947906494, acc: 0.9077490568161011)
[2025-02-16 13:31:15,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:16,120][root][INFO] - Training Epoch: 1/2, step 12724/23838 completed (loss: 0.30101290345191956, acc: 0.9262295365333557)
[2025-02-16 13:31:16,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:16,587][root][INFO] - Training Epoch: 1/2, step 12725/23838 completed (loss: 0.5018442273139954, acc: 0.8793103694915771)
[2025-02-16 13:31:16,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:17,046][root][INFO] - Training Epoch: 1/2, step 12726/23838 completed (loss: 0.5361840724945068, acc: 0.8541666865348816)
[2025-02-16 13:31:17,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:17,638][root][INFO] - Training Epoch: 1/2, step 12727/23838 completed (loss: 0.402143269777298, acc: 0.8852459192276001)
[2025-02-16 13:31:17,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:18,093][root][INFO] - Training Epoch: 1/2, step 12728/23838 completed (loss: 0.2831350862979889, acc: 0.9222797751426697)
[2025-02-16 13:31:18,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:18,603][root][INFO] - Training Epoch: 1/2, step 12729/23838 completed (loss: 0.359963983297348, acc: 0.9142857193946838)
[2025-02-16 13:31:18,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:19,045][root][INFO] - Training Epoch: 1/2, step 12730/23838 completed (loss: 0.1377914398908615, acc: 0.969072163105011)
[2025-02-16 13:31:19,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:19,590][root][INFO] - Training Epoch: 1/2, step 12731/23838 completed (loss: 0.4156331717967987, acc: 0.890625)
[2025-02-16 13:31:19,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:20,069][root][INFO] - Training Epoch: 1/2, step 12732/23838 completed (loss: 0.2603660821914673, acc: 0.9351851940155029)
[2025-02-16 13:31:20,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:20,601][root][INFO] - Training Epoch: 1/2, step 12733/23838 completed (loss: 0.41207391023635864, acc: 0.9074074029922485)
[2025-02-16 13:31:20,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:21,043][root][INFO] - Training Epoch: 1/2, step 12734/23838 completed (loss: 0.2574358582496643, acc: 0.9153845906257629)
[2025-02-16 13:31:21,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:21,554][root][INFO] - Training Epoch: 1/2, step 12735/23838 completed (loss: 0.38256698846817017, acc: 0.8947368264198303)
[2025-02-16 13:31:21,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:22,037][root][INFO] - Training Epoch: 1/2, step 12736/23838 completed (loss: 0.4964772164821625, acc: 0.8536585569381714)
[2025-02-16 13:31:22,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:22,501][root][INFO] - Training Epoch: 1/2, step 12737/23838 completed (loss: 0.7417190670967102, acc: 0.7903226017951965)
[2025-02-16 13:31:22,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:22,944][root][INFO] - Training Epoch: 1/2, step 12738/23838 completed (loss: 0.25377699732780457, acc: 0.9202454090118408)
[2025-02-16 13:31:23,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:23,550][root][INFO] - Training Epoch: 1/2, step 12739/23838 completed (loss: 0.5214388370513916, acc: 0.8251748085021973)
[2025-02-16 13:31:23,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:24,007][root][INFO] - Training Epoch: 1/2, step 12740/23838 completed (loss: 0.3656928837299347, acc: 0.8880000114440918)
[2025-02-16 13:31:24,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:24,488][root][INFO] - Training Epoch: 1/2, step 12741/23838 completed (loss: 0.16997219622135162, acc: 0.948051929473877)
[2025-02-16 13:31:24,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:24,917][root][INFO] - Training Epoch: 1/2, step 12742/23838 completed (loss: 0.27081701159477234, acc: 0.9064748287200928)
[2025-02-16 13:31:25,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:25,482][root][INFO] - Training Epoch: 1/2, step 12743/23838 completed (loss: 0.32632774114608765, acc: 0.8960000276565552)
[2025-02-16 13:31:25,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:25,961][root][INFO] - Training Epoch: 1/2, step 12744/23838 completed (loss: 0.2765369117259979, acc: 0.9123711585998535)
[2025-02-16 13:31:26,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:26,465][root][INFO] - Training Epoch: 1/2, step 12745/23838 completed (loss: 0.15208637714385986, acc: 0.967391312122345)
[2025-02-16 13:31:26,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:26,889][root][INFO] - Training Epoch: 1/2, step 12746/23838 completed (loss: 0.29900673031806946, acc: 0.9266055226325989)
[2025-02-16 13:31:27,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:27,260][root][INFO] - Training Epoch: 1/2, step 12747/23838 completed (loss: 0.3980225920677185, acc: 0.892307698726654)
[2025-02-16 13:31:27,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:27,766][root][INFO] - Training Epoch: 1/2, step 12748/23838 completed (loss: 0.3600049316883087, acc: 0.8903225660324097)
[2025-02-16 13:31:27,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:28,243][root][INFO] - Training Epoch: 1/2, step 12749/23838 completed (loss: 0.3552131652832031, acc: 0.8860759735107422)
[2025-02-16 13:31:28,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:28,745][root][INFO] - Training Epoch: 1/2, step 12750/23838 completed (loss: 0.2741909921169281, acc: 0.9329897165298462)
[2025-02-16 13:31:28,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:29,109][root][INFO] - Training Epoch: 1/2, step 12751/23838 completed (loss: 0.36212357878685, acc: 0.8809523582458496)
[2025-02-16 13:31:29,258][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:29,492][root][INFO] - Training Epoch: 1/2, step 12752/23838 completed (loss: 0.27017149329185486, acc: 0.9425287246704102)
[2025-02-16 13:31:29,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:29,988][root][INFO] - Training Epoch: 1/2, step 12753/23838 completed (loss: 0.3075331449508667, acc: 0.8876404762268066)
[2025-02-16 13:31:30,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:30,460][root][INFO] - Training Epoch: 1/2, step 12754/23838 completed (loss: 0.3180435597896576, acc: 0.9328358173370361)
[2025-02-16 13:31:30,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:30,930][root][INFO] - Training Epoch: 1/2, step 12755/23838 completed (loss: 0.3353863060474396, acc: 0.9142857193946838)
[2025-02-16 13:31:31,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:31,386][root][INFO] - Training Epoch: 1/2, step 12756/23838 completed (loss: 0.2107090801000595, acc: 0.9146341681480408)
[2025-02-16 13:31:31,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:31,970][root][INFO] - Training Epoch: 1/2, step 12757/23838 completed (loss: 0.2550334632396698, acc: 0.9175257682800293)
[2025-02-16 13:31:32,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:32,401][root][INFO] - Training Epoch: 1/2, step 12758/23838 completed (loss: 0.09103957563638687, acc: 0.9677419066429138)
[2025-02-16 13:31:32,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:32,850][root][INFO] - Training Epoch: 1/2, step 12759/23838 completed (loss: 0.4561798870563507, acc: 0.8767123222351074)
[2025-02-16 13:31:33,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:33,342][root][INFO] - Training Epoch: 1/2, step 12760/23838 completed (loss: 0.3905016779899597, acc: 0.8799999952316284)
[2025-02-16 13:31:33,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:33,824][root][INFO] - Training Epoch: 1/2, step 12761/23838 completed (loss: 0.1849387139081955, acc: 0.9532710313796997)
[2025-02-16 13:31:34,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:34,306][root][INFO] - Training Epoch: 1/2, step 12762/23838 completed (loss: 0.4961206316947937, acc: 0.8615384697914124)
[2025-02-16 13:31:34,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:34,714][root][INFO] - Training Epoch: 1/2, step 12763/23838 completed (loss: 0.28474512696266174, acc: 0.8865979313850403)
[2025-02-16 13:31:34,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:35,259][root][INFO] - Training Epoch: 1/2, step 12764/23838 completed (loss: 0.30753764510154724, acc: 0.9321267008781433)
[2025-02-16 13:31:35,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:35,758][root][INFO] - Training Epoch: 1/2, step 12765/23838 completed (loss: 0.7041205167770386, acc: 0.8192771077156067)
[2025-02-16 13:31:35,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:36,236][root][INFO] - Training Epoch: 1/2, step 12766/23838 completed (loss: 0.2331371158361435, acc: 0.916167676448822)
[2025-02-16 13:31:36,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:36,797][root][INFO] - Training Epoch: 1/2, step 12767/23838 completed (loss: 0.3001845180988312, acc: 0.915032684803009)
[2025-02-16 13:31:37,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:37,269][root][INFO] - Training Epoch: 1/2, step 12768/23838 completed (loss: 0.29280006885528564, acc: 0.9259259104728699)
[2025-02-16 13:31:37,534][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:37,770][root][INFO] - Training Epoch: 1/2, step 12769/23838 completed (loss: 0.44497302174568176, acc: 0.8695651888847351)
[2025-02-16 13:31:37,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:38,209][root][INFO] - Training Epoch: 1/2, step 12770/23838 completed (loss: 0.22072264552116394, acc: 0.9637681245803833)
[2025-02-16 13:31:38,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:38,830][root][INFO] - Training Epoch: 1/2, step 12771/23838 completed (loss: 0.11817188560962677, acc: 0.9604316353797913)
[2025-02-16 13:31:39,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:39,272][root][INFO] - Training Epoch: 1/2, step 12772/23838 completed (loss: 0.71908038854599, acc: 0.7903226017951965)
[2025-02-16 13:31:39,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:39,799][root][INFO] - Training Epoch: 1/2, step 12773/23838 completed (loss: 0.3694755733013153, acc: 0.8870967626571655)
[2025-02-16 13:31:40,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:40,334][root][INFO] - Training Epoch: 1/2, step 12774/23838 completed (loss: 0.09488530457019806, acc: 0.9801324605941772)
[2025-02-16 13:31:40,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:40,798][root][INFO] - Training Epoch: 1/2, step 12775/23838 completed (loss: 0.5529949069023132, acc: 0.8684210777282715)
[2025-02-16 13:31:41,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:41,236][root][INFO] - Training Epoch: 1/2, step 12776/23838 completed (loss: 0.501150906085968, acc: 0.8925619721412659)
[2025-02-16 13:31:41,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:41,699][root][INFO] - Training Epoch: 1/2, step 12777/23838 completed (loss: 0.32458674907684326, acc: 0.9142857193946838)
[2025-02-16 13:31:41,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:42,129][root][INFO] - Training Epoch: 1/2, step 12778/23838 completed (loss: 0.429126113653183, acc: 0.8640776872634888)
[2025-02-16 13:31:42,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:42,576][root][INFO] - Training Epoch: 1/2, step 12779/23838 completed (loss: 0.34823310375213623, acc: 0.9097744226455688)
[2025-02-16 13:31:42,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:43,148][root][INFO] - Training Epoch: 1/2, step 12780/23838 completed (loss: 0.3117419183254242, acc: 0.9289940595626831)
[2025-02-16 13:31:43,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:43,626][root][INFO] - Training Epoch: 1/2, step 12781/23838 completed (loss: 0.43083494901657104, acc: 0.8817204236984253)
[2025-02-16 13:31:43,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:44,094][root][INFO] - Training Epoch: 1/2, step 12782/23838 completed (loss: 0.4098275303840637, acc: 0.8600000143051147)
[2025-02-16 13:31:44,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:44,575][root][INFO] - Training Epoch: 1/2, step 12783/23838 completed (loss: 0.7357213497161865, acc: 0.805084764957428)
[2025-02-16 13:31:44,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:44,973][root][INFO] - Training Epoch: 1/2, step 12784/23838 completed (loss: 0.4302935004234314, acc: 0.8484848737716675)
[2025-02-16 13:31:45,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:45,470][root][INFO] - Training Epoch: 1/2, step 12785/23838 completed (loss: 0.38954201340675354, acc: 0.9217391014099121)
[2025-02-16 13:31:45,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:45,914][root][INFO] - Training Epoch: 1/2, step 12786/23838 completed (loss: 0.4861408472061157, acc: 0.8598130941390991)
[2025-02-16 13:31:46,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:46,393][root][INFO] - Training Epoch: 1/2, step 12787/23838 completed (loss: 0.5156412124633789, acc: 0.8468468189239502)
[2025-02-16 13:31:46,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:46,858][root][INFO] - Training Epoch: 1/2, step 12788/23838 completed (loss: 0.3014910817146301, acc: 0.8952381014823914)
[2025-02-16 13:31:47,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:47,312][root][INFO] - Training Epoch: 1/2, step 12789/23838 completed (loss: 0.6321519613265991, acc: 0.8676470518112183)
[2025-02-16 13:31:47,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:47,821][root][INFO] - Training Epoch: 1/2, step 12790/23838 completed (loss: 0.5177841186523438, acc: 0.8675496578216553)
[2025-02-16 13:31:48,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:48,234][root][INFO] - Training Epoch: 1/2, step 12791/23838 completed (loss: 0.318920373916626, acc: 0.8879310488700867)
[2025-02-16 13:31:48,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:48,653][root][INFO] - Training Epoch: 1/2, step 12792/23838 completed (loss: 0.3096138834953308, acc: 0.9150943160057068)
[2025-02-16 13:31:48,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:49,051][root][INFO] - Training Epoch: 1/2, step 12793/23838 completed (loss: 0.2582513391971588, acc: 0.9191918969154358)
[2025-02-16 13:31:49,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:49,434][root][INFO] - Training Epoch: 1/2, step 12794/23838 completed (loss: 0.4212796688079834, acc: 0.8831169009208679)
[2025-02-16 13:31:49,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:49,834][root][INFO] - Training Epoch: 1/2, step 12795/23838 completed (loss: 0.6972438097000122, acc: 0.7735849022865295)
[2025-02-16 13:31:50,092][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:50,321][root][INFO] - Training Epoch: 1/2, step 12796/23838 completed (loss: 0.3765956461429596, acc: 0.9095744490623474)
[2025-02-16 13:31:50,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:50,757][root][INFO] - Training Epoch: 1/2, step 12797/23838 completed (loss: 0.8840651512145996, acc: 0.734375)
[2025-02-16 13:31:50,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:51,227][root][INFO] - Training Epoch: 1/2, step 12798/23838 completed (loss: 0.20025910437107086, acc: 0.9431818127632141)
[2025-02-16 13:31:51,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:51,651][root][INFO] - Training Epoch: 1/2, step 12799/23838 completed (loss: 0.12264882028102875, acc: 0.9666666388511658)
[2025-02-16 13:31:51,874][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:52,129][root][INFO] - Training Epoch: 1/2, step 12800/23838 completed (loss: 0.350826233625412, acc: 0.914893627166748)
[2025-02-16 13:31:52,338][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:52,642][root][INFO] - Training Epoch: 1/2, step 12801/23838 completed (loss: 0.5977960824966431, acc: 0.8023256063461304)
[2025-02-16 13:31:52,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:53,179][root][INFO] - Training Epoch: 1/2, step 12802/23838 completed (loss: 0.24534250795841217, acc: 0.953125)
[2025-02-16 13:31:53,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:53,757][root][INFO] - Training Epoch: 1/2, step 12803/23838 completed (loss: 0.687414824962616, acc: 0.8148148059844971)
[2025-02-16 13:31:53,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:54,240][root][INFO] - Training Epoch: 1/2, step 12804/23838 completed (loss: 0.5379464626312256, acc: 0.8799999952316284)
[2025-02-16 13:31:54,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:54,732][root][INFO] - Training Epoch: 1/2, step 12805/23838 completed (loss: 0.13632559776306152, acc: 0.9552238583564758)
[2025-02-16 13:31:54,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:55,235][root][INFO] - Training Epoch: 1/2, step 12806/23838 completed (loss: 0.3710596263408661, acc: 0.9270833134651184)
[2025-02-16 13:31:55,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:55,825][root][INFO] - Training Epoch: 1/2, step 12807/23838 completed (loss: 0.4613248407840729, acc: 0.9027777910232544)
[2025-02-16 13:31:56,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:56,235][root][INFO] - Training Epoch: 1/2, step 12808/23838 completed (loss: 0.6880003809928894, acc: 0.824999988079071)
[2025-02-16 13:31:56,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:56,764][root][INFO] - Training Epoch: 1/2, step 12809/23838 completed (loss: 0.33167600631713867, acc: 0.9107142686843872)
[2025-02-16 13:31:56,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:57,191][root][INFO] - Training Epoch: 1/2, step 12810/23838 completed (loss: 0.5701119303703308, acc: 0.8461538553237915)
[2025-02-16 13:31:57,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:57,679][root][INFO] - Training Epoch: 1/2, step 12811/23838 completed (loss: 0.32957881689071655, acc: 0.9158878326416016)
[2025-02-16 13:31:57,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:58,139][root][INFO] - Training Epoch: 1/2, step 12812/23838 completed (loss: 0.17486584186553955, acc: 0.9629629850387573)
[2025-02-16 13:31:58,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:58,578][root][INFO] - Training Epoch: 1/2, step 12813/23838 completed (loss: 0.2505306899547577, acc: 0.9195402264595032)
[2025-02-16 13:31:58,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:59,067][root][INFO] - Training Epoch: 1/2, step 12814/23838 completed (loss: 0.24077783524990082, acc: 0.9304347634315491)
[2025-02-16 13:31:59,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:31:59,516][root][INFO] - Training Epoch: 1/2, step 12815/23838 completed (loss: 0.08672471344470978, acc: 0.9803921580314636)
[2025-02-16 13:31:59,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:00,010][root][INFO] - Training Epoch: 1/2, step 12816/23838 completed (loss: 0.7572596073150635, acc: 0.8314606547355652)
[2025-02-16 13:32:00,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:00,416][root][INFO] - Training Epoch: 1/2, step 12817/23838 completed (loss: 0.26548531651496887, acc: 0.9210526347160339)
[2025-02-16 13:32:00,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:00,868][root][INFO] - Training Epoch: 1/2, step 12818/23838 completed (loss: 0.28106987476348877, acc: 0.9166666865348816)
[2025-02-16 13:32:01,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:01,574][root][INFO] - Training Epoch: 1/2, step 12819/23838 completed (loss: 0.4578949809074402, acc: 0.8720930218696594)
[2025-02-16 13:32:01,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:02,075][root][INFO] - Training Epoch: 1/2, step 12820/23838 completed (loss: 0.5449652671813965, acc: 0.8656716346740723)
[2025-02-16 13:32:02,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:02,590][root][INFO] - Training Epoch: 1/2, step 12821/23838 completed (loss: 0.2014150321483612, acc: 0.9528301954269409)
[2025-02-16 13:32:02,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:03,033][root][INFO] - Training Epoch: 1/2, step 12822/23838 completed (loss: 0.07865068316459656, acc: 0.9599999785423279)
[2025-02-16 13:32:03,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:03,421][root][INFO] - Training Epoch: 1/2, step 12823/23838 completed (loss: 0.3081033229827881, acc: 0.9156626462936401)
[2025-02-16 13:32:03,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:03,867][root][INFO] - Training Epoch: 1/2, step 12824/23838 completed (loss: 0.34969744086265564, acc: 0.8899082541465759)
[2025-02-16 13:32:04,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:04,299][root][INFO] - Training Epoch: 1/2, step 12825/23838 completed (loss: 0.12902693450450897, acc: 0.9553571343421936)
[2025-02-16 13:32:04,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:04,779][root][INFO] - Training Epoch: 1/2, step 12826/23838 completed (loss: 0.2784976661205292, acc: 0.8958333134651184)
[2025-02-16 13:32:05,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:05,358][root][INFO] - Training Epoch: 1/2, step 12827/23838 completed (loss: 0.2583274841308594, acc: 0.921875)
[2025-02-16 13:32:05,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:05,956][root][INFO] - Training Epoch: 1/2, step 12828/23838 completed (loss: 0.37512922286987305, acc: 0.9039999842643738)
[2025-02-16 13:32:06,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:06,414][root][INFO] - Training Epoch: 1/2, step 12829/23838 completed (loss: 0.2672799527645111, acc: 0.9136690497398376)
[2025-02-16 13:32:06,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:06,870][root][INFO] - Training Epoch: 1/2, step 12830/23838 completed (loss: 0.18940432369709015, acc: 0.957446813583374)
[2025-02-16 13:32:07,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:07,308][root][INFO] - Training Epoch: 1/2, step 12831/23838 completed (loss: 0.5313231945037842, acc: 0.8787878751754761)
[2025-02-16 13:32:07,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:07,784][root][INFO] - Training Epoch: 1/2, step 12832/23838 completed (loss: 0.4726262092590332, acc: 0.875)
[2025-02-16 13:32:07,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:08,198][root][INFO] - Training Epoch: 1/2, step 12833/23838 completed (loss: 0.31650489568710327, acc: 0.9375)
[2025-02-16 13:32:08,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:08,658][root][INFO] - Training Epoch: 1/2, step 12834/23838 completed (loss: 0.38285136222839355, acc: 0.931034505367279)
[2025-02-16 13:32:08,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:09,157][root][INFO] - Training Epoch: 1/2, step 12835/23838 completed (loss: 0.3670835793018341, acc: 0.9021739363670349)
[2025-02-16 13:32:09,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:09,684][root][INFO] - Training Epoch: 1/2, step 12836/23838 completed (loss: 0.47232475876808167, acc: 0.887417197227478)
[2025-02-16 13:32:09,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:10,105][root][INFO] - Training Epoch: 1/2, step 12837/23838 completed (loss: 0.13522034883499146, acc: 0.9402984976768494)
[2025-02-16 13:32:10,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:10,497][root][INFO] - Training Epoch: 1/2, step 12838/23838 completed (loss: 0.25099989771842957, acc: 0.9193548560142517)
[2025-02-16 13:32:10,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:11,140][root][INFO] - Training Epoch: 1/2, step 12839/23838 completed (loss: 0.5329527854919434, acc: 0.8805969953536987)
[2025-02-16 13:32:11,416][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:11,648][root][INFO] - Training Epoch: 1/2, step 12840/23838 completed (loss: 0.317218154668808, acc: 0.9099099040031433)
[2025-02-16 13:32:12,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:12,585][root][INFO] - Training Epoch: 1/2, step 12841/23838 completed (loss: 0.6243185997009277, acc: 0.8333333134651184)
[2025-02-16 13:32:12,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:13,076][root][INFO] - Training Epoch: 1/2, step 12842/23838 completed (loss: 0.2511892020702362, acc: 0.9279279112815857)
[2025-02-16 13:32:13,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:13,556][root][INFO] - Training Epoch: 1/2, step 12843/23838 completed (loss: 0.4278944134712219, acc: 0.8947368264198303)
[2025-02-16 13:32:14,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:14,422][root][INFO] - Training Epoch: 1/2, step 12844/23838 completed (loss: 0.29251936078071594, acc: 0.9358974099159241)
[2025-02-16 13:32:14,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:14,855][root][INFO] - Training Epoch: 1/2, step 12845/23838 completed (loss: 0.26351290941238403, acc: 0.9154929518699646)
[2025-02-16 13:32:15,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:15,266][root][INFO] - Training Epoch: 1/2, step 12846/23838 completed (loss: 0.3167167901992798, acc: 0.8909090757369995)
[2025-02-16 13:32:15,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:15,724][root][INFO] - Training Epoch: 1/2, step 12847/23838 completed (loss: 0.41693273186683655, acc: 0.8983050584793091)
[2025-02-16 13:32:15,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:16,113][root][INFO] - Training Epoch: 1/2, step 12848/23838 completed (loss: 0.058585286140441895, acc: 1.0)
[2025-02-16 13:32:16,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:16,573][root][INFO] - Training Epoch: 1/2, step 12849/23838 completed (loss: 0.15945658087730408, acc: 0.9615384340286255)
[2025-02-16 13:32:16,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:16,968][root][INFO] - Training Epoch: 1/2, step 12850/23838 completed (loss: 0.26073694229125977, acc: 0.8809523582458496)
[2025-02-16 13:32:17,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:17,618][root][INFO] - Training Epoch: 1/2, step 12851/23838 completed (loss: 0.11539236456155777, acc: 0.9626865386962891)
[2025-02-16 13:32:17,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:18,114][root][INFO] - Training Epoch: 1/2, step 12852/23838 completed (loss: 0.1255427896976471, acc: 0.9672130942344666)
[2025-02-16 13:32:18,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:18,594][root][INFO] - Training Epoch: 1/2, step 12853/23838 completed (loss: 0.4351617991924286, acc: 0.8602150678634644)
[2025-02-16 13:32:19,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:19,416][root][INFO] - Training Epoch: 1/2, step 12854/23838 completed (loss: 0.4160802960395813, acc: 0.9007633328437805)
[2025-02-16 13:32:19,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:19,990][root][INFO] - Training Epoch: 1/2, step 12855/23838 completed (loss: 0.5134530067443848, acc: 0.8676470518112183)
[2025-02-16 13:32:20,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:20,551][root][INFO] - Training Epoch: 1/2, step 12856/23838 completed (loss: 0.46641775965690613, acc: 0.8545454740524292)
[2025-02-16 13:32:20,688][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:20,969][root][INFO] - Training Epoch: 1/2, step 12857/23838 completed (loss: 0.26197901368141174, acc: 0.8983050584793091)
[2025-02-16 13:32:21,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:21,534][root][INFO] - Training Epoch: 1/2, step 12858/23838 completed (loss: 0.25350627303123474, acc: 0.9431818127632141)
[2025-02-16 13:32:21,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:22,050][root][INFO] - Training Epoch: 1/2, step 12859/23838 completed (loss: 0.22389793395996094, acc: 0.9473684430122375)
[2025-02-16 13:32:22,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:22,516][root][INFO] - Training Epoch: 1/2, step 12860/23838 completed (loss: 0.3704175055027008, acc: 0.9189189076423645)
[2025-02-16 13:32:22,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:22,986][root][INFO] - Training Epoch: 1/2, step 12861/23838 completed (loss: 0.5872451066970825, acc: 0.8571428656578064)
[2025-02-16 13:32:23,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:23,517][root][INFO] - Training Epoch: 1/2, step 12862/23838 completed (loss: 0.1634277105331421, acc: 0.9609375)
[2025-02-16 13:32:23,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:23,974][root][INFO] - Training Epoch: 1/2, step 12863/23838 completed (loss: 0.3711910545825958, acc: 0.8761904835700989)
[2025-02-16 13:32:24,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:24,643][root][INFO] - Training Epoch: 1/2, step 12864/23838 completed (loss: 0.10226050019264221, acc: 0.9655172228813171)
[2025-02-16 13:32:24,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:25,125][root][INFO] - Training Epoch: 1/2, step 12865/23838 completed (loss: 0.1291266530752182, acc: 0.9646017551422119)
[2025-02-16 13:32:25,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:25,676][root][INFO] - Training Epoch: 1/2, step 12866/23838 completed (loss: 0.3546977639198303, acc: 0.9130434989929199)
[2025-02-16 13:32:25,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:26,212][root][INFO] - Training Epoch: 1/2, step 12867/23838 completed (loss: 0.416812926530838, acc: 0.8639456033706665)
[2025-02-16 13:32:26,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:26,658][root][INFO] - Training Epoch: 1/2, step 12868/23838 completed (loss: 0.24325202405452728, acc: 0.948051929473877)
[2025-02-16 13:32:26,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:27,142][root][INFO] - Training Epoch: 1/2, step 12869/23838 completed (loss: 0.2765232026576996, acc: 0.8928571343421936)
[2025-02-16 13:32:27,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:27,603][root][INFO] - Training Epoch: 1/2, step 12870/23838 completed (loss: 0.3103443682193756, acc: 0.9120000004768372)
[2025-02-16 13:32:28,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:28,483][root][INFO] - Training Epoch: 1/2, step 12871/23838 completed (loss: 0.5611646771430969, acc: 0.8586387634277344)
[2025-02-16 13:32:28,712][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:28,962][root][INFO] - Training Epoch: 1/2, step 12872/23838 completed (loss: 0.5860821604728699, acc: 0.8348624110221863)
[2025-02-16 13:32:29,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:29,373][root][INFO] - Training Epoch: 1/2, step 12873/23838 completed (loss: 0.30641451478004456, acc: 0.9021739363670349)
[2025-02-16 13:32:29,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:29,894][root][INFO] - Training Epoch: 1/2, step 12874/23838 completed (loss: 0.09576372802257538, acc: 0.9745222926139832)
[2025-02-16 13:32:30,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:30,384][root][INFO] - Training Epoch: 1/2, step 12875/23838 completed (loss: 0.4517744183540344, acc: 0.8859649300575256)
[2025-02-16 13:32:30,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:30,980][root][INFO] - Training Epoch: 1/2, step 12876/23838 completed (loss: 0.2118000090122223, acc: 0.9495798349380493)
[2025-02-16 13:32:31,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:31,457][root][INFO] - Training Epoch: 1/2, step 12877/23838 completed (loss: 0.38307011127471924, acc: 0.9224137663841248)
[2025-02-16 13:32:31,660][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:31,885][root][INFO] - Training Epoch: 1/2, step 12878/23838 completed (loss: 0.5740485787391663, acc: 0.8829787373542786)
[2025-02-16 13:32:32,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:32,356][root][INFO] - Training Epoch: 1/2, step 12879/23838 completed (loss: 0.1627855747938156, acc: 0.954023003578186)
[2025-02-16 13:32:32,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:32,789][root][INFO] - Training Epoch: 1/2, step 12880/23838 completed (loss: 0.3630319833755493, acc: 0.8780487775802612)
[2025-02-16 13:32:33,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:33,385][root][INFO] - Training Epoch: 1/2, step 12881/23838 completed (loss: 0.2526758015155792, acc: 0.9245283007621765)
[2025-02-16 13:32:33,621][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:33,912][root][INFO] - Training Epoch: 1/2, step 12882/23838 completed (loss: 0.1524287462234497, acc: 0.957446813583374)
[2025-02-16 13:32:34,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:34,318][root][INFO] - Training Epoch: 1/2, step 12883/23838 completed (loss: 0.5318646430969238, acc: 0.8382353186607361)
[2025-02-16 13:32:34,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:34,868][root][INFO] - Training Epoch: 1/2, step 12884/23838 completed (loss: 0.5633554458618164, acc: 0.848739504814148)
[2025-02-16 13:32:35,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:35,315][root][INFO] - Training Epoch: 1/2, step 12885/23838 completed (loss: 0.3625391125679016, acc: 0.8888888955116272)
[2025-02-16 13:32:35,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:35,739][root][INFO] - Training Epoch: 1/2, step 12886/23838 completed (loss: 0.40580734610557556, acc: 0.8653846383094788)
[2025-02-16 13:32:35,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:36,239][root][INFO] - Training Epoch: 1/2, step 12887/23838 completed (loss: 0.11270621418952942, acc: 0.9836065769195557)
[2025-02-16 13:32:36,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:36,669][root][INFO] - Training Epoch: 1/2, step 12888/23838 completed (loss: 0.11846499890089035, acc: 0.9571428298950195)
[2025-02-16 13:32:36,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:37,166][root][INFO] - Training Epoch: 1/2, step 12889/23838 completed (loss: 0.5452775955200195, acc: 0.8720930218696594)
[2025-02-16 13:32:37,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:37,587][root][INFO] - Training Epoch: 1/2, step 12890/23838 completed (loss: 0.29128023982048035, acc: 0.9354838728904724)
[2025-02-16 13:32:37,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:37,966][root][INFO] - Training Epoch: 1/2, step 12891/23838 completed (loss: 0.47646066546440125, acc: 0.8815789222717285)
[2025-02-16 13:32:38,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:38,457][root][INFO] - Training Epoch: 1/2, step 12892/23838 completed (loss: 0.3811773359775543, acc: 0.8764045238494873)
[2025-02-16 13:32:38,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:39,047][root][INFO] - Training Epoch: 1/2, step 12893/23838 completed (loss: 0.3269157111644745, acc: 0.9171974658966064)
[2025-02-16 13:32:39,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:39,664][root][INFO] - Training Epoch: 1/2, step 12894/23838 completed (loss: 0.1407921016216278, acc: 0.9444444179534912)
[2025-02-16 13:32:39,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:40,181][root][INFO] - Training Epoch: 1/2, step 12895/23838 completed (loss: 0.19340120255947113, acc: 0.9587628841400146)
[2025-02-16 13:32:40,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:40,608][root][INFO] - Training Epoch: 1/2, step 12896/23838 completed (loss: 0.3096270263195038, acc: 0.9438202381134033)
[2025-02-16 13:32:40,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:41,342][root][INFO] - Training Epoch: 1/2, step 12897/23838 completed (loss: 0.21324288845062256, acc: 0.9205297827720642)
[2025-02-16 13:32:41,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:41,945][root][INFO] - Training Epoch: 1/2, step 12898/23838 completed (loss: 0.21270175278186798, acc: 0.9491525292396545)
[2025-02-16 13:32:42,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:42,384][root][INFO] - Training Epoch: 1/2, step 12899/23838 completed (loss: 0.46845555305480957, acc: 0.8214285969734192)
[2025-02-16 13:32:42,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:43,129][root][INFO] - Training Epoch: 1/2, step 12900/23838 completed (loss: 0.9547411799430847, acc: 0.7333333492279053)
[2025-02-16 13:32:43,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:43,553][root][INFO] - Training Epoch: 1/2, step 12901/23838 completed (loss: 0.4835229516029358, acc: 0.942307710647583)
[2025-02-16 13:32:43,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:44,054][root][INFO] - Training Epoch: 1/2, step 12902/23838 completed (loss: 0.4789835512638092, acc: 0.8666666746139526)
[2025-02-16 13:32:44,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:44,468][root][INFO] - Training Epoch: 1/2, step 12903/23838 completed (loss: 0.07447868585586548, acc: 0.9795918464660645)
[2025-02-16 13:32:44,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:44,908][root][INFO] - Training Epoch: 1/2, step 12904/23838 completed (loss: 0.2059137523174286, acc: 0.9491525292396545)
[2025-02-16 13:32:45,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:45,324][root][INFO] - Training Epoch: 1/2, step 12905/23838 completed (loss: 0.5707806944847107, acc: 0.843137264251709)
[2025-02-16 13:32:45,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:45,775][root][INFO] - Training Epoch: 1/2, step 12906/23838 completed (loss: 0.28082558512687683, acc: 0.936170220375061)
[2025-02-16 13:32:46,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:46,286][root][INFO] - Training Epoch: 1/2, step 12907/23838 completed (loss: 0.1510588526725769, acc: 0.9824561476707458)
[2025-02-16 13:32:46,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:46,703][root][INFO] - Training Epoch: 1/2, step 12908/23838 completed (loss: 0.8916013240814209, acc: 0.7766990065574646)
[2025-02-16 13:32:46,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:47,217][root][INFO] - Training Epoch: 1/2, step 12909/23838 completed (loss: 0.2645695209503174, acc: 0.9279279112815857)
[2025-02-16 13:32:47,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:47,677][root][INFO] - Training Epoch: 1/2, step 12910/23838 completed (loss: 0.38591527938842773, acc: 0.9130434989929199)
[2025-02-16 13:32:47,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:48,233][root][INFO] - Training Epoch: 1/2, step 12911/23838 completed (loss: 0.3580591082572937, acc: 0.921875)
[2025-02-16 13:32:48,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:48,638][root][INFO] - Training Epoch: 1/2, step 12912/23838 completed (loss: 0.6802343726158142, acc: 0.8301886916160583)
[2025-02-16 13:32:48,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:49,082][root][INFO] - Training Epoch: 1/2, step 12913/23838 completed (loss: 0.2043185979127884, acc: 0.9473684430122375)
[2025-02-16 13:32:49,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:49,565][root][INFO] - Training Epoch: 1/2, step 12914/23838 completed (loss: 0.9093866944313049, acc: 0.773809552192688)
[2025-02-16 13:32:49,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:49,984][root][INFO] - Training Epoch: 1/2, step 12915/23838 completed (loss: 0.4043588936328888, acc: 0.8823529481887817)
[2025-02-16 13:32:50,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:50,476][root][INFO] - Training Epoch: 1/2, step 12916/23838 completed (loss: 0.14360760152339935, acc: 0.957446813583374)
[2025-02-16 13:32:50,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:50,899][root][INFO] - Training Epoch: 1/2, step 12917/23838 completed (loss: 1.2620892524719238, acc: 0.6829268336296082)
[2025-02-16 13:32:51,131][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:51,378][root][INFO] - Training Epoch: 1/2, step 12918/23838 completed (loss: 0.13029278814792633, acc: 0.95652174949646)
[2025-02-16 13:32:51,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:51,882][root][INFO] - Training Epoch: 1/2, step 12919/23838 completed (loss: 0.5063251852989197, acc: 0.8842975497245789)
[2025-02-16 13:32:52,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:52,297][root][INFO] - Training Epoch: 1/2, step 12920/23838 completed (loss: 0.12445731461048126, acc: 0.9677419066429138)
[2025-02-16 13:32:52,584][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:52,874][root][INFO] - Training Epoch: 1/2, step 12921/23838 completed (loss: 0.594857931137085, acc: 0.8396946787834167)
[2025-02-16 13:32:53,031][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:53,252][root][INFO] - Training Epoch: 1/2, step 12922/23838 completed (loss: 0.38730719685554504, acc: 0.8636363744735718)
[2025-02-16 13:32:53,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:53,665][root][INFO] - Training Epoch: 1/2, step 12923/23838 completed (loss: 0.2443695366382599, acc: 0.9259259104728699)
[2025-02-16 13:32:53,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:54,138][root][INFO] - Training Epoch: 1/2, step 12924/23838 completed (loss: 0.06791753321886063, acc: 1.0)
[2025-02-16 13:32:54,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:54,614][root][INFO] - Training Epoch: 1/2, step 12925/23838 completed (loss: 0.8697627782821655, acc: 0.7674418687820435)
[2025-02-16 13:32:54,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:55,219][root][INFO] - Training Epoch: 1/2, step 12926/23838 completed (loss: 0.40116721391677856, acc: 0.8888888955116272)
[2025-02-16 13:32:55,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:55,742][root][INFO] - Training Epoch: 1/2, step 12927/23838 completed (loss: 0.24525704979896545, acc: 0.942307710647583)
[2025-02-16 13:32:55,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:56,160][root][INFO] - Training Epoch: 1/2, step 12928/23838 completed (loss: 0.6790577173233032, acc: 0.8653846383094788)
[2025-02-16 13:32:56,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:56,562][root][INFO] - Training Epoch: 1/2, step 12929/23838 completed (loss: 0.11530771851539612, acc: 1.0)
[2025-02-16 13:32:56,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:56,974][root][INFO] - Training Epoch: 1/2, step 12930/23838 completed (loss: 0.5771000981330872, acc: 0.8674699068069458)
[2025-02-16 13:32:57,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:57,389][root][INFO] - Training Epoch: 1/2, step 12931/23838 completed (loss: 0.158885195851326, acc: 0.9736841917037964)
[2025-02-16 13:32:57,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:57,805][root][INFO] - Training Epoch: 1/2, step 12932/23838 completed (loss: 0.9522433876991272, acc: 0.7297297120094299)
[2025-02-16 13:32:58,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:58,280][root][INFO] - Training Epoch: 1/2, step 12933/23838 completed (loss: 0.1175542026758194, acc: 0.9696969985961914)
[2025-02-16 13:32:58,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:58,811][root][INFO] - Training Epoch: 1/2, step 12934/23838 completed (loss: 0.5707370042800903, acc: 0.8877550959587097)
[2025-02-16 13:32:59,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:59,275][root][INFO] - Training Epoch: 1/2, step 12935/23838 completed (loss: 0.1281432807445526, acc: 0.9655172228813171)
[2025-02-16 13:32:59,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:32:59,729][root][INFO] - Training Epoch: 1/2, step 12936/23838 completed (loss: 0.05537951737642288, acc: 0.9736841917037964)
[2025-02-16 13:33:00,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:00,561][root][INFO] - Training Epoch: 1/2, step 12937/23838 completed (loss: 0.2864377498626709, acc: 0.926174521446228)
[2025-02-16 13:33:00,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:01,129][root][INFO] - Training Epoch: 1/2, step 12938/23838 completed (loss: 0.5005717873573303, acc: 0.876288652420044)
[2025-02-16 13:33:01,292][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:01,513][root][INFO] - Training Epoch: 1/2, step 12939/23838 completed (loss: 0.48900237679481506, acc: 0.8947368264198303)
[2025-02-16 13:33:01,735][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:01,990][root][INFO] - Training Epoch: 1/2, step 12940/23838 completed (loss: 0.19933009147644043, acc: 0.9333333373069763)
[2025-02-16 13:33:02,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:02,371][root][INFO] - Training Epoch: 1/2, step 12941/23838 completed (loss: 0.42225381731987, acc: 0.8780487775802612)
[2025-02-16 13:33:02,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:03,057][root][INFO] - Training Epoch: 1/2, step 12942/23838 completed (loss: 0.6623594760894775, acc: 0.8165137767791748)
[2025-02-16 13:33:03,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:04,001][root][INFO] - Training Epoch: 1/2, step 12943/23838 completed (loss: 0.34472420811653137, acc: 0.8900523781776428)
[2025-02-16 13:33:04,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:04,427][root][INFO] - Training Epoch: 1/2, step 12944/23838 completed (loss: 0.13171793520450592, acc: 0.942307710647583)
[2025-02-16 13:33:04,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:05,369][root][INFO] - Training Epoch: 1/2, step 12945/23838 completed (loss: 0.3805805742740631, acc: 0.8761904835700989)
[2025-02-16 13:33:05,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:05,837][root][INFO] - Training Epoch: 1/2, step 12946/23838 completed (loss: 0.18764278292655945, acc: 0.9186046719551086)
[2025-02-16 13:33:06,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:06,291][root][INFO] - Training Epoch: 1/2, step 12947/23838 completed (loss: 0.3757250905036926, acc: 0.8831169009208679)
[2025-02-16 13:33:06,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:06,833][root][INFO] - Training Epoch: 1/2, step 12948/23838 completed (loss: 0.18174682557582855, acc: 0.9603960514068604)
[2025-02-16 13:33:07,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:07,335][root][INFO] - Training Epoch: 1/2, step 12949/23838 completed (loss: 0.8993112444877625, acc: 0.8031495809555054)
[2025-02-16 13:33:07,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:08,182][root][INFO] - Training Epoch: 1/2, step 12950/23838 completed (loss: 0.5791655778884888, acc: 0.8563829660415649)
[2025-02-16 13:33:08,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:08,893][root][INFO] - Training Epoch: 1/2, step 12951/23838 completed (loss: 0.1861269474029541, acc: 0.9324324131011963)
[2025-02-16 13:33:09,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:09,378][root][INFO] - Training Epoch: 1/2, step 12952/23838 completed (loss: 0.27184781432151794, acc: 0.8947368264198303)
[2025-02-16 13:33:09,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:09,897][root][INFO] - Training Epoch: 1/2, step 12953/23838 completed (loss: 0.3073517084121704, acc: 0.9240506291389465)
[2025-02-16 13:33:10,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:10,279][root][INFO] - Training Epoch: 1/2, step 12954/23838 completed (loss: 0.26406359672546387, acc: 0.9655172228813171)
[2025-02-16 13:33:10,459][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:10,727][root][INFO] - Training Epoch: 1/2, step 12955/23838 completed (loss: 0.3773111402988434, acc: 0.9117646813392639)
[2025-02-16 13:33:11,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:11,424][root][INFO] - Training Epoch: 1/2, step 12956/23838 completed (loss: 0.47535374760627747, acc: 0.901098906993866)
[2025-02-16 13:33:11,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:11,988][root][INFO] - Training Epoch: 1/2, step 12957/23838 completed (loss: 0.27294057607650757, acc: 0.9230769276618958)
[2025-02-16 13:33:12,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:12,421][root][INFO] - Training Epoch: 1/2, step 12958/23838 completed (loss: 0.4368974566459656, acc: 0.868852436542511)
[2025-02-16 13:33:12,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:12,888][root][INFO] - Training Epoch: 1/2, step 12959/23838 completed (loss: 0.19177505373954773, acc: 0.9599999785423279)
[2025-02-16 13:33:13,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:13,554][root][INFO] - Training Epoch: 1/2, step 12960/23838 completed (loss: 0.7148097157478333, acc: 0.8241758346557617)
[2025-02-16 13:33:13,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:13,972][root][INFO] - Training Epoch: 1/2, step 12961/23838 completed (loss: 0.16218845546245575, acc: 0.9599999785423279)
[2025-02-16 13:33:14,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:14,376][root][INFO] - Training Epoch: 1/2, step 12962/23838 completed (loss: 0.40601369738578796, acc: 0.931034505367279)
[2025-02-16 13:33:14,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:14,733][root][INFO] - Training Epoch: 1/2, step 12963/23838 completed (loss: 0.5546892881393433, acc: 0.8399999737739563)
[2025-02-16 13:33:14,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:15,144][root][INFO] - Training Epoch: 1/2, step 12964/23838 completed (loss: 0.27595749497413635, acc: 0.9545454382896423)
[2025-02-16 13:33:15,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:15,776][root][INFO] - Training Epoch: 1/2, step 12965/23838 completed (loss: 0.11739989370107651, acc: 0.9624413251876831)
[2025-02-16 13:33:16,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:16,467][root][INFO] - Training Epoch: 1/2, step 12966/23838 completed (loss: 0.18751269578933716, acc: 0.9346405267715454)
[2025-02-16 13:33:16,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:17,230][root][INFO] - Training Epoch: 1/2, step 12967/23838 completed (loss: 0.3178454637527466, acc: 0.9130434989929199)
[2025-02-16 13:33:17,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:17,660][root][INFO] - Training Epoch: 1/2, step 12968/23838 completed (loss: 0.2766352891921997, acc: 0.9090909361839294)
[2025-02-16 13:33:17,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:18,077][root][INFO] - Training Epoch: 1/2, step 12969/23838 completed (loss: 0.2767256498336792, acc: 0.9122806787490845)
[2025-02-16 13:33:18,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:18,882][root][INFO] - Training Epoch: 1/2, step 12970/23838 completed (loss: 0.36840394139289856, acc: 0.9057591557502747)
[2025-02-16 13:33:19,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:19,397][root][INFO] - Training Epoch: 1/2, step 12971/23838 completed (loss: 0.3862069547176361, acc: 0.9230769276618958)
[2025-02-16 13:33:19,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:19,839][root][INFO] - Training Epoch: 1/2, step 12972/23838 completed (loss: 0.9575302004814148, acc: 0.7179487347602844)
[2025-02-16 13:33:20,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:20,307][root][INFO] - Training Epoch: 1/2, step 12973/23838 completed (loss: 0.4797510802745819, acc: 0.8857142925262451)
[2025-02-16 13:33:20,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:20,772][root][INFO] - Training Epoch: 1/2, step 12974/23838 completed (loss: 0.47184568643569946, acc: 0.8714285492897034)
[2025-02-16 13:33:21,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:21,248][root][INFO] - Training Epoch: 1/2, step 12975/23838 completed (loss: 0.2346711903810501, acc: 0.920634925365448)
[2025-02-16 13:33:21,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:21,764][root][INFO] - Training Epoch: 1/2, step 12976/23838 completed (loss: 0.5930375456809998, acc: 0.8068181872367859)
[2025-02-16 13:33:21,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:22,191][root][INFO] - Training Epoch: 1/2, step 12977/23838 completed (loss: 0.7181049585342407, acc: 0.8205128312110901)
[2025-02-16 13:33:22,438][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:22,690][root][INFO] - Training Epoch: 1/2, step 12978/23838 completed (loss: 0.2765671908855438, acc: 0.932584285736084)
[2025-02-16 13:33:22,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:23,184][root][INFO] - Training Epoch: 1/2, step 12979/23838 completed (loss: 0.17270052433013916, acc: 0.9642857313156128)
[2025-02-16 13:33:23,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:23,630][root][INFO] - Training Epoch: 1/2, step 12980/23838 completed (loss: 0.5763846635818481, acc: 0.8809523582458496)
[2025-02-16 13:33:23,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:24,190][root][INFO] - Training Epoch: 1/2, step 12981/23838 completed (loss: 0.1484302282333374, acc: 0.9558823704719543)
[2025-02-16 13:33:24,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:24,650][root][INFO] - Training Epoch: 1/2, step 12982/23838 completed (loss: 0.30584120750427246, acc: 0.9008264541625977)
[2025-02-16 13:33:24,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:25,189][root][INFO] - Training Epoch: 1/2, step 12983/23838 completed (loss: 0.2338274121284485, acc: 0.9333333373069763)
[2025-02-16 13:33:25,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:25,675][root][INFO] - Training Epoch: 1/2, step 12984/23838 completed (loss: 0.32366594672203064, acc: 0.9047619104385376)
[2025-02-16 13:33:25,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:26,109][root][INFO] - Training Epoch: 1/2, step 12985/23838 completed (loss: 0.5044038891792297, acc: 0.8888888955116272)
[2025-02-16 13:33:26,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:26,814][root][INFO] - Training Epoch: 1/2, step 12986/23838 completed (loss: 0.3072316348552704, acc: 0.9363636374473572)
[2025-02-16 13:33:27,024][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:27,254][root][INFO] - Training Epoch: 1/2, step 12987/23838 completed (loss: 0.22995087504386902, acc: 0.9047619104385376)
[2025-02-16 13:33:27,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:27,788][root][INFO] - Training Epoch: 1/2, step 12988/23838 completed (loss: 0.25645631551742554, acc: 0.9230769276618958)
[2025-02-16 13:33:27,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:28,186][root][INFO] - Training Epoch: 1/2, step 12989/23838 completed (loss: 0.3677482604980469, acc: 0.8888888955116272)
[2025-02-16 13:33:28,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:28,860][root][INFO] - Training Epoch: 1/2, step 12990/23838 completed (loss: 0.32425758242607117, acc: 0.9420289993286133)
[2025-02-16 13:33:29,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:29,718][root][INFO] - Training Epoch: 1/2, step 12991/23838 completed (loss: 0.26573672890663147, acc: 0.9197080135345459)
[2025-02-16 13:33:29,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:30,147][root][INFO] - Training Epoch: 1/2, step 12992/23838 completed (loss: 0.2904512584209442, acc: 0.9069767594337463)
[2025-02-16 13:33:30,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:30,584][root][INFO] - Training Epoch: 1/2, step 12993/23838 completed (loss: 0.17431999742984772, acc: 0.954954981803894)
[2025-02-16 13:33:30,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:31,144][root][INFO] - Training Epoch: 1/2, step 12994/23838 completed (loss: 0.44328874349594116, acc: 0.8879310488700867)
[2025-02-16 13:33:31,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:31,584][root][INFO] - Training Epoch: 1/2, step 12995/23838 completed (loss: 0.27929890155792236, acc: 0.9160305261611938)
[2025-02-16 13:33:31,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:32,200][root][INFO] - Training Epoch: 1/2, step 12996/23838 completed (loss: 0.4737229347229004, acc: 0.8717948794364929)
[2025-02-16 13:33:32,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:32,638][root][INFO] - Training Epoch: 1/2, step 12997/23838 completed (loss: 0.513260006904602, acc: 0.8615384697914124)
[2025-02-16 13:33:32,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:33,156][root][INFO] - Training Epoch: 1/2, step 12998/23838 completed (loss: 0.24228662252426147, acc: 0.9195402264595032)
[2025-02-16 13:33:33,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:33,682][root][INFO] - Training Epoch: 1/2, step 12999/23838 completed (loss: 0.6126248240470886, acc: 0.8571428656578064)
[2025-02-16 13:33:33,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:34,197][root][INFO] - Training Epoch: 1/2, step 13000/23838 completed (loss: 0.2362787127494812, acc: 0.938144326210022)
[2025-02-16 13:33:34,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:34,675][root][INFO] - Training Epoch: 1/2, step 13001/23838 completed (loss: 0.25044727325439453, acc: 0.9186046719551086)
[2025-02-16 13:33:34,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:35,176][root][INFO] - Training Epoch: 1/2, step 13002/23838 completed (loss: 0.1481258124113083, acc: 0.957446813583374)
[2025-02-16 13:33:35,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:35,690][root][INFO] - Training Epoch: 1/2, step 13003/23838 completed (loss: 0.17137368023395538, acc: 0.9516128897666931)
[2025-02-16 13:33:35,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:36,201][root][INFO] - Training Epoch: 1/2, step 13004/23838 completed (loss: 0.33188313245773315, acc: 0.8910890817642212)
[2025-02-16 13:33:36,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:37,078][root][INFO] - Training Epoch: 1/2, step 13005/23838 completed (loss: 0.23868736624717712, acc: 0.949999988079071)
[2025-02-16 13:33:37,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:37,520][root][INFO] - Training Epoch: 1/2, step 13006/23838 completed (loss: 0.08684642612934113, acc: 0.9805825352668762)
[2025-02-16 13:33:37,767][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:38,047][root][INFO] - Training Epoch: 1/2, step 13007/23838 completed (loss: 0.17872360348701477, acc: 0.9457364082336426)
[2025-02-16 13:33:38,329][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:38,631][root][INFO] - Training Epoch: 1/2, step 13008/23838 completed (loss: 0.4298708140850067, acc: 0.9157894849777222)
[2025-02-16 13:33:38,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:39,081][root][INFO] - Training Epoch: 1/2, step 13009/23838 completed (loss: 0.4863719642162323, acc: 0.8787878751754761)
[2025-02-16 13:33:39,282][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:39,508][root][INFO] - Training Epoch: 1/2, step 13010/23838 completed (loss: 0.1692119538784027, acc: 0.9577465057373047)
[2025-02-16 13:33:39,646][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:39,872][root][INFO] - Training Epoch: 1/2, step 13011/23838 completed (loss: 0.09054108709096909, acc: 0.9807692170143127)
[2025-02-16 13:33:40,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:40,325][root][INFO] - Training Epoch: 1/2, step 13012/23838 completed (loss: 0.19741681218147278, acc: 0.9615384340286255)
[2025-02-16 13:33:40,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:40,780][root][INFO] - Training Epoch: 1/2, step 13013/23838 completed (loss: 0.29472869634628296, acc: 0.9354838728904724)
[2025-02-16 13:33:41,021][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:41,318][root][INFO] - Training Epoch: 1/2, step 13014/23838 completed (loss: 0.06175835803151131, acc: 0.9666666388511658)
[2025-02-16 13:33:41,602][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:41,849][root][INFO] - Training Epoch: 1/2, step 13015/23838 completed (loss: 0.41014033555984497, acc: 0.8983050584793091)
[2025-02-16 13:33:42,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:42,346][root][INFO] - Training Epoch: 1/2, step 13016/23838 completed (loss: 0.28532880544662476, acc: 0.9122806787490845)
[2025-02-16 13:33:42,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:42,815][root][INFO] - Training Epoch: 1/2, step 13017/23838 completed (loss: 0.40874558687210083, acc: 0.8870967626571655)
[2025-02-16 13:33:43,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:43,298][root][INFO] - Training Epoch: 1/2, step 13018/23838 completed (loss: 0.3212909698486328, acc: 0.9090909361839294)
[2025-02-16 13:33:43,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:43,774][root][INFO] - Training Epoch: 1/2, step 13019/23838 completed (loss: 0.2909480035305023, acc: 0.9384615421295166)
[2025-02-16 13:33:44,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:44,256][root][INFO] - Training Epoch: 1/2, step 13020/23838 completed (loss: 0.2425013929605484, acc: 0.9402984976768494)
[2025-02-16 13:33:44,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:44,648][root][INFO] - Training Epoch: 1/2, step 13021/23838 completed (loss: 0.1339520961046219, acc: 0.9354838728904724)
[2025-02-16 13:33:44,880][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:45,118][root][INFO] - Training Epoch: 1/2, step 13022/23838 completed (loss: 0.05648171901702881, acc: 1.0)
[2025-02-16 13:33:45,382][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:45,633][root][INFO] - Training Epoch: 1/2, step 13023/23838 completed (loss: 0.2828464210033417, acc: 0.939393937587738)
[2025-02-16 13:33:45,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:46,097][root][INFO] - Training Epoch: 1/2, step 13024/23838 completed (loss: 0.14733603596687317, acc: 0.9607843160629272)
[2025-02-16 13:33:46,256][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:46,485][root][INFO] - Training Epoch: 1/2, step 13025/23838 completed (loss: 0.3074185848236084, acc: 0.9140625)
[2025-02-16 13:33:46,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:46,921][root][INFO] - Training Epoch: 1/2, step 13026/23838 completed (loss: 0.24509380757808685, acc: 0.9222221970558167)
[2025-02-16 13:33:47,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:47,311][root][INFO] - Training Epoch: 1/2, step 13027/23838 completed (loss: 0.125288724899292, acc: 0.9583333134651184)
[2025-02-16 13:33:47,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:47,765][root][INFO] - Training Epoch: 1/2, step 13028/23838 completed (loss: 0.23292036354541779, acc: 0.9122806787490845)
[2025-02-16 13:33:47,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:48,210][root][INFO] - Training Epoch: 1/2, step 13029/23838 completed (loss: 0.05834279581904411, acc: 0.9900000095367432)
[2025-02-16 13:33:48,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:48,673][root][INFO] - Training Epoch: 1/2, step 13030/23838 completed (loss: 0.20752985775470734, acc: 0.954023003578186)
[2025-02-16 13:33:48,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:49,204][root][INFO] - Training Epoch: 1/2, step 13031/23838 completed (loss: 0.5247515439987183, acc: 0.8374999761581421)
[2025-02-16 13:33:49,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:49,921][root][INFO] - Training Epoch: 1/2, step 13032/23838 completed (loss: 0.2951863706111908, acc: 0.918367326259613)
[2025-02-16 13:33:50,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:50,698][root][INFO] - Training Epoch: 1/2, step 13033/23838 completed (loss: 0.32635703682899475, acc: 0.9217391014099121)
[2025-02-16 13:33:50,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:51,229][root][INFO] - Training Epoch: 1/2, step 13034/23838 completed (loss: 0.2884844243526459, acc: 0.9251700639724731)
[2025-02-16 13:33:51,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:51,701][root][INFO] - Training Epoch: 1/2, step 13035/23838 completed (loss: 0.35390737652778625, acc: 0.9220778942108154)
[2025-02-16 13:33:51,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:52,098][root][INFO] - Training Epoch: 1/2, step 13036/23838 completed (loss: 0.046832166612148285, acc: 1.0)
[2025-02-16 13:33:52,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:52,553][root][INFO] - Training Epoch: 1/2, step 13037/23838 completed (loss: 0.8277644515037537, acc: 0.8333333134651184)
[2025-02-16 13:33:52,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:53,010][root][INFO] - Training Epoch: 1/2, step 13038/23838 completed (loss: 0.10166291892528534, acc: 0.9772727489471436)
[2025-02-16 13:33:53,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:53,530][root][INFO] - Training Epoch: 1/2, step 13039/23838 completed (loss: 0.19608208537101746, acc: 0.9448819160461426)
[2025-02-16 13:33:53,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:53,994][root][INFO] - Training Epoch: 1/2, step 13040/23838 completed (loss: 0.9001852869987488, acc: 0.7260273694992065)
[2025-02-16 13:33:54,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:54,412][root][INFO] - Training Epoch: 1/2, step 13041/23838 completed (loss: 0.31809601187705994, acc: 0.9090909361839294)
[2025-02-16 13:33:54,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:54,940][root][INFO] - Training Epoch: 1/2, step 13042/23838 completed (loss: 0.10700617730617523, acc: 0.9658119678497314)
[2025-02-16 13:33:55,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:55,317][root][INFO] - Training Epoch: 1/2, step 13043/23838 completed (loss: 0.32077738642692566, acc: 0.9146341681480408)
[2025-02-16 13:33:55,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:56,021][root][INFO] - Training Epoch: 1/2, step 13044/23838 completed (loss: 0.3183231055736542, acc: 0.9391891956329346)
[2025-02-16 13:33:56,190][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:56,393][root][INFO] - Training Epoch: 1/2, step 13045/23838 completed (loss: 0.2812126576900482, acc: 0.9117646813392639)
[2025-02-16 13:33:56,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:56,796][root][INFO] - Training Epoch: 1/2, step 13046/23838 completed (loss: 0.1448892205953598, acc: 0.9523809552192688)
[2025-02-16 13:33:57,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:57,628][root][INFO] - Training Epoch: 1/2, step 13047/23838 completed (loss: 0.30234184861183167, acc: 0.902255654335022)
[2025-02-16 13:33:57,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:58,033][root][INFO] - Training Epoch: 1/2, step 13048/23838 completed (loss: 0.1768871694803238, acc: 0.9646017551422119)
[2025-02-16 13:33:58,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:58,495][root][INFO] - Training Epoch: 1/2, step 13049/23838 completed (loss: 0.18700851500034332, acc: 0.9285714030265808)
[2025-02-16 13:33:58,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:58,936][root][INFO] - Training Epoch: 1/2, step 13050/23838 completed (loss: 0.18171808123588562, acc: 0.9599999785423279)
[2025-02-16 13:33:59,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:33:59,466][root][INFO] - Training Epoch: 1/2, step 13051/23838 completed (loss: 0.1869678795337677, acc: 0.9375)
[2025-02-16 13:33:59,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:00,073][root][INFO] - Training Epoch: 1/2, step 13052/23838 completed (loss: 0.46490252017974854, acc: 0.8918918967247009)
[2025-02-16 13:34:00,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:00,724][root][INFO] - Training Epoch: 1/2, step 13053/23838 completed (loss: 0.36867228150367737, acc: 0.9090909361839294)
[2025-02-16 13:34:01,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:01,234][root][INFO] - Training Epoch: 1/2, step 13054/23838 completed (loss: 0.5246419906616211, acc: 0.8157894611358643)
[2025-02-16 13:34:01,475][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:01,721][root][INFO] - Training Epoch: 1/2, step 13055/23838 completed (loss: 0.346155047416687, acc: 0.9055555462837219)
[2025-02-16 13:34:02,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:02,403][root][INFO] - Training Epoch: 1/2, step 13056/23838 completed (loss: 0.3115837275981903, acc: 0.9047619104385376)
[2025-02-16 13:34:02,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:02,918][root][INFO] - Training Epoch: 1/2, step 13057/23838 completed (loss: 0.2166183739900589, acc: 0.9537037014961243)
[2025-02-16 13:34:03,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:03,328][root][INFO] - Training Epoch: 1/2, step 13058/23838 completed (loss: 0.4069509506225586, acc: 0.875)
[2025-02-16 13:34:03,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:03,912][root][INFO] - Training Epoch: 1/2, step 13059/23838 completed (loss: 0.22801412642002106, acc: 0.9591836929321289)
[2025-02-16 13:34:04,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:04,390][root][INFO] - Training Epoch: 1/2, step 13060/23838 completed (loss: 0.20264707505702972, acc: 0.949999988079071)
[2025-02-16 13:34:04,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:05,226][root][INFO] - Training Epoch: 1/2, step 13061/23838 completed (loss: 0.2699545919895172, acc: 0.9395973086357117)
[2025-02-16 13:34:05,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:05,758][root][INFO] - Training Epoch: 1/2, step 13062/23838 completed (loss: 0.18263904750347137, acc: 0.9215686321258545)
[2025-02-16 13:34:06,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:06,473][root][INFO] - Training Epoch: 1/2, step 13063/23838 completed (loss: 0.3921765387058258, acc: 0.8918918967247009)
[2025-02-16 13:34:06,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:07,240][root][INFO] - Training Epoch: 1/2, step 13064/23838 completed (loss: 0.593493640422821, acc: 0.8292682766914368)
[2025-02-16 13:34:07,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:07,725][root][INFO] - Training Epoch: 1/2, step 13065/23838 completed (loss: 0.38332638144493103, acc: 0.8958333134651184)
[2025-02-16 13:34:08,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:08,257][root][INFO] - Training Epoch: 1/2, step 13066/23838 completed (loss: 0.1138780415058136, acc: 0.9523809552192688)
[2025-02-16 13:34:08,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:08,727][root][INFO] - Training Epoch: 1/2, step 13067/23838 completed (loss: 0.20516707003116608, acc: 0.929411768913269)
[2025-02-16 13:34:08,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:09,154][root][INFO] - Training Epoch: 1/2, step 13068/23838 completed (loss: 0.1137692853808403, acc: 0.9767441749572754)
[2025-02-16 13:34:09,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:09,879][root][INFO] - Training Epoch: 1/2, step 13069/23838 completed (loss: 0.17692390084266663, acc: 0.957317054271698)
[2025-02-16 13:34:10,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:10,379][root][INFO] - Training Epoch: 1/2, step 13070/23838 completed (loss: 0.3154425621032715, acc: 0.9154929518699646)
[2025-02-16 13:34:10,715][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:11,053][root][INFO] - Training Epoch: 1/2, step 13071/23838 completed (loss: 0.15992940962314606, acc: 0.9714285731315613)
[2025-02-16 13:34:11,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:11,473][root][INFO] - Training Epoch: 1/2, step 13072/23838 completed (loss: 0.49121055006980896, acc: 0.8717948794364929)
[2025-02-16 13:34:11,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:11,937][root][INFO] - Training Epoch: 1/2, step 13073/23838 completed (loss: 0.08467055857181549, acc: 0.9677419066429138)
[2025-02-16 13:34:12,133][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:12,377][root][INFO] - Training Epoch: 1/2, step 13074/23838 completed (loss: 0.2826522886753082, acc: 0.9259259104728699)
[2025-02-16 13:34:12,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:12,821][root][INFO] - Training Epoch: 1/2, step 13075/23838 completed (loss: 0.435930073261261, acc: 0.9253731369972229)
[2025-02-16 13:34:13,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:13,332][root][INFO] - Training Epoch: 1/2, step 13076/23838 completed (loss: 0.27425235509872437, acc: 0.9275362491607666)
[2025-02-16 13:34:13,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:13,903][root][INFO] - Training Epoch: 1/2, step 13077/23838 completed (loss: 0.1262928545475006, acc: 0.9636363387107849)
[2025-02-16 13:34:14,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:14,646][root][INFO] - Training Epoch: 1/2, step 13078/23838 completed (loss: 0.8837398290634155, acc: 0.6428571343421936)
[2025-02-16 13:34:14,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:15,150][root][INFO] - Training Epoch: 1/2, step 13079/23838 completed (loss: 0.0657239556312561, acc: 0.9879518151283264)
[2025-02-16 13:34:15,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:15,682][root][INFO] - Training Epoch: 1/2, step 13080/23838 completed (loss: 0.4818037450313568, acc: 0.8571428656578064)
[2025-02-16 13:34:15,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:16,194][root][INFO] - Training Epoch: 1/2, step 13081/23838 completed (loss: 0.23154965043067932, acc: 0.9338235259056091)
[2025-02-16 13:34:16,496][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:16,794][root][INFO] - Training Epoch: 1/2, step 13082/23838 completed (loss: 0.355680912733078, acc: 0.9090909361839294)
[2025-02-16 13:34:16,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:17,244][root][INFO] - Training Epoch: 1/2, step 13083/23838 completed (loss: 0.31589367985725403, acc: 0.9024389982223511)
[2025-02-16 13:34:17,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:17,733][root][INFO] - Training Epoch: 1/2, step 13084/23838 completed (loss: 0.41015052795410156, acc: 0.9047619104385376)
[2025-02-16 13:34:18,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:18,477][root][INFO] - Training Epoch: 1/2, step 13085/23838 completed (loss: 0.23675280809402466, acc: 0.9506173133850098)
[2025-02-16 13:34:18,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:19,250][root][INFO] - Training Epoch: 1/2, step 13086/23838 completed (loss: 0.23282109200954437, acc: 0.9428571462631226)
[2025-02-16 13:34:19,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:19,757][root][INFO] - Training Epoch: 1/2, step 13087/23838 completed (loss: 0.3051283061504364, acc: 0.9375)
[2025-02-16 13:34:19,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:20,231][root][INFO] - Training Epoch: 1/2, step 13088/23838 completed (loss: 0.10211514681577682, acc: 0.9841269850730896)
[2025-02-16 13:34:20,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:20,805][root][INFO] - Training Epoch: 1/2, step 13089/23838 completed (loss: 0.25528720021247864, acc: 0.945652186870575)
[2025-02-16 13:34:21,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:21,312][root][INFO] - Training Epoch: 1/2, step 13090/23838 completed (loss: 0.20665636658668518, acc: 0.9350649118423462)
[2025-02-16 13:34:21,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:21,724][root][INFO] - Training Epoch: 1/2, step 13091/23838 completed (loss: 0.25338420271873474, acc: 0.9350649118423462)
[2025-02-16 13:34:21,932][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:22,172][root][INFO] - Training Epoch: 1/2, step 13092/23838 completed (loss: 0.8339413404464722, acc: 0.7796609997749329)
[2025-02-16 13:34:22,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:22,630][root][INFO] - Training Epoch: 1/2, step 13093/23838 completed (loss: 0.2654903531074524, acc: 0.9411764740943909)
[2025-02-16 13:34:22,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:23,072][root][INFO] - Training Epoch: 1/2, step 13094/23838 completed (loss: 0.21619708836078644, acc: 0.9428571462631226)
[2025-02-16 13:34:23,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:23,596][root][INFO] - Training Epoch: 1/2, step 13095/23838 completed (loss: 0.19646091759204865, acc: 0.931506872177124)
[2025-02-16 13:34:23,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:24,054][root][INFO] - Training Epoch: 1/2, step 13096/23838 completed (loss: 0.1075037345290184, acc: 0.9587628841400146)
[2025-02-16 13:34:24,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:24,544][root][INFO] - Training Epoch: 1/2, step 13097/23838 completed (loss: 0.31067848205566406, acc: 0.932692289352417)
[2025-02-16 13:34:24,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:25,018][root][INFO] - Training Epoch: 1/2, step 13098/23838 completed (loss: 0.23394449055194855, acc: 0.9375)
[2025-02-16 13:34:25,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:25,484][root][INFO] - Training Epoch: 1/2, step 13099/23838 completed (loss: 0.11997842788696289, acc: 0.9790209531784058)
[2025-02-16 13:34:25,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:25,882][root][INFO] - Training Epoch: 1/2, step 13100/23838 completed (loss: 0.2837309241294861, acc: 0.9387755393981934)
[2025-02-16 13:34:26,240][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:26,590][root][INFO] - Training Epoch: 1/2, step 13101/23838 completed (loss: 0.3121907413005829, acc: 0.9343065619468689)
[2025-02-16 13:34:26,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:27,136][root][INFO] - Training Epoch: 1/2, step 13102/23838 completed (loss: 0.4027326703071594, acc: 0.8550724387168884)
[2025-02-16 13:34:27,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:27,711][root][INFO] - Training Epoch: 1/2, step 13103/23838 completed (loss: 0.3619520962238312, acc: 0.9156626462936401)
[2025-02-16 13:34:27,954][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:28,210][root][INFO] - Training Epoch: 1/2, step 13104/23838 completed (loss: 0.35170477628707886, acc: 0.9039999842643738)
[2025-02-16 13:34:28,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:28,673][root][INFO] - Training Epoch: 1/2, step 13105/23838 completed (loss: 0.2082872837781906, acc: 0.9722222089767456)
[2025-02-16 13:34:28,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:29,138][root][INFO] - Training Epoch: 1/2, step 13106/23838 completed (loss: 0.43052080273628235, acc: 0.9230769276618958)
[2025-02-16 13:34:29,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:29,571][root][INFO] - Training Epoch: 1/2, step 13107/23838 completed (loss: 0.0879746675491333, acc: 0.9793814420700073)
[2025-02-16 13:34:29,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:30,053][root][INFO] - Training Epoch: 1/2, step 13108/23838 completed (loss: 0.2721099853515625, acc: 0.9375)
[2025-02-16 13:34:30,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:31,089][root][INFO] - Training Epoch: 1/2, step 13109/23838 completed (loss: 0.21176853775978088, acc: 0.9450549483299255)
[2025-02-16 13:34:31,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:31,803][root][INFO] - Training Epoch: 1/2, step 13110/23838 completed (loss: 0.3028923571109772, acc: 0.9166666865348816)
[2025-02-16 13:34:32,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:32,250][root][INFO] - Training Epoch: 1/2, step 13111/23838 completed (loss: 0.4381346106529236, acc: 0.8888888955116272)
[2025-02-16 13:34:32,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:33,215][root][INFO] - Training Epoch: 1/2, step 13112/23838 completed (loss: 0.2712480425834656, acc: 0.8909090757369995)
[2025-02-16 13:34:33,413][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:33,685][root][INFO] - Training Epoch: 1/2, step 13113/23838 completed (loss: 0.32972580194473267, acc: 0.8703703880310059)
[2025-02-16 13:34:34,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:34,309][root][INFO] - Training Epoch: 1/2, step 13114/23838 completed (loss: 0.34006813168525696, acc: 0.9245283007621765)
[2025-02-16 13:34:34,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:35,016][root][INFO] - Training Epoch: 1/2, step 13115/23838 completed (loss: 0.19860656559467316, acc: 0.9485714435577393)
[2025-02-16 13:34:35,334][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:35,639][root][INFO] - Training Epoch: 1/2, step 13116/23838 completed (loss: 0.5051361918449402, acc: 0.8782608509063721)
[2025-02-16 13:34:36,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:36,374][root][INFO] - Training Epoch: 1/2, step 13117/23838 completed (loss: 0.2134505659341812, acc: 0.9508196711540222)
[2025-02-16 13:34:36,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:36,933][root][INFO] - Training Epoch: 1/2, step 13118/23838 completed (loss: 0.05236416310071945, acc: 0.9907407164573669)
[2025-02-16 13:34:37,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:37,415][root][INFO] - Training Epoch: 1/2, step 13119/23838 completed (loss: 0.3158601224422455, acc: 0.9245283007621765)
[2025-02-16 13:34:37,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:37,880][root][INFO] - Training Epoch: 1/2, step 13120/23838 completed (loss: 0.13639314472675323, acc: 0.9468085169792175)
[2025-02-16 13:34:38,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:38,386][root][INFO] - Training Epoch: 1/2, step 13121/23838 completed (loss: 0.5771083235740662, acc: 0.8796296119689941)
[2025-02-16 13:34:38,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:38,792][root][INFO] - Training Epoch: 1/2, step 13122/23838 completed (loss: 0.6199348568916321, acc: 0.8367347121238708)
[2025-02-16 13:34:39,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:39,247][root][INFO] - Training Epoch: 1/2, step 13123/23838 completed (loss: 0.29645735025405884, acc: 0.9305555820465088)
[2025-02-16 13:34:39,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:39,762][root][INFO] - Training Epoch: 1/2, step 13124/23838 completed (loss: 0.08335985243320465, acc: 0.9669421315193176)
[2025-02-16 13:34:40,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:40,212][root][INFO] - Training Epoch: 1/2, step 13125/23838 completed (loss: 0.22702355682849884, acc: 0.9450549483299255)
[2025-02-16 13:34:40,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:40,661][root][INFO] - Training Epoch: 1/2, step 13126/23838 completed (loss: 0.3972978889942169, acc: 0.8823529481887817)
[2025-02-16 13:34:40,855][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:41,077][root][INFO] - Training Epoch: 1/2, step 13127/23838 completed (loss: 0.4613321125507355, acc: 0.9056603908538818)
[2025-02-16 13:34:41,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:41,656][root][INFO] - Training Epoch: 1/2, step 13128/23838 completed (loss: 0.11782429367303848, acc: 0.9646017551422119)
[2025-02-16 13:34:41,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:42,149][root][INFO] - Training Epoch: 1/2, step 13129/23838 completed (loss: 0.11500940471887589, acc: 0.9885057210922241)
[2025-02-16 13:34:42,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:42,556][root][INFO] - Training Epoch: 1/2, step 13130/23838 completed (loss: 0.3771824240684509, acc: 0.8928571343421936)
[2025-02-16 13:34:42,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:43,089][root][INFO] - Training Epoch: 1/2, step 13131/23838 completed (loss: 0.32109305262565613, acc: 0.9263157844543457)
[2025-02-16 13:34:43,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:43,570][root][INFO] - Training Epoch: 1/2, step 13132/23838 completed (loss: 0.1277204006910324, acc: 0.9677419066429138)
[2025-02-16 13:34:43,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:44,183][root][INFO] - Training Epoch: 1/2, step 13133/23838 completed (loss: 0.18372279405593872, acc: 0.9439252614974976)
[2025-02-16 13:34:44,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:44,675][root][INFO] - Training Epoch: 1/2, step 13134/23838 completed (loss: 0.1394500732421875, acc: 0.9268292784690857)
[2025-02-16 13:34:44,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:45,192][root][INFO] - Training Epoch: 1/2, step 13135/23838 completed (loss: 0.24166730046272278, acc: 0.9396551847457886)
[2025-02-16 13:34:45,406][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:45,658][root][INFO] - Training Epoch: 1/2, step 13136/23838 completed (loss: 0.4000341296195984, acc: 0.875)
[2025-02-16 13:34:45,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:46,167][root][INFO] - Training Epoch: 1/2, step 13137/23838 completed (loss: 0.2849789261817932, acc: 0.9142857193946838)
[2025-02-16 13:34:46,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:46,666][root][INFO] - Training Epoch: 1/2, step 13138/23838 completed (loss: 0.2992955446243286, acc: 0.9145299196243286)
[2025-02-16 13:34:46,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:47,097][root][INFO] - Training Epoch: 1/2, step 13139/23838 completed (loss: 0.4973461925983429, acc: 0.8662420511245728)
[2025-02-16 13:34:47,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:47,501][root][INFO] - Training Epoch: 1/2, step 13140/23838 completed (loss: 0.7339377999305725, acc: 0.8153846263885498)
[2025-02-16 13:34:47,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:47,892][root][INFO] - Training Epoch: 1/2, step 13141/23838 completed (loss: 0.4252035915851593, acc: 0.9090909361839294)
[2025-02-16 13:34:48,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:48,350][root][INFO] - Training Epoch: 1/2, step 13142/23838 completed (loss: 0.27281469106674194, acc: 0.932330846786499)
[2025-02-16 13:34:48,559][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:48,784][root][INFO] - Training Epoch: 1/2, step 13143/23838 completed (loss: 0.29896965622901917, acc: 0.9320388436317444)
[2025-02-16 13:34:48,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:49,210][root][INFO] - Training Epoch: 1/2, step 13144/23838 completed (loss: 0.3246718645095825, acc: 0.9158878326416016)
[2025-02-16 13:34:49,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:49,652][root][INFO] - Training Epoch: 1/2, step 13145/23838 completed (loss: 0.49925169348716736, acc: 0.8867924809455872)
[2025-02-16 13:34:49,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:50,111][root][INFO] - Training Epoch: 1/2, step 13146/23838 completed (loss: 0.35663750767707825, acc: 0.90625)
[2025-02-16 13:34:50,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:50,856][root][INFO] - Training Epoch: 1/2, step 13147/23838 completed (loss: 0.47863686084747314, acc: 0.8870967626571655)
[2025-02-16 13:34:51,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:51,333][root][INFO] - Training Epoch: 1/2, step 13148/23838 completed (loss: 0.3742842674255371, acc: 0.9007633328437805)
[2025-02-16 13:34:51,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:51,849][root][INFO] - Training Epoch: 1/2, step 13149/23838 completed (loss: 0.20788487792015076, acc: 0.9489051103591919)
[2025-02-16 13:34:52,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:52,323][root][INFO] - Training Epoch: 1/2, step 13150/23838 completed (loss: 0.2424333095550537, acc: 0.9016393423080444)
[2025-02-16 13:34:52,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:52,815][root][INFO] - Training Epoch: 1/2, step 13151/23838 completed (loss: 0.24818940460681915, acc: 0.931034505367279)
[2025-02-16 13:34:53,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:53,235][root][INFO] - Training Epoch: 1/2, step 13152/23838 completed (loss: 0.3397688865661621, acc: 0.914893627166748)
[2025-02-16 13:34:53,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:53,606][root][INFO] - Training Epoch: 1/2, step 13153/23838 completed (loss: 0.20097716152668, acc: 0.9514563083648682)
[2025-02-16 13:34:53,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:54,069][root][INFO] - Training Epoch: 1/2, step 13154/23838 completed (loss: 0.19269497692584991, acc: 0.9382022619247437)
[2025-02-16 13:34:54,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:54,483][root][INFO] - Training Epoch: 1/2, step 13155/23838 completed (loss: 0.38674187660217285, acc: 0.901098906993866)
[2025-02-16 13:34:54,627][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:54,907][root][INFO] - Training Epoch: 1/2, step 13156/23838 completed (loss: 0.2473336011171341, acc: 0.9207921028137207)
[2025-02-16 13:34:55,177][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:55,427][root][INFO] - Training Epoch: 1/2, step 13157/23838 completed (loss: 0.3207970857620239, acc: 0.8974359035491943)
[2025-02-16 13:34:55,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:55,940][root][INFO] - Training Epoch: 1/2, step 13158/23838 completed (loss: 0.3079546093940735, acc: 0.925000011920929)
[2025-02-16 13:34:56,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:56,390][root][INFO] - Training Epoch: 1/2, step 13159/23838 completed (loss: 0.27835437655448914, acc: 0.9417475461959839)
[2025-02-16 13:34:56,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:56,893][root][INFO] - Training Epoch: 1/2, step 13160/23838 completed (loss: 0.2716505825519562, acc: 0.9044944047927856)
[2025-02-16 13:34:57,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:57,328][root][INFO] - Training Epoch: 1/2, step 13161/23838 completed (loss: 0.20047718286514282, acc: 0.930232584476471)
[2025-02-16 13:34:57,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:57,843][root][INFO] - Training Epoch: 1/2, step 13162/23838 completed (loss: 0.11983685940504074, acc: 0.9736841917037964)
[2025-02-16 13:34:57,993][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:58,274][root][INFO] - Training Epoch: 1/2, step 13163/23838 completed (loss: 0.2664348781108856, acc: 0.9339622855186462)
[2025-02-16 13:34:58,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:58,767][root][INFO] - Training Epoch: 1/2, step 13164/23838 completed (loss: 0.31005725264549255, acc: 0.9112426042556763)
[2025-02-16 13:34:58,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:34:59,160][root][INFO] - Training Epoch: 1/2, step 13165/23838 completed (loss: 0.20977991819381714, acc: 0.9254658222198486)
[2025-02-16 13:34:59,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:00,071][root][INFO] - Training Epoch: 1/2, step 13166/23838 completed (loss: 0.37436118721961975, acc: 0.9059829115867615)
[2025-02-16 13:35:00,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:00,488][root][INFO] - Training Epoch: 1/2, step 13167/23838 completed (loss: 0.4642355442047119, acc: 0.8602941036224365)
[2025-02-16 13:35:00,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:00,857][root][INFO] - Training Epoch: 1/2, step 13168/23838 completed (loss: 0.4862437844276428, acc: 0.8536585569381714)
[2025-02-16 13:35:01,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:01,324][root][INFO] - Training Epoch: 1/2, step 13169/23838 completed (loss: 0.2629173696041107, acc: 0.9464285969734192)
[2025-02-16 13:35:01,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:01,787][root][INFO] - Training Epoch: 1/2, step 13170/23838 completed (loss: 0.18866556882858276, acc: 0.9497206807136536)
[2025-02-16 13:35:01,966][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:02,200][root][INFO] - Training Epoch: 1/2, step 13171/23838 completed (loss: 0.1781618595123291, acc: 0.9577465057373047)
[2025-02-16 13:35:02,385][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:02,648][root][INFO] - Training Epoch: 1/2, step 13172/23838 completed (loss: 0.40078651905059814, acc: 0.9041095972061157)
[2025-02-16 13:35:02,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:03,017][root][INFO] - Training Epoch: 1/2, step 13173/23838 completed (loss: 0.4982234835624695, acc: 0.8399999737739563)
[2025-02-16 13:35:03,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:03,400][root][INFO] - Training Epoch: 1/2, step 13174/23838 completed (loss: 0.4667896628379822, acc: 0.8559321761131287)
[2025-02-16 13:35:03,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:03,869][root][INFO] - Training Epoch: 1/2, step 13175/23838 completed (loss: 0.40186479687690735, acc: 0.8936170339584351)
[2025-02-16 13:35:04,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:04,417][root][INFO] - Training Epoch: 1/2, step 13176/23838 completed (loss: 0.2900867164134979, acc: 0.9329268336296082)
[2025-02-16 13:35:04,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:05,095][root][INFO] - Training Epoch: 1/2, step 13177/23838 completed (loss: 0.23903383314609528, acc: 0.9294605851173401)
[2025-02-16 13:35:05,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:05,464][root][INFO] - Training Epoch: 1/2, step 13178/23838 completed (loss: 0.21267765760421753, acc: 0.9463087320327759)
[2025-02-16 13:35:05,614][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:05,845][root][INFO] - Training Epoch: 1/2, step 13179/23838 completed (loss: 0.3272882103919983, acc: 0.8982036113739014)
[2025-02-16 13:35:06,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:06,281][root][INFO] - Training Epoch: 1/2, step 13180/23838 completed (loss: 0.2292204648256302, acc: 0.9263157844543457)
[2025-02-16 13:35:06,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:06,689][root][INFO] - Training Epoch: 1/2, step 13181/23838 completed (loss: 0.33274465799331665, acc: 0.8902438879013062)
[2025-02-16 13:35:06,835][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:07,075][root][INFO] - Training Epoch: 1/2, step 13182/23838 completed (loss: 0.30936017632484436, acc: 0.9029850959777832)
[2025-02-16 13:35:07,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:07,640][root][INFO] - Training Epoch: 1/2, step 13183/23838 completed (loss: 0.3089063763618469, acc: 0.9013158082962036)
[2025-02-16 13:35:07,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:08,102][root][INFO] - Training Epoch: 1/2, step 13184/23838 completed (loss: 0.07558029145002365, acc: 0.9753086566925049)
[2025-02-16 13:35:08,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:08,551][root][INFO] - Training Epoch: 1/2, step 13185/23838 completed (loss: 0.17403952777385712, acc: 0.9618320465087891)
[2025-02-16 13:35:08,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:09,000][root][INFO] - Training Epoch: 1/2, step 13186/23838 completed (loss: 0.27835938334465027, acc: 0.9200000166893005)
[2025-02-16 13:35:09,219][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:09,476][root][INFO] - Training Epoch: 1/2, step 13187/23838 completed (loss: 0.6044458150863647, acc: 0.797468364238739)
[2025-02-16 13:35:09,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:09,944][root][INFO] - Training Epoch: 1/2, step 13188/23838 completed (loss: 0.5333639979362488, acc: 0.8271604776382446)
[2025-02-16 13:35:10,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:10,386][root][INFO] - Training Epoch: 1/2, step 13189/23838 completed (loss: 0.4049309492111206, acc: 0.8873239159584045)
[2025-02-16 13:35:10,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:11,035][root][INFO] - Training Epoch: 1/2, step 13190/23838 completed (loss: 0.3402017652988434, acc: 0.8991596698760986)
[2025-02-16 13:35:11,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:11,472][root][INFO] - Training Epoch: 1/2, step 13191/23838 completed (loss: 0.2023603320121765, acc: 0.9398906826972961)
[2025-02-16 13:35:11,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:11,936][root][INFO] - Training Epoch: 1/2, step 13192/23838 completed (loss: 0.27290359139442444, acc: 0.9386503100395203)
[2025-02-16 13:35:12,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:12,449][root][INFO] - Training Epoch: 1/2, step 13193/23838 completed (loss: 0.32963523268699646, acc: 0.8799999952316284)
[2025-02-16 13:35:12,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:12,904][root][INFO] - Training Epoch: 1/2, step 13194/23838 completed (loss: 0.4210866689682007, acc: 0.9047619104385376)
[2025-02-16 13:35:13,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:13,398][root][INFO] - Training Epoch: 1/2, step 13195/23838 completed (loss: 0.2027713656425476, acc: 0.9473684430122375)
[2025-02-16 13:35:13,618][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:13,848][root][INFO] - Training Epoch: 1/2, step 13196/23838 completed (loss: 0.24754242599010468, acc: 0.9523809552192688)
[2025-02-16 13:35:14,086][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:14,326][root][INFO] - Training Epoch: 1/2, step 13197/23838 completed (loss: 0.11607452481985092, acc: 0.9830508232116699)
[2025-02-16 13:35:14,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:14,835][root][INFO] - Training Epoch: 1/2, step 13198/23838 completed (loss: 1.040484070777893, acc: 0.7333333492279053)
[2025-02-16 13:35:15,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:15,268][root][INFO] - Training Epoch: 1/2, step 13199/23838 completed (loss: 0.1046886146068573, acc: 0.9590163826942444)
[2025-02-16 13:35:15,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:15,710][root][INFO] - Training Epoch: 1/2, step 13200/23838 completed (loss: 0.44647976756095886, acc: 0.8507462739944458)
[2025-02-16 13:35:15,906][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:16,188][root][INFO] - Training Epoch: 1/2, step 13201/23838 completed (loss: 0.41041454672813416, acc: 0.8679245114326477)
[2025-02-16 13:35:16,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:16,691][root][INFO] - Training Epoch: 1/2, step 13202/23838 completed (loss: 0.2079748809337616, acc: 0.9290322661399841)
[2025-02-16 13:35:16,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:17,266][root][INFO] - Training Epoch: 1/2, step 13203/23838 completed (loss: 0.2689695656299591, acc: 0.9222221970558167)
[2025-02-16 13:35:17,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:17,698][root][INFO] - Training Epoch: 1/2, step 13204/23838 completed (loss: 0.15774406492710114, acc: 0.949999988079071)
[2025-02-16 13:35:17,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:18,167][root][INFO] - Training Epoch: 1/2, step 13205/23838 completed (loss: 0.2947080135345459, acc: 0.9262295365333557)
[2025-02-16 13:35:18,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:18,749][root][INFO] - Training Epoch: 1/2, step 13206/23838 completed (loss: 0.24094469845294952, acc: 0.9317269325256348)
[2025-02-16 13:35:18,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:19,219][root][INFO] - Training Epoch: 1/2, step 13207/23838 completed (loss: 0.3399241864681244, acc: 0.8936170339584351)
[2025-02-16 13:35:19,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:19,714][root][INFO] - Training Epoch: 1/2, step 13208/23838 completed (loss: 0.21988433599472046, acc: 0.9273743033409119)
[2025-02-16 13:35:19,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:20,106][root][INFO] - Training Epoch: 1/2, step 13209/23838 completed (loss: 0.12405060231685638, acc: 0.9586777091026306)
[2025-02-16 13:35:20,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:20,619][root][INFO] - Training Epoch: 1/2, step 13210/23838 completed (loss: 0.08514616638422012, acc: 0.9824561476707458)
[2025-02-16 13:35:20,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:21,124][root][INFO] - Training Epoch: 1/2, step 13211/23838 completed (loss: 0.15227460861206055, acc: 0.9677419066429138)
[2025-02-16 13:35:21,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:21,724][root][INFO] - Training Epoch: 1/2, step 13212/23838 completed (loss: 0.24000556766986847, acc: 0.9473684430122375)
[2025-02-16 13:35:21,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:22,157][root][INFO] - Training Epoch: 1/2, step 13213/23838 completed (loss: 0.29339784383773804, acc: 0.915032684803009)
[2025-02-16 13:35:22,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:22,855][root][INFO] - Training Epoch: 1/2, step 13214/23838 completed (loss: 0.22995860874652863, acc: 0.9444444179534912)
[2025-02-16 13:35:23,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:23,340][root][INFO] - Training Epoch: 1/2, step 13215/23838 completed (loss: 0.30065837502479553, acc: 0.9278350472450256)
[2025-02-16 13:35:23,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:23,765][root][INFO] - Training Epoch: 1/2, step 13216/23838 completed (loss: 0.3971347212791443, acc: 0.8703703880310059)
[2025-02-16 13:35:23,991][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:24,237][root][INFO] - Training Epoch: 1/2, step 13217/23838 completed (loss: 0.5663286447525024, acc: 0.8642857074737549)
[2025-02-16 13:35:24,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:24,662][root][INFO] - Training Epoch: 1/2, step 13218/23838 completed (loss: 0.5974835753440857, acc: 0.8571428656578064)
[2025-02-16 13:35:24,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:25,119][root][INFO] - Training Epoch: 1/2, step 13219/23838 completed (loss: 0.25186169147491455, acc: 0.9007633328437805)
[2025-02-16 13:35:25,384][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:25,619][root][INFO] - Training Epoch: 1/2, step 13220/23838 completed (loss: 0.07070474326610565, acc: 0.9895287752151489)
[2025-02-16 13:35:25,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:26,025][root][INFO] - Training Epoch: 1/2, step 13221/23838 completed (loss: 0.31246787309646606, acc: 0.9056603908538818)
[2025-02-16 13:35:26,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:26,501][root][INFO] - Training Epoch: 1/2, step 13222/23838 completed (loss: 0.28748610615730286, acc: 0.9108280539512634)
[2025-02-16 13:35:26,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:27,023][root][INFO] - Training Epoch: 1/2, step 13223/23838 completed (loss: 0.2826838791370392, acc: 0.9130434989929199)
[2025-02-16 13:35:27,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:27,508][root][INFO] - Training Epoch: 1/2, step 13224/23838 completed (loss: 0.3887678384780884, acc: 0.8738738894462585)
[2025-02-16 13:35:27,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:28,338][root][INFO] - Training Epoch: 1/2, step 13225/23838 completed (loss: 0.18826745450496674, acc: 0.9583333134651184)
[2025-02-16 13:35:28,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:28,788][root][INFO] - Training Epoch: 1/2, step 13226/23838 completed (loss: 0.2085898071527481, acc: 0.9514563083648682)
[2025-02-16 13:35:28,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:29,219][root][INFO] - Training Epoch: 1/2, step 13227/23838 completed (loss: 0.29872995615005493, acc: 0.9236640930175781)
[2025-02-16 13:35:29,431][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:29,672][root][INFO] - Training Epoch: 1/2, step 13228/23838 completed (loss: 0.22498485445976257, acc: 0.9557521939277649)
[2025-02-16 13:35:29,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:30,134][root][INFO] - Training Epoch: 1/2, step 13229/23838 completed (loss: 0.14809508621692657, acc: 0.9523809552192688)
[2025-02-16 13:35:30,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:30,614][root][INFO] - Training Epoch: 1/2, step 13230/23838 completed (loss: 0.11216820776462555, acc: 0.9586777091026306)
[2025-02-16 13:35:30,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:31,340][root][INFO] - Training Epoch: 1/2, step 13231/23838 completed (loss: 0.1696653515100479, acc: 0.9581589698791504)
[2025-02-16 13:35:31,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:31,849][root][INFO] - Training Epoch: 1/2, step 13232/23838 completed (loss: 0.19641269743442535, acc: 0.9526627063751221)
[2025-02-16 13:35:31,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:32,220][root][INFO] - Training Epoch: 1/2, step 13233/23838 completed (loss: 0.05788346379995346, acc: 0.9909090995788574)
[2025-02-16 13:35:32,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:32,819][root][INFO] - Training Epoch: 1/2, step 13234/23838 completed (loss: 0.23036383092403412, acc: 0.9292929172515869)
[2025-02-16 13:35:32,980][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:33,221][root][INFO] - Training Epoch: 1/2, step 13235/23838 completed (loss: 0.39589372277259827, acc: 0.8833333253860474)
[2025-02-16 13:35:33,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:33,786][root][INFO] - Training Epoch: 1/2, step 13236/23838 completed (loss: 0.19845475256443024, acc: 0.9485294222831726)
[2025-02-16 13:35:33,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:34,231][root][INFO] - Training Epoch: 1/2, step 13237/23838 completed (loss: 0.36949875950813293, acc: 0.9134615659713745)
[2025-02-16 13:35:34,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:34,695][root][INFO] - Training Epoch: 1/2, step 13238/23838 completed (loss: 0.33771616220474243, acc: 0.9292929172515869)
[2025-02-16 13:35:34,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:35,203][root][INFO] - Training Epoch: 1/2, step 13239/23838 completed (loss: 0.202985942363739, acc: 0.9236111044883728)
[2025-02-16 13:35:35,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:35,652][root][INFO] - Training Epoch: 1/2, step 13240/23838 completed (loss: 0.30142372846603394, acc: 0.9338235259056091)
[2025-02-16 13:35:35,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:36,163][root][INFO] - Training Epoch: 1/2, step 13241/23838 completed (loss: 0.24140340089797974, acc: 0.93388432264328)
[2025-02-16 13:35:36,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:36,616][root][INFO] - Training Epoch: 1/2, step 13242/23838 completed (loss: 0.30618518590927124, acc: 0.9140625)
[2025-02-16 13:35:36,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:36,992][root][INFO] - Training Epoch: 1/2, step 13243/23838 completed (loss: 0.13354936242103577, acc: 0.9655172228813171)
[2025-02-16 13:35:37,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:37,767][root][INFO] - Training Epoch: 1/2, step 13244/23838 completed (loss: 0.2660636007785797, acc: 0.9464285969734192)
[2025-02-16 13:35:38,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:38,291][root][INFO] - Training Epoch: 1/2, step 13245/23838 completed (loss: 0.1625913679599762, acc: 0.9466666579246521)
[2025-02-16 13:35:38,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:38,764][root][INFO] - Training Epoch: 1/2, step 13246/23838 completed (loss: 0.44434699416160583, acc: 0.8895348906517029)
[2025-02-16 13:35:38,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:39,241][root][INFO] - Training Epoch: 1/2, step 13247/23838 completed (loss: 0.5654444098472595, acc: 0.8443113565444946)
[2025-02-16 13:35:39,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:39,677][root][INFO] - Training Epoch: 1/2, step 13248/23838 completed (loss: 0.3531351685523987, acc: 0.89682537317276)
[2025-02-16 13:35:39,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:40,194][root][INFO] - Training Epoch: 1/2, step 13249/23838 completed (loss: 0.46158501505851746, acc: 0.8513513803482056)
[2025-02-16 13:35:40,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:40,726][root][INFO] - Training Epoch: 1/2, step 13250/23838 completed (loss: 0.16631627082824707, acc: 0.9497487545013428)
[2025-02-16 13:35:41,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:41,512][root][INFO] - Training Epoch: 1/2, step 13251/23838 completed (loss: 0.3797835111618042, acc: 0.875)
[2025-02-16 13:35:41,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:41,966][root][INFO] - Training Epoch: 1/2, step 13252/23838 completed (loss: 0.2731657028198242, acc: 0.9285714030265808)
[2025-02-16 13:35:42,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:42,326][root][INFO] - Training Epoch: 1/2, step 13253/23838 completed (loss: 0.16532322764396667, acc: 0.9578313231468201)
[2025-02-16 13:35:42,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:42,762][root][INFO] - Training Epoch: 1/2, step 13254/23838 completed (loss: 0.5728943943977356, acc: 0.8539325594902039)
[2025-02-16 13:35:42,976][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:43,228][root][INFO] - Training Epoch: 1/2, step 13255/23838 completed (loss: 0.26812198758125305, acc: 0.9350649118423462)
[2025-02-16 13:35:43,510][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:43,804][root][INFO] - Training Epoch: 1/2, step 13256/23838 completed (loss: 0.22058190405368805, acc: 0.9418604373931885)
[2025-02-16 13:35:44,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:44,692][root][INFO] - Training Epoch: 1/2, step 13257/23838 completed (loss: 0.1368529498577118, acc: 0.957446813583374)
[2025-02-16 13:35:44,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:45,224][root][INFO] - Training Epoch: 1/2, step 13258/23838 completed (loss: 0.17249682545661926, acc: 0.945652186870575)
[2025-02-16 13:35:45,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:45,705][root][INFO] - Training Epoch: 1/2, step 13259/23838 completed (loss: 0.4952068626880646, acc: 0.8444444537162781)
[2025-02-16 13:35:45,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:46,177][root][INFO] - Training Epoch: 1/2, step 13260/23838 completed (loss: 0.40580281615257263, acc: 0.8877550959587097)
[2025-02-16 13:35:46,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:46,583][root][INFO] - Training Epoch: 1/2, step 13261/23838 completed (loss: 0.5610406398773193, acc: 0.8214285969734192)
[2025-02-16 13:35:46,785][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:47,028][root][INFO] - Training Epoch: 1/2, step 13262/23838 completed (loss: 0.6432121396064758, acc: 0.792792797088623)
[2025-02-16 13:35:47,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:47,450][root][INFO] - Training Epoch: 1/2, step 13263/23838 completed (loss: 0.30958420038223267, acc: 0.9108280539512634)
[2025-02-16 13:35:47,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:48,016][root][INFO] - Training Epoch: 1/2, step 13264/23838 completed (loss: 0.31558409333229065, acc: 0.9114583134651184)
[2025-02-16 13:35:48,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:48,381][root][INFO] - Training Epoch: 1/2, step 13265/23838 completed (loss: 0.3076651394367218, acc: 0.9252336621284485)
[2025-02-16 13:35:48,564][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:48,798][root][INFO] - Training Epoch: 1/2, step 13266/23838 completed (loss: 0.3106542229652405, acc: 0.9019607901573181)
[2025-02-16 13:35:48,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:49,256][root][INFO] - Training Epoch: 1/2, step 13267/23838 completed (loss: 0.7462756037712097, acc: 0.7761194109916687)
[2025-02-16 13:35:49,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:49,669][root][INFO] - Training Epoch: 1/2, step 13268/23838 completed (loss: 0.7495584487915039, acc: 0.800000011920929)
[2025-02-16 13:35:49,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:50,144][root][INFO] - Training Epoch: 1/2, step 13269/23838 completed (loss: 0.5294778347015381, acc: 0.8725489974021912)
[2025-02-16 13:35:50,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:50,591][root][INFO] - Training Epoch: 1/2, step 13270/23838 completed (loss: 0.5834463238716125, acc: 0.8421052694320679)
[2025-02-16 13:35:50,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:51,223][root][INFO] - Training Epoch: 1/2, step 13271/23838 completed (loss: 0.43148231506347656, acc: 0.8540145754814148)
[2025-02-16 13:35:51,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:51,840][root][INFO] - Training Epoch: 1/2, step 13272/23838 completed (loss: 0.43249794840812683, acc: 0.884353756904602)
[2025-02-16 13:35:52,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:52,304][root][INFO] - Training Epoch: 1/2, step 13273/23838 completed (loss: 0.393370658159256, acc: 0.8773584961891174)
[2025-02-16 13:35:52,486][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:52,731][root][INFO] - Training Epoch: 1/2, step 13274/23838 completed (loss: 0.4592597186565399, acc: 0.8829787373542786)
[2025-02-16 13:35:52,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:53,220][root][INFO] - Training Epoch: 1/2, step 13275/23838 completed (loss: 0.19193388521671295, acc: 0.9506173133850098)
[2025-02-16 13:35:53,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:53,706][root][INFO] - Training Epoch: 1/2, step 13276/23838 completed (loss: 0.3467794358730316, acc: 0.9119496941566467)
[2025-02-16 13:35:53,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:54,128][root][INFO] - Training Epoch: 1/2, step 13277/23838 completed (loss: 0.383226215839386, acc: 0.8926174640655518)
[2025-02-16 13:35:54,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:54,547][root][INFO] - Training Epoch: 1/2, step 13278/23838 completed (loss: 0.3780604302883148, acc: 0.9109588861465454)
[2025-02-16 13:35:54,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:55,013][root][INFO] - Training Epoch: 1/2, step 13279/23838 completed (loss: 0.12211273610591888, acc: 0.9583333134651184)
[2025-02-16 13:35:55,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:55,540][root][INFO] - Training Epoch: 1/2, step 13280/23838 completed (loss: 0.21800078451633453, acc: 0.9213973879814148)
[2025-02-16 13:35:55,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:55,965][root][INFO] - Training Epoch: 1/2, step 13281/23838 completed (loss: 0.3891400694847107, acc: 0.8939393758773804)
[2025-02-16 13:35:56,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:56,393][root][INFO] - Training Epoch: 1/2, step 13282/23838 completed (loss: 0.4431372880935669, acc: 0.8783783912658691)
[2025-02-16 13:35:56,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:56,871][root][INFO] - Training Epoch: 1/2, step 13283/23838 completed (loss: 0.19321982562541962, acc: 0.9460784196853638)
[2025-02-16 13:35:57,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:57,315][root][INFO] - Training Epoch: 1/2, step 13284/23838 completed (loss: 0.18287937343120575, acc: 0.9527559280395508)
[2025-02-16 13:35:57,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:57,722][root][INFO] - Training Epoch: 1/2, step 13285/23838 completed (loss: 0.16042542457580566, acc: 0.9523809552192688)
[2025-02-16 13:35:57,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:58,150][root][INFO] - Training Epoch: 1/2, step 13286/23838 completed (loss: 0.9730740785598755, acc: 0.7307692170143127)
[2025-02-16 13:35:58,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:58,513][root][INFO] - Training Epoch: 1/2, step 13287/23838 completed (loss: 0.31160759925842285, acc: 0.9215686321258545)
[2025-02-16 13:35:58,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:58,955][root][INFO] - Training Epoch: 1/2, step 13288/23838 completed (loss: 0.2735896706581116, acc: 0.9277108311653137)
[2025-02-16 13:35:59,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:59,371][root][INFO] - Training Epoch: 1/2, step 13289/23838 completed (loss: 0.43367642164230347, acc: 0.8785046935081482)
[2025-02-16 13:35:59,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:35:59,748][root][INFO] - Training Epoch: 1/2, step 13290/23838 completed (loss: 0.21600376069545746, acc: 0.9433962106704712)
[2025-02-16 13:35:59,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:00,118][root][INFO] - Training Epoch: 1/2, step 13291/23838 completed (loss: 0.3677771985530853, acc: 0.8873239159584045)
[2025-02-16 13:36:00,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:00,490][root][INFO] - Training Epoch: 1/2, step 13292/23838 completed (loss: 0.17525546252727509, acc: 0.9591836929321289)
[2025-02-16 13:36:00,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:00,890][root][INFO] - Training Epoch: 1/2, step 13293/23838 completed (loss: 0.3486431837081909, acc: 0.908450722694397)
[2025-02-16 13:36:01,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:01,274][root][INFO] - Training Epoch: 1/2, step 13294/23838 completed (loss: 0.22991631925106049, acc: 0.939393937587738)
[2025-02-16 13:36:01,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:01,735][root][INFO] - Training Epoch: 1/2, step 13295/23838 completed (loss: 0.7282059788703918, acc: 0.817460298538208)
[2025-02-16 13:36:01,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:02,268][root][INFO] - Training Epoch: 1/2, step 13296/23838 completed (loss: 0.26130056381225586, acc: 0.9452736377716064)
[2025-02-16 13:36:02,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:02,741][root][INFO] - Training Epoch: 1/2, step 13297/23838 completed (loss: 0.13520853221416473, acc: 0.9440559148788452)
[2025-02-16 13:36:02,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:03,252][root][INFO] - Training Epoch: 1/2, step 13298/23838 completed (loss: 0.48404037952423096, acc: 0.8541666865348816)
[2025-02-16 13:36:03,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:03,725][root][INFO] - Training Epoch: 1/2, step 13299/23838 completed (loss: 0.6445794701576233, acc: 0.8264462947845459)
[2025-02-16 13:36:03,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:04,150][root][INFO] - Training Epoch: 1/2, step 13300/23838 completed (loss: 0.1361389458179474, acc: 0.9748427867889404)
[2025-02-16 13:36:04,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:04,571][root][INFO] - Training Epoch: 1/2, step 13301/23838 completed (loss: 0.3796563744544983, acc: 0.9044944047927856)
[2025-02-16 13:36:04,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:05,131][root][INFO] - Training Epoch: 1/2, step 13302/23838 completed (loss: 0.27801594138145447, acc: 0.931034505367279)
[2025-02-16 13:36:05,349][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:05,579][root][INFO] - Training Epoch: 1/2, step 13303/23838 completed (loss: 0.32810306549072266, acc: 0.9319728016853333)
[2025-02-16 13:36:05,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:06,081][root][INFO] - Training Epoch: 1/2, step 13304/23838 completed (loss: 0.48510420322418213, acc: 0.8971962332725525)
[2025-02-16 13:36:06,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:06,577][root][INFO] - Training Epoch: 1/2, step 13305/23838 completed (loss: 0.14402510225772858, acc: 0.96875)
[2025-02-16 13:36:06,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:06,986][root][INFO] - Training Epoch: 1/2, step 13306/23838 completed (loss: 0.07843545824289322, acc: 0.9863013625144958)
[2025-02-16 13:36:07,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:07,375][root][INFO] - Training Epoch: 1/2, step 13307/23838 completed (loss: 0.16015565395355225, acc: 0.9370078444480896)
[2025-02-16 13:36:07,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:07,765][root][INFO] - Training Epoch: 1/2, step 13308/23838 completed (loss: 0.47443217039108276, acc: 0.8448275923728943)
[2025-02-16 13:36:07,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:08,196][root][INFO] - Training Epoch: 1/2, step 13309/23838 completed (loss: 0.33756768703460693, acc: 0.8730158805847168)
[2025-02-16 13:36:08,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:08,655][root][INFO] - Training Epoch: 1/2, step 13310/23838 completed (loss: 0.28460872173309326, acc: 0.9243243336677551)
[2025-02-16 13:36:08,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:09,137][root][INFO] - Training Epoch: 1/2, step 13311/23838 completed (loss: 0.28024211525917053, acc: 0.925000011920929)
[2025-02-16 13:36:09,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:09,520][root][INFO] - Training Epoch: 1/2, step 13312/23838 completed (loss: 0.6353858709335327, acc: 0.813725471496582)
[2025-02-16 13:36:09,725][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:09,960][root][INFO] - Training Epoch: 1/2, step 13313/23838 completed (loss: 0.42455312609672546, acc: 0.8769230842590332)
[2025-02-16 13:36:10,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:10,362][root][INFO] - Training Epoch: 1/2, step 13314/23838 completed (loss: 0.33056721091270447, acc: 0.895348846912384)
[2025-02-16 13:36:10,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:10,810][root][INFO] - Training Epoch: 1/2, step 13315/23838 completed (loss: 0.18871092796325684, acc: 0.9754098653793335)
[2025-02-16 13:36:11,026][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:11,270][root][INFO] - Training Epoch: 1/2, step 13316/23838 completed (loss: 0.2926747500896454, acc: 0.8888888955116272)
[2025-02-16 13:36:11,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:11,734][root][INFO] - Training Epoch: 1/2, step 13317/23838 completed (loss: 0.28557249903678894, acc: 0.9239130616188049)
[2025-02-16 13:36:11,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:12,132][root][INFO] - Training Epoch: 1/2, step 13318/23838 completed (loss: 0.21430271863937378, acc: 0.9384615421295166)
[2025-02-16 13:36:12,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:12,544][root][INFO] - Training Epoch: 1/2, step 13319/23838 completed (loss: 0.31994450092315674, acc: 0.9411764740943909)
[2025-02-16 13:36:12,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:12,994][root][INFO] - Training Epoch: 1/2, step 13320/23838 completed (loss: 0.34873223304748535, acc: 0.9153845906257629)
[2025-02-16 13:36:13,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:13,440][root][INFO] - Training Epoch: 1/2, step 13321/23838 completed (loss: 0.4797027111053467, acc: 0.8659793734550476)
[2025-02-16 13:36:13,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:13,902][root][INFO] - Training Epoch: 1/2, step 13322/23838 completed (loss: 0.27510443329811096, acc: 0.9428571462631226)
[2025-02-16 13:36:14,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:14,382][root][INFO] - Training Epoch: 1/2, step 13323/23838 completed (loss: 0.1233760416507721, acc: 0.9722222089767456)
[2025-02-16 13:36:14,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:14,903][root][INFO] - Training Epoch: 1/2, step 13324/23838 completed (loss: 0.4529557526111603, acc: 0.8993710875511169)
[2025-02-16 13:36:15,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:15,396][root][INFO] - Training Epoch: 1/2, step 13325/23838 completed (loss: 0.42506763339042664, acc: 0.8721804618835449)
[2025-02-16 13:36:15,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:15,858][root][INFO] - Training Epoch: 1/2, step 13326/23838 completed (loss: 0.5041775107383728, acc: 0.8671875)
[2025-02-16 13:36:16,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:16,263][root][INFO] - Training Epoch: 1/2, step 13327/23838 completed (loss: 0.14618678390979767, acc: 0.9459459185600281)
[2025-02-16 13:36:16,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:16,715][root][INFO] - Training Epoch: 1/2, step 13328/23838 completed (loss: 0.5617690086364746, acc: 0.8527131676673889)
[2025-02-16 13:36:16,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:17,229][root][INFO] - Training Epoch: 1/2, step 13329/23838 completed (loss: 0.7443975210189819, acc: 0.7755101919174194)
[2025-02-16 13:36:17,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:17,681][root][INFO] - Training Epoch: 1/2, step 13330/23838 completed (loss: 0.4273001551628113, acc: 0.9021739363670349)
[2025-02-16 13:36:17,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:18,163][root][INFO] - Training Epoch: 1/2, step 13331/23838 completed (loss: 0.32732436060905457, acc: 0.9237667918205261)
[2025-02-16 13:36:18,360][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:18,585][root][INFO] - Training Epoch: 1/2, step 13332/23838 completed (loss: 0.1885330080986023, acc: 0.9420289993286133)
[2025-02-16 13:36:18,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:19,007][root][INFO] - Training Epoch: 1/2, step 13333/23838 completed (loss: 0.3292043209075928, acc: 0.9266055226325989)
[2025-02-16 13:36:19,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:19,414][root][INFO] - Training Epoch: 1/2, step 13334/23838 completed (loss: 1.0908937454223633, acc: 0.761904776096344)
[2025-02-16 13:36:19,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:19,815][root][INFO] - Training Epoch: 1/2, step 13335/23838 completed (loss: 0.4223856031894684, acc: 0.8518518805503845)
[2025-02-16 13:36:19,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:20,208][root][INFO] - Training Epoch: 1/2, step 13336/23838 completed (loss: 0.35781556367874146, acc: 0.912162184715271)
[2025-02-16 13:36:20,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:20,635][root][INFO] - Training Epoch: 1/2, step 13337/23838 completed (loss: 0.5954056978225708, acc: 0.8368794322013855)
[2025-02-16 13:36:20,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:21,086][root][INFO] - Training Epoch: 1/2, step 13338/23838 completed (loss: 0.329428106546402, acc: 0.9026548862457275)
[2025-02-16 13:36:21,285][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:21,520][root][INFO] - Training Epoch: 1/2, step 13339/23838 completed (loss: 0.16528600454330444, acc: 0.9453125)
[2025-02-16 13:36:21,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:21,983][root][INFO] - Training Epoch: 1/2, step 13340/23838 completed (loss: 0.8216651082038879, acc: 0.807692289352417)
[2025-02-16 13:36:22,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:22,406][root][INFO] - Training Epoch: 1/2, step 13341/23838 completed (loss: 0.19028398394584656, acc: 0.9494949579238892)
[2025-02-16 13:36:22,567][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:22,805][root][INFO] - Training Epoch: 1/2, step 13342/23838 completed (loss: 0.18086566030979156, acc: 0.9305555820465088)
[2025-02-16 13:36:23,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:23,310][root][INFO] - Training Epoch: 1/2, step 13343/23838 completed (loss: 0.403734415769577, acc: 0.8880000114440918)
[2025-02-16 13:36:23,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:23,755][root][INFO] - Training Epoch: 1/2, step 13344/23838 completed (loss: 0.15447653830051422, acc: 0.9560439586639404)
[2025-02-16 13:36:23,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:24,133][root][INFO] - Training Epoch: 1/2, step 13345/23838 completed (loss: 0.27316299080848694, acc: 0.9191918969154358)
[2025-02-16 13:36:24,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:24,578][root][INFO] - Training Epoch: 1/2, step 13346/23838 completed (loss: 0.2115289866924286, acc: 0.9404761791229248)
[2025-02-16 13:36:24,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:25,008][root][INFO] - Training Epoch: 1/2, step 13347/23838 completed (loss: 0.32213884592056274, acc: 0.9029850959777832)
[2025-02-16 13:36:25,207][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:25,426][root][INFO] - Training Epoch: 1/2, step 13348/23838 completed (loss: 0.19826938211917877, acc: 0.9369369149208069)
[2025-02-16 13:36:25,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:25,816][root][INFO] - Training Epoch: 1/2, step 13349/23838 completed (loss: 0.5500103831291199, acc: 0.8394160866737366)
[2025-02-16 13:36:26,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:26,294][root][INFO] - Training Epoch: 1/2, step 13350/23838 completed (loss: 0.2337067723274231, acc: 0.9341317415237427)
[2025-02-16 13:36:26,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:26,736][root][INFO] - Training Epoch: 1/2, step 13351/23838 completed (loss: 0.14218805730342865, acc: 0.9807692170143127)
[2025-02-16 13:36:26,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:27,185][root][INFO] - Training Epoch: 1/2, step 13352/23838 completed (loss: 0.09978817403316498, acc: 0.9591836929321289)
[2025-02-16 13:36:27,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:27,609][root][INFO] - Training Epoch: 1/2, step 13353/23838 completed (loss: 0.5222606062889099, acc: 0.8999999761581421)
[2025-02-16 13:36:27,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:28,060][root][INFO] - Training Epoch: 1/2, step 13354/23838 completed (loss: 0.576443612575531, acc: 0.8571428656578064)
[2025-02-16 13:36:28,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:28,509][root][INFO] - Training Epoch: 1/2, step 13355/23838 completed (loss: 0.24522356688976288, acc: 0.9285714030265808)
[2025-02-16 13:36:28,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:28,916][root][INFO] - Training Epoch: 1/2, step 13356/23838 completed (loss: 0.4439994692802429, acc: 0.8706896305084229)
[2025-02-16 13:36:29,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:29,427][root][INFO] - Training Epoch: 1/2, step 13357/23838 completed (loss: 0.6025673747062683, acc: 0.8403361439704895)
[2025-02-16 13:36:29,662][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:29,893][root][INFO] - Training Epoch: 1/2, step 13358/23838 completed (loss: 0.19049207866191864, acc: 0.9428571462631226)
[2025-02-16 13:36:30,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:30,304][root][INFO] - Training Epoch: 1/2, step 13359/23838 completed (loss: 0.24082104861736298, acc: 0.9145299196243286)
[2025-02-16 13:36:30,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:30,797][root][INFO] - Training Epoch: 1/2, step 13360/23838 completed (loss: 0.5179166793823242, acc: 0.8571428656578064)
[2025-02-16 13:36:30,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:31,186][root][INFO] - Training Epoch: 1/2, step 13361/23838 completed (loss: 0.5434994101524353, acc: 0.8571428656578064)
[2025-02-16 13:36:31,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:31,657][root][INFO] - Training Epoch: 1/2, step 13362/23838 completed (loss: 0.36612096428871155, acc: 0.8888888955116272)
[2025-02-16 13:36:31,823][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:32,044][root][INFO] - Training Epoch: 1/2, step 13363/23838 completed (loss: 0.4470211863517761, acc: 0.8823529481887817)
[2025-02-16 13:36:32,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:32,489][root][INFO] - Training Epoch: 1/2, step 13364/23838 completed (loss: 0.27436596155166626, acc: 0.9075144529342651)
[2025-02-16 13:36:32,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:32,923][root][INFO] - Training Epoch: 1/2, step 13365/23838 completed (loss: 0.28224503993988037, acc: 0.9285714030265808)
[2025-02-16 13:36:33,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:33,817][root][INFO] - Training Epoch: 1/2, step 13366/23838 completed (loss: 0.3608158826828003, acc: 0.9149797558784485)
[2025-02-16 13:36:33,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:34,224][root][INFO] - Training Epoch: 1/2, step 13367/23838 completed (loss: 0.13837704062461853, acc: 0.9647576808929443)
[2025-02-16 13:36:34,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:34,634][root][INFO] - Training Epoch: 1/2, step 13368/23838 completed (loss: 0.21141187846660614, acc: 0.9534883499145508)
[2025-02-16 13:36:34,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:35,027][root][INFO] - Training Epoch: 1/2, step 13369/23838 completed (loss: 0.20667976140975952, acc: 0.9271523356437683)
[2025-02-16 13:36:35,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:35,411][root][INFO] - Training Epoch: 1/2, step 13370/23838 completed (loss: 0.14966246485710144, acc: 0.9552238583564758)
[2025-02-16 13:36:35,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:35,849][root][INFO] - Training Epoch: 1/2, step 13371/23838 completed (loss: 0.4484877288341522, acc: 0.8888888955116272)
[2025-02-16 13:36:36,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:36,369][root][INFO] - Training Epoch: 1/2, step 13372/23838 completed (loss: 0.1987723857164383, acc: 0.9599999785423279)
[2025-02-16 13:36:36,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:36,790][root][INFO] - Training Epoch: 1/2, step 13373/23838 completed (loss: 0.1646856814622879, acc: 0.9292929172515869)
[2025-02-16 13:36:36,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:37,171][root][INFO] - Training Epoch: 1/2, step 13374/23838 completed (loss: 0.12348426133394241, acc: 0.9583333134651184)
[2025-02-16 13:36:37,376][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:37,626][root][INFO] - Training Epoch: 1/2, step 13375/23838 completed (loss: 0.30093470215797424, acc: 0.9245283007621765)
[2025-02-16 13:36:37,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:38,063][root][INFO] - Training Epoch: 1/2, step 13376/23838 completed (loss: 0.4848599135875702, acc: 0.8725489974021912)
[2025-02-16 13:36:38,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:38,445][root][INFO] - Training Epoch: 1/2, step 13377/23838 completed (loss: 0.3350370228290558, acc: 0.914893627166748)
[2025-02-16 13:36:38,597][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:38,834][root][INFO] - Training Epoch: 1/2, step 13378/23838 completed (loss: 0.36477693915367126, acc: 0.9252336621284485)
[2025-02-16 13:36:39,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:39,220][root][INFO] - Training Epoch: 1/2, step 13379/23838 completed (loss: 0.11522621661424637, acc: 0.9708737730979919)
[2025-02-16 13:36:39,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:39,629][root][INFO] - Training Epoch: 1/2, step 13380/23838 completed (loss: 0.31477901339530945, acc: 0.8965517282485962)
[2025-02-16 13:36:39,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:40,105][root][INFO] - Training Epoch: 1/2, step 13381/23838 completed (loss: 0.18848636746406555, acc: 0.9523809552192688)
[2025-02-16 13:36:40,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:40,541][root][INFO] - Training Epoch: 1/2, step 13382/23838 completed (loss: 0.13262125849723816, acc: 0.9611650705337524)
[2025-02-16 13:36:40,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:41,001][root][INFO] - Training Epoch: 1/2, step 13383/23838 completed (loss: 0.14143314957618713, acc: 0.9655172228813171)
[2025-02-16 13:36:41,205][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:41,450][root][INFO] - Training Epoch: 1/2, step 13384/23838 completed (loss: 0.13453887403011322, acc: 0.9602649211883545)
[2025-02-16 13:36:41,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:41,869][root][INFO] - Training Epoch: 1/2, step 13385/23838 completed (loss: 0.6504212021827698, acc: 0.8333333134651184)
[2025-02-16 13:36:42,012][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:42,245][root][INFO] - Training Epoch: 1/2, step 13386/23838 completed (loss: 0.4622722268104553, acc: 0.8833333253860474)
[2025-02-16 13:36:42,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:42,685][root][INFO] - Training Epoch: 1/2, step 13387/23838 completed (loss: 0.32156068086624146, acc: 0.8916666507720947)
[2025-02-16 13:36:42,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:43,113][root][INFO] - Training Epoch: 1/2, step 13388/23838 completed (loss: 0.514039933681488, acc: 0.8448275923728943)
[2025-02-16 13:36:43,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:43,530][root][INFO] - Training Epoch: 1/2, step 13389/23838 completed (loss: 0.19051304459571838, acc: 0.957446813583374)
[2025-02-16 13:36:43,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:43,987][root][INFO] - Training Epoch: 1/2, step 13390/23838 completed (loss: 0.45282045006752014, acc: 0.89552241563797)
[2025-02-16 13:36:44,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:44,424][root][INFO] - Training Epoch: 1/2, step 13391/23838 completed (loss: 0.2056276798248291, acc: 0.9473684430122375)
[2025-02-16 13:36:44,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:44,936][root][INFO] - Training Epoch: 1/2, step 13392/23838 completed (loss: 0.2796156704425812, acc: 0.8888888955116272)
[2025-02-16 13:36:45,128][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:45,346][root][INFO] - Training Epoch: 1/2, step 13393/23838 completed (loss: 0.3925746977329254, acc: 0.8684210777282715)
[2025-02-16 13:36:45,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:45,743][root][INFO] - Training Epoch: 1/2, step 13394/23838 completed (loss: 0.12783078849315643, acc: 0.9803921580314636)
[2025-02-16 13:36:45,926][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:46,154][root][INFO] - Training Epoch: 1/2, step 13395/23838 completed (loss: 0.09677151590585709, acc: 0.9647887349128723)
[2025-02-16 13:36:46,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:46,551][root][INFO] - Training Epoch: 1/2, step 13396/23838 completed (loss: 0.17989343404769897, acc: 0.930232584476471)
[2025-02-16 13:36:46,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:46,932][root][INFO] - Training Epoch: 1/2, step 13397/23838 completed (loss: 0.1491871476173401, acc: 0.945652186870575)
[2025-02-16 13:36:47,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:47,329][root][INFO] - Training Epoch: 1/2, step 13398/23838 completed (loss: 0.17804113030433655, acc: 0.9255319237709045)
[2025-02-16 13:36:47,463][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:47,694][root][INFO] - Training Epoch: 1/2, step 13399/23838 completed (loss: 0.20831604301929474, acc: 0.9477611780166626)
[2025-02-16 13:36:47,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:48,130][root][INFO] - Training Epoch: 1/2, step 13400/23838 completed (loss: 0.2494172602891922, acc: 0.9200000166893005)
[2025-02-16 13:36:48,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:48,590][root][INFO] - Training Epoch: 1/2, step 13401/23838 completed (loss: 0.6064125895500183, acc: 0.8271604776382446)
[2025-02-16 13:36:48,763][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:48,983][root][INFO] - Training Epoch: 1/2, step 13402/23838 completed (loss: 0.15288174152374268, acc: 0.9253731369972229)
[2025-02-16 13:36:49,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:49,380][root][INFO] - Training Epoch: 1/2, step 13403/23838 completed (loss: 0.1537386178970337, acc: 0.95652174949646)
[2025-02-16 13:36:49,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:49,768][root][INFO] - Training Epoch: 1/2, step 13404/23838 completed (loss: 0.3044101893901825, acc: 0.9069767594337463)
[2025-02-16 13:36:49,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:50,170][root][INFO] - Training Epoch: 1/2, step 13405/23838 completed (loss: 0.2823927700519562, acc: 0.9259259104728699)
[2025-02-16 13:36:50,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:50,614][root][INFO] - Training Epoch: 1/2, step 13406/23838 completed (loss: 0.7939335107803345, acc: 0.8068181872367859)
[2025-02-16 13:36:50,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:51,081][root][INFO] - Training Epoch: 1/2, step 13407/23838 completed (loss: 0.2245713770389557, acc: 0.9477611780166626)
[2025-02-16 13:36:51,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:51,534][root][INFO] - Training Epoch: 1/2, step 13408/23838 completed (loss: 0.6723465323448181, acc: 0.7981651425361633)
[2025-02-16 13:36:51,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:51,929][root][INFO] - Training Epoch: 1/2, step 13409/23838 completed (loss: 0.38118159770965576, acc: 0.8999999761581421)
[2025-02-16 13:36:52,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:52,359][root][INFO] - Training Epoch: 1/2, step 13410/23838 completed (loss: 0.32067933678627014, acc: 0.9117646813392639)
[2025-02-16 13:36:52,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:52,822][root][INFO] - Training Epoch: 1/2, step 13411/23838 completed (loss: 0.21590924263000488, acc: 0.9122806787490845)
[2025-02-16 13:36:53,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:53,306][root][INFO] - Training Epoch: 1/2, step 13412/23838 completed (loss: 0.4210413694381714, acc: 0.8777777552604675)
[2025-02-16 13:36:53,508][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:53,731][root][INFO] - Training Epoch: 1/2, step 13413/23838 completed (loss: 0.38838276267051697, acc: 0.8999999761581421)
[2025-02-16 13:36:53,871][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:54,119][root][INFO] - Training Epoch: 1/2, step 13414/23838 completed (loss: 0.3024289011955261, acc: 0.8985507488250732)
[2025-02-16 13:36:54,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:54,524][root][INFO] - Training Epoch: 1/2, step 13415/23838 completed (loss: 0.8421269059181213, acc: 0.800000011920929)
[2025-02-16 13:36:54,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:54,993][root][INFO] - Training Epoch: 1/2, step 13416/23838 completed (loss: 0.3479801118373871, acc: 0.8936170339584351)
[2025-02-16 13:36:55,199][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:55,449][root][INFO] - Training Epoch: 1/2, step 13417/23838 completed (loss: 0.19088731706142426, acc: 0.9624999761581421)
[2025-02-16 13:36:55,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:55,857][root][INFO] - Training Epoch: 1/2, step 13418/23838 completed (loss: 0.21049444377422333, acc: 0.9404761791229248)
[2025-02-16 13:36:56,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:56,287][root][INFO] - Training Epoch: 1/2, step 13419/23838 completed (loss: 0.3291396200656891, acc: 0.8943089246749878)
[2025-02-16 13:36:56,497][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:56,739][root][INFO] - Training Epoch: 1/2, step 13420/23838 completed (loss: 0.26436847448349, acc: 0.9134615659713745)
[2025-02-16 13:36:56,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:57,149][root][INFO] - Training Epoch: 1/2, step 13421/23838 completed (loss: 0.35493430495262146, acc: 0.9150943160057068)
[2025-02-16 13:36:57,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:57,586][root][INFO] - Training Epoch: 1/2, step 13422/23838 completed (loss: 0.34992069005966187, acc: 0.932692289352417)
[2025-02-16 13:36:57,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:58,016][root][INFO] - Training Epoch: 1/2, step 13423/23838 completed (loss: 0.8220565319061279, acc: 0.75)
[2025-02-16 13:36:58,195][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:58,412][root][INFO] - Training Epoch: 1/2, step 13424/23838 completed (loss: 0.3413415849208832, acc: 0.90625)
[2025-02-16 13:36:58,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:58,814][root][INFO] - Training Epoch: 1/2, step 13425/23838 completed (loss: 0.1284058839082718, acc: 0.9558823704719543)
[2025-02-16 13:36:59,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:59,280][root][INFO] - Training Epoch: 1/2, step 13426/23838 completed (loss: 0.39620161056518555, acc: 0.8888888955116272)
[2025-02-16 13:36:59,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:36:59,766][root][INFO] - Training Epoch: 1/2, step 13427/23838 completed (loss: 0.27522850036621094, acc: 0.9139785170555115)
[2025-02-16 13:36:59,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:00,222][root][INFO] - Training Epoch: 1/2, step 13428/23838 completed (loss: 0.1759907454252243, acc: 0.9444444179534912)
[2025-02-16 13:37:00,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:00,667][root][INFO] - Training Epoch: 1/2, step 13429/23838 completed (loss: 0.17922508716583252, acc: 0.9576271176338196)
[2025-02-16 13:37:00,924][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:01,160][root][INFO] - Training Epoch: 1/2, step 13430/23838 completed (loss: 0.39415526390075684, acc: 0.9210526347160339)
[2025-02-16 13:37:01,375][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:01,610][root][INFO] - Training Epoch: 1/2, step 13431/23838 completed (loss: 0.28343477845191956, acc: 0.8941176533699036)
[2025-02-16 13:37:01,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:02,013][root][INFO] - Training Epoch: 1/2, step 13432/23838 completed (loss: 0.47726836800575256, acc: 0.8548387289047241)
[2025-02-16 13:37:02,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:02,446][root][INFO] - Training Epoch: 1/2, step 13433/23838 completed (loss: 0.19157397747039795, acc: 0.9655172228813171)
[2025-02-16 13:37:02,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:02,834][root][INFO] - Training Epoch: 1/2, step 13434/23838 completed (loss: 0.13130715489387512, acc: 0.9722222089767456)
[2025-02-16 13:37:03,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:03,304][root][INFO] - Training Epoch: 1/2, step 13435/23838 completed (loss: 0.7232621312141418, acc: 0.800000011920929)
[2025-02-16 13:37:03,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:03,757][root][INFO] - Training Epoch: 1/2, step 13436/23838 completed (loss: 0.29432085156440735, acc: 0.9238095283508301)
[2025-02-16 13:37:03,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:04,245][root][INFO] - Training Epoch: 1/2, step 13437/23838 completed (loss: 0.472284197807312, acc: 0.8787878751754761)
[2025-02-16 13:37:04,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:04,674][root][INFO] - Training Epoch: 1/2, step 13438/23838 completed (loss: 0.36668524146080017, acc: 0.8984375)
[2025-02-16 13:37:04,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:05,110][root][INFO] - Training Epoch: 1/2, step 13439/23838 completed (loss: 0.1831192672252655, acc: 0.9365079402923584)
[2025-02-16 13:37:05,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:05,583][root][INFO] - Training Epoch: 1/2, step 13440/23838 completed (loss: 0.23914481699466705, acc: 0.9160305261611938)
[2025-02-16 13:37:05,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:05,948][root][INFO] - Training Epoch: 1/2, step 13441/23838 completed (loss: 0.3225293457508087, acc: 0.8904109597206116)
[2025-02-16 13:37:06,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:06,370][root][INFO] - Training Epoch: 1/2, step 13442/23838 completed (loss: 0.1394943743944168, acc: 0.9576271176338196)
[2025-02-16 13:37:06,542][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:06,766][root][INFO] - Training Epoch: 1/2, step 13443/23838 completed (loss: 0.7385894060134888, acc: 0.78125)
[2025-02-16 13:37:06,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:07,238][root][INFO] - Training Epoch: 1/2, step 13444/23838 completed (loss: 0.4005594551563263, acc: 0.9015151262283325)
[2025-02-16 13:37:07,449][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:07,684][root][INFO] - Training Epoch: 1/2, step 13445/23838 completed (loss: 0.6975905299186707, acc: 0.8548387289047241)
[2025-02-16 13:37:07,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:08,104][root][INFO] - Training Epoch: 1/2, step 13446/23838 completed (loss: 0.45924150943756104, acc: 0.8607594966888428)
[2025-02-16 13:37:08,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:08,480][root][INFO] - Training Epoch: 1/2, step 13447/23838 completed (loss: 0.40378376841545105, acc: 0.8732394576072693)
[2025-02-16 13:37:08,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:08,957][root][INFO] - Training Epoch: 1/2, step 13448/23838 completed (loss: 0.33960285782814026, acc: 0.9090909361839294)
[2025-02-16 13:37:09,136][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:09,388][root][INFO] - Training Epoch: 1/2, step 13449/23838 completed (loss: 0.1991012543439865, acc: 0.9484536051750183)
[2025-02-16 13:37:09,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:09,800][root][INFO] - Training Epoch: 1/2, step 13450/23838 completed (loss: 0.33672621846199036, acc: 0.8888888955116272)
[2025-02-16 13:37:10,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:10,256][root][INFO] - Training Epoch: 1/2, step 13451/23838 completed (loss: 0.40279659628868103, acc: 0.8999999761581421)
[2025-02-16 13:37:10,478][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:10,718][root][INFO] - Training Epoch: 1/2, step 13452/23838 completed (loss: 0.18153326213359833, acc: 0.9390243887901306)
[2025-02-16 13:37:10,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:11,122][root][INFO] - Training Epoch: 1/2, step 13453/23838 completed (loss: 0.07694588601589203, acc: 0.9741935729980469)
[2025-02-16 13:37:11,307][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:11,537][root][INFO] - Training Epoch: 1/2, step 13454/23838 completed (loss: 0.2582651376724243, acc: 0.9285714030265808)
[2025-02-16 13:37:11,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:11,932][root][INFO] - Training Epoch: 1/2, step 13455/23838 completed (loss: 0.19547522068023682, acc: 0.9473684430122375)
[2025-02-16 13:37:12,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:12,415][root][INFO] - Training Epoch: 1/2, step 13456/23838 completed (loss: 0.08907359838485718, acc: 0.9736841917037964)
[2025-02-16 13:37:12,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:12,898][root][INFO] - Training Epoch: 1/2, step 13457/23838 completed (loss: 0.471946656703949, acc: 0.8571428656578064)
[2025-02-16 13:37:13,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:13,311][root][INFO] - Training Epoch: 1/2, step 13458/23838 completed (loss: 0.18813635408878326, acc: 0.9431818127632141)
[2025-02-16 13:37:13,531][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:13,789][root][INFO] - Training Epoch: 1/2, step 13459/23838 completed (loss: 0.30197396874427795, acc: 0.875)
[2025-02-16 13:37:13,972][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:14,213][root][INFO] - Training Epoch: 1/2, step 13460/23838 completed (loss: 0.2468256950378418, acc: 0.942105233669281)
[2025-02-16 13:37:14,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:14,650][root][INFO] - Training Epoch: 1/2, step 13461/23838 completed (loss: 0.17945599555969238, acc: 0.9594594836235046)
[2025-02-16 13:37:14,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:15,127][root][INFO] - Training Epoch: 1/2, step 13462/23838 completed (loss: 0.17921914160251617, acc: 0.9404761791229248)
[2025-02-16 13:37:15,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:15,616][root][INFO] - Training Epoch: 1/2, step 13463/23838 completed (loss: 0.25337234139442444, acc: 0.9202898740768433)
[2025-02-16 13:37:15,878][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:16,128][root][INFO] - Training Epoch: 1/2, step 13464/23838 completed (loss: 0.09706027060747147, acc: 0.9722222089767456)
[2025-02-16 13:37:16,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:16,604][root][INFO] - Training Epoch: 1/2, step 13465/23838 completed (loss: 0.3026117980480194, acc: 0.9384615421295166)
[2025-02-16 13:37:16,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:17,042][root][INFO] - Training Epoch: 1/2, step 13466/23838 completed (loss: 0.4383567273616791, acc: 0.8659793734550476)
[2025-02-16 13:37:17,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:17,516][root][INFO] - Training Epoch: 1/2, step 13467/23838 completed (loss: 0.12075755000114441, acc: 0.9759036302566528)
[2025-02-16 13:37:17,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:18,016][root][INFO] - Training Epoch: 1/2, step 13468/23838 completed (loss: 0.3051701784133911, acc: 0.9009009003639221)
[2025-02-16 13:37:18,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:18,422][root][INFO] - Training Epoch: 1/2, step 13469/23838 completed (loss: 0.31149736046791077, acc: 0.9066666960716248)
[2025-02-16 13:37:18,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:18,815][root][INFO] - Training Epoch: 1/2, step 13470/23838 completed (loss: 0.08766727894544601, acc: 0.9741935729980469)
[2025-02-16 13:37:18,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:19,174][root][INFO] - Training Epoch: 1/2, step 13471/23838 completed (loss: 0.3835892379283905, acc: 0.8736842274665833)
[2025-02-16 13:37:19,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:19,545][root][INFO] - Training Epoch: 1/2, step 13472/23838 completed (loss: 0.3481389284133911, acc: 0.875)
[2025-02-16 13:37:19,727][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:19,970][root][INFO] - Training Epoch: 1/2, step 13473/23838 completed (loss: 0.24751484394073486, acc: 0.9239130616188049)
[2025-02-16 13:37:20,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:20,409][root][INFO] - Training Epoch: 1/2, step 13474/23838 completed (loss: 0.18100382387638092, acc: 0.9741379022598267)
[2025-02-16 13:37:20,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:20,807][root][INFO] - Training Epoch: 1/2, step 13475/23838 completed (loss: 0.22759029269218445, acc: 0.9152542352676392)
[2025-02-16 13:37:20,997][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:21,258][root][INFO] - Training Epoch: 1/2, step 13476/23838 completed (loss: 0.5518560409545898, acc: 0.8510638475418091)
[2025-02-16 13:37:21,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:21,712][root][INFO] - Training Epoch: 1/2, step 13477/23838 completed (loss: 0.11926309764385223, acc: 0.9807692170143127)
[2025-02-16 13:37:21,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:22,070][root][INFO] - Training Epoch: 1/2, step 13478/23838 completed (loss: 0.2753981947898865, acc: 0.949999988079071)
[2025-02-16 13:37:22,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:22,455][root][INFO] - Training Epoch: 1/2, step 13479/23838 completed (loss: 0.5975102186203003, acc: 0.8416666388511658)
[2025-02-16 13:37:22,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:22,892][root][INFO] - Training Epoch: 1/2, step 13480/23838 completed (loss: 0.1474282294511795, acc: 0.9459459185600281)
[2025-02-16 13:37:23,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:23,288][root][INFO] - Training Epoch: 1/2, step 13481/23838 completed (loss: 0.525813102722168, acc: 0.8709677457809448)
[2025-02-16 13:37:23,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:23,712][root][INFO] - Training Epoch: 1/2, step 13482/23838 completed (loss: 0.45446255803108215, acc: 0.9016393423080444)
[2025-02-16 13:37:23,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:24,138][root][INFO] - Training Epoch: 1/2, step 13483/23838 completed (loss: 0.2603047788143158, acc: 0.9379844665527344)
[2025-02-16 13:37:24,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:24,616][root][INFO] - Training Epoch: 1/2, step 13484/23838 completed (loss: 0.380551815032959, acc: 0.9215686321258545)
[2025-02-16 13:37:24,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:24,996][root][INFO] - Training Epoch: 1/2, step 13485/23838 completed (loss: 0.14823585748672485, acc: 0.9534883499145508)
[2025-02-16 13:37:25,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:25,438][root][INFO] - Training Epoch: 1/2, step 13486/23838 completed (loss: 0.6639488339424133, acc: 0.8089887499809265)
[2025-02-16 13:37:25,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:25,882][root][INFO] - Training Epoch: 1/2, step 13487/23838 completed (loss: 0.7173700928688049, acc: 0.8153846263885498)
[2025-02-16 13:37:26,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:26,324][root][INFO] - Training Epoch: 1/2, step 13488/23838 completed (loss: 0.3667406737804413, acc: 0.9130434989929199)
[2025-02-16 13:37:26,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:26,820][root][INFO] - Training Epoch: 1/2, step 13489/23838 completed (loss: 0.06761567294597626, acc: 0.9791666865348816)
[2025-02-16 13:37:27,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:27,306][root][INFO] - Training Epoch: 1/2, step 13490/23838 completed (loss: 0.34685003757476807, acc: 0.9347826242446899)
[2025-02-16 13:37:27,569][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:27,768][root][INFO] - Training Epoch: 1/2, step 13491/23838 completed (loss: 0.3940183222293854, acc: 0.8867924809455872)
[2025-02-16 13:37:27,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:28,165][root][INFO] - Training Epoch: 1/2, step 13492/23838 completed (loss: 0.3047166168689728, acc: 0.9152542352676392)
[2025-02-16 13:37:28,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:28,619][root][INFO] - Training Epoch: 1/2, step 13493/23838 completed (loss: 0.32232069969177246, acc: 0.9036144614219666)
[2025-02-16 13:37:28,839][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:29,079][root][INFO] - Training Epoch: 1/2, step 13494/23838 completed (loss: 0.31126996874809265, acc: 0.9155844449996948)
[2025-02-16 13:37:29,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:29,568][root][INFO] - Training Epoch: 1/2, step 13495/23838 completed (loss: 0.21026360988616943, acc: 0.9375)
[2025-02-16 13:37:29,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:29,998][root][INFO] - Training Epoch: 1/2, step 13496/23838 completed (loss: 0.44317832589149475, acc: 0.8791208863258362)
[2025-02-16 13:37:30,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:30,462][root][INFO] - Training Epoch: 1/2, step 13497/23838 completed (loss: 0.3159342408180237, acc: 0.9051094651222229)
[2025-02-16 13:37:30,626][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:30,844][root][INFO] - Training Epoch: 1/2, step 13498/23838 completed (loss: 0.6008057594299316, acc: 0.8095238208770752)
[2025-02-16 13:37:31,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:31,313][root][INFO] - Training Epoch: 1/2, step 13499/23838 completed (loss: 0.23942093551158905, acc: 0.9269406199455261)
[2025-02-16 13:37:31,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:31,814][root][INFO] - Training Epoch: 1/2, step 13500/23838 completed (loss: 0.5188096165657043, acc: 0.868686854839325)
[2025-02-16 13:37:32,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:32,286][root][INFO] - Training Epoch: 1/2, step 13501/23838 completed (loss: 0.4981037676334381, acc: 0.8417721390724182)
[2025-02-16 13:37:32,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:32,718][root][INFO] - Training Epoch: 1/2, step 13502/23838 completed (loss: 0.09162483364343643, acc: 0.9865771532058716)
[2025-02-16 13:37:32,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:33,220][root][INFO] - Training Epoch: 1/2, step 13503/23838 completed (loss: 0.48233720660209656, acc: 0.8612716794013977)
[2025-02-16 13:37:33,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:33,619][root][INFO] - Training Epoch: 1/2, step 13504/23838 completed (loss: 0.4456109404563904, acc: 0.8780487775802612)
[2025-02-16 13:37:33,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:34,056][root][INFO] - Training Epoch: 1/2, step 13505/23838 completed (loss: 0.7222347259521484, acc: 0.8227847814559937)
[2025-02-16 13:37:34,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:34,533][root][INFO] - Training Epoch: 1/2, step 13506/23838 completed (loss: 0.3074537217617035, acc: 0.9439252614974976)
[2025-02-16 13:37:34,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:34,979][root][INFO] - Training Epoch: 1/2, step 13507/23838 completed (loss: 0.11733285337686539, acc: 0.9612902998924255)
[2025-02-16 13:37:35,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:35,450][root][INFO] - Training Epoch: 1/2, step 13508/23838 completed (loss: 0.19978433847427368, acc: 0.9681528806686401)
[2025-02-16 13:37:35,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:35,886][root][INFO] - Training Epoch: 1/2, step 13509/23838 completed (loss: 0.2591041624546051, acc: 0.935251772403717)
[2025-02-16 13:37:36,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:36,343][root][INFO] - Training Epoch: 1/2, step 13510/23838 completed (loss: 0.5464946627616882, acc: 0.8695651888847351)
[2025-02-16 13:37:36,513][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:36,786][root][INFO] - Training Epoch: 1/2, step 13511/23838 completed (loss: 0.41270217299461365, acc: 0.8474576473236084)
[2025-02-16 13:37:36,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:37,180][root][INFO] - Training Epoch: 1/2, step 13512/23838 completed (loss: 0.35851332545280457, acc: 0.89682537317276)
[2025-02-16 13:37:37,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:37,524][root][INFO] - Training Epoch: 1/2, step 13513/23838 completed (loss: 0.2544858753681183, acc: 0.9350649118423462)
[2025-02-16 13:37:37,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:37,980][root][INFO] - Training Epoch: 1/2, step 13514/23838 completed (loss: 0.09611015766859055, acc: 0.9833333492279053)
[2025-02-16 13:37:38,206][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:38,438][root][INFO] - Training Epoch: 1/2, step 13515/23838 completed (loss: 0.3325142562389374, acc: 0.931034505367279)
[2025-02-16 13:37:38,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:38,811][root][INFO] - Training Epoch: 1/2, step 13516/23838 completed (loss: 0.09140164405107498, acc: 0.9813084006309509)
[2025-02-16 13:37:38,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:39,214][root][INFO] - Training Epoch: 1/2, step 13517/23838 completed (loss: 0.40721482038497925, acc: 0.8888888955116272)
[2025-02-16 13:37:39,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:39,774][root][INFO] - Training Epoch: 1/2, step 13518/23838 completed (loss: 0.25874558091163635, acc: 0.9407894611358643)
[2025-02-16 13:37:40,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:40,261][root][INFO] - Training Epoch: 1/2, step 13519/23838 completed (loss: 0.2095082849264145, acc: 0.9300699234008789)
[2025-02-16 13:37:40,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:40,686][root][INFO] - Training Epoch: 1/2, step 13520/23838 completed (loss: 0.1262950897216797, acc: 0.9599999785423279)
[2025-02-16 13:37:40,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:41,157][root][INFO] - Training Epoch: 1/2, step 13521/23838 completed (loss: 0.2650008499622345, acc: 0.9375)
[2025-02-16 13:37:41,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:41,634][root][INFO] - Training Epoch: 1/2, step 13522/23838 completed (loss: 0.3522500991821289, acc: 0.9107142686843872)
[2025-02-16 13:37:41,832][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:42,082][root][INFO] - Training Epoch: 1/2, step 13523/23838 completed (loss: 0.11527223140001297, acc: 0.9572649598121643)
[2025-02-16 13:37:42,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:42,470][root][INFO] - Training Epoch: 1/2, step 13524/23838 completed (loss: 0.2720004618167877, acc: 0.931506872177124)
[2025-02-16 13:37:42,684][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:42,951][root][INFO] - Training Epoch: 1/2, step 13525/23838 completed (loss: 0.4068644940853119, acc: 0.9181286692619324)
[2025-02-16 13:37:43,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:43,347][root][INFO] - Training Epoch: 1/2, step 13526/23838 completed (loss: 0.7977409362792969, acc: 0.8139534592628479)
[2025-02-16 13:37:43,499][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:43,739][root][INFO] - Training Epoch: 1/2, step 13527/23838 completed (loss: 0.32713761925697327, acc: 0.8809523582458496)
[2025-02-16 13:37:43,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:44,113][root][INFO] - Training Epoch: 1/2, step 13528/23838 completed (loss: 0.5111650824546814, acc: 0.875)
[2025-02-16 13:37:44,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:44,516][root][INFO] - Training Epoch: 1/2, step 13529/23838 completed (loss: 0.6149656772613525, acc: 0.8588235378265381)
[2025-02-16 13:37:44,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:44,985][root][INFO] - Training Epoch: 1/2, step 13530/23838 completed (loss: 0.42597630620002747, acc: 0.8648648858070374)
[2025-02-16 13:37:45,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:45,436][root][INFO] - Training Epoch: 1/2, step 13531/23838 completed (loss: 0.434982031583786, acc: 0.8986784219741821)
[2025-02-16 13:37:45,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:45,838][root][INFO] - Training Epoch: 1/2, step 13532/23838 completed (loss: 0.03455812484025955, acc: 1.0)
[2025-02-16 13:37:46,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:46,302][root][INFO] - Training Epoch: 1/2, step 13533/23838 completed (loss: 0.2349184900522232, acc: 0.9359999895095825)
[2025-02-16 13:37:46,554][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:46,797][root][INFO] - Training Epoch: 1/2, step 13534/23838 completed (loss: 0.28244027495384216, acc: 0.9265536665916443)
[2025-02-16 13:37:46,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:47,219][root][INFO] - Training Epoch: 1/2, step 13535/23838 completed (loss: 0.4516819715499878, acc: 0.8641975522041321)
[2025-02-16 13:37:47,441][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:47,663][root][INFO] - Training Epoch: 1/2, step 13536/23838 completed (loss: 0.3782636821269989, acc: 0.8888888955116272)
[2025-02-16 13:37:47,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:48,137][root][INFO] - Training Epoch: 1/2, step 13537/23838 completed (loss: 0.17858463525772095, acc: 0.9303797483444214)
[2025-02-16 13:37:48,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:48,590][root][INFO] - Training Epoch: 1/2, step 13538/23838 completed (loss: 0.4297550618648529, acc: 0.8888888955116272)
[2025-02-16 13:37:48,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:49,079][root][INFO] - Training Epoch: 1/2, step 13539/23838 completed (loss: 0.13686183094978333, acc: 0.9518072009086609)
[2025-02-16 13:37:49,310][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:49,566][root][INFO] - Training Epoch: 1/2, step 13540/23838 completed (loss: 0.18649417161941528, acc: 0.9444444179534912)
[2025-02-16 13:37:49,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:50,055][root][INFO] - Training Epoch: 1/2, step 13541/23838 completed (loss: 0.09275836497545242, acc: 0.960629940032959)
[2025-02-16 13:37:50,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:50,499][root][INFO] - Training Epoch: 1/2, step 13542/23838 completed (loss: 0.33055350184440613, acc: 0.9264705777168274)
[2025-02-16 13:37:50,701][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:50,931][root][INFO] - Training Epoch: 1/2, step 13543/23838 completed (loss: 0.248430997133255, acc: 0.9389312863349915)
[2025-02-16 13:37:51,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:51,394][root][INFO] - Training Epoch: 1/2, step 13544/23838 completed (loss: 0.20628459751605988, acc: 0.9437500238418579)
[2025-02-16 13:37:51,651][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:51,917][root][INFO] - Training Epoch: 1/2, step 13545/23838 completed (loss: 0.40267470479011536, acc: 0.8991596698760986)
[2025-02-16 13:37:52,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:52,376][root][INFO] - Training Epoch: 1/2, step 13546/23838 completed (loss: 0.2844600975513458, acc: 0.9132652878761292)
[2025-02-16 13:37:52,591][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:52,836][root][INFO] - Training Epoch: 1/2, step 13547/23838 completed (loss: 0.34879761934280396, acc: 0.8918918967247009)
[2025-02-16 13:37:53,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:53,291][root][INFO] - Training Epoch: 1/2, step 13548/23838 completed (loss: 1.029212236404419, acc: 0.7333333492279053)
[2025-02-16 13:37:53,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:53,788][root][INFO] - Training Epoch: 1/2, step 13549/23838 completed (loss: 0.39778971672058105, acc: 0.885496199131012)
[2025-02-16 13:37:53,996][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:54,246][root][INFO] - Training Epoch: 1/2, step 13550/23838 completed (loss: 0.4132251441478729, acc: 0.8636363744735718)
[2025-02-16 13:37:54,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:54,694][root][INFO] - Training Epoch: 1/2, step 13551/23838 completed (loss: 0.22695131599903107, acc: 0.9105691313743591)
[2025-02-16 13:37:54,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:55,113][root][INFO] - Training Epoch: 1/2, step 13552/23838 completed (loss: 0.10948766767978668, acc: 0.9743589758872986)
[2025-02-16 13:37:55,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:55,487][root][INFO] - Training Epoch: 1/2, step 13553/23838 completed (loss: 0.13400323688983917, acc: 0.9577465057373047)
[2025-02-16 13:37:55,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:55,946][root][INFO] - Training Epoch: 1/2, step 13554/23838 completed (loss: 0.5663536787033081, acc: 0.8760330677032471)
[2025-02-16 13:37:56,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:56,395][root][INFO] - Training Epoch: 1/2, step 13555/23838 completed (loss: 0.38040146231651306, acc: 0.8873239159584045)
[2025-02-16 13:37:56,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:56,797][root][INFO] - Training Epoch: 1/2, step 13556/23838 completed (loss: 0.5233128070831299, acc: 0.8888888955116272)
[2025-02-16 13:37:57,013][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:57,262][root][INFO] - Training Epoch: 1/2, step 13557/23838 completed (loss: 0.251236230134964, acc: 0.9411764740943909)
[2025-02-16 13:37:57,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:57,710][root][INFO] - Training Epoch: 1/2, step 13558/23838 completed (loss: 0.3460044264793396, acc: 0.8965517282485962)
[2025-02-16 13:37:57,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:58,119][root][INFO] - Training Epoch: 1/2, step 13559/23838 completed (loss: 0.4372159242630005, acc: 0.9009009003639221)
[2025-02-16 13:37:58,373][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:58,624][root][INFO] - Training Epoch: 1/2, step 13560/23838 completed (loss: 0.25269415974617004, acc: 0.9506173133850098)
[2025-02-16 13:37:58,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:59,103][root][INFO] - Training Epoch: 1/2, step 13561/23838 completed (loss: 0.2808058261871338, acc: 0.9072847962379456)
[2025-02-16 13:37:59,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:37:59,572][root][INFO] - Training Epoch: 1/2, step 13562/23838 completed (loss: 0.1817454695701599, acc: 0.9684210419654846)
[2025-02-16 13:37:59,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:00,040][root][INFO] - Training Epoch: 1/2, step 13563/23838 completed (loss: 0.3561497926712036, acc: 0.9005848169326782)
[2025-02-16 13:38:00,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:00,472][root][INFO] - Training Epoch: 1/2, step 13564/23838 completed (loss: 0.2642931640148163, acc: 0.9166666865348816)
[2025-02-16 13:38:00,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:00,855][root][INFO] - Training Epoch: 1/2, step 13565/23838 completed (loss: 0.27023810148239136, acc: 0.9468085169792175)
[2025-02-16 13:38:01,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:01,272][root][INFO] - Training Epoch: 1/2, step 13566/23838 completed (loss: 0.14885811507701874, acc: 0.9473684430122375)
[2025-02-16 13:38:01,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:01,716][root][INFO] - Training Epoch: 1/2, step 13567/23838 completed (loss: 0.20313026010990143, acc: 0.9359999895095825)
[2025-02-16 13:38:01,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:02,175][root][INFO] - Training Epoch: 1/2, step 13568/23838 completed (loss: 0.3649986684322357, acc: 0.914893627166748)
[2025-02-16 13:38:02,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:02,585][root][INFO] - Training Epoch: 1/2, step 13569/23838 completed (loss: 0.09756819903850555, acc: 0.9662162065505981)
[2025-02-16 13:38:02,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:02,984][root][INFO] - Training Epoch: 1/2, step 13570/23838 completed (loss: 0.4439263939857483, acc: 0.8235294222831726)
[2025-02-16 13:38:03,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:03,443][root][INFO] - Training Epoch: 1/2, step 13571/23838 completed (loss: 0.3107261061668396, acc: 0.8775510191917419)
[2025-02-16 13:38:03,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:03,856][root][INFO] - Training Epoch: 1/2, step 13572/23838 completed (loss: 0.2686673402786255, acc: 0.9210526347160339)
[2025-02-16 13:38:04,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:04,304][root][INFO] - Training Epoch: 1/2, step 13573/23838 completed (loss: 0.8829909563064575, acc: 0.7882353067398071)
[2025-02-16 13:38:04,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:04,786][root][INFO] - Training Epoch: 1/2, step 13574/23838 completed (loss: 0.2585793435573578, acc: 0.9281437397003174)
[2025-02-16 13:38:05,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:05,275][root][INFO] - Training Epoch: 1/2, step 13575/23838 completed (loss: 0.30104392766952515, acc: 0.9200000166893005)
[2025-02-16 13:38:05,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:05,746][root][INFO] - Training Epoch: 1/2, step 13576/23838 completed (loss: 0.2411593496799469, acc: 0.926174521446228)
[2025-02-16 13:38:05,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:06,196][root][INFO] - Training Epoch: 1/2, step 13577/23838 completed (loss: 0.379423052072525, acc: 0.9096774458885193)
[2025-02-16 13:38:06,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:06,657][root][INFO] - Training Epoch: 1/2, step 13578/23838 completed (loss: 0.2213088870048523, acc: 0.9433962106704712)
[2025-02-16 13:38:06,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:07,110][root][INFO] - Training Epoch: 1/2, step 13579/23838 completed (loss: 0.3339332044124603, acc: 0.8941176533699036)
[2025-02-16 13:38:07,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:07,548][root][INFO] - Training Epoch: 1/2, step 13580/23838 completed (loss: 0.575752317905426, acc: 0.8367347121238708)
[2025-02-16 13:38:07,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:07,996][root][INFO] - Training Epoch: 1/2, step 13581/23838 completed (loss: 0.3827768564224243, acc: 0.8732394576072693)
[2025-02-16 13:38:08,154][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:08,429][root][INFO] - Training Epoch: 1/2, step 13582/23838 completed (loss: 0.3817133605480194, acc: 0.8679245114326477)
[2025-02-16 13:38:08,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:08,862][root][INFO] - Training Epoch: 1/2, step 13583/23838 completed (loss: 0.15192267298698425, acc: 0.9406779408454895)
[2025-02-16 13:38:09,062][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:09,288][root][INFO] - Training Epoch: 1/2, step 13584/23838 completed (loss: 0.22335657477378845, acc: 0.9519230723381042)
[2025-02-16 13:38:09,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:09,715][root][INFO] - Training Epoch: 1/2, step 13585/23838 completed (loss: 0.28544560074806213, acc: 0.9240506291389465)
[2025-02-16 13:38:09,873][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:10,093][root][INFO] - Training Epoch: 1/2, step 13586/23838 completed (loss: 0.4546927213668823, acc: 0.8586956262588501)
[2025-02-16 13:38:10,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:10,525][root][INFO] - Training Epoch: 1/2, step 13587/23838 completed (loss: 0.5868266820907593, acc: 0.841269850730896)
[2025-02-16 13:38:10,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:11,056][root][INFO] - Training Epoch: 1/2, step 13588/23838 completed (loss: 0.24738065898418427, acc: 0.925000011920929)
[2025-02-16 13:38:11,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:11,531][root][INFO] - Training Epoch: 1/2, step 13589/23838 completed (loss: 0.38207587599754333, acc: 0.8881118893623352)
[2025-02-16 13:38:11,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:11,990][root][INFO] - Training Epoch: 1/2, step 13590/23838 completed (loss: 0.38823553919792175, acc: 0.9075630307197571)
[2025-02-16 13:38:12,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:12,388][root][INFO] - Training Epoch: 1/2, step 13591/23838 completed (loss: 0.5433472990989685, acc: 0.8316831588745117)
[2025-02-16 13:38:12,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:12,725][root][INFO] - Training Epoch: 1/2, step 13592/23838 completed (loss: 0.2801286280155182, acc: 0.914893627166748)
[2025-02-16 13:38:12,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:13,199][root][INFO] - Training Epoch: 1/2, step 13593/23838 completed (loss: 0.4420691132545471, acc: 0.8870967626571655)
[2025-02-16 13:38:13,407][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:13,651][root][INFO] - Training Epoch: 1/2, step 13594/23838 completed (loss: 0.28959953784942627, acc: 0.939393937587738)
[2025-02-16 13:38:13,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:14,089][root][INFO] - Training Epoch: 1/2, step 13595/23838 completed (loss: 0.2144179344177246, acc: 0.949999988079071)
[2025-02-16 13:38:14,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:14,498][root][INFO] - Training Epoch: 1/2, step 13596/23838 completed (loss: 0.26759546995162964, acc: 0.9406779408454895)
[2025-02-16 13:38:14,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:14,939][root][INFO] - Training Epoch: 1/2, step 13597/23838 completed (loss: 0.16647599637508392, acc: 0.9555555582046509)
[2025-02-16 13:38:15,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:15,348][root][INFO] - Training Epoch: 1/2, step 13598/23838 completed (loss: 0.45301979780197144, acc: 0.8947368264198303)
[2025-02-16 13:38:15,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:15,787][root][INFO] - Training Epoch: 1/2, step 13599/23838 completed (loss: 0.1963113695383072, acc: 0.9354838728904724)
[2025-02-16 13:38:16,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:16,264][root][INFO] - Training Epoch: 1/2, step 13600/23838 completed (loss: 0.18534521758556366, acc: 0.948387086391449)
[2025-02-16 13:38:16,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:16,679][root][INFO] - Training Epoch: 1/2, step 13601/23838 completed (loss: 0.370991587638855, acc: 0.9065420627593994)
[2025-02-16 13:38:16,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:17,101][root][INFO] - Training Epoch: 1/2, step 13602/23838 completed (loss: 0.35518333315849304, acc: 0.9285714030265808)
[2025-02-16 13:38:17,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:17,484][root][INFO] - Training Epoch: 1/2, step 13603/23838 completed (loss: 0.15345460176467896, acc: 0.9655172228813171)
[2025-02-16 13:38:17,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:17,802][root][INFO] - Training Epoch: 1/2, step 13604/23838 completed (loss: 0.3381824195384979, acc: 0.901098906993866)
[2025-02-16 13:38:18,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:18,251][root][INFO] - Training Epoch: 1/2, step 13605/23838 completed (loss: 0.6060164570808411, acc: 0.8157894611358643)
[2025-02-16 13:38:18,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:18,629][root][INFO] - Training Epoch: 1/2, step 13606/23838 completed (loss: 0.10143237560987473, acc: 0.9743589758872986)
[2025-02-16 13:38:18,901][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:19,144][root][INFO] - Training Epoch: 1/2, step 13607/23838 completed (loss: 0.40588870644569397, acc: 0.8952381014823914)
[2025-02-16 13:38:19,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:19,598][root][INFO] - Training Epoch: 1/2, step 13608/23838 completed (loss: 0.33856117725372314, acc: 0.8974359035491943)
[2025-02-16 13:38:19,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:19,996][root][INFO] - Training Epoch: 1/2, step 13609/23838 completed (loss: 0.31797757744789124, acc: 0.8857142925262451)
[2025-02-16 13:38:20,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:20,458][root][INFO] - Training Epoch: 1/2, step 13610/23838 completed (loss: 0.5213281512260437, acc: 0.8529411554336548)
[2025-02-16 13:38:20,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:20,897][root][INFO] - Training Epoch: 1/2, step 13611/23838 completed (loss: 0.2318655103445053, acc: 0.9292929172515869)
[2025-02-16 13:38:21,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:21,282][root][INFO] - Training Epoch: 1/2, step 13612/23838 completed (loss: 0.2771313190460205, acc: 0.9354838728904724)
[2025-02-16 13:38:21,446][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:21,680][root][INFO] - Training Epoch: 1/2, step 13613/23838 completed (loss: 0.6533436179161072, acc: 0.8409090638160706)
[2025-02-16 13:38:21,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:22,154][root][INFO] - Training Epoch: 1/2, step 13614/23838 completed (loss: 0.16193638741970062, acc: 0.970588207244873)
[2025-02-16 13:38:22,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:22,541][root][INFO] - Training Epoch: 1/2, step 13615/23838 completed (loss: 0.1888485699892044, acc: 0.9277108311653137)
[2025-02-16 13:38:22,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:22,947][root][INFO] - Training Epoch: 1/2, step 13616/23838 completed (loss: 0.6379166841506958, acc: 0.8732394576072693)
[2025-02-16 13:38:23,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:23,419][root][INFO] - Training Epoch: 1/2, step 13617/23838 completed (loss: 0.17540055513381958, acc: 0.9506173133850098)
[2025-02-16 13:38:23,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:23,869][root][INFO] - Training Epoch: 1/2, step 13618/23838 completed (loss: 0.6026762127876282, acc: 0.8026315569877625)
[2025-02-16 13:38:24,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:24,262][root][INFO] - Training Epoch: 1/2, step 13619/23838 completed (loss: 0.6936314702033997, acc: 0.7943925261497498)
[2025-02-16 13:38:24,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:24,679][root][INFO] - Training Epoch: 1/2, step 13620/23838 completed (loss: 0.12621426582336426, acc: 0.9716981053352356)
[2025-02-16 13:38:24,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:25,088][root][INFO] - Training Epoch: 1/2, step 13621/23838 completed (loss: 0.501522421836853, acc: 0.8478260636329651)
[2025-02-16 13:38:25,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:25,522][root][INFO] - Training Epoch: 1/2, step 13622/23838 completed (loss: 0.3941163718700409, acc: 0.9200000166893005)
[2025-02-16 13:38:25,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:25,933][root][INFO] - Training Epoch: 1/2, step 13623/23838 completed (loss: 0.15460896492004395, acc: 0.9459459185600281)
[2025-02-16 13:38:26,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:26,398][root][INFO] - Training Epoch: 1/2, step 13624/23838 completed (loss: 0.2839994430541992, acc: 0.9345794320106506)
[2025-02-16 13:38:26,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:26,858][root][INFO] - Training Epoch: 1/2, step 13625/23838 completed (loss: 0.31538692116737366, acc: 0.9054054021835327)
[2025-02-16 13:38:27,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:27,295][root][INFO] - Training Epoch: 1/2, step 13626/23838 completed (loss: 0.3924243152141571, acc: 0.8640776872634888)
[2025-02-16 13:38:27,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:27,676][root][INFO] - Training Epoch: 1/2, step 13627/23838 completed (loss: 0.49728551506996155, acc: 0.852173924446106)
[2025-02-16 13:38:27,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:28,087][root][INFO] - Training Epoch: 1/2, step 13628/23838 completed (loss: 0.18578770756721497, acc: 0.9537037014961243)
[2025-02-16 13:38:28,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:28,491][root][INFO] - Training Epoch: 1/2, step 13629/23838 completed (loss: 0.20667089521884918, acc: 0.9583333134651184)
[2025-02-16 13:38:28,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:28,923][root][INFO] - Training Epoch: 1/2, step 13630/23838 completed (loss: 0.1731572449207306, acc: 0.966292142868042)
[2025-02-16 13:38:29,142][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:29,370][root][INFO] - Training Epoch: 1/2, step 13631/23838 completed (loss: 0.346295565366745, acc: 0.9318181872367859)
[2025-02-16 13:38:29,568][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:29,829][root][INFO] - Training Epoch: 1/2, step 13632/23838 completed (loss: 0.11004076153039932, acc: 0.9622641801834106)
[2025-02-16 13:38:30,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:30,309][root][INFO] - Training Epoch: 1/2, step 13633/23838 completed (loss: 0.12588976323604584, acc: 0.9645389914512634)
[2025-02-16 13:38:30,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:30,758][root][INFO] - Training Epoch: 1/2, step 13634/23838 completed (loss: 0.1819860339164734, acc: 0.9470198750495911)
[2025-02-16 13:38:30,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:31,148][root][INFO] - Training Epoch: 1/2, step 13635/23838 completed (loss: 0.260891318321228, acc: 0.9191176295280457)
[2025-02-16 13:38:31,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:31,584][root][INFO] - Training Epoch: 1/2, step 13636/23838 completed (loss: 0.7694329619407654, acc: 0.7976190447807312)
[2025-02-16 13:38:31,766][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:31,996][root][INFO] - Training Epoch: 1/2, step 13637/23838 completed (loss: 0.18798615038394928, acc: 0.9303797483444214)
[2025-02-16 13:38:32,181][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:32,417][root][INFO] - Training Epoch: 1/2, step 13638/23838 completed (loss: 0.32451775670051575, acc: 0.905063271522522)
[2025-02-16 13:38:32,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:32,936][root][INFO] - Training Epoch: 1/2, step 13639/23838 completed (loss: 0.22747160494327545, acc: 0.9430894255638123)
[2025-02-16 13:38:33,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:33,385][root][INFO] - Training Epoch: 1/2, step 13640/23838 completed (loss: 0.6404843926429749, acc: 0.8653846383094788)
[2025-02-16 13:38:33,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:33,837][root][INFO] - Training Epoch: 1/2, step 13641/23838 completed (loss: 0.18458296358585358, acc: 0.9398496150970459)
[2025-02-16 13:38:34,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:34,554][root][INFO] - Training Epoch: 1/2, step 13642/23838 completed (loss: 0.19190390408039093, acc: 0.9462810158729553)
[2025-02-16 13:38:34,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:34,930][root][INFO] - Training Epoch: 1/2, step 13643/23838 completed (loss: 0.2915070652961731, acc: 0.9322034120559692)
[2025-02-16 13:38:35,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:35,298][root][INFO] - Training Epoch: 1/2, step 13644/23838 completed (loss: 0.30204102396965027, acc: 0.9242424368858337)
[2025-02-16 13:38:35,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:35,704][root][INFO] - Training Epoch: 1/2, step 13645/23838 completed (loss: 0.17149491608142853, acc: 0.949999988079071)
[2025-02-16 13:38:35,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:36,083][root][INFO] - Training Epoch: 1/2, step 13646/23838 completed (loss: 0.3049631118774414, acc: 0.9195402264595032)
[2025-02-16 13:38:36,268][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:36,550][root][INFO] - Training Epoch: 1/2, step 13647/23838 completed (loss: 0.05483297258615494, acc: 0.982758641242981)
[2025-02-16 13:38:36,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:37,011][root][INFO] - Training Epoch: 1/2, step 13648/23838 completed (loss: 0.3545895516872406, acc: 0.9042553305625916)
[2025-02-16 13:38:37,179][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:37,401][root][INFO] - Training Epoch: 1/2, step 13649/23838 completed (loss: 0.1429988443851471, acc: 0.9444444179534912)
[2025-02-16 13:38:37,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:37,895][root][INFO] - Training Epoch: 1/2, step 13650/23838 completed (loss: 0.10447756201028824, acc: 0.9642857313156128)
[2025-02-16 13:38:38,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:38,345][root][INFO] - Training Epoch: 1/2, step 13651/23838 completed (loss: 0.17453697323799133, acc: 0.9629629850387573)
[2025-02-16 13:38:38,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:38,754][root][INFO] - Training Epoch: 1/2, step 13652/23838 completed (loss: 0.3665844202041626, acc: 0.8985507488250732)
[2025-02-16 13:38:38,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:39,193][root][INFO] - Training Epoch: 1/2, step 13653/23838 completed (loss: 0.2656744718551636, acc: 0.9444444179534912)
[2025-02-16 13:38:39,380][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:39,616][root][INFO] - Training Epoch: 1/2, step 13654/23838 completed (loss: 0.2010812759399414, acc: 0.9350000023841858)
[2025-02-16 13:38:39,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:40,068][root][INFO] - Training Epoch: 1/2, step 13655/23838 completed (loss: 0.3696174621582031, acc: 0.9032257795333862)
[2025-02-16 13:38:40,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:40,529][root][INFO] - Training Epoch: 1/2, step 13656/23838 completed (loss: 0.7765316963195801, acc: 0.782608687877655)
[2025-02-16 13:38:40,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:41,028][root][INFO] - Training Epoch: 1/2, step 13657/23838 completed (loss: 0.4255382716655731, acc: 0.8636363744735718)
[2025-02-16 13:38:41,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:41,440][root][INFO] - Training Epoch: 1/2, step 13658/23838 completed (loss: 0.05640337988734245, acc: 0.9918032884597778)
[2025-02-16 13:38:41,613][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:41,845][root][INFO] - Training Epoch: 1/2, step 13659/23838 completed (loss: 0.3380720615386963, acc: 0.9116021990776062)
[2025-02-16 13:38:42,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:42,278][root][INFO] - Training Epoch: 1/2, step 13660/23838 completed (loss: 0.4753253757953644, acc: 0.8900523781776428)
[2025-02-16 13:38:42,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:42,681][root][INFO] - Training Epoch: 1/2, step 13661/23838 completed (loss: 0.08084115386009216, acc: 0.9797979593276978)
[2025-02-16 13:38:42,891][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:43,146][root][INFO] - Training Epoch: 1/2, step 13662/23838 completed (loss: 0.17463672161102295, acc: 0.9465649127960205)
[2025-02-16 13:38:43,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:43,491][root][INFO] - Training Epoch: 1/2, step 13663/23838 completed (loss: 0.25441473722457886, acc: 0.9529411792755127)
[2025-02-16 13:38:43,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:43,917][root][INFO] - Training Epoch: 1/2, step 13664/23838 completed (loss: 0.41998913884162903, acc: 0.8909090757369995)
[2025-02-16 13:38:44,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:44,346][root][INFO] - Training Epoch: 1/2, step 13665/23838 completed (loss: 0.2326941192150116, acc: 0.9518072009086609)
[2025-02-16 13:38:44,503][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:44,727][root][INFO] - Training Epoch: 1/2, step 13666/23838 completed (loss: 0.23198716342449188, acc: 0.9411764740943909)
[2025-02-16 13:38:44,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:45,114][root][INFO] - Training Epoch: 1/2, step 13667/23838 completed (loss: 0.19147339463233948, acc: 0.9359999895095825)
[2025-02-16 13:38:45,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:45,567][root][INFO] - Training Epoch: 1/2, step 13668/23838 completed (loss: 0.14544638991355896, acc: 0.9583333134651184)
[2025-02-16 13:38:45,775][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:46,015][root][INFO] - Training Epoch: 1/2, step 13669/23838 completed (loss: 0.692949652671814, acc: 0.7872340679168701)
[2025-02-16 13:38:46,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:46,397][root][INFO] - Training Epoch: 1/2, step 13670/23838 completed (loss: 0.28595730662345886, acc: 0.9102563858032227)
[2025-02-16 13:38:46,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:46,756][root][INFO] - Training Epoch: 1/2, step 13671/23838 completed (loss: 0.37532392144203186, acc: 0.9047619104385376)
[2025-02-16 13:38:46,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:47,223][root][INFO] - Training Epoch: 1/2, step 13672/23838 completed (loss: 0.6707708835601807, acc: 0.8500000238418579)
[2025-02-16 13:38:47,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:47,682][root][INFO] - Training Epoch: 1/2, step 13673/23838 completed (loss: 0.36653226613998413, acc: 0.9075144529342651)
[2025-02-16 13:38:47,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:48,140][root][INFO] - Training Epoch: 1/2, step 13674/23838 completed (loss: 0.28842389583587646, acc: 0.9459459185600281)
[2025-02-16 13:38:48,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:48,604][root][INFO] - Training Epoch: 1/2, step 13675/23838 completed (loss: 0.21128928661346436, acc: 0.9494949579238892)
[2025-02-16 13:38:48,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:49,089][root][INFO] - Training Epoch: 1/2, step 13676/23838 completed (loss: 0.4881119728088379, acc: 0.8541666865348816)
[2025-02-16 13:38:49,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:49,529][root][INFO] - Training Epoch: 1/2, step 13677/23838 completed (loss: 0.3520084321498871, acc: 0.9200000166893005)
[2025-02-16 13:38:49,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:49,934][root][INFO] - Training Epoch: 1/2, step 13678/23838 completed (loss: 0.10813651233911514, acc: 0.9602272510528564)
[2025-02-16 13:38:50,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:50,340][root][INFO] - Training Epoch: 1/2, step 13679/23838 completed (loss: 0.32967081665992737, acc: 0.914893627166748)
[2025-02-16 13:38:50,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:50,748][root][INFO] - Training Epoch: 1/2, step 13680/23838 completed (loss: 0.3297860324382782, acc: 0.8775510191917419)
[2025-02-16 13:38:50,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:51,107][root][INFO] - Training Epoch: 1/2, step 13681/23838 completed (loss: 0.3043551743030548, acc: 0.9285714030265808)
[2025-02-16 13:38:51,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:51,557][root][INFO] - Training Epoch: 1/2, step 13682/23838 completed (loss: 0.19753749668598175, acc: 0.9333333373069763)
[2025-02-16 13:38:51,795][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:52,018][root][INFO] - Training Epoch: 1/2, step 13683/23838 completed (loss: 0.4233030080795288, acc: 0.8545454740524292)
[2025-02-16 13:38:52,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:52,465][root][INFO] - Training Epoch: 1/2, step 13684/23838 completed (loss: 0.11910684406757355, acc: 0.9543147087097168)
[2025-02-16 13:38:52,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:52,848][root][INFO] - Training Epoch: 1/2, step 13685/23838 completed (loss: 0.12332884222269058, acc: 0.9807692170143127)
[2025-02-16 13:38:53,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:53,236][root][INFO] - Training Epoch: 1/2, step 13686/23838 completed (loss: 0.10512080788612366, acc: 0.9756097793579102)
[2025-02-16 13:38:53,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:53,652][root][INFO] - Training Epoch: 1/2, step 13687/23838 completed (loss: 0.11528895795345306, acc: 0.9763779640197754)
[2025-02-16 13:38:53,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:54,057][root][INFO] - Training Epoch: 1/2, step 13688/23838 completed (loss: 0.09504619985818863, acc: 0.9661017060279846)
[2025-02-16 13:38:54,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:54,443][root][INFO] - Training Epoch: 1/2, step 13689/23838 completed (loss: 0.13059811294078827, acc: 0.9639639854431152)
[2025-02-16 13:38:54,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:54,905][root][INFO] - Training Epoch: 1/2, step 13690/23838 completed (loss: 0.44166114926338196, acc: 0.8769230842590332)
[2025-02-16 13:38:55,096][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:55,357][root][INFO] - Training Epoch: 1/2, step 13691/23838 completed (loss: 0.0967736467719078, acc: 0.96875)
[2025-02-16 13:38:55,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:55,767][root][INFO] - Training Epoch: 1/2, step 13692/23838 completed (loss: 0.15727685391902924, acc: 0.9659090638160706)
[2025-02-16 13:38:55,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:56,149][root][INFO] - Training Epoch: 1/2, step 13693/23838 completed (loss: 0.5000112652778625, acc: 0.8761062026023865)
[2025-02-16 13:38:56,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:56,601][root][INFO] - Training Epoch: 1/2, step 13694/23838 completed (loss: 0.3105447590351105, acc: 0.9166666865348816)
[2025-02-16 13:38:56,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:56,995][root][INFO] - Training Epoch: 1/2, step 13695/23838 completed (loss: 0.5779715180397034, acc: 0.8560606241226196)
[2025-02-16 13:38:57,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:57,441][root][INFO] - Training Epoch: 1/2, step 13696/23838 completed (loss: 0.1786537915468216, acc: 0.9142857193946838)
[2025-02-16 13:38:57,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:57,843][root][INFO] - Training Epoch: 1/2, step 13697/23838 completed (loss: 0.7349461317062378, acc: 0.7945205569267273)
[2025-02-16 13:38:58,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:58,319][root][INFO] - Training Epoch: 1/2, step 13698/23838 completed (loss: 0.44650915265083313, acc: 0.8552631735801697)
[2025-02-16 13:38:58,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:58,792][root][INFO] - Training Epoch: 1/2, step 13699/23838 completed (loss: 0.3305756151676178, acc: 0.8977272510528564)
[2025-02-16 13:38:58,962][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:59,237][root][INFO] - Training Epoch: 1/2, step 13700/23838 completed (loss: 0.4129214286804199, acc: 0.8986486196517944)
[2025-02-16 13:38:59,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:38:59,725][root][INFO] - Training Epoch: 1/2, step 13701/23838 completed (loss: 0.34569573402404785, acc: 0.9059829115867615)
[2025-02-16 13:38:59,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:00,145][root][INFO] - Training Epoch: 1/2, step 13702/23838 completed (loss: 0.18749143183231354, acc: 0.9444444179534912)
[2025-02-16 13:39:00,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:00,589][root][INFO] - Training Epoch: 1/2, step 13703/23838 completed (loss: 1.0475748777389526, acc: 0.6823529601097107)
[2025-02-16 13:39:00,808][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:01,036][root][INFO] - Training Epoch: 1/2, step 13704/23838 completed (loss: 0.21567116677761078, acc: 0.9465649127960205)
[2025-02-16 13:39:01,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:01,415][root][INFO] - Training Epoch: 1/2, step 13705/23838 completed (loss: 0.7813537120819092, acc: 0.8103448152542114)
[2025-02-16 13:39:01,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:01,865][root][INFO] - Training Epoch: 1/2, step 13706/23838 completed (loss: 0.3108832538127899, acc: 0.9108911156654358)
[2025-02-16 13:39:02,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:02,287][root][INFO] - Training Epoch: 1/2, step 13707/23838 completed (loss: 0.3716452121734619, acc: 0.8849557638168335)
[2025-02-16 13:39:02,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:02,664][root][INFO] - Training Epoch: 1/2, step 13708/23838 completed (loss: 0.4220413863658905, acc: 0.8689655065536499)
[2025-02-16 13:39:02,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:03,080][root][INFO] - Training Epoch: 1/2, step 13709/23838 completed (loss: 0.23682571947574615, acc: 0.9385964870452881)
[2025-02-16 13:39:03,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:03,544][root][INFO] - Training Epoch: 1/2, step 13710/23838 completed (loss: 0.326409250497818, acc: 0.896774172782898)
[2025-02-16 13:39:03,721][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:03,944][root][INFO] - Training Epoch: 1/2, step 13711/23838 completed (loss: 0.10367178171873093, acc: 0.9693251252174377)
[2025-02-16 13:39:04,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:04,427][root][INFO] - Training Epoch: 1/2, step 13712/23838 completed (loss: 0.2453376203775406, acc: 0.930232584476471)
[2025-02-16 13:39:04,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:04,803][root][INFO] - Training Epoch: 1/2, step 13713/23838 completed (loss: 0.35624709725379944, acc: 0.8910890817642212)
[2025-02-16 13:39:04,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:05,239][root][INFO] - Training Epoch: 1/2, step 13714/23838 completed (loss: 0.5267418026924133, acc: 0.8394160866737366)
[2025-02-16 13:39:05,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:05,721][root][INFO] - Training Epoch: 1/2, step 13715/23838 completed (loss: 0.2509908080101013, acc: 0.9142857193946838)
[2025-02-16 13:39:05,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:06,114][root][INFO] - Training Epoch: 1/2, step 13716/23838 completed (loss: 0.3134445250034332, acc: 0.9115646481513977)
[2025-02-16 13:39:06,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:06,638][root][INFO] - Training Epoch: 1/2, step 13717/23838 completed (loss: 0.15800191462039948, acc: 0.9658536314964294)
[2025-02-16 13:39:06,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:07,103][root][INFO] - Training Epoch: 1/2, step 13718/23838 completed (loss: 0.6412774324417114, acc: 0.8153846263885498)
[2025-02-16 13:39:07,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:07,723][root][INFO] - Training Epoch: 1/2, step 13719/23838 completed (loss: 0.23160101473331451, acc: 0.9402173757553101)
[2025-02-16 13:39:07,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:08,193][root][INFO] - Training Epoch: 1/2, step 13720/23838 completed (loss: 0.35592740774154663, acc: 0.8773584961891174)
[2025-02-16 13:39:08,466][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:08,812][root][INFO] - Training Epoch: 1/2, step 13721/23838 completed (loss: 0.07797538489103317, acc: 0.9753694534301758)
[2025-02-16 13:39:09,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:09,243][root][INFO] - Training Epoch: 1/2, step 13722/23838 completed (loss: 0.16928258538246155, acc: 0.9514563083648682)
[2025-02-16 13:39:09,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:09,854][root][INFO] - Training Epoch: 1/2, step 13723/23838 completed (loss: 0.15745137631893158, acc: 0.9354838728904724)
[2025-02-16 13:39:10,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:10,276][root][INFO] - Training Epoch: 1/2, step 13724/23838 completed (loss: 1.0342556238174438, acc: 0.7115384340286255)
[2025-02-16 13:39:10,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:10,938][root][INFO] - Training Epoch: 1/2, step 13725/23838 completed (loss: 0.1498272716999054, acc: 0.9594095945358276)
[2025-02-16 13:39:11,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:11,398][root][INFO] - Training Epoch: 1/2, step 13726/23838 completed (loss: 0.09087938815355301, acc: 0.971222996711731)
[2025-02-16 13:39:11,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:11,868][root][INFO] - Training Epoch: 1/2, step 13727/23838 completed (loss: 0.3553978502750397, acc: 0.9200000166893005)
[2025-02-16 13:39:12,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:12,232][root][INFO] - Training Epoch: 1/2, step 13728/23838 completed (loss: 0.22980113327503204, acc: 0.907216489315033)
[2025-02-16 13:39:12,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:12,823][root][INFO] - Training Epoch: 1/2, step 13729/23838 completed (loss: 0.6123340129852295, acc: 0.8421052694320679)
[2025-02-16 13:39:12,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:13,223][root][INFO] - Training Epoch: 1/2, step 13730/23838 completed (loss: 0.2871628999710083, acc: 0.9266666769981384)
[2025-02-16 13:39:13,421][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:13,639][root][INFO] - Training Epoch: 1/2, step 13731/23838 completed (loss: 0.46955204010009766, acc: 0.8839285969734192)
[2025-02-16 13:39:13,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:14,012][root][INFO] - Training Epoch: 1/2, step 13732/23838 completed (loss: 0.49801477789878845, acc: 0.8770053386688232)
[2025-02-16 13:39:14,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:14,451][root][INFO] - Training Epoch: 1/2, step 13733/23838 completed (loss: 0.37190133333206177, acc: 0.9027777910232544)
[2025-02-16 13:39:14,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:14,855][root][INFO] - Training Epoch: 1/2, step 13734/23838 completed (loss: 0.38077542185783386, acc: 0.8974359035491943)
[2025-02-16 13:39:15,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:15,228][root][INFO] - Training Epoch: 1/2, step 13735/23838 completed (loss: 0.7561808228492737, acc: 0.7956989407539368)
[2025-02-16 13:39:15,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:15,648][root][INFO] - Training Epoch: 1/2, step 13736/23838 completed (loss: 0.23146562278270721, acc: 0.9426229596138)
[2025-02-16 13:39:15,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:16,144][root][INFO] - Training Epoch: 1/2, step 13737/23838 completed (loss: 0.3761994242668152, acc: 0.9097744226455688)
[2025-02-16 13:39:16,339][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:16,565][root][INFO] - Training Epoch: 1/2, step 13738/23838 completed (loss: 0.178937166929245, acc: 0.9466666579246521)
[2025-02-16 13:39:16,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:17,034][root][INFO] - Training Epoch: 1/2, step 13739/23838 completed (loss: 0.5800203680992126, acc: 0.8255813717842102)
[2025-02-16 13:39:17,202][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:17,423][root][INFO] - Training Epoch: 1/2, step 13740/23838 completed (loss: 0.5345505475997925, acc: 0.8867924809455872)
[2025-02-16 13:39:17,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:17,819][root][INFO] - Training Epoch: 1/2, step 13741/23838 completed (loss: 0.11965841054916382, acc: 0.9607843160629272)
[2025-02-16 13:39:18,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:18,245][root][INFO] - Training Epoch: 1/2, step 13742/23838 completed (loss: 0.34518420696258545, acc: 0.8837209343910217)
[2025-02-16 13:39:18,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:18,683][root][INFO] - Training Epoch: 1/2, step 13743/23838 completed (loss: 0.8821572661399841, acc: 0.7384615540504456)
[2025-02-16 13:39:18,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:19,126][root][INFO] - Training Epoch: 1/2, step 13744/23838 completed (loss: 0.4860645830631256, acc: 0.8846153616905212)
[2025-02-16 13:39:19,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:19,586][root][INFO] - Training Epoch: 1/2, step 13745/23838 completed (loss: 0.21239511668682098, acc: 0.9234972596168518)
[2025-02-16 13:39:19,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:20,323][root][INFO] - Training Epoch: 1/2, step 13746/23838 completed (loss: 0.20901788771152496, acc: 0.9420289993286133)
[2025-02-16 13:39:20,491][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:20,739][root][INFO] - Training Epoch: 1/2, step 13747/23838 completed (loss: 0.14903594553470612, acc: 0.9503105878829956)
[2025-02-16 13:39:20,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:21,208][root][INFO] - Training Epoch: 1/2, step 13748/23838 completed (loss: 1.2194678783416748, acc: 0.7051281929016113)
[2025-02-16 13:39:21,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:21,672][root][INFO] - Training Epoch: 1/2, step 13749/23838 completed (loss: 0.15200698375701904, acc: 0.9661017060279846)
[2025-02-16 13:39:21,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:22,141][root][INFO] - Training Epoch: 1/2, step 13750/23838 completed (loss: 0.30723825097084045, acc: 0.9259259104728699)
[2025-02-16 13:39:22,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:22,573][root][INFO] - Training Epoch: 1/2, step 13751/23838 completed (loss: 0.39132893085479736, acc: 0.9277108311653137)
[2025-02-16 13:39:22,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:23,125][root][INFO] - Training Epoch: 1/2, step 13752/23838 completed (loss: 0.2340177595615387, acc: 0.9371069073677063)
[2025-02-16 13:39:23,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:23,583][root][INFO] - Training Epoch: 1/2, step 13753/23838 completed (loss: 0.1289919763803482, acc: 0.9589040875434875)
[2025-02-16 13:39:23,770][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:23,999][root][INFO] - Training Epoch: 1/2, step 13754/23838 completed (loss: 0.31998106837272644, acc: 0.918367326259613)
[2025-02-16 13:39:24,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:24,400][root][INFO] - Training Epoch: 1/2, step 13755/23838 completed (loss: 0.18835073709487915, acc: 0.9602649211883545)
[2025-02-16 13:39:24,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:24,783][root][INFO] - Training Epoch: 1/2, step 13756/23838 completed (loss: 0.24994438886642456, acc: 0.9197080135345459)
[2025-02-16 13:39:24,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:25,220][root][INFO] - Training Epoch: 1/2, step 13757/23838 completed (loss: 0.9595546722412109, acc: 0.7647058963775635)
[2025-02-16 13:39:25,433][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:25,683][root][INFO] - Training Epoch: 1/2, step 13758/23838 completed (loss: 0.29644227027893066, acc: 0.9081632494926453)
[2025-02-16 13:39:25,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:26,158][root][INFO] - Training Epoch: 1/2, step 13759/23838 completed (loss: 0.2642636299133301, acc: 0.9346405267715454)
[2025-02-16 13:39:26,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:26,616][root][INFO] - Training Epoch: 1/2, step 13760/23838 completed (loss: 0.22850346565246582, acc: 0.9090909361839294)
[2025-02-16 13:39:26,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:27,010][root][INFO] - Training Epoch: 1/2, step 13761/23838 completed (loss: 0.29961958527565, acc: 0.8805969953536987)
[2025-02-16 13:39:27,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:27,409][root][INFO] - Training Epoch: 1/2, step 13762/23838 completed (loss: 0.43495887517929077, acc: 0.9042553305625916)
[2025-02-16 13:39:27,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:27,790][root][INFO] - Training Epoch: 1/2, step 13763/23838 completed (loss: 0.43114742636680603, acc: 0.8999999761581421)
[2025-02-16 13:39:28,023][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:28,257][root][INFO] - Training Epoch: 1/2, step 13764/23838 completed (loss: 0.10913857072591782, acc: 0.9626168012619019)
[2025-02-16 13:39:28,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:28,661][root][INFO] - Training Epoch: 1/2, step 13765/23838 completed (loss: 0.3154553472995758, acc: 0.9344262480735779)
[2025-02-16 13:39:28,837][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:29,080][root][INFO] - Training Epoch: 1/2, step 13766/23838 completed (loss: 0.24086502194404602, acc: 0.9358974099159241)
[2025-02-16 13:39:29,295][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:29,526][root][INFO] - Training Epoch: 1/2, step 13767/23838 completed (loss: 0.2879219651222229, acc: 0.892307698726654)
[2025-02-16 13:39:29,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:30,036][root][INFO] - Training Epoch: 1/2, step 13768/23838 completed (loss: 0.16303355991840363, acc: 0.954023003578186)
[2025-02-16 13:39:30,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:30,419][root][INFO] - Training Epoch: 1/2, step 13769/23838 completed (loss: 0.15969350934028625, acc: 0.9692307710647583)
[2025-02-16 13:39:30,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:30,786][root][INFO] - Training Epoch: 1/2, step 13770/23838 completed (loss: 0.06961344182491302, acc: 0.984375)
[2025-02-16 13:39:31,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:31,281][root][INFO] - Training Epoch: 1/2, step 13771/23838 completed (loss: 0.04875341057777405, acc: 0.9935064911842346)
[2025-02-16 13:39:31,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:31,687][root][INFO] - Training Epoch: 1/2, step 13772/23838 completed (loss: 0.5311224460601807, acc: 0.8253968358039856)
[2025-02-16 13:39:31,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:32,150][root][INFO] - Training Epoch: 1/2, step 13773/23838 completed (loss: 0.3969421982765198, acc: 0.9086538553237915)
[2025-02-16 13:39:32,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:32,616][root][INFO] - Training Epoch: 1/2, step 13774/23838 completed (loss: 0.27889081835746765, acc: 0.9047619104385376)
[2025-02-16 13:39:32,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:33,045][root][INFO] - Training Epoch: 1/2, step 13775/23838 completed (loss: 0.19835156202316284, acc: 0.9370078444480896)
[2025-02-16 13:39:33,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:33,441][root][INFO] - Training Epoch: 1/2, step 13776/23838 completed (loss: 0.23278076946735382, acc: 0.9365079402923584)
[2025-02-16 13:39:33,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:34,102][root][INFO] - Training Epoch: 1/2, step 13777/23838 completed (loss: 0.19464218616485596, acc: 0.9482758641242981)
[2025-02-16 13:39:34,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:34,457][root][INFO] - Training Epoch: 1/2, step 13778/23838 completed (loss: 0.28156545758247375, acc: 0.9074074029922485)
[2025-02-16 13:39:34,611][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:34,826][root][INFO] - Training Epoch: 1/2, step 13779/23838 completed (loss: 0.5440856218338013, acc: 0.8349514603614807)
[2025-02-16 13:39:35,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:35,236][root][INFO] - Training Epoch: 1/2, step 13780/23838 completed (loss: 0.2074735164642334, acc: 0.9639639854431152)
[2025-02-16 13:39:35,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:35,623][root][INFO] - Training Epoch: 1/2, step 13781/23838 completed (loss: 0.12460607290267944, acc: 0.9603960514068604)
[2025-02-16 13:39:35,810][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:36,026][root][INFO] - Training Epoch: 1/2, step 13782/23838 completed (loss: 0.2473214566707611, acc: 0.8902438879013062)
[2025-02-16 13:39:36,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:36,425][root][INFO] - Training Epoch: 1/2, step 13783/23838 completed (loss: 0.2862641513347626, acc: 0.9130434989929199)
[2025-02-16 13:39:36,669][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:36,958][root][INFO] - Training Epoch: 1/2, step 13784/23838 completed (loss: 0.3058125078678131, acc: 0.9109588861465454)
[2025-02-16 13:39:37,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:37,415][root][INFO] - Training Epoch: 1/2, step 13785/23838 completed (loss: 0.16240723431110382, acc: 0.9415584206581116)
[2025-02-16 13:39:37,659][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:37,888][root][INFO] - Training Epoch: 1/2, step 13786/23838 completed (loss: 0.1845293492078781, acc: 0.95652174949646)
[2025-02-16 13:39:38,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:38,281][root][INFO] - Training Epoch: 1/2, step 13787/23838 completed (loss: 0.21127909421920776, acc: 0.9583333134651184)
[2025-02-16 13:39:38,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:38,709][root][INFO] - Training Epoch: 1/2, step 13788/23838 completed (loss: 0.3118857741355896, acc: 0.9090909361839294)
[2025-02-16 13:39:38,929][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:39,150][root][INFO] - Training Epoch: 1/2, step 13789/23838 completed (loss: 0.2632606327533722, acc: 0.9224806427955627)
[2025-02-16 13:39:39,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:39,528][root][INFO] - Training Epoch: 1/2, step 13790/23838 completed (loss: 0.17594827711582184, acc: 0.9490445852279663)
[2025-02-16 13:39:39,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:39,967][root][INFO] - Training Epoch: 1/2, step 13791/23838 completed (loss: 0.18422561883926392, acc: 0.9567901492118835)
[2025-02-16 13:39:40,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:40,415][root][INFO] - Training Epoch: 1/2, step 13792/23838 completed (loss: 0.2643055319786072, acc: 0.922535240650177)
[2025-02-16 13:39:40,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:40,820][root][INFO] - Training Epoch: 1/2, step 13793/23838 completed (loss: 0.36150115728378296, acc: 0.9150943160057068)
[2025-02-16 13:39:40,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:41,217][root][INFO] - Training Epoch: 1/2, step 13794/23838 completed (loss: 0.11491916328668594, acc: 0.9786096215248108)
[2025-02-16 13:39:41,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:41,612][root][INFO] - Training Epoch: 1/2, step 13795/23838 completed (loss: 0.18256963789463043, acc: 0.9571428298950195)
[2025-02-16 13:39:41,846][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:42,107][root][INFO] - Training Epoch: 1/2, step 13796/23838 completed (loss: 0.2661190927028656, acc: 0.9370078444480896)
[2025-02-16 13:39:42,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:42,495][root][INFO] - Training Epoch: 1/2, step 13797/23838 completed (loss: 0.09639667719602585, acc: 0.9791666865348816)
[2025-02-16 13:39:42,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:42,917][root][INFO] - Training Epoch: 1/2, step 13798/23838 completed (loss: 0.08602414280176163, acc: 0.9817073345184326)
[2025-02-16 13:39:43,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:43,328][root][INFO] - Training Epoch: 1/2, step 13799/23838 completed (loss: 0.26508668065071106, acc: 0.9278350472450256)
[2025-02-16 13:39:43,490][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:43,718][root][INFO] - Training Epoch: 1/2, step 13800/23838 completed (loss: 0.2063150852918625, acc: 0.9538461565971375)
[2025-02-16 13:39:43,868][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:44,136][root][INFO] - Training Epoch: 1/2, step 13801/23838 completed (loss: 0.06803165376186371, acc: 0.987500011920929)
[2025-02-16 13:39:44,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:44,615][root][INFO] - Training Epoch: 1/2, step 13802/23838 completed (loss: 0.4863434433937073, acc: 0.8333333134651184)
[2025-02-16 13:39:44,870][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:45,094][root][INFO] - Training Epoch: 1/2, step 13803/23838 completed (loss: 0.08534137904644012, acc: 0.9818181991577148)
[2025-02-16 13:39:45,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:45,481][root][INFO] - Training Epoch: 1/2, step 13804/23838 completed (loss: 0.06285125017166138, acc: 0.9779411554336548)
[2025-02-16 13:39:45,670][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:45,893][root][INFO] - Training Epoch: 1/2, step 13805/23838 completed (loss: 0.03389669582247734, acc: 0.991304337978363)
[2025-02-16 13:39:46,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:46,292][root][INFO] - Training Epoch: 1/2, step 13806/23838 completed (loss: 0.43454352021217346, acc: 0.8947368264198303)
[2025-02-16 13:39:46,471][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:46,707][root][INFO] - Training Epoch: 1/2, step 13807/23838 completed (loss: 0.38057953119277954, acc: 0.9101123809814453)
[2025-02-16 13:39:46,934][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:47,228][root][INFO] - Training Epoch: 1/2, step 13808/23838 completed (loss: 0.3370005488395691, acc: 0.9051094651222229)
[2025-02-16 13:39:47,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:47,641][root][INFO] - Training Epoch: 1/2, step 13809/23838 completed (loss: 0.3860674202442169, acc: 0.8938053250312805)
[2025-02-16 13:39:47,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:48,138][root][INFO] - Training Epoch: 1/2, step 13810/23838 completed (loss: 0.1292356550693512, acc: 0.9668246507644653)
[2025-02-16 13:39:48,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:48,584][root][INFO] - Training Epoch: 1/2, step 13811/23838 completed (loss: 0.1111803725361824, acc: 0.9684210419654846)
[2025-02-16 13:39:48,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:49,053][root][INFO] - Training Epoch: 1/2, step 13812/23838 completed (loss: 0.41934695839881897, acc: 0.8991596698760986)
[2025-02-16 13:39:49,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:49,500][root][INFO] - Training Epoch: 1/2, step 13813/23838 completed (loss: 0.5252086520195007, acc: 0.8815165758132935)
[2025-02-16 13:39:49,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:50,082][root][INFO] - Training Epoch: 1/2, step 13814/23838 completed (loss: 0.17343398928642273, acc: 0.9340101480484009)
[2025-02-16 13:39:50,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:50,508][root][INFO] - Training Epoch: 1/2, step 13815/23838 completed (loss: 0.13319027423858643, acc: 0.9629629850387573)
[2025-02-16 13:39:50,706][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:50,965][root][INFO] - Training Epoch: 1/2, step 13816/23838 completed (loss: 0.16384398937225342, acc: 0.9166666865348816)
[2025-02-16 13:39:51,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:51,378][root][INFO] - Training Epoch: 1/2, step 13817/23838 completed (loss: 0.16172009706497192, acc: 0.9669421315193176)
[2025-02-16 13:39:51,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:51,808][root][INFO] - Training Epoch: 1/2, step 13818/23838 completed (loss: 0.30446332693099976, acc: 0.9269663095474243)
[2025-02-16 13:39:52,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:52,291][root][INFO] - Training Epoch: 1/2, step 13819/23838 completed (loss: 0.5936115980148315, acc: 0.818791925907135)
[2025-02-16 13:39:52,489][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:52,707][root][INFO] - Training Epoch: 1/2, step 13820/23838 completed (loss: 0.49715691804885864, acc: 0.8469387888908386)
[2025-02-16 13:39:52,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:53,167][root][INFO] - Training Epoch: 1/2, step 13821/23838 completed (loss: 0.19727715849876404, acc: 0.9537572264671326)
[2025-02-16 13:39:53,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:53,629][root][INFO] - Training Epoch: 1/2, step 13822/23838 completed (loss: 0.5284565091133118, acc: 0.8648648858070374)
[2025-02-16 13:39:53,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:54,060][root][INFO] - Training Epoch: 1/2, step 13823/23838 completed (loss: 0.2536514699459076, acc: 0.8939393758773804)
[2025-02-16 13:39:54,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:54,510][root][INFO] - Training Epoch: 1/2, step 13824/23838 completed (loss: 0.20278507471084595, acc: 0.9098360538482666)
[2025-02-16 13:39:54,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:54,913][root][INFO] - Training Epoch: 1/2, step 13825/23838 completed (loss: 0.4388793706893921, acc: 0.9039999842643738)
[2025-02-16 13:39:55,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:55,301][root][INFO] - Training Epoch: 1/2, step 13826/23838 completed (loss: 0.321563184261322, acc: 0.9058823585510254)
[2025-02-16 13:39:55,474][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:55,719][root][INFO] - Training Epoch: 1/2, step 13827/23838 completed (loss: 0.047659292817115784, acc: 1.0)
[2025-02-16 13:39:55,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:56,154][root][INFO] - Training Epoch: 1/2, step 13828/23838 completed (loss: 0.41621923446655273, acc: 0.8918918967247009)
[2025-02-16 13:39:56,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:56,595][root][INFO] - Training Epoch: 1/2, step 13829/23838 completed (loss: 0.2931317985057831, acc: 0.9390243887901306)
[2025-02-16 13:39:56,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:57,009][root][INFO] - Training Epoch: 1/2, step 13830/23838 completed (loss: 0.18212556838989258, acc: 0.9399999976158142)
[2025-02-16 13:39:57,175][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:57,394][root][INFO] - Training Epoch: 1/2, step 13831/23838 completed (loss: 0.297140896320343, acc: 0.8970588445663452)
[2025-02-16 13:39:57,543][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:57,763][root][INFO] - Training Epoch: 1/2, step 13832/23838 completed (loss: 0.335206538438797, acc: 0.9324324131011963)
[2025-02-16 13:39:57,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:58,217][root][INFO] - Training Epoch: 1/2, step 13833/23838 completed (loss: 0.11287837475538254, acc: 0.9689119458198547)
[2025-02-16 13:39:58,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:58,662][root][INFO] - Training Epoch: 1/2, step 13834/23838 completed (loss: 0.16325518488883972, acc: 0.954023003578186)
[2025-02-16 13:39:58,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:59,146][root][INFO] - Training Epoch: 1/2, step 13835/23838 completed (loss: 0.2296465039253235, acc: 0.9615384340286255)
[2025-02-16 13:39:59,321][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:39:59,545][root][INFO] - Training Epoch: 1/2, step 13836/23838 completed (loss: 0.2784755825996399, acc: 0.9217391014099121)
[2025-02-16 13:39:59,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:00,020][root][INFO] - Training Epoch: 1/2, step 13837/23838 completed (loss: 0.3206202983856201, acc: 0.9134199023246765)
[2025-02-16 13:40:00,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:00,460][root][INFO] - Training Epoch: 1/2, step 13838/23838 completed (loss: 0.20171815156936646, acc: 0.932584285736084)
[2025-02-16 13:40:00,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:00,898][root][INFO] - Training Epoch: 1/2, step 13839/23838 completed (loss: 0.084988534450531, acc: 0.9669421315193176)
[2025-02-16 13:40:01,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:01,322][root][INFO] - Training Epoch: 1/2, step 13840/23838 completed (loss: 0.8186222910881042, acc: 0.75)
[2025-02-16 13:40:01,522][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:01,764][root][INFO] - Training Epoch: 1/2, step 13841/23838 completed (loss: 0.3369100093841553, acc: 0.9268292784690857)
[2025-02-16 13:40:02,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:02,250][root][INFO] - Training Epoch: 1/2, step 13842/23838 completed (loss: 0.4275294542312622, acc: 0.8763440847396851)
[2025-02-16 13:40:02,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:02,638][root][INFO] - Training Epoch: 1/2, step 13843/23838 completed (loss: 0.3614518940448761, acc: 0.8536585569381714)
[2025-02-16 13:40:02,849][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:03,073][root][INFO] - Training Epoch: 1/2, step 13844/23838 completed (loss: 0.5306819081306458, acc: 0.8360655903816223)
[2025-02-16 13:40:03,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:03,531][root][INFO] - Training Epoch: 1/2, step 13845/23838 completed (loss: 0.09150486439466476, acc: 0.9746192693710327)
[2025-02-16 13:40:03,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:04,353][root][INFO] - Training Epoch: 1/2, step 13846/23838 completed (loss: 0.17120155692100525, acc: 0.959770143032074)
[2025-02-16 13:40:04,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:04,724][root][INFO] - Training Epoch: 1/2, step 13847/23838 completed (loss: 0.5392702221870422, acc: 0.8588235378265381)
[2025-02-16 13:40:04,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:05,163][root][INFO] - Training Epoch: 1/2, step 13848/23838 completed (loss: 0.09974896907806396, acc: 0.9729729890823364)
[2025-02-16 13:40:05,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:05,616][root][INFO] - Training Epoch: 1/2, step 13849/23838 completed (loss: 0.15825647115707397, acc: 0.9532710313796997)
[2025-02-16 13:40:05,831][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:06,069][root][INFO] - Training Epoch: 1/2, step 13850/23838 completed (loss: 0.2961212992668152, acc: 0.8648648858070374)
[2025-02-16 13:40:06,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:06,482][root][INFO] - Training Epoch: 1/2, step 13851/23838 completed (loss: 0.31050926446914673, acc: 0.9174311757087708)
[2025-02-16 13:40:06,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:06,977][root][INFO] - Training Epoch: 1/2, step 13852/23838 completed (loss: 0.4372520446777344, acc: 0.8648648858070374)
[2025-02-16 13:40:07,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:07,438][root][INFO] - Training Epoch: 1/2, step 13853/23838 completed (loss: 0.4232402443885803, acc: 0.8533333539962769)
[2025-02-16 13:40:07,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:07,860][root][INFO] - Training Epoch: 1/2, step 13854/23838 completed (loss: 0.10106024146080017, acc: 0.9599999785423279)
[2025-02-16 13:40:08,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:08,250][root][INFO] - Training Epoch: 1/2, step 13855/23838 completed (loss: 0.1922081559896469, acc: 0.970588207244873)
[2025-02-16 13:40:08,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:08,659][root][INFO] - Training Epoch: 1/2, step 13856/23838 completed (loss: 0.29161813855171204, acc: 0.9217391014099121)
[2025-02-16 13:40:08,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:09,072][root][INFO] - Training Epoch: 1/2, step 13857/23838 completed (loss: 0.3390115797519684, acc: 0.9354838728904724)
[2025-02-16 13:40:09,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:09,509][root][INFO] - Training Epoch: 1/2, step 13858/23838 completed (loss: 0.2238265424966812, acc: 0.9296875)
[2025-02-16 13:40:09,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:09,983][root][INFO] - Training Epoch: 1/2, step 13859/23838 completed (loss: 0.27210164070129395, acc: 0.9173553586006165)
[2025-02-16 13:40:10,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:10,450][root][INFO] - Training Epoch: 1/2, step 13860/23838 completed (loss: 0.15361927449703217, acc: 0.9776119589805603)
[2025-02-16 13:40:10,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:10,869][root][INFO] - Training Epoch: 1/2, step 13861/23838 completed (loss: 0.05998214706778526, acc: 0.9895833134651184)
[2025-02-16 13:40:11,043][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:11,281][root][INFO] - Training Epoch: 1/2, step 13862/23838 completed (loss: 0.47473201155662537, acc: 0.9054054021835327)
[2025-02-16 13:40:11,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:11,727][root][INFO] - Training Epoch: 1/2, step 13863/23838 completed (loss: 0.2292385846376419, acc: 0.9134615659713745)
[2025-02-16 13:40:11,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:12,166][root][INFO] - Training Epoch: 1/2, step 13864/23838 completed (loss: 0.12598945200443268, acc: 0.9664804339408875)
[2025-02-16 13:40:12,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:12,648][root][INFO] - Training Epoch: 1/2, step 13865/23838 completed (loss: 0.15781348943710327, acc: 0.9593023061752319)
[2025-02-16 13:40:12,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:13,121][root][INFO] - Training Epoch: 1/2, step 13866/23838 completed (loss: 0.10209488123655319, acc: 0.955974817276001)
[2025-02-16 13:40:13,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:13,853][root][INFO] - Training Epoch: 1/2, step 13867/23838 completed (loss: 0.24445495009422302, acc: 0.9292035102844238)
[2025-02-16 13:40:14,038][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:14,257][root][INFO] - Training Epoch: 1/2, step 13868/23838 completed (loss: 0.4084160327911377, acc: 0.8771929740905762)
[2025-02-16 13:40:14,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:14,692][root][INFO] - Training Epoch: 1/2, step 13869/23838 completed (loss: 0.28055593371391296, acc: 0.9370629191398621)
[2025-02-16 13:40:14,886][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:15,116][root][INFO] - Training Epoch: 1/2, step 13870/23838 completed (loss: 0.26880770921707153, acc: 0.9197080135345459)
[2025-02-16 13:40:15,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:15,489][root][INFO] - Training Epoch: 1/2, step 13871/23838 completed (loss: 0.14258462190628052, acc: 0.9599999785423279)
[2025-02-16 13:40:15,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:15,877][root][INFO] - Training Epoch: 1/2, step 13872/23838 completed (loss: 0.5891757607460022, acc: 0.8924731016159058)
[2025-02-16 13:40:16,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:16,314][root][INFO] - Training Epoch: 1/2, step 13873/23838 completed (loss: 0.22588855028152466, acc: 0.9513513445854187)
[2025-02-16 13:40:16,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:16,783][root][INFO] - Training Epoch: 1/2, step 13874/23838 completed (loss: 0.3443529009819031, acc: 0.8620689511299133)
[2025-02-16 13:40:16,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:17,193][root][INFO] - Training Epoch: 1/2, step 13875/23838 completed (loss: 0.1684839129447937, acc: 0.9382715821266174)
[2025-02-16 13:40:17,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:17,597][root][INFO] - Training Epoch: 1/2, step 13876/23838 completed (loss: 0.422534704208374, acc: 0.89682537317276)
[2025-02-16 13:40:17,830][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:18,099][root][INFO] - Training Epoch: 1/2, step 13877/23838 completed (loss: 0.5647714138031006, acc: 0.8562091588973999)
[2025-02-16 13:40:18,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:18,552][root][INFO] - Training Epoch: 1/2, step 13878/23838 completed (loss: 0.21664749085903168, acc: 0.9491525292396545)
[2025-02-16 13:40:18,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:18,978][root][INFO] - Training Epoch: 1/2, step 13879/23838 completed (loss: 0.39138078689575195, acc: 0.8735632300376892)
[2025-02-16 13:40:19,194][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:19,427][root][INFO] - Training Epoch: 1/2, step 13880/23838 completed (loss: 0.1788388341665268, acc: 0.9343065619468689)
[2025-02-16 13:40:19,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:19,827][root][INFO] - Training Epoch: 1/2, step 13881/23838 completed (loss: 0.0637633427977562, acc: 0.9893617033958435)
[2025-02-16 13:40:19,988][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:20,267][root][INFO] - Training Epoch: 1/2, step 13882/23838 completed (loss: 0.14752714335918427, acc: 0.9644970297813416)
[2025-02-16 13:40:20,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:20,759][root][INFO] - Training Epoch: 1/2, step 13883/23838 completed (loss: 0.31477755308151245, acc: 0.9253731369972229)
[2025-02-16 13:40:20,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:21,214][root][INFO] - Training Epoch: 1/2, step 13884/23838 completed (loss: 0.13519775867462158, acc: 0.9692307710647583)
[2025-02-16 13:40:21,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:21,664][root][INFO] - Training Epoch: 1/2, step 13885/23838 completed (loss: 0.2159254252910614, acc: 0.9611650705337524)
[2025-02-16 13:40:21,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:22,077][root][INFO] - Training Epoch: 1/2, step 13886/23838 completed (loss: 0.09209299832582474, acc: 0.9925373196601868)
[2025-02-16 13:40:22,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:22,518][root][INFO] - Training Epoch: 1/2, step 13887/23838 completed (loss: 0.2742675542831421, acc: 0.9090909361839294)
[2025-02-16 13:40:22,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:22,941][root][INFO] - Training Epoch: 1/2, step 13888/23838 completed (loss: 0.09959699958562851, acc: 0.9801980257034302)
[2025-02-16 13:40:23,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:23,347][root][INFO] - Training Epoch: 1/2, step 13889/23838 completed (loss: 0.28457939624786377, acc: 0.9166666865348816)
[2025-02-16 13:40:23,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:23,745][root][INFO] - Training Epoch: 1/2, step 13890/23838 completed (loss: 0.3616689443588257, acc: 0.8999999761581421)
[2025-02-16 13:40:23,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:24,113][root][INFO] - Training Epoch: 1/2, step 13891/23838 completed (loss: 0.23449748754501343, acc: 0.918181836605072)
[2025-02-16 13:40:24,267][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:24,491][root][INFO] - Training Epoch: 1/2, step 13892/23838 completed (loss: 0.11156154423952103, acc: 0.9679999947547913)
[2025-02-16 13:40:24,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:24,930][root][INFO] - Training Epoch: 1/2, step 13893/23838 completed (loss: 0.2217831313610077, acc: 0.9452054500579834)
[2025-02-16 13:40:25,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:25,343][root][INFO] - Training Epoch: 1/2, step 13894/23838 completed (loss: 0.3904576301574707, acc: 0.8999999761581421)
[2025-02-16 13:40:25,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:25,796][root][INFO] - Training Epoch: 1/2, step 13895/23838 completed (loss: 0.5074237585067749, acc: 0.849056601524353)
[2025-02-16 13:40:26,078][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:26,413][root][INFO] - Training Epoch: 1/2, step 13896/23838 completed (loss: 0.30567988753318787, acc: 0.9318181872367859)
[2025-02-16 13:40:26,562][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:26,791][root][INFO] - Training Epoch: 1/2, step 13897/23838 completed (loss: 0.3628682792186737, acc: 0.8799999952316284)
[2025-02-16 13:40:26,958][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:27,186][root][INFO] - Training Epoch: 1/2, step 13898/23838 completed (loss: 0.7063534259796143, acc: 0.8220338821411133)
[2025-02-16 13:40:27,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:27,612][root][INFO] - Training Epoch: 1/2, step 13899/23838 completed (loss: 0.3478287160396576, acc: 0.9186046719551086)
[2025-02-16 13:40:27,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:28,041][root][INFO] - Training Epoch: 1/2, step 13900/23838 completed (loss: 0.1522768884897232, acc: 0.9224137663841248)
[2025-02-16 13:40:28,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:28,464][root][INFO] - Training Epoch: 1/2, step 13901/23838 completed (loss: 0.08701257407665253, acc: 0.9673202633857727)
[2025-02-16 13:40:28,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:28,849][root][INFO] - Training Epoch: 1/2, step 13902/23838 completed (loss: 0.39205458760261536, acc: 0.8963963985443115)
[2025-02-16 13:40:29,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:29,278][root][INFO] - Training Epoch: 1/2, step 13903/23838 completed (loss: 0.25550776720046997, acc: 0.9407407641410828)
[2025-02-16 13:40:29,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:29,747][root][INFO] - Training Epoch: 1/2, step 13904/23838 completed (loss: 0.09004107862710953, acc: 0.9710144996643066)
[2025-02-16 13:40:29,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:30,214][root][INFO] - Training Epoch: 1/2, step 13905/23838 completed (loss: 0.3717631995677948, acc: 0.8888888955116272)
[2025-02-16 13:40:30,389][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:30,617][root][INFO] - Training Epoch: 1/2, step 13906/23838 completed (loss: 0.28190577030181885, acc: 0.921875)
[2025-02-16 13:40:30,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:31,038][root][INFO] - Training Epoch: 1/2, step 13907/23838 completed (loss: 0.3568848967552185, acc: 0.9375)
[2025-02-16 13:40:31,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:31,462][root][INFO] - Training Epoch: 1/2, step 13908/23838 completed (loss: 0.37990355491638184, acc: 0.8620689511299133)
[2025-02-16 13:40:31,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:31,918][root][INFO] - Training Epoch: 1/2, step 13909/23838 completed (loss: 0.4456925392150879, acc: 0.8859649300575256)
[2025-02-16 13:40:32,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:32,364][root][INFO] - Training Epoch: 1/2, step 13910/23838 completed (loss: 0.16378359496593475, acc: 0.9444444179534912)
[2025-02-16 13:40:32,610][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:32,845][root][INFO] - Training Epoch: 1/2, step 13911/23838 completed (loss: 0.15381406247615814, acc: 0.9548872113227844)
[2025-02-16 13:40:33,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:33,301][root][INFO] - Training Epoch: 1/2, step 13912/23838 completed (loss: 0.14326176047325134, acc: 0.9402984976768494)
[2025-02-16 13:40:33,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:33,710][root][INFO] - Training Epoch: 1/2, step 13913/23838 completed (loss: 0.49404385685920715, acc: 0.886227548122406)
[2025-02-16 13:40:33,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:34,140][root][INFO] - Training Epoch: 1/2, step 13914/23838 completed (loss: 0.28090307116508484, acc: 0.9158878326416016)
[2025-02-16 13:40:34,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:34,522][root][INFO] - Training Epoch: 1/2, step 13915/23838 completed (loss: 0.08205783367156982, acc: 0.9846153855323792)
[2025-02-16 13:40:34,676][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:34,917][root][INFO] - Training Epoch: 1/2, step 13916/23838 completed (loss: 0.18553203344345093, acc: 0.9626865386962891)
[2025-02-16 13:40:35,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:35,361][root][INFO] - Training Epoch: 1/2, step 13917/23838 completed (loss: 0.23489193618297577, acc: 0.9230769276618958)
[2025-02-16 13:40:35,674][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:35,985][root][INFO] - Training Epoch: 1/2, step 13918/23838 completed (loss: 0.274502158164978, acc: 0.9242424368858337)
[2025-02-16 13:40:36,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:36,381][root][INFO] - Training Epoch: 1/2, step 13919/23838 completed (loss: 0.31860822439193726, acc: 0.9175257682800293)
[2025-02-16 13:40:36,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:36,829][root][INFO] - Training Epoch: 1/2, step 13920/23838 completed (loss: 0.2170678675174713, acc: 0.9436619877815247)
[2025-02-16 13:40:37,036][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:37,257][root][INFO] - Training Epoch: 1/2, step 13921/23838 completed (loss: 0.17255465686321259, acc: 0.9247311949729919)
[2025-02-16 13:40:37,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:37,695][root][INFO] - Training Epoch: 1/2, step 13922/23838 completed (loss: 0.41577982902526855, acc: 0.8720930218696594)
[2025-02-16 13:40:37,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:38,142][root][INFO] - Training Epoch: 1/2, step 13923/23838 completed (loss: 0.49012675881385803, acc: 0.8837209343910217)
[2025-02-16 13:40:38,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:38,576][root][INFO] - Training Epoch: 1/2, step 13924/23838 completed (loss: 0.31996288895606995, acc: 0.9101123809814453)
[2025-02-16 13:40:38,757][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:38,995][root][INFO] - Training Epoch: 1/2, step 13925/23838 completed (loss: 0.24509988725185394, acc: 0.9555555582046509)
[2025-02-16 13:40:39,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:39,446][root][INFO] - Training Epoch: 1/2, step 13926/23838 completed (loss: 0.4183545708656311, acc: 0.8799999952316284)
[2025-02-16 13:40:39,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:39,883][root][INFO] - Training Epoch: 1/2, step 13927/23838 completed (loss: 0.17940305173397064, acc: 0.9292929172515869)
[2025-02-16 13:40:40,088][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:40,327][root][INFO] - Training Epoch: 1/2, step 13928/23838 completed (loss: 0.13207580149173737, acc: 0.9702970385551453)
[2025-02-16 13:40:40,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:40,803][root][INFO] - Training Epoch: 1/2, step 13929/23838 completed (loss: 0.4848130941390991, acc: 0.9278350472450256)
[2025-02-16 13:40:40,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:41,210][root][INFO] - Training Epoch: 1/2, step 13930/23838 completed (loss: 0.3507674038410187, acc: 0.930232584476471)
[2025-02-16 13:40:41,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:41,620][root][INFO] - Training Epoch: 1/2, step 13931/23838 completed (loss: 0.16934283077716827, acc: 0.9464285969734192)
[2025-02-16 13:40:41,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:42,067][root][INFO] - Training Epoch: 1/2, step 13932/23838 completed (loss: 0.12633848190307617, acc: 0.9646017551422119)
[2025-02-16 13:40:42,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:42,565][root][INFO] - Training Epoch: 1/2, step 13933/23838 completed (loss: 0.15663069486618042, acc: 0.9468085169792175)
[2025-02-16 13:40:42,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:43,035][root][INFO] - Training Epoch: 1/2, step 13934/23838 completed (loss: 0.20803004503250122, acc: 0.9516128897666931)
[2025-02-16 13:40:43,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:43,448][root][INFO] - Training Epoch: 1/2, step 13935/23838 completed (loss: 0.4524742364883423, acc: 0.8780487775802612)
[2025-02-16 13:40:43,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:43,918][root][INFO] - Training Epoch: 1/2, step 13936/23838 completed (loss: 0.16710901260375977, acc: 0.9464285969734192)
[2025-02-16 13:40:44,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:44,380][root][INFO] - Training Epoch: 1/2, step 13937/23838 completed (loss: 0.37333372235298157, acc: 0.893203854560852)
[2025-02-16 13:40:44,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:44,790][root][INFO] - Training Epoch: 1/2, step 13938/23838 completed (loss: 0.37315458059310913, acc: 0.8936170339584351)
[2025-02-16 13:40:45,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:45,228][root][INFO] - Training Epoch: 1/2, step 13939/23838 completed (loss: 0.18297837674617767, acc: 0.9589040875434875)
[2025-02-16 13:40:45,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:45,676][root][INFO] - Training Epoch: 1/2, step 13940/23838 completed (loss: 0.14788110554218292, acc: 0.9555555582046509)
[2025-02-16 13:40:45,883][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:46,097][root][INFO] - Training Epoch: 1/2, step 13941/23838 completed (loss: 0.10167931020259857, acc: 0.9743589758872986)
[2025-02-16 13:40:46,276][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:46,511][root][INFO] - Training Epoch: 1/2, step 13942/23838 completed (loss: 1.2891963720321655, acc: 0.6899999976158142)
[2025-02-16 13:40:46,755][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:47,009][root][INFO] - Training Epoch: 1/2, step 13943/23838 completed (loss: 0.6951111555099487, acc: 0.8199999928474426)
[2025-02-16 13:40:47,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:47,412][root][INFO] - Training Epoch: 1/2, step 13944/23838 completed (loss: 0.5697709321975708, acc: 0.8730158805847168)
[2025-02-16 13:40:47,600][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:47,850][root][INFO] - Training Epoch: 1/2, step 13945/23838 completed (loss: 0.11038314551115036, acc: 0.9642857313156128)
[2025-02-16 13:40:48,025][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:48,248][root][INFO] - Training Epoch: 1/2, step 13946/23838 completed (loss: 0.21474100649356842, acc: 0.9292035102844238)
[2025-02-16 13:40:48,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:48,673][root][INFO] - Training Epoch: 1/2, step 13947/23838 completed (loss: 0.2522542476654053, acc: 0.9104477763175964)
[2025-02-16 13:40:48,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:49,108][root][INFO] - Training Epoch: 1/2, step 13948/23838 completed (loss: 0.2897225320339203, acc: 0.9175257682800293)
[2025-02-16 13:40:49,318][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:49,554][root][INFO] - Training Epoch: 1/2, step 13949/23838 completed (loss: 0.23144467175006866, acc: 0.9166666865348816)
[2025-02-16 13:40:49,764][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:49,998][root][INFO] - Training Epoch: 1/2, step 13950/23838 completed (loss: 0.05444766953587532, acc: 0.9873417615890503)
[2025-02-16 13:40:50,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:50,364][root][INFO] - Training Epoch: 1/2, step 13951/23838 completed (loss: 0.17490456998348236, acc: 0.9583333134651184)
[2025-02-16 13:40:50,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:50,773][root][INFO] - Training Epoch: 1/2, step 13952/23838 completed (loss: 0.1369326263666153, acc: 0.9621211886405945)
[2025-02-16 13:40:50,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:51,204][root][INFO] - Training Epoch: 1/2, step 13953/23838 completed (loss: 0.26988357305526733, acc: 0.9139785170555115)
[2025-02-16 13:40:51,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:51,608][root][INFO] - Training Epoch: 1/2, step 13954/23838 completed (loss: 0.2867671251296997, acc: 0.9239130616188049)
[2025-02-16 13:40:51,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:51,991][root][INFO] - Training Epoch: 1/2, step 13955/23838 completed (loss: 0.16313588619232178, acc: 0.9404761791229248)
[2025-02-16 13:40:52,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:52,453][root][INFO] - Training Epoch: 1/2, step 13956/23838 completed (loss: 0.298740416765213, acc: 0.9150943160057068)
[2025-02-16 13:40:52,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:52,827][root][INFO] - Training Epoch: 1/2, step 13957/23838 completed (loss: 0.09637103229761124, acc: 0.9767441749572754)
[2025-02-16 13:40:53,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:53,223][root][INFO] - Training Epoch: 1/2, step 13958/23838 completed (loss: 0.20261549949645996, acc: 0.9449541568756104)
[2025-02-16 13:40:53,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:53,706][root][INFO] - Training Epoch: 1/2, step 13959/23838 completed (loss: 0.06803374737501144, acc: 0.9795918464660645)
[2025-02-16 13:40:53,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:54,179][root][INFO] - Training Epoch: 1/2, step 13960/23838 completed (loss: 1.0361840724945068, acc: 0.7105262875556946)
[2025-02-16 13:40:54,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:54,603][root][INFO] - Training Epoch: 1/2, step 13961/23838 completed (loss: 0.9671435356140137, acc: 0.8157894611358643)
[2025-02-16 13:40:54,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:55,062][root][INFO] - Training Epoch: 1/2, step 13962/23838 completed (loss: 0.2948220670223236, acc: 0.9047619104385376)
[2025-02-16 13:40:55,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:55,500][root][INFO] - Training Epoch: 1/2, step 13963/23838 completed (loss: 0.2643457353115082, acc: 0.9259259104728699)
[2025-02-16 13:40:55,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:55,969][root][INFO] - Training Epoch: 1/2, step 13964/23838 completed (loss: 0.11250646412372589, acc: 0.9646017551422119)
[2025-02-16 13:40:56,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:56,402][root][INFO] - Training Epoch: 1/2, step 13965/23838 completed (loss: 0.5561662912368774, acc: 0.8474576473236084)
[2025-02-16 13:40:56,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:56,830][root][INFO] - Training Epoch: 1/2, step 13966/23838 completed (loss: 0.11240562796592712, acc: 0.9784172773361206)
[2025-02-16 13:40:57,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:57,294][root][INFO] - Training Epoch: 1/2, step 13967/23838 completed (loss: 0.2717161178588867, acc: 0.9189189076423645)
[2025-02-16 13:40:57,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:57,754][root][INFO] - Training Epoch: 1/2, step 13968/23838 completed (loss: 0.10921086370944977, acc: 0.9666666388511658)
[2025-02-16 13:40:57,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:58,162][root][INFO] - Training Epoch: 1/2, step 13969/23838 completed (loss: 0.41838282346725464, acc: 0.8902438879013062)
[2025-02-16 13:40:58,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:58,634][root][INFO] - Training Epoch: 1/2, step 13970/23838 completed (loss: 0.4418112337589264, acc: 0.8777777552604675)
[2025-02-16 13:40:58,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:59,025][root][INFO] - Training Epoch: 1/2, step 13971/23838 completed (loss: 0.17743870615959167, acc: 0.925000011920929)
[2025-02-16 13:40:59,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:59,456][root][INFO] - Training Epoch: 1/2, step 13972/23838 completed (loss: 0.2618517279624939, acc: 0.9300000071525574)
[2025-02-16 13:40:59,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:40:59,821][root][INFO] - Training Epoch: 1/2, step 13973/23838 completed (loss: 0.08547289669513702, acc: 0.9805825352668762)
[2025-02-16 13:41:00,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:00,272][root][INFO] - Training Epoch: 1/2, step 13974/23838 completed (loss: 0.18007446825504303, acc: 0.9602649211883545)
[2025-02-16 13:41:00,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:00,772][root][INFO] - Training Epoch: 1/2, step 13975/23838 completed (loss: 0.13186660408973694, acc: 0.9668874144554138)
[2025-02-16 13:41:00,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:01,176][root][INFO] - Training Epoch: 1/2, step 13976/23838 completed (loss: 0.11865510046482086, acc: 0.95652174949646)
[2025-02-16 13:41:01,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:01,633][root][INFO] - Training Epoch: 1/2, step 13977/23838 completed (loss: 0.20948246121406555, acc: 0.8999999761581421)
[2025-02-16 13:41:01,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:02,041][root][INFO] - Training Epoch: 1/2, step 13978/23838 completed (loss: 0.2226840704679489, acc: 0.9747899174690247)
[2025-02-16 13:41:02,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:02,470][root][INFO] - Training Epoch: 1/2, step 13979/23838 completed (loss: 0.14221665263175964, acc: 0.9735099077224731)
[2025-02-16 13:41:02,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:02,899][root][INFO] - Training Epoch: 1/2, step 13980/23838 completed (loss: 0.0959659144282341, acc: 0.9683544039726257)
[2025-02-16 13:41:03,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:03,367][root][INFO] - Training Epoch: 1/2, step 13981/23838 completed (loss: 0.06095002591609955, acc: 0.9831932783126831)
[2025-02-16 13:41:03,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:03,810][root][INFO] - Training Epoch: 1/2, step 13982/23838 completed (loss: 0.14143988490104675, acc: 0.9489796161651611)
[2025-02-16 13:41:04,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:04,251][root][INFO] - Training Epoch: 1/2, step 13983/23838 completed (loss: 0.13846878707408905, acc: 0.9514563083648682)
[2025-02-16 13:41:04,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:04,611][root][INFO] - Training Epoch: 1/2, step 13984/23838 completed (loss: 0.18555448949337006, acc: 0.9583333134651184)
[2025-02-16 13:41:04,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:05,234][root][INFO] - Training Epoch: 1/2, step 13985/23838 completed (loss: 0.16796192526817322, acc: 0.9628252983093262)
[2025-02-16 13:41:05,430][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:05,661][root][INFO] - Training Epoch: 1/2, step 13986/23838 completed (loss: 0.05759581923484802, acc: 0.9805825352668762)
[2025-02-16 13:41:05,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:06,075][root][INFO] - Training Epoch: 1/2, step 13987/23838 completed (loss: 0.20688924193382263, acc: 0.9484536051750183)
[2025-02-16 13:41:06,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:06,445][root][INFO] - Training Epoch: 1/2, step 13988/23838 completed (loss: 0.0809856429696083, acc: 0.9801324605941772)
[2025-02-16 13:41:06,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:06,912][root][INFO] - Training Epoch: 1/2, step 13989/23838 completed (loss: 0.19887271523475647, acc: 0.9357798099517822)
[2025-02-16 13:41:07,079][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:07,310][root][INFO] - Training Epoch: 1/2, step 13990/23838 completed (loss: 0.140848308801651, acc: 0.9838709831237793)
[2025-02-16 13:41:07,482][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:07,704][root][INFO] - Training Epoch: 1/2, step 13991/23838 completed (loss: 0.06841369718313217, acc: 0.9696969985961914)
[2025-02-16 13:41:07,848][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:08,063][root][INFO] - Training Epoch: 1/2, step 13992/23838 completed (loss: 0.208334818482399, acc: 0.9130434989929199)
[2025-02-16 13:41:08,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:08,484][root][INFO] - Training Epoch: 1/2, step 13993/23838 completed (loss: 0.12943315505981445, acc: 0.9491525292396545)
[2025-02-16 13:41:08,691][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:08,912][root][INFO] - Training Epoch: 1/2, step 13994/23838 completed (loss: 0.3676634430885315, acc: 0.8333333134651184)
[2025-02-16 13:41:09,132][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:09,352][root][INFO] - Training Epoch: 1/2, step 13995/23838 completed (loss: 0.22943885624408722, acc: 0.9200000166893005)
[2025-02-16 13:41:09,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:09,722][root][INFO] - Training Epoch: 1/2, step 13996/23838 completed (loss: 0.31267887353897095, acc: 0.93388432264328)
[2025-02-16 13:41:09,949][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:10,175][root][INFO] - Training Epoch: 1/2, step 13997/23838 completed (loss: 0.21474219858646393, acc: 0.9642857313156128)
[2025-02-16 13:41:10,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:10,630][root][INFO] - Training Epoch: 1/2, step 13998/23838 completed (loss: 0.16014084219932556, acc: 0.9736841917037964)
[2025-02-16 13:41:10,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:11,049][root][INFO] - Training Epoch: 1/2, step 13999/23838 completed (loss: 0.4832300841808319, acc: 0.8266666531562805)
[2025-02-16 13:41:11,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:11,441][root][INFO] - Training Epoch: 1/2, step 14000/23838 completed (loss: 0.1745385080575943, acc: 0.9319728016853333)
[2025-02-16 13:41:11,690][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:11,929][root][INFO] - Training Epoch: 1/2, step 14001/23838 completed (loss: 0.19485346972942352, acc: 0.9380530714988708)
[2025-02-16 13:41:12,093][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:12,326][root][INFO] - Training Epoch: 1/2, step 14002/23838 completed (loss: 0.6399182081222534, acc: 0.8085106611251831)
[2025-02-16 13:41:12,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:12,729][root][INFO] - Training Epoch: 1/2, step 14003/23838 completed (loss: 0.36303454637527466, acc: 0.9054054021835327)
[2025-02-16 13:41:12,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:13,261][root][INFO] - Training Epoch: 1/2, step 14004/23838 completed (loss: 0.30716803669929504, acc: 0.9060773253440857)
[2025-02-16 13:41:13,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:13,655][root][INFO] - Training Epoch: 1/2, step 14005/23838 completed (loss: 0.364165335893631, acc: 0.8888888955116272)
[2025-02-16 13:41:13,922][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:14,204][root][INFO] - Training Epoch: 1/2, step 14006/23838 completed (loss: 0.28676536679267883, acc: 0.9399999976158142)
[2025-02-16 13:41:14,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:14,639][root][INFO] - Training Epoch: 1/2, step 14007/23838 completed (loss: 0.15537559986114502, acc: 0.9587628841400146)
[2025-02-16 13:41:14,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:15,023][root][INFO] - Training Epoch: 1/2, step 14008/23838 completed (loss: 0.21429279446601868, acc: 0.9459459185600281)
[2025-02-16 13:41:15,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:15,385][root][INFO] - Training Epoch: 1/2, step 14009/23838 completed (loss: 0.15113666653633118, acc: 0.9444444179534912)
[2025-02-16 13:41:15,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:15,782][root][INFO] - Training Epoch: 1/2, step 14010/23838 completed (loss: 0.29394540190696716, acc: 0.90625)
[2025-02-16 13:41:15,989][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:16,230][root][INFO] - Training Epoch: 1/2, step 14011/23838 completed (loss: 0.20799410343170166, acc: 0.9572649598121643)
[2025-02-16 13:41:16,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:16,703][root][INFO] - Training Epoch: 1/2, step 14012/23838 completed (loss: 0.3095794916152954, acc: 0.8876404762268066)
[2025-02-16 13:41:16,916][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:17,148][root][INFO] - Training Epoch: 1/2, step 14013/23838 completed (loss: 0.32392314076423645, acc: 0.9009901285171509)
[2025-02-16 13:41:17,344][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:17,559][root][INFO] - Training Epoch: 1/2, step 14014/23838 completed (loss: 0.7781867384910583, acc: 0.7777777910232544)
[2025-02-16 13:41:17,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:17,921][root][INFO] - Training Epoch: 1/2, step 14015/23838 completed (loss: 0.13649940490722656, acc: 0.9666666388511658)
[2025-02-16 13:41:18,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:18,293][root][INFO] - Training Epoch: 1/2, step 14016/23838 completed (loss: 0.15506313741207123, acc: 0.9462365508079529)
[2025-02-16 13:41:18,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:18,718][root][INFO] - Training Epoch: 1/2, step 14017/23838 completed (loss: 0.24159374833106995, acc: 0.9058823585510254)
[2025-02-16 13:41:18,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:19,161][root][INFO] - Training Epoch: 1/2, step 14018/23838 completed (loss: 0.07519161701202393, acc: 0.9821428656578064)
[2025-02-16 13:41:19,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:19,580][root][INFO] - Training Epoch: 1/2, step 14019/23838 completed (loss: 0.1253417283296585, acc: 0.9519650936126709)
[2025-02-16 13:41:19,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:20,023][root][INFO] - Training Epoch: 1/2, step 14020/23838 completed (loss: 0.41105949878692627, acc: 0.8939393758773804)
[2025-02-16 13:41:20,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:20,431][root][INFO] - Training Epoch: 1/2, step 14021/23838 completed (loss: 0.2835121750831604, acc: 0.936170220375061)
[2025-02-16 13:41:20,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:20,807][root][INFO] - Training Epoch: 1/2, step 14022/23838 completed (loss: 0.25754809379577637, acc: 0.9425287246704102)
[2025-02-16 13:41:20,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:21,213][root][INFO] - Training Epoch: 1/2, step 14023/23838 completed (loss: 0.3493201732635498, acc: 0.9078947305679321)
[2025-02-16 13:41:21,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:21,641][root][INFO] - Training Epoch: 1/2, step 14024/23838 completed (loss: 0.1918114870786667, acc: 0.9425287246704102)
[2025-02-16 13:41:21,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:22,091][root][INFO] - Training Epoch: 1/2, step 14025/23838 completed (loss: 0.07990790158510208, acc: 0.9920634627342224)
[2025-02-16 13:41:22,313][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:22,543][root][INFO] - Training Epoch: 1/2, step 14026/23838 completed (loss: 0.31408727169036865, acc: 0.9139785170555115)
[2025-02-16 13:41:22,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:22,988][root][INFO] - Training Epoch: 1/2, step 14027/23838 completed (loss: 0.20543858408927917, acc: 0.934959352016449)
[2025-02-16 13:41:23,174][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:23,430][root][INFO] - Training Epoch: 1/2, step 14028/23838 completed (loss: 0.46629849076271057, acc: 0.8421052694320679)
[2025-02-16 13:41:23,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:23,843][root][INFO] - Training Epoch: 1/2, step 14029/23838 completed (loss: 0.23500487208366394, acc: 0.9220778942108154)
[2025-02-16 13:41:24,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:24,277][root][INFO] - Training Epoch: 1/2, step 14030/23838 completed (loss: 0.290376216173172, acc: 0.9127516746520996)
[2025-02-16 13:41:24,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:24,679][root][INFO] - Training Epoch: 1/2, step 14031/23838 completed (loss: 0.32871320843696594, acc: 0.9433962106704712)
[2025-02-16 13:41:24,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:25,112][root][INFO] - Training Epoch: 1/2, step 14032/23838 completed (loss: 0.18874819576740265, acc: 0.971222996711731)
[2025-02-16 13:41:25,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:25,553][root][INFO] - Training Epoch: 1/2, step 14033/23838 completed (loss: 0.07952312380075455, acc: 0.9834710955619812)
[2025-02-16 13:41:25,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:26,012][root][INFO] - Training Epoch: 1/2, step 14034/23838 completed (loss: 0.08606993407011032, acc: 0.9492753744125366)
[2025-02-16 13:41:26,230][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:26,416][root][INFO] - Training Epoch: 1/2, step 14035/23838 completed (loss: 0.5093159675598145, acc: 0.8225806355476379)
[2025-02-16 13:41:26,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:26,913][root][INFO] - Training Epoch: 1/2, step 14036/23838 completed (loss: 0.22367247939109802, acc: 0.9529411792755127)
[2025-02-16 13:41:27,140][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:27,382][root][INFO] - Training Epoch: 1/2, step 14037/23838 completed (loss: 0.14420099556446075, acc: 0.957317054271698)
[2025-02-16 13:41:27,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:27,924][root][INFO] - Training Epoch: 1/2, step 14038/23838 completed (loss: 0.12196188420057297, acc: 0.9572649598121643)
[2025-02-16 13:41:28,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:28,410][root][INFO] - Training Epoch: 1/2, step 14039/23838 completed (loss: 0.04886648803949356, acc: 0.98591548204422)
[2025-02-16 13:41:28,598][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:28,836][root][INFO] - Training Epoch: 1/2, step 14040/23838 completed (loss: 0.43014809489250183, acc: 0.8947368264198303)
[2025-02-16 13:41:29,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:29,348][root][INFO] - Training Epoch: 1/2, step 14041/23838 completed (loss: 0.1298290491104126, acc: 0.9777777791023254)
[2025-02-16 13:41:29,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:29,786][root][INFO] - Training Epoch: 1/2, step 14042/23838 completed (loss: 0.3058432638645172, acc: 0.9126983880996704)
[2025-02-16 13:41:30,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:30,256][root][INFO] - Training Epoch: 1/2, step 14043/23838 completed (loss: 0.3088899254798889, acc: 0.921875)
[2025-02-16 13:41:30,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:30,658][root][INFO] - Training Epoch: 1/2, step 14044/23838 completed (loss: 0.2702542841434479, acc: 0.8971962332725525)
[2025-02-16 13:41:30,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:31,107][root][INFO] - Training Epoch: 1/2, step 14045/23838 completed (loss: 0.2237974852323532, acc: 0.9378882050514221)
[2025-02-16 13:41:31,293][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:31,491][root][INFO] - Training Epoch: 1/2, step 14046/23838 completed (loss: 0.4251568615436554, acc: 0.8942307829856873)
[2025-02-16 13:41:31,648][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:31,908][root][INFO] - Training Epoch: 1/2, step 14047/23838 completed (loss: 0.34551504254341125, acc: 0.875)
[2025-02-16 13:41:32,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:32,303][root][INFO] - Training Epoch: 1/2, step 14048/23838 completed (loss: 0.5819966793060303, acc: 0.8333333134651184)
[2025-02-16 13:41:32,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:32,763][root][INFO] - Training Epoch: 1/2, step 14049/23838 completed (loss: 0.2116442769765854, acc: 0.939393937587738)
[2025-02-16 13:41:33,014][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:33,256][root][INFO] - Training Epoch: 1/2, step 14050/23838 completed (loss: 0.21170523762702942, acc: 0.9208333492279053)
[2025-02-16 13:41:33,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:33,692][root][INFO] - Training Epoch: 1/2, step 14051/23838 completed (loss: 0.34322333335876465, acc: 0.9216867685317993)
[2025-02-16 13:41:33,892][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:34,142][root][INFO] - Training Epoch: 1/2, step 14052/23838 completed (loss: 0.10323397070169449, acc: 0.9803921580314636)
[2025-02-16 13:41:34,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:34,542][root][INFO] - Training Epoch: 1/2, step 14053/23838 completed (loss: 0.11289216578006744, acc: 0.9610389471054077)
[2025-02-16 13:41:34,768][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:35,005][root][INFO] - Training Epoch: 1/2, step 14054/23838 completed (loss: 0.4123091399669647, acc: 0.9057971239089966)
[2025-02-16 13:41:35,227][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:35,453][root][INFO] - Training Epoch: 1/2, step 14055/23838 completed (loss: 0.3281271159648895, acc: 0.9230769276618958)
[2025-02-16 13:41:35,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:35,910][root][INFO] - Training Epoch: 1/2, step 14056/23838 completed (loss: 0.18322989344596863, acc: 0.9390243887901306)
[2025-02-16 13:41:36,164][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:36,395][root][INFO] - Training Epoch: 1/2, step 14057/23838 completed (loss: 0.3705604076385498, acc: 0.8999999761581421)
[2025-02-16 13:41:36,556][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:36,788][root][INFO] - Training Epoch: 1/2, step 14058/23838 completed (loss: 0.06717873364686966, acc: 0.9894737005233765)
[2025-02-16 13:41:36,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:37,221][root][INFO] - Training Epoch: 1/2, step 14059/23838 completed (loss: 0.2713567018508911, acc: 0.9104477763175964)
[2025-02-16 13:41:37,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:37,660][root][INFO] - Training Epoch: 1/2, step 14060/23838 completed (loss: 0.21617113053798676, acc: 0.954023003578186)
[2025-02-16 13:41:37,872][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:38,108][root][INFO] - Training Epoch: 1/2, step 14061/23838 completed (loss: 0.5812067985534668, acc: 0.8589743375778198)
[2025-02-16 13:41:38,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:38,519][root][INFO] - Training Epoch: 1/2, step 14062/23838 completed (loss: 0.3301003873348236, acc: 0.9342105388641357)
[2025-02-16 13:41:38,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:38,942][root][INFO] - Training Epoch: 1/2, step 14063/23838 completed (loss: 0.2921288311481476, acc: 0.9215686321258545)
[2025-02-16 13:41:39,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:39,389][root][INFO] - Training Epoch: 1/2, step 14064/23838 completed (loss: 0.11854244023561478, acc: 0.9586777091026306)
[2025-02-16 13:41:39,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:39,874][root][INFO] - Training Epoch: 1/2, step 14065/23838 completed (loss: 0.25894737243652344, acc: 0.9230769276618958)
[2025-02-16 13:41:40,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:40,354][root][INFO] - Training Epoch: 1/2, step 14066/23838 completed (loss: 0.3607221841812134, acc: 0.9101123809814453)
[2025-02-16 13:41:40,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:40,799][root][INFO] - Training Epoch: 1/2, step 14067/23838 completed (loss: 0.35279229283332825, acc: 0.9130434989929199)
[2025-02-16 13:41:40,981][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:41,216][root][INFO] - Training Epoch: 1/2, step 14068/23838 completed (loss: 0.20220215618610382, acc: 0.9210526347160339)
[2025-02-16 13:41:41,424][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:41,673][root][INFO] - Training Epoch: 1/2, step 14069/23838 completed (loss: 0.27003350853919983, acc: 0.9398496150970459)
[2025-02-16 13:41:41,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:42,068][root][INFO] - Training Epoch: 1/2, step 14070/23838 completed (loss: 0.16407398879528046, acc: 0.9430894255638123)
[2025-02-16 13:41:42,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:42,450][root][INFO] - Training Epoch: 1/2, step 14071/23838 completed (loss: 0.26704439520835876, acc: 0.925000011920929)
[2025-02-16 13:41:42,629][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:42,863][root][INFO] - Training Epoch: 1/2, step 14072/23838 completed (loss: 0.22189950942993164, acc: 0.9452054500579834)
[2025-02-16 13:41:43,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:43,335][root][INFO] - Training Epoch: 1/2, step 14073/23838 completed (loss: 0.11047367006540298, acc: 0.9671052694320679)
[2025-02-16 13:41:43,582][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:43,829][root][INFO] - Training Epoch: 1/2, step 14074/23838 completed (loss: 0.08963216841220856, acc: 0.9732620120048523)
[2025-02-16 13:41:44,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:44,281][root][INFO] - Training Epoch: 1/2, step 14075/23838 completed (loss: 0.2598128318786621, acc: 0.9238095283508301)
[2025-02-16 13:41:44,502][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:44,752][root][INFO] - Training Epoch: 1/2, step 14076/23838 completed (loss: 0.123220294713974, acc: 0.9736841917037964)
[2025-02-16 13:41:44,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:45,206][root][INFO] - Training Epoch: 1/2, step 14077/23838 completed (loss: 0.1373857855796814, acc: 0.953125)
[2025-02-16 13:41:45,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:45,632][root][INFO] - Training Epoch: 1/2, step 14078/23838 completed (loss: 0.25451016426086426, acc: 0.9390243887901306)
[2025-02-16 13:41:45,819][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:46,035][root][INFO] - Training Epoch: 1/2, step 14079/23838 completed (loss: 0.45817285776138306, acc: 0.8709677457809448)
[2025-02-16 13:41:46,234][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:46,469][root][INFO] - Training Epoch: 1/2, step 14080/23838 completed (loss: 0.35336974263191223, acc: 0.9100000262260437)
[2025-02-16 13:41:46,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:46,939][root][INFO] - Training Epoch: 1/2, step 14081/23838 completed (loss: 0.5505774021148682, acc: 0.8125)
[2025-02-16 13:41:47,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:47,337][root][INFO] - Training Epoch: 1/2, step 14082/23838 completed (loss: 0.4713670015335083, acc: 0.8712871074676514)
[2025-02-16 13:41:47,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:47,858][root][INFO] - Training Epoch: 1/2, step 14083/23838 completed (loss: 0.18341456353664398, acc: 0.9642857313156128)
[2025-02-16 13:41:48,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:48,334][root][INFO] - Training Epoch: 1/2, step 14084/23838 completed (loss: 0.20255254209041595, acc: 0.9529411792755127)
[2025-02-16 13:41:48,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:48,725][root][INFO] - Training Epoch: 1/2, step 14085/23838 completed (loss: 0.2479621320962906, acc: 0.898876428604126)
[2025-02-16 13:41:48,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:49,104][root][INFO] - Training Epoch: 1/2, step 14086/23838 completed (loss: 0.1925751119852066, acc: 0.9354838728904724)
[2025-02-16 13:41:49,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:49,542][root][INFO] - Training Epoch: 1/2, step 14087/23838 completed (loss: 0.04042584076523781, acc: 0.98591548204422)
[2025-02-16 13:41:49,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:49,974][root][INFO] - Training Epoch: 1/2, step 14088/23838 completed (loss: 0.503741979598999, acc: 0.8764045238494873)
[2025-02-16 13:41:50,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:50,423][root][INFO] - Training Epoch: 1/2, step 14089/23838 completed (loss: 0.17652536928653717, acc: 0.9378238320350647)
[2025-02-16 13:41:50,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:50,928][root][INFO] - Training Epoch: 1/2, step 14090/23838 completed (loss: 0.2060578167438507, acc: 0.9534883499145508)
[2025-02-16 13:41:51,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:51,317][root][INFO] - Training Epoch: 1/2, step 14091/23838 completed (loss: 0.2763458490371704, acc: 0.932584285736084)
[2025-02-16 13:41:51,476][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:51,708][root][INFO] - Training Epoch: 1/2, step 14092/23838 completed (loss: 0.12992849946022034, acc: 0.96875)
[2025-02-16 13:41:51,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:52,139][root][INFO] - Training Epoch: 1/2, step 14093/23838 completed (loss: 0.4274635314941406, acc: 0.8854166865348816)
[2025-02-16 13:41:52,332][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:52,611][root][INFO] - Training Epoch: 1/2, step 14094/23838 completed (loss: 0.39136943221092224, acc: 0.8790322542190552)
[2025-02-16 13:41:52,816][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:53,045][root][INFO] - Training Epoch: 1/2, step 14095/23838 completed (loss: 0.198771134018898, acc: 0.9583333134651184)
[2025-02-16 13:41:53,269][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:53,500][root][INFO] - Training Epoch: 1/2, step 14096/23838 completed (loss: 0.3104672133922577, acc: 0.9047619104385376)
[2025-02-16 13:41:53,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:53,942][root][INFO] - Training Epoch: 1/2, step 14097/23838 completed (loss: 0.25347021222114563, acc: 0.9237288236618042)
[2025-02-16 13:41:54,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:54,374][root][INFO] - Training Epoch: 1/2, step 14098/23838 completed (loss: 0.37660980224609375, acc: 0.9090909361839294)
[2025-02-16 13:41:54,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:54,801][root][INFO] - Training Epoch: 1/2, step 14099/23838 completed (loss: 0.5511578917503357, acc: 0.8641975522041321)
[2025-02-16 13:41:55,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:55,254][root][INFO] - Training Epoch: 1/2, step 14100/23838 completed (loss: 0.1448047012090683, acc: 0.9632353186607361)
[2025-02-16 13:41:55,419][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:55,683][root][INFO] - Training Epoch: 1/2, step 14101/23838 completed (loss: 0.22304190695285797, acc: 0.9358974099159241)
[2025-02-16 13:41:55,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:56,106][root][INFO] - Training Epoch: 1/2, step 14102/23838 completed (loss: 0.10366925597190857, acc: 0.9695122241973877)
[2025-02-16 13:41:56,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:56,529][root][INFO] - Training Epoch: 1/2, step 14103/23838 completed (loss: 0.20215803384780884, acc: 0.9157894849777222)
[2025-02-16 13:41:56,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:56,988][root][INFO] - Training Epoch: 1/2, step 14104/23838 completed (loss: 0.1452503353357315, acc: 0.95652174949646)
[2025-02-16 13:41:57,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:57,481][root][INFO] - Training Epoch: 1/2, step 14105/23838 completed (loss: 0.04241225868463516, acc: 0.9887640476226807)
[2025-02-16 13:41:57,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:57,926][root][INFO] - Training Epoch: 1/2, step 14106/23838 completed (loss: 0.06338275223970413, acc: 0.985401451587677)
[2025-02-16 13:41:58,107][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:58,349][root][INFO] - Training Epoch: 1/2, step 14107/23838 completed (loss: 0.05878940224647522, acc: 0.9894737005233765)
[2025-02-16 13:41:58,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:58,769][root][INFO] - Training Epoch: 1/2, step 14108/23838 completed (loss: 0.23443154990673065, acc: 0.9239130616188049)
[2025-02-16 13:41:58,910][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:59,131][root][INFO] - Training Epoch: 1/2, step 14109/23838 completed (loss: 0.07074343413114548, acc: 0.9876543283462524)
[2025-02-16 13:41:59,356][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:41:59,586][root][INFO] - Training Epoch: 1/2, step 14110/23838 completed (loss: 0.2900502383708954, acc: 0.9320388436317444)
[2025-02-16 13:41:59,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:00,055][root][INFO] - Training Epoch: 1/2, step 14111/23838 completed (loss: 0.2317751795053482, acc: 0.942307710647583)
[2025-02-16 13:42:00,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:00,541][root][INFO] - Training Epoch: 1/2, step 14112/23838 completed (loss: 0.3890194892883301, acc: 0.9306930899620056)
[2025-02-16 13:42:00,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:00,944][root][INFO] - Training Epoch: 1/2, step 14113/23838 completed (loss: 0.21939226984977722, acc: 0.940119743347168)
[2025-02-16 13:42:01,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:01,362][root][INFO] - Training Epoch: 1/2, step 14114/23838 completed (loss: 0.05165291577577591, acc: 0.9850746393203735)
[2025-02-16 13:42:01,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:01,772][root][INFO] - Training Epoch: 1/2, step 14115/23838 completed (loss: 0.24726243317127228, acc: 0.9299362897872925)
[2025-02-16 13:42:02,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:02,388][root][INFO] - Training Epoch: 1/2, step 14116/23838 completed (loss: 0.41444602608680725, acc: 0.8809523582458496)
[2025-02-16 13:42:02,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:02,975][root][INFO] - Training Epoch: 1/2, step 14117/23838 completed (loss: 0.36248600482940674, acc: 0.9034482836723328)
[2025-02-16 13:42:03,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:03,853][root][INFO] - Training Epoch: 1/2, step 14118/23838 completed (loss: 0.4288722574710846, acc: 0.8791208863258362)
[2025-02-16 13:42:04,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:04,513][root][INFO] - Training Epoch: 1/2, step 14119/23838 completed (loss: 0.24188126623630524, acc: 0.9308176040649414)
[2025-02-16 13:42:04,730][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:04,975][root][INFO] - Training Epoch: 1/2, step 14120/23838 completed (loss: 0.1598667949438095, acc: 0.9263157844543457)
[2025-02-16 13:42:05,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:05,474][root][INFO] - Training Epoch: 1/2, step 14121/23838 completed (loss: 0.1753561645746231, acc: 0.9236640930175781)
[2025-02-16 13:42:05,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:05,951][root][INFO] - Training Epoch: 1/2, step 14122/23838 completed (loss: 0.22316057980060577, acc: 0.9384615421295166)
[2025-02-16 13:42:06,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:06,491][root][INFO] - Training Epoch: 1/2, step 14123/23838 completed (loss: 0.20182448625564575, acc: 0.9523809552192688)
[2025-02-16 13:42:06,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:06,902][root][INFO] - Training Epoch: 1/2, step 14124/23838 completed (loss: 0.4234143793582916, acc: 0.835616409778595)
[2025-02-16 13:42:07,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:07,509][root][INFO] - Training Epoch: 1/2, step 14125/23838 completed (loss: 0.3079543709754944, acc: 0.9107142686843872)
[2025-02-16 13:42:07,771][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:08,054][root][INFO] - Training Epoch: 1/2, step 14126/23838 completed (loss: 0.2715432345867157, acc: 0.9196428656578064)
[2025-02-16 13:42:08,294][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:08,560][root][INFO] - Training Epoch: 1/2, step 14127/23838 completed (loss: 0.3088950514793396, acc: 0.9015151262283325)
[2025-02-16 13:42:08,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:09,042][root][INFO] - Training Epoch: 1/2, step 14128/23838 completed (loss: 0.25917619466781616, acc: 0.9344262480735779)
[2025-02-16 13:42:09,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:09,656][root][INFO] - Training Epoch: 1/2, step 14129/23838 completed (loss: 0.11157121509313583, acc: 0.9696969985961914)
[2025-02-16 13:42:09,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:10,125][root][INFO] - Training Epoch: 1/2, step 14130/23838 completed (loss: 0.27181369066238403, acc: 0.9333333373069763)
[2025-02-16 13:42:10,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:10,690][root][INFO] - Training Epoch: 1/2, step 14131/23838 completed (loss: 0.13878554105758667, acc: 0.9467213153839111)
[2025-02-16 13:42:10,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:11,094][root][INFO] - Training Epoch: 1/2, step 14132/23838 completed (loss: 0.1071551963686943, acc: 0.9659090638160706)
[2025-02-16 13:42:11,571][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:12,002][root][INFO] - Training Epoch: 1/2, step 14133/23838 completed (loss: 0.24742011725902557, acc: 0.9380530714988708)
[2025-02-16 13:42:12,141][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:12,347][root][INFO] - Training Epoch: 1/2, step 14134/23838 completed (loss: 0.3371627926826477, acc: 0.9047619104385376)
[2025-02-16 13:42:12,530][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:12,748][root][INFO] - Training Epoch: 1/2, step 14135/23838 completed (loss: 0.5061743259429932, acc: 0.8399999737739563)
[2025-02-16 13:42:12,961][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:13,194][root][INFO] - Training Epoch: 1/2, step 14136/23838 completed (loss: 0.22201302647590637, acc: 0.9411764740943909)
[2025-02-16 13:42:13,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:13,683][root][INFO] - Training Epoch: 1/2, step 14137/23838 completed (loss: 0.211725652217865, acc: 0.9545454382896423)
[2025-02-16 13:42:13,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:14,117][root][INFO] - Training Epoch: 1/2, step 14138/23838 completed (loss: 0.44441038370132446, acc: 0.8980891704559326)
[2025-02-16 13:42:14,322][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:14,577][root][INFO] - Training Epoch: 1/2, step 14139/23838 completed (loss: 0.20259453356266022, acc: 0.9504950642585754)
[2025-02-16 13:42:14,820][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:15,045][root][INFO] - Training Epoch: 1/2, step 14140/23838 completed (loss: 0.19329248368740082, acc: 0.942105233669281)
[2025-02-16 13:42:15,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:15,511][root][INFO] - Training Epoch: 1/2, step 14141/23838 completed (loss: 0.3175831437110901, acc: 0.8898305296897888)
[2025-02-16 13:42:15,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:16,132][root][INFO] - Training Epoch: 1/2, step 14142/23838 completed (loss: 0.24023233354091644, acc: 0.945147693157196)
[2025-02-16 13:42:16,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:16,500][root][INFO] - Training Epoch: 1/2, step 14143/23838 completed (loss: 0.11556948721408844, acc: 0.9700000286102295)
[2025-02-16 13:42:16,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:17,145][root][INFO] - Training Epoch: 1/2, step 14144/23838 completed (loss: 0.39984196424484253, acc: 0.904411792755127)
[2025-02-16 13:42:17,354][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:17,597][root][INFO] - Training Epoch: 1/2, step 14145/23838 completed (loss: 0.2674589157104492, acc: 0.9312977194786072)
[2025-02-16 13:42:17,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:18,070][root][INFO] - Training Epoch: 1/2, step 14146/23838 completed (loss: 0.2410936951637268, acc: 0.9200000166893005)
[2025-02-16 13:42:18,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:18,534][root][INFO] - Training Epoch: 1/2, step 14147/23838 completed (loss: 0.15239457786083221, acc: 0.9629629850387573)
[2025-02-16 13:42:18,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:18,984][root][INFO] - Training Epoch: 1/2, step 14148/23838 completed (loss: 0.11192038655281067, acc: 0.9411764740943909)
[2025-02-16 13:42:19,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:19,493][root][INFO] - Training Epoch: 1/2, step 14149/23838 completed (loss: 0.1175151839852333, acc: 0.9557521939277649)
[2025-02-16 13:42:19,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:19,911][root][INFO] - Training Epoch: 1/2, step 14150/23838 completed (loss: 0.13471311330795288, acc: 0.9638554453849792)
[2025-02-16 13:42:20,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:20,392][root][INFO] - Training Epoch: 1/2, step 14151/23838 completed (loss: 0.14444662630558014, acc: 0.954285740852356)
[2025-02-16 13:42:20,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:20,831][root][INFO] - Training Epoch: 1/2, step 14152/23838 completed (loss: 0.24631468951702118, acc: 0.932330846786499)
[2025-02-16 13:42:21,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:21,385][root][INFO] - Training Epoch: 1/2, step 14153/23838 completed (loss: 0.18326374888420105, acc: 0.9507042169570923)
[2025-02-16 13:42:21,693][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:22,036][root][INFO] - Training Epoch: 1/2, step 14154/23838 completed (loss: 0.10939545929431915, acc: 0.9711934328079224)
[2025-02-16 13:42:22,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:22,495][root][INFO] - Training Epoch: 1/2, step 14155/23838 completed (loss: 0.3387177884578705, acc: 0.9124087691307068)
[2025-02-16 13:42:22,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:23,030][root][INFO] - Training Epoch: 1/2, step 14156/23838 completed (loss: 0.30425524711608887, acc: 0.90055251121521)
[2025-02-16 13:42:23,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:23,665][root][INFO] - Training Epoch: 1/2, step 14157/23838 completed (loss: 0.17865866422653198, acc: 0.9389312863349915)
[2025-02-16 13:42:23,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:24,153][root][INFO] - Training Epoch: 1/2, step 14158/23838 completed (loss: 0.23044230043888092, acc: 0.949999988079071)
[2025-02-16 13:42:24,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:24,594][root][INFO] - Training Epoch: 1/2, step 14159/23838 completed (loss: 0.1863541305065155, acc: 0.9494949579238892)
[2025-02-16 13:42:24,887][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:25,197][root][INFO] - Training Epoch: 1/2, step 14160/23838 completed (loss: 0.2795780599117279, acc: 0.920187771320343)
[2025-02-16 13:42:25,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:25,836][root][INFO] - Training Epoch: 1/2, step 14161/23838 completed (loss: 0.3497416079044342, acc: 0.9115044474601746)
[2025-02-16 13:42:26,006][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:26,252][root][INFO] - Training Epoch: 1/2, step 14162/23838 completed (loss: 0.7573307752609253, acc: 0.791304349899292)
[2025-02-16 13:42:26,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:26,757][root][INFO] - Training Epoch: 1/2, step 14163/23838 completed (loss: 0.2843596935272217, acc: 0.9469696879386902)
[2025-02-16 13:42:27,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:27,312][root][INFO] - Training Epoch: 1/2, step 14164/23838 completed (loss: 0.16047713160514832, acc: 0.9406779408454895)
[2025-02-16 13:42:27,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:27,834][root][INFO] - Training Epoch: 1/2, step 14165/23838 completed (loss: 0.1533365249633789, acc: 0.970588207244873)
[2025-02-16 13:42:28,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:28,339][root][INFO] - Training Epoch: 1/2, step 14166/23838 completed (loss: 0.329205721616745, acc: 0.8947368264198303)
[2025-02-16 13:42:28,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:28,876][root][INFO] - Training Epoch: 1/2, step 14167/23838 completed (loss: 0.3815625011920929, acc: 0.9197860956192017)
[2025-02-16 13:42:29,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:29,348][root][INFO] - Training Epoch: 1/2, step 14168/23838 completed (loss: 0.04284725338220596, acc: 0.9921875)
[2025-02-16 13:42:29,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:29,766][root][INFO] - Training Epoch: 1/2, step 14169/23838 completed (loss: 0.1077563464641571, acc: 0.984000027179718)
[2025-02-16 13:42:29,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:30,182][root][INFO] - Training Epoch: 1/2, step 14170/23838 completed (loss: 0.2011849582195282, acc: 0.9351851940155029)
[2025-02-16 13:42:30,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:30,569][root][INFO] - Training Epoch: 1/2, step 14171/23838 completed (loss: 0.2639341354370117, acc: 0.931034505367279)
[2025-02-16 13:42:30,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:31,119][root][INFO] - Training Epoch: 1/2, step 14172/23838 completed (loss: 0.156036376953125, acc: 0.9561403393745422)
[2025-02-16 13:42:31,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:31,517][root][INFO] - Training Epoch: 1/2, step 14173/23838 completed (loss: 0.8503628969192505, acc: 0.7875000238418579)
[2025-02-16 13:42:31,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:31,980][root][INFO] - Training Epoch: 1/2, step 14174/23838 completed (loss: 0.07327902317047119, acc: 0.9829059839248657)
[2025-02-16 13:42:32,193][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:32,447][root][INFO] - Training Epoch: 1/2, step 14175/23838 completed (loss: 0.9722025394439697, acc: 0.7346938848495483)
[2025-02-16 13:42:32,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:32,935][root][INFO] - Training Epoch: 1/2, step 14176/23838 completed (loss: 0.40868812799453735, acc: 0.8888888955116272)
[2025-02-16 13:42:33,119][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:33,345][root][INFO] - Training Epoch: 1/2, step 14177/23838 completed (loss: 0.09309739619493484, acc: 0.9658119678497314)
[2025-02-16 13:42:33,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:33,795][root][INFO] - Training Epoch: 1/2, step 14178/23838 completed (loss: 0.23183780908584595, acc: 0.9444444179534912)
[2025-02-16 13:42:34,052][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:34,300][root][INFO] - Training Epoch: 1/2, step 14179/23838 completed (loss: 0.3763435482978821, acc: 0.9200000166893005)
[2025-02-16 13:42:34,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:35,104][root][INFO] - Training Epoch: 1/2, step 14180/23838 completed (loss: 0.1594047099351883, acc: 0.9494949579238892)
[2025-02-16 13:42:35,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:35,930][root][INFO] - Training Epoch: 1/2, step 14181/23838 completed (loss: 0.17872273921966553, acc: 0.9714285731315613)
[2025-02-16 13:42:36,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:36,686][root][INFO] - Training Epoch: 1/2, step 14182/23838 completed (loss: 0.30078211426734924, acc: 0.9264705777168274)
[2025-02-16 13:42:37,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:37,521][root][INFO] - Training Epoch: 1/2, step 14183/23838 completed (loss: 0.260098397731781, acc: 0.9285714030265808)
[2025-02-16 13:42:37,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:38,138][root][INFO] - Training Epoch: 1/2, step 14184/23838 completed (loss: 0.26841384172439575, acc: 0.9380530714988708)
[2025-02-16 13:42:38,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:39,121][root][INFO] - Training Epoch: 1/2, step 14185/23838 completed (loss: 0.14314618706703186, acc: 0.9526315927505493)
[2025-02-16 13:42:39,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:39,702][root][INFO] - Training Epoch: 1/2, step 14186/23838 completed (loss: 0.4134621024131775, acc: 0.9008264541625977)
[2025-02-16 13:42:39,918][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:40,161][root][INFO] - Training Epoch: 1/2, step 14187/23838 completed (loss: 0.23008322715759277, acc: 0.9358974099159241)
[2025-02-16 13:42:40,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:40,632][root][INFO] - Training Epoch: 1/2, step 14188/23838 completed (loss: 0.4335167109966278, acc: 0.8723404407501221)
[2025-02-16 13:42:40,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:41,344][root][INFO] - Training Epoch: 1/2, step 14189/23838 completed (loss: 0.17082710564136505, acc: 0.9548386931419373)
[2025-02-16 13:42:41,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:41,858][root][INFO] - Training Epoch: 1/2, step 14190/23838 completed (loss: 0.09998948127031326, acc: 0.976190447807312)
[2025-02-16 13:42:42,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:42,336][root][INFO] - Training Epoch: 1/2, step 14191/23838 completed (loss: 0.034179892390966415, acc: 0.9945945739746094)
[2025-02-16 13:42:42,553][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:42,772][root][INFO] - Training Epoch: 1/2, step 14192/23838 completed (loss: 0.06654568761587143, acc: 0.982758641242981)
[2025-02-16 13:42:42,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:43,158][root][INFO] - Training Epoch: 1/2, step 14193/23838 completed (loss: 0.11502301692962646, acc: 0.9791666865348816)
[2025-02-16 13:42:43,378][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:43,621][root][INFO] - Training Epoch: 1/2, step 14194/23838 completed (loss: 0.2062428593635559, acc: 0.9489051103591919)
[2025-02-16 13:42:43,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:44,036][root][INFO] - Training Epoch: 1/2, step 14195/23838 completed (loss: 0.27676230669021606, acc: 0.931034505367279)
[2025-02-16 13:42:44,221][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:44,449][root][INFO] - Training Epoch: 1/2, step 14196/23838 completed (loss: 0.5569553375244141, acc: 0.8399999737739563)
[2025-02-16 13:42:44,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:44,806][root][INFO] - Training Epoch: 1/2, step 14197/23838 completed (loss: 0.5792486071586609, acc: 0.791208803653717)
[2025-02-16 13:42:45,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:45,309][root][INFO] - Training Epoch: 1/2, step 14198/23838 completed (loss: 0.3326486349105835, acc: 0.8711340427398682)
[2025-02-16 13:42:45,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:45,688][root][INFO] - Training Epoch: 1/2, step 14199/23838 completed (loss: 0.6382176280021667, acc: 0.7735849022865295)
[2025-02-16 13:42:45,858][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:46,087][root][INFO] - Training Epoch: 1/2, step 14200/23838 completed (loss: 0.44374099373817444, acc: 0.8673469424247742)
[2025-02-16 13:42:46,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:46,653][root][INFO] - Training Epoch: 1/2, step 14201/23838 completed (loss: 0.32431426644325256, acc: 0.9271523356437683)
[2025-02-16 13:42:46,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:47,131][root][INFO] - Training Epoch: 1/2, step 14202/23838 completed (loss: 0.39267370104789734, acc: 0.9047619104385376)
[2025-02-16 13:42:47,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:47,629][root][INFO] - Training Epoch: 1/2, step 14203/23838 completed (loss: 0.4373782277107239, acc: 0.8666666746139526)
[2025-02-16 13:42:48,070][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:48,493][root][INFO] - Training Epoch: 1/2, step 14204/23838 completed (loss: 0.2726632058620453, acc: 0.9362549781799316)
[2025-02-16 13:42:48,863][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:49,239][root][INFO] - Training Epoch: 1/2, step 14205/23838 completed (loss: 0.3365352153778076, acc: 0.9226804375648499)
[2025-02-16 13:42:49,432][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:49,663][root][INFO] - Training Epoch: 1/2, step 14206/23838 completed (loss: 0.10822601616382599, acc: 0.9558823704719543)
[2025-02-16 13:42:49,812][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:50,038][root][INFO] - Training Epoch: 1/2, step 14207/23838 completed (loss: 0.19837214052677155, acc: 0.9375)
[2025-02-16 13:42:50,270][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:50,524][root][INFO] - Training Epoch: 1/2, step 14208/23838 completed (loss: 0.14694197475910187, acc: 0.969924807548523)
[2025-02-16 13:42:50,814][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:51,106][root][INFO] - Training Epoch: 1/2, step 14209/23838 completed (loss: 0.2609458565711975, acc: 0.939393937587738)
[2025-02-16 13:42:51,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:51,724][root][INFO] - Training Epoch: 1/2, step 14210/23838 completed (loss: 0.2673743963241577, acc: 0.9457831382751465)
[2025-02-16 13:42:51,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:52,122][root][INFO] - Training Epoch: 1/2, step 14211/23838 completed (loss: 0.18646632134914398, acc: 0.9763779640197754)
[2025-02-16 13:42:52,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:52,505][root][INFO] - Training Epoch: 1/2, step 14212/23838 completed (loss: 0.27665838599205017, acc: 0.9339622855186462)
[2025-02-16 13:42:52,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:52,969][root][INFO] - Training Epoch: 1/2, step 14213/23838 completed (loss: 0.32261067628860474, acc: 0.9347826242446899)
[2025-02-16 13:42:53,214][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:53,457][root][INFO] - Training Epoch: 1/2, step 14214/23838 completed (loss: 0.13034506142139435, acc: 0.9629629850387573)
[2025-02-16 13:42:53,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:53,969][root][INFO] - Training Epoch: 1/2, step 14215/23838 completed (loss: 0.10741159319877625, acc: 0.9672130942344666)
[2025-02-16 13:42:54,215][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:54,444][root][INFO] - Training Epoch: 1/2, step 14216/23838 completed (loss: 0.2661975026130676, acc: 0.9435483813285828)
[2025-02-16 13:42:54,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:54,932][root][INFO] - Training Epoch: 1/2, step 14217/23838 completed (loss: 0.1308533400297165, acc: 0.9685863852500916)
[2025-02-16 13:42:55,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:55,341][root][INFO] - Training Epoch: 1/2, step 14218/23838 completed (loss: 0.3211818039417267, acc: 0.9380530714988708)
[2025-02-16 13:42:55,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:55,798][root][INFO] - Training Epoch: 1/2, step 14219/23838 completed (loss: 0.5115190744400024, acc: 0.8600000143051147)
[2025-02-16 13:42:56,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:56,270][root][INFO] - Training Epoch: 1/2, step 14220/23838 completed (loss: 0.2511715888977051, acc: 0.929729700088501)
[2025-02-16 13:42:56,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:56,775][root][INFO] - Training Epoch: 1/2, step 14221/23838 completed (loss: 0.15834155678749084, acc: 0.9745762944221497)
[2025-02-16 13:42:56,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:57,193][root][INFO] - Training Epoch: 1/2, step 14222/23838 completed (loss: 0.12066231667995453, acc: 0.9624999761581421)
[2025-02-16 13:42:57,409][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:57,639][root][INFO] - Training Epoch: 1/2, step 14223/23838 completed (loss: 0.07664458453655243, acc: 0.9863945841789246)
[2025-02-16 13:42:57,840][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:58,079][root][INFO] - Training Epoch: 1/2, step 14224/23838 completed (loss: 0.17873144149780273, acc: 0.969924807548523)
[2025-02-16 13:42:58,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:58,505][root][INFO] - Training Epoch: 1/2, step 14225/23838 completed (loss: 0.2547249495983124, acc: 0.9327731132507324)
[2025-02-16 13:42:58,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:58,913][root][INFO] - Training Epoch: 1/2, step 14226/23838 completed (loss: 0.07991756498813629, acc: 0.9883720874786377)
[2025-02-16 13:42:59,171][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:59,438][root][INFO] - Training Epoch: 1/2, step 14227/23838 completed (loss: 0.21844901144504547, acc: 0.94017094373703)
[2025-02-16 13:42:59,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:42:59,894][root][INFO] - Training Epoch: 1/2, step 14228/23838 completed (loss: 0.06722892820835114, acc: 0.9923076629638672)
[2025-02-16 13:43:00,080][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:00,288][root][INFO] - Training Epoch: 1/2, step 14229/23838 completed (loss: 0.2560894191265106, acc: 0.9135802388191223)
[2025-02-16 13:43:00,451][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:00,694][root][INFO] - Training Epoch: 1/2, step 14230/23838 completed (loss: 0.4594920873641968, acc: 0.8598130941390991)
[2025-02-16 13:43:01,319][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:01,877][root][INFO] - Training Epoch: 1/2, step 14231/23838 completed (loss: 0.18367420136928558, acc: 0.9400749206542969)
[2025-02-16 13:43:02,289][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:02,674][root][INFO] - Training Epoch: 1/2, step 14232/23838 completed (loss: 0.7337042093276978, acc: 0.8507462739944458)
[2025-02-16 13:43:02,885][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:03,120][root][INFO] - Training Epoch: 1/2, step 14233/23838 completed (loss: 0.10929260402917862, acc: 0.9726027250289917)
[2025-02-16 13:43:03,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:03,672][root][INFO] - Training Epoch: 1/2, step 14234/23838 completed (loss: 0.2024894654750824, acc: 0.9583333134651184)
[2025-02-16 13:43:03,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:04,329][root][INFO] - Training Epoch: 1/2, step 14235/23838 completed (loss: 0.2275373935699463, acc: 0.936170220375061)
[2025-02-16 13:43:04,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:04,766][root][INFO] - Training Epoch: 1/2, step 14236/23838 completed (loss: 0.4233940541744232, acc: 0.918367326259613)
[2025-02-16 13:43:05,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:05,307][root][INFO] - Training Epoch: 1/2, step 14237/23838 completed (loss: 0.2113264501094818, acc: 0.9487179517745972)
[2025-02-16 13:43:05,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:05,754][root][INFO] - Training Epoch: 1/2, step 14238/23838 completed (loss: 0.1584731489419937, acc: 0.9610389471054077)
[2025-02-16 13:43:06,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:06,459][root][INFO] - Training Epoch: 1/2, step 14239/23838 completed (loss: 0.29988178610801697, acc: 0.912162184715271)
[2025-02-16 13:43:06,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:06,940][root][INFO] - Training Epoch: 1/2, step 14240/23838 completed (loss: 0.13484802842140198, acc: 0.9605262875556946)
[2025-02-16 13:43:07,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:07,532][root][INFO] - Training Epoch: 1/2, step 14241/23838 completed (loss: 0.18034401535987854, acc: 0.9406779408454895)
[2025-02-16 13:43:07,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:08,079][root][INFO] - Training Epoch: 1/2, step 14242/23838 completed (loss: 0.24158363044261932, acc: 0.949999988079071)
[2025-02-16 13:43:08,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:08,708][root][INFO] - Training Epoch: 1/2, step 14243/23838 completed (loss: 0.10008791834115982, acc: 0.9664804339408875)
[2025-02-16 13:43:08,861][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:09,077][root][INFO] - Training Epoch: 1/2, step 14244/23838 completed (loss: 0.33066558837890625, acc: 0.8695651888847351)
[2025-02-16 13:43:09,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:09,471][root][INFO] - Training Epoch: 1/2, step 14245/23838 completed (loss: 0.17877177894115448, acc: 0.9490445852279663)
[2025-02-16 13:43:09,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:09,922][root][INFO] - Training Epoch: 1/2, step 14246/23838 completed (loss: 0.05597866699099541, acc: 0.9868995547294617)
[2025-02-16 13:43:10,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:10,355][root][INFO] - Training Epoch: 1/2, step 14247/23838 completed (loss: 0.2438964992761612, acc: 0.9298245906829834)
[2025-02-16 13:43:10,536][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:10,799][root][INFO] - Training Epoch: 1/2, step 14248/23838 completed (loss: 0.47160935401916504, acc: 0.8823529481887817)
[2025-02-16 13:43:11,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:11,300][root][INFO] - Training Epoch: 1/2, step 14249/23838 completed (loss: 0.24299365282058716, acc: 0.9320987462997437)
[2025-02-16 13:43:11,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:11,741][root][INFO] - Training Epoch: 1/2, step 14250/23838 completed (loss: 0.2688666880130768, acc: 0.9516128897666931)
[2025-02-16 13:43:11,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:12,181][root][INFO] - Training Epoch: 1/2, step 14251/23838 completed (loss: 0.04308783635497093, acc: 0.9913793206214905)
[2025-02-16 13:43:12,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:12,867][root][INFO] - Training Epoch: 1/2, step 14252/23838 completed (loss: 0.19273220002651215, acc: 0.9642857313156128)
[2025-02-16 13:43:13,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:13,307][root][INFO] - Training Epoch: 1/2, step 14253/23838 completed (loss: 0.09940037131309509, acc: 0.9803921580314636)
[2025-02-16 13:43:13,523][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:13,799][root][INFO] - Training Epoch: 1/2, step 14254/23838 completed (loss: 0.09302174299955368, acc: 0.9719626307487488)
[2025-02-16 13:43:14,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:14,619][root][INFO] - Training Epoch: 1/2, step 14255/23838 completed (loss: 0.07839016616344452, acc: 0.9741379022598267)
[2025-02-16 13:43:15,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:15,343][root][INFO] - Training Epoch: 1/2, step 14256/23838 completed (loss: 0.18913087248802185, acc: 0.9588235020637512)
[2025-02-16 13:43:15,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:15,907][root][INFO] - Training Epoch: 1/2, step 14257/23838 completed (loss: 0.1537860631942749, acc: 0.9567567706108093)
[2025-02-16 13:43:16,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:16,372][root][INFO] - Training Epoch: 1/2, step 14258/23838 completed (loss: 0.24543128907680511, acc: 0.9146341681480408)
[2025-02-16 13:43:16,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:16,814][root][INFO] - Training Epoch: 1/2, step 14259/23838 completed (loss: 0.1306869238615036, acc: 0.96875)
[2025-02-16 13:43:17,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:17,302][root][INFO] - Training Epoch: 1/2, step 14260/23838 completed (loss: 0.0838886946439743, acc: 0.979899525642395)
[2025-02-16 13:43:17,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:17,679][root][INFO] - Training Epoch: 1/2, step 14261/23838 completed (loss: 0.15138521790504456, acc: 0.9729729890823364)
[2025-02-16 13:43:17,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:18,237][root][INFO] - Training Epoch: 1/2, step 14262/23838 completed (loss: 0.14776422083377838, acc: 0.9547738432884216)
[2025-02-16 13:43:18,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:18,654][root][INFO] - Training Epoch: 1/2, step 14263/23838 completed (loss: 0.16833342611789703, acc: 0.9577465057373047)
[2025-02-16 13:43:18,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:19,182][root][INFO] - Training Epoch: 1/2, step 14264/23838 completed (loss: 0.20777300000190735, acc: 0.9380530714988708)
[2025-02-16 13:43:19,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:19,638][root][INFO] - Training Epoch: 1/2, step 14265/23838 completed (loss: 0.14346900582313538, acc: 0.9576271176338196)
[2025-02-16 13:43:19,895][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:20,147][root][INFO] - Training Epoch: 1/2, step 14266/23838 completed (loss: 0.38915857672691345, acc: 0.8990825414657593)
[2025-02-16 13:43:20,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:20,785][root][INFO] - Training Epoch: 1/2, step 14267/23838 completed (loss: 0.15956509113311768, acc: 0.967391312122345)
[2025-02-16 13:43:21,071][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:21,359][root][INFO] - Training Epoch: 1/2, step 14268/23838 completed (loss: 0.10657613724470139, acc: 0.9718309640884399)
[2025-02-16 13:43:21,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:21,799][root][INFO] - Training Epoch: 1/2, step 14269/23838 completed (loss: 0.07330427318811417, acc: 0.9789473414421082)
[2025-02-16 13:43:22,112][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:22,384][root][INFO] - Training Epoch: 1/2, step 14270/23838 completed (loss: 0.09452550113201141, acc: 0.9626865386962891)
[2025-02-16 13:43:22,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:22,745][root][INFO] - Training Epoch: 1/2, step 14271/23838 completed (loss: 0.04975424334406853, acc: 0.9894737005233765)
[2025-02-16 13:43:22,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:23,222][root][INFO] - Training Epoch: 1/2, step 14272/23838 completed (loss: 0.21933940052986145, acc: 0.9470587968826294)
[2025-02-16 13:43:23,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:23,828][root][INFO] - Training Epoch: 1/2, step 14273/23838 completed (loss: 0.23548583686351776, acc: 0.939226508140564)
[2025-02-16 13:43:24,138][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:24,450][root][INFO] - Training Epoch: 1/2, step 14274/23838 completed (loss: 0.4641425609588623, acc: 0.8733333349227905)
[2025-02-16 13:43:24,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:24,882][root][INFO] - Training Epoch: 1/2, step 14275/23838 completed (loss: 0.23251143097877502, acc: 0.9259259104728699)
[2025-02-16 13:43:25,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:25,270][root][INFO] - Training Epoch: 1/2, step 14276/23838 completed (loss: 0.13319480419158936, acc: 0.9800000190734863)
[2025-02-16 13:43:25,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:26,150][root][INFO] - Training Epoch: 1/2, step 14277/23838 completed (loss: 0.2627445161342621, acc: 0.934272289276123)
[2025-02-16 13:43:26,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:26,709][root][INFO] - Training Epoch: 1/2, step 14278/23838 completed (loss: 0.23564457893371582, acc: 0.9166666865348816)
[2025-02-16 13:43:27,153][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:27,533][root][INFO] - Training Epoch: 1/2, step 14279/23838 completed (loss: 0.22453166544437408, acc: 0.939130425453186)
[2025-02-16 13:43:27,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:27,951][root][INFO] - Training Epoch: 1/2, step 14280/23838 completed (loss: 0.2795015871524811, acc: 0.9173553586006165)
[2025-02-16 13:43:28,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:28,364][root][INFO] - Training Epoch: 1/2, step 14281/23838 completed (loss: 0.18131023645401, acc: 0.9430894255638123)
[2025-02-16 13:43:28,529][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:28,761][root][INFO] - Training Epoch: 1/2, step 14282/23838 completed (loss: 0.29320457577705383, acc: 0.8991596698760986)
[2025-02-16 13:43:29,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:29,263][root][INFO] - Training Epoch: 1/2, step 14283/23838 completed (loss: 0.30134594440460205, acc: 0.9108911156654358)
[2025-02-16 13:43:29,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:29,701][root][INFO] - Training Epoch: 1/2, step 14284/23838 completed (loss: 0.07213158905506134, acc: 0.9923664331436157)
[2025-02-16 13:43:29,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:30,153][root][INFO] - Training Epoch: 1/2, step 14285/23838 completed (loss: 0.48113495111465454, acc: 0.8999999761581421)
[2025-02-16 13:43:30,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:30,612][root][INFO] - Training Epoch: 1/2, step 14286/23838 completed (loss: 0.28976204991340637, acc: 0.9200000166893005)
[2025-02-16 13:43:30,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:31,087][root][INFO] - Training Epoch: 1/2, step 14287/23838 completed (loss: 0.17804118990898132, acc: 0.9587628841400146)
[2025-02-16 13:43:31,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:31,497][root][INFO] - Training Epoch: 1/2, step 14288/23838 completed (loss: 0.3180655241012573, acc: 0.9083969593048096)
[2025-02-16 13:43:31,644][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:31,873][root][INFO] - Training Epoch: 1/2, step 14289/23838 completed (loss: 1.3830739259719849, acc: 0.707317054271698)
[2025-02-16 13:43:32,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:32,342][root][INFO] - Training Epoch: 1/2, step 14290/23838 completed (loss: 0.21288567781448364, acc: 0.9268292784690857)
[2025-02-16 13:43:32,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:32,741][root][INFO] - Training Epoch: 1/2, step 14291/23838 completed (loss: 0.10788765549659729, acc: 0.957446813583374)
[2025-02-16 13:43:32,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:33,224][root][INFO] - Training Epoch: 1/2, step 14292/23838 completed (loss: 0.16132675111293793, acc: 0.957446813583374)
[2025-02-16 13:43:33,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:33,639][root][INFO] - Training Epoch: 1/2, step 14293/23838 completed (loss: 0.3684422969818115, acc: 0.90625)
[2025-02-16 13:43:33,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:34,143][root][INFO] - Training Epoch: 1/2, step 14294/23838 completed (loss: 0.1256963461637497, acc: 0.9626865386962891)
[2025-02-16 13:43:34,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:34,538][root][INFO] - Training Epoch: 1/2, step 14295/23838 completed (loss: 0.3882530927658081, acc: 0.8888888955116272)
[2025-02-16 13:43:34,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:35,304][root][INFO] - Training Epoch: 1/2, step 14296/23838 completed (loss: 0.30530214309692383, acc: 0.9281045794487)
[2025-02-16 13:43:35,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:35,762][root][INFO] - Training Epoch: 1/2, step 14297/23838 completed (loss: 0.34656763076782227, acc: 0.9255319237709045)
[2025-02-16 13:43:36,033][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:36,345][root][INFO] - Training Epoch: 1/2, step 14298/23838 completed (loss: 0.17310841381549835, acc: 0.9685534834861755)
[2025-02-16 13:43:36,695][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:37,030][root][INFO] - Training Epoch: 1/2, step 14299/23838 completed (loss: 0.303932249546051, acc: 0.9157894849777222)
[2025-02-16 13:43:37,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:37,765][root][INFO] - Training Epoch: 1/2, step 14300/23838 completed (loss: 0.1754167079925537, acc: 0.9512194991111755)
[2025-02-16 13:43:38,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:38,325][root][INFO] - Training Epoch: 1/2, step 14301/23838 completed (loss: 0.13019214570522308, acc: 0.9642857313156128)
[2025-02-16 13:43:38,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:39,045][root][INFO] - Training Epoch: 1/2, step 14302/23838 completed (loss: 0.14252077043056488, acc: 0.966292142868042)
[2025-02-16 13:43:39,370][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:39,688][root][INFO] - Training Epoch: 1/2, step 14303/23838 completed (loss: 0.0431574322283268, acc: 0.9935897588729858)
[2025-02-16 13:43:39,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:40,253][root][INFO] - Training Epoch: 1/2, step 14304/23838 completed (loss: 0.06562930345535278, acc: 0.9875776171684265)
[2025-02-16 13:43:40,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:41,025][root][INFO] - Training Epoch: 1/2, step 14305/23838 completed (loss: 0.20137177407741547, acc: 0.9425287246704102)
[2025-02-16 13:43:41,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:41,959][root][INFO] - Training Epoch: 1/2, step 14306/23838 completed (loss: 0.11670003086328506, acc: 0.9684210419654846)
[2025-02-16 13:43:42,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:42,485][root][INFO] - Training Epoch: 1/2, step 14307/23838 completed (loss: 0.1126173734664917, acc: 0.9552238583564758)
[2025-02-16 13:43:42,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:43,018][root][INFO] - Training Epoch: 1/2, step 14308/23838 completed (loss: 0.19823795557022095, acc: 0.9459459185600281)
[2025-02-16 13:43:43,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:43,486][root][INFO] - Training Epoch: 1/2, step 14309/23838 completed (loss: 0.3882236182689667, acc: 0.9065420627593994)
[2025-02-16 13:43:43,699][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:43,946][root][INFO] - Training Epoch: 1/2, step 14310/23838 completed (loss: 0.6358681321144104, acc: 0.8311688303947449)
[2025-02-16 13:43:44,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:44,688][root][INFO] - Training Epoch: 1/2, step 14311/23838 completed (loss: 0.4704115688800812, acc: 0.8611111044883728)
[2025-02-16 13:43:45,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:45,473][root][INFO] - Training Epoch: 1/2, step 14312/23838 completed (loss: 0.22018340229988098, acc: 0.9289617538452148)
[2025-02-16 13:43:45,713][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:45,995][root][INFO] - Training Epoch: 1/2, step 14313/23838 completed (loss: 0.12055390328168869, acc: 0.9568345546722412)
[2025-02-16 13:43:46,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:46,382][root][INFO] - Training Epoch: 1/2, step 14314/23838 completed (loss: 0.3280923366546631, acc: 0.9411764740943909)
[2025-02-16 13:43:46,576][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:46,833][root][INFO] - Training Epoch: 1/2, step 14315/23838 completed (loss: 0.08775941282510757, acc: 0.9715909361839294)
[2025-02-16 13:43:46,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:47,227][root][INFO] - Training Epoch: 1/2, step 14316/23838 completed (loss: 0.22021140158176422, acc: 0.931506872177124)
[2025-02-16 13:43:47,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:47,762][root][INFO] - Training Epoch: 1/2, step 14317/23838 completed (loss: 0.26732608675956726, acc: 0.907975435256958)
[2025-02-16 13:43:47,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:48,238][root][INFO] - Training Epoch: 1/2, step 14318/23838 completed (loss: 0.06495349109172821, acc: 0.9864864945411682)
[2025-02-16 13:43:48,417][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:48,655][root][INFO] - Training Epoch: 1/2, step 14319/23838 completed (loss: 0.15188761055469513, acc: 0.9733333587646484)
[2025-02-16 13:43:48,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:49,158][root][INFO] - Training Epoch: 1/2, step 14320/23838 completed (loss: 0.15364116430282593, acc: 0.9669421315193176)
[2025-02-16 13:43:49,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:49,587][root][INFO] - Training Epoch: 1/2, step 14321/23838 completed (loss: 0.19977305829524994, acc: 0.9541666507720947)
[2025-02-16 13:43:50,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:50,428][root][INFO] - Training Epoch: 1/2, step 14322/23838 completed (loss: 0.2799152433872223, acc: 0.9326424598693848)
[2025-02-16 13:43:50,583][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:50,851][root][INFO] - Training Epoch: 1/2, step 14323/23838 completed (loss: 0.16437800228595734, acc: 0.9651162624359131)
[2025-02-16 13:43:51,039][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:51,254][root][INFO] - Training Epoch: 1/2, step 14324/23838 completed (loss: 0.2547285854816437, acc: 0.93388432264328)
[2025-02-16 13:43:51,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:51,926][root][INFO] - Training Epoch: 1/2, step 14325/23838 completed (loss: 0.3405468463897705, acc: 0.9185185432434082)
[2025-02-16 13:43:52,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:52,311][root][INFO] - Training Epoch: 1/2, step 14326/23838 completed (loss: 0.44149070978164673, acc: 0.8939393758773804)
[2025-02-16 13:43:52,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:52,733][root][INFO] - Training Epoch: 1/2, step 14327/23838 completed (loss: 0.0870855301618576, acc: 0.9815950989723206)
[2025-02-16 13:43:52,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:53,174][root][INFO] - Training Epoch: 1/2, step 14328/23838 completed (loss: 0.15411421656608582, acc: 0.9523809552192688)
[2025-02-16 13:43:53,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:53,650][root][INFO] - Training Epoch: 1/2, step 14329/23838 completed (loss: 0.4249166250228882, acc: 0.8954248428344727)
[2025-02-16 13:43:53,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:54,080][root][INFO] - Training Epoch: 1/2, step 14330/23838 completed (loss: 0.14007629454135895, acc: 0.9702970385551453)
[2025-02-16 13:43:54,312][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:54,549][root][INFO] - Training Epoch: 1/2, step 14331/23838 completed (loss: 0.38080042600631714, acc: 0.8888888955116272)
[2025-02-16 13:43:54,731][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:54,947][root][INFO] - Training Epoch: 1/2, step 14332/23838 completed (loss: 0.16991811990737915, acc: 0.9448819160461426)
[2025-02-16 13:43:55,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:55,434][root][INFO] - Training Epoch: 1/2, step 14333/23838 completed (loss: 0.14116163551807404, acc: 0.9649122953414917)
[2025-02-16 13:43:55,696][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:55,935][root][INFO] - Training Epoch: 1/2, step 14334/23838 completed (loss: 0.14410392940044403, acc: 0.9596773982048035)
[2025-02-16 13:43:56,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:56,708][root][INFO] - Training Epoch: 1/2, step 14335/23838 completed (loss: 0.13836853206157684, acc: 0.959770143032074)
[2025-02-16 13:43:56,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:57,220][root][INFO] - Training Epoch: 1/2, step 14336/23838 completed (loss: 0.1327979564666748, acc: 0.9685534834861755)
[2025-02-16 13:43:57,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:57,729][root][INFO] - Training Epoch: 1/2, step 14337/23838 completed (loss: 0.16654786467552185, acc: 0.9615384340286255)
[2025-02-16 13:43:57,998][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:58,290][root][INFO] - Training Epoch: 1/2, step 14338/23838 completed (loss: 0.11179383844137192, acc: 0.9714285731315613)
[2025-02-16 13:43:58,525][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:58,768][root][INFO] - Training Epoch: 1/2, step 14339/23838 completed (loss: 0.32074224948883057, acc: 0.9238095283508301)
[2025-02-16 13:43:59,027][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:59,318][root][INFO] - Training Epoch: 1/2, step 14340/23838 completed (loss: 0.09289231896400452, acc: 0.9752475023269653)
[2025-02-16 13:43:59,579][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:43:59,872][root][INFO] - Training Epoch: 1/2, step 14341/23838 completed (loss: 0.08443497121334076, acc: 0.9849624037742615)
[2025-02-16 13:44:00,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:00,297][root][INFO] - Training Epoch: 1/2, step 14342/23838 completed (loss: 0.1628275215625763, acc: 0.9747899174690247)
[2025-02-16 13:44:00,653][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:01,015][root][INFO] - Training Epoch: 1/2, step 14343/23838 completed (loss: 0.187971830368042, acc: 0.9647058844566345)
[2025-02-16 13:44:01,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:01,670][root][INFO] - Training Epoch: 1/2, step 14344/23838 completed (loss: 0.15355631709098816, acc: 0.9666666388511658)
[2025-02-16 13:44:01,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:02,120][root][INFO] - Training Epoch: 1/2, step 14345/23838 completed (loss: 0.3507591784000397, acc: 0.8934911489486694)
[2025-02-16 13:44:02,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:02,798][root][INFO] - Training Epoch: 1/2, step 14346/23838 completed (loss: 0.45716702938079834, acc: 0.8527131676673889)
[2025-02-16 13:44:03,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:03,242][root][INFO] - Training Epoch: 1/2, step 14347/23838 completed (loss: 0.5003409385681152, acc: 0.8369565010070801)
[2025-02-16 13:44:03,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:03,699][root][INFO] - Training Epoch: 1/2, step 14348/23838 completed (loss: 0.4871331751346588, acc: 0.8690476417541504)
[2025-02-16 13:44:03,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:04,142][root][INFO] - Training Epoch: 1/2, step 14349/23838 completed (loss: 0.34182289242744446, acc: 0.9101123809814453)
[2025-02-16 13:44:04,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:04,599][root][INFO] - Training Epoch: 1/2, step 14350/23838 completed (loss: 0.7149900197982788, acc: 0.7553191781044006)
[2025-02-16 13:44:04,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:05,043][root][INFO] - Training Epoch: 1/2, step 14351/23838 completed (loss: 0.8847718834877014, acc: 0.7179487347602844)
[2025-02-16 13:44:05,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:05,443][root][INFO] - Training Epoch: 1/2, step 14352/23838 completed (loss: 0.2691919803619385, acc: 0.9178082346916199)
[2025-02-16 13:44:05,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:05,808][root][INFO] - Training Epoch: 1/2, step 14353/23838 completed (loss: 0.43274521827697754, acc: 0.8818897604942322)
[2025-02-16 13:44:05,971][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:06,201][root][INFO] - Training Epoch: 1/2, step 14354/23838 completed (loss: 0.7434918284416199, acc: 0.7547169923782349)
[2025-02-16 13:44:06,346][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:06,639][root][INFO] - Training Epoch: 1/2, step 14355/23838 completed (loss: 0.327210009098053, acc: 0.8907563090324402)
[2025-02-16 13:44:06,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:07,038][root][INFO] - Training Epoch: 1/2, step 14356/23838 completed (loss: 0.15101037919521332, acc: 0.9677419066429138)
[2025-02-16 13:44:07,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:07,496][root][INFO] - Training Epoch: 1/2, step 14357/23838 completed (loss: 0.2802920937538147, acc: 0.9351351261138916)
[2025-02-16 13:44:07,720][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:07,950][root][INFO] - Training Epoch: 1/2, step 14358/23838 completed (loss: 0.3254801034927368, acc: 0.9405940771102905)
[2025-02-16 13:44:08,137][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:08,372][root][INFO] - Training Epoch: 1/2, step 14359/23838 completed (loss: 0.39459145069122314, acc: 0.8636363744735718)
[2025-02-16 13:44:08,572][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:08,812][root][INFO] - Training Epoch: 1/2, step 14360/23838 completed (loss: 0.18747928738594055, acc: 0.9604519605636597)
[2025-02-16 13:44:09,022][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:09,273][root][INFO] - Training Epoch: 1/2, step 14361/23838 completed (loss: 0.14510934054851532, acc: 0.969072163105011)
[2025-02-16 13:44:09,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:09,768][root][INFO] - Training Epoch: 1/2, step 14362/23838 completed (loss: 0.2868325412273407, acc: 0.9236640930175781)
[2025-02-16 13:44:09,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:10,155][root][INFO] - Training Epoch: 1/2, step 14363/23838 completed (loss: 0.23169884085655212, acc: 0.9439252614974976)
[2025-02-16 13:44:10,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:10,536][root][INFO] - Training Epoch: 1/2, step 14364/23838 completed (loss: 0.21749921143054962, acc: 0.9347826242446899)
[2025-02-16 13:44:10,736][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:10,981][root][INFO] - Training Epoch: 1/2, step 14365/23838 completed (loss: 0.18111763894557953, acc: 0.9621211886405945)
[2025-02-16 13:44:11,148][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:11,392][root][INFO] - Training Epoch: 1/2, step 14366/23838 completed (loss: 0.3863411545753479, acc: 0.9016393423080444)
[2025-02-16 13:44:11,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:11,796][root][INFO] - Training Epoch: 1/2, step 14367/23838 completed (loss: 0.11757631599903107, acc: 0.970588207244873)
[2025-02-16 13:44:11,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:12,198][root][INFO] - Training Epoch: 1/2, step 14368/23838 completed (loss: 0.35708385705947876, acc: 0.8659793734550476)
[2025-02-16 13:44:12,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:12,625][root][INFO] - Training Epoch: 1/2, step 14369/23838 completed (loss: 0.8412324786186218, acc: 0.8145161271095276)
[2025-02-16 13:44:12,825][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:13,058][root][INFO] - Training Epoch: 1/2, step 14370/23838 completed (loss: 0.5108715295791626, acc: 0.8636363744735718)
[2025-02-16 13:44:13,286][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:13,507][root][INFO] - Training Epoch: 1/2, step 14371/23838 completed (loss: 0.340074747800827, acc: 0.9242424368858337)
[2025-02-16 13:44:13,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:13,940][root][INFO] - Training Epoch: 1/2, step 14372/23838 completed (loss: 0.35754960775375366, acc: 0.9090909361839294)
[2025-02-16 13:44:14,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:14,388][root][INFO] - Training Epoch: 1/2, step 14373/23838 completed (loss: 0.3271879255771637, acc: 0.9389312863349915)
[2025-02-16 13:44:14,549][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:14,774][root][INFO] - Training Epoch: 1/2, step 14374/23838 completed (loss: 0.43479400873184204, acc: 0.9066666960716248)
[2025-02-16 13:44:14,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:15,208][root][INFO] - Training Epoch: 1/2, step 14375/23838 completed (loss: 0.29274964332580566, acc: 0.934959352016449)
[2025-02-16 13:44:15,425][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:15,666][root][INFO] - Training Epoch: 1/2, step 14376/23838 completed (loss: 0.5887727737426758, acc: 0.8478260636329651)
[2025-02-16 13:44:15,903][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:16,144][root][INFO] - Training Epoch: 1/2, step 14377/23838 completed (loss: 0.3980468809604645, acc: 0.8735632300376892)
[2025-02-16 13:44:16,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:16,630][root][INFO] - Training Epoch: 1/2, step 14378/23838 completed (loss: 0.15901713073253632, acc: 0.954023003578186)
[2025-02-16 13:44:16,856][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:17,082][root][INFO] - Training Epoch: 1/2, step 14379/23838 completed (loss: 0.6115282773971558, acc: 0.8445945978164673)
[2025-02-16 13:44:17,253][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:17,480][root][INFO] - Training Epoch: 1/2, step 14380/23838 completed (loss: 0.20165863633155823, acc: 0.9466666579246521)
[2025-02-16 13:44:17,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:17,975][root][INFO] - Training Epoch: 1/2, step 14381/23838 completed (loss: 0.193781316280365, acc: 0.9583333134651184)
[2025-02-16 13:44:18,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:18,369][root][INFO] - Training Epoch: 1/2, step 14382/23838 completed (loss: 0.07445299625396729, acc: 0.9896907210350037)
[2025-02-16 13:44:18,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:18,792][root][INFO] - Training Epoch: 1/2, step 14383/23838 completed (loss: 0.18582037091255188, acc: 0.9612902998924255)
[2025-02-16 13:44:18,945][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:19,165][root][INFO] - Training Epoch: 1/2, step 14384/23838 completed (loss: 0.510722815990448, acc: 0.8958333134651184)
[2025-02-16 13:44:19,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:19,615][root][INFO] - Training Epoch: 1/2, step 14385/23838 completed (loss: 0.27557244896888733, acc: 0.9173553586006165)
[2025-02-16 13:44:19,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:20,023][root][INFO] - Training Epoch: 1/2, step 14386/23838 completed (loss: 0.36398229002952576, acc: 0.9252336621284485)
[2025-02-16 13:44:20,184][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:20,410][root][INFO] - Training Epoch: 1/2, step 14387/23838 completed (loss: 0.6223061084747314, acc: 0.8048780560493469)
[2025-02-16 13:44:20,632][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:20,872][root][INFO] - Training Epoch: 1/2, step 14388/23838 completed (loss: 0.364587664604187, acc: 0.8767123222351074)
[2025-02-16 13:44:21,056][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:21,277][root][INFO] - Training Epoch: 1/2, step 14389/23838 completed (loss: 0.17280016839504242, acc: 0.9417475461959839)
[2025-02-16 13:44:21,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:21,643][root][INFO] - Training Epoch: 1/2, step 14390/23838 completed (loss: 0.267560213804245, acc: 0.9074074029922485)
[2025-02-16 13:44:21,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:22,014][root][INFO] - Training Epoch: 1/2, step 14391/23838 completed (loss: 0.5403838753700256, acc: 0.8444444537162781)
[2025-02-16 13:44:22,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:22,483][root][INFO] - Training Epoch: 1/2, step 14392/23838 completed (loss: 0.5224664211273193, acc: 0.8552631735801697)
[2025-02-16 13:44:22,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:22,904][root][INFO] - Training Epoch: 1/2, step 14393/23838 completed (loss: 0.5024092793464661, acc: 0.875)
[2025-02-16 13:44:23,061][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:23,274][root][INFO] - Training Epoch: 1/2, step 14394/23838 completed (loss: 0.9892733097076416, acc: 0.7816091775894165)
[2025-02-16 13:44:23,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:23,664][root][INFO] - Training Epoch: 1/2, step 14395/23838 completed (loss: 0.06695716083049774, acc: 0.9775280952453613)
[2025-02-16 13:44:23,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:24,084][root][INFO] - Training Epoch: 1/2, step 14396/23838 completed (loss: 0.19113309681415558, acc: 0.9436619877815247)
[2025-02-16 13:44:24,245][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:24,463][root][INFO] - Training Epoch: 1/2, step 14397/23838 completed (loss: 0.2232728898525238, acc: 0.9264705777168274)
[2025-02-16 13:44:24,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:24,870][root][INFO] - Training Epoch: 1/2, step 14398/23838 completed (loss: 0.6721366047859192, acc: 0.8030303120613098)
[2025-02-16 13:44:25,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:25,335][root][INFO] - Training Epoch: 1/2, step 14399/23838 completed (loss: 0.3628374934196472, acc: 0.8877005577087402)
[2025-02-16 13:44:25,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:25,836][root][INFO] - Training Epoch: 1/2, step 14400/23838 completed (loss: 0.36827635765075684, acc: 0.8941176533699036)
[2025-02-16 13:44:26,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:26,267][root][INFO] - Training Epoch: 1/2, step 14401/23838 completed (loss: 0.2728136479854584, acc: 0.8986486196517944)
[2025-02-16 13:44:26,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:26,671][root][INFO] - Training Epoch: 1/2, step 14402/23838 completed (loss: 0.20175442099571228, acc: 0.9279279112815857)
[2025-02-16 13:44:26,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:27,072][root][INFO] - Training Epoch: 1/2, step 14403/23838 completed (loss: 0.13748466968536377, acc: 0.9530201554298401)
[2025-02-16 13:44:27,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:27,437][root][INFO] - Training Epoch: 1/2, step 14404/23838 completed (loss: 0.5183091163635254, acc: 0.8737863898277283)
[2025-02-16 13:44:27,642][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:27,851][root][INFO] - Training Epoch: 1/2, step 14405/23838 completed (loss: 0.16115659475326538, acc: 0.9716312289237976)
[2025-02-16 13:44:28,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:28,322][root][INFO] - Training Epoch: 1/2, step 14406/23838 completed (loss: 0.43376627564430237, acc: 0.8653846383094788)
[2025-02-16 13:44:28,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:28,693][root][INFO] - Training Epoch: 1/2, step 14407/23838 completed (loss: 0.26808735728263855, acc: 0.9248120188713074)
[2025-02-16 13:44:28,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:29,070][root][INFO] - Training Epoch: 1/2, step 14408/23838 completed (loss: 0.2809469401836395, acc: 0.9038461446762085)
[2025-02-16 13:44:29,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:29,506][root][INFO] - Training Epoch: 1/2, step 14409/23838 completed (loss: 0.35167038440704346, acc: 0.8951612710952759)
[2025-02-16 13:44:29,673][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:29,892][root][INFO] - Training Epoch: 1/2, step 14410/23838 completed (loss: 0.35685694217681885, acc: 0.8981481194496155)
[2025-02-16 13:44:30,040][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:30,249][root][INFO] - Training Epoch: 1/2, step 14411/23838 completed (loss: 0.3681127429008484, acc: 0.9181286692619324)
[2025-02-16 13:44:30,403][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:30,620][root][INFO] - Training Epoch: 1/2, step 14412/23838 completed (loss: 0.7877727746963501, acc: 0.7903226017951965)
[2025-02-16 13:44:30,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:31,002][root][INFO] - Training Epoch: 1/2, step 14413/23838 completed (loss: 0.2511833608150482, acc: 0.9032257795333862)
[2025-02-16 13:44:31,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:31,380][root][INFO] - Training Epoch: 1/2, step 14414/23838 completed (loss: 0.3419935405254364, acc: 0.8961748480796814)
[2025-02-16 13:44:31,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:31,794][root][INFO] - Training Epoch: 1/2, step 14415/23838 completed (loss: 0.33139827847480774, acc: 0.9039999842643738)
[2025-02-16 13:44:31,979][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:32,227][root][INFO] - Training Epoch: 1/2, step 14416/23838 completed (loss: 0.3421604633331299, acc: 0.9210526347160339)
[2025-02-16 13:44:32,402][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:32,617][root][INFO] - Training Epoch: 1/2, step 14417/23838 completed (loss: 0.28952187299728394, acc: 0.925000011920929)
[2025-02-16 13:44:32,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:33,057][root][INFO] - Training Epoch: 1/2, step 14418/23838 completed (loss: 0.46052002906799316, acc: 0.8768116235733032)
[2025-02-16 13:44:33,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:33,507][root][INFO] - Training Epoch: 1/2, step 14419/23838 completed (loss: 0.27856090664863586, acc: 0.9285714030265808)
[2025-02-16 13:44:33,707][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:33,952][root][INFO] - Training Epoch: 1/2, step 14420/23838 completed (loss: 0.30010560154914856, acc: 0.9185185432434082)
[2025-02-16 13:44:34,149][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:34,408][root][INFO] - Training Epoch: 1/2, step 14421/23838 completed (loss: 0.391509085893631, acc: 0.8928571343421936)
[2025-02-16 13:44:34,634][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:34,894][root][INFO] - Training Epoch: 1/2, step 14422/23838 completed (loss: 0.5157370567321777, acc: 0.8759124279022217)
[2025-02-16 13:44:35,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:35,314][root][INFO] - Training Epoch: 1/2, step 14423/23838 completed (loss: 0.16306845843791962, acc: 0.9541984796524048)
[2025-02-16 13:44:35,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:35,737][root][INFO] - Training Epoch: 1/2, step 14424/23838 completed (loss: 0.11286897957324982, acc: 0.9824561476707458)
[2025-02-16 13:44:35,956][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:36,189][root][INFO] - Training Epoch: 1/2, step 14425/23838 completed (loss: 0.3368332087993622, acc: 0.9160305261611938)
[2025-02-16 13:44:36,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:36,643][root][INFO] - Training Epoch: 1/2, step 14426/23838 completed (loss: 0.29730716347694397, acc: 0.8978102207183838)
[2025-02-16 13:44:36,859][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:37,079][root][INFO] - Training Epoch: 1/2, step 14427/23838 completed (loss: 0.18885108828544617, acc: 0.9333333373069763)
[2025-02-16 13:44:37,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:37,460][root][INFO] - Training Epoch: 1/2, step 14428/23838 completed (loss: 0.2593955397605896, acc: 0.9268292784690857)
[2025-02-16 13:44:37,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:37,896][root][INFO] - Training Epoch: 1/2, step 14429/23838 completed (loss: 0.4983662962913513, acc: 0.8815789222717285)
[2025-02-16 13:44:38,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:38,338][root][INFO] - Training Epoch: 1/2, step 14430/23838 completed (loss: 0.18626271188259125, acc: 0.9714285731315613)
[2025-02-16 13:44:38,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:38,757][root][INFO] - Training Epoch: 1/2, step 14431/23838 completed (loss: 0.34424516558647156, acc: 0.8829787373542786)
[2025-02-16 13:44:38,967][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:39,230][root][INFO] - Training Epoch: 1/2, step 14432/23838 completed (loss: 0.3004510700702667, acc: 0.8839285969734192)
[2025-02-16 13:44:39,423][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:39,655][root][INFO] - Training Epoch: 1/2, step 14433/23838 completed (loss: 0.17113830149173737, acc: 0.939130425453186)
[2025-02-16 13:44:39,876][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:40,141][root][INFO] - Training Epoch: 1/2, step 14434/23838 completed (loss: 0.1802651286125183, acc: 0.9318181872367859)
[2025-02-16 13:44:40,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:40,598][root][INFO] - Training Epoch: 1/2, step 14435/23838 completed (loss: 0.7714020609855652, acc: 0.8484848737716675)
[2025-02-16 13:44:40,797][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:41,018][root][INFO] - Training Epoch: 1/2, step 14436/23838 completed (loss: 0.295598566532135, acc: 0.9166666865348816)
[2025-02-16 13:44:41,192][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:41,400][root][INFO] - Training Epoch: 1/2, step 14437/23838 completed (loss: 0.18482705950737, acc: 0.9572649598121643)
[2025-02-16 13:44:41,586][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:41,808][root][INFO] - Training Epoch: 1/2, step 14438/23838 completed (loss: 0.5329639315605164, acc: 0.8349514603614807)
[2025-02-16 13:44:41,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:42,192][root][INFO] - Training Epoch: 1/2, step 14439/23838 completed (loss: 0.29266709089279175, acc: 0.8993710875511169)
[2025-02-16 13:44:42,368][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:42,595][root][INFO] - Training Epoch: 1/2, step 14440/23838 completed (loss: 0.45366594195365906, acc: 0.8786126971244812)
[2025-02-16 13:44:42,798][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:43,020][root][INFO] - Training Epoch: 1/2, step 14441/23838 completed (loss: 0.37323427200317383, acc: 0.8979591727256775)
[2025-02-16 13:44:43,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:43,447][root][INFO] - Training Epoch: 1/2, step 14442/23838 completed (loss: 0.40787404775619507, acc: 0.9032257795333862)
[2025-02-16 13:44:43,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:43,823][root][INFO] - Training Epoch: 1/2, step 14443/23838 completed (loss: 0.4622728228569031, acc: 0.868686854839325)
[2025-02-16 13:44:44,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:44,277][root][INFO] - Training Epoch: 1/2, step 14444/23838 completed (loss: 0.4230131208896637, acc: 0.9020618796348572)
[2025-02-16 13:44:44,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:44,717][root][INFO] - Training Epoch: 1/2, step 14445/23838 completed (loss: 0.6136347055435181, acc: 0.8357142806053162)
[2025-02-16 13:44:44,912][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:45,152][root][INFO] - Training Epoch: 1/2, step 14446/23838 completed (loss: 0.29314154386520386, acc: 0.9357798099517822)
[2025-02-16 13:44:45,387][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:45,645][root][INFO] - Training Epoch: 1/2, step 14447/23838 completed (loss: 0.3033156991004944, acc: 0.9256198406219482)
[2025-02-16 13:44:45,833][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:46,059][root][INFO] - Training Epoch: 1/2, step 14448/23838 completed (loss: 0.18206802010536194, acc: 0.9718309640884399)
[2025-02-16 13:44:46,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:46,459][root][INFO] - Training Epoch: 1/2, step 14449/23838 completed (loss: 0.3962349593639374, acc: 0.8974359035491943)
[2025-02-16 13:44:46,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:46,902][root][INFO] - Training Epoch: 1/2, step 14450/23838 completed (loss: 0.2075771540403366, acc: 0.9428571462631226)
[2025-02-16 13:44:47,122][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:47,357][root][INFO] - Training Epoch: 1/2, step 14451/23838 completed (loss: 0.15499958395957947, acc: 0.939393937587738)
[2025-02-16 13:44:47,527][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:47,757][root][INFO] - Training Epoch: 1/2, step 14452/23838 completed (loss: 0.17170925438404083, acc: 0.95333331823349)
[2025-02-16 13:44:47,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:48,133][root][INFO] - Training Epoch: 1/2, step 14453/23838 completed (loss: 0.4709705412387848, acc: 0.8482142686843872)
[2025-02-16 13:44:48,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:48,469][root][INFO] - Training Epoch: 1/2, step 14454/23838 completed (loss: 0.6695511341094971, acc: 0.8115941882133484)
[2025-02-16 13:44:48,666][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:48,905][root][INFO] - Training Epoch: 1/2, step 14455/23838 completed (loss: 0.6384256482124329, acc: 0.7972972989082336)
[2025-02-16 13:44:49,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:49,338][root][INFO] - Training Epoch: 1/2, step 14456/23838 completed (loss: 0.4811701476573944, acc: 0.8695651888847351)
[2025-02-16 13:44:49,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:49,757][root][INFO] - Training Epoch: 1/2, step 14457/23838 completed (loss: 0.3742886185646057, acc: 0.8726114630699158)
[2025-02-16 13:44:49,900][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:50,129][root][INFO] - Training Epoch: 1/2, step 14458/23838 completed (loss: 0.9131660461425781, acc: 0.737500011920929)
[2025-02-16 13:44:50,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:50,594][root][INFO] - Training Epoch: 1/2, step 14459/23838 completed (loss: 0.2679222524166107, acc: 0.9210526347160339)
[2025-02-16 13:44:50,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:51,154][root][INFO] - Training Epoch: 1/2, step 14460/23838 completed (loss: 0.11892401427030563, acc: 0.9665272235870361)
[2025-02-16 13:44:51,363][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:51,616][root][INFO] - Training Epoch: 1/2, step 14461/23838 completed (loss: 0.07578037679195404, acc: 0.9711934328079224)
[2025-02-16 13:44:52,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:52,747][root][INFO] - Training Epoch: 1/2, step 14462/23838 completed (loss: 0.23500028252601624, acc: 0.928909957408905)
[2025-02-16 13:44:52,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:53,157][root][INFO] - Training Epoch: 1/2, step 14463/23838 completed (loss: 0.14477187395095825, acc: 0.975806474685669)
[2025-02-16 13:44:53,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:53,648][root][INFO] - Training Epoch: 1/2, step 14464/23838 completed (loss: 0.22412584722042084, acc: 0.9290322661399841)
[2025-02-16 13:44:53,851][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:54,089][root][INFO] - Training Epoch: 1/2, step 14465/23838 completed (loss: 0.25193119049072266, acc: 0.9126213788986206)
[2025-02-16 13:44:54,304][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:54,550][root][INFO] - Training Epoch: 1/2, step 14466/23838 completed (loss: 0.3398037254810333, acc: 0.9175257682800293)
[2025-02-16 13:44:54,774][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:55,011][root][INFO] - Training Epoch: 1/2, step 14467/23838 completed (loss: 0.25854024291038513, acc: 0.9333333373069763)
[2025-02-16 13:44:55,210][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:55,468][root][INFO] - Training Epoch: 1/2, step 14468/23838 completed (loss: 0.3055618405342102, acc: 0.9347826242446899)
[2025-02-16 13:44:55,709][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:55,954][root][INFO] - Training Epoch: 1/2, step 14469/23838 completed (loss: 0.20750850439071655, acc: 0.9301310181617737)
[2025-02-16 13:44:56,144][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:56,404][root][INFO] - Training Epoch: 1/2, step 14470/23838 completed (loss: 0.16231849789619446, acc: 0.9399999976158142)
[2025-02-16 13:44:56,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:56,874][root][INFO] - Training Epoch: 1/2, step 14471/23838 completed (loss: 0.3052152395248413, acc: 0.8857142925262451)
[2025-02-16 13:44:57,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:57,343][root][INFO] - Training Epoch: 1/2, step 14472/23838 completed (loss: 0.4626275599002838, acc: 0.8839285969734192)
[2025-02-16 13:44:57,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:57,813][root][INFO] - Training Epoch: 1/2, step 14473/23838 completed (loss: 0.2339615523815155, acc: 0.940397322177887)
[2025-02-16 13:44:58,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:58,231][root][INFO] - Training Epoch: 1/2, step 14474/23838 completed (loss: 0.23296257853507996, acc: 0.9444444179534912)
[2025-02-16 13:44:58,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:58,729][root][INFO] - Training Epoch: 1/2, step 14475/23838 completed (loss: 0.23553897440433502, acc: 0.9377777576446533)
[2025-02-16 13:44:58,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:59,110][root][INFO] - Training Epoch: 1/2, step 14476/23838 completed (loss: 0.16180838644504547, acc: 0.9776119589805603)
[2025-02-16 13:44:59,335][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:44:59,566][root][INFO] - Training Epoch: 1/2, step 14477/23838 completed (loss: 0.23275788128376007, acc: 0.9415204524993896)
[2025-02-16 13:44:59,778][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:00,044][root][INFO] - Training Epoch: 1/2, step 14478/23838 completed (loss: 0.20011985301971436, acc: 0.9320388436317444)
[2025-02-16 13:45:00,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:00,486][root][INFO] - Training Epoch: 1/2, step 14479/23838 completed (loss: 0.2040267139673233, acc: 0.9313725233078003)
[2025-02-16 13:45:00,751][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:01,022][root][INFO] - Training Epoch: 1/2, step 14480/23838 completed (loss: 0.1477845013141632, acc: 0.954285740852356)
[2025-02-16 13:45:01,263][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:01,493][root][INFO] - Training Epoch: 1/2, step 14481/23838 completed (loss: 0.31826767325401306, acc: 0.9146341681480408)
[2025-02-16 13:45:01,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:01,942][root][INFO] - Training Epoch: 1/2, step 14482/23838 completed (loss: 0.17683584988117218, acc: 0.9575757384300232)
[2025-02-16 13:45:02,104][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:02,323][root][INFO] - Training Epoch: 1/2, step 14483/23838 completed (loss: 0.37815526127815247, acc: 0.8918918967247009)
[2025-02-16 13:45:02,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:02,806][root][INFO] - Training Epoch: 1/2, step 14484/23838 completed (loss: 0.04237176105380058, acc: 0.9866666793823242)
[2025-02-16 13:45:03,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:03,298][root][INFO] - Training Epoch: 1/2, step 14485/23838 completed (loss: 0.2532538175582886, acc: 0.9402984976768494)
[2025-02-16 13:45:03,498][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:03,742][root][INFO] - Training Epoch: 1/2, step 14486/23838 completed (loss: 0.39311647415161133, acc: 0.8773584961891174)
[2025-02-16 13:45:03,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:04,205][root][INFO] - Training Epoch: 1/2, step 14487/23838 completed (loss: 0.20886476337909698, acc: 0.9452054500579834)
[2025-02-16 13:45:04,458][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:04,732][root][INFO] - Training Epoch: 1/2, step 14488/23838 completed (loss: 0.2328113168478012, acc: 0.9172413945198059)
[2025-02-16 13:45:04,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:05,124][root][INFO] - Training Epoch: 1/2, step 14489/23838 completed (loss: 0.42515379190444946, acc: 0.875)
[2025-02-16 13:45:05,327][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:05,593][root][INFO] - Training Epoch: 1/2, step 14490/23838 completed (loss: 0.14091287553310394, acc: 0.9516128897666931)
[2025-02-16 13:45:05,818][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:06,053][root][INFO] - Training Epoch: 1/2, step 14491/23838 completed (loss: 0.1793394386768341, acc: 0.9634146094322205)
[2025-02-16 13:45:06,283][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:06,515][root][INFO] - Training Epoch: 1/2, step 14492/23838 completed (loss: 0.1559317260980606, acc: 0.9426229596138)
[2025-02-16 13:45:06,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:06,982][root][INFO] - Training Epoch: 1/2, step 14493/23838 completed (loss: 0.2781820297241211, acc: 0.9179104566574097)
[2025-02-16 13:45:07,162][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:07,378][root][INFO] - Training Epoch: 1/2, step 14494/23838 completed (loss: 0.27389946579933167, acc: 0.9259259104728699)
[2025-02-16 13:45:07,541][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:07,769][root][INFO] - Training Epoch: 1/2, step 14495/23838 completed (loss: 0.10489990562200546, acc: 0.9743589758872986)
[2025-02-16 13:45:07,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:08,155][root][INFO] - Training Epoch: 1/2, step 14496/23838 completed (loss: 0.05668994411826134, acc: 0.9902912378311157)
[2025-02-16 13:45:08,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:08,576][root][INFO] - Training Epoch: 1/2, step 14497/23838 completed (loss: 0.23043397068977356, acc: 0.9191918969154358)
[2025-02-16 13:45:08,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:09,073][root][INFO] - Training Epoch: 1/2, step 14498/23838 completed (loss: 0.11605954170227051, acc: 0.9612902998924255)
[2025-02-16 13:45:09,226][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:09,450][root][INFO] - Training Epoch: 1/2, step 14499/23838 completed (loss: 0.28995463252067566, acc: 0.898876428604126)
[2025-02-16 13:45:09,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:09,940][root][INFO] - Training Epoch: 1/2, step 14500/23838 completed (loss: 0.27409827709198, acc: 0.9174311757087708)
[2025-02-16 13:45:10,161][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:10,393][root][INFO] - Training Epoch: 1/2, step 14501/23838 completed (loss: 0.4246859848499298, acc: 0.8985507488250732)
[2025-02-16 13:45:10,524][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:10,747][root][INFO] - Training Epoch: 1/2, step 14502/23838 completed (loss: 0.4234793186187744, acc: 0.8615384697914124)
[2025-02-16 13:45:10,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:11,142][root][INFO] - Training Epoch: 1/2, step 14503/23838 completed (loss: 0.7410954236984253, acc: 0.8194444179534912)
[2025-02-16 13:45:11,288][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:11,514][root][INFO] - Training Epoch: 1/2, step 14504/23838 completed (loss: 0.1910073608160019, acc: 0.9333333373069763)
[2025-02-16 13:45:11,702][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:11,970][root][INFO] - Training Epoch: 1/2, step 14505/23838 completed (loss: 0.6611371636390686, acc: 0.8292682766914368)
[2025-02-16 13:45:12,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:12,395][root][INFO] - Training Epoch: 1/2, step 14506/23838 completed (loss: 0.17851446568965912, acc: 0.9405940771102905)
[2025-02-16 13:45:12,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:12,862][root][INFO] - Training Epoch: 1/2, step 14507/23838 completed (loss: 0.18300005793571472, acc: 0.9462365508079529)
[2025-02-16 13:45:13,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:13,258][root][INFO] - Training Epoch: 1/2, step 14508/23838 completed (loss: 0.29177793860435486, acc: 0.9056603908538818)
[2025-02-16 13:45:13,408][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:13,694][root][INFO] - Training Epoch: 1/2, step 14509/23838 completed (loss: 0.046281684190034866, acc: 0.984375)
[2025-02-16 13:45:13,927][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:14,167][root][INFO] - Training Epoch: 1/2, step 14510/23838 completed (loss: 0.3602689206600189, acc: 0.9008264541625977)
[2025-02-16 13:45:14,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:14,559][root][INFO] - Training Epoch: 1/2, step 14511/23838 completed (loss: 0.22373506426811218, acc: 0.9203540086746216)
[2025-02-16 13:45:14,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:15,033][root][INFO] - Training Epoch: 1/2, step 14512/23838 completed (loss: 0.4369180202484131, acc: 0.8902438879013062)
[2025-02-16 13:45:15,187][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:15,421][root][INFO] - Training Epoch: 1/2, step 14513/23838 completed (loss: 0.07043786346912384, acc: 0.9780219793319702)
[2025-02-16 13:45:15,608][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:15,842][root][INFO] - Training Epoch: 1/2, step 14514/23838 completed (loss: 0.18595492839813232, acc: 0.9636363387107849)
[2025-02-16 13:45:16,000][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:16,227][root][INFO] - Training Epoch: 1/2, step 14515/23838 completed (loss: 0.20619036257266998, acc: 0.969072163105011)
[2025-02-16 13:45:16,434][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:16,695][root][INFO] - Training Epoch: 1/2, step 14516/23838 completed (loss: 0.22361157834529877, acc: 0.9175257682800293)
[2025-02-16 13:45:16,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:17,199][root][INFO] - Training Epoch: 1/2, step 14517/23838 completed (loss: 0.44651269912719727, acc: 0.859649121761322)
[2025-02-16 13:45:17,427][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:17,678][root][INFO] - Training Epoch: 1/2, step 14518/23838 completed (loss: 0.4893443286418915, acc: 0.8641975522041321)
[2025-02-16 13:45:17,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:18,142][root][INFO] - Training Epoch: 1/2, step 14519/23838 completed (loss: 0.16786912083625793, acc: 0.9494949579238892)
[2025-02-16 13:45:18,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:18,678][root][INFO] - Training Epoch: 1/2, step 14520/23838 completed (loss: 0.22993220388889313, acc: 0.9411764740943909)
[2025-02-16 13:45:18,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:19,146][root][INFO] - Training Epoch: 1/2, step 14521/23838 completed (loss: 0.2536095380783081, acc: 0.9189189076423645)
[2025-02-16 13:45:19,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:19,547][root][INFO] - Training Epoch: 1/2, step 14522/23838 completed (loss: 0.24855266511440277, acc: 0.9375)
[2025-02-16 13:45:19,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:20,035][root][INFO] - Training Epoch: 1/2, step 14523/23838 completed (loss: 0.40452516078948975, acc: 0.8983050584793091)
[2025-02-16 13:45:20,232][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:20,495][root][INFO] - Training Epoch: 1/2, step 14524/23838 completed (loss: 0.15994800627231598, acc: 0.9444444179534912)
[2025-02-16 13:45:20,793][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:21,104][root][INFO] - Training Epoch: 1/2, step 14525/23838 completed (loss: 0.12182635068893433, acc: 0.9795918464660645)
[2025-02-16 13:45:21,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:21,634][root][INFO] - Training Epoch: 1/2, step 14526/23838 completed (loss: 0.21273913979530334, acc: 0.9621211886405945)
[2025-02-16 13:45:21,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:22,217][root][INFO] - Training Epoch: 1/2, step 14527/23838 completed (loss: 0.16805465519428253, acc: 0.9515151381492615)
[2025-02-16 13:45:22,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:22,720][root][INFO] - Training Epoch: 1/2, step 14528/23838 completed (loss: 0.24689914286136627, acc: 0.942307710647583)
[2025-02-16 13:45:22,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:23,179][root][INFO] - Training Epoch: 1/2, step 14529/23838 completed (loss: 0.14998742938041687, acc: 0.9575757384300232)
[2025-02-16 13:45:23,647][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:24,132][root][INFO] - Training Epoch: 1/2, step 14530/23838 completed (loss: 0.1243823990225792, acc: 0.9685039520263672)
[2025-02-16 13:45:24,361][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:24,606][root][INFO] - Training Epoch: 1/2, step 14531/23838 completed (loss: 0.2654699981212616, acc: 0.9558823704719543)
[2025-02-16 13:45:24,817][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:25,051][root][INFO] - Training Epoch: 1/2, step 14532/23838 completed (loss: 0.24381700158119202, acc: 0.9269406199455261)
[2025-02-16 13:45:25,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:25,476][root][INFO] - Training Epoch: 1/2, step 14533/23838 completed (loss: 0.4512673020362854, acc: 0.8727272748947144)
[2025-02-16 13:45:25,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:26,044][root][INFO] - Training Epoch: 1/2, step 14534/23838 completed (loss: 0.21382124722003937, acc: 0.932624101638794)
[2025-02-16 13:45:26,233][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:26,467][root][INFO] - Training Epoch: 1/2, step 14535/23838 completed (loss: 0.3165823519229889, acc: 0.913294792175293)
[2025-02-16 13:45:26,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:27,157][root][INFO] - Training Epoch: 1/2, step 14536/23838 completed (loss: 0.19896472990512848, acc: 0.939226508140564)
[2025-02-16 13:45:27,320][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:27,565][root][INFO] - Training Epoch: 1/2, step 14537/23838 completed (loss: 0.12316378951072693, acc: 0.9756097793579102)
[2025-02-16 13:45:27,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:28,013][root][INFO] - Training Epoch: 1/2, step 14538/23838 completed (loss: 0.25109174847602844, acc: 0.9259259104728699)
[2025-02-16 13:45:28,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:28,406][root][INFO] - Training Epoch: 1/2, step 14539/23838 completed (loss: 0.12213235348463058, acc: 0.9626168012619019)
[2025-02-16 13:45:28,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:28,888][root][INFO] - Training Epoch: 1/2, step 14540/23838 completed (loss: 0.4557034373283386, acc: 0.892405092716217)
[2025-02-16 13:45:29,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:29,345][root][INFO] - Training Epoch: 1/2, step 14541/23838 completed (loss: 0.25179532170295715, acc: 0.9375)
[2025-02-16 13:45:29,590][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:29,842][root][INFO] - Training Epoch: 1/2, step 14542/23838 completed (loss: 0.16323842108249664, acc: 0.9333333373069763)
[2025-02-16 13:45:30,032][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:30,271][root][INFO] - Training Epoch: 1/2, step 14543/23838 completed (loss: 0.4661473035812378, acc: 0.9012345671653748)
[2025-02-16 13:45:30,440][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:30,624][root][INFO] - Training Epoch: 1/2, step 14544/23838 completed (loss: 0.6672796607017517, acc: 0.8428571224212646)
[2025-02-16 13:45:30,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:31,054][root][INFO] - Training Epoch: 1/2, step 14545/23838 completed (loss: 0.3265494108200073, acc: 0.9104477763175964)
[2025-02-16 13:45:31,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:31,514][root][INFO] - Training Epoch: 1/2, step 14546/23838 completed (loss: 0.3016968369483948, acc: 0.9453125)
[2025-02-16 13:45:31,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:31,941][root][INFO] - Training Epoch: 1/2, step 14547/23838 completed (loss: 0.5577638745307922, acc: 0.8243243098258972)
[2025-02-16 13:45:32,121][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:32,359][root][INFO] - Training Epoch: 1/2, step 14548/23838 completed (loss: 0.06805861741304398, acc: 0.97826087474823)
[2025-02-16 13:45:32,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:32,911][root][INFO] - Training Epoch: 1/2, step 14549/23838 completed (loss: 0.39897236227989197, acc: 0.884393036365509)
[2025-02-16 13:45:33,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:33,345][root][INFO] - Training Epoch: 1/2, step 14550/23838 completed (loss: 0.2197319120168686, acc: 0.9571428298950195)
[2025-02-16 13:45:33,479][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:33,709][root][INFO] - Training Epoch: 1/2, step 14551/23838 completed (loss: 0.21646039187908173, acc: 0.9450549483299255)
[2025-02-16 13:45:33,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:34,108][root][INFO] - Training Epoch: 1/2, step 14552/23838 completed (loss: 0.2511088252067566, acc: 0.9420289993286133)
[2025-02-16 13:45:34,341][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:34,650][root][INFO] - Training Epoch: 1/2, step 14553/23838 completed (loss: 0.08188163489103317, acc: 0.9776119589805603)
[2025-02-16 13:45:34,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:35,083][root][INFO] - Training Epoch: 1/2, step 14554/23838 completed (loss: 0.1577470451593399, acc: 0.9402984976768494)
[2025-02-16 13:45:35,259][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:35,502][root][INFO] - Training Epoch: 1/2, step 14555/23838 completed (loss: 0.14255203306674957, acc: 0.945652186870575)
[2025-02-16 13:45:35,733][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:36,027][root][INFO] - Training Epoch: 1/2, step 14556/23838 completed (loss: 0.06593174487352371, acc: 0.9940828680992126)
[2025-02-16 13:45:36,261][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:36,495][root][INFO] - Training Epoch: 1/2, step 14557/23838 completed (loss: 0.17621013522148132, acc: 0.9577465057373047)
[2025-02-16 13:45:36,970][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:37,424][root][INFO] - Training Epoch: 1/2, step 14558/23838 completed (loss: 0.14392702281475067, acc: 0.9504950642585754)
[2025-02-16 13:45:37,697][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:37,943][root][INFO] - Training Epoch: 1/2, step 14559/23838 completed (loss: 0.11990834027528763, acc: 0.9725274443626404)
[2025-02-16 13:45:38,146][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:38,419][root][INFO] - Training Epoch: 1/2, step 14560/23838 completed (loss: 0.1465805470943451, acc: 0.9599999785423279)
[2025-02-16 13:45:38,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:38,874][root][INFO] - Training Epoch: 1/2, step 14561/23838 completed (loss: 0.24608200788497925, acc: 0.9281045794487)
[2025-02-16 13:45:39,110][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:39,356][root][INFO] - Training Epoch: 1/2, step 14562/23838 completed (loss: 0.31115928292274475, acc: 0.9259259104728699)
[2025-02-16 13:45:39,570][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:39,815][root][INFO] - Training Epoch: 1/2, step 14563/23838 completed (loss: 0.062203940004110336, acc: 0.9742268323898315)
[2025-02-16 13:45:40,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:40,269][root][INFO] - Training Epoch: 1/2, step 14564/23838 completed (loss: 0.08395608514547348, acc: 0.9767441749572754)
[2025-02-16 13:45:40,480][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:40,729][root][INFO] - Training Epoch: 1/2, step 14565/23838 completed (loss: 0.18440872430801392, acc: 0.9459459185600281)
[2025-02-16 13:45:40,905][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:41,126][root][INFO] - Training Epoch: 1/2, step 14566/23838 completed (loss: 0.4352419376373291, acc: 0.8865979313850403)
[2025-02-16 13:45:41,311][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:41,569][root][INFO] - Training Epoch: 1/2, step 14567/23838 completed (loss: 0.19469542801380157, acc: 0.9587628841400146)
[2025-02-16 13:45:41,759][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:41,995][root][INFO] - Training Epoch: 1/2, step 14568/23838 completed (loss: 0.4940401315689087, acc: 0.8933333158493042)
[2025-02-16 13:45:42,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:42,466][root][INFO] - Training Epoch: 1/2, step 14569/23838 completed (loss: 0.18932227790355682, acc: 0.9379844665527344)
[2025-02-16 13:45:42,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:43,091][root][INFO] - Training Epoch: 1/2, step 14570/23838 completed (loss: 0.05493144318461418, acc: 0.9821428656578064)
[2025-02-16 13:45:43,309][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:43,575][root][INFO] - Training Epoch: 1/2, step 14571/23838 completed (loss: 0.14836862683296204, acc: 0.9417475461959839)
[2025-02-16 13:45:43,749][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:44,036][root][INFO] - Training Epoch: 1/2, step 14572/23838 completed (loss: 0.14321187138557434, acc: 0.95652174949646)
[2025-02-16 13:45:44,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:44,475][root][INFO] - Training Epoch: 1/2, step 14573/23838 completed (loss: 0.10056135058403015, acc: 0.9754098653793335)
[2025-02-16 13:45:44,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:44,944][root][INFO] - Training Epoch: 1/2, step 14574/23838 completed (loss: 0.2336307317018509, acc: 0.9470198750495911)
[2025-02-16 13:45:45,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:45,397][root][INFO] - Training Epoch: 1/2, step 14575/23838 completed (loss: 0.23815520107746124, acc: 0.9462365508079529)
[2025-02-16 13:45:45,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:45,804][root][INFO] - Training Epoch: 1/2, step 14576/23838 completed (loss: 0.1583770364522934, acc: 0.9492753744125366)
[2025-02-16 13:45:45,983][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:46,216][root][INFO] - Training Epoch: 1/2, step 14577/23838 completed (loss: 0.16230539977550507, acc: 0.9507042169570923)
[2025-02-16 13:45:46,355][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:46,601][root][INFO] - Training Epoch: 1/2, step 14578/23838 completed (loss: 0.42021363973617554, acc: 0.9084967374801636)
[2025-02-16 13:45:46,742][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:46,965][root][INFO] - Training Epoch: 1/2, step 14579/23838 completed (loss: 0.20009799301624298, acc: 0.9473684430122375)
[2025-02-16 13:45:47,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:47,378][root][INFO] - Training Epoch: 1/2, step 14580/23838 completed (loss: 0.20195451378822327, acc: 0.9324324131011963)
[2025-02-16 13:45:47,519][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:47,807][root][INFO] - Training Epoch: 1/2, step 14581/23838 completed (loss: 0.27782177925109863, acc: 0.9090909361839294)
[2025-02-16 13:45:48,019][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:48,237][root][INFO] - Training Epoch: 1/2, step 14582/23838 completed (loss: 0.333398699760437, acc: 0.9041095972061157)
[2025-02-16 13:45:48,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:48,664][root][INFO] - Training Epoch: 1/2, step 14583/23838 completed (loss: 0.10018133372068405, acc: 0.9801980257034302)
[2025-02-16 13:45:48,939][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:49,195][root][INFO] - Training Epoch: 1/2, step 14584/23838 completed (loss: 0.3398158550262451, acc: 0.9166666865348816)
[2025-02-16 13:45:49,397][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:49,628][root][INFO] - Training Epoch: 1/2, step 14585/23838 completed (loss: 0.15157797932624817, acc: 0.9492753744125366)
[2025-02-16 13:45:49,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:50,039][root][INFO] - Training Epoch: 1/2, step 14586/23838 completed (loss: 0.10424748063087463, acc: 0.9634146094322205)
[2025-02-16 13:45:50,274][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:50,517][root][INFO] - Training Epoch: 1/2, step 14587/23838 completed (loss: 0.31589826941490173, acc: 0.9236640930175781)
[2025-02-16 13:45:50,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:50,982][root][INFO] - Training Epoch: 1/2, step 14588/23838 completed (loss: 0.27763161063194275, acc: 0.9177215099334717)
[2025-02-16 13:45:51,167][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:51,402][root][INFO] - Training Epoch: 1/2, step 14589/23838 completed (loss: 0.09905888885259628, acc: 0.9664804339408875)
[2025-02-16 13:45:51,604][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:51,843][root][INFO] - Training Epoch: 1/2, step 14590/23838 completed (loss: 0.10650455206632614, acc: 0.9746835231781006)
[2025-02-16 13:45:52,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:52,340][root][INFO] - Training Epoch: 1/2, step 14591/23838 completed (loss: 0.3010701835155487, acc: 0.9230769276618958)
[2025-02-16 13:45:52,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:52,795][root][INFO] - Training Epoch: 1/2, step 14592/23838 completed (loss: 0.4175928831100464, acc: 0.898876428604126)
[2025-02-16 13:45:53,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:53,655][root][INFO] - Training Epoch: 1/2, step 14593/23838 completed (loss: 0.10039367526769638, acc: 0.9764150977134705)
[2025-02-16 13:45:53,786][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:54,024][root][INFO] - Training Epoch: 1/2, step 14594/23838 completed (loss: 0.11188856512308121, acc: 0.9716981053352356)
[2025-02-16 13:45:54,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:54,440][root][INFO] - Training Epoch: 1/2, step 14595/23838 completed (loss: 0.20426012575626373, acc: 0.9433962106704712)
[2025-02-16 13:45:54,577][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:54,804][root][INFO] - Training Epoch: 1/2, step 14596/23838 completed (loss: 0.09624259173870087, acc: 0.9772727489471436)
[2025-02-16 13:45:55,114][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:55,410][root][INFO] - Training Epoch: 1/2, step 14597/23838 completed (loss: 0.24992436170578003, acc: 0.9491525292396545)
[2025-02-16 13:45:55,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:55,839][root][INFO] - Training Epoch: 1/2, step 14598/23838 completed (loss: 0.19161772727966309, acc: 0.9651162624359131)
[2025-02-16 13:45:55,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:56,224][root][INFO] - Training Epoch: 1/2, step 14599/23838 completed (loss: 0.1648710072040558, acc: 0.9629629850387573)
[2025-02-16 13:45:56,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:56,670][root][INFO] - Training Epoch: 1/2, step 14600/23838 completed (loss: 0.0844055786728859, acc: 0.9791666865348816)
[2025-02-16 13:45:56,897][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:57,131][root][INFO] - Training Epoch: 1/2, step 14601/23838 completed (loss: 0.13986361026763916, acc: 0.9518072009086609)
[2025-02-16 13:45:57,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:57,559][root][INFO] - Training Epoch: 1/2, step 14602/23838 completed (loss: 0.10361797362565994, acc: 0.9622641801834106)
[2025-02-16 13:45:57,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:57,975][root][INFO] - Training Epoch: 1/2, step 14603/23838 completed (loss: 0.08159028738737106, acc: 0.9774436354637146)
[2025-02-16 13:45:58,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:58,408][root][INFO] - Training Epoch: 1/2, step 14604/23838 completed (loss: 0.27885904908180237, acc: 0.9509803652763367)
[2025-02-16 13:45:58,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:58,767][root][INFO] - Training Epoch: 1/2, step 14605/23838 completed (loss: 0.1534620076417923, acc: 0.9756097793579102)
[2025-02-16 13:45:58,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:59,134][root][INFO] - Training Epoch: 1/2, step 14606/23838 completed (loss: 0.2093108743429184, acc: 0.9333333373069763)
[2025-02-16 13:45:59,347][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:59,589][root][INFO] - Training Epoch: 1/2, step 14607/23838 completed (loss: 0.2834915220737457, acc: 0.9322034120559692)
[2025-02-16 13:45:59,754][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:45:59,983][root][INFO] - Training Epoch: 1/2, step 14608/23838 completed (loss: 0.17168563604354858, acc: 0.954954981803894)
[2025-02-16 13:46:00,241][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:00,483][root][INFO] - Training Epoch: 1/2, step 14609/23838 completed (loss: 0.15934033691883087, acc: 0.9485294222831726)
[2025-02-16 13:46:00,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:00,838][root][INFO] - Training Epoch: 1/2, step 14610/23838 completed (loss: 0.38333308696746826, acc: 0.8846153616905212)
[2025-02-16 13:46:01,003][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:01,226][root][INFO] - Training Epoch: 1/2, step 14611/23838 completed (loss: 0.1470683366060257, acc: 0.9655172228813171)
[2025-02-16 13:46:01,468][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:01,724][root][INFO] - Training Epoch: 1/2, step 14612/23838 completed (loss: 0.31391194462776184, acc: 0.8778625726699829)
[2025-02-16 13:46:01,963][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:02,217][root][INFO] - Training Epoch: 1/2, step 14613/23838 completed (loss: 0.14863137900829315, acc: 0.9670329689979553)
[2025-02-16 13:46:02,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:02,680][root][INFO] - Training Epoch: 1/2, step 14614/23838 completed (loss: 0.17382723093032837, acc: 0.9230769276618958)
[2025-02-16 13:46:02,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:03,196][root][INFO] - Training Epoch: 1/2, step 14615/23838 completed (loss: 0.12924548983573914, acc: 0.96875)
[2025-02-16 13:46:03,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:03,718][root][INFO] - Training Epoch: 1/2, step 14616/23838 completed (loss: 0.18946443498134613, acc: 0.9526315927505493)
[2025-02-16 13:46:03,936][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:04,167][root][INFO] - Training Epoch: 1/2, step 14617/23838 completed (loss: 0.0985722616314888, acc: 0.9638554453849792)
[2025-02-16 13:46:04,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:04,857][root][INFO] - Training Epoch: 1/2, step 14618/23838 completed (loss: 0.11538924276828766, acc: 0.9704142212867737)
[2025-02-16 13:46:05,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:05,845][root][INFO] - Training Epoch: 1/2, step 14619/23838 completed (loss: 0.1858459860086441, acc: 0.9503546357154846)
[2025-02-16 13:46:06,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:06,605][root][INFO] - Training Epoch: 1/2, step 14620/23838 completed (loss: 0.043312717229127884, acc: 0.9878048896789551)
[2025-02-16 13:46:06,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:07,147][root][INFO] - Training Epoch: 1/2, step 14621/23838 completed (loss: 0.3929383456707001, acc: 0.9173553586006165)
[2025-02-16 13:46:07,515][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:07,883][root][INFO] - Training Epoch: 1/2, step 14622/23838 completed (loss: 0.07635258883237839, acc: 0.9744681119918823)
[2025-02-16 13:46:08,091][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:08,335][root][INFO] - Training Epoch: 1/2, step 14623/23838 completed (loss: 0.2055765837430954, acc: 0.9347826242446899)
[2025-02-16 13:46:08,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:08,915][root][INFO] - Training Epoch: 1/2, step 14624/23838 completed (loss: 0.43409377336502075, acc: 0.8783783912658691)
[2025-02-16 13:46:09,118][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:09,372][root][INFO] - Training Epoch: 1/2, step 14625/23838 completed (loss: 0.33369147777557373, acc: 0.9060773253440857)
[2025-02-16 13:46:09,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:09,861][root][INFO] - Training Epoch: 1/2, step 14626/23838 completed (loss: 0.16719545423984528, acc: 0.9534883499145508)
[2025-02-16 13:46:10,073][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:10,326][root][INFO] - Training Epoch: 1/2, step 14627/23838 completed (loss: 0.25848475098609924, acc: 0.9379310607910156)
[2025-02-16 13:46:10,528][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:10,753][root][INFO] - Training Epoch: 1/2, step 14628/23838 completed (loss: 0.1268421709537506, acc: 0.967391312122345)
[2025-02-16 13:46:10,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:11,120][root][INFO] - Training Epoch: 1/2, step 14629/23838 completed (loss: 0.02275977097451687, acc: 0.9919999837875366)
[2025-02-16 13:46:11,306][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:11,529][root][INFO] - Training Epoch: 1/2, step 14630/23838 completed (loss: 0.18221409618854523, acc: 0.9435483813285828)
[2025-02-16 13:46:11,685][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:11,905][root][INFO] - Training Epoch: 1/2, step 14631/23838 completed (loss: 0.23617257177829742, acc: 0.9514563083648682)
[2025-02-16 13:46:12,218][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:12,525][root][INFO] - Training Epoch: 1/2, step 14632/23838 completed (loss: 0.1191270723938942, acc: 0.9790209531784058)
[2025-02-16 13:46:12,711][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:12,957][root][INFO] - Training Epoch: 1/2, step 14633/23838 completed (loss: 0.2477979212999344, acc: 0.9125000238418579)
[2025-02-16 13:46:13,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:13,425][root][INFO] - Training Epoch: 1/2, step 14634/23838 completed (loss: 0.2579919695854187, acc: 0.930232584476471)
[2025-02-16 13:46:13,617][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:13,855][root][INFO] - Training Epoch: 1/2, step 14635/23838 completed (loss: 0.19134512543678284, acc: 0.936170220375061)
[2025-02-16 13:46:14,029][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:14,253][root][INFO] - Training Epoch: 1/2, step 14636/23838 completed (loss: 0.38681328296661377, acc: 0.8989899158477783)
[2025-02-16 13:46:14,442][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:14,702][root][INFO] - Training Epoch: 1/2, step 14637/23838 completed (loss: 0.39628615975379944, acc: 0.8765432238578796)
[2025-02-16 13:46:14,914][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:15,145][root][INFO] - Training Epoch: 1/2, step 14638/23838 completed (loss: 0.15240046381950378, acc: 0.9495798349380493)
[2025-02-16 13:46:15,359][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:15,599][root][INFO] - Training Epoch: 1/2, step 14639/23838 completed (loss: 0.20495975017547607, acc: 0.9685039520263672)
[2025-02-16 13:46:15,838][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:16,043][root][INFO] - Training Epoch: 1/2, step 14640/23838 completed (loss: 0.3223206698894501, acc: 0.8863636255264282)
[2025-02-16 13:46:16,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:16,520][root][INFO] - Training Epoch: 1/2, step 14641/23838 completed (loss: 0.3586878478527069, acc: 0.9090909361839294)
[2025-02-16 13:46:16,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:16,926][root][INFO] - Training Epoch: 1/2, step 14642/23838 completed (loss: 0.39885038137435913, acc: 0.8928571343421936)
[2025-02-16 13:46:17,124][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:17,366][root][INFO] - Training Epoch: 1/2, step 14643/23838 completed (loss: 0.1947513222694397, acc: 0.95652174949646)
[2025-02-16 13:46:17,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:17,801][root][INFO] - Training Epoch: 1/2, step 14644/23838 completed (loss: 0.5718554258346558, acc: 0.8235294222831726)
[2025-02-16 13:46:17,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:18,202][root][INFO] - Training Epoch: 1/2, step 14645/23838 completed (loss: 0.30325090885162354, acc: 0.8769230842590332)
[2025-02-16 13:46:18,374][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:18,617][root][INFO] - Training Epoch: 1/2, step 14646/23838 completed (loss: 0.08771681785583496, acc: 0.9868420958518982)
[2025-02-16 13:46:18,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:19,041][root][INFO] - Training Epoch: 1/2, step 14647/23838 completed (loss: 0.17492225766181946, acc: 0.9682539701461792)
[2025-02-16 13:46:19,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:19,422][root][INFO] - Training Epoch: 1/2, step 14648/23838 completed (loss: 0.41331005096435547, acc: 0.8958333134651184)
[2025-02-16 13:46:19,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:19,828][root][INFO] - Training Epoch: 1/2, step 14649/23838 completed (loss: 0.3596556782722473, acc: 0.8620689511299133)
[2025-02-16 13:46:20,011][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:20,247][root][INFO] - Training Epoch: 1/2, step 14650/23838 completed (loss: 0.8907198309898376, acc: 0.800000011920929)
[2025-02-16 13:46:20,392][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:20,607][root][INFO] - Training Epoch: 1/2, step 14651/23838 completed (loss: 0.49285683035850525, acc: 0.8857142925262451)
[2025-02-16 13:46:20,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:20,987][root][INFO] - Training Epoch: 1/2, step 14652/23838 completed (loss: 1.3245526552200317, acc: 0.625)
[2025-02-16 13:46:21,134][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:21,384][root][INFO] - Training Epoch: 1/2, step 14653/23838 completed (loss: 0.9172552824020386, acc: 0.7222222089767456)
[2025-02-16 13:46:21,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:21,922][root][INFO] - Training Epoch: 1/2, step 14654/23838 completed (loss: 1.0948506593704224, acc: 0.6590909361839294)
[2025-02-16 13:46:22,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:22,306][root][INFO] - Training Epoch: 1/2, step 14655/23838 completed (loss: 1.4244641065597534, acc: 0.5862069129943848)
[2025-02-16 13:46:22,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:22,775][root][INFO] - Training Epoch: 1/2, step 14656/23838 completed (loss: 1.1703736782073975, acc: 0.75)
[2025-02-16 13:46:22,930][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:23,156][root][INFO] - Training Epoch: 1/2, step 14657/23838 completed (loss: 1.2279701232910156, acc: 0.6499999761581421)
[2025-02-16 13:46:23,314][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:23,540][root][INFO] - Training Epoch: 1/2, step 14658/23838 completed (loss: 0.9971296787261963, acc: 0.7317073345184326)
[2025-02-16 13:46:23,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:23,931][root][INFO] - Training Epoch: 1/2, step 14659/23838 completed (loss: 1.1114559173583984, acc: 0.6969696879386902)
[2025-02-16 13:46:24,068][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:24,298][root][INFO] - Training Epoch: 1/2, step 14660/23838 completed (loss: 1.1185283660888672, acc: 0.739130437374115)
[2025-02-16 13:46:24,470][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:24,699][root][INFO] - Training Epoch: 1/2, step 14661/23838 completed (loss: 1.4971365928649902, acc: 0.6382978558540344)
[2025-02-16 13:46:24,889][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:25,119][root][INFO] - Training Epoch: 1/2, step 14662/23838 completed (loss: 0.882131040096283, acc: 0.7037037014961243)
[2025-02-16 13:46:25,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:25,551][root][INFO] - Training Epoch: 1/2, step 14663/23838 completed (loss: 0.6876806020736694, acc: 0.7692307829856873)
[2025-02-16 13:46:25,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:25,930][root][INFO] - Training Epoch: 1/2, step 14664/23838 completed (loss: 0.7298881411552429, acc: 0.8235294222831726)
[2025-02-16 13:46:26,145][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:26,384][root][INFO] - Training Epoch: 1/2, step 14665/23838 completed (loss: 0.646590530872345, acc: 0.7878788113594055)
[2025-02-16 13:46:26,560][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:26,809][root][INFO] - Training Epoch: 1/2, step 14666/23838 completed (loss: 1.742639183998108, acc: 0.6111111044883728)
[2025-02-16 13:46:26,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:27,202][root][INFO] - Training Epoch: 1/2, step 14667/23838 completed (loss: 0.969618558883667, acc: 0.65625)
[2025-02-16 13:46:27,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:27,611][root][INFO] - Training Epoch: 1/2, step 14668/23838 completed (loss: 0.9331578612327576, acc: 0.800000011920929)
[2025-02-16 13:46:27,827][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:28,054][root][INFO] - Training Epoch: 1/2, step 14669/23838 completed (loss: 0.8323667645454407, acc: 0.6842105388641357)
[2025-02-16 13:46:28,277][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:28,516][root][INFO] - Training Epoch: 1/2, step 14670/23838 completed (loss: 1.3495441675186157, acc: 0.6551724076271057)
[2025-02-16 13:46:28,682][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:28,922][root][INFO] - Training Epoch: 1/2, step 14671/23838 completed (loss: 1.0994293689727783, acc: 0.65625)
[2025-02-16 13:46:29,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:29,399][root][INFO] - Training Epoch: 1/2, step 14672/23838 completed (loss: 1.5604408979415894, acc: 0.6666666865348816)
[2025-02-16 13:46:29,555][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:29,772][root][INFO] - Training Epoch: 1/2, step 14673/23838 completed (loss: 1.204604983329773, acc: 0.6875)
[2025-02-16 13:46:30,001][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:30,261][root][INFO] - Training Epoch: 1/2, step 14674/23838 completed (loss: 0.6132084131240845, acc: 0.8235294222831726)
[2025-02-16 13:46:30,453][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:30,719][root][INFO] - Training Epoch: 1/2, step 14675/23838 completed (loss: 0.16431924700737, acc: 0.942307710647583)
[2025-02-16 13:46:30,946][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:31,184][root][INFO] - Training Epoch: 1/2, step 14676/23838 completed (loss: 0.5539051294326782, acc: 0.8765432238578796)
[2025-02-16 13:46:31,435][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:31,704][root][INFO] - Training Epoch: 1/2, step 14677/23838 completed (loss: 0.4898560345172882, acc: 0.890625)
[2025-02-16 13:46:31,968][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:32,202][root][INFO] - Training Epoch: 1/2, step 14678/23838 completed (loss: 0.13973645865917206, acc: 0.9811320900917053)
[2025-02-16 13:46:32,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:33,186][root][INFO] - Training Epoch: 1/2, step 14679/23838 completed (loss: 0.42925554513931274, acc: 0.8809523582458496)
[2025-02-16 13:46:33,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:33,666][root][INFO] - Training Epoch: 1/2, step 14680/23838 completed (loss: 0.36574679613113403, acc: 0.8974359035491943)
[2025-02-16 13:46:33,920][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:34,167][root][INFO] - Training Epoch: 1/2, step 14681/23838 completed (loss: 0.5711377263069153, acc: 0.8695651888847351)
[2025-02-16 13:46:34,381][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:34,623][root][INFO] - Training Epoch: 1/2, step 14682/23838 completed (loss: 0.24529531598091125, acc: 0.9259259104728699)
[2025-02-16 13:46:34,822][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:35,046][root][INFO] - Training Epoch: 1/2, step 14683/23838 completed (loss: 0.8010638952255249, acc: 0.7692307829856873)
[2025-02-16 13:46:35,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:35,455][root][INFO] - Training Epoch: 1/2, step 14684/23838 completed (loss: 1.1014701128005981, acc: 0.6000000238418579)
[2025-02-16 13:46:35,628][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:35,862][root][INFO] - Training Epoch: 1/2, step 14685/23838 completed (loss: 0.29492777585983276, acc: 0.9166666865348816)
[2025-02-16 13:46:36,237][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:36,598][root][INFO] - Training Epoch: 1/2, step 14686/23838 completed (loss: 0.19786633551120758, acc: 0.9328358173370361)
[2025-02-16 13:46:36,734][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:36,957][root][INFO] - Training Epoch: 1/2, step 14687/23838 completed (loss: 0.18603865802288055, acc: 0.97826087474823)
[2025-02-16 13:46:37,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:37,354][root][INFO] - Training Epoch: 1/2, step 14688/23838 completed (loss: 0.8755224943161011, acc: 0.7538461685180664)
[2025-02-16 13:46:37,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:37,720][root][INFO] - Training Epoch: 1/2, step 14689/23838 completed (loss: 0.7203688621520996, acc: 0.8571428656578064)
[2025-02-16 13:46:37,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:38,102][root][INFO] - Training Epoch: 1/2, step 14690/23838 completed (loss: 0.2667785584926605, acc: 0.9142857193946838)
[2025-02-16 13:46:38,362][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:38,657][root][INFO] - Training Epoch: 1/2, step 14691/23838 completed (loss: 0.8668641448020935, acc: 0.8399999737739563)
[2025-02-16 13:46:38,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:39,078][root][INFO] - Training Epoch: 1/2, step 14692/23838 completed (loss: 0.6272005438804626, acc: 0.8275862336158752)
[2025-02-16 13:46:39,278][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:39,506][root][INFO] - Training Epoch: 1/2, step 14693/23838 completed (loss: 0.23890748620033264, acc: 0.9090909361839294)
[2025-02-16 13:46:39,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:40,078][root][INFO] - Training Epoch: 1/2, step 14694/23838 completed (loss: 0.19440169632434845, acc: 0.9130434989929199)
[2025-02-16 13:46:40,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:40,519][root][INFO] - Training Epoch: 1/2, step 14695/23838 completed (loss: 0.2962617576122284, acc: 0.9629629850387573)
[2025-02-16 13:46:40,692][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:40,916][root][INFO] - Training Epoch: 1/2, step 14696/23838 completed (loss: 0.14982552826404572, acc: 0.9649122953414917)
[2025-02-16 13:46:41,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:41,308][root][INFO] - Training Epoch: 1/2, step 14697/23838 completed (loss: 0.6020627021789551, acc: 0.8367347121238708)
[2025-02-16 13:46:41,538][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:41,763][root][INFO] - Training Epoch: 1/2, step 14698/23838 completed (loss: 0.015542831271886826, acc: 1.0)
[2025-02-16 13:46:42,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:42,624][root][INFO] - Training Epoch: 1/2, step 14699/23838 completed (loss: 0.16714948415756226, acc: 0.9629629850387573)
[2025-02-16 13:46:42,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:43,143][root][INFO] - Training Epoch: 1/2, step 14700/23838 completed (loss: 0.35520419478416443, acc: 0.9473684430122375)
[2025-02-16 13:46:43,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:43,589][root][INFO] - Training Epoch: 1/2, step 14701/23838 completed (loss: 0.3363596498966217, acc: 0.918367326259613)
[2025-02-16 13:46:43,776][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:43,996][root][INFO] - Training Epoch: 1/2, step 14702/23838 completed (loss: 0.07237860560417175, acc: 1.0)
[2025-02-16 13:46:44,180][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:44,406][root][INFO] - Training Epoch: 1/2, step 14703/23838 completed (loss: 0.7154725193977356, acc: 0.807692289352417)
[2025-02-16 13:46:44,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:44,863][root][INFO] - Training Epoch: 1/2, step 14704/23838 completed (loss: 0.6047500371932983, acc: 0.8888888955116272)
[2025-02-16 13:46:45,473][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:45,973][root][INFO] - Training Epoch: 1/2, step 14705/23838 completed (loss: 0.9876687526702881, acc: 0.7317073345184326)
[2025-02-16 13:46:46,239][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:46,514][root][INFO] - Training Epoch: 1/2, step 14706/23838 completed (loss: 0.5907696485519409, acc: 0.8292682766914368)
[2025-02-16 13:46:46,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:47,371][root][INFO] - Training Epoch: 1/2, step 14707/23838 completed (loss: 0.8987204432487488, acc: 0.790123462677002)
[2025-02-16 13:46:47,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:48,161][root][INFO] - Training Epoch: 1/2, step 14708/23838 completed (loss: 0.7532904744148254, acc: 0.8163265585899353)
[2025-02-16 13:46:48,439][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:48,671][root][INFO] - Training Epoch: 1/2, step 14709/23838 completed (loss: 1.802329182624817, acc: 0.5588235259056091)
[2025-02-16 13:46:48,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:49,142][root][INFO] - Training Epoch: 1/2, step 14710/23838 completed (loss: 0.6943116188049316, acc: 0.78125)
[2025-02-16 13:46:49,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:49,602][root][INFO] - Training Epoch: 1/2, step 14711/23838 completed (loss: 1.3720426559448242, acc: 0.6304348111152649)
[2025-02-16 13:46:49,791][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:50,030][root][INFO] - Training Epoch: 1/2, step 14712/23838 completed (loss: 0.7904263734817505, acc: 0.7058823704719543)
[2025-02-16 13:46:50,250][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:50,501][root][INFO] - Training Epoch: 1/2, step 14713/23838 completed (loss: 1.8786559104919434, acc: 0.4693877696990967)
[2025-02-16 13:46:50,705][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:50,941][root][INFO] - Training Epoch: 1/2, step 14714/23838 completed (loss: 0.8133695721626282, acc: 0.8055555820465088)
[2025-02-16 13:46:51,301][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:51,609][root][INFO] - Training Epoch: 1/2, step 14715/23838 completed (loss: 1.2647185325622559, acc: 0.6818181872367859)
[2025-02-16 13:46:51,879][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:52,157][root][INFO] - Training Epoch: 1/2, step 14716/23838 completed (loss: 1.0945707559585571, acc: 0.7419354915618896)
[2025-02-16 13:46:52,418][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:52,686][root][INFO] - Training Epoch: 1/2, step 14717/23838 completed (loss: 1.0263476371765137, acc: 0.75)
[2025-02-16 13:46:53,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:53,455][root][INFO] - Training Epoch: 1/2, step 14718/23838 completed (loss: 0.9784677624702454, acc: 0.7272727489471436)
[2025-02-16 13:46:53,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:53,987][root][INFO] - Training Epoch: 1/2, step 14719/23838 completed (loss: 0.469865620136261, acc: 0.8863636255264282)
[2025-02-16 13:46:54,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:54,464][root][INFO] - Training Epoch: 1/2, step 14720/23838 completed (loss: 1.040183186531067, acc: 0.7230769395828247)
[2025-02-16 13:46:54,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:54,947][root][INFO] - Training Epoch: 1/2, step 14721/23838 completed (loss: 0.5860688090324402, acc: 0.8333333134651184)
[2025-02-16 13:46:55,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:55,427][root][INFO] - Training Epoch: 1/2, step 14722/23838 completed (loss: 0.6184216141700745, acc: 0.8947368264198303)
[2025-02-16 13:46:55,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:55,933][root][INFO] - Training Epoch: 1/2, step 14723/23838 completed (loss: 1.173976182937622, acc: 0.7049180269241333)
[2025-02-16 13:46:56,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:56,661][root][INFO] - Training Epoch: 1/2, step 14724/23838 completed (loss: 0.8919398784637451, acc: 0.807692289352417)
[2025-02-16 13:46:57,280][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:57,752][root][INFO] - Training Epoch: 1/2, step 14725/23838 completed (loss: 0.9364146590232849, acc: 0.7870370149612427)
[2025-02-16 13:46:57,978][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:58,247][root][INFO] - Training Epoch: 1/2, step 14726/23838 completed (loss: 1.6733818054199219, acc: 0.6499999761581421)
[2025-02-16 13:46:58,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:58,678][root][INFO] - Training Epoch: 1/2, step 14727/23838 completed (loss: 1.228338360786438, acc: 0.65625)
[2025-02-16 13:46:58,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:59,196][root][INFO] - Training Epoch: 1/2, step 14728/23838 completed (loss: 1.3310688734054565, acc: 0.7272727489471436)
[2025-02-16 13:46:59,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:46:59,662][root][INFO] - Training Epoch: 1/2, step 14729/23838 completed (loss: 1.1822751760482788, acc: 0.692307710647583)
[2025-02-16 13:46:59,843][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:00,093][root][INFO] - Training Epoch: 1/2, step 14730/23838 completed (loss: 1.1677906513214111, acc: 0.7209302186965942)
[2025-02-16 13:47:00,390][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:00,652][root][INFO] - Training Epoch: 1/2, step 14731/23838 completed (loss: 1.1045249700546265, acc: 0.7272727489471436)
[2025-02-16 13:47:00,828][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:01,065][root][INFO] - Training Epoch: 1/2, step 14732/23838 completed (loss: 0.5477471947669983, acc: 0.8837209343910217)
[2025-02-16 13:47:01,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:01,521][root][INFO] - Training Epoch: 1/2, step 14733/23838 completed (loss: 0.7998089790344238, acc: 0.7358490824699402)
[2025-02-16 13:47:01,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:01,938][root][INFO] - Training Epoch: 1/2, step 14734/23838 completed (loss: 0.8299999833106995, acc: 0.8235294222831726)
[2025-02-16 13:47:02,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:02,433][root][INFO] - Training Epoch: 1/2, step 14735/23838 completed (loss: 0.64655601978302, acc: 0.8266666531562805)
[2025-02-16 13:47:02,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:03,018][root][INFO] - Training Epoch: 1/2, step 14736/23838 completed (loss: 0.6839533448219299, acc: 0.8169013857841492)
[2025-02-16 13:47:03,242][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:03,494][root][INFO] - Training Epoch: 1/2, step 14737/23838 completed (loss: 0.44523945450782776, acc: 0.875)
[2025-02-16 13:47:03,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:04,018][root][INFO] - Training Epoch: 1/2, step 14738/23838 completed (loss: 0.6531962156295776, acc: 0.8275862336158752)
[2025-02-16 13:47:04,236][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:04,487][root][INFO] - Training Epoch: 1/2, step 14739/23838 completed (loss: 0.4882450997829437, acc: 0.8604651093482971)
[2025-02-16 13:47:04,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:04,878][root][INFO] - Training Epoch: 1/2, step 14740/23838 completed (loss: 0.8815019726753235, acc: 0.7755101919174194)
[2025-02-16 13:47:05,059][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:05,300][root][INFO] - Training Epoch: 1/2, step 14741/23838 completed (loss: 0.45231568813323975, acc: 0.8888888955116272)
[2025-02-16 13:47:05,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:05,781][root][INFO] - Training Epoch: 1/2, step 14742/23838 completed (loss: 0.34059348702430725, acc: 0.8857142925262451)
[2025-02-16 13:47:06,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:06,260][root][INFO] - Training Epoch: 1/2, step 14743/23838 completed (loss: 1.214060664176941, acc: 0.699999988079071)
[2025-02-16 13:47:06,461][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:06,701][root][INFO] - Training Epoch: 1/2, step 14744/23838 completed (loss: 0.7257209420204163, acc: 0.7941176295280457)
[2025-02-16 13:47:06,888][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:07,115][root][INFO] - Training Epoch: 1/2, step 14745/23838 completed (loss: 0.4008525609970093, acc: 0.9056603908538818)
[2025-02-16 13:47:07,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:07,647][root][INFO] - Training Epoch: 1/2, step 14746/23838 completed (loss: 0.8616312146186829, acc: 0.7735849022865295)
[2025-02-16 13:47:07,790][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:08,085][root][INFO] - Training Epoch: 1/2, step 14747/23838 completed (loss: 0.8271510004997253, acc: 0.843137264251709)
[2025-02-16 13:47:08,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:08,554][root][INFO] - Training Epoch: 1/2, step 14748/23838 completed (loss: 0.25443562865257263, acc: 0.9130434989929199)
[2025-02-16 13:47:08,726][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:08,944][root][INFO] - Training Epoch: 1/2, step 14749/23838 completed (loss: 1.370106816291809, acc: 0.6785714030265808)
[2025-02-16 13:47:09,106][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:09,333][root][INFO] - Training Epoch: 1/2, step 14750/23838 completed (loss: 1.2379921674728394, acc: 0.6774193644523621)
[2025-02-16 13:47:09,488][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:09,731][root][INFO] - Training Epoch: 1/2, step 14751/23838 completed (loss: 0.9972362518310547, acc: 0.7045454382896423)
[2025-02-16 13:47:09,919][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:10,142][root][INFO] - Training Epoch: 1/2, step 14752/23838 completed (loss: 0.6134969592094421, acc: 0.7659574747085571)
[2025-02-16 13:47:10,365][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:10,607][root][INFO] - Training Epoch: 1/2, step 14753/23838 completed (loss: 0.4305305480957031, acc: 0.8620689511299133)
[2025-02-16 13:47:10,769][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:10,999][root][INFO] - Training Epoch: 1/2, step 14754/23838 completed (loss: 0.6561892032623291, acc: 0.8285714387893677)
[2025-02-16 13:47:11,224][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:11,457][root][INFO] - Training Epoch: 1/2, step 14755/23838 completed (loss: 0.4642169177532196, acc: 0.8833333253860474)
[2025-02-16 13:47:11,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:12,041][root][INFO] - Training Epoch: 1/2, step 14756/23838 completed (loss: 0.3386937379837036, acc: 0.9117646813392639)
[2025-02-16 13:47:12,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:12,596][root][INFO] - Training Epoch: 1/2, step 14757/23838 completed (loss: 0.40459969639778137, acc: 0.8541666865348816)
[2025-02-16 13:47:12,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:13,080][root][INFO] - Training Epoch: 1/2, step 14758/23838 completed (loss: 1.137842059135437, acc: 0.7857142686843872)
[2025-02-16 13:47:13,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:13,637][root][INFO] - Training Epoch: 1/2, step 14759/23838 completed (loss: 0.5721800327301025, acc: 0.8245614171028137)
[2025-02-16 13:47:13,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:14,104][root][INFO] - Training Epoch: 1/2, step 14760/23838 completed (loss: 0.3564000427722931, acc: 0.9166666865348816)
[2025-02-16 13:47:14,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:14,529][root][INFO] - Training Epoch: 1/2, step 14761/23838 completed (loss: 0.9009892344474792, acc: 0.7428571581840515)
[2025-02-16 13:47:14,809][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:15,042][root][INFO] - Training Epoch: 1/2, step 14762/23838 completed (loss: 0.39985355734825134, acc: 0.9074074029922485)
[2025-02-16 13:47:15,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:15,447][root][INFO] - Training Epoch: 1/2, step 14763/23838 completed (loss: 0.6982243657112122, acc: 0.8478260636329651)
[2025-02-16 13:47:15,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:15,857][root][INFO] - Training Epoch: 1/2, step 14764/23838 completed (loss: 0.4974554777145386, acc: 0.8857142925262451)
[2025-02-16 13:47:16,139][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:16,440][root][INFO] - Training Epoch: 1/2, step 14765/23838 completed (loss: 0.742562472820282, acc: 0.800000011920929)
[2025-02-16 13:47:16,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:16,899][root][INFO] - Training Epoch: 1/2, step 14766/23838 completed (loss: 1.0084806680679321, acc: 0.6744186282157898)
[2025-02-16 13:47:17,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:17,478][root][INFO] - Training Epoch: 1/2, step 14767/23838 completed (loss: 0.3572406470775604, acc: 0.9333333373069763)
[2025-02-16 13:47:17,679][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:17,912][root][INFO] - Training Epoch: 1/2, step 14768/23838 completed (loss: 0.29199084639549255, acc: 0.859375)
[2025-02-16 13:47:18,075][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:18,306][root][INFO] - Training Epoch: 1/2, step 14769/23838 completed (loss: 0.8750419020652771, acc: 0.7666666507720947)
[2025-02-16 13:47:18,457][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:18,685][root][INFO] - Training Epoch: 1/2, step 14770/23838 completed (loss: 1.6612558364868164, acc: 0.5588235259056091)
[2025-02-16 13:47:18,950][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:19,230][root][INFO] - Training Epoch: 1/2, step 14771/23838 completed (loss: 0.4123322069644928, acc: 0.8958333134651184)
[2025-02-16 13:47:19,400][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:19,620][root][INFO] - Training Epoch: 1/2, step 14772/23838 completed (loss: 0.8955712914466858, acc: 0.7435897588729858)
[2025-02-16 13:47:19,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:20,102][root][INFO] - Training Epoch: 1/2, step 14773/23838 completed (loss: 1.0884954929351807, acc: 0.7352941036224365)
[2025-02-16 13:47:20,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:20,594][root][INFO] - Training Epoch: 1/2, step 14774/23838 completed (loss: 1.1053684949874878, acc: 0.7222222089767456)
[2025-02-16 13:47:20,824][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:21,062][root][INFO] - Training Epoch: 1/2, step 14775/23838 completed (loss: 0.5749024748802185, acc: 0.8620689511299133)
[2025-02-16 13:47:21,217][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:21,444][root][INFO] - Training Epoch: 1/2, step 14776/23838 completed (loss: 0.6937604546546936, acc: 0.8125)
[2025-02-16 13:47:21,654][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:21,910][root][INFO] - Training Epoch: 1/2, step 14777/23838 completed (loss: 0.33165988326072693, acc: 0.90625)
[2025-02-16 13:47:22,074][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:22,365][root][INFO] - Training Epoch: 1/2, step 14778/23838 completed (loss: 0.27311569452285767, acc: 0.9069767594337463)
[2025-02-16 13:47:22,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:22,841][root][INFO] - Training Epoch: 1/2, step 14779/23838 completed (loss: 0.5188997983932495, acc: 0.8815789222717285)
[2025-02-16 13:47:23,054][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:23,284][root][INFO] - Training Epoch: 1/2, step 14780/23838 completed (loss: 0.7165488600730896, acc: 0.8070175647735596)
[2025-02-16 13:47:23,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:23,689][root][INFO] - Training Epoch: 1/2, step 14781/23838 completed (loss: 0.6618004441261292, acc: 0.8333333134651184)
[2025-02-16 13:47:23,867][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:24,093][root][INFO] - Training Epoch: 1/2, step 14782/23838 completed (loss: 0.22054211795330048, acc: 0.9253731369972229)
[2025-02-16 13:47:24,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:24,473][root][INFO] - Training Epoch: 1/2, step 14783/23838 completed (loss: 0.9817652106285095, acc: 0.6904761791229248)
[2025-02-16 13:47:24,661][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:24,879][root][INFO] - Training Epoch: 1/2, step 14784/23838 completed (loss: 0.31854209303855896, acc: 0.8636363744735718)
[2025-02-16 13:47:25,069][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:25,252][root][INFO] - Training Epoch: 1/2, step 14785/23838 completed (loss: 0.49318447709083557, acc: 0.8500000238418579)
[2025-02-16 13:47:25,388][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:25,587][root][INFO] - Training Epoch: 1/2, step 14786/23838 completed (loss: 0.5703874230384827, acc: 0.8260869383811951)
[2025-02-16 13:47:25,789][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:26,005][root][INFO] - Training Epoch: 1/2, step 14787/23838 completed (loss: 1.5500975847244263, acc: 0.6382978558540344)
[2025-02-16 13:47:26,229][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:26,490][root][INFO] - Training Epoch: 1/2, step 14788/23838 completed (loss: 0.447737455368042, acc: 0.875)
[2025-02-16 13:47:26,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:26,915][root][INFO] - Training Epoch: 1/2, step 14789/23838 completed (loss: 1.2163890600204468, acc: 0.692307710647583)
[2025-02-16 13:47:27,123][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:27,368][root][INFO] - Training Epoch: 1/2, step 14790/23838 completed (loss: 0.7235010862350464, acc: 0.8444444537162781)
[2025-02-16 13:47:27,514][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:27,746][root][INFO] - Training Epoch: 1/2, step 14791/23838 completed (loss: 0.4782032370567322, acc: 0.8181818127632141)
[2025-02-16 13:47:27,974][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:28,220][root][INFO] - Training Epoch: 1/2, step 14792/23838 completed (loss: 0.7849393486976624, acc: 0.7878788113594055)
[2025-02-16 13:47:28,452][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:28,686][root][INFO] - Training Epoch: 1/2, step 14793/23838 completed (loss: 1.334804892539978, acc: 0.6521739363670349)
[2025-02-16 13:47:28,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:29,261][root][INFO] - Training Epoch: 1/2, step 14794/23838 completed (loss: 0.8840214610099792, acc: 0.78125)
[2025-02-16 13:47:29,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:29,723][root][INFO] - Training Epoch: 1/2, step 14795/23838 completed (loss: 1.7807097434997559, acc: 0.5090909004211426)
[2025-02-16 13:47:29,931][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:30,161][root][INFO] - Training Epoch: 1/2, step 14796/23838 completed (loss: 1.0761611461639404, acc: 0.6666666865348816)
[2025-02-16 13:47:30,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:30,540][root][INFO] - Training Epoch: 1/2, step 14797/23838 completed (loss: 0.9369673132896423, acc: 0.6666666865348816)
[2025-02-16 13:47:30,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:30,993][root][INFO] - Training Epoch: 1/2, step 14798/23838 completed (loss: 1.567841649055481, acc: 0.6052631735801697)
[2025-02-16 13:47:31,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:31,450][root][INFO] - Training Epoch: 1/2, step 14799/23838 completed (loss: 0.8213759064674377, acc: 0.800000011920929)
[2025-02-16 13:47:31,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:31,861][root][INFO] - Training Epoch: 1/2, step 14800/23838 completed (loss: 0.2921154797077179, acc: 0.939393937587738)
[2025-02-16 13:47:32,105][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:32,351][root][INFO] - Training Epoch: 1/2, step 14801/23838 completed (loss: 1.3170065879821777, acc: 0.6938775777816772)
[2025-02-16 13:47:32,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:32,792][root][INFO] - Training Epoch: 1/2, step 14802/23838 completed (loss: 1.3867824077606201, acc: 0.6666666865348816)
[2025-02-16 13:47:32,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:33,178][root][INFO] - Training Epoch: 1/2, step 14803/23838 completed (loss: 0.5672231912612915, acc: 0.8571428656578064)
[2025-02-16 13:47:33,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:33,668][root][INFO] - Training Epoch: 1/2, step 14804/23838 completed (loss: 0.061415039002895355, acc: 1.0)
[2025-02-16 13:47:33,821][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:34,068][root][INFO] - Training Epoch: 1/2, step 14805/23838 completed (loss: 0.9075740575790405, acc: 0.7857142686843872)
[2025-02-16 13:47:34,290][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:34,554][root][INFO] - Training Epoch: 1/2, step 14806/23838 completed (loss: 1.1443049907684326, acc: 0.6857143044471741)
[2025-02-16 13:47:34,842][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:35,108][root][INFO] - Training Epoch: 1/2, step 14807/23838 completed (loss: 0.4713938236236572, acc: 0.8787878751754761)
[2025-02-16 13:47:35,487][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:35,831][root][INFO] - Training Epoch: 1/2, step 14808/23838 completed (loss: 1.5881184339523315, acc: 0.5555555820465088)
[2025-02-16 13:47:36,042][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:36,263][root][INFO] - Training Epoch: 1/2, step 14809/23838 completed (loss: 1.660999059677124, acc: 0.5)
[2025-02-16 13:47:36,428][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:36,656][root][INFO] - Training Epoch: 1/2, step 14810/23838 completed (loss: 0.6387608051300049, acc: 0.8484848737716675)
[2025-02-16 13:47:37,099][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:37,470][root][INFO] - Training Epoch: 1/2, step 14811/23838 completed (loss: 1.4461455345153809, acc: 0.6415094137191772)
[2025-02-16 13:47:37,635][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:37,859][root][INFO] - Training Epoch: 1/2, step 14812/23838 completed (loss: 0.8980653882026672, acc: 0.7878788113594055)
[2025-02-16 13:47:38,045][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:38,268][root][INFO] - Training Epoch: 1/2, step 14813/23838 completed (loss: 0.9692579507827759, acc: 0.6969696879386902)
[2025-02-16 13:47:38,467][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:38,664][root][INFO] - Training Epoch: 1/2, step 14814/23838 completed (loss: 0.8614605069160461, acc: 0.75)
[2025-02-16 13:47:38,875][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:39,113][root][INFO] - Training Epoch: 1/2, step 14815/23838 completed (loss: 1.2440309524536133, acc: 0.6944444179534912)
[2025-02-16 13:47:39,298][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:39,558][root][INFO] - Training Epoch: 1/2, step 14816/23838 completed (loss: 0.32464808225631714, acc: 0.8965517282485962)
[2025-02-16 13:47:39,779][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:40,016][root][INFO] - Training Epoch: 1/2, step 14817/23838 completed (loss: 0.445872962474823, acc: 0.8421052694320679)
[2025-02-16 13:47:40,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:40,346][root][INFO] - Training Epoch: 1/2, step 14818/23838 completed (loss: 0.299838125705719, acc: 0.9655172228813171)
[2025-02-16 13:47:40,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:40,779][root][INFO] - Training Epoch: 1/2, step 14819/23838 completed (loss: 0.580799400806427, acc: 0.8867924809455872)
[2025-02-16 13:47:41,004][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:41,234][root][INFO] - Training Epoch: 1/2, step 14820/23838 completed (loss: 0.5067616105079651, acc: 0.8362069129943848)
[2025-02-16 13:47:41,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:41,640][root][INFO] - Training Epoch: 1/2, step 14821/23838 completed (loss: 0.4501039981842041, acc: 0.8620689511299133)
[2025-02-16 13:47:41,815][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:42,036][root][INFO] - Training Epoch: 1/2, step 14822/23838 completed (loss: 0.2644660472869873, acc: 0.931034505367279)
[2025-02-16 13:47:42,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:42,500][root][INFO] - Training Epoch: 1/2, step 14823/23838 completed (loss: 0.2137846052646637, acc: 0.9383561611175537)
[2025-02-16 13:47:42,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:42,903][root][INFO] - Training Epoch: 1/2, step 14824/23838 completed (loss: 0.46946126222610474, acc: 0.8695651888847351)
[2025-02-16 13:47:43,186][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:43,469][root][INFO] - Training Epoch: 1/2, step 14825/23838 completed (loss: 0.24686792492866516, acc: 0.926174521446228)
[2025-02-16 13:47:43,689][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:43,953][root][INFO] - Training Epoch: 1/2, step 14826/23838 completed (loss: 0.32637864351272583, acc: 0.9115384817123413)
[2025-02-16 13:47:44,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:44,553][root][INFO] - Training Epoch: 1/2, step 14827/23838 completed (loss: 0.33373600244522095, acc: 0.9220778942108154)
[2025-02-16 13:47:44,739][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:44,971][root][INFO] - Training Epoch: 1/2, step 14828/23838 completed (loss: 0.4147184193134308, acc: 0.8677248954772949)
[2025-02-16 13:47:45,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:45,411][root][INFO] - Training Epoch: 1/2, step 14829/23838 completed (loss: 0.38083428144454956, acc: 0.9006211161613464)
[2025-02-16 13:47:45,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:45,974][root][INFO] - Training Epoch: 1/2, step 14830/23838 completed (loss: 0.33565303683280945, acc: 0.8799999952316284)
[2025-02-16 13:47:46,223][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:46,488][root][INFO] - Training Epoch: 1/2, step 14831/23838 completed (loss: 0.46489882469177246, acc: 0.8826815485954285)
[2025-02-16 13:47:46,704][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:46,956][root][INFO] - Training Epoch: 1/2, step 14832/23838 completed (loss: 0.04990965873003006, acc: 1.0)
[2025-02-16 13:47:47,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:47,451][root][INFO] - Training Epoch: 1/2, step 14833/23838 completed (loss: 0.18524156510829926, acc: 0.9450980424880981)
[2025-02-16 13:47:47,675][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:47,920][root][INFO] - Training Epoch: 1/2, step 14834/23838 completed (loss: 0.3916481137275696, acc: 0.8918918967247009)
[2025-02-16 13:47:48,165][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:48,393][root][INFO] - Training Epoch: 1/2, step 14835/23838 completed (loss: 0.9628412127494812, acc: 0.7763158082962036)
[2025-02-16 13:47:48,587][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:48,823][root][INFO] - Training Epoch: 1/2, step 14836/23838 completed (loss: 0.48086681962013245, acc: 0.8483412265777588)
[2025-02-16 13:47:49,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:49,334][root][INFO] - Training Epoch: 1/2, step 14837/23838 completed (loss: 0.4177170693874359, acc: 0.8963730335235596)
[2025-02-16 13:47:49,535][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:49,758][root][INFO] - Training Epoch: 1/2, step 14838/23838 completed (loss: 0.5915274024009705, acc: 0.8181818127632141)
[2025-02-16 13:47:49,943][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:50,188][root][INFO] - Training Epoch: 1/2, step 14839/23838 completed (loss: 0.4416554272174835, acc: 0.8860759735107422)
[2025-02-16 13:47:50,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:50,630][root][INFO] - Training Epoch: 1/2, step 14840/23838 completed (loss: 0.4323115348815918, acc: 0.9019607901573181)
[2025-02-16 13:47:50,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:51,078][root][INFO] - Training Epoch: 1/2, step 14841/23838 completed (loss: 0.5455608367919922, acc: 0.8429751992225647)
[2025-02-16 13:47:51,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:51,588][root][INFO] - Training Epoch: 1/2, step 14842/23838 completed (loss: 0.3320167660713196, acc: 0.8990384340286255)
[2025-02-16 13:47:51,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:52,014][root][INFO] - Training Epoch: 1/2, step 14843/23838 completed (loss: 0.5551995038986206, acc: 0.84375)
[2025-02-16 13:47:52,182][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:52,414][root][INFO] - Training Epoch: 1/2, step 14844/23838 completed (loss: 0.45117464661598206, acc: 0.8471337556838989)
[2025-02-16 13:47:52,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:52,880][root][INFO] - Training Epoch: 1/2, step 14845/23838 completed (loss: 0.3207703232765198, acc: 0.895652174949646)
[2025-02-16 13:47:53,087][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:53,318][root][INFO] - Training Epoch: 1/2, step 14846/23838 completed (loss: 0.43668562173843384, acc: 0.8805969953536987)
[2025-02-16 13:47:53,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:53,769][root][INFO] - Training Epoch: 1/2, step 14847/23838 completed (loss: 0.31652402877807617, acc: 0.9051094651222229)
[2025-02-16 13:47:53,925][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:54,170][root][INFO] - Training Epoch: 1/2, step 14848/23838 completed (loss: 0.45783358812332153, acc: 0.844660222530365)
[2025-02-16 13:47:54,371][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:54,606][root][INFO] - Training Epoch: 1/2, step 14849/23838 completed (loss: 0.3320922553539276, acc: 0.8796296119689941)
[2025-02-16 13:47:54,805][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:55,041][root][INFO] - Training Epoch: 1/2, step 14850/23838 completed (loss: 0.3699698746204376, acc: 0.8741722106933594)
[2025-02-16 13:47:55,247][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:55,505][root][INFO] - Training Epoch: 1/2, step 14851/23838 completed (loss: 0.41792431473731995, acc: 0.8684210777282715)
[2025-02-16 13:47:55,719][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:55,954][root][INFO] - Training Epoch: 1/2, step 14852/23838 completed (loss: 0.1956108808517456, acc: 0.9404761791229248)
[2025-02-16 13:47:56,358][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:56,730][root][INFO] - Training Epoch: 1/2, step 14853/23838 completed (loss: 0.2681215703487396, acc: 0.8913043737411499)
[2025-02-16 13:47:56,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:57,185][root][INFO] - Training Epoch: 1/2, step 14854/23838 completed (loss: 0.41110751032829285, acc: 0.8974359035491943)
[2025-02-16 13:47:57,437][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:57,689][root][INFO] - Training Epoch: 1/2, step 14855/23838 completed (loss: 0.2946174740791321, acc: 0.9217391014099121)
[2025-02-16 13:47:57,829][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:58,059][root][INFO] - Training Epoch: 1/2, step 14856/23838 completed (loss: 0.25981175899505615, acc: 0.9200000166893005)
[2025-02-16 13:47:58,279][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:58,517][root][INFO] - Training Epoch: 1/2, step 14857/23838 completed (loss: 0.16280822455883026, acc: 0.9605262875556946)
[2025-02-16 13:47:58,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:58,896][root][INFO] - Training Epoch: 1/2, step 14858/23838 completed (loss: 0.46782177686691284, acc: 0.8392857313156128)
[2025-02-16 13:47:59,113][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:59,367][root][INFO] - Training Epoch: 1/2, step 14859/23838 completed (loss: 0.3501809239387512, acc: 0.918367326259613)
[2025-02-16 13:47:59,578][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:47:59,806][root][INFO] - Training Epoch: 1/2, step 14860/23838 completed (loss: 0.45574599504470825, acc: 0.8905109763145447)
[2025-02-16 13:48:00,041][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:00,278][root][INFO] - Training Epoch: 1/2, step 14861/23838 completed (loss: 0.17038552463054657, acc: 0.9308176040649414)
[2025-02-16 13:48:00,509][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:00,741][root][INFO] - Training Epoch: 1/2, step 14862/23838 completed (loss: 0.0511331632733345, acc: 0.9904761910438538)
[2025-02-16 13:48:00,928][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:01,153][root][INFO] - Training Epoch: 1/2, step 14863/23838 completed (loss: 0.14709769189357758, acc: 0.9583333134651184)
[2025-02-16 13:48:01,391][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:01,630][root][INFO] - Training Epoch: 1/2, step 14864/23838 completed (loss: 0.3823658227920532, acc: 0.9318181872367859)
[2025-02-16 13:48:01,899][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:02,130][root][INFO] - Training Epoch: 1/2, step 14865/23838 completed (loss: 0.12847641110420227, acc: 0.9516128897666931)
[2025-02-16 13:48:02,353][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:02,577][root][INFO] - Training Epoch: 1/2, step 14866/23838 completed (loss: 0.688801646232605, acc: 0.8583333492279053)
[2025-02-16 13:48:02,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:03,008][root][INFO] - Training Epoch: 1/2, step 14867/23838 completed (loss: 0.20192061364650726, acc: 0.9370629191398621)
[2025-02-16 13:48:03,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:03,430][root][INFO] - Training Epoch: 1/2, step 14868/23838 completed (loss: 0.6299482583999634, acc: 0.7887324094772339)
[2025-02-16 13:48:03,620][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:03,873][root][INFO] - Training Epoch: 1/2, step 14869/23838 completed (loss: 0.35728219151496887, acc: 0.9035087823867798)
[2025-02-16 13:48:04,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:04,334][root][INFO] - Training Epoch: 1/2, step 14870/23838 completed (loss: 0.5075625777244568, acc: 0.8402062058448792)
[2025-02-16 13:48:04,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:04,797][root][INFO] - Training Epoch: 1/2, step 14871/23838 completed (loss: 0.7817034721374512, acc: 0.7641509175300598)
[2025-02-16 13:48:05,016][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:05,261][root][INFO] - Training Epoch: 1/2, step 14872/23838 completed (loss: 0.28362706303596497, acc: 0.9135135412216187)
[2025-02-16 13:48:05,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:05,742][root][INFO] - Training Epoch: 1/2, step 14873/23838 completed (loss: 0.5208861827850342, acc: 0.8626373410224915)
[2025-02-16 13:48:05,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:06,202][root][INFO] - Training Epoch: 1/2, step 14874/23838 completed (loss: 0.435430645942688, acc: 0.8636363744735718)
[2025-02-16 13:48:06,415][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:06,630][root][INFO] - Training Epoch: 1/2, step 14875/23838 completed (loss: 0.7068747282028198, acc: 0.7982456088066101)
[2025-02-16 13:48:06,860][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:07,101][root][INFO] - Training Epoch: 1/2, step 14876/23838 completed (loss: 0.6456596255302429, acc: 0.804347813129425)
[2025-02-16 13:48:07,326][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:07,556][root][INFO] - Training Epoch: 1/2, step 14877/23838 completed (loss: 0.49396347999572754, acc: 0.8785046935081482)
[2025-02-16 13:48:07,722][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:07,951][root][INFO] - Training Epoch: 1/2, step 14878/23838 completed (loss: 0.5230515003204346, acc: 0.8703703880310059)
[2025-02-16 13:48:08,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:08,416][root][INFO] - Training Epoch: 1/2, step 14879/23838 completed (loss: 0.3556968569755554, acc: 0.9044585824012756)
[2025-02-16 13:48:08,639][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:08,872][root][INFO] - Training Epoch: 1/2, step 14880/23838 completed (loss: 0.37366339564323425, acc: 0.895348846912384)
[2025-02-16 13:48:09,020][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:09,249][root][INFO] - Training Epoch: 1/2, step 14881/23838 completed (loss: 0.22978442907333374, acc: 0.9451219439506531)
[2025-02-16 13:48:09,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:09,675][root][INFO] - Training Epoch: 1/2, step 14882/23838 completed (loss: 0.21066872775554657, acc: 0.94017094373703)
[2025-02-16 13:48:09,869][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:10,128][root][INFO] - Training Epoch: 1/2, step 14883/23838 completed (loss: 0.4005967974662781, acc: 0.9027777910232544)
[2025-02-16 13:48:10,315][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:10,549][root][INFO] - Training Epoch: 1/2, step 14884/23838 completed (loss: 0.49110832810401917, acc: 0.858208954334259)
[2025-02-16 13:48:10,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:10,972][root][INFO] - Training Epoch: 1/2, step 14885/23838 completed (loss: 0.13341201841831207, acc: 0.9701492786407471)
[2025-02-16 13:48:11,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:11,440][root][INFO] - Training Epoch: 1/2, step 14886/23838 completed (loss: 0.1680261641740799, acc: 0.9537037014961243)
[2025-02-16 13:48:11,640][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:11,888][root][INFO] - Training Epoch: 1/2, step 14887/23838 completed (loss: 0.2483053207397461, acc: 0.9519230723381042)
[2025-02-16 13:48:12,083][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:12,317][root][INFO] - Training Epoch: 1/2, step 14888/23838 completed (loss: 0.2163659781217575, acc: 0.9441340565681458)
[2025-02-16 13:48:12,507][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:12,744][root][INFO] - Training Epoch: 1/2, step 14889/23838 completed (loss: 0.24402284622192383, acc: 0.9473684430122375)
[2025-02-16 13:48:12,953][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:13,189][root][INFO] - Training Epoch: 1/2, step 14890/23838 completed (loss: 0.3792775273323059, acc: 0.8909090757369995)
[2025-02-16 13:48:13,422][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:13,651][root][INFO] - Training Epoch: 1/2, step 14891/23838 completed (loss: 0.50606769323349, acc: 0.8452380895614624)
[2025-02-16 13:48:13,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:14,116][root][INFO] - Training Epoch: 1/2, step 14892/23838 completed (loss: 0.40294626355171204, acc: 0.8865979313850403)
[2025-02-16 13:48:14,235][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:14,462][root][INFO] - Training Epoch: 1/2, step 14893/23838 completed (loss: 0.5531980395317078, acc: 0.8217821717262268)
[2025-02-16 13:48:14,747][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:15,027][root][INFO] - Training Epoch: 1/2, step 14894/23838 completed (loss: 0.1854083240032196, acc: 0.9632353186607361)
[2025-02-16 13:48:15,203][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:15,432][root][INFO] - Training Epoch: 1/2, step 14895/23838 completed (loss: 0.5375464558601379, acc: 0.8021978139877319)
[2025-02-16 13:48:15,609][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:15,841][root][INFO] - Training Epoch: 1/2, step 14896/23838 completed (loss: 0.40254640579223633, acc: 0.8848921060562134)
[2025-02-16 13:48:16,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:16,444][root][INFO] - Training Epoch: 1/2, step 14897/23838 completed (loss: 0.31509891152381897, acc: 0.9034749269485474)
[2025-02-16 13:48:16,649][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:16,901][root][INFO] - Training Epoch: 1/2, step 14898/23838 completed (loss: 0.2204679697751999, acc: 0.9238095283508301)
[2025-02-16 13:48:17,081][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:17,299][root][INFO] - Training Epoch: 1/2, step 14899/23838 completed (loss: 0.22694049775600433, acc: 0.9277108311653137)
[2025-02-16 13:48:17,469][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:17,714][root][INFO] - Training Epoch: 1/2, step 14900/23838 completed (loss: 0.20848533511161804, acc: 0.9312169551849365)
[2025-02-16 13:48:17,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:18,140][root][INFO] - Training Epoch: 1/2, step 14901/23838 completed (loss: 0.6401084065437317, acc: 0.8363636136054993)
[2025-02-16 13:48:18,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:18,578][root][INFO] - Training Epoch: 1/2, step 14902/23838 completed (loss: 0.5999622941017151, acc: 0.8653846383094788)
[2025-02-16 13:48:18,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:19,043][root][INFO] - Training Epoch: 1/2, step 14903/23838 completed (loss: 0.3837071359157562, acc: 0.8936170339584351)
[2025-02-16 13:48:19,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:19,501][root][INFO] - Training Epoch: 1/2, step 14904/23838 completed (loss: 0.2132846713066101, acc: 0.9266055226325989)
[2025-02-16 13:48:19,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:19,956][root][INFO] - Training Epoch: 1/2, step 14905/23838 completed (loss: 0.3093106150627136, acc: 0.9099099040031433)
[2025-02-16 13:48:20,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:20,397][root][INFO] - Training Epoch: 1/2, step 14906/23838 completed (loss: 0.20647761225700378, acc: 0.9078947305679321)
[2025-02-16 13:48:20,588][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:20,836][root][INFO] - Training Epoch: 1/2, step 14907/23838 completed (loss: 0.3264274001121521, acc: 0.9235668778419495)
[2025-02-16 13:48:21,005][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:21,233][root][INFO] - Training Epoch: 1/2, step 14908/23838 completed (loss: 0.3762100040912628, acc: 0.8985507488250732)
[2025-02-16 13:48:21,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:21,626][root][INFO] - Training Epoch: 1/2, step 14909/23838 completed (loss: 0.18929952383041382, acc: 0.9407894611358643)
[2025-02-16 13:48:21,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:22,040][root][INFO] - Training Epoch: 1/2, step 14910/23838 completed (loss: 0.2631790041923523, acc: 0.9125000238418579)
[2025-02-16 13:48:22,208][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:22,442][root][INFO] - Training Epoch: 1/2, step 14911/23838 completed (loss: 0.10642987489700317, acc: 0.9677419066429138)
[2025-02-16 13:48:22,606][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:22,838][root][INFO] - Training Epoch: 1/2, step 14912/23838 completed (loss: 0.6663527488708496, acc: 0.8676470518112183)
[2025-02-16 13:48:22,999][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:23,220][root][INFO] - Training Epoch: 1/2, step 14913/23838 completed (loss: 0.16884103417396545, acc: 0.9609375)
[2025-02-16 13:48:23,517][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:23,845][root][INFO] - Training Epoch: 1/2, step 14914/23838 completed (loss: 0.1387900710105896, acc: 0.949999988079071)
[2025-02-16 13:48:24,066][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:24,357][root][INFO] - Training Epoch: 1/2, step 14915/23838 completed (loss: 0.29281187057495117, acc: 0.9316770434379578)
[2025-02-16 13:48:24,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:24,806][root][INFO] - Training Epoch: 1/2, step 14916/23838 completed (loss: 0.33928489685058594, acc: 0.8854166865348816)
[2025-02-16 13:48:25,017][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:25,243][root][INFO] - Training Epoch: 1/2, step 14917/23838 completed (loss: 0.33659109473228455, acc: 0.8767123222351074)
[2025-02-16 13:48:25,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:25,618][root][INFO] - Training Epoch: 1/2, step 14918/23838 completed (loss: 0.4579433500766754, acc: 0.8623853325843811)
[2025-02-16 13:48:25,800][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:26,020][root][INFO] - Training Epoch: 1/2, step 14919/23838 completed (loss: 0.558252215385437, acc: 0.8803418874740601)
[2025-02-16 13:48:26,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:26,433][root][INFO] - Training Epoch: 1/2, step 14920/23838 completed (loss: 0.3119707405567169, acc: 0.9117646813392639)
[2025-02-16 13:48:26,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:26,854][root][INFO] - Training Epoch: 1/2, step 14921/23838 completed (loss: 0.20996035635471344, acc: 0.9416666626930237)
[2025-02-16 13:48:27,095][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:27,349][root][INFO] - Training Epoch: 1/2, step 14922/23838 completed (loss: 0.32015931606292725, acc: 0.9204545617103577)
[2025-02-16 13:48:27,526][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:27,752][root][INFO] - Training Epoch: 1/2, step 14923/23838 completed (loss: 0.41204869747161865, acc: 0.8701298832893372)
[2025-02-16 13:48:27,957][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:28,249][root][INFO] - Training Epoch: 1/2, step 14924/23838 completed (loss: 0.2864671051502228, acc: 0.8979591727256775)
[2025-02-16 13:48:28,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:28,698][root][INFO] - Training Epoch: 1/2, step 14925/23838 completed (loss: 0.3477155268192291, acc: 0.8832116723060608)
[2025-02-16 13:48:28,911][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:29,183][root][INFO] - Training Epoch: 1/2, step 14926/23838 completed (loss: 0.37180444598197937, acc: 0.8826815485954285)
[2025-02-16 13:48:29,357][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:29,575][root][INFO] - Training Epoch: 1/2, step 14927/23838 completed (loss: 0.6213082671165466, acc: 0.8297872543334961)
[2025-02-16 13:48:29,740][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:29,960][root][INFO] - Training Epoch: 1/2, step 14928/23838 completed (loss: 0.3585866689682007, acc: 0.8922155499458313)
[2025-02-16 13:48:30,173][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:30,405][root][INFO] - Training Epoch: 1/2, step 14929/23838 completed (loss: 0.21019990742206573, acc: 0.9390243887901306)
[2025-02-16 13:48:30,652][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:30,881][root][INFO] - Training Epoch: 1/2, step 14930/23838 completed (loss: 0.2748425304889679, acc: 0.8815789222717285)
[2025-02-16 13:48:31,055][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:31,298][root][INFO] - Training Epoch: 1/2, step 14931/23838 completed (loss: 0.35325682163238525, acc: 0.8767123222351074)
[2025-02-16 13:48:31,483][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:31,703][root][INFO] - Training Epoch: 1/2, step 14932/23838 completed (loss: 0.25685378909111023, acc: 0.9117646813392639)
[2025-02-16 13:48:31,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:32,076][root][INFO] - Training Epoch: 1/2, step 14933/23838 completed (loss: 0.5558249354362488, acc: 0.84375)
[2025-02-16 13:48:32,246][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:32,479][root][INFO] - Training Epoch: 1/2, step 14934/23838 completed (loss: 0.671963632106781, acc: 0.800000011920929)
[2025-02-16 13:48:32,645][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:32,894][root][INFO] - Training Epoch: 1/2, step 14935/23838 completed (loss: 0.22887471318244934, acc: 0.9430052042007446)
[2025-02-16 13:48:33,125][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:33,365][root][INFO] - Training Epoch: 1/2, step 14936/23838 completed (loss: 0.20873922109603882, acc: 0.9378530979156494)
[2025-02-16 13:48:33,518][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:33,791][root][INFO] - Training Epoch: 1/2, step 14937/23838 completed (loss: 0.2032894343137741, acc: 0.9307692050933838)
[2025-02-16 13:48:33,973][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:34,165][root][INFO] - Training Epoch: 1/2, step 14938/23838 completed (loss: 0.4354196786880493, acc: 0.8461538553237915)
[2025-02-16 13:48:34,395][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:34,649][root][INFO] - Training Epoch: 1/2, step 14939/23838 completed (loss: 0.18201619386672974, acc: 0.9473684430122375)
[2025-02-16 13:48:34,826][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:35,070][root][INFO] - Training Epoch: 1/2, step 14940/23838 completed (loss: 0.16350817680358887, acc: 0.9358974099159241)
[2025-02-16 13:48:35,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:35,541][root][INFO] - Training Epoch: 1/2, step 14941/23838 completed (loss: 0.4075496792793274, acc: 0.9058823585510254)
[2025-02-16 13:48:35,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:35,960][root][INFO] - Training Epoch: 1/2, step 14942/23838 completed (loss: 0.25481998920440674, acc: 0.9389312863349915)
[2025-02-16 13:48:36,101][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:36,333][root][INFO] - Training Epoch: 1/2, step 14943/23838 completed (loss: 0.21519458293914795, acc: 0.9539473652839661)
[2025-02-16 13:48:36,521][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:36,765][root][INFO] - Training Epoch: 1/2, step 14944/23838 completed (loss: 0.23746119439601898, acc: 0.9357143044471741)
[2025-02-16 13:48:36,992][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:37,228][root][INFO] - Training Epoch: 1/2, step 14945/23838 completed (loss: 0.35101979970932007, acc: 0.8999999761581421)
[2025-02-16 13:48:37,394][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:37,627][root][INFO] - Training Epoch: 1/2, step 14946/23838 completed (loss: 0.46609875559806824, acc: 0.8510638475418091)
[2025-02-16 13:48:37,853][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:38,093][root][INFO] - Training Epoch: 1/2, step 14947/23838 completed (loss: 0.4678332209587097, acc: 0.9026548862457275)
[2025-02-16 13:48:38,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:38,559][root][INFO] - Training Epoch: 1/2, step 14948/23838 completed (loss: 0.3941064178943634, acc: 0.8918918967247009)
[2025-02-16 13:48:38,803][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:39,067][root][INFO] - Training Epoch: 1/2, step 14949/23838 completed (loss: 0.19312751293182373, acc: 0.9702970385551453)
[2025-02-16 13:48:39,273][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:39,541][root][INFO] - Training Epoch: 1/2, step 14950/23838 completed (loss: 0.13405410945415497, acc: 0.959770143032074)
[2025-02-16 13:48:39,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:40,020][root][INFO] - Training Epoch: 1/2, step 14951/23838 completed (loss: 0.05617121234536171, acc: 0.9844961166381836)
[2025-02-16 13:48:40,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:40,563][root][INFO] - Training Epoch: 1/2, step 14952/23838 completed (loss: 0.2085302174091339, acc: 0.9547325372695923)
[2025-02-16 13:48:40,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:41,034][root][INFO] - Training Epoch: 1/2, step 14953/23838 completed (loss: 0.6239426732063293, acc: 0.8303571343421936)
[2025-02-16 13:48:41,238][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:41,488][root][INFO] - Training Epoch: 1/2, step 14954/23838 completed (loss: 0.5125173926353455, acc: 0.8947368264198303)
[2025-02-16 13:48:41,678][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:41,913][root][INFO] - Training Epoch: 1/2, step 14955/23838 completed (loss: 0.1514347344636917, acc: 0.9477611780166626)
[2025-02-16 13:48:42,143][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:42,405][root][INFO] - Training Epoch: 1/2, step 14956/23838 completed (loss: 0.23828794062137604, acc: 0.9496855139732361)
[2025-02-16 13:48:42,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:42,900][root][INFO] - Training Epoch: 1/2, step 14957/23838 completed (loss: 0.25249090790748596, acc: 0.9215686321258545)
[2025-02-16 13:48:43,155][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:43,419][root][INFO] - Training Epoch: 1/2, step 14958/23838 completed (loss: 0.3551263213157654, acc: 0.9139344096183777)
[2025-02-16 13:48:43,668][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:43,896][root][INFO] - Training Epoch: 1/2, step 14959/23838 completed (loss: 0.35568901896476746, acc: 0.9047619104385376)
[2025-02-16 13:48:44,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:44,466][root][INFO] - Training Epoch: 1/2, step 14960/23838 completed (loss: 0.34951159358024597, acc: 0.9069767594337463)
[2025-02-16 13:48:44,667][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:44,931][root][INFO] - Training Epoch: 1/2, step 14961/23838 completed (loss: 0.07398709654808044, acc: 0.9661017060279846)
[2025-02-16 13:48:45,183][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:45,468][root][INFO] - Training Epoch: 1/2, step 14962/23838 completed (loss: 0.29500678181648254, acc: 0.9239766001701355)
[2025-02-16 13:48:45,743][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:45,975][root][INFO] - Training Epoch: 1/2, step 14963/23838 completed (loss: 0.30097806453704834, acc: 0.9120879173278809)
[2025-02-16 13:48:46,281][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:46,580][root][INFO] - Training Epoch: 1/2, step 14964/23838 completed (loss: 0.20951630175113678, acc: 0.9356435537338257)
[2025-02-16 13:48:46,788][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:47,048][root][INFO] - Training Epoch: 1/2, step 14965/23838 completed (loss: 0.6999720335006714, acc: 0.7848101258277893)
[2025-02-16 13:48:47,352][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:47,622][root][INFO] - Training Epoch: 1/2, step 14966/23838 completed (loss: 0.22892963886260986, acc: 0.9211618304252625)
[2025-02-16 13:48:47,801][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:48,051][root][INFO] - Training Epoch: 1/2, step 14967/23838 completed (loss: 0.23618382215499878, acc: 0.9189189076423645)
[2025-02-16 13:48:48,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:48,548][root][INFO] - Training Epoch: 1/2, step 14968/23838 completed (loss: 0.20368538796901703, acc: 0.940397322177887)
[2025-02-16 13:48:48,748][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:48,988][root][INFO] - Training Epoch: 1/2, step 14969/23838 completed (loss: 0.23942959308624268, acc: 0.9185185432434082)
[2025-02-16 13:48:49,211][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:49,455][root][INFO] - Training Epoch: 1/2, step 14970/23838 completed (loss: 0.300222784280777, acc: 0.909604549407959)
[2025-02-16 13:48:49,593][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:49,824][root][INFO] - Training Epoch: 1/2, step 14971/23838 completed (loss: 0.5640255212783813, acc: 0.8433734774589539)
[2025-02-16 13:48:50,057][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:50,306][root][INFO] - Training Epoch: 1/2, step 14972/23838 completed (loss: 0.5174018144607544, acc: 0.8738738894462585)
[2025-02-16 13:48:50,500][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:50,753][root][INFO] - Training Epoch: 1/2, step 14973/23838 completed (loss: 0.22171515226364136, acc: 0.9651162624359131)
[2025-02-16 13:48:50,952][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:51,179][root][INFO] - Training Epoch: 1/2, step 14974/23838 completed (loss: 0.40232157707214355, acc: 0.902255654335022)
[2025-02-16 13:48:51,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:51,567][root][INFO] - Training Epoch: 1/2, step 14975/23838 completed (loss: 0.4111935794353485, acc: 0.8859060406684875)
[2025-02-16 13:48:51,741][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:51,965][root][INFO] - Training Epoch: 1/2, step 14976/23838 completed (loss: 0.08661609888076782, acc: 0.9780219793319702)
[2025-02-16 13:48:52,209][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:52,466][root][INFO] - Training Epoch: 1/2, step 14977/23838 completed (loss: 0.051751378923654556, acc: 0.9763779640197754)
[2025-02-16 13:48:52,657][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:52,901][root][INFO] - Training Epoch: 1/2, step 14978/23838 completed (loss: 0.12947478890419006, acc: 0.9579831957817078)
[2025-02-16 13:48:53,097][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:53,334][root][INFO] - Training Epoch: 1/2, step 14979/23838 completed (loss: 0.48184114694595337, acc: 0.8632478713989258)
[2025-02-16 13:48:53,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:53,740][root][INFO] - Training Epoch: 1/2, step 14980/23838 completed (loss: 0.320029616355896, acc: 0.8765432238578796)
[2025-02-16 13:48:53,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:54,186][root][INFO] - Training Epoch: 1/2, step 14981/23838 completed (loss: 0.3813530504703522, acc: 0.9333333373069763)
[2025-02-16 13:48:54,420][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:54,685][root][INFO] - Training Epoch: 1/2, step 14982/23838 completed (loss: 0.450507789850235, acc: 0.9175257682800293)
[2025-02-16 13:48:54,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:55,263][root][INFO] - Training Epoch: 1/2, step 14983/23838 completed (loss: 0.4537002146244049, acc: 0.8571428656578064)
[2025-02-16 13:48:55,558][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:55,873][root][INFO] - Training Epoch: 1/2, step 14984/23838 completed (loss: 0.38801974058151245, acc: 0.8910890817642212)
[2025-02-16 13:48:56,135][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:56,434][root][INFO] - Training Epoch: 1/2, step 14985/23838 completed (loss: 0.23572905361652374, acc: 0.9652777910232544)
[2025-02-16 13:48:56,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:56,876][root][INFO] - Training Epoch: 1/2, step 14986/23838 completed (loss: 0.4049105942249298, acc: 0.9207921028137207)
[2025-02-16 13:48:57,048][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:57,275][root][INFO] - Training Epoch: 1/2, step 14987/23838 completed (loss: 0.2941853404045105, acc: 0.9507042169570923)
[2025-02-16 13:48:57,716][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:58,172][root][INFO] - Training Epoch: 1/2, step 14988/23838 completed (loss: 0.3377569019794464, acc: 0.9103773832321167)
[2025-02-16 13:48:58,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:58,556][root][INFO] - Training Epoch: 1/2, step 14989/23838 completed (loss: 0.5518136024475098, acc: 0.8354430198669434)
[2025-02-16 13:48:58,915][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:59,298][root][INFO] - Training Epoch: 1/2, step 14990/23838 completed (loss: 0.2775725722312927, acc: 0.938524603843689)
[2025-02-16 13:48:59,485][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:48:59,709][root][INFO] - Training Epoch: 1/2, step 14991/23838 completed (loss: 0.4650830328464508, acc: 0.8504672646522522)
[2025-02-16 13:48:59,921][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:00,158][root][INFO] - Training Epoch: 1/2, step 14992/23838 completed (loss: 0.5279831290245056, acc: 0.8527131676673889)
[2025-02-16 13:49:00,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:00,647][root][INFO] - Training Epoch: 1/2, step 14993/23838 completed (loss: 0.30025824904441833, acc: 0.9248120188713074)
[2025-02-16 13:49:00,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:01,075][root][INFO] - Training Epoch: 1/2, step 14994/23838 completed (loss: 0.43679291009902954, acc: 0.8602941036224365)
[2025-02-16 13:49:01,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:01,579][root][INFO] - Training Epoch: 1/2, step 14995/23838 completed (loss: 0.3209851384162903, acc: 0.906593382358551)
[2025-02-16 13:49:01,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:02,043][root][INFO] - Training Epoch: 1/2, step 14996/23838 completed (loss: 0.5685940980911255, acc: 0.8780487775802612)
[2025-02-16 13:49:02,249][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:02,500][root][INFO] - Training Epoch: 1/2, step 14997/23838 completed (loss: 0.6197330951690674, acc: 0.7983193397521973)
[2025-02-16 13:49:02,677][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:02,918][root][INFO] - Training Epoch: 1/2, step 14998/23838 completed (loss: 0.49483686685562134, acc: 0.8461538553237915)
[2025-02-16 13:49:03,308][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:03,685][root][INFO] - Training Epoch: 1/2, step 14999/23838 completed (loss: 0.19946569204330444, acc: 0.9523809552192688)
[2025-02-16 13:49:03,884][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:04,140][root][INFO] - Training Epoch: 1/2, step 15000/23838 completed (loss: 0.5819835066795349, acc: 0.8255813717842102)
[2025-02-16 13:49:04,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:04,553][root][INFO] - Training Epoch: 1/2, step 15001/23838 completed (loss: 0.7713883519172668, acc: 0.78899085521698)
[2025-02-16 13:49:04,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:04,979][root][INFO] - Training Epoch: 1/2, step 15002/23838 completed (loss: 0.3317074477672577, acc: 0.9090909361839294)
[2025-02-16 13:49:05,303][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:05,615][root][INFO] - Training Epoch: 1/2, step 15003/23838 completed (loss: 0.4425118565559387, acc: 0.8808664083480835)
[2025-02-16 13:49:05,811][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:06,033][root][INFO] - Training Epoch: 1/2, step 15004/23838 completed (loss: 0.6158875823020935, acc: 0.8144329786300659)
[2025-02-16 13:49:06,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:06,453][root][INFO] - Training Epoch: 1/2, step 15005/23838 completed (loss: 0.3177785575389862, acc: 0.9294871687889099)
[2025-02-16 13:49:06,663][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:06,905][root][INFO] - Training Epoch: 1/2, step 15006/23838 completed (loss: 0.8297685980796814, acc: 0.7799999713897705)
[2025-02-16 13:49:07,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:07,339][root][INFO] - Training Epoch: 1/2, step 15007/23838 completed (loss: 0.8703150749206543, acc: 0.795918345451355)
[2025-02-16 13:49:07,506][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:07,725][root][INFO] - Training Epoch: 1/2, step 15008/23838 completed (loss: 0.7377399802207947, acc: 0.7857142686843872)
[2025-02-16 13:49:07,933][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:08,174][root][INFO] - Training Epoch: 1/2, step 15009/23838 completed (loss: 0.32601672410964966, acc: 0.9158878326416016)
[2025-02-16 13:49:08,414][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:08,677][root][INFO] - Training Epoch: 1/2, step 15010/23838 completed (loss: 0.49985384941101074, acc: 0.8527131676673889)
[2025-02-16 13:49:08,864][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:09,088][root][INFO] - Training Epoch: 1/2, step 15011/23838 completed (loss: 0.3092343807220459, acc: 0.9166666865348816)
[2025-02-16 13:49:09,297][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:09,556][root][INFO] - Training Epoch: 1/2, step 15012/23838 completed (loss: 0.1889623999595642, acc: 0.936170220375061)
[2025-02-16 13:49:09,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:09,992][root][INFO] - Training Epoch: 1/2, step 15013/23838 completed (loss: 0.1328859031200409, acc: 0.9626168012619019)
[2025-02-16 13:49:10,201][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:10,436][root][INFO] - Training Epoch: 1/2, step 15014/23838 completed (loss: 0.3543753921985626, acc: 0.8987341523170471)
[2025-02-16 13:49:10,655][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:10,886][root][INFO] - Training Epoch: 1/2, step 15015/23838 completed (loss: 0.20323128998279572, acc: 0.942148745059967)
[2025-02-16 13:49:11,034][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:11,263][root][INFO] - Training Epoch: 1/2, step 15016/23838 completed (loss: 0.05665528029203415, acc: 0.9792746305465698)
[2025-02-16 13:49:11,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:11,689][root][INFO] - Training Epoch: 1/2, step 15017/23838 completed (loss: 0.7506153583526611, acc: 0.7878788113594055)
[2025-02-16 13:49:11,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:12,131][root][INFO] - Training Epoch: 1/2, step 15018/23838 completed (loss: 0.4228578805923462, acc: 0.8780487775802612)
[2025-02-16 13:49:12,316][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:12,540][root][INFO] - Training Epoch: 1/2, step 15019/23838 completed (loss: 0.7388767004013062, acc: 0.8039215803146362)
[2025-02-16 13:49:12,708][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:12,934][root][INFO] - Training Epoch: 1/2, step 15020/23838 completed (loss: 0.2999916076660156, acc: 0.90625)
[2025-02-16 13:49:13,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:13,336][root][INFO] - Training Epoch: 1/2, step 15021/23838 completed (loss: 0.20252566039562225, acc: 0.931034505367279)
[2025-02-16 13:49:13,548][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:13,803][root][INFO] - Training Epoch: 1/2, step 15022/23838 completed (loss: 0.6569387912750244, acc: 0.8805969953536987)
[2025-02-16 13:49:14,002][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:14,242][root][INFO] - Training Epoch: 1/2, step 15023/23838 completed (loss: 0.2039649933576584, acc: 0.8999999761581421)
[2025-02-16 13:49:14,448][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:14,679][root][INFO] - Training Epoch: 1/2, step 15024/23838 completed (loss: 0.38976871967315674, acc: 0.8945147395133972)
[2025-02-16 13:49:14,850][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:15,079][root][INFO] - Training Epoch: 1/2, step 15025/23838 completed (loss: 0.49035948514938354, acc: 0.8623188138008118)
[2025-02-16 13:49:15,231][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:15,453][root][INFO] - Training Epoch: 1/2, step 15026/23838 completed (loss: 0.49104219675064087, acc: 0.8529411554336548)
[2025-02-16 13:49:15,636][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:15,873][root][INFO] - Training Epoch: 1/2, step 15027/23838 completed (loss: 0.266190767288208, acc: 0.9176470637321472)
[2025-02-16 13:49:16,049][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:16,273][root][INFO] - Training Epoch: 1/2, step 15028/23838 completed (loss: 0.5173729658126831, acc: 0.8428571224212646)
[2025-02-16 13:49:16,477][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:16,712][root][INFO] - Training Epoch: 1/2, step 15029/23838 completed (loss: 0.6086462736129761, acc: 0.8235294222831726)
[2025-02-16 13:49:16,881][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:17,107][root][INFO] - Training Epoch: 1/2, step 15030/23838 completed (loss: 0.14103978872299194, acc: 0.9693877696990967)
[2025-02-16 13:49:17,257][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:17,438][root][INFO] - Training Epoch: 1/2, step 15031/23838 completed (loss: 0.36878037452697754, acc: 0.8999999761581421)
[2025-02-16 13:49:17,592][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:17,825][root][INFO] - Training Epoch: 1/2, step 15032/23838 completed (loss: 0.29066577553749084, acc: 0.921875)
[2025-02-16 13:49:17,975][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:18,208][root][INFO] - Training Epoch: 1/2, step 15033/23838 completed (loss: 0.6525987982749939, acc: 0.8658536672592163)
[2025-02-16 13:49:18,426][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:18,657][root][INFO] - Training Epoch: 1/2, step 15034/23838 completed (loss: 0.4387308657169342, acc: 0.8640000224113464)
[2025-02-16 13:49:18,836][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:19,061][root][INFO] - Training Epoch: 1/2, step 15035/23838 completed (loss: 0.29802677035331726, acc: 0.9354838728904724)
[2025-02-16 13:49:19,220][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:19,507][root][INFO] - Training Epoch: 1/2, step 15036/23838 completed (loss: 0.29494795203208923, acc: 0.9337349534034729)
[2025-02-16 13:49:19,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:19,977][root][INFO] - Training Epoch: 1/2, step 15037/23838 completed (loss: 0.9957242608070374, acc: 0.7627118825912476)
[2025-02-16 13:49:20,176][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:20,432][root][INFO] - Training Epoch: 1/2, step 15038/23838 completed (loss: 0.4514727294445038, acc: 0.882758617401123)
[2025-02-16 13:49:20,650][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:20,874][root][INFO] - Training Epoch: 1/2, step 15039/23838 completed (loss: 0.3742132782936096, acc: 0.890625)
[2025-02-16 13:49:21,044][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:21,276][root][INFO] - Training Epoch: 1/2, step 15040/23838 completed (loss: 0.3711661696434021, acc: 0.9038461446762085)
[2025-02-16 13:49:21,450][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:21,731][root][INFO] - Training Epoch: 1/2, step 15041/23838 completed (loss: 0.4375104308128357, acc: 0.8607594966888428)
[2025-02-16 13:49:22,037][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:22,320][root][INFO] - Training Epoch: 1/2, step 15042/23838 completed (loss: 0.3327189087867737, acc: 0.9181286692619324)
[2025-02-16 13:49:22,501][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:22,749][root][INFO] - Training Epoch: 1/2, step 15043/23838 completed (loss: 0.505311906337738, acc: 0.8842105269432068)
[2025-02-16 13:49:22,940][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:23,165][root][INFO] - Training Epoch: 1/2, step 15044/23838 completed (loss: 0.2934614419937134, acc: 0.9340659379959106)
[2025-02-16 13:49:23,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:23,592][root][INFO] - Training Epoch: 1/2, step 15045/23838 completed (loss: 0.17860743403434753, acc: 0.9491525292396545)
[2025-02-16 13:49:23,802][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:24,040][root][INFO] - Training Epoch: 1/2, step 15046/23838 completed (loss: 0.23407575488090515, acc: 0.9532163739204407)
[2025-02-16 13:49:24,317][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:24,663][root][INFO] - Training Epoch: 1/2, step 15047/23838 completed (loss: 0.463795006275177, acc: 0.8609625697135925)
[2025-02-16 13:49:24,937][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:25,219][root][INFO] - Training Epoch: 1/2, step 15048/23838 completed (loss: 0.2877042591571808, acc: 0.9351851940155029)
[2025-02-16 13:49:25,444][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:25,682][root][INFO] - Training Epoch: 1/2, step 15049/23838 completed (loss: 0.26373088359832764, acc: 0.9333333373069763)
[2025-02-16 13:49:25,890][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:26,152][root][INFO] - Training Epoch: 1/2, step 15050/23838 completed (loss: 0.3644677996635437, acc: 0.893750011920929)
[2025-02-16 13:49:26,328][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:26,585][root][INFO] - Training Epoch: 1/2, step 15051/23838 completed (loss: 0.2947182357311249, acc: 0.9313304424285889)
[2025-02-16 13:49:26,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:27,043][root][INFO] - Training Epoch: 1/2, step 15052/23838 completed (loss: 0.30423951148986816, acc: 0.89952152967453)
[2025-02-16 13:49:27,222][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:27,448][root][INFO] - Training Epoch: 1/2, step 15053/23838 completed (loss: 0.08482760936021805, acc: 0.9795918464660645)
[2025-02-16 13:49:27,672][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:27,976][root][INFO] - Training Epoch: 1/2, step 15054/23838 completed (loss: 0.2705133557319641, acc: 0.8999999761581421)
[2025-02-16 13:49:28,244][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:28,514][root][INFO] - Training Epoch: 1/2, step 15055/23838 completed (loss: 0.26253369450569153, acc: 0.9316770434379578)
[2025-02-16 13:49:28,772][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:29,074][root][INFO] - Training Epoch: 1/2, step 15056/23838 completed (loss: 0.1486988216638565, acc: 0.954081654548645)
[2025-02-16 13:49:29,272][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:29,502][root][INFO] - Training Epoch: 1/2, step 15057/23838 completed (loss: 0.4701440930366516, acc: 0.8651685118675232)
[2025-02-16 13:49:29,703][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:29,932][root][INFO] - Training Epoch: 1/2, step 15058/23838 completed (loss: 0.2827398478984833, acc: 0.9290780425071716)
[2025-02-16 13:49:30,170][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:30,425][root][INFO] - Training Epoch: 1/2, step 15059/23838 completed (loss: 0.35821545124053955, acc: 0.8837209343910217)
[2025-02-16 13:49:30,643][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:30,887][root][INFO] - Training Epoch: 1/2, step 15060/23838 completed (loss: 0.3562885820865631, acc: 0.8660714030265808)
[2025-02-16 13:49:31,094][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:31,352][root][INFO] - Training Epoch: 1/2, step 15061/23838 completed (loss: 0.4388205409049988, acc: 0.8859649300575256)
[2025-02-16 13:49:31,551][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:31,780][root][INFO] - Training Epoch: 1/2, step 15062/23838 completed (loss: 0.1352052241563797, acc: 0.9617834687232971)
[2025-02-16 13:49:32,007][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:32,246][root][INFO] - Training Epoch: 1/2, step 15063/23838 completed (loss: 0.31506720185279846, acc: 0.9127907156944275)
[2025-02-16 13:49:32,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:32,734][root][INFO] - Training Epoch: 1/2, step 15064/23838 completed (loss: 0.3504449725151062, acc: 0.9238095283508301)
[2025-02-16 13:49:32,935][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:33,162][root][INFO] - Training Epoch: 1/2, step 15065/23838 completed (loss: 0.17316460609436035, acc: 0.957446813583374)
[2025-02-16 13:49:33,342][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:33,608][root][INFO] - Training Epoch: 1/2, step 15066/23838 completed (loss: 0.2813290059566498, acc: 0.9111111164093018)
[2025-02-16 13:49:33,902][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:34,185][root][INFO] - Training Epoch: 1/2, step 15067/23838 completed (loss: 0.7389224767684937, acc: 0.835616409778595)
[2025-02-16 13:49:34,396][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:34,648][root][INFO] - Training Epoch: 1/2, step 15068/23838 completed (loss: 0.517260730266571, acc: 0.8803418874740601)
[2025-02-16 13:49:34,894][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:35,164][root][INFO] - Training Epoch: 1/2, step 15069/23838 completed (loss: 0.3119485378265381, acc: 0.9137930870056152)
[2025-02-16 13:49:35,401][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:35,653][root][INFO] - Training Epoch: 1/2, step 15070/23838 completed (loss: 0.29419997334480286, acc: 0.9059829115867615)
[2025-02-16 13:49:36,077][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:36,458][root][INFO] - Training Epoch: 1/2, step 15071/23838 completed (loss: 0.39625829458236694, acc: 0.896774172782898)
[2025-02-16 13:49:36,729][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:36,987][root][INFO] - Training Epoch: 1/2, step 15072/23838 completed (loss: 0.2262854129076004, acc: 0.9435483813285828)
[2025-02-16 13:49:37,188][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:37,468][root][INFO] - Training Epoch: 1/2, step 15073/23838 completed (loss: 0.658689558506012, acc: 0.8636363744735718)
[2025-02-16 13:49:37,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:38,307][root][INFO] - Training Epoch: 1/2, step 15074/23838 completed (loss: 0.6635932326316833, acc: 0.8333333134651184)
[2025-02-16 13:49:38,938][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:39,414][root][INFO] - Training Epoch: 1/2, step 15075/23838 completed (loss: 0.43252861499786377, acc: 0.8804348111152649)
[2025-02-16 13:49:39,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:39,872][root][INFO] - Training Epoch: 1/2, step 15076/23838 completed (loss: 0.40865227580070496, acc: 0.8854166865348816)
[2025-02-16 13:49:40,115][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:40,364][root][INFO] - Training Epoch: 1/2, step 15077/23838 completed (loss: 0.7927420139312744, acc: 0.7613636255264282)
[2025-02-16 13:49:40,622][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:40,877][root][INFO] - Training Epoch: 1/2, step 15078/23838 completed (loss: 0.4341505765914917, acc: 0.8777777552604675)
[2025-02-16 13:49:41,102][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:41,345][root][INFO] - Training Epoch: 1/2, step 15079/23838 completed (loss: 0.43809235095977783, acc: 0.9090909361839294)
[2025-02-16 13:49:41,683][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:42,005][root][INFO] - Training Epoch: 1/2, step 15080/23838 completed (loss: 0.8423081636428833, acc: 0.7358490824699402)
[2025-02-16 13:49:42,291][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:42,538][root][INFO] - Training Epoch: 1/2, step 15081/23838 completed (loss: 0.30812665820121765, acc: 0.9027777910232544)
[2025-02-16 13:49:42,787][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:43,021][root][INFO] - Training Epoch: 1/2, step 15082/23838 completed (loss: 0.5789467096328735, acc: 0.8133333325386047)
[2025-02-16 13:49:43,225][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:43,457][root][INFO] - Training Epoch: 1/2, step 15083/23838 completed (loss: 0.6298467516899109, acc: 0.8552631735801697)
[2025-02-16 13:49:43,637][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:43,888][root][INFO] - Training Epoch: 1/2, step 15084/23838 completed (loss: 0.6085606813430786, acc: 0.8333333134651184)
[2025-02-16 13:49:44,103][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:44,340][root][INFO] - Training Epoch: 1/2, step 15085/23838 completed (loss: 0.13467851281166077, acc: 0.9752066135406494)
[2025-02-16 13:49:44,565][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:44,802][root][INFO] - Training Epoch: 1/2, step 15086/23838 completed (loss: 0.4087010324001312, acc: 0.8888888955116272)
[2025-02-16 13:49:44,986][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:45,258][root][INFO] - Training Epoch: 1/2, step 15087/23838 completed (loss: 0.23106297850608826, acc: 0.9137930870056152)
[2025-02-16 13:49:45,462][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:45,695][root][INFO] - Training Epoch: 1/2, step 15088/23838 completed (loss: 0.14097513258457184, acc: 0.9523809552192688)
[2025-02-16 13:49:45,896][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:46,181][root][INFO] - Training Epoch: 1/2, step 15089/23838 completed (loss: 0.1019941195845604, acc: 0.97826087474823)
[2025-02-16 13:49:46,364][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:46,564][root][INFO] - Training Epoch: 1/2, step 15090/23838 completed (loss: 0.16244088113307953, acc: 0.9504950642585754)
[2025-02-16 13:49:46,773][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:47,010][root][INFO] - Training Epoch: 1/2, step 15091/23838 completed (loss: 0.026696955785155296, acc: 1.0)
[2025-02-16 13:49:47,248][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:47,504][root][INFO] - Training Epoch: 1/2, step 15092/23838 completed (loss: 0.0734003484249115, acc: 0.9769230484962463)
[2025-02-16 13:49:47,671][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:47,910][root][INFO] - Training Epoch: 1/2, step 15093/23838 completed (loss: 0.06862864643335342, acc: 0.9712643623352051)
[2025-02-16 13:49:48,150][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:48,414][root][INFO] - Training Epoch: 1/2, step 15094/23838 completed (loss: 0.19758839905261993, acc: 0.9576271176338196)
[2025-02-16 13:49:48,631][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:48,881][root][INFO] - Training Epoch: 1/2, step 15095/23838 completed (loss: 0.04474233463406563, acc: 0.9893617033958435)
[2025-02-16 13:49:49,098][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:49,341][root][INFO] - Training Epoch: 1/2, step 15096/23838 completed (loss: 0.09710866212844849, acc: 0.9770992398262024)
[2025-02-16 13:49:49,581][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:49,826][root][INFO] - Training Epoch: 1/2, step 15097/23838 completed (loss: 0.07527873665094376, acc: 0.9868420958518982)
[2025-02-16 13:49:50,172][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:50,516][root][INFO] - Training Epoch: 1/2, step 15098/23838 completed (loss: 0.13496087491512299, acc: 0.9804878234863281)
[2025-02-16 13:49:50,664][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:50,912][root][INFO] - Training Epoch: 1/2, step 15099/23838 completed (loss: 0.10982285439968109, acc: 0.9622641801834106)
[2025-02-16 13:49:51,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:51,337][root][INFO] - Training Epoch: 1/2, step 15100/23838 completed (loss: 0.22513435781002045, acc: 0.9464285969734192)
[2025-02-16 13:49:51,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:51,790][root][INFO] - Training Epoch: 1/2, step 15101/23838 completed (loss: 0.17980286478996277, acc: 0.9809523820877075)
[2025-02-16 13:49:52,060][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:52,351][root][INFO] - Training Epoch: 1/2, step 15102/23838 completed (loss: 0.05697119981050491, acc: 0.9873417615890503)
[2025-02-16 13:49:52,494][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:52,708][root][INFO] - Training Epoch: 1/2, step 15103/23838 completed (loss: 0.22024475038051605, acc: 0.9555555582046509)
[2025-02-16 13:49:52,852][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:53,147][root][INFO] - Training Epoch: 1/2, step 15104/23838 completed (loss: 0.11346152424812317, acc: 0.9718309640884399)
[2025-02-16 13:49:53,398][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:53,668][root][INFO] - Training Epoch: 1/2, step 15105/23838 completed (loss: 0.3494531810283661, acc: 0.9318181872367859)
[2025-02-16 13:49:53,857][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:54,092][root][INFO] - Training Epoch: 1/2, step 15106/23838 completed (loss: 0.09678395092487335, acc: 0.9775280952453613)
[2025-02-16 13:49:54,284][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:54,531][root][INFO] - Training Epoch: 1/2, step 15107/23838 completed (loss: 0.056701287627220154, acc: 0.9856114983558655)
[2025-02-16 13:49:54,745][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:54,974][root][INFO] - Training Epoch: 1/2, step 15108/23838 completed (loss: 0.05121392384171486, acc: 0.9780701994895935)
[2025-02-16 13:49:55,185][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:55,435][root][INFO] - Training Epoch: 1/2, step 15109/23838 completed (loss: 0.10745124518871307, acc: 0.96875)
[2025-02-16 13:49:55,681][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:55,926][root][INFO] - Training Epoch: 1/2, step 15110/23838 completed (loss: 0.24991686642169952, acc: 0.9399999976158142)
[2025-02-16 13:49:56,147][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:56,372][root][INFO] - Training Epoch: 1/2, step 15111/23838 completed (loss: 0.1497345268726349, acc: 0.9514563083648682)
[2025-02-16 13:49:56,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:56,787][root][INFO] - Training Epoch: 1/2, step 15112/23838 completed (loss: 0.09761383384466171, acc: 0.9504950642585754)
[2025-02-16 13:49:56,990][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:57,230][root][INFO] - Training Epoch: 1/2, step 15113/23838 completed (loss: 0.10765114426612854, acc: 0.9785714149475098)
[2025-02-16 13:49:57,454][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:57,711][root][INFO] - Training Epoch: 1/2, step 15114/23838 completed (loss: 0.15891747176647186, acc: 0.9504950642585754)
[2025-02-16 13:49:57,942][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:58,181][root][INFO] - Training Epoch: 1/2, step 15115/23838 completed (loss: 0.26712268590927124, acc: 0.9411764740943909)
[2025-02-16 13:49:58,411][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:58,646][root][INFO] - Training Epoch: 1/2, step 15116/23838 completed (loss: 0.1023007184267044, acc: 0.9646017551422119)
[2025-02-16 13:49:58,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:59,033][root][INFO] - Training Epoch: 1/2, step 15117/23838 completed (loss: 0.033750131726264954, acc: 0.9879518151283264)
[2025-02-16 13:49:59,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:59,412][root][INFO] - Training Epoch: 1/2, step 15118/23838 completed (loss: 0.17313186824321747, acc: 0.9669421315193176)
[2025-02-16 13:49:59,616][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:49:59,839][root][INFO] - Training Epoch: 1/2, step 15119/23838 completed (loss: 0.09850484132766724, acc: 0.9743589758872986)
[2025-02-16 13:50:00,053][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:00,285][root][INFO] - Training Epoch: 1/2, step 15120/23838 completed (loss: 0.18712535500526428, acc: 0.9444444179534912)
[2025-02-16 13:50:00,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:00,747][root][INFO] - Training Epoch: 1/2, step 15121/23838 completed (loss: 0.20999151468276978, acc: 0.9572649598121643)
[2025-02-16 13:50:00,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:01,185][root][INFO] - Training Epoch: 1/2, step 15122/23838 completed (loss: 0.05738454684615135, acc: 0.9807692170143127)
[2025-02-16 13:50:01,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:01,649][root][INFO] - Training Epoch: 1/2, step 15123/23838 completed (loss: 0.10967902094125748, acc: 0.960629940032959)
[2025-02-16 13:50:01,804][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:02,039][root][INFO] - Training Epoch: 1/2, step 15124/23838 completed (loss: 0.42769405245780945, acc: 0.8961039185523987)
[2025-02-16 13:50:02,265][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:02,478][root][INFO] - Training Epoch: 1/2, step 15125/23838 completed (loss: 0.0925336480140686, acc: 0.9895833134651184)
[2025-02-16 13:50:02,710][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:02,952][root][INFO] - Training Epoch: 1/2, step 15126/23838 completed (loss: 0.24928981065750122, acc: 0.9285714030265808)
[2025-02-16 13:50:03,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:03,337][root][INFO] - Training Epoch: 1/2, step 15127/23838 completed (loss: 0.22000664472579956, acc: 0.9523809552192688)
[2025-02-16 13:50:03,495][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:03,721][root][INFO] - Training Epoch: 1/2, step 15128/23838 completed (loss: 0.367213636636734, acc: 0.9052631855010986)
[2025-02-16 13:50:03,908][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:04,137][root][INFO] - Training Epoch: 1/2, step 15129/23838 completed (loss: 0.24724355340003967, acc: 0.9504950642585754)
[2025-02-16 13:50:04,296][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:04,517][root][INFO] - Training Epoch: 1/2, step 15130/23838 completed (loss: 0.22593557834625244, acc: 0.9411764740943909)
[2025-02-16 13:50:04,753][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:04,989][root][INFO] - Training Epoch: 1/2, step 15131/23838 completed (loss: 0.5212605595588684, acc: 0.8962264060974121)
[2025-02-16 13:50:05,196][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:05,447][root][INFO] - Training Epoch: 1/2, step 15132/23838 completed (loss: 0.2742783725261688, acc: 0.9101123809814453)
[2025-02-16 13:50:05,641][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:05,881][root][INFO] - Training Epoch: 1/2, step 15133/23838 completed (loss: 0.0413961187005043, acc: 1.0)
[2025-02-16 13:50:06,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:06,319][root][INFO] - Training Epoch: 1/2, step 15134/23838 completed (loss: 0.17295929789543152, acc: 0.978723406791687)
[2025-02-16 13:50:06,505][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:06,757][root][INFO] - Training Epoch: 1/2, step 15135/23838 completed (loss: 0.04591837152838707, acc: 0.9879518151283264)
[2025-02-16 13:50:06,969][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:07,186][root][INFO] - Training Epoch: 1/2, step 15136/23838 completed (loss: 0.036729879677295685, acc: 0.9885057210922241)
[2025-02-16 13:50:07,379][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:07,611][root][INFO] - Training Epoch: 1/2, step 15137/23838 completed (loss: 0.034625764936208725, acc: 1.0)
[2025-02-16 13:50:07,756][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:07,968][root][INFO] - Training Epoch: 1/2, step 15138/23838 completed (loss: 0.15862303972244263, acc: 0.9545454382896423)
[2025-02-16 13:50:08,157][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:08,386][root][INFO] - Training Epoch: 1/2, step 15139/23838 completed (loss: 0.024707337841391563, acc: 1.0)
[2025-02-16 13:50:08,516][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:08,751][root][INFO] - Training Epoch: 1/2, step 15140/23838 completed (loss: 0.0887429267168045, acc: 0.9722222089767456)
[2025-02-16 13:50:08,987][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:09,237][root][INFO] - Training Epoch: 1/2, step 15141/23838 completed (loss: 0.14392906427383423, acc: 0.9612902998924255)
[2025-02-16 13:50:09,412][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:09,652][root][INFO] - Training Epoch: 1/2, step 15142/23838 completed (loss: 0.6948447227478027, acc: 0.7976190447807312)
[2025-02-16 13:50:09,799][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:10,027][root][INFO] - Training Epoch: 1/2, step 15143/23838 completed (loss: 0.11289623379707336, acc: 0.9571428298950195)
[2025-02-16 13:50:10,198][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:10,415][root][INFO] - Training Epoch: 1/2, step 15144/23838 completed (loss: 0.050512418150901794, acc: 0.9714285731315613)
[2025-02-16 13:50:10,561][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:10,785][root][INFO] - Training Epoch: 1/2, step 15145/23838 completed (loss: 0.09152984619140625, acc: 0.9696969985961914)
[2025-02-16 13:50:10,941][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:11,161][root][INFO] - Training Epoch: 1/2, step 15146/23838 completed (loss: 0.14798343181610107, acc: 0.9626168012619019)
[2025-02-16 13:50:11,300][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:11,524][root][INFO] - Training Epoch: 1/2, step 15147/23838 completed (loss: 0.08085214346647263, acc: 0.96875)
[2025-02-16 13:50:11,717][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:11,951][root][INFO] - Training Epoch: 1/2, step 15148/23838 completed (loss: 0.07264038920402527, acc: 0.9833333492279053)
[2025-02-16 13:50:12,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:12,328][root][INFO] - Training Epoch: 1/2, step 15149/23838 completed (loss: 0.14852909743785858, acc: 0.9733333587646484)
[2025-02-16 13:50:12,504][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:12,748][root][INFO] - Training Epoch: 1/2, step 15150/23838 completed (loss: 0.20333360135555267, acc: 0.9425287246704102)
[2025-02-16 13:50:12,959][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:13,187][root][INFO] - Training Epoch: 1/2, step 15151/23838 completed (loss: 0.18060803413391113, acc: 0.9659090638160706)
[2025-02-16 13:50:13,337][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:13,555][root][INFO] - Training Epoch: 1/2, step 15152/23838 completed (loss: 0.046521227806806564, acc: 0.991304337978363)
[2025-02-16 13:50:13,744][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:13,982][root][INFO] - Training Epoch: 1/2, step 15153/23838 completed (loss: 0.14012442529201508, acc: 0.9479166865348816)
[2025-02-16 13:50:14,191][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:14,411][root][INFO] - Training Epoch: 1/2, step 15154/23838 completed (loss: 0.12353809922933578, acc: 0.9661017060279846)
[2025-02-16 13:50:14,547][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:14,774][root][INFO] - Training Epoch: 1/2, step 15155/23838 completed (loss: 0.163116917014122, acc: 0.9473684430122375)
[2025-02-16 13:50:14,944][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:15,210][root][INFO] - Training Epoch: 1/2, step 15156/23838 completed (loss: 0.0959344282746315, acc: 0.9670329689979553)
[2025-02-16 13:50:15,393][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:15,629][root][INFO] - Training Epoch: 1/2, step 15157/23838 completed (loss: 0.40173274278640747, acc: 0.9054054021835327)
[2025-02-16 13:50:15,841][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:16,070][root][INFO] - Training Epoch: 1/2, step 15158/23838 completed (loss: 0.026853181421756744, acc: 0.9921875)
[2025-02-16 13:50:16,275][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:16,516][root][INFO] - Training Epoch: 1/2, step 15159/23838 completed (loss: 0.0749957337975502, acc: 0.9779411554336548)
[2025-02-16 13:50:16,728][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:16,945][root][INFO] - Training Epoch: 1/2, step 15160/23838 completed (loss: 0.20705077052116394, acc: 0.9370629191398621)
[2025-02-16 13:50:17,108][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:17,330][root][INFO] - Training Epoch: 1/2, step 15161/23838 completed (loss: 0.2883605360984802, acc: 0.94017094373703)
[2025-02-16 13:50:17,493][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:17,718][root][INFO] - Training Epoch: 1/2, step 15162/23838 completed (loss: 0.0985725149512291, acc: 0.9545454382896423)
[2025-02-16 13:50:17,947][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:18,185][root][INFO] - Training Epoch: 1/2, step 15163/23838 completed (loss: 0.07954179495573044, acc: 0.9733333587646484)
[2025-02-16 13:50:18,333][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:18,549][root][INFO] - Training Epoch: 1/2, step 15164/23838 completed (loss: 0.24889744818210602, acc: 0.9130434989929199)
[2025-02-16 13:50:18,700][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:18,931][root][INFO] - Training Epoch: 1/2, step 15165/23838 completed (loss: 0.5535309314727783, acc: 0.8421052694320679)
[2025-02-16 13:50:19,129][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:19,351][root][INFO] - Training Epoch: 1/2, step 15166/23838 completed (loss: 0.1718955934047699, acc: 0.9384615421295166)
[2025-02-16 13:50:19,532][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:19,763][root][INFO] - Training Epoch: 1/2, step 15167/23838 completed (loss: 0.13867248594760895, acc: 0.9718309640884399)
[2025-02-16 13:50:19,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:20,189][root][INFO] - Training Epoch: 1/2, step 15168/23838 completed (loss: 0.04971925914287567, acc: 0.9842519760131836)
[2025-02-16 13:50:20,336][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:20,565][root][INFO] - Training Epoch: 1/2, step 15169/23838 completed (loss: 0.405705064535141, acc: 0.8999999761581421)
[2025-02-16 13:50:20,761][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:21,052][root][INFO] - Training Epoch: 1/2, step 15170/23838 completed (loss: 0.5984251499176025, acc: 0.8452380895614624)
[2025-02-16 13:50:21,254][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:21,491][root][INFO] - Training Epoch: 1/2, step 15171/23838 completed (loss: 0.27640777826309204, acc: 0.918367326259613)
[2025-02-16 13:50:21,680][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:21,924][root][INFO] - Training Epoch: 1/2, step 15172/23838 completed (loss: 0.35743197798728943, acc: 0.9099099040031433)
[2025-02-16 13:50:22,109][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:22,333][root][INFO] - Training Epoch: 1/2, step 15173/23838 completed (loss: 0.3196438252925873, acc: 0.9591836929321289)
[2025-02-16 13:50:22,520][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:22,761][root][INFO] - Training Epoch: 1/2, step 15174/23838 completed (loss: 0.3075392246246338, acc: 0.9292929172515869)
[2025-02-16 13:50:22,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:23,225][root][INFO] - Training Epoch: 1/2, step 15175/23838 completed (loss: 0.03666672855615616, acc: 0.9944444298744202)
[2025-02-16 13:50:23,399][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:23,640][root][INFO] - Training Epoch: 1/2, step 15176/23838 completed (loss: 0.09039221704006195, acc: 0.976190447807312)
[2025-02-16 13:50:23,813][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:24,050][root][INFO] - Training Epoch: 1/2, step 15177/23838 completed (loss: 0.061885904520750046, acc: 0.9879518151283264)
[2025-02-16 13:50:24,252][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:24,497][root][INFO] - Training Epoch: 1/2, step 15178/23838 completed (loss: 0.08804573863744736, acc: 0.9868420958518982)
[2025-02-16 13:50:24,698][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:24,923][root][INFO] - Training Epoch: 1/2, step 15179/23838 completed (loss: 0.01703200303018093, acc: 1.0)
[2025-02-16 13:50:25,116][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:25,342][root][INFO] - Training Epoch: 1/2, step 15180/23838 completed (loss: 0.29586899280548096, acc: 0.93388432264328)
[2025-02-16 13:50:25,550][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:25,798][root][INFO] - Training Epoch: 1/2, step 15181/23838 completed (loss: 0.09929100424051285, acc: 0.9726027250289917)
[2025-02-16 13:50:25,982][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:26,222][root][INFO] - Training Epoch: 1/2, step 15182/23838 completed (loss: 0.20362217724323273, acc: 0.9552238583564758)
[2025-02-16 13:50:26,429][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:26,659][root][INFO] - Training Epoch: 1/2, step 15183/23838 completed (loss: 0.1201346293091774, acc: 0.9622641801834106)
[2025-02-16 13:50:26,917][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:27,174][root][INFO] - Training Epoch: 1/2, step 15184/23838 completed (loss: 0.06537558138370514, acc: 0.9819004535675049)
[2025-02-16 13:50:27,377][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:27,615][root][INFO] - Training Epoch: 1/2, step 15185/23838 completed (loss: 0.11180856078863144, acc: 0.9734513163566589)
[2025-02-16 13:50:27,877][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:28,134][root][INFO] - Training Epoch: 1/2, step 15186/23838 completed (loss: 0.1836240440607071, acc: 0.9488189220428467)
[2025-02-16 13:50:28,340][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:28,573][root][INFO] - Training Epoch: 1/2, step 15187/23838 completed (loss: 0.07065300643444061, acc: 0.9685039520263672)
[2025-02-16 13:50:28,732][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:28,958][root][INFO] - Training Epoch: 1/2, step 15188/23838 completed (loss: 0.047477591782808304, acc: 0.9846153855323792)
[2025-02-16 13:50:29,159][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:29,383][root][INFO] - Training Epoch: 1/2, step 15189/23838 completed (loss: 0.2856917679309845, acc: 0.9390243887901306)
[2025-02-16 13:50:29,623][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:29,856][root][INFO] - Training Epoch: 1/2, step 15190/23838 completed (loss: 0.5277451872825623, acc: 0.9055117964744568)
[2025-02-16 13:50:30,127][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:30,409][root][INFO] - Training Epoch: 1/2, step 15191/23838 completed (loss: 0.15353085100650787, acc: 0.9707317352294922)
[2025-02-16 13:50:30,566][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:30,800][root][INFO] - Training Epoch: 1/2, step 15192/23838 completed (loss: 0.06851322203874588, acc: 0.9746835231781006)
[2025-02-16 13:50:30,977][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:31,208][root][INFO] - Training Epoch: 1/2, step 15193/23838 completed (loss: 0.1562090814113617, acc: 0.9645389914512634)
[2025-02-16 13:50:31,369][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:31,624][root][INFO] - Training Epoch: 1/2, step 15194/23838 completed (loss: 0.02337520942091942, acc: 1.0)
[2025-02-16 13:50:32,008][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:32,393][root][INFO] - Training Epoch: 1/2, step 15195/23838 completed (loss: 0.30004170536994934, acc: 0.9022988677024841)
[2025-02-16 13:50:32,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:32,838][root][INFO] - Training Epoch: 1/2, step 15196/23838 completed (loss: 0.03284965083003044, acc: 0.9919999837875366)
[2025-02-16 13:50:33,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:33,314][root][INFO] - Training Epoch: 1/2, step 15197/23838 completed (loss: 0.06904863566160202, acc: 0.9795918464660645)
[2025-02-16 13:50:33,492][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:33,718][root][INFO] - Training Epoch: 1/2, step 15198/23838 completed (loss: 0.15113237500190735, acc: 0.9318181872367859)
[2025-02-16 13:50:33,854][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:34,086][root][INFO] - Training Epoch: 1/2, step 15199/23838 completed (loss: 0.14981770515441895, acc: 0.9626865386962891)
[2025-02-16 13:50:34,287][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:34,531][root][INFO] - Training Epoch: 1/2, step 15200/23838 completed (loss: 0.04844348132610321, acc: 0.97826087474823)
[2025-02-16 13:50:34,724][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:34,980][root][INFO] - Training Epoch: 1/2, step 15201/23838 completed (loss: 0.24211129546165466, acc: 0.9047619104385376)
[2025-02-16 13:50:35,166][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:35,394][root][INFO] - Training Epoch: 1/2, step 15202/23838 completed (loss: 0.06101410090923309, acc: 0.98591548204422)
[2025-02-16 13:50:35,601][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:35,850][root][INFO] - Training Epoch: 1/2, step 15203/23838 completed (loss: 0.3147260546684265, acc: 0.8981481194496155)
[2025-02-16 13:50:36,072][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:36,272][root][INFO] - Training Epoch: 1/2, step 15204/23838 completed (loss: 0.06202420964837074, acc: 0.9734513163566589)
[2025-02-16 13:50:36,533][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:36,765][root][INFO] - Training Epoch: 1/2, step 15205/23838 completed (loss: 0.11282433569431305, acc: 0.957446813583374)
[2025-02-16 13:50:36,995][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:37,204][root][INFO] - Training Epoch: 1/2, step 15206/23838 completed (loss: 0.10825107246637344, acc: 0.9642857313156128)
[2025-02-16 13:50:37,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:37,609][root][INFO] - Training Epoch: 1/2, step 15207/23838 completed (loss: 0.050323065370321274, acc: 0.9886363744735718)
[2025-02-16 13:50:37,783][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:38,001][root][INFO] - Training Epoch: 1/2, step 15208/23838 completed (loss: 0.07268351316452026, acc: 0.9858155846595764)
[2025-02-16 13:50:38,156][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:38,390][root][INFO] - Training Epoch: 1/2, step 15209/23838 completed (loss: 0.18309716880321503, acc: 0.9488636255264282)
[2025-02-16 13:50:38,545][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:38,775][root][INFO] - Training Epoch: 1/2, step 15210/23838 completed (loss: 0.07967016100883484, acc: 0.9624999761581421)
[2025-02-16 13:50:38,984][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:39,223][root][INFO] - Training Epoch: 1/2, step 15211/23838 completed (loss: 0.13780266046524048, acc: 0.978723406791687)
[2025-02-16 13:50:39,410][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:39,634][root][INFO] - Training Epoch: 1/2, step 15212/23838 completed (loss: 0.015196234919130802, acc: 1.0)
[2025-02-16 13:50:39,777][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:40,003][root][INFO] - Training Epoch: 1/2, step 15213/23838 completed (loss: 0.06600801646709442, acc: 0.97826087474823)
[2025-02-16 13:50:40,189][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:40,411][root][INFO] - Training Epoch: 1/2, step 15214/23838 completed (loss: 0.012073894031345844, acc: 1.0)
[2025-02-16 13:50:40,596][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:40,850][root][INFO] - Training Epoch: 1/2, step 15215/23838 completed (loss: 0.14077240228652954, acc: 0.9444444179534912)
[2025-02-16 13:50:41,058][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:41,290][root][INFO] - Training Epoch: 1/2, step 15216/23838 completed (loss: 0.14582441747188568, acc: 0.9708737730979919)
[2025-02-16 13:50:41,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:41,689][root][INFO] - Training Epoch: 1/2, step 15217/23838 completed (loss: 0.04209247604012489, acc: 0.991304337978363)
[2025-02-16 13:50:41,909][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:42,146][root][INFO] - Training Epoch: 1/2, step 15218/23838 completed (loss: 0.23304815590381622, acc: 0.936170220375061)
[2025-02-16 13:50:42,350][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:42,589][root][INFO] - Training Epoch: 1/2, step 15219/23838 completed (loss: 0.04643138125538826, acc: 0.9908257126808167)
[2025-02-16 13:50:42,758][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:42,985][root][INFO] - Training Epoch: 1/2, step 15220/23838 completed (loss: 0.11619316786527634, acc: 0.9655172228813171)
[2025-02-16 13:50:43,152][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:43,393][root][INFO] - Training Epoch: 1/2, step 15221/23838 completed (loss: 0.1532127559185028, acc: 0.9714285731315613)
[2025-02-16 13:50:43,599][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:43,827][root][INFO] - Training Epoch: 1/2, step 15222/23838 completed (loss: 0.20331116020679474, acc: 0.9356725215911865)
[2025-02-16 13:50:44,264][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:44,732][root][INFO] - Training Epoch: 1/2, step 15223/23838 completed (loss: 0.1615956723690033, acc: 0.9672897458076477)
[2025-02-16 13:50:44,893][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:45,134][root][INFO] - Training Epoch: 1/2, step 15224/23838 completed (loss: 0.06774365156888962, acc: 0.9718309640884399)
[2025-02-16 13:50:45,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:45,553][root][INFO] - Training Epoch: 1/2, step 15225/23838 completed (loss: 0.12204252928495407, acc: 0.9461538195610046)
[2025-02-16 13:50:45,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:46,080][root][INFO] - Training Epoch: 1/2, step 15226/23838 completed (loss: 0.1186431497335434, acc: 0.9760000109672546)
[2025-02-16 13:50:46,299][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:46,535][root][INFO] - Training Epoch: 1/2, step 15227/23838 completed (loss: 0.22977307438850403, acc: 0.9473684430122375)
[2025-02-16 13:50:46,738][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:46,989][root][INFO] - Training Epoch: 1/2, step 15228/23838 completed (loss: 0.10076659917831421, acc: 0.9800000190734863)
[2025-02-16 13:50:47,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:47,384][root][INFO] - Training Epoch: 1/2, step 15229/23838 completed (loss: 0.3503740429878235, acc: 0.918181836605072)
[2025-02-16 13:50:47,615][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:47,859][root][INFO] - Training Epoch: 1/2, step 15230/23838 completed (loss: 0.06989800930023193, acc: 0.9797979593276978)
[2025-02-16 13:50:48,076][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:48,301][root][INFO] - Training Epoch: 1/2, step 15231/23838 completed (loss: 0.26036563515663147, acc: 0.9507042169570923)
[2025-02-16 13:50:48,447][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:48,718][root][INFO] - Training Epoch: 1/2, step 15232/23838 completed (loss: 0.07378226518630981, acc: 0.9655172228813171)
[2025-02-16 13:50:48,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:49,146][root][INFO] - Training Epoch: 1/2, step 15233/23838 completed (loss: 0.40535151958465576, acc: 0.8850574493408203)
[2025-02-16 13:50:49,330][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:49,520][root][INFO] - Training Epoch: 1/2, step 15234/23838 completed (loss: 0.1960761994123459, acc: 0.9316239356994629)
[2025-02-16 13:50:49,665][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:49,898][root][INFO] - Training Epoch: 1/2, step 15235/23838 completed (loss: 0.13943932950496674, acc: 0.9375)
[2025-02-16 13:50:50,120][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:50,366][root][INFO] - Training Epoch: 1/2, step 15236/23838 completed (loss: 0.4050315022468567, acc: 0.9100000262260437)
[2025-02-16 13:50:50,557][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:50,782][root][INFO] - Training Epoch: 1/2, step 15237/23838 completed (loss: 0.10937681049108505, acc: 0.9861111044883728)
[2025-02-16 13:50:51,009][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:51,247][root][INFO] - Training Epoch: 1/2, step 15238/23838 completed (loss: 0.21462802588939667, acc: 0.9506173133850098)
[2025-02-16 13:50:51,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:51,684][root][INFO] - Training Epoch: 1/2, step 15239/23838 completed (loss: 0.04895184934139252, acc: 0.9800000190734863)
[2025-02-16 13:50:51,948][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:52,206][root][INFO] - Training Epoch: 1/2, step 15240/23838 completed (loss: 0.24733415246009827, acc: 0.9171270728111267)
[2025-02-16 13:50:52,386][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:52,636][root][INFO] - Training Epoch: 1/2, step 15241/23838 completed (loss: 0.15265227854251862, acc: 0.9384615421295166)
[2025-02-16 13:50:52,844][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:53,066][root][INFO] - Training Epoch: 1/2, step 15242/23838 completed (loss: 0.28385379910469055, acc: 0.9193548560142517)
[2025-02-16 13:50:53,200][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:53,427][root][INFO] - Training Epoch: 1/2, step 15243/23838 completed (loss: 0.03295757621526718, acc: 0.9919999837875366)
[2025-02-16 13:50:53,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:53,860][root][INFO] - Training Epoch: 1/2, step 15244/23838 completed (loss: 0.46485501527786255, acc: 0.8888888955116272)
[2025-02-16 13:50:54,046][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:54,269][root][INFO] - Training Epoch: 1/2, step 15245/23838 completed (loss: 0.26535850763320923, acc: 0.914893627166748)
[2025-02-16 13:50:54,404][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:54,626][root][INFO] - Training Epoch: 1/2, step 15246/23838 completed (loss: 0.4682006239891052, acc: 0.8780487775802612)
[2025-02-16 13:50:55,015][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:55,349][root][INFO] - Training Epoch: 1/2, step 15247/23838 completed (loss: 0.13091640174388885, acc: 0.9603174328804016)
[2025-02-16 13:50:55,603][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:55,869][root][INFO] - Training Epoch: 1/2, step 15248/23838 completed (loss: 0.37612101435661316, acc: 0.9363636374473572)
[2025-02-16 13:50:56,085][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:56,325][root][INFO] - Training Epoch: 1/2, step 15249/23838 completed (loss: 0.19148263335227966, acc: 0.949999988079071)
[2025-02-16 13:50:56,512][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:56,734][root][INFO] - Training Epoch: 1/2, step 15250/23838 completed (loss: 0.2086237519979477, acc: 0.9646017551422119)
[2025-02-16 13:50:56,882][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:57,136][root][INFO] - Training Epoch: 1/2, step 15251/23838 completed (loss: 0.3679429888725281, acc: 0.891566276550293)
[2025-02-16 13:50:57,351][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:57,595][root][INFO] - Training Epoch: 1/2, step 15252/23838 completed (loss: 0.20386654138565063, acc: 0.921875)
[2025-02-16 13:50:57,794][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:58,031][root][INFO] - Training Epoch: 1/2, step 15253/23838 completed (loss: 0.4585151672363281, acc: 0.9009901285171509)
[2025-02-16 13:50:58,262][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:58,484][root][INFO] - Training Epoch: 1/2, step 15254/23838 completed (loss: 0.2025628387928009, acc: 0.94017094373703)
[2025-02-16 13:50:58,656][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:58,867][root][INFO] - Training Epoch: 1/2, step 15255/23838 completed (loss: 0.2375466674566269, acc: 0.9387755393981934)
[2025-02-16 13:50:59,010][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:59,244][root][INFO] - Training Epoch: 1/2, step 15256/23838 completed (loss: 0.4221703112125397, acc: 0.8632478713989258)
[2025-02-16 13:50:59,464][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:50:59,713][root][INFO] - Training Epoch: 1/2, step 15257/23838 completed (loss: 0.5472986698150635, acc: 0.8500000238418579)
[2025-02-16 13:50:59,904][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:00,139][root][INFO] - Training Epoch: 1/2, step 15258/23838 completed (loss: 0.7655606269836426, acc: 0.7582417726516724)
[2025-02-16 13:51:00,345][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:00,564][root][INFO] - Training Epoch: 1/2, step 15259/23838 completed (loss: 0.29547449946403503, acc: 0.9173553586006165)
[2025-02-16 13:51:00,752][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:00,983][root][INFO] - Training Epoch: 1/2, step 15260/23838 completed (loss: 0.4767012298107147, acc: 0.8389830589294434)
[2025-02-16 13:51:01,197][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:01,428][root][INFO] - Training Epoch: 1/2, step 15261/23838 completed (loss: 0.33889883756637573, acc: 0.9180327653884888)
[2025-02-16 13:51:01,633][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:01,868][root][INFO] - Training Epoch: 1/2, step 15262/23838 completed (loss: 0.4709016680717468, acc: 0.8540145754814148)
[2025-02-16 13:51:02,305][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:02,714][root][INFO] - Training Epoch: 1/2, step 15263/23838 completed (loss: 0.30224183201789856, acc: 0.9136690497398376)
[2025-02-16 13:51:03,028][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:03,314][root][INFO] - Training Epoch: 1/2, step 15264/23838 completed (loss: 0.316048264503479, acc: 0.9014084339141846)
[2025-02-16 13:51:03,574][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:03,813][root][INFO] - Training Epoch: 1/2, step 15265/23838 completed (loss: 0.8614081740379333, acc: 0.75)
[2025-02-16 13:51:04,050][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:04,288][root][INFO] - Training Epoch: 1/2, step 15266/23838 completed (loss: 0.21449492871761322, acc: 0.920634925365448)
[2025-02-16 13:51:04,625][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:04,962][root][INFO] - Training Epoch: 1/2, step 15267/23838 completed (loss: 0.2950664460659027, acc: 0.9055117964744568)
[2025-02-16 13:51:05,178][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:05,410][root][INFO] - Training Epoch: 1/2, step 15268/23838 completed (loss: 0.3568868637084961, acc: 0.8904109597206116)
[2025-02-16 13:51:05,781][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:06,150][root][INFO] - Training Epoch: 1/2, step 15269/23838 completed (loss: 0.24925121665000916, acc: 0.9083969593048096)
[2025-02-16 13:51:06,366][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:06,597][root][INFO] - Training Epoch: 1/2, step 15270/23838 completed (loss: 0.20667022466659546, acc: 0.9435897469520569)
[2025-02-16 13:51:06,806][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:07,039][root][INFO] - Training Epoch: 1/2, step 15271/23838 completed (loss: 0.254706472158432, acc: 0.9290780425071716)
[2025-02-16 13:51:07,372][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:07,707][root][INFO] - Training Epoch: 1/2, step 15272/23838 completed (loss: 0.5907849669456482, acc: 0.8358209133148193)
[2025-02-16 13:51:07,866][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:08,101][root][INFO] - Training Epoch: 1/2, step 15273/23838 completed (loss: 0.45972639322280884, acc: 0.893203854560852)
[2025-02-16 13:51:08,343][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:08,660][root][INFO] - Training Epoch: 1/2, step 15274/23838 completed (loss: 0.4313771724700928, acc: 0.8765432238578796)
[2025-02-16 13:51:08,913][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:09,220][root][INFO] - Training Epoch: 1/2, step 15275/23838 completed (loss: 0.18617305159568787, acc: 0.9615384340286255)
[2025-02-16 13:51:09,465][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:09,719][root][INFO] - Training Epoch: 1/2, step 15276/23838 completed (loss: 0.5597165822982788, acc: 0.8247422575950623)
[2025-02-16 13:51:09,923][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:10,163][root][INFO] - Training Epoch: 1/2, step 15277/23838 completed (loss: 0.26561009883880615, acc: 0.9461538195610046)
[2025-02-16 13:51:10,456][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:10,754][root][INFO] - Training Epoch: 1/2, step 15278/23838 completed (loss: 0.4451744854450226, acc: 0.9090909361839294)
[2025-02-16 13:51:10,951][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:11,178][root][INFO] - Training Epoch: 1/2, step 15279/23838 completed (loss: 0.5651342868804932, acc: 0.8199999928474426)
[2025-02-16 13:51:11,573][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:11,977][root][INFO] - Training Epoch: 1/2, step 15280/23838 completed (loss: 0.3486658036708832, acc: 0.9259259104728699)
[2025-02-16 13:51:12,204][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:12,451][root][INFO] - Training Epoch: 1/2, step 15281/23838 completed (loss: 0.9095972180366516, acc: 0.7767857313156128)
[2025-02-16 13:51:12,723][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:13,002][root][INFO] - Training Epoch: 1/2, step 15282/23838 completed (loss: 0.8617087602615356, acc: 0.8040540814399719)
[2025-02-16 13:51:13,168][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:13,392][root][INFO] - Training Epoch: 1/2, step 15283/23838 completed (loss: 0.3572816252708435, acc: 0.9210526347160339)
[2025-02-16 13:51:13,607][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:13,843][root][INFO] - Training Epoch: 1/2, step 15284/23838 completed (loss: 0.382148414850235, acc: 0.8965517282485962)
[2025-02-16 13:51:14,018][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:14,248][root][INFO] - Training Epoch: 1/2, step 15285/23838 completed (loss: 0.31984132528305054, acc: 0.8999999761581421)
[2025-02-16 13:51:14,563][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:14,878][root][INFO] - Training Epoch: 1/2, step 15286/23838 completed (loss: 0.3063255548477173, acc: 0.9255319237709045)
[2025-02-16 13:51:15,325][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:15,742][root][INFO] - Training Epoch: 1/2, step 15287/23838 completed (loss: 0.4937785565853119, acc: 0.888059675693512)
[2025-02-16 13:51:15,964][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:16,200][root][INFO] - Training Epoch: 1/2, step 15288/23838 completed (loss: 0.2376893162727356, acc: 0.9222221970558167)
[2025-02-16 13:51:16,537][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:16,881][root][INFO] - Training Epoch: 1/2, step 15289/23838 completed (loss: 0.3331839442253113, acc: 0.9193548560142517)
[2025-02-16 13:51:17,089][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:17,351][root][INFO] - Training Epoch: 1/2, step 15290/23838 completed (loss: 0.3863535225391388, acc: 0.9042553305625916)
[2025-02-16 13:51:17,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:17,771][root][INFO] - Training Epoch: 1/2, step 15291/23838 completed (loss: 0.31751424074172974, acc: 0.8989899158477783)
[2025-02-16 13:51:18,130][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:18,487][root][INFO] - Training Epoch: 1/2, step 15292/23838 completed (loss: 0.3999897837638855, acc: 0.89570552110672)
[2025-02-16 13:51:18,792][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:19,060][root][INFO] - Training Epoch: 1/2, step 15293/23838 completed (loss: 0.26057058572769165, acc: 0.9210526347160339)
[2025-02-16 13:51:19,212][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:19,440][root][INFO] - Training Epoch: 1/2, step 15294/23838 completed (loss: 0.31939324736595154, acc: 0.9203540086746216)
[2025-02-16 13:51:20,063][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:20,550][root][INFO] - Training Epoch: 1/2, step 15295/23838 completed (loss: 0.3712114989757538, acc: 0.916167676448822)
[2025-02-16 13:51:20,862][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:21,151][root][INFO] - Training Epoch: 1/2, step 15296/23838 completed (loss: 0.3007785379886627, acc: 0.9225806593894958)
[2025-02-16 13:51:21,540][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:21,872][root][INFO] - Training Epoch: 1/2, step 15297/23838 completed (loss: 0.20715922117233276, acc: 0.9477124214172363)
[2025-02-16 13:51:22,445][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:22,906][root][INFO] - Training Epoch: 1/2, step 15298/23838 completed (loss: 0.24995961785316467, acc: 0.9350649118423462)
[2025-02-16 13:51:23,151][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:23,393][root][INFO] - Training Epoch: 1/2, step 15299/23838 completed (loss: 0.20013000071048737, acc: 0.9677419066429138)
[2025-02-16 13:51:23,718][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:24,028][root][INFO] - Training Epoch: 1/2, step 15300/23838 completed (loss: 0.29865482449531555, acc: 0.9090909361839294)
[2025-02-16 13:51:24,228][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:24,474][root][INFO] - Training Epoch: 1/2, step 15301/23838 completed (loss: 0.5241881608963013, acc: 0.8661417365074158)
[2025-02-16 13:51:24,694][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:24,960][root][INFO] - Training Epoch: 1/2, step 15302/23838 completed (loss: 0.22097279131412506, acc: 0.9370629191398621)
[2025-02-16 13:51:25,160][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:25,419][root][INFO] - Training Epoch: 1/2, step 15303/23838 completed (loss: 0.24389851093292236, acc: 0.9090909361839294)
[2025-02-16 13:51:25,807][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-16 13:51:26,158][root][INFO] - Training Epoch: 1/2, step 15304/23838 completed (loss: 0.23558276891708374, acc: 0.9384615421295166)
