[2024-12-12 02:13:23,494][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True}
[2024-12-12 02:13:23,495][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-12 02:13:23,495][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 4, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-12 02:13:23,495][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2024-12-12_02-13-22.txt', 'log_interval': 5}
[2024-12-12 02:13:44,664][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-12 02:13:52,014][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-12 02:13:52,017][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-12 02:13:52,025][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-12 02:13:52,029][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-12 02:13:56,358][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-12 02:13:56,360][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-12 02:13:56,360][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-12 02:13:56,822][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-12 02:13:56,828][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-12 02:13:56,964][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-12 02:13:56,966][slam_llm.utils.train_utils][INFO] - --> linear has 12.587008 Million params

[2024-12-12 02:13:56,967][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-12 02:13:56,974][slam_llm.utils.train_utils][INFO] - --> asr has 18.223104 Million params

[2024-12-12 02:13:56,987][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-12 02:13:57,599][root][INFO] - --> Training Set Length = 2298
[2024-12-12 02:13:57,607][root][INFO] - --> Validation Set Length = 341
[2024-12-12 02:13:57,608][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-12-12 02:13:57,610][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-12-12 02:15:36,425][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True}
[2024-12-12 02:15:36,426][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-12 02:15:36,426][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 4, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-12 02:15:36,426][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2024-12-12_02-15-35.txt', 'log_interval': 5}
[2024-12-12 02:15:59,883][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-12 02:16:05,318][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-12 02:16:05,321][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-12 02:16:05,323][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-12 02:16:05,324][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-12 02:16:14,413][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-12 02:16:14,415][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-12 02:16:14,415][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-12 02:16:14,680][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-12 02:16:14,682][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-12 02:16:14,780][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-12 02:16:14,780][slam_llm.utils.train_utils][INFO] - --> linear has 12.587008 Million params

[2024-12-12 02:16:14,781][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-12 02:16:14,784][slam_llm.utils.train_utils][INFO] - --> asr has 18.223104 Million params

[2024-12-12 02:16:16,730][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-12 02:16:18,506][root][INFO] - --> Training Set Length = 2298
[2024-12-12 02:16:18,512][root][INFO] - --> Validation Set Length = 341
[2024-12-12 02:16:18,514][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-12-12 02:16:18,515][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-12-12 02:16:21,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:22,844][numexpr.utils][INFO] - Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-12-12 02:16:23,968][root][INFO] - Training Epoch: 1/10, step 0/574 completed (loss: 8.837038040161133, acc: 0.03703703731298447)
[2024-12-12 02:16:24,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:24,354][root][INFO] - Training Epoch: 1/10, step 1/574 completed (loss: 8.265246391296387, acc: 0.0)
[2024-12-12 02:16:24,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:24,732][root][INFO] - Training Epoch: 1/10, step 2/574 completed (loss: 7.735874652862549, acc: 0.0)
[2024-12-12 02:16:24,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:25,297][root][INFO] - Training Epoch: 1/10, step 3/574 completed (loss: 7.430300235748291, acc: 0.07894736528396606)
[2024-12-12 02:16:25,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:25,611][root][INFO] - Training Epoch: 1/10, step 4/574 completed (loss: 6.931695938110352, acc: 0.0810810774564743)
[2024-12-12 02:16:25,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:25,981][root][INFO] - Training Epoch: 1/10, step 5/574 completed (loss: 8.144059181213379, acc: 0.0357142873108387)
[2024-12-12 02:16:26,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:26,381][root][INFO] - Training Epoch: 1/10, step 6/574 completed (loss: 7.440897464752197, acc: 0.0)
[2024-12-12 02:16:26,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:26,726][root][INFO] - Training Epoch: 1/10, step 7/574 completed (loss: 7.802778244018555, acc: 0.0)
[2024-12-12 02:16:26,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:27,145][root][INFO] - Training Epoch: 1/10, step 8/574 completed (loss: 9.717668533325195, acc: 0.0)
[2024-12-12 02:16:27,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:27,552][root][INFO] - Training Epoch: 1/10, step 9/574 completed (loss: 7.4827494621276855, acc: 0.0)
[2024-12-12 02:16:27,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:27,974][root][INFO] - Training Epoch: 1/10, step 10/574 completed (loss: 8.426141738891602, acc: 0.0)
[2024-12-12 02:16:28,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:28,408][root][INFO] - Training Epoch: 1/10, step 11/574 completed (loss: 6.869565486907959, acc: 0.0)
[2024-12-12 02:16:28,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:28,732][root][INFO] - Training Epoch: 1/10, step 12/574 completed (loss: 8.090989112854004, acc: 0.0)
[2024-12-12 02:16:28,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:29,069][root][INFO] - Training Epoch: 1/10, step 13/574 completed (loss: 7.031383991241455, acc: 0.06521739065647125)
[2024-12-12 02:16:29,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:29,407][root][INFO] - Training Epoch: 1/10, step 14/574 completed (loss: 7.438109874725342, acc: 0.0)
[2024-12-12 02:16:29,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:29,745][root][INFO] - Training Epoch: 1/10, step 15/574 completed (loss: 6.389805793762207, acc: 0.10204081982374191)
[2024-12-12 02:16:29,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:30,100][root][INFO] - Training Epoch: 1/10, step 16/574 completed (loss: 9.666487693786621, acc: 0.0)
[2024-12-12 02:16:30,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:30,454][root][INFO] - Training Epoch: 1/10, step 17/574 completed (loss: 8.349285125732422, acc: 0.0)
[2024-12-12 02:16:30,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:30,834][root][INFO] - Training Epoch: 1/10, step 18/574 completed (loss: 6.823163032531738, acc: 0.0833333358168602)
[2024-12-12 02:16:30,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:31,211][root][INFO] - Training Epoch: 1/10, step 19/574 completed (loss: 8.55192756652832, acc: 0.05263157933950424)
[2024-12-12 02:16:31,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:31,586][root][INFO] - Training Epoch: 1/10, step 20/574 completed (loss: 7.974604606628418, acc: 0.0)
[2024-12-12 02:16:31,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:31,962][root][INFO] - Training Epoch: 1/10, step 21/574 completed (loss: 7.14190149307251, acc: 0.0)
[2024-12-12 02:16:32,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:32,307][root][INFO] - Training Epoch: 1/10, step 22/574 completed (loss: 7.686644077301025, acc: 0.03999999910593033)
[2024-12-12 02:16:32,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:32,713][root][INFO] - Training Epoch: 1/10, step 23/574 completed (loss: 8.74383544921875, acc: 0.0476190485060215)
[2024-12-12 02:16:32,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:33,132][root][INFO] - Training Epoch: 1/10, step 24/574 completed (loss: 8.824576377868652, acc: 0.0)
[2024-12-12 02:16:33,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:33,615][root][INFO] - Training Epoch: 1/10, step 25/574 completed (loss: 6.129569053649902, acc: 0.07547169923782349)
[2024-12-12 02:16:33,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:34,012][root][INFO] - Training Epoch: 1/10, step 26/574 completed (loss: 5.611230850219727, acc: 0.10958904027938843)
[2024-12-12 02:16:34,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:35,376][root][INFO] - Training Epoch: 1/10, step 27/574 completed (loss: 4.028237342834473, acc: 0.23715415596961975)
[2024-12-12 02:16:35,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:35,732][root][INFO] - Training Epoch: 1/10, step 28/574 completed (loss: 6.698723316192627, acc: 0.06976744532585144)
[2024-12-12 02:16:35,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:36,192][root][INFO] - Training Epoch: 1/10, step 29/574 completed (loss: 5.47977876663208, acc: 0.15662650763988495)
[2024-12-12 02:16:36,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:36,640][root][INFO] - Training Epoch: 1/10, step 30/574 completed (loss: 5.1266679763793945, acc: 0.12345679104328156)
[2024-12-12 02:16:36,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:37,037][root][INFO] - Training Epoch: 1/10, step 31/574 completed (loss: 7.515956878662109, acc: 0.0)
[2024-12-12 02:16:37,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:37,434][root][INFO] - Training Epoch: 1/10, step 32/574 completed (loss: 7.215978622436523, acc: 0.0)
[2024-12-12 02:16:37,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:37,838][root][INFO] - Training Epoch: 1/10, step 33/574 completed (loss: 7.90995979309082, acc: 0.043478261679410934)
[2024-12-12 02:16:37,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:38,229][root][INFO] - Training Epoch: 1/10, step 34/574 completed (loss: 4.849481105804443, acc: 0.1428571492433548)
[2024-12-12 02:16:38,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:38,580][root][INFO] - Training Epoch: 1/10, step 35/574 completed (loss: 5.380301475524902, acc: 0.16393442451953888)
[2024-12-12 02:16:38,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:38,979][root][INFO] - Training Epoch: 1/10, step 36/574 completed (loss: 5.134257793426514, acc: 0.1428571492433548)
[2024-12-12 02:16:39,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:39,375][root][INFO] - Training Epoch: 1/10, step 37/574 completed (loss: 6.213895797729492, acc: 0.016949152573943138)
[2024-12-12 02:16:39,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:39,757][root][INFO] - Training Epoch: 1/10, step 38/574 completed (loss: 4.966796875, acc: 0.2183908075094223)
[2024-12-12 02:16:39,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:40,103][root][INFO] - Training Epoch: 1/10, step 39/574 completed (loss: 8.194905281066895, acc: 0.0)
[2024-12-12 02:16:40,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:40,477][root][INFO] - Training Epoch: 1/10, step 40/574 completed (loss: 6.923613548278809, acc: 0.03846153989434242)
[2024-12-12 02:16:40,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:40,940][root][INFO] - Training Epoch: 1/10, step 41/574 completed (loss: 4.989728927612305, acc: 0.18918919563293457)
[2024-12-12 02:16:41,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:41,323][root][INFO] - Training Epoch: 1/10, step 42/574 completed (loss: 5.317142009735107, acc: 0.13846154510974884)
[2024-12-12 02:16:41,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:41,796][root][INFO] - Training Epoch: 1/10, step 43/574 completed (loss: 4.9962382316589355, acc: 0.2222222238779068)
[2024-12-12 02:16:41,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:42,242][root][INFO] - Training Epoch: 1/10, step 44/574 completed (loss: 4.522080421447754, acc: 0.25773194432258606)
[2024-12-12 02:16:42,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:42,669][root][INFO] - Training Epoch: 1/10, step 45/574 completed (loss: 4.72243070602417, acc: 0.1764705926179886)
[2024-12-12 02:16:42,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:43,086][root][INFO] - Training Epoch: 1/10, step 46/574 completed (loss: 7.66458797454834, acc: 0.0)
[2024-12-12 02:16:43,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:43,484][root][INFO] - Training Epoch: 1/10, step 47/574 completed (loss: 6.416898250579834, acc: 0.03703703731298447)
[2024-12-12 02:16:43,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:43,879][root][INFO] - Training Epoch: 1/10, step 48/574 completed (loss: 6.86284875869751, acc: 0.0)
[2024-12-12 02:16:44,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:44,325][root][INFO] - Training Epoch: 1/10, step 49/574 completed (loss: 5.979001998901367, acc: 0.0555555559694767)
[2024-12-12 02:16:44,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:44,698][root][INFO] - Training Epoch: 1/10, step 50/574 completed (loss: 5.053947925567627, acc: 0.24561403691768646)
[2024-12-12 02:16:44,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:45,101][root][INFO] - Training Epoch: 1/10, step 51/574 completed (loss: 5.173613548278809, acc: 0.1269841343164444)
[2024-12-12 02:16:45,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:45,461][root][INFO] - Training Epoch: 1/10, step 52/574 completed (loss: 5.171865463256836, acc: 0.15492957830429077)
[2024-12-12 02:16:45,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:45,989][root][INFO] - Training Epoch: 1/10, step 53/574 completed (loss: 4.467430114746094, acc: 0.23333333432674408)
[2024-12-12 02:16:46,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:46,449][root][INFO] - Training Epoch: 1/10, step 54/574 completed (loss: 6.256792068481445, acc: 0.054054055362939835)
[2024-12-12 02:16:46,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:46,862][root][INFO] - Training Epoch: 1/10, step 55/574 completed (loss: 6.79710054397583, acc: 0.0)
[2024-12-12 02:16:48,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:49,871][root][INFO] - Training Epoch: 1/10, step 56/574 completed (loss: 3.1429381370544434, acc: 0.3924914598464966)
[2024-12-12 02:16:50,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:51,221][root][INFO] - Training Epoch: 1/10, step 57/574 completed (loss: 3.5103466510772705, acc: 0.3050108850002289)
[2024-12-12 02:16:51,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:51,901][root][INFO] - Training Epoch: 1/10, step 58/574 completed (loss: 3.9174811840057373, acc: 0.2613636255264282)
[2024-12-12 02:16:52,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:52,479][root][INFO] - Training Epoch: 1/10, step 59/574 completed (loss: 4.156064987182617, acc: 0.23529411852359772)
[2024-12-12 02:16:52,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:53,055][root][INFO] - Training Epoch: 1/10, step 60/574 completed (loss: 3.854567050933838, acc: 0.25362318754196167)
[2024-12-12 02:16:53,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:53,517][root][INFO] - Training Epoch: 1/10, step 61/574 completed (loss: 4.216745376586914, acc: 0.2750000059604645)
[2024-12-12 02:16:53,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:53,872][root][INFO] - Training Epoch: 1/10, step 62/574 completed (loss: 5.531703948974609, acc: 0.0)
[2024-12-12 02:16:54,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:54,270][root][INFO] - Training Epoch: 1/10, step 63/574 completed (loss: 5.597842216491699, acc: 0.0833333358168602)
[2024-12-12 02:16:54,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:54,624][root][INFO] - Training Epoch: 1/10, step 64/574 completed (loss: 3.93693470954895, acc: 0.328125)
[2024-12-12 02:16:54,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:55,045][root][INFO] - Training Epoch: 1/10, step 65/574 completed (loss: 4.703833103179932, acc: 0.24137930572032928)
[2024-12-12 02:16:55,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:55,454][root][INFO] - Training Epoch: 1/10, step 66/574 completed (loss: 4.455307960510254, acc: 0.1785714328289032)
[2024-12-12 02:16:55,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:55,805][root][INFO] - Training Epoch: 1/10, step 67/574 completed (loss: 4.861069679260254, acc: 0.10000000149011612)
[2024-12-12 02:16:55,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:56,210][root][INFO] - Training Epoch: 1/10, step 68/574 completed (loss: 5.9220290184021, acc: 0.03999999910593033)
[2024-12-12 02:16:56,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:56,590][root][INFO] - Training Epoch: 1/10, step 69/574 completed (loss: 5.282278060913086, acc: 0.1388888955116272)
[2024-12-12 02:16:56,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:57,001][root][INFO] - Training Epoch: 1/10, step 70/574 completed (loss: 5.568477630615234, acc: 0.03030303120613098)
[2024-12-12 02:16:57,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:57,432][root][INFO] - Training Epoch: 1/10, step 71/574 completed (loss: 4.095860958099365, acc: 0.25735294818878174)
[2024-12-12 02:16:57,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:57,835][root][INFO] - Training Epoch: 1/10, step 72/574 completed (loss: 3.7297654151916504, acc: 0.2539682686328888)
[2024-12-12 02:16:57,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:58,248][root][INFO] - Training Epoch: 1/10, step 73/574 completed (loss: 4.03913688659668, acc: 0.25128206610679626)
[2024-12-12 02:16:58,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:58,596][root][INFO] - Training Epoch: 1/10, step 74/574 completed (loss: 4.675806999206543, acc: 0.16326530277729034)
[2024-12-12 02:16:58,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:58,961][root][INFO] - Training Epoch: 1/10, step 75/574 completed (loss: 4.519534111022949, acc: 0.13432836532592773)
[2024-12-12 02:16:59,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:59,367][root][INFO] - Training Epoch: 1/10, step 76/574 completed (loss: 3.6852686405181885, acc: 0.277372270822525)
[2024-12-12 02:16:59,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:16:59,751][root][INFO] - Training Epoch: 1/10, step 77/574 completed (loss: 5.811692237854004, acc: 0.0)
[2024-12-12 02:16:59,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:00,120][root][INFO] - Training Epoch: 1/10, step 78/574 completed (loss: 5.193873882293701, acc: 0.0)
[2024-12-12 02:17:00,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:00,527][root][INFO] - Training Epoch: 1/10, step 79/574 completed (loss: 4.697265148162842, acc: 0.0)
[2024-12-12 02:17:00,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:00,901][root][INFO] - Training Epoch: 1/10, step 80/574 completed (loss: 4.781702518463135, acc: 0.11538461595773697)
[2024-12-12 02:17:01,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:01,309][root][INFO] - Training Epoch: 1/10, step 81/574 completed (loss: 4.621586799621582, acc: 0.11538461595773697)
[2024-12-12 02:17:01,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:01,724][root][INFO] - Training Epoch: 1/10, step 82/574 completed (loss: 4.6641082763671875, acc: 0.17307692766189575)
[2024-12-12 02:17:01,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:02,061][root][INFO] - Training Epoch: 1/10, step 83/574 completed (loss: 3.906949520111084, acc: 0.25)
[2024-12-12 02:17:02,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:02,432][root][INFO] - Training Epoch: 1/10, step 84/574 completed (loss: 4.561496734619141, acc: 0.15942029654979706)
[2024-12-12 02:17:02,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:02,888][root][INFO] - Training Epoch: 1/10, step 85/574 completed (loss: 4.187914848327637, acc: 0.20000000298023224)
[2024-12-12 02:17:03,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:03,332][root][INFO] - Training Epoch: 1/10, step 86/574 completed (loss: 4.889242649078369, acc: 0.1304347813129425)
[2024-12-12 02:17:03,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:03,843][root][INFO] - Training Epoch: 1/10, step 87/574 completed (loss: 4.022436618804932, acc: 0.2800000011920929)
[2024-12-12 02:17:03,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:04,225][root][INFO] - Training Epoch: 1/10, step 88/574 completed (loss: 3.723043441772461, acc: 0.291262149810791)
[2024-12-12 02:17:04,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:05,419][root][INFO] - Training Epoch: 1/10, step 89/574 completed (loss: 3.4955849647521973, acc: 0.35922330617904663)
[2024-12-12 02:17:05,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:06,272][root][INFO] - Training Epoch: 1/10, step 90/574 completed (loss: 3.615341901779175, acc: 0.27419355511665344)
[2024-12-12 02:17:06,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:07,095][root][INFO] - Training Epoch: 1/10, step 91/574 completed (loss: 3.1442739963531494, acc: 0.4137931168079376)
[2024-12-12 02:17:07,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:07,862][root][INFO] - Training Epoch: 1/10, step 92/574 completed (loss: 3.494175672531128, acc: 0.3263157904148102)
[2024-12-12 02:17:08,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:08,872][root][INFO] - Training Epoch: 1/10, step 93/574 completed (loss: 3.568380832672119, acc: 0.22772277891635895)
[2024-12-12 02:17:08,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:09,209][root][INFO] - Training Epoch: 1/10, step 94/574 completed (loss: 3.724024534225464, acc: 0.24193547666072845)
[2024-12-12 02:17:09,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:09,610][root][INFO] - Training Epoch: 1/10, step 95/574 completed (loss: 3.98061203956604, acc: 0.21739129722118378)
[2024-12-12 02:17:09,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:10,026][root][INFO] - Training Epoch: 1/10, step 96/574 completed (loss: 3.9969048500061035, acc: 0.13445378839969635)
[2024-12-12 02:17:10,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:10,440][root][INFO] - Training Epoch: 1/10, step 97/574 completed (loss: 3.8260507583618164, acc: 0.2211538404226303)
[2024-12-12 02:17:10,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:10,868][root][INFO] - Training Epoch: 1/10, step 98/574 completed (loss: 3.668438673019409, acc: 0.2700729966163635)
[2024-12-12 02:17:10,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:11,256][root][INFO] - Training Epoch: 1/10, step 99/574 completed (loss: 4.309807300567627, acc: 0.19402985274791718)
[2024-12-12 02:17:11,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:11,621][root][INFO] - Training Epoch: 1/10, step 100/574 completed (loss: 4.3505096435546875, acc: 0.10000000149011612)
[2024-12-12 02:17:11,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:11,970][root][INFO] - Training Epoch: 1/10, step 101/574 completed (loss: 3.5996124744415283, acc: 0.22727273404598236)
[2024-12-12 02:17:12,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:12,390][root][INFO] - Training Epoch: 1/10, step 102/574 completed (loss: 3.2190511226654053, acc: 0.17391304671764374)
[2024-12-12 02:17:12,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:12,799][root][INFO] - Training Epoch: 1/10, step 103/574 completed (loss: 3.4455673694610596, acc: 0.22727273404598236)
[2024-12-12 02:17:12,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:13,196][root][INFO] - Training Epoch: 1/10, step 104/574 completed (loss: 3.724146842956543, acc: 0.17241379618644714)
[2024-12-12 02:17:13,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:13,554][root][INFO] - Training Epoch: 1/10, step 105/574 completed (loss: 3.777630090713501, acc: 0.1860465109348297)
[2024-12-12 02:17:13,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:13,898][root][INFO] - Training Epoch: 1/10, step 106/574 completed (loss: 3.268904685974121, acc: 0.3199999928474426)
[2024-12-12 02:17:14,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:14,227][root][INFO] - Training Epoch: 1/10, step 107/574 completed (loss: 3.6486823558807373, acc: 0.1764705926179886)
[2024-12-12 02:17:14,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:14,544][root][INFO] - Training Epoch: 1/10, step 108/574 completed (loss: 3.531033754348755, acc: 0.23076923191547394)
[2024-12-12 02:17:14,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:14,929][root][INFO] - Training Epoch: 1/10, step 109/574 completed (loss: 3.8667871952056885, acc: 0.2142857164144516)
[2024-12-12 02:17:15,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:15,336][root][INFO] - Training Epoch: 1/10, step 110/574 completed (loss: 3.7106080055236816, acc: 0.2769230902194977)
[2024-12-12 02:17:15,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:15,803][root][INFO] - Training Epoch: 1/10, step 111/574 completed (loss: 3.5933287143707275, acc: 0.22807016968727112)
[2024-12-12 02:17:15,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:16,241][root][INFO] - Training Epoch: 1/10, step 112/574 completed (loss: 3.2499759197235107, acc: 0.3333333432674408)
[2024-12-12 02:17:16,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:16,619][root][INFO] - Training Epoch: 1/10, step 113/574 completed (loss: 4.0280351638793945, acc: 0.3076923191547394)
[2024-12-12 02:17:16,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:17,072][root][INFO] - Training Epoch: 1/10, step 114/574 completed (loss: 3.4164557456970215, acc: 0.26530611515045166)
[2024-12-12 02:17:17,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:17,458][root][INFO] - Training Epoch: 1/10, step 115/574 completed (loss: 3.2541792392730713, acc: 0.3181818127632141)
[2024-12-12 02:17:17,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:17,819][root][INFO] - Training Epoch: 1/10, step 116/574 completed (loss: 3.0780603885650635, acc: 0.2539682686328888)
[2024-12-12 02:17:17,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:18,137][root][INFO] - Training Epoch: 1/10, step 117/574 completed (loss: 3.1963467597961426, acc: 0.26829269528388977)
[2024-12-12 02:17:18,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:18,547][root][INFO] - Training Epoch: 1/10, step 118/574 completed (loss: 3.219815492630005, acc: 0.33870968222618103)
[2024-12-12 02:17:18,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:19,459][root][INFO] - Training Epoch: 1/10, step 119/574 completed (loss: 2.979438543319702, acc: 0.34220531582832336)
[2024-12-12 02:17:19,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:19,868][root][INFO] - Training Epoch: 1/10, step 120/574 completed (loss: 3.1535568237304688, acc: 0.3199999928474426)
[2024-12-12 02:17:20,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:20,334][root][INFO] - Training Epoch: 1/10, step 121/574 completed (loss: 3.37526798248291, acc: 0.32692307233810425)
[2024-12-12 02:17:20,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:20,735][root][INFO] - Training Epoch: 1/10, step 122/574 completed (loss: 3.7176449298858643, acc: 0.25)
[2024-12-12 02:17:20,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:21,064][root][INFO] - Training Epoch: 1/10, step 123/574 completed (loss: 3.038112163543701, acc: 0.2631579041481018)
[2024-12-12 02:17:21,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:21,503][root][INFO] - Training Epoch: 1/10, step 124/574 completed (loss: 3.3988239765167236, acc: 0.2269938588142395)
[2024-12-12 02:17:21,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:21,893][root][INFO] - Training Epoch: 1/10, step 125/574 completed (loss: 2.601987361907959, acc: 0.4166666567325592)
[2024-12-12 02:17:22,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:22,325][root][INFO] - Training Epoch: 1/10, step 126/574 completed (loss: 3.0728230476379395, acc: 0.24166665971279144)
[2024-12-12 02:17:22,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:22,698][root][INFO] - Training Epoch: 1/10, step 127/574 completed (loss: 3.0239648818969727, acc: 0.2380952388048172)
[2024-12-12 02:17:22,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:23,097][root][INFO] - Training Epoch: 1/10, step 128/574 completed (loss: 3.2036871910095215, acc: 0.29230770468711853)
[2024-12-12 02:17:23,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:23,539][root][INFO] - Training Epoch: 1/10, step 129/574 completed (loss: 2.760193347930908, acc: 0.36764705181121826)
[2024-12-12 02:17:23,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:23,925][root][INFO] - Training Epoch: 1/10, step 130/574 completed (loss: 3.4870986938476562, acc: 0.23076923191547394)
[2024-12-12 02:17:24,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:24,294][root][INFO] - Training Epoch: 1/10, step 131/574 completed (loss: 2.564405679702759, acc: 0.3478260934352875)
[2024-12-12 02:17:24,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:24,695][root][INFO] - Training Epoch: 1/10, step 132/574 completed (loss: 3.6491994857788086, acc: 0.1875)
[2024-12-12 02:17:24,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:25,094][root][INFO] - Training Epoch: 1/10, step 133/574 completed (loss: 2.508739709854126, acc: 0.3478260934352875)
[2024-12-12 02:17:25,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:25,453][root][INFO] - Training Epoch: 1/10, step 134/574 completed (loss: 2.890284776687622, acc: 0.22857142984867096)
[2024-12-12 02:17:25,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:25,851][root][INFO] - Training Epoch: 1/10, step 135/574 completed (loss: 2.7044575214385986, acc: 0.3076923191547394)
[2024-12-12 02:17:25,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:26,206][root][INFO] - Training Epoch: 1/10, step 136/574 completed (loss: 3.388399362564087, acc: 0.2142857164144516)
[2024-12-12 02:17:26,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:26,533][root][INFO] - Training Epoch: 1/10, step 137/574 completed (loss: 2.49477481842041, acc: 0.4333333373069763)
[2024-12-12 02:17:26,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:26,915][root][INFO] - Training Epoch: 1/10, step 138/574 completed (loss: 2.355268955230713, acc: 0.30434781312942505)
[2024-12-12 02:17:27,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:27,315][root][INFO] - Training Epoch: 1/10, step 139/574 completed (loss: 3.7423760890960693, acc: 0.2380952388048172)
[2024-12-12 02:17:27,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:27,714][root][INFO] - Training Epoch: 1/10, step 140/574 completed (loss: 3.0656068325042725, acc: 0.38461539149284363)
[2024-12-12 02:17:27,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:28,106][root][INFO] - Training Epoch: 1/10, step 141/574 completed (loss: 3.210967779159546, acc: 0.19354838132858276)
[2024-12-12 02:17:28,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:28,529][root][INFO] - Training Epoch: 1/10, step 142/574 completed (loss: 2.935415744781494, acc: 0.2432432472705841)
[2024-12-12 02:17:29,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:29,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:29,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:30,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:30,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:31,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:31,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:31,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:32,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:32,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:33,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:33,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:33,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:34,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:34,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:35,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:35,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:35,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:35,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:36,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:36,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:36,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:37,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:37,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:38,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:38,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:38,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:39,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:39,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:39,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:40,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:40,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:40,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:41,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:41,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:41,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:42,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:42,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:42,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:43,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:43,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:44,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:44,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:44,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:45,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:45,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:46,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:46,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:46,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:47,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:47,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:48,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:48,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:48,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:49,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:49,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:49,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:50,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:50,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:51,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:51,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:51,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:52,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:52,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:53,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:53,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:53,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:54,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:54,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:54,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:55,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:55,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:56,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:56,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:56,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:57,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:57,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:57,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:58,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:58,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:58,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:59,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:59,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:17:59,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:00,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:00,888][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(19.8438, device='cuda:0') eval_epoch_loss=tensor(2.9879, device='cuda:0') eval_epoch_acc=tensor(0.2815, device='cuda:0')
[2024-12-12 02:18:00,890][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:18:00,890][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:18:01,111][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_1_step_143_loss_2.987891435623169/model.pt
[2024-12-12 02:18:01,123][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:18:01,124][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 2.987891435623169
[2024-12-12 02:18:01,125][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.2814539670944214
[2024-12-12 02:18:01,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:01,728][root][INFO] - Training Epoch: 1/10, step 143/574 completed (loss: 2.4920601844787598, acc: 0.429824560880661)
[2024-12-12 02:18:01,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:02,078][root][INFO] - Training Epoch: 1/10, step 144/574 completed (loss: 2.356518507003784, acc: 0.43283581733703613)
[2024-12-12 02:18:02,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:02,444][root][INFO] - Training Epoch: 1/10, step 145/574 completed (loss: 2.8386309146881104, acc: 0.30612245202064514)
[2024-12-12 02:18:02,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:02,928][root][INFO] - Training Epoch: 1/10, step 146/574 completed (loss: 2.7475035190582275, acc: 0.3191489279270172)
[2024-12-12 02:18:03,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:03,271][root][INFO] - Training Epoch: 1/10, step 147/574 completed (loss: 2.592667818069458, acc: 0.4000000059604645)
[2024-12-12 02:18:03,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:03,655][root][INFO] - Training Epoch: 1/10, step 148/574 completed (loss: 3.426665782928467, acc: 0.1071428582072258)
[2024-12-12 02:18:03,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:03,977][root][INFO] - Training Epoch: 1/10, step 149/574 completed (loss: 2.5152359008789062, acc: 0.3478260934352875)
[2024-12-12 02:18:04,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:04,322][root][INFO] - Training Epoch: 1/10, step 150/574 completed (loss: 3.0021846294403076, acc: 0.24137930572032928)
[2024-12-12 02:18:04,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:04,655][root][INFO] - Training Epoch: 1/10, step 151/574 completed (loss: 2.703420639038086, acc: 0.30434781312942505)
[2024-12-12 02:18:04,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:04,975][root][INFO] - Training Epoch: 1/10, step 152/574 completed (loss: 2.555974245071411, acc: 0.3050847351551056)
[2024-12-12 02:18:05,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:05,277][root][INFO] - Training Epoch: 1/10, step 153/574 completed (loss: 3.1438045501708984, acc: 0.22807016968727112)
[2024-12-12 02:18:05,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:05,683][root][INFO] - Training Epoch: 1/10, step 154/574 completed (loss: 2.8945953845977783, acc: 0.3513513505458832)
[2024-12-12 02:18:05,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:06,050][root][INFO] - Training Epoch: 1/10, step 155/574 completed (loss: 2.778733968734741, acc: 0.3928571343421936)
[2024-12-12 02:18:06,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:06,394][root][INFO] - Training Epoch: 1/10, step 156/574 completed (loss: 2.3771700859069824, acc: 0.43478259444236755)
[2024-12-12 02:18:06,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:06,740][root][INFO] - Training Epoch: 1/10, step 157/574 completed (loss: 2.8505191802978516, acc: 0.2631579041481018)
[2024-12-12 02:18:07,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:08,491][root][INFO] - Training Epoch: 1/10, step 158/574 completed (loss: 2.883629322052002, acc: 0.4054054021835327)
[2024-12-12 02:18:08,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:08,903][root][INFO] - Training Epoch: 1/10, step 159/574 completed (loss: 2.8509535789489746, acc: 0.29629629850387573)
[2024-12-12 02:18:09,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:09,357][root][INFO] - Training Epoch: 1/10, step 160/574 completed (loss: 2.9373297691345215, acc: 0.3488371968269348)
[2024-12-12 02:18:09,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:09,983][root][INFO] - Training Epoch: 1/10, step 161/574 completed (loss: 2.8246989250183105, acc: 0.364705890417099)
[2024-12-12 02:18:10,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:10,560][root][INFO] - Training Epoch: 1/10, step 162/574 completed (loss: 3.065931797027588, acc: 0.31460675597190857)
[2024-12-12 02:18:10,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:10,930][root][INFO] - Training Epoch: 1/10, step 163/574 completed (loss: 2.6361706256866455, acc: 0.3636363744735718)
[2024-12-12 02:18:11,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:11,279][root][INFO] - Training Epoch: 1/10, step 164/574 completed (loss: 2.7025210857391357, acc: 0.3333333432674408)
[2024-12-12 02:18:11,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:11,615][root][INFO] - Training Epoch: 1/10, step 165/574 completed (loss: 3.0267069339752197, acc: 0.27586206793785095)
[2024-12-12 02:18:11,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:12,017][root][INFO] - Training Epoch: 1/10, step 166/574 completed (loss: 2.086087226867676, acc: 0.5306122303009033)
[2024-12-12 02:18:12,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:12,382][root][INFO] - Training Epoch: 1/10, step 167/574 completed (loss: 2.471764326095581, acc: 0.36000001430511475)
[2024-12-12 02:18:12,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:12,830][root][INFO] - Training Epoch: 1/10, step 168/574 completed (loss: 2.4957704544067383, acc: 0.3888888955116272)
[2024-12-12 02:18:12,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:13,211][root][INFO] - Training Epoch: 1/10, step 169/574 completed (loss: 2.566694736480713, acc: 0.3333333432674408)
[2024-12-12 02:18:13,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:14,287][root][INFO] - Training Epoch: 1/10, step 170/574 completed (loss: 2.932929515838623, acc: 0.33561643958091736)
[2024-12-12 02:18:14,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:14,657][root][INFO] - Training Epoch: 1/10, step 171/574 completed (loss: 2.2490789890289307, acc: 0.375)
[2024-12-12 02:18:14,761][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:14,986][root][INFO] - Training Epoch: 1/10, step 172/574 completed (loss: 2.929774761199951, acc: 0.2222222238779068)
[2024-12-12 02:18:15,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:15,318][root][INFO] - Training Epoch: 1/10, step 173/574 completed (loss: 2.6378772258758545, acc: 0.3214285671710968)
[2024-12-12 02:18:15,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:15,867][root][INFO] - Training Epoch: 1/10, step 174/574 completed (loss: 2.2807223796844482, acc: 0.4955752193927765)
[2024-12-12 02:18:15,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:16,195][root][INFO] - Training Epoch: 1/10, step 175/574 completed (loss: 2.7007229328155518, acc: 0.28985506296157837)
[2024-12-12 02:18:16,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:16,626][root][INFO] - Training Epoch: 1/10, step 176/574 completed (loss: 2.8967390060424805, acc: 0.2613636255264282)
[2024-12-12 02:18:16,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:17,556][root][INFO] - Training Epoch: 1/10, step 177/574 completed (loss: 2.886476516723633, acc: 0.290076345205307)
[2024-12-12 02:18:17,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:18,265][root][INFO] - Training Epoch: 1/10, step 178/574 completed (loss: 2.981893301010132, acc: 0.2518518567085266)
[2024-12-12 02:18:18,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:18,592][root][INFO] - Training Epoch: 1/10, step 179/574 completed (loss: 2.6571197509765625, acc: 0.2786885201931)
[2024-12-12 02:18:18,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:18,957][root][INFO] - Training Epoch: 1/10, step 180/574 completed (loss: 2.128466844558716, acc: 0.2916666567325592)
[2024-12-12 02:18:19,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:19,349][root][INFO] - Training Epoch: 1/10, step 181/574 completed (loss: 2.3803293704986572, acc: 0.4000000059604645)
[2024-12-12 02:18:19,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:19,732][root][INFO] - Training Epoch: 1/10, step 182/574 completed (loss: 3.000558376312256, acc: 0.25)
[2024-12-12 02:18:19,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:20,106][root][INFO] - Training Epoch: 1/10, step 183/574 completed (loss: 3.065290927886963, acc: 0.23170731961727142)
[2024-12-12 02:18:20,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:20,503][root][INFO] - Training Epoch: 1/10, step 184/574 completed (loss: 2.9291727542877197, acc: 0.2839879095554352)
[2024-12-12 02:18:20,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:20,903][root][INFO] - Training Epoch: 1/10, step 185/574 completed (loss: 3.0157949924468994, acc: 0.25072047114372253)
[2024-12-12 02:18:21,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:21,426][root][INFO] - Training Epoch: 1/10, step 186/574 completed (loss: 3.019056797027588, acc: 0.25312501192092896)
[2024-12-12 02:18:21,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:21,964][root][INFO] - Training Epoch: 1/10, step 187/574 completed (loss: 2.7413744926452637, acc: 0.3151969909667969)
[2024-12-12 02:18:22,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:22,393][root][INFO] - Training Epoch: 1/10, step 188/574 completed (loss: 2.7393364906311035, acc: 0.33451956510543823)
[2024-12-12 02:18:22,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:22,780][root][INFO] - Training Epoch: 1/10, step 189/574 completed (loss: 3.5602529048919678, acc: 0.23999999463558197)
[2024-12-12 02:18:22,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:23,362][root][INFO] - Training Epoch: 1/10, step 190/574 completed (loss: 2.6832025051116943, acc: 0.3372093141078949)
[2024-12-12 02:18:23,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:24,170][root][INFO] - Training Epoch: 1/10, step 191/574 completed (loss: 2.6857030391693115, acc: 0.3650793731212616)
[2024-12-12 02:18:24,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:25,094][root][INFO] - Training Epoch: 1/10, step 192/574 completed (loss: 2.6433727741241455, acc: 0.39393940567970276)
[2024-12-12 02:18:25,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:25,851][root][INFO] - Training Epoch: 1/10, step 193/574 completed (loss: 2.290818691253662, acc: 0.48235294222831726)
[2024-12-12 02:18:26,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:26,937][root][INFO] - Training Epoch: 1/10, step 194/574 completed (loss: 2.386585235595703, acc: 0.3641975224018097)
[2024-12-12 02:18:27,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:27,893][root][INFO] - Training Epoch: 1/10, step 195/574 completed (loss: 2.319845199584961, acc: 0.4032258093357086)
[2024-12-12 02:18:27,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:28,194][root][INFO] - Training Epoch: 1/10, step 196/574 completed (loss: 2.389486789703369, acc: 0.3571428656578064)
[2024-12-12 02:18:28,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:28,514][root][INFO] - Training Epoch: 1/10, step 197/574 completed (loss: 2.964959144592285, acc: 0.22499999403953552)
[2024-12-12 02:18:28,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:28,953][root][INFO] - Training Epoch: 1/10, step 198/574 completed (loss: 3.155106544494629, acc: 0.30882352590560913)
[2024-12-12 02:18:29,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:29,348][root][INFO] - Training Epoch: 1/10, step 199/574 completed (loss: 2.5751616954803467, acc: 0.40441176295280457)
[2024-12-12 02:18:29,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:29,739][root][INFO] - Training Epoch: 1/10, step 200/574 completed (loss: 2.808593511581421, acc: 0.29661017656326294)
[2024-12-12 02:18:29,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:30,114][root][INFO] - Training Epoch: 1/10, step 201/574 completed (loss: 2.767141342163086, acc: 0.3731343150138855)
[2024-12-12 02:18:30,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:30,478][root][INFO] - Training Epoch: 1/10, step 202/574 completed (loss: 2.86542010307312, acc: 0.3300970792770386)
[2024-12-12 02:18:30,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:30,906][root][INFO] - Training Epoch: 1/10, step 203/574 completed (loss: 2.6611886024475098, acc: 0.3650793731212616)
[2024-12-12 02:18:31,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:31,269][root][INFO] - Training Epoch: 1/10, step 204/574 completed (loss: 2.68414568901062, acc: 0.32967033982276917)
[2024-12-12 02:18:31,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:31,684][root][INFO] - Training Epoch: 1/10, step 205/574 completed (loss: 2.773693799972534, acc: 0.30493274331092834)
[2024-12-12 02:18:31,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:32,120][root][INFO] - Training Epoch: 1/10, step 206/574 completed (loss: 2.6185429096221924, acc: 0.35826772451400757)
[2024-12-12 02:18:32,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:32,459][root][INFO] - Training Epoch: 1/10, step 207/574 completed (loss: 2.678295373916626, acc: 0.3103448152542114)
[2024-12-12 02:18:32,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:32,813][root][INFO] - Training Epoch: 1/10, step 208/574 completed (loss: 2.4485816955566406, acc: 0.4057970941066742)
[2024-12-12 02:18:32,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:33,180][root][INFO] - Training Epoch: 1/10, step 209/574 completed (loss: 2.7341668605804443, acc: 0.2996108829975128)
[2024-12-12 02:18:33,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:33,483][root][INFO] - Training Epoch: 1/10, step 210/574 completed (loss: 2.824483871459961, acc: 0.27173912525177)
[2024-12-12 02:18:33,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:33,839][root][INFO] - Training Epoch: 1/10, step 211/574 completed (loss: 2.969892740249634, acc: 0.30434781312942505)
[2024-12-12 02:18:33,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:34,263][root][INFO] - Training Epoch: 1/10, step 212/574 completed (loss: 2.9880542755126953, acc: 0.1071428582072258)
[2024-12-12 02:18:34,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:34,692][root][INFO] - Training Epoch: 1/10, step 213/574 completed (loss: 2.635399341583252, acc: 0.23404255509376526)
[2024-12-12 02:18:34,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:35,412][root][INFO] - Training Epoch: 1/10, step 214/574 completed (loss: 2.6418676376342773, acc: 0.30000001192092896)
[2024-12-12 02:18:35,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:35,761][root][INFO] - Training Epoch: 1/10, step 215/574 completed (loss: 2.5071845054626465, acc: 0.3243243098258972)
[2024-12-12 02:18:35,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:36,108][root][INFO] - Training Epoch: 1/10, step 216/574 completed (loss: 2.670713186264038, acc: 0.3255814015865326)
[2024-12-12 02:18:36,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:36,681][root][INFO] - Training Epoch: 1/10, step 217/574 completed (loss: 2.4694721698760986, acc: 0.36936935782432556)
[2024-12-12 02:18:36,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:37,118][root][INFO] - Training Epoch: 1/10, step 218/574 completed (loss: 2.4885575771331787, acc: 0.35555556416511536)
[2024-12-12 02:18:37,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:37,514][root][INFO] - Training Epoch: 1/10, step 219/574 completed (loss: 2.180023670196533, acc: 0.42424243688583374)
[2024-12-12 02:18:37,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:37,821][root][INFO] - Training Epoch: 1/10, step 220/574 completed (loss: 1.8524149656295776, acc: 0.48148149251937866)
[2024-12-12 02:18:37,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:38,207][root][INFO] - Training Epoch: 1/10, step 221/574 completed (loss: 2.017904043197632, acc: 0.3199999928474426)
[2024-12-12 02:18:38,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:38,632][root][INFO] - Training Epoch: 1/10, step 222/574 completed (loss: 2.770874500274658, acc: 0.3076923191547394)
[2024-12-12 02:18:38,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:39,483][root][INFO] - Training Epoch: 1/10, step 223/574 completed (loss: 2.294497013092041, acc: 0.42391303181648254)
[2024-12-12 02:18:39,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:40,031][root][INFO] - Training Epoch: 1/10, step 224/574 completed (loss: 2.591890335083008, acc: 0.3295454680919647)
[2024-12-12 02:18:40,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:40,481][root][INFO] - Training Epoch: 1/10, step 225/574 completed (loss: 2.714588165283203, acc: 0.28723403811454773)
[2024-12-12 02:18:40,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:40,888][root][INFO] - Training Epoch: 1/10, step 226/574 completed (loss: 2.7272634506225586, acc: 0.2830188572406769)
[2024-12-12 02:18:41,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:41,258][root][INFO] - Training Epoch: 1/10, step 227/574 completed (loss: 2.4106924533843994, acc: 0.4166666567325592)
[2024-12-12 02:18:41,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:41,575][root][INFO] - Training Epoch: 1/10, step 228/574 completed (loss: 2.027787208557129, acc: 0.41860464215278625)
[2024-12-12 02:18:41,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:41,861][root][INFO] - Training Epoch: 1/10, step 229/574 completed (loss: 2.355448007583618, acc: 0.3333333432674408)
[2024-12-12 02:18:41,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:42,233][root][INFO] - Training Epoch: 1/10, step 230/574 completed (loss: 2.7691049575805664, acc: 0.3052631616592407)
[2024-12-12 02:18:42,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:42,565][root][INFO] - Training Epoch: 1/10, step 231/574 completed (loss: 2.3499083518981934, acc: 0.35555556416511536)
[2024-12-12 02:18:42,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:43,053][root][INFO] - Training Epoch: 1/10, step 232/574 completed (loss: 2.1442887783050537, acc: 0.4611110985279083)
[2024-12-12 02:18:43,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:43,557][root][INFO] - Training Epoch: 1/10, step 233/574 completed (loss: 2.3166909217834473, acc: 0.4128440320491791)
[2024-12-12 02:18:43,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:44,035][root][INFO] - Training Epoch: 1/10, step 234/574 completed (loss: 2.1690380573272705, acc: 0.42307692766189575)
[2024-12-12 02:18:44,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:44,348][root][INFO] - Training Epoch: 1/10, step 235/574 completed (loss: 1.9326350688934326, acc: 0.2631579041481018)
[2024-12-12 02:18:44,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:44,709][root][INFO] - Training Epoch: 1/10, step 236/574 completed (loss: 1.980446457862854, acc: 0.3333333432674408)
[2024-12-12 02:18:44,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:45,073][root][INFO] - Training Epoch: 1/10, step 237/574 completed (loss: 2.4577951431274414, acc: 0.3181818127632141)
[2024-12-12 02:18:45,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:45,400][root][INFO] - Training Epoch: 1/10, step 238/574 completed (loss: 2.0156843662261963, acc: 0.3333333432674408)
[2024-12-12 02:18:45,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:45,741][root][INFO] - Training Epoch: 1/10, step 239/574 completed (loss: 2.2040131092071533, acc: 0.5428571701049805)
[2024-12-12 02:18:45,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:46,104][root][INFO] - Training Epoch: 1/10, step 240/574 completed (loss: 2.0143022537231445, acc: 0.4545454680919647)
[2024-12-12 02:18:46,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:46,424][root][INFO] - Training Epoch: 1/10, step 241/574 completed (loss: 2.2556779384613037, acc: 0.5)
[2024-12-12 02:18:46,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:47,033][root][INFO] - Training Epoch: 1/10, step 242/574 completed (loss: 2.367371082305908, acc: 0.3709677457809448)
[2024-12-12 02:18:47,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:47,568][root][INFO] - Training Epoch: 1/10, step 243/574 completed (loss: 2.2365212440490723, acc: 0.40909090638160706)
[2024-12-12 02:18:47,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:47,943][root][INFO] - Training Epoch: 1/10, step 244/574 completed (loss: 1.8238706588745117, acc: 0.4285714328289032)
[2024-12-12 02:18:48,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:48,324][root][INFO] - Training Epoch: 1/10, step 245/574 completed (loss: 2.303589344024658, acc: 0.3461538553237915)
[2024-12-12 02:18:48,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:48,702][root][INFO] - Training Epoch: 1/10, step 246/574 completed (loss: 2.9141170978546143, acc: 0.22580644488334656)
[2024-12-12 02:18:48,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:49,034][root][INFO] - Training Epoch: 1/10, step 247/574 completed (loss: 2.070608377456665, acc: 0.44999998807907104)
[2024-12-12 02:18:49,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:49,473][root][INFO] - Training Epoch: 1/10, step 248/574 completed (loss: 2.3541762828826904, acc: 0.4054054021835327)
[2024-12-12 02:18:49,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:49,905][root][INFO] - Training Epoch: 1/10, step 249/574 completed (loss: 2.269416093826294, acc: 0.3513513505458832)
[2024-12-12 02:18:50,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:50,271][root][INFO] - Training Epoch: 1/10, step 250/574 completed (loss: 2.450157880783081, acc: 0.3243243098258972)
[2024-12-12 02:18:50,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:50,693][root][INFO] - Training Epoch: 1/10, step 251/574 completed (loss: 2.6356794834136963, acc: 0.38235294818878174)
[2024-12-12 02:18:50,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:51,039][root][INFO] - Training Epoch: 1/10, step 252/574 completed (loss: 1.6362254619598389, acc: 0.4878048896789551)
[2024-12-12 02:18:51,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:51,409][root][INFO] - Training Epoch: 1/10, step 253/574 completed (loss: 1.714198112487793, acc: 0.6399999856948853)
[2024-12-12 02:18:51,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:51,761][root][INFO] - Training Epoch: 1/10, step 254/574 completed (loss: 1.334296464920044, acc: 0.6000000238418579)
[2024-12-12 02:18:51,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:52,067][root][INFO] - Training Epoch: 1/10, step 255/574 completed (loss: 2.3937392234802246, acc: 0.32258063554763794)
[2024-12-12 02:18:52,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:52,436][root][INFO] - Training Epoch: 1/10, step 256/574 completed (loss: 2.5736396312713623, acc: 0.35087719559669495)
[2024-12-12 02:18:52,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:52,823][root][INFO] - Training Epoch: 1/10, step 257/574 completed (loss: 2.7231452465057373, acc: 0.3142857253551483)
[2024-12-12 02:18:52,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:53,189][root][INFO] - Training Epoch: 1/10, step 258/574 completed (loss: 2.4175946712493896, acc: 0.3815789520740509)
[2024-12-12 02:18:53,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:53,762][root][INFO] - Training Epoch: 1/10, step 259/574 completed (loss: 2.3606655597686768, acc: 0.38679245114326477)
[2024-12-12 02:18:53,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:54,377][root][INFO] - Training Epoch: 1/10, step 260/574 completed (loss: 2.476402521133423, acc: 0.3583333194255829)
[2024-12-12 02:18:54,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:54,696][root][INFO] - Training Epoch: 1/10, step 261/574 completed (loss: 2.434907913208008, acc: 0.4444444477558136)
[2024-12-12 02:18:54,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:55,119][root][INFO] - Training Epoch: 1/10, step 262/574 completed (loss: 2.640997886657715, acc: 0.32258063554763794)
[2024-12-12 02:18:55,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:55,525][root][INFO] - Training Epoch: 1/10, step 263/574 completed (loss: 2.9887022972106934, acc: 0.3199999928474426)
[2024-12-12 02:18:55,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:55,922][root][INFO] - Training Epoch: 1/10, step 264/574 completed (loss: 2.692176580429077, acc: 0.375)
[2024-12-12 02:18:56,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:56,773][root][INFO] - Training Epoch: 1/10, step 265/574 completed (loss: 2.7177035808563232, acc: 0.31200000643730164)
[2024-12-12 02:18:56,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:57,161][root][INFO] - Training Epoch: 1/10, step 266/574 completed (loss: 2.5164198875427246, acc: 0.33707866072654724)
[2024-12-12 02:18:57,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:57,535][root][INFO] - Training Epoch: 1/10, step 267/574 completed (loss: 2.64658784866333, acc: 0.3243243098258972)
[2024-12-12 02:18:57,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:57,994][root][INFO] - Training Epoch: 1/10, step 268/574 completed (loss: 2.0062899589538574, acc: 0.43103447556495667)
[2024-12-12 02:18:58,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:58,355][root][INFO] - Training Epoch: 1/10, step 269/574 completed (loss: 2.216832160949707, acc: 0.3636363744735718)
[2024-12-12 02:18:58,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:58,721][root][INFO] - Training Epoch: 1/10, step 270/574 completed (loss: 1.8581757545471191, acc: 0.5)
[2024-12-12 02:18:58,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:59,119][root][INFO] - Training Epoch: 1/10, step 271/574 completed (loss: 1.9117352962493896, acc: 0.5)
[2024-12-12 02:18:59,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:59,480][root][INFO] - Training Epoch: 1/10, step 272/574 completed (loss: 2.002324342727661, acc: 0.46666666865348816)
[2024-12-12 02:18:59,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:18:59,910][root][INFO] - Training Epoch: 1/10, step 273/574 completed (loss: 2.372627019882202, acc: 0.38333332538604736)
[2024-12-12 02:19:00,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:00,227][root][INFO] - Training Epoch: 1/10, step 274/574 completed (loss: 2.1618518829345703, acc: 0.375)
[2024-12-12 02:19:00,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:00,621][root][INFO] - Training Epoch: 1/10, step 275/574 completed (loss: 1.8139039278030396, acc: 0.6000000238418579)
[2024-12-12 02:19:00,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:01,036][root][INFO] - Training Epoch: 1/10, step 276/574 completed (loss: 2.1593892574310303, acc: 0.37931033968925476)
[2024-12-12 02:19:01,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:01,330][root][INFO] - Training Epoch: 1/10, step 277/574 completed (loss: 1.974724292755127, acc: 0.4399999976158142)
[2024-12-12 02:19:01,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:01,638][root][INFO] - Training Epoch: 1/10, step 278/574 completed (loss: 2.6283907890319824, acc: 0.3191489279270172)
[2024-12-12 02:19:01,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:02,017][root][INFO] - Training Epoch: 1/10, step 279/574 completed (loss: 2.1786577701568604, acc: 0.4375)
[2024-12-12 02:19:02,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:02,448][root][INFO] - Training Epoch: 1/10, step 280/574 completed (loss: 2.021841287612915, acc: 0.4318181872367859)
[2024-12-12 02:19:02,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:02,949][root][INFO] - Training Epoch: 1/10, step 281/574 completed (loss: 2.5822582244873047, acc: 0.34939759969711304)
[2024-12-12 02:19:03,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:03,367][root][INFO] - Training Epoch: 1/10, step 282/574 completed (loss: 2.5706770420074463, acc: 0.35185185074806213)
[2024-12-12 02:19:03,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:03,734][root][INFO] - Training Epoch: 1/10, step 283/574 completed (loss: 2.6702349185943604, acc: 0.21052631735801697)
[2024-12-12 02:19:03,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:04,140][root][INFO] - Training Epoch: 1/10, step 284/574 completed (loss: 2.583256483078003, acc: 0.1764705926179886)
[2024-12-12 02:19:04,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:04,492][root][INFO] - Training Epoch: 1/10, step 285/574 completed (loss: 2.397250175476074, acc: 0.2750000059604645)
[2024-12-12 02:19:05,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:05,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:05,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:06,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:06,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:06,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:07,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:07,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:07,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:08,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:08,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:09,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:09,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:09,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:10,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:10,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:10,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:11,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:11,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:12,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:12,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:12,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:13,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:13,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:13,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:14,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:14,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:14,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:15,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:15,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:15,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:16,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:16,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:16,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:17,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:17,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:17,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:18,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:18,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:18,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:19,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:19,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:19,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:20,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:20,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:20,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:21,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:21,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:21,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:22,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:22,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:22,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:23,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:23,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:23,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:24,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:24,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:24,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:25,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:25,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:25,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:26,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:26,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:26,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:27,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:27,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:27,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:28,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:28,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:29,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:29,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:29,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:30,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:30,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:30,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:31,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:31,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:31,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:32,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:32,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:33,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:33,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:33,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:34,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:34,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:35,159][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(10.5779, device='cuda:0') eval_epoch_loss=tensor(2.3588, device='cuda:0') eval_epoch_acc=tensor(0.3930, device='cuda:0')
[2024-12-12 02:19:35,160][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:19:35,160][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:19:35,370][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_1_step_286_loss_2.358767032623291/model.pt
[2024-12-12 02:19:35,373][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:19:35,374][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 2.358767032623291
[2024-12-12 02:19:35,374][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.3929707705974579
[2024-12-12 02:19:35,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:35,793][root][INFO] - Training Epoch: 1/10, step 286/574 completed (loss: 2.41267991065979, acc: 0.359375)
[2024-12-12 02:19:35,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:36,163][root][INFO] - Training Epoch: 1/10, step 287/574 completed (loss: 2.6872596740722656, acc: 0.2800000011920929)
[2024-12-12 02:19:36,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:36,529][root][INFO] - Training Epoch: 1/10, step 288/574 completed (loss: 2.3194000720977783, acc: 0.4175824224948883)
[2024-12-12 02:19:36,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:36,845][root][INFO] - Training Epoch: 1/10, step 289/574 completed (loss: 2.668382167816162, acc: 0.2484472095966339)
[2024-12-12 02:19:36,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:37,200][root][INFO] - Training Epoch: 1/10, step 290/574 completed (loss: 2.686685800552368, acc: 0.3041236996650696)
[2024-12-12 02:19:37,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:37,508][root][INFO] - Training Epoch: 1/10, step 291/574 completed (loss: 1.4662258625030518, acc: 0.7272727489471436)
[2024-12-12 02:19:37,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:37,848][root][INFO] - Training Epoch: 1/10, step 292/574 completed (loss: 2.521040916442871, acc: 0.3095238208770752)
[2024-12-12 02:19:37,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:38,262][root][INFO] - Training Epoch: 1/10, step 293/574 completed (loss: 2.1298904418945312, acc: 0.5)
[2024-12-12 02:19:38,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:38,787][root][INFO] - Training Epoch: 1/10, step 294/574 completed (loss: 1.8365105390548706, acc: 0.5090909004211426)
[2024-12-12 02:19:38,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:39,357][root][INFO] - Training Epoch: 1/10, step 295/574 completed (loss: 2.1238930225372314, acc: 0.469072163105011)
[2024-12-12 02:19:39,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:39,686][root][INFO] - Training Epoch: 1/10, step 296/574 completed (loss: 2.5144553184509277, acc: 0.37931033968925476)
[2024-12-12 02:19:39,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:40,054][root][INFO] - Training Epoch: 1/10, step 297/574 completed (loss: 2.2779672145843506, acc: 0.48148149251937866)
[2024-12-12 02:19:40,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:40,441][root][INFO] - Training Epoch: 1/10, step 298/574 completed (loss: 2.331329345703125, acc: 0.44736841320991516)
[2024-12-12 02:19:40,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:40,835][root][INFO] - Training Epoch: 1/10, step 299/574 completed (loss: 2.3303773403167725, acc: 0.4642857015132904)
[2024-12-12 02:19:40,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:41,261][root][INFO] - Training Epoch: 1/10, step 300/574 completed (loss: 2.154785394668579, acc: 0.40625)
[2024-12-12 02:19:41,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:41,646][root][INFO] - Training Epoch: 1/10, step 301/574 completed (loss: 2.543303966522217, acc: 0.3396226465702057)
[2024-12-12 02:19:41,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:42,065][root][INFO] - Training Epoch: 1/10, step 302/574 completed (loss: 1.6249605417251587, acc: 0.5849056839942932)
[2024-12-12 02:19:42,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:42,469][root][INFO] - Training Epoch: 1/10, step 303/574 completed (loss: 2.001236915588379, acc: 0.47058823704719543)
[2024-12-12 02:19:42,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:42,805][root][INFO] - Training Epoch: 1/10, step 304/574 completed (loss: 2.5210471153259277, acc: 0.28125)
[2024-12-12 02:19:42,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:43,203][root][INFO] - Training Epoch: 1/10, step 305/574 completed (loss: 1.9852334260940552, acc: 0.4754098355770111)
[2024-12-12 02:19:43,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:43,586][root][INFO] - Training Epoch: 1/10, step 306/574 completed (loss: 1.5599521398544312, acc: 0.5666666626930237)
[2024-12-12 02:19:43,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:43,920][root][INFO] - Training Epoch: 1/10, step 307/574 completed (loss: 1.3463867902755737, acc: 0.6315789222717285)
[2024-12-12 02:19:44,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:44,247][root][INFO] - Training Epoch: 1/10, step 308/574 completed (loss: 2.69064998626709, acc: 0.30434781312942505)
[2024-12-12 02:19:44,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:44,672][root][INFO] - Training Epoch: 1/10, step 309/574 completed (loss: 2.2461490631103516, acc: 0.4305555522441864)
[2024-12-12 02:19:44,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:45,043][root][INFO] - Training Epoch: 1/10, step 310/574 completed (loss: 2.190964698791504, acc: 0.34939759969711304)
[2024-12-12 02:19:45,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:45,426][root][INFO] - Training Epoch: 1/10, step 311/574 completed (loss: 2.6318233013153076, acc: 0.29487180709838867)
[2024-12-12 02:19:45,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:45,803][root][INFO] - Training Epoch: 1/10, step 312/574 completed (loss: 2.7703046798706055, acc: 0.2551020383834839)
[2024-12-12 02:19:45,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:46,138][root][INFO] - Training Epoch: 1/10, step 313/574 completed (loss: 0.9988629817962646, acc: 0.7083333134651184)
[2024-12-12 02:19:46,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:46,500][root][INFO] - Training Epoch: 1/10, step 314/574 completed (loss: 1.8691130876541138, acc: 0.5416666865348816)
[2024-12-12 02:19:46,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:46,882][root][INFO] - Training Epoch: 1/10, step 315/574 completed (loss: 2.029076337814331, acc: 0.3870967626571655)
[2024-12-12 02:19:47,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:47,248][root][INFO] - Training Epoch: 1/10, step 316/574 completed (loss: 2.3788416385650635, acc: 0.35483869910240173)
[2024-12-12 02:19:47,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:47,649][root][INFO] - Training Epoch: 1/10, step 317/574 completed (loss: 1.9875128269195557, acc: 0.5223880410194397)
[2024-12-12 02:19:47,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:48,025][root][INFO] - Training Epoch: 1/10, step 318/574 completed (loss: 1.8261024951934814, acc: 0.5480769276618958)
[2024-12-12 02:19:48,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:48,385][root][INFO] - Training Epoch: 1/10, step 319/574 completed (loss: 2.790950298309326, acc: 0.20000000298023224)
[2024-12-12 02:19:48,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:48,747][root][INFO] - Training Epoch: 1/10, step 320/574 completed (loss: 2.238321542739868, acc: 0.4354838728904724)
[2024-12-12 02:19:48,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:49,118][root][INFO] - Training Epoch: 1/10, step 321/574 completed (loss: 1.4982928037643433, acc: 0.6600000262260437)
[2024-12-12 02:19:49,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:49,472][root][INFO] - Training Epoch: 1/10, step 322/574 completed (loss: 3.1104776859283447, acc: 0.25925925374031067)
[2024-12-12 02:19:49,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:49,830][root][INFO] - Training Epoch: 1/10, step 323/574 completed (loss: 3.7670886516571045, acc: 0.08571428805589676)
[2024-12-12 02:19:49,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:50,234][root][INFO] - Training Epoch: 1/10, step 324/574 completed (loss: 2.8004469871520996, acc: 0.25641027092933655)
[2024-12-12 02:19:50,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:50,646][root][INFO] - Training Epoch: 1/10, step 325/574 completed (loss: 2.898256540298462, acc: 0.3658536672592163)
[2024-12-12 02:19:50,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:50,995][root][INFO] - Training Epoch: 1/10, step 326/574 completed (loss: 2.37839412689209, acc: 0.42105263471603394)
[2024-12-12 02:19:51,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:51,347][root][INFO] - Training Epoch: 1/10, step 327/574 completed (loss: 1.7933176755905151, acc: 0.5789473652839661)
[2024-12-12 02:19:51,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:51,705][root][INFO] - Training Epoch: 1/10, step 328/574 completed (loss: 1.77530038356781, acc: 0.4642857015132904)
[2024-12-12 02:19:51,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:52,023][root][INFO] - Training Epoch: 1/10, step 329/574 completed (loss: 2.391737937927246, acc: 0.3333333432674408)
[2024-12-12 02:19:52,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:52,435][root][INFO] - Training Epoch: 1/10, step 330/574 completed (loss: 1.6667879819869995, acc: 0.53125)
[2024-12-12 02:19:52,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:52,796][root][INFO] - Training Epoch: 1/10, step 331/574 completed (loss: 2.3828394412994385, acc: 0.3709677457809448)
[2024-12-12 02:19:52,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:53,189][root][INFO] - Training Epoch: 1/10, step 332/574 completed (loss: 2.0298070907592773, acc: 0.4385964870452881)
[2024-12-12 02:19:53,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:53,527][root][INFO] - Training Epoch: 1/10, step 333/574 completed (loss: 2.658705711364746, acc: 0.25)
[2024-12-12 02:19:53,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:53,890][root][INFO] - Training Epoch: 1/10, step 334/574 completed (loss: 1.88821280002594, acc: 0.5666666626930237)
[2024-12-12 02:19:53,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:54,198][root][INFO] - Training Epoch: 1/10, step 335/574 completed (loss: 2.126284122467041, acc: 0.42105263471603394)
[2024-12-12 02:19:54,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:54,611][root][INFO] - Training Epoch: 1/10, step 336/574 completed (loss: 2.372537612915039, acc: 0.3199999928474426)
[2024-12-12 02:19:54,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:54,992][root][INFO] - Training Epoch: 1/10, step 337/574 completed (loss: 2.4930269718170166, acc: 0.3563218414783478)
[2024-12-12 02:19:55,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:55,386][root][INFO] - Training Epoch: 1/10, step 338/574 completed (loss: 2.611663579940796, acc: 0.3617021143436432)
[2024-12-12 02:19:55,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:55,757][root][INFO] - Training Epoch: 1/10, step 339/574 completed (loss: 2.6010313034057617, acc: 0.40963855385780334)
[2024-12-12 02:19:55,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:56,114][root][INFO] - Training Epoch: 1/10, step 340/574 completed (loss: 1.856216311454773, acc: 0.6521739363670349)
[2024-12-12 02:19:56,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:56,400][root][INFO] - Training Epoch: 1/10, step 341/574 completed (loss: 2.713225841522217, acc: 0.3589743673801422)
[2024-12-12 02:19:56,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:56,773][root][INFO] - Training Epoch: 1/10, step 342/574 completed (loss: 2.928299903869629, acc: 0.27710843086242676)
[2024-12-12 02:19:56,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:57,140][root][INFO] - Training Epoch: 1/10, step 343/574 completed (loss: 2.1174583435058594, acc: 0.4150943458080292)
[2024-12-12 02:19:57,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:57,477][root][INFO] - Training Epoch: 1/10, step 344/574 completed (loss: 2.615030527114868, acc: 0.3037974536418915)
[2024-12-12 02:19:57,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:57,859][root][INFO] - Training Epoch: 1/10, step 345/574 completed (loss: 2.414907455444336, acc: 0.3921568691730499)
[2024-12-12 02:19:58,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:58,229][root][INFO] - Training Epoch: 1/10, step 346/574 completed (loss: 2.828752040863037, acc: 0.31343284249305725)
[2024-12-12 02:19:58,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:58,610][root][INFO] - Training Epoch: 1/10, step 347/574 completed (loss: 1.9340918064117432, acc: 0.6000000238418579)
[2024-12-12 02:19:58,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:59,014][root][INFO] - Training Epoch: 1/10, step 348/574 completed (loss: 1.8783820867538452, acc: 0.47999998927116394)
[2024-12-12 02:19:59,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:59,478][root][INFO] - Training Epoch: 1/10, step 349/574 completed (loss: 1.7381155490875244, acc: 0.6388888955116272)
[2024-12-12 02:19:59,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:19:59,877][root][INFO] - Training Epoch: 1/10, step 350/574 completed (loss: 2.3918113708496094, acc: 0.39534884691238403)
[2024-12-12 02:20:00,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:00,272][root][INFO] - Training Epoch: 1/10, step 351/574 completed (loss: 2.23048734664917, acc: 0.3589743673801422)
[2024-12-12 02:20:00,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:00,682][root][INFO] - Training Epoch: 1/10, step 352/574 completed (loss: 2.3478872776031494, acc: 0.3777777850627899)
[2024-12-12 02:20:00,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:01,062][root][INFO] - Training Epoch: 1/10, step 353/574 completed (loss: 1.4534502029418945, acc: 0.47826087474823)
[2024-12-12 02:20:01,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:01,428][root][INFO] - Training Epoch: 1/10, step 354/574 completed (loss: 2.753960609436035, acc: 0.3076923191547394)
[2024-12-12 02:20:01,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:01,832][root][INFO] - Training Epoch: 1/10, step 355/574 completed (loss: 2.738637924194336, acc: 0.2967033088207245)
[2024-12-12 02:20:02,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:02,394][root][INFO] - Training Epoch: 1/10, step 356/574 completed (loss: 2.2135229110717773, acc: 0.38260868191719055)
[2024-12-12 02:20:02,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:02,728][root][INFO] - Training Epoch: 1/10, step 357/574 completed (loss: 2.4427318572998047, acc: 0.27173912525177)
[2024-12-12 02:20:02,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:03,127][root][INFO] - Training Epoch: 1/10, step 358/574 completed (loss: 2.47219181060791, acc: 0.2857142984867096)
[2024-12-12 02:20:03,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:03,484][root][INFO] - Training Epoch: 1/10, step 359/574 completed (loss: 1.0468299388885498, acc: 0.7083333134651184)
[2024-12-12 02:20:03,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:03,872][root][INFO] - Training Epoch: 1/10, step 360/574 completed (loss: 1.9390289783477783, acc: 0.4615384638309479)
[2024-12-12 02:20:04,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:04,249][root][INFO] - Training Epoch: 1/10, step 361/574 completed (loss: 2.329564094543457, acc: 0.39024388790130615)
[2024-12-12 02:20:04,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:04,565][root][INFO] - Training Epoch: 1/10, step 362/574 completed (loss: 1.986632227897644, acc: 0.46666666865348816)
[2024-12-12 02:20:04,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:04,942][root][INFO] - Training Epoch: 1/10, step 363/574 completed (loss: 2.4502129554748535, acc: 0.3947368562221527)
[2024-12-12 02:20:05,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:05,305][root][INFO] - Training Epoch: 1/10, step 364/574 completed (loss: 2.4903619289398193, acc: 0.39024388790130615)
[2024-12-12 02:20:05,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:05,628][root][INFO] - Training Epoch: 1/10, step 365/574 completed (loss: 2.5369057655334473, acc: 0.3636363744735718)
[2024-12-12 02:20:05,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:06,026][root][INFO] - Training Epoch: 1/10, step 366/574 completed (loss: 1.1850212812423706, acc: 0.6666666865348816)
[2024-12-12 02:20:06,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:06,356][root][INFO] - Training Epoch: 1/10, step 367/574 completed (loss: 0.9227114319801331, acc: 0.739130437374115)
[2024-12-12 02:20:06,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:06,681][root][INFO] - Training Epoch: 1/10, step 368/574 completed (loss: 1.6793363094329834, acc: 0.5)
[2024-12-12 02:20:06,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:07,058][root][INFO] - Training Epoch: 1/10, step 369/574 completed (loss: 1.9209511280059814, acc: 0.46875)
[2024-12-12 02:20:07,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:07,717][root][INFO] - Training Epoch: 1/10, step 370/574 completed (loss: 2.3170998096466064, acc: 0.41818180680274963)
[2024-12-12 02:20:08,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:08,660][root][INFO] - Training Epoch: 1/10, step 371/574 completed (loss: 1.7368437051773071, acc: 0.5849056839942932)
[2024-12-12 02:20:08,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:09,079][root][INFO] - Training Epoch: 1/10, step 372/574 completed (loss: 2.159815788269043, acc: 0.4333333373069763)
[2024-12-12 02:20:09,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:09,450][root][INFO] - Training Epoch: 1/10, step 373/574 completed (loss: 2.2333858013153076, acc: 0.4107142984867096)
[2024-12-12 02:20:09,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:09,817][root][INFO] - Training Epoch: 1/10, step 374/574 completed (loss: 1.2625737190246582, acc: 0.6571428775787354)
[2024-12-12 02:20:09,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:10,138][root][INFO] - Training Epoch: 1/10, step 375/574 completed (loss: 0.851235032081604, acc: 0.7599999904632568)
[2024-12-12 02:20:10,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:10,509][root][INFO] - Training Epoch: 1/10, step 376/574 completed (loss: 1.0937262773513794, acc: 0.6521739363670349)
[2024-12-12 02:20:10,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:10,829][root][INFO] - Training Epoch: 1/10, step 377/574 completed (loss: 2.9192378520965576, acc: 0.2708333432674408)
[2024-12-12 02:20:11,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:11,289][root][INFO] - Training Epoch: 1/10, step 378/574 completed (loss: 2.126586675643921, acc: 0.43157893419265747)
[2024-12-12 02:20:11,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:11,893][root][INFO] - Training Epoch: 1/10, step 379/574 completed (loss: 2.14469838142395, acc: 0.455089807510376)
[2024-12-12 02:20:12,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:12,290][root][INFO] - Training Epoch: 1/10, step 380/574 completed (loss: 1.913834810256958, acc: 0.49624061584472656)
[2024-12-12 02:20:12,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:13,623][root][INFO] - Training Epoch: 1/10, step 381/574 completed (loss: 2.0403337478637695, acc: 0.45989304780960083)
[2024-12-12 02:20:13,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:14,195][root][INFO] - Training Epoch: 1/10, step 382/574 completed (loss: 1.7282501459121704, acc: 0.522522509098053)
[2024-12-12 02:20:14,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:14,569][root][INFO] - Training Epoch: 1/10, step 383/574 completed (loss: 1.6520376205444336, acc: 0.4285714328289032)
[2024-12-12 02:20:14,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:14,912][root][INFO] - Training Epoch: 1/10, step 384/574 completed (loss: 1.2492755651474, acc: 0.7142857313156128)
[2024-12-12 02:20:15,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:15,278][root][INFO] - Training Epoch: 1/10, step 385/574 completed (loss: 2.058835983276367, acc: 0.4375)
[2024-12-12 02:20:15,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:15,661][root][INFO] - Training Epoch: 1/10, step 386/574 completed (loss: 2.1543235778808594, acc: 0.3888888955116272)
[2024-12-12 02:20:15,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:16,011][root][INFO] - Training Epoch: 1/10, step 387/574 completed (loss: 1.992445945739746, acc: 0.3947368562221527)
[2024-12-12 02:20:16,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:16,416][root][INFO] - Training Epoch: 1/10, step 388/574 completed (loss: 1.8695591688156128, acc: 0.5454545617103577)
[2024-12-12 02:20:16,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:16,771][root][INFO] - Training Epoch: 1/10, step 389/574 completed (loss: 1.9110243320465088, acc: 0.5)
[2024-12-12 02:20:16,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:17,097][root][INFO] - Training Epoch: 1/10, step 390/574 completed (loss: 1.8422597646713257, acc: 0.380952388048172)
[2024-12-12 02:20:17,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:17,473][root][INFO] - Training Epoch: 1/10, step 391/574 completed (loss: 2.4895145893096924, acc: 0.37037035822868347)
[2024-12-12 02:20:17,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:17,847][root][INFO] - Training Epoch: 1/10, step 392/574 completed (loss: 2.576061964035034, acc: 0.34951457381248474)
[2024-12-12 02:20:17,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:18,385][root][INFO] - Training Epoch: 1/10, step 393/574 completed (loss: 2.1483898162841797, acc: 0.4264705777168274)
[2024-12-12 02:20:18,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:18,769][root][INFO] - Training Epoch: 1/10, step 394/574 completed (loss: 2.612884283065796, acc: 0.3733333349227905)
[2024-12-12 02:20:18,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:19,210][root][INFO] - Training Epoch: 1/10, step 395/574 completed (loss: 2.4018774032592773, acc: 0.4097222089767456)
[2024-12-12 02:20:19,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:19,613][root][INFO] - Training Epoch: 1/10, step 396/574 completed (loss: 2.43744158744812, acc: 0.4883720874786377)
[2024-12-12 02:20:19,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:20,031][root][INFO] - Training Epoch: 1/10, step 397/574 completed (loss: 1.5963821411132812, acc: 0.5416666865348816)
[2024-12-12 02:20:20,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:20,398][root][INFO] - Training Epoch: 1/10, step 398/574 completed (loss: 2.060410737991333, acc: 0.39534884691238403)
[2024-12-12 02:20:20,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:20,719][root][INFO] - Training Epoch: 1/10, step 399/574 completed (loss: 2.248642683029175, acc: 0.4000000059604645)
[2024-12-12 02:20:20,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:21,282][root][INFO] - Training Epoch: 1/10, step 400/574 completed (loss: 2.1933765411376953, acc: 0.45588234066963196)
[2024-12-12 02:20:21,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:21,600][root][INFO] - Training Epoch: 1/10, step 401/574 completed (loss: 2.0774986743927, acc: 0.47999998927116394)
[2024-12-12 02:20:21,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:21,921][root][INFO] - Training Epoch: 1/10, step 402/574 completed (loss: 1.7798268795013428, acc: 0.5454545617103577)
[2024-12-12 02:20:22,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:22,315][root][INFO] - Training Epoch: 1/10, step 403/574 completed (loss: 2.0269601345062256, acc: 0.4545454680919647)
[2024-12-12 02:20:22,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:22,723][root][INFO] - Training Epoch: 1/10, step 404/574 completed (loss: 1.7023556232452393, acc: 0.4838709533214569)
[2024-12-12 02:20:22,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:23,119][root][INFO] - Training Epoch: 1/10, step 405/574 completed (loss: 2.373770236968994, acc: 0.40740740299224854)
[2024-12-12 02:20:23,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:23,513][root][INFO] - Training Epoch: 1/10, step 406/574 completed (loss: 1.1868706941604614, acc: 0.7200000286102295)
[2024-12-12 02:20:23,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:23,886][root][INFO] - Training Epoch: 1/10, step 407/574 completed (loss: 1.3272242546081543, acc: 0.5555555820465088)
[2024-12-12 02:20:23,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:24,266][root][INFO] - Training Epoch: 1/10, step 408/574 completed (loss: 1.3814858198165894, acc: 0.5555555820465088)
[2024-12-12 02:20:24,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:24,655][root][INFO] - Training Epoch: 1/10, step 409/574 completed (loss: 1.6900174617767334, acc: 0.5769230723381042)
[2024-12-12 02:20:24,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:25,022][root][INFO] - Training Epoch: 1/10, step 410/574 completed (loss: 1.7516255378723145, acc: 0.5517241358757019)
[2024-12-12 02:20:25,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:25,396][root][INFO] - Training Epoch: 1/10, step 411/574 completed (loss: 1.1990948915481567, acc: 0.6785714030265808)
[2024-12-12 02:20:25,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:25,740][root][INFO] - Training Epoch: 1/10, step 412/574 completed (loss: 1.396342158317566, acc: 0.6666666865348816)
[2024-12-12 02:20:25,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:26,101][root][INFO] - Training Epoch: 1/10, step 413/574 completed (loss: 1.588439702987671, acc: 0.5757575631141663)
[2024-12-12 02:20:26,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:26,405][root][INFO] - Training Epoch: 1/10, step 414/574 completed (loss: 2.1491317749023438, acc: 0.3636363744735718)
[2024-12-12 02:20:26,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:26,797][root][INFO] - Training Epoch: 1/10, step 415/574 completed (loss: 2.353351354598999, acc: 0.5098039507865906)
[2024-12-12 02:20:26,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:27,199][root][INFO] - Training Epoch: 1/10, step 416/574 completed (loss: 2.2923243045806885, acc: 0.4615384638309479)
[2024-12-12 02:20:27,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:27,525][root][INFO] - Training Epoch: 1/10, step 417/574 completed (loss: 1.8761850595474243, acc: 0.5)
[2024-12-12 02:20:27,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:27,862][root][INFO] - Training Epoch: 1/10, step 418/574 completed (loss: 2.0560483932495117, acc: 0.550000011920929)
[2024-12-12 02:20:27,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:28,204][root][INFO] - Training Epoch: 1/10, step 419/574 completed (loss: 3.1418404579162598, acc: 0.4000000059604645)
[2024-12-12 02:20:28,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:28,556][root][INFO] - Training Epoch: 1/10, step 420/574 completed (loss: 0.8500633239746094, acc: 0.8095238208770752)
[2024-12-12 02:20:28,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:28,970][root][INFO] - Training Epoch: 1/10, step 421/574 completed (loss: 2.552142858505249, acc: 0.36666667461395264)
[2024-12-12 02:20:29,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:29,340][root][INFO] - Training Epoch: 1/10, step 422/574 completed (loss: 2.386871814727783, acc: 0.375)
[2024-12-12 02:20:29,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:29,715][root][INFO] - Training Epoch: 1/10, step 423/574 completed (loss: 1.9751580953598022, acc: 0.5)
[2024-12-12 02:20:29,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:30,040][root][INFO] - Training Epoch: 1/10, step 424/574 completed (loss: 1.8639482259750366, acc: 0.48148149251937866)
[2024-12-12 02:20:30,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:30,399][root][INFO] - Training Epoch: 1/10, step 425/574 completed (loss: 1.5775446891784668, acc: 0.6060606241226196)
[2024-12-12 02:20:30,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:30,775][root][INFO] - Training Epoch: 1/10, step 426/574 completed (loss: 1.4909206628799438, acc: 0.52173912525177)
[2024-12-12 02:20:30,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:31,122][root][INFO] - Training Epoch: 1/10, step 427/574 completed (loss: 1.6359978914260864, acc: 0.5675675868988037)
[2024-12-12 02:20:31,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:31,425][root][INFO] - Training Epoch: 1/10, step 428/574 completed (loss: 1.399214744567871, acc: 0.6666666865348816)
[2024-12-12 02:20:32,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:32,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:32,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:33,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:33,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:33,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:34,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:34,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:34,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:35,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:35,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:36,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:36,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:36,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:37,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:37,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:37,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:38,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:38,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:38,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:39,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:39,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:39,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:39,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:40,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:40,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:40,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:41,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:41,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:41,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:42,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:42,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:42,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:43,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:43,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:43,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:44,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:44,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:45,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:45,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:45,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:46,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:46,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:46,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:47,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:47,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:47,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:48,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:48,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:48,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:49,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:49,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:49,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:50,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:50,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:50,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:51,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:51,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:52,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:52,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:52,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:53,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:53,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:54,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:54,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:54,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:55,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:55,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:55,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:56,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:56,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:56,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:57,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:57,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:57,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:58,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:58,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:58,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:59,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:59,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:20:59,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:00,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:00,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:00,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:01,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:01,821][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(11.2573, device='cuda:0') eval_epoch_loss=tensor(2.4210, device='cuda:0') eval_epoch_acc=tensor(0.3807, device='cuda:0')
[2024-12-12 02:21:01,824][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:21:01,825][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:21:02,121][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_1_step_429_loss_2.4210212230682373/model.pt
[2024-12-12 02:21:02,133][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:21:02,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:02,495][root][INFO] - Training Epoch: 1/10, step 429/574 completed (loss: 2.15199875831604, acc: 0.47826087474823)
[2024-12-12 02:21:02,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:02,815][root][INFO] - Training Epoch: 1/10, step 430/574 completed (loss: 1.1849454641342163, acc: 0.7407407164573669)
[2024-12-12 02:21:02,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:03,100][root][INFO] - Training Epoch: 1/10, step 431/574 completed (loss: 1.1038269996643066, acc: 0.6666666865348816)
[2024-12-12 02:21:03,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:03,394][root][INFO] - Training Epoch: 1/10, step 432/574 completed (loss: 1.8819012641906738, acc: 0.52173912525177)
[2024-12-12 02:21:03,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:03,784][root][INFO] - Training Epoch: 1/10, step 433/574 completed (loss: 1.6829614639282227, acc: 0.5833333134651184)
[2024-12-12 02:21:03,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:04,184][root][INFO] - Training Epoch: 1/10, step 434/574 completed (loss: 0.8723269701004028, acc: 0.7599999904632568)
[2024-12-12 02:21:04,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:04,551][root][INFO] - Training Epoch: 1/10, step 435/574 completed (loss: 1.8153777122497559, acc: 0.4848484992980957)
[2024-12-12 02:21:04,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:04,886][root][INFO] - Training Epoch: 1/10, step 436/574 completed (loss: 1.7009094953536987, acc: 0.4722222089767456)
[2024-12-12 02:21:05,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:05,236][root][INFO] - Training Epoch: 1/10, step 437/574 completed (loss: 1.9481929540634155, acc: 0.5454545617103577)
[2024-12-12 02:21:05,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:05,596][root][INFO] - Training Epoch: 1/10, step 438/574 completed (loss: 0.7019597887992859, acc: 0.761904776096344)
[2024-12-12 02:21:05,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:05,999][root][INFO] - Training Epoch: 1/10, step 439/574 completed (loss: 2.3745741844177246, acc: 0.4615384638309479)
[2024-12-12 02:21:06,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:06,529][root][INFO] - Training Epoch: 1/10, step 440/574 completed (loss: 2.449388265609741, acc: 0.3636363744735718)
[2024-12-12 02:21:06,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:07,346][root][INFO] - Training Epoch: 1/10, step 441/574 completed (loss: 3.2111854553222656, acc: 0.25600001215934753)
[2024-12-12 02:21:07,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:07,783][root][INFO] - Training Epoch: 1/10, step 442/574 completed (loss: 2.8772997856140137, acc: 0.3145161271095276)
[2024-12-12 02:21:08,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:08,490][root][INFO] - Training Epoch: 1/10, step 443/574 completed (loss: 2.5747647285461426, acc: 0.3283582031726837)
[2024-12-12 02:21:08,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:08,837][root][INFO] - Training Epoch: 1/10, step 444/574 completed (loss: 2.5576391220092773, acc: 0.3396226465702057)
[2024-12-12 02:21:08,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:09,273][root][INFO] - Training Epoch: 1/10, step 445/574 completed (loss: 1.669185757637024, acc: 0.5454545617103577)
[2024-12-12 02:21:09,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:09,637][root][INFO] - Training Epoch: 1/10, step 446/574 completed (loss: 2.056206703186035, acc: 0.52173912525177)
[2024-12-12 02:21:09,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:09,961][root][INFO] - Training Epoch: 1/10, step 447/574 completed (loss: 1.9557874202728271, acc: 0.5384615659713745)
[2024-12-12 02:21:10,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:10,365][root][INFO] - Training Epoch: 1/10, step 448/574 completed (loss: 1.7372218370437622, acc: 0.4642857015132904)
[2024-12-12 02:21:10,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:10,755][root][INFO] - Training Epoch: 1/10, step 449/574 completed (loss: 2.70329213142395, acc: 0.35820895433425903)
[2024-12-12 02:21:10,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:11,085][root][INFO] - Training Epoch: 1/10, step 450/574 completed (loss: 2.1733415126800537, acc: 0.4583333432674408)
[2024-12-12 02:21:11,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:11,479][root][INFO] - Training Epoch: 1/10, step 451/574 completed (loss: 2.5098471641540527, acc: 0.3804347813129425)
[2024-12-12 02:21:11,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:11,830][root][INFO] - Training Epoch: 1/10, step 452/574 completed (loss: 2.5250353813171387, acc: 0.3589743673801422)
[2024-12-12 02:21:11,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:12,164][root][INFO] - Training Epoch: 1/10, step 453/574 completed (loss: 2.7824432849884033, acc: 0.3552631437778473)
[2024-12-12 02:21:12,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:12,524][root][INFO] - Training Epoch: 1/10, step 454/574 completed (loss: 2.0118820667266846, acc: 0.4897959232330322)
[2024-12-12 02:21:12,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:12,936][root][INFO] - Training Epoch: 1/10, step 455/574 completed (loss: 1.8232885599136353, acc: 0.5151515007019043)
[2024-12-12 02:21:13,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:13,310][root][INFO] - Training Epoch: 1/10, step 456/574 completed (loss: 2.3721044063568115, acc: 0.2989690601825714)
[2024-12-12 02:21:13,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:13,755][root][INFO] - Training Epoch: 1/10, step 457/574 completed (loss: 2.264326810836792, acc: 0.37142857909202576)
[2024-12-12 02:21:13,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:14,208][root][INFO] - Training Epoch: 1/10, step 458/574 completed (loss: 2.298038959503174, acc: 0.3720930218696594)
[2024-12-12 02:21:14,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:14,556][root][INFO] - Training Epoch: 1/10, step 459/574 completed (loss: 2.5175845623016357, acc: 0.375)
[2024-12-12 02:21:14,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:14,881][root][INFO] - Training Epoch: 1/10, step 460/574 completed (loss: 2.2502708435058594, acc: 0.37037035822868347)
[2024-12-12 02:21:14,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:15,202][root][INFO] - Training Epoch: 1/10, step 461/574 completed (loss: 1.7027348279953003, acc: 0.5277777910232544)
[2024-12-12 02:21:15,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:15,492][root][INFO] - Training Epoch: 1/10, step 462/574 completed (loss: 1.8233356475830078, acc: 0.5625)
[2024-12-12 02:21:15,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:15,815][root][INFO] - Training Epoch: 1/10, step 463/574 completed (loss: 1.9397411346435547, acc: 0.5384615659713745)
[2024-12-12 02:21:15,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:16,157][root][INFO] - Training Epoch: 1/10, step 464/574 completed (loss: 2.298577070236206, acc: 0.45652174949645996)
[2024-12-12 02:21:16,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:16,545][root][INFO] - Training Epoch: 1/10, step 465/574 completed (loss: 2.4891130924224854, acc: 0.261904776096344)
[2024-12-12 02:21:16,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:16,965][root][INFO] - Training Epoch: 1/10, step 466/574 completed (loss: 2.5904343128204346, acc: 0.3253012001514435)
[2024-12-12 02:21:17,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:17,415][root][INFO] - Training Epoch: 1/10, step 467/574 completed (loss: 2.2416484355926514, acc: 0.4144144058227539)
[2024-12-12 02:21:17,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:17,845][root][INFO] - Training Epoch: 1/10, step 468/574 completed (loss: 2.3663744926452637, acc: 0.4563106894493103)
[2024-12-12 02:21:17,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:18,226][root][INFO] - Training Epoch: 1/10, step 469/574 completed (loss: 2.1242167949676514, acc: 0.4146341383457184)
[2024-12-12 02:21:18,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:18,519][root][INFO] - Training Epoch: 1/10, step 470/574 completed (loss: 1.885656714439392, acc: 0.5)
[2024-12-12 02:21:18,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:18,887][root][INFO] - Training Epoch: 1/10, step 471/574 completed (loss: 2.7873876094818115, acc: 0.2857142984867096)
[2024-12-12 02:21:19,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:19,328][root][INFO] - Training Epoch: 1/10, step 472/574 completed (loss: 2.3558526039123535, acc: 0.3333333432674408)
[2024-12-12 02:21:19,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:19,690][root][INFO] - Training Epoch: 1/10, step 473/574 completed (loss: 2.5086019039154053, acc: 0.31877729296684265)
[2024-12-12 02:21:19,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:20,023][root][INFO] - Training Epoch: 1/10, step 474/574 completed (loss: 2.4012787342071533, acc: 0.3645833432674408)
[2024-12-12 02:21:20,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:20,405][root][INFO] - Training Epoch: 1/10, step 475/574 completed (loss: 2.406599521636963, acc: 0.3680981695652008)
[2024-12-12 02:21:20,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:20,756][root][INFO] - Training Epoch: 1/10, step 476/574 completed (loss: 2.527111053466797, acc: 0.36690646409988403)
[2024-12-12 02:21:20,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:21,097][root][INFO] - Training Epoch: 1/10, step 477/574 completed (loss: 2.440185546875, acc: 0.35175880789756775)
[2024-12-12 02:21:21,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:21,400][root][INFO] - Training Epoch: 1/10, step 478/574 completed (loss: 1.6598536968231201, acc: 0.5833333134651184)
[2024-12-12 02:21:21,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:21,714][root][INFO] - Training Epoch: 1/10, step 479/574 completed (loss: 1.6567323207855225, acc: 0.5454545617103577)
[2024-12-12 02:21:21,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:22,030][root][INFO] - Training Epoch: 1/10, step 480/574 completed (loss: 1.795120358467102, acc: 0.37037035822868347)
[2024-12-12 02:21:22,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:22,417][root][INFO] - Training Epoch: 1/10, step 481/574 completed (loss: 1.9994043111801147, acc: 0.550000011920929)
[2024-12-12 02:21:22,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:22,775][root][INFO] - Training Epoch: 1/10, step 482/574 completed (loss: 0.9125941395759583, acc: 0.75)
[2024-12-12 02:21:22,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:23,185][root][INFO] - Training Epoch: 1/10, step 483/574 completed (loss: 1.8820226192474365, acc: 0.517241358757019)
[2024-12-12 02:21:23,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:23,510][root][INFO] - Training Epoch: 1/10, step 484/574 completed (loss: 1.4653018712997437, acc: 0.6774193644523621)
[2024-12-12 02:21:23,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:23,878][root][INFO] - Training Epoch: 1/10, step 485/574 completed (loss: 1.2950737476348877, acc: 0.7368420958518982)
[2024-12-12 02:21:23,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:24,211][root][INFO] - Training Epoch: 1/10, step 486/574 completed (loss: 2.3894407749176025, acc: 0.40740740299224854)
[2024-12-12 02:21:24,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:24,577][root][INFO] - Training Epoch: 1/10, step 487/574 completed (loss: 2.2452402114868164, acc: 0.3333333432674408)
[2024-12-12 02:21:24,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:24,921][root][INFO] - Training Epoch: 1/10, step 488/574 completed (loss: 1.7355750799179077, acc: 0.5454545617103577)
[2024-12-12 02:21:25,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:25,335][root][INFO] - Training Epoch: 1/10, step 489/574 completed (loss: 2.1121184825897217, acc: 0.4000000059604645)
[2024-12-12 02:21:25,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:25,672][root][INFO] - Training Epoch: 1/10, step 490/574 completed (loss: 1.5473811626434326, acc: 0.6000000238418579)
[2024-12-12 02:21:25,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:26,010][root][INFO] - Training Epoch: 1/10, step 491/574 completed (loss: 1.7392158508300781, acc: 0.5862069129943848)
[2024-12-12 02:21:26,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:26,321][root][INFO] - Training Epoch: 1/10, step 492/574 completed (loss: 2.1339640617370605, acc: 0.4117647111415863)
[2024-12-12 02:21:26,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:26,700][root][INFO] - Training Epoch: 1/10, step 493/574 completed (loss: 1.9283325672149658, acc: 0.48275861144065857)
[2024-12-12 02:21:26,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:27,048][root][INFO] - Training Epoch: 1/10, step 494/574 completed (loss: 0.9453500509262085, acc: 0.7368420958518982)
[2024-12-12 02:21:27,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:27,360][root][INFO] - Training Epoch: 1/10, step 495/574 completed (loss: 3.5654518604278564, acc: 0.21052631735801697)
[2024-12-12 02:21:27,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:27,799][root][INFO] - Training Epoch: 1/10, step 496/574 completed (loss: 2.235119104385376, acc: 0.4107142984867096)
[2024-12-12 02:21:27,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:28,245][root][INFO] - Training Epoch: 1/10, step 497/574 completed (loss: 2.1323330402374268, acc: 0.40449437499046326)
[2024-12-12 02:21:28,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:28,665][root][INFO] - Training Epoch: 1/10, step 498/574 completed (loss: 2.4370522499084473, acc: 0.3820224702358246)
[2024-12-12 02:21:28,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:29,065][root][INFO] - Training Epoch: 1/10, step 499/574 completed (loss: 2.592026710510254, acc: 0.3404255211353302)
[2024-12-12 02:21:29,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:29,481][root][INFO] - Training Epoch: 1/10, step 500/574 completed (loss: 2.5956435203552246, acc: 0.3586956560611725)
[2024-12-12 02:21:29,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:29,820][root][INFO] - Training Epoch: 1/10, step 501/574 completed (loss: 0.9247189164161682, acc: 0.8399999737739563)
[2024-12-12 02:21:29,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:30,213][root][INFO] - Training Epoch: 1/10, step 502/574 completed (loss: 1.3248088359832764, acc: 0.7692307829856873)
[2024-12-12 02:21:30,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:30,605][root][INFO] - Training Epoch: 1/10, step 503/574 completed (loss: 1.1138489246368408, acc: 0.7037037014961243)
[2024-12-12 02:21:30,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:30,962][root][INFO] - Training Epoch: 1/10, step 504/574 completed (loss: 2.035705804824829, acc: 0.48148149251937866)
[2024-12-12 02:21:31,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:31,310][root][INFO] - Training Epoch: 1/10, step 505/574 completed (loss: 1.8729053735733032, acc: 0.5094339847564697)
[2024-12-12 02:21:31,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:31,676][root][INFO] - Training Epoch: 1/10, step 506/574 completed (loss: 1.351336121559143, acc: 0.6206896305084229)
[2024-12-12 02:21:31,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:32,325][root][INFO] - Training Epoch: 1/10, step 507/574 completed (loss: 2.334378957748413, acc: 0.3963963985443115)
[2024-12-12 02:21:32,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:32,796][root][INFO] - Training Epoch: 1/10, step 508/574 completed (loss: 2.301809787750244, acc: 0.35211268067359924)
[2024-12-12 02:21:32,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:33,096][root][INFO] - Training Epoch: 1/10, step 509/574 completed (loss: 0.4421662390232086, acc: 0.949999988079071)
[2024-12-12 02:21:33,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:33,363][root][INFO] - Training Epoch: 1/10, step 510/574 completed (loss: 0.9260782599449158, acc: 0.7333333492279053)
[2024-12-12 02:21:33,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:33,671][root][INFO] - Training Epoch: 1/10, step 511/574 completed (loss: 1.3378570079803467, acc: 0.692307710647583)
[2024-12-12 02:21:35,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:36,595][root][INFO] - Training Epoch: 1/10, step 512/574 completed (loss: 2.2980051040649414, acc: 0.41428571939468384)
[2024-12-12 02:21:36,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:37,374][root][INFO] - Training Epoch: 1/10, step 513/574 completed (loss: 2.3318355083465576, acc: 0.4523809552192688)
[2024-12-12 02:21:37,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:37,712][root][INFO] - Training Epoch: 1/10, step 514/574 completed (loss: 1.8368562459945679, acc: 0.5357142686843872)
[2024-12-12 02:21:37,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:38,073][root][INFO] - Training Epoch: 1/10, step 515/574 completed (loss: 1.8913557529449463, acc: 0.5)
[2024-12-12 02:21:38,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:38,796][root][INFO] - Training Epoch: 1/10, step 516/574 completed (loss: 2.1266939640045166, acc: 0.5)
[2024-12-12 02:21:38,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:39,179][root][INFO] - Training Epoch: 1/10, step 517/574 completed (loss: 0.6263493299484253, acc: 0.807692289352417)
[2024-12-12 02:21:39,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:39,534][root][INFO] - Training Epoch: 1/10, step 518/574 completed (loss: 2.2512221336364746, acc: 0.4193548262119293)
[2024-12-12 02:21:39,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:39,848][root][INFO] - Training Epoch: 1/10, step 519/574 completed (loss: 2.655463457107544, acc: 0.44999998807907104)
[2024-12-12 02:21:39,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:40,186][root][INFO] - Training Epoch: 1/10, step 520/574 completed (loss: 2.329352855682373, acc: 0.37037035822868347)
[2024-12-12 02:21:40,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:41,250][root][INFO] - Training Epoch: 1/10, step 521/574 completed (loss: 2.2896223068237305, acc: 0.39830508828163147)
[2024-12-12 02:21:41,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:41,682][root][INFO] - Training Epoch: 1/10, step 522/574 completed (loss: 2.362298011779785, acc: 0.38805970549583435)
[2024-12-12 02:21:41,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:42,078][root][INFO] - Training Epoch: 1/10, step 523/574 completed (loss: 2.3160769939422607, acc: 0.36496350169181824)
[2024-12-12 02:21:42,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:42,666][root][INFO] - Training Epoch: 1/10, step 524/574 completed (loss: 2.126227378845215, acc: 0.45500001311302185)
[2024-12-12 02:21:42,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:43,053][root][INFO] - Training Epoch: 1/10, step 525/574 completed (loss: 2.3250067234039307, acc: 0.29629629850387573)
[2024-12-12 02:21:43,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:43,414][root][INFO] - Training Epoch: 1/10, step 526/574 completed (loss: 1.9981096982955933, acc: 0.48076921701431274)
[2024-12-12 02:21:43,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:43,724][root][INFO] - Training Epoch: 1/10, step 527/574 completed (loss: 2.3161916732788086, acc: 0.2857142984867096)
[2024-12-12 02:21:43,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:44,098][root][INFO] - Training Epoch: 1/10, step 528/574 completed (loss: 3.0233170986175537, acc: 0.1803278625011444)
[2024-12-12 02:21:44,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:44,433][root][INFO] - Training Epoch: 1/10, step 529/574 completed (loss: 2.0410268306732178, acc: 0.47457626461982727)
[2024-12-12 02:21:44,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:44,790][root][INFO] - Training Epoch: 1/10, step 530/574 completed (loss: 2.577552080154419, acc: 0.302325576543808)
[2024-12-12 02:21:44,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:45,179][root][INFO] - Training Epoch: 1/10, step 531/574 completed (loss: 2.4097089767456055, acc: 0.4318181872367859)
[2024-12-12 02:21:45,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:45,535][root][INFO] - Training Epoch: 1/10, step 532/574 completed (loss: 2.510580062866211, acc: 0.3396226465702057)
[2024-12-12 02:21:45,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:45,869][root][INFO] - Training Epoch: 1/10, step 533/574 completed (loss: 2.329442262649536, acc: 0.47727271914482117)
[2024-12-12 02:21:45,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:46,168][root][INFO] - Training Epoch: 1/10, step 534/574 completed (loss: 1.8340915441513062, acc: 0.6000000238418579)
[2024-12-12 02:21:46,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:46,491][root][INFO] - Training Epoch: 1/10, step 535/574 completed (loss: 2.0097126960754395, acc: 0.44999998807907104)
[2024-12-12 02:21:46,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:46,802][root][INFO] - Training Epoch: 1/10, step 536/574 completed (loss: 1.4649841785430908, acc: 0.5)
[2024-12-12 02:21:46,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:47,208][root][INFO] - Training Epoch: 1/10, step 537/574 completed (loss: 2.0756733417510986, acc: 0.4923076927661896)
[2024-12-12 02:21:47,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:47,543][root][INFO] - Training Epoch: 1/10, step 538/574 completed (loss: 2.0060439109802246, acc: 0.515625)
[2024-12-12 02:21:47,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:47,936][root][INFO] - Training Epoch: 1/10, step 539/574 completed (loss: 1.331026315689087, acc: 0.75)
[2024-12-12 02:21:48,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:48,361][root][INFO] - Training Epoch: 1/10, step 540/574 completed (loss: 2.241968870162964, acc: 0.3333333432674408)
[2024-12-12 02:21:48,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:48,701][root][INFO] - Training Epoch: 1/10, step 541/574 completed (loss: 0.7848401069641113, acc: 0.75)
[2024-12-12 02:21:48,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:48,991][root][INFO] - Training Epoch: 1/10, step 542/574 completed (loss: 1.2812037467956543, acc: 0.5806451439857483)
[2024-12-12 02:21:49,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:49,359][root][INFO] - Training Epoch: 1/10, step 543/574 completed (loss: 0.781517505645752, acc: 0.782608687877655)
[2024-12-12 02:21:49,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:49,693][root][INFO] - Training Epoch: 1/10, step 544/574 completed (loss: 2.288390636444092, acc: 0.4000000059604645)
[2024-12-12 02:21:49,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:50,022][root][INFO] - Training Epoch: 1/10, step 545/574 completed (loss: 1.9375327825546265, acc: 0.4146341383457184)
[2024-12-12 02:21:50,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:50,383][root][INFO] - Training Epoch: 1/10, step 546/574 completed (loss: 1.2857341766357422, acc: 0.6571428775787354)
[2024-12-12 02:21:50,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:50,773][root][INFO] - Training Epoch: 1/10, step 547/574 completed (loss: 1.8562281131744385, acc: 0.5789473652839661)
[2024-12-12 02:21:50,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:51,168][root][INFO] - Training Epoch: 1/10, step 548/574 completed (loss: 1.9437308311462402, acc: 0.4838709533214569)
[2024-12-12 02:21:51,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:51,533][root][INFO] - Training Epoch: 1/10, step 549/574 completed (loss: 0.77850741147995, acc: 0.800000011920929)
[2024-12-12 02:21:51,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:51,895][root][INFO] - Training Epoch: 1/10, step 550/574 completed (loss: 1.5547904968261719, acc: 0.5151515007019043)
[2024-12-12 02:21:52,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:52,301][root][INFO] - Training Epoch: 1/10, step 551/574 completed (loss: 1.422391653060913, acc: 0.574999988079071)
[2024-12-12 02:21:52,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:52,680][root][INFO] - Training Epoch: 1/10, step 552/574 completed (loss: 1.791227102279663, acc: 0.5142857432365417)
[2024-12-12 02:21:52,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:53,095][root][INFO] - Training Epoch: 1/10, step 553/574 completed (loss: 2.6210973262786865, acc: 0.24817518889904022)
[2024-12-12 02:21:53,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:53,475][root][INFO] - Training Epoch: 1/10, step 554/574 completed (loss: 2.220527410507202, acc: 0.4137931168079376)
[2024-12-12 02:21:53,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:53,849][root][INFO] - Training Epoch: 1/10, step 555/574 completed (loss: 2.9474496841430664, acc: 0.20000000298023224)
[2024-12-12 02:21:53,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:54,219][root][INFO] - Training Epoch: 1/10, step 556/574 completed (loss: 2.8703582286834717, acc: 0.19205297529697418)
[2024-12-12 02:21:54,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:54,564][root][INFO] - Training Epoch: 1/10, step 557/574 completed (loss: 2.3210904598236084, acc: 0.4017094075679779)
[2024-12-12 02:21:54,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:54,956][root][INFO] - Training Epoch: 1/10, step 558/574 completed (loss: 0.9707233309745789, acc: 0.8399999737739563)
[2024-12-12 02:21:55,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:55,321][root][INFO] - Training Epoch: 1/10, step 559/574 completed (loss: 1.7661850452423096, acc: 0.5384615659713745)
[2024-12-12 02:21:55,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:55,653][root][INFO] - Training Epoch: 1/10, step 560/574 completed (loss: 1.242703914642334, acc: 0.6153846383094788)
[2024-12-12 02:21:55,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:56,022][root][INFO] - Training Epoch: 1/10, step 561/574 completed (loss: 1.8717559576034546, acc: 0.4871794879436493)
[2024-12-12 02:21:56,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:56,389][root][INFO] - Training Epoch: 1/10, step 562/574 completed (loss: 1.952091097831726, acc: 0.47777777910232544)
[2024-12-12 02:21:56,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:56,795][root][INFO] - Training Epoch: 1/10, step 563/574 completed (loss: 2.1897342205047607, acc: 0.37662336230278015)
[2024-12-12 02:21:56,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:57,174][root][INFO] - Training Epoch: 1/10, step 564/574 completed (loss: 2.1446468830108643, acc: 0.3958333432674408)
[2024-12-12 02:21:57,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:57,576][root][INFO] - Training Epoch: 1/10, step 565/574 completed (loss: 2.3221564292907715, acc: 0.4137931168079376)
[2024-12-12 02:21:57,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:57,964][root][INFO] - Training Epoch: 1/10, step 566/574 completed (loss: 2.080209970474243, acc: 0.4166666567325592)
[2024-12-12 02:21:58,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:58,271][root][INFO] - Training Epoch: 1/10, step 567/574 completed (loss: 1.8446482419967651, acc: 0.4736842215061188)
[2024-12-12 02:21:58,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:58,627][root][INFO] - Training Epoch: 1/10, step 568/574 completed (loss: 1.9383509159088135, acc: 0.48148149251937866)
[2024-12-12 02:21:58,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:59,075][root][INFO] - Training Epoch: 1/10, step 569/574 completed (loss: 2.3106536865234375, acc: 0.3636363744735718)
[2024-12-12 02:21:59,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:59,468][root][INFO] - Training Epoch: 1/10, step 570/574 completed (loss: 1.7384538650512695, acc: 0.5483871102333069)
[2024-12-12 02:21:59,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:21:59,835][root][INFO] - Training Epoch: 1/10, step 571/574 completed (loss: 2.2967331409454346, acc: 0.38461539149284363)
[2024-12-12 02:22:00,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:00,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:01,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:01,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:02,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:02,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:02,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:03,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:03,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:03,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:04,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:04,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:05,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:05,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:05,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:06,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:06,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:06,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:07,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:07,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:08,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:08,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:08,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:09,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:09,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:09,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:10,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:10,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:10,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:11,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:11,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:11,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:12,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:12,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:12,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:13,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:13,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:13,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:14,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:14,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:14,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:15,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:15,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:15,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:16,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:16,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:17,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:17,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:17,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:17,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:18,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:18,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:18,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:19,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:19,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:19,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:20,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:20,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:20,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:21,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:21,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:21,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:22,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:22,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:23,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:23,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:23,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:24,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:24,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:25,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:25,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:25,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:26,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:26,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:26,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:26,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:27,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:27,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:28,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:28,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:28,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:29,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:29,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:29,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:30,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:30,791][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(7.5319, device='cuda:0') eval_epoch_loss=tensor(2.0191, device='cuda:0') eval_epoch_acc=tensor(0.4569, device='cuda:0')
[2024-12-12 02:22:30,792][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:22:30,793][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:22:31,013][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_1_step_572_loss_2.0191421508789062/model.pt
[2024-12-12 02:22:31,016][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:22:31,016][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 2.0191421508789062
[2024-12-12 02:22:31,017][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.45693036913871765
[2024-12-12 02:22:31,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:31,302][root][INFO] - Training Epoch: 1/10, step 572/574 completed (loss: 2.529092311859131, acc: 0.34183672070503235)
[2024-12-12 02:22:31,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:31,641][root][INFO] - Training Epoch: 1/10, step 573/574 completed (loss: 2.4442691802978516, acc: 0.3333333432674408)
[2024-12-12 02:22:32,147][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=18.0623, train_epoch_loss=2.8938, epoch time 373.622375998646s
[2024-12-12 02:22:32,148][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-12-12 02:22:32,148][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2024-12-12 02:22:32,148][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-12-12 02:22:32,148][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2024-12-12 02:22:32,148][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:22:32,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:32,936][root][INFO] - Training Epoch: 2/10, step 0/574 completed (loss: 1.6954470872879028, acc: 0.5185185074806213)
[2024-12-12 02:22:33,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:33,306][root][INFO] - Training Epoch: 2/10, step 1/574 completed (loss: 2.2526164054870605, acc: 0.4399999976158142)
[2024-12-12 02:22:33,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:33,677][root][INFO] - Training Epoch: 2/10, step 2/574 completed (loss: 2.8582472801208496, acc: 0.3513513505458832)
[2024-12-12 02:22:33,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:34,067][root][INFO] - Training Epoch: 2/10, step 3/574 completed (loss: 2.522045612335205, acc: 0.31578946113586426)
[2024-12-12 02:22:34,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:34,422][root][INFO] - Training Epoch: 2/10, step 4/574 completed (loss: 2.0773417949676514, acc: 0.4054054021835327)
[2024-12-12 02:22:34,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:34,784][root][INFO] - Training Epoch: 2/10, step 5/574 completed (loss: 2.097940683364868, acc: 0.3571428656578064)
[2024-12-12 02:22:34,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:35,080][root][INFO] - Training Epoch: 2/10, step 6/574 completed (loss: 2.461348056793213, acc: 0.3469387888908386)
[2024-12-12 02:22:35,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:35,372][root][INFO] - Training Epoch: 2/10, step 7/574 completed (loss: 1.9865256547927856, acc: 0.46666666865348816)
[2024-12-12 02:22:35,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:35,691][root][INFO] - Training Epoch: 2/10, step 8/574 completed (loss: 0.4392218291759491, acc: 0.9090909361839294)
[2024-12-12 02:22:35,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:36,076][root][INFO] - Training Epoch: 2/10, step 9/574 completed (loss: 0.7721354365348816, acc: 0.7692307829856873)
[2024-12-12 02:22:36,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:36,420][root][INFO] - Training Epoch: 2/10, step 10/574 completed (loss: 1.3116486072540283, acc: 0.5555555820465088)
[2024-12-12 02:22:36,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:36,735][root][INFO] - Training Epoch: 2/10, step 11/574 completed (loss: 1.9739160537719727, acc: 0.41025641560554504)
[2024-12-12 02:22:36,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:37,078][root][INFO] - Training Epoch: 2/10, step 12/574 completed (loss: 1.9483811855316162, acc: 0.4545454680919647)
[2024-12-12 02:22:37,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:37,458][root][INFO] - Training Epoch: 2/10, step 13/574 completed (loss: 1.9842318296432495, acc: 0.3478260934352875)
[2024-12-12 02:22:37,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:37,840][root][INFO] - Training Epoch: 2/10, step 14/574 completed (loss: 2.336780548095703, acc: 0.4117647111415863)
[2024-12-12 02:22:37,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:38,194][root][INFO] - Training Epoch: 2/10, step 15/574 completed (loss: 1.830812931060791, acc: 0.5102040767669678)
[2024-12-12 02:22:38,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:38,561][root][INFO] - Training Epoch: 2/10, step 16/574 completed (loss: 0.8849205374717712, acc: 0.7368420958518982)
[2024-12-12 02:22:38,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:38,914][root][INFO] - Training Epoch: 2/10, step 17/574 completed (loss: 2.124234199523926, acc: 0.3333333432674408)
[2024-12-12 02:22:39,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:39,253][root][INFO] - Training Epoch: 2/10, step 18/574 completed (loss: 2.476557731628418, acc: 0.3888888955116272)
[2024-12-12 02:22:39,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:39,628][root][INFO] - Training Epoch: 2/10, step 19/574 completed (loss: 1.9121345281600952, acc: 0.4736842215061188)
[2024-12-12 02:22:39,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:39,966][root][INFO] - Training Epoch: 2/10, step 20/574 completed (loss: 1.9303405284881592, acc: 0.5384615659713745)
[2024-12-12 02:22:40,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:40,269][root][INFO] - Training Epoch: 2/10, step 21/574 completed (loss: 2.1184751987457275, acc: 0.5517241358757019)
[2024-12-12 02:22:40,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:40,641][root][INFO] - Training Epoch: 2/10, step 22/574 completed (loss: 1.925211787223816, acc: 0.47999998927116394)
[2024-12-12 02:22:40,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:41,035][root][INFO] - Training Epoch: 2/10, step 23/574 completed (loss: 1.0802401304244995, acc: 0.7142857313156128)
[2024-12-12 02:22:41,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:41,417][root][INFO] - Training Epoch: 2/10, step 24/574 completed (loss: 1.9362068176269531, acc: 0.375)
[2024-12-12 02:22:41,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:41,831][root][INFO] - Training Epoch: 2/10, step 25/574 completed (loss: 2.740952968597412, acc: 0.24528302252292633)
[2024-12-12 02:22:41,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:42,180][root][INFO] - Training Epoch: 2/10, step 26/574 completed (loss: 2.4516115188598633, acc: 0.4109589159488678)
[2024-12-12 02:22:42,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:43,453][root][INFO] - Training Epoch: 2/10, step 27/574 completed (loss: 2.46488356590271, acc: 0.3320158123970032)
[2024-12-12 02:22:43,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:43,812][root][INFO] - Training Epoch: 2/10, step 28/574 completed (loss: 2.2995033264160156, acc: 0.41860464215278625)
[2024-12-12 02:22:43,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:44,192][root][INFO] - Training Epoch: 2/10, step 29/574 completed (loss: 2.3423826694488525, acc: 0.39759036898612976)
[2024-12-12 02:22:44,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:44,586][root][INFO] - Training Epoch: 2/10, step 30/574 completed (loss: 2.3273720741271973, acc: 0.3580246865749359)
[2024-12-12 02:22:44,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:44,906][root][INFO] - Training Epoch: 2/10, step 31/574 completed (loss: 2.4134268760681152, acc: 0.3928571343421936)
[2024-12-12 02:22:44,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:45,250][root][INFO] - Training Epoch: 2/10, step 32/574 completed (loss: 1.6234492063522339, acc: 0.5185185074806213)
[2024-12-12 02:22:45,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:45,623][root][INFO] - Training Epoch: 2/10, step 33/574 completed (loss: 1.9110853672027588, acc: 0.47826087474823)
[2024-12-12 02:22:45,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:46,027][root][INFO] - Training Epoch: 2/10, step 34/574 completed (loss: 2.2505953311920166, acc: 0.42016807198524475)
[2024-12-12 02:22:46,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:46,390][root][INFO] - Training Epoch: 2/10, step 35/574 completed (loss: 1.9722846746444702, acc: 0.4754098355770111)
[2024-12-12 02:22:46,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:46,796][root][INFO] - Training Epoch: 2/10, step 36/574 completed (loss: 2.1270787715911865, acc: 0.380952388048172)
[2024-12-12 02:22:46,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:47,135][root][INFO] - Training Epoch: 2/10, step 37/574 completed (loss: 2.445434093475342, acc: 0.35593220591545105)
[2024-12-12 02:22:47,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:47,529][root][INFO] - Training Epoch: 2/10, step 38/574 completed (loss: 1.718648076057434, acc: 0.5287356376647949)
[2024-12-12 02:22:47,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:47,919][root][INFO] - Training Epoch: 2/10, step 39/574 completed (loss: 1.2397258281707764, acc: 0.6666666865348816)
[2024-12-12 02:22:48,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:48,289][root][INFO] - Training Epoch: 2/10, step 40/574 completed (loss: 2.3661577701568604, acc: 0.3076923191547394)
[2024-12-12 02:22:48,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:48,672][root][INFO] - Training Epoch: 2/10, step 41/574 completed (loss: 2.8503971099853516, acc: 0.2432432472705841)
[2024-12-12 02:22:48,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:49,026][root][INFO] - Training Epoch: 2/10, step 42/574 completed (loss: 2.2326669692993164, acc: 0.3692307770252228)
[2024-12-12 02:22:49,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:49,430][root][INFO] - Training Epoch: 2/10, step 43/574 completed (loss: 2.490079879760742, acc: 0.34343433380126953)
[2024-12-12 02:22:49,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:49,848][root][INFO] - Training Epoch: 2/10, step 44/574 completed (loss: 2.0391716957092285, acc: 0.4329896867275238)
[2024-12-12 02:22:49,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:50,251][root][INFO] - Training Epoch: 2/10, step 45/574 completed (loss: 2.305339813232422, acc: 0.40441176295280457)
[2024-12-12 02:22:50,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:50,605][root][INFO] - Training Epoch: 2/10, step 46/574 completed (loss: 0.9586890339851379, acc: 0.807692289352417)
[2024-12-12 02:22:50,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:50,966][root][INFO] - Training Epoch: 2/10, step 47/574 completed (loss: 0.856630265712738, acc: 0.7777777910232544)
[2024-12-12 02:22:51,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:51,298][root][INFO] - Training Epoch: 2/10, step 48/574 completed (loss: 1.5126687288284302, acc: 0.5714285969734192)
[2024-12-12 02:22:51,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:51,645][root][INFO] - Training Epoch: 2/10, step 49/574 completed (loss: 1.2801601886749268, acc: 0.5833333134651184)
[2024-12-12 02:22:51,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:52,004][root][INFO] - Training Epoch: 2/10, step 50/574 completed (loss: 1.6338402032852173, acc: 0.5438596606254578)
[2024-12-12 02:22:52,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:52,330][root][INFO] - Training Epoch: 2/10, step 51/574 completed (loss: 1.771833062171936, acc: 0.5555555820465088)
[2024-12-12 02:22:52,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:52,627][root][INFO] - Training Epoch: 2/10, step 52/574 completed (loss: 2.287945032119751, acc: 0.4084506928920746)
[2024-12-12 02:22:52,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:53,071][root][INFO] - Training Epoch: 2/10, step 53/574 completed (loss: 2.611618995666504, acc: 0.4000000059604645)
[2024-12-12 02:22:53,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:53,367][root][INFO] - Training Epoch: 2/10, step 54/574 completed (loss: 1.3534504175186157, acc: 0.7027027010917664)
[2024-12-12 02:22:53,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:53,712][root][INFO] - Training Epoch: 2/10, step 55/574 completed (loss: 0.7846751809120178, acc: 0.7307692170143127)
[2024-12-12 02:22:55,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:56,795][root][INFO] - Training Epoch: 2/10, step 56/574 completed (loss: 2.0906178951263428, acc: 0.4744027256965637)
[2024-12-12 02:22:57,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:58,154][root][INFO] - Training Epoch: 2/10, step 57/574 completed (loss: 2.695222854614258, acc: 0.37472766637802124)
[2024-12-12 02:22:58,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:58,778][root][INFO] - Training Epoch: 2/10, step 58/574 completed (loss: 2.1694483757019043, acc: 0.47727271914482117)
[2024-12-12 02:22:58,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:59,347][root][INFO] - Training Epoch: 2/10, step 59/574 completed (loss: 2.4192867279052734, acc: 0.36764705181121826)
[2024-12-12 02:22:59,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:22:59,912][root][INFO] - Training Epoch: 2/10, step 60/574 completed (loss: 2.475330114364624, acc: 0.36231884360313416)
[2024-12-12 02:23:00,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:00,317][root][INFO] - Training Epoch: 2/10, step 61/574 completed (loss: 1.8942979574203491, acc: 0.5625)
[2024-12-12 02:23:00,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:00,672][root][INFO] - Training Epoch: 2/10, step 62/574 completed (loss: 1.7147539854049683, acc: 0.529411792755127)
[2024-12-12 02:23:00,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:01,054][root][INFO] - Training Epoch: 2/10, step 63/574 completed (loss: 1.9790763854980469, acc: 0.5)
[2024-12-12 02:23:01,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:01,423][root][INFO] - Training Epoch: 2/10, step 64/574 completed (loss: 1.8778268098831177, acc: 0.546875)
[2024-12-12 02:23:01,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:01,797][root][INFO] - Training Epoch: 2/10, step 65/574 completed (loss: 0.9853144288063049, acc: 0.7586206793785095)
[2024-12-12 02:23:01,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:02,146][root][INFO] - Training Epoch: 2/10, step 66/574 completed (loss: 2.4299862384796143, acc: 0.3928571343421936)
[2024-12-12 02:23:02,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:02,465][root][INFO] - Training Epoch: 2/10, step 67/574 completed (loss: 2.2959916591644287, acc: 0.36666667461395264)
[2024-12-12 02:23:02,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:02,777][root][INFO] - Training Epoch: 2/10, step 68/574 completed (loss: 1.014223575592041, acc: 0.6800000071525574)
[2024-12-12 02:23:02,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:03,076][root][INFO] - Training Epoch: 2/10, step 69/574 completed (loss: 1.470420002937317, acc: 0.6388888955116272)
[2024-12-12 02:23:03,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:03,434][root][INFO] - Training Epoch: 2/10, step 70/574 completed (loss: 1.6385942697525024, acc: 0.6060606241226196)
[2024-12-12 02:23:03,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:03,813][root][INFO] - Training Epoch: 2/10, step 71/574 completed (loss: 2.2003443241119385, acc: 0.44117647409439087)
[2024-12-12 02:23:03,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:04,159][root][INFO] - Training Epoch: 2/10, step 72/574 completed (loss: 2.154183864593506, acc: 0.3968254029750824)
[2024-12-12 02:23:04,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:04,520][root][INFO] - Training Epoch: 2/10, step 73/574 completed (loss: 2.396083116531372, acc: 0.34871795773506165)
[2024-12-12 02:23:04,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:04,882][root][INFO] - Training Epoch: 2/10, step 74/574 completed (loss: 2.1322009563446045, acc: 0.37755101919174194)
[2024-12-12 02:23:04,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:05,202][root][INFO] - Training Epoch: 2/10, step 75/574 completed (loss: 2.588472843170166, acc: 0.26119402050971985)
[2024-12-12 02:23:05,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:05,596][root][INFO] - Training Epoch: 2/10, step 76/574 completed (loss: 2.3347368240356445, acc: 0.36496350169181824)
[2024-12-12 02:23:05,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:05,938][root][INFO] - Training Epoch: 2/10, step 77/574 completed (loss: 0.6094468235969543, acc: 0.8095238208770752)
[2024-12-12 02:23:06,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:06,261][root][INFO] - Training Epoch: 2/10, step 78/574 completed (loss: 0.6906339526176453, acc: 0.9166666865348816)
[2024-12-12 02:23:06,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:06,629][root][INFO] - Training Epoch: 2/10, step 79/574 completed (loss: 1.2844842672348022, acc: 0.6666666865348816)
[2024-12-12 02:23:06,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:06,992][root][INFO] - Training Epoch: 2/10, step 80/574 completed (loss: 1.284708857536316, acc: 0.692307710647583)
[2024-12-12 02:23:07,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:07,399][root][INFO] - Training Epoch: 2/10, step 81/574 completed (loss: 2.0423684120178223, acc: 0.4423076808452606)
[2024-12-12 02:23:07,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:07,747][root][INFO] - Training Epoch: 2/10, step 82/574 completed (loss: 2.4224941730499268, acc: 0.38461539149284363)
[2024-12-12 02:23:07,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:08,088][root][INFO] - Training Epoch: 2/10, step 83/574 completed (loss: 1.7376809120178223, acc: 0.5)
[2024-12-12 02:23:08,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:08,429][root][INFO] - Training Epoch: 2/10, step 84/574 completed (loss: 2.2751264572143555, acc: 0.43478259444236755)
[2024-12-12 02:23:08,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:08,795][root][INFO] - Training Epoch: 2/10, step 85/574 completed (loss: 1.770833134651184, acc: 0.4399999976158142)
[2024-12-12 02:23:08,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:09,143][root][INFO] - Training Epoch: 2/10, step 86/574 completed (loss: 2.33654522895813, acc: 0.47826087474823)
[2024-12-12 02:23:09,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:09,600][root][INFO] - Training Epoch: 2/10, step 87/574 completed (loss: 2.4674179553985596, acc: 0.3400000035762787)
[2024-12-12 02:23:09,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:10,019][root][INFO] - Training Epoch: 2/10, step 88/574 completed (loss: 2.03554368019104, acc: 0.5145630836486816)
[2024-12-12 02:23:10,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:11,142][root][INFO] - Training Epoch: 2/10, step 89/574 completed (loss: 1.9962520599365234, acc: 0.5145630836486816)
[2024-12-12 02:23:11,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:11,964][root][INFO] - Training Epoch: 2/10, step 90/574 completed (loss: 2.148477554321289, acc: 0.39247313141822815)
[2024-12-12 02:23:12,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:12,771][root][INFO] - Training Epoch: 2/10, step 91/574 completed (loss: 1.9536871910095215, acc: 0.5215517282485962)
[2024-12-12 02:23:12,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:13,515][root][INFO] - Training Epoch: 2/10, step 92/574 completed (loss: 1.6989247798919678, acc: 0.49473685026168823)
[2024-12-12 02:23:13,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:14,508][root][INFO] - Training Epoch: 2/10, step 93/574 completed (loss: 2.4536235332489014, acc: 0.31683167815208435)
[2024-12-12 02:23:14,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:14,873][root][INFO] - Training Epoch: 2/10, step 94/574 completed (loss: 2.4347665309906006, acc: 0.33870968222618103)
[2024-12-12 02:23:15,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:15,276][root][INFO] - Training Epoch: 2/10, step 95/574 completed (loss: 2.4947731494903564, acc: 0.3333333432674408)
[2024-12-12 02:23:15,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:15,637][root][INFO] - Training Epoch: 2/10, step 96/574 completed (loss: 2.6212611198425293, acc: 0.24369747936725616)
[2024-12-12 02:23:15,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:16,046][root][INFO] - Training Epoch: 2/10, step 97/574 completed (loss: 2.735858917236328, acc: 0.21153846383094788)
[2024-12-12 02:23:16,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:16,462][root][INFO] - Training Epoch: 2/10, step 98/574 completed (loss: 2.5504515171051025, acc: 0.33576643466949463)
[2024-12-12 02:23:16,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:16,847][root][INFO] - Training Epoch: 2/10, step 99/574 completed (loss: 2.665691375732422, acc: 0.31343284249305725)
[2024-12-12 02:23:16,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:17,219][root][INFO] - Training Epoch: 2/10, step 100/574 completed (loss: 1.6007654666900635, acc: 0.699999988079071)
[2024-12-12 02:23:17,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:17,547][root][INFO] - Training Epoch: 2/10, step 101/574 completed (loss: 0.9898452162742615, acc: 0.8636363744735718)
[2024-12-12 02:23:17,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:17,897][root][INFO] - Training Epoch: 2/10, step 102/574 completed (loss: 1.1748650074005127, acc: 0.5652173757553101)
[2024-12-12 02:23:17,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:18,232][root][INFO] - Training Epoch: 2/10, step 103/574 completed (loss: 1.8324756622314453, acc: 0.40909090638160706)
[2024-12-12 02:23:18,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:18,609][root][INFO] - Training Epoch: 2/10, step 104/574 completed (loss: 2.1205339431762695, acc: 0.3965517282485962)
[2024-12-12 02:23:18,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:18,979][root][INFO] - Training Epoch: 2/10, step 105/574 completed (loss: 1.9017813205718994, acc: 0.5116279125213623)
[2024-12-12 02:23:19,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:19,353][root][INFO] - Training Epoch: 2/10, step 106/574 completed (loss: 1.4104604721069336, acc: 0.6800000071525574)
[2024-12-12 02:23:19,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:19,713][root][INFO] - Training Epoch: 2/10, step 107/574 completed (loss: 0.697830080986023, acc: 0.7647058963775635)
[2024-12-12 02:23:19,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:20,091][root][INFO] - Training Epoch: 2/10, step 108/574 completed (loss: 0.42384010553359985, acc: 0.8461538553237915)
[2024-12-12 02:23:20,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:20,471][root][INFO] - Training Epoch: 2/10, step 109/574 completed (loss: 1.8307210206985474, acc: 0.5)
[2024-12-12 02:23:20,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:20,871][root][INFO] - Training Epoch: 2/10, step 110/574 completed (loss: 2.0267560482025146, acc: 0.5076923370361328)
[2024-12-12 02:23:21,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:21,280][root][INFO] - Training Epoch: 2/10, step 111/574 completed (loss: 2.024700880050659, acc: 0.4736842215061188)
[2024-12-12 02:23:21,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:21,661][root][INFO] - Training Epoch: 2/10, step 112/574 completed (loss: 1.931445837020874, acc: 0.4385964870452881)
[2024-12-12 02:23:21,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:22,036][root][INFO] - Training Epoch: 2/10, step 113/574 completed (loss: 2.167109966278076, acc: 0.41025641560554504)
[2024-12-12 02:23:22,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:22,428][root][INFO] - Training Epoch: 2/10, step 114/574 completed (loss: 1.6991568803787231, acc: 0.6530612111091614)
[2024-12-12 02:23:22,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:22,791][root][INFO] - Training Epoch: 2/10, step 115/574 completed (loss: 0.4802248775959015, acc: 0.8636363744735718)
[2024-12-12 02:23:22,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:23,190][root][INFO] - Training Epoch: 2/10, step 116/574 completed (loss: 2.2443809509277344, acc: 0.4444444477558136)
[2024-12-12 02:23:23,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:23,570][root][INFO] - Training Epoch: 2/10, step 117/574 completed (loss: 2.17497181892395, acc: 0.4878048896789551)
[2024-12-12 02:23:23,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:23,963][root][INFO] - Training Epoch: 2/10, step 118/574 completed (loss: 1.7924965620040894, acc: 0.5645161271095276)
[2024-12-12 02:23:24,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:24,854][root][INFO] - Training Epoch: 2/10, step 119/574 completed (loss: 2.1456587314605713, acc: 0.4524714946746826)
[2024-12-12 02:23:24,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:25,233][root][INFO] - Training Epoch: 2/10, step 120/574 completed (loss: 1.8544126749038696, acc: 0.54666668176651)
[2024-12-12 02:23:25,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:25,653][root][INFO] - Training Epoch: 2/10, step 121/574 completed (loss: 1.7299551963806152, acc: 0.5769230723381042)
[2024-12-12 02:23:25,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:26,014][root][INFO] - Training Epoch: 2/10, step 122/574 completed (loss: 0.9792109131813049, acc: 0.8333333134651184)
[2024-12-12 02:23:26,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:26,372][root][INFO] - Training Epoch: 2/10, step 123/574 completed (loss: 1.7394194602966309, acc: 0.4736842215061188)
[2024-12-12 02:23:26,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:26,764][root][INFO] - Training Epoch: 2/10, step 124/574 completed (loss: 2.2844910621643066, acc: 0.4110429584980011)
[2024-12-12 02:23:26,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:27,195][root][INFO] - Training Epoch: 2/10, step 125/574 completed (loss: 1.9224739074707031, acc: 0.5069444179534912)
[2024-12-12 02:23:27,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:27,516][root][INFO] - Training Epoch: 2/10, step 126/574 completed (loss: 2.354497194290161, acc: 0.3166666626930237)
[2024-12-12 02:23:27,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:27,916][root][INFO] - Training Epoch: 2/10, step 127/574 completed (loss: 2.3736109733581543, acc: 0.3095238208770752)
[2024-12-12 02:23:28,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:28,303][root][INFO] - Training Epoch: 2/10, step 128/574 completed (loss: 2.210019826889038, acc: 0.43589743971824646)
[2024-12-12 02:23:28,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:28,714][root][INFO] - Training Epoch: 2/10, step 129/574 completed (loss: 1.984715223312378, acc: 0.4852941036224365)
[2024-12-12 02:23:28,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:29,050][root][INFO] - Training Epoch: 2/10, step 130/574 completed (loss: 1.1648343801498413, acc: 0.6153846383094788)
[2024-12-12 02:23:29,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:29,348][root][INFO] - Training Epoch: 2/10, step 131/574 completed (loss: 0.9193611145019531, acc: 0.782608687877655)
[2024-12-12 02:23:29,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:29,653][root][INFO] - Training Epoch: 2/10, step 132/574 completed (loss: 1.6959112882614136, acc: 0.5625)
[2024-12-12 02:23:29,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:30,002][root][INFO] - Training Epoch: 2/10, step 133/574 completed (loss: 1.9928663969039917, acc: 0.47826087474823)
[2024-12-12 02:23:30,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:30,311][root][INFO] - Training Epoch: 2/10, step 134/574 completed (loss: 1.4366768598556519, acc: 0.6571428775787354)
[2024-12-12 02:23:30,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:30,666][root][INFO] - Training Epoch: 2/10, step 135/574 completed (loss: 1.5225903987884521, acc: 0.5384615659713745)
[2024-12-12 02:23:30,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:31,021][root][INFO] - Training Epoch: 2/10, step 136/574 completed (loss: 2.3027799129486084, acc: 0.3095238208770752)
[2024-12-12 02:23:31,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:31,342][root][INFO] - Training Epoch: 2/10, step 137/574 completed (loss: 1.5520758628845215, acc: 0.5333333611488342)
[2024-12-12 02:23:31,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:31,672][root][INFO] - Training Epoch: 2/10, step 138/574 completed (loss: 1.9138561487197876, acc: 0.52173912525177)
[2024-12-12 02:23:31,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:31,978][root][INFO] - Training Epoch: 2/10, step 139/574 completed (loss: 2.289475202560425, acc: 0.4761904776096344)
[2024-12-12 02:23:32,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:32,320][root][INFO] - Training Epoch: 2/10, step 140/574 completed (loss: 2.2983033657073975, acc: 0.3461538553237915)
[2024-12-12 02:23:33,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:33,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:33,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:34,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:34,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:34,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:35,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:35,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:35,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:36,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:36,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:36,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:37,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:37,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:38,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:38,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:38,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:39,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:39,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:39,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:40,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:40,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:40,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:40,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:41,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:41,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:42,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:42,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:42,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:43,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:43,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:43,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:44,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:44,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:44,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:45,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:45,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:45,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:46,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:46,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:46,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:47,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:47,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:48,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:48,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:48,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:49,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:49,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:49,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:50,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:50,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:50,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:51,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:51,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:51,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:52,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:52,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:52,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:53,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:53,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:53,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:54,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:54,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:55,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:55,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:55,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:56,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:56,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:57,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:57,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:57,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:58,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:58,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:58,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:59,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:59,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:23:59,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:00,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:00,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:00,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:01,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:01,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:01,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:02,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:02,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:03,145][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(8.0515, device='cuda:0') eval_epoch_loss=tensor(2.0859, device='cuda:0') eval_epoch_acc=tensor(0.4432, device='cuda:0')
[2024-12-12 02:24:03,147][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:24:03,147][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:24:03,338][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_2_step_141_loss_2.085859537124634/model.pt
[2024-12-12 02:24:03,342][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:24:03,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:03,725][root][INFO] - Training Epoch: 2/10, step 141/574 completed (loss: 2.9198029041290283, acc: 0.22580644488334656)
[2024-12-12 02:24:03,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:04,109][root][INFO] - Training Epoch: 2/10, step 142/574 completed (loss: 2.4858312606811523, acc: 0.37837839126586914)
[2024-12-12 02:24:04,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:04,643][root][INFO] - Training Epoch: 2/10, step 143/574 completed (loss: 2.24251651763916, acc: 0.37719297409057617)
[2024-12-12 02:24:04,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:04,981][root][INFO] - Training Epoch: 2/10, step 144/574 completed (loss: 1.9723711013793945, acc: 0.44029849767684937)
[2024-12-12 02:24:05,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:05,345][root][INFO] - Training Epoch: 2/10, step 145/574 completed (loss: 2.489936351776123, acc: 0.3265306055545807)
[2024-12-12 02:24:05,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:05,777][root][INFO] - Training Epoch: 2/10, step 146/574 completed (loss: 2.2248716354370117, acc: 0.3404255211353302)
[2024-12-12 02:24:05,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:06,096][root][INFO] - Training Epoch: 2/10, step 147/574 completed (loss: 1.9540257453918457, acc: 0.4285714328289032)
[2024-12-12 02:24:06,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:06,430][root][INFO] - Training Epoch: 2/10, step 148/574 completed (loss: 2.505048990249634, acc: 0.4285714328289032)
[2024-12-12 02:24:06,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:06,799][root][INFO] - Training Epoch: 2/10, step 149/574 completed (loss: 1.609034776687622, acc: 0.52173912525177)
[2024-12-12 02:24:06,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:07,133][root][INFO] - Training Epoch: 2/10, step 150/574 completed (loss: 2.272031545639038, acc: 0.3103448152542114)
[2024-12-12 02:24:07,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:07,461][root][INFO] - Training Epoch: 2/10, step 151/574 completed (loss: 2.264665365219116, acc: 0.45652174949645996)
[2024-12-12 02:24:07,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:07,792][root][INFO] - Training Epoch: 2/10, step 152/574 completed (loss: 2.0822737216949463, acc: 0.4067796468734741)
[2024-12-12 02:24:07,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:08,131][root][INFO] - Training Epoch: 2/10, step 153/574 completed (loss: 2.5381970405578613, acc: 0.38596490025520325)
[2024-12-12 02:24:08,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:08,476][root][INFO] - Training Epoch: 2/10, step 154/574 completed (loss: 2.0537188053131104, acc: 0.47297295928001404)
[2024-12-12 02:24:08,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:08,831][root][INFO] - Training Epoch: 2/10, step 155/574 completed (loss: 1.8807729482650757, acc: 0.5357142686843872)
[2024-12-12 02:24:08,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:09,230][root][INFO] - Training Epoch: 2/10, step 156/574 completed (loss: 1.5284299850463867, acc: 0.5652173757553101)
[2024-12-12 02:24:09,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:09,611][root][INFO] - Training Epoch: 2/10, step 157/574 completed (loss: 2.2658872604370117, acc: 0.42105263471603394)
[2024-12-12 02:24:10,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:11,284][root][INFO] - Training Epoch: 2/10, step 158/574 completed (loss: 1.8926810026168823, acc: 0.5)
[2024-12-12 02:24:11,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:11,601][root][INFO] - Training Epoch: 2/10, step 159/574 completed (loss: 2.159940242767334, acc: 0.46296295523643494)
[2024-12-12 02:24:11,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:12,017][root][INFO] - Training Epoch: 2/10, step 160/574 completed (loss: 2.098459482192993, acc: 0.38372093439102173)
[2024-12-12 02:24:12,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:12,610][root][INFO] - Training Epoch: 2/10, step 161/574 completed (loss: 1.9661931991577148, acc: 0.43529412150382996)
[2024-12-12 02:24:12,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:13,166][root][INFO] - Training Epoch: 2/10, step 162/574 completed (loss: 2.3080496788024902, acc: 0.3932584226131439)
[2024-12-12 02:24:13,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:13,531][root][INFO] - Training Epoch: 2/10, step 163/574 completed (loss: 2.0569496154785156, acc: 0.5)
[2024-12-12 02:24:13,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:13,903][root][INFO] - Training Epoch: 2/10, step 164/574 completed (loss: 1.8913164138793945, acc: 0.4761904776096344)
[2024-12-12 02:24:14,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:14,268][root][INFO] - Training Epoch: 2/10, step 165/574 completed (loss: 2.028568744659424, acc: 0.48275861144065857)
[2024-12-12 02:24:14,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:14,569][root][INFO] - Training Epoch: 2/10, step 166/574 completed (loss: 1.6182829141616821, acc: 0.5918367505073547)
[2024-12-12 02:24:14,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:14,873][root][INFO] - Training Epoch: 2/10, step 167/574 completed (loss: 1.865372896194458, acc: 0.46000000834465027)
[2024-12-12 02:24:14,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:15,269][root][INFO] - Training Epoch: 2/10, step 168/574 completed (loss: 1.810599684715271, acc: 0.4722222089767456)
[2024-12-12 02:24:15,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:15,589][root][INFO] - Training Epoch: 2/10, step 169/574 completed (loss: 2.067823648452759, acc: 0.47058823704719543)
[2024-12-12 02:24:15,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:16,623][root][INFO] - Training Epoch: 2/10, step 170/574 completed (loss: 2.415203332901001, acc: 0.42465752363204956)
[2024-12-12 02:24:16,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:16,936][root][INFO] - Training Epoch: 2/10, step 171/574 completed (loss: 1.3019894361495972, acc: 0.75)
[2024-12-12 02:24:17,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:17,282][root][INFO] - Training Epoch: 2/10, step 172/574 completed (loss: 1.237284541130066, acc: 0.6666666865348816)
[2024-12-12 02:24:17,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:17,648][root][INFO] - Training Epoch: 2/10, step 173/574 completed (loss: 1.8054629564285278, acc: 0.5357142686843872)
[2024-12-12 02:24:17,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:18,190][root][INFO] - Training Epoch: 2/10, step 174/574 completed (loss: 1.9779740571975708, acc: 0.4955752193927765)
[2024-12-12 02:24:18,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:18,487][root][INFO] - Training Epoch: 2/10, step 175/574 completed (loss: 1.923919439315796, acc: 0.47826087474823)
[2024-12-12 02:24:18,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:18,784][root][INFO] - Training Epoch: 2/10, step 176/574 completed (loss: 2.0903944969177246, acc: 0.4545454680919647)
[2024-12-12 02:24:19,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:19,700][root][INFO] - Training Epoch: 2/10, step 177/574 completed (loss: 2.5121548175811768, acc: 0.35877862572669983)
[2024-12-12 02:24:19,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:20,379][root][INFO] - Training Epoch: 2/10, step 178/574 completed (loss: 2.3911404609680176, acc: 0.34074074029922485)
[2024-12-12 02:24:20,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:20,734][root][INFO] - Training Epoch: 2/10, step 179/574 completed (loss: 1.6582818031311035, acc: 0.5409836173057556)
[2024-12-12 02:24:20,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:21,071][root][INFO] - Training Epoch: 2/10, step 180/574 completed (loss: 0.8017024397850037, acc: 0.8333333134651184)
[2024-12-12 02:24:21,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:21,422][root][INFO] - Training Epoch: 2/10, step 181/574 completed (loss: 1.6265202760696411, acc: 0.6000000238418579)
[2024-12-12 02:24:21,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:21,790][root][INFO] - Training Epoch: 2/10, step 182/574 completed (loss: 1.3768001794815063, acc: 0.6428571343421936)
[2024-12-12 02:24:21,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:22,120][root][INFO] - Training Epoch: 2/10, step 183/574 completed (loss: 2.2996280193328857, acc: 0.3658536672592163)
[2024-12-12 02:24:22,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:22,491][root][INFO] - Training Epoch: 2/10, step 184/574 completed (loss: 2.4735655784606934, acc: 0.36858007311820984)
[2024-12-12 02:24:22,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:22,861][root][INFO] - Training Epoch: 2/10, step 185/574 completed (loss: 2.5407795906066895, acc: 0.3342939615249634)
[2024-12-12 02:24:22,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:23,341][root][INFO] - Training Epoch: 2/10, step 186/574 completed (loss: 2.559727668762207, acc: 0.36250001192092896)
[2024-12-12 02:24:23,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:23,870][root][INFO] - Training Epoch: 2/10, step 187/574 completed (loss: 2.2969017028808594, acc: 0.38273921608924866)
[2024-12-12 02:24:24,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:24,287][root][INFO] - Training Epoch: 2/10, step 188/574 completed (loss: 2.3265554904937744, acc: 0.38434162735939026)
[2024-12-12 02:24:24,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:24,619][root][INFO] - Training Epoch: 2/10, step 189/574 completed (loss: 2.472856044769287, acc: 0.5199999809265137)
[2024-12-12 02:24:24,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:25,180][root][INFO] - Training Epoch: 2/10, step 190/574 completed (loss: 2.3371689319610596, acc: 0.38372093439102173)
[2024-12-12 02:24:25,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:25,979][root][INFO] - Training Epoch: 2/10, step 191/574 completed (loss: 2.083371877670288, acc: 0.4761904776096344)
[2024-12-12 02:24:26,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:26,898][root][INFO] - Training Epoch: 2/10, step 192/574 completed (loss: 2.2086381912231445, acc: 0.42424243688583374)
[2024-12-12 02:24:27,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:27,643][root][INFO] - Training Epoch: 2/10, step 193/574 completed (loss: 1.942237377166748, acc: 0.48235294222831726)
[2024-12-12 02:24:27,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:28,721][root][INFO] - Training Epoch: 2/10, step 194/574 completed (loss: 1.8773348331451416, acc: 0.48148149251937866)
[2024-12-12 02:24:28,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:29,677][root][INFO] - Training Epoch: 2/10, step 195/574 completed (loss: 1.5034986734390259, acc: 0.5645161271095276)
[2024-12-12 02:24:29,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:30,002][root][INFO] - Training Epoch: 2/10, step 196/574 completed (loss: 0.8668438792228699, acc: 0.7142857313156128)
[2024-12-12 02:24:30,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:30,395][root][INFO] - Training Epoch: 2/10, step 197/574 completed (loss: 2.1265883445739746, acc: 0.5249999761581421)
[2024-12-12 02:24:30,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:30,741][root][INFO] - Training Epoch: 2/10, step 198/574 completed (loss: 2.2231481075286865, acc: 0.45588234066963196)
[2024-12-12 02:24:30,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:31,117][root][INFO] - Training Epoch: 2/10, step 199/574 completed (loss: 2.0939130783081055, acc: 0.44117647409439087)
[2024-12-12 02:24:31,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:31,500][root][INFO] - Training Epoch: 2/10, step 200/574 completed (loss: 2.3190503120422363, acc: 0.3644067943096161)
[2024-12-12 02:24:31,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:31,878][root][INFO] - Training Epoch: 2/10, step 201/574 completed (loss: 2.4096829891204834, acc: 0.41044774651527405)
[2024-12-12 02:24:31,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:32,236][root][INFO] - Training Epoch: 2/10, step 202/574 completed (loss: 2.2689480781555176, acc: 0.3689320385456085)
[2024-12-12 02:24:32,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:32,617][root][INFO] - Training Epoch: 2/10, step 203/574 completed (loss: 2.1203622817993164, acc: 0.460317462682724)
[2024-12-12 02:24:32,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:32,981][root][INFO] - Training Epoch: 2/10, step 204/574 completed (loss: 2.0450327396392822, acc: 0.4285714328289032)
[2024-12-12 02:24:33,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:33,337][root][INFO] - Training Epoch: 2/10, step 205/574 completed (loss: 2.2842907905578613, acc: 0.3766816258430481)
[2024-12-12 02:24:33,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:33,750][root][INFO] - Training Epoch: 2/10, step 206/574 completed (loss: 2.166043281555176, acc: 0.4291338622570038)
[2024-12-12 02:24:33,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:34,106][root][INFO] - Training Epoch: 2/10, step 207/574 completed (loss: 2.182070255279541, acc: 0.4267241358757019)
[2024-12-12 02:24:34,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:34,474][root][INFO] - Training Epoch: 2/10, step 208/574 completed (loss: 2.107377529144287, acc: 0.4420289993286133)
[2024-12-12 02:24:34,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:34,844][root][INFO] - Training Epoch: 2/10, step 209/574 completed (loss: 2.3301687240600586, acc: 0.37354084849357605)
[2024-12-12 02:24:34,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:35,184][root][INFO] - Training Epoch: 2/10, step 210/574 completed (loss: 2.4950597286224365, acc: 0.4021739065647125)
[2024-12-12 02:24:35,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:35,502][root][INFO] - Training Epoch: 2/10, step 211/574 completed (loss: 1.59088933467865, acc: 0.5652173757553101)
[2024-12-12 02:24:35,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:35,788][root][INFO] - Training Epoch: 2/10, step 212/574 completed (loss: 1.9048211574554443, acc: 0.5357142686843872)
[2024-12-12 02:24:35,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:36,149][root][INFO] - Training Epoch: 2/10, step 213/574 completed (loss: 1.5789130926132202, acc: 0.5744680762290955)
[2024-12-12 02:24:36,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:36,835][root][INFO] - Training Epoch: 2/10, step 214/574 completed (loss: 1.9798539876937866, acc: 0.4769230782985687)
[2024-12-12 02:24:36,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:37,161][root][INFO] - Training Epoch: 2/10, step 215/574 completed (loss: 1.8294219970703125, acc: 0.5540540814399719)
[2024-12-12 02:24:37,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:37,561][root][INFO] - Training Epoch: 2/10, step 216/574 completed (loss: 1.7168415784835815, acc: 0.5465116500854492)
[2024-12-12 02:24:37,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:38,096][root][INFO] - Training Epoch: 2/10, step 217/574 completed (loss: 1.902869701385498, acc: 0.522522509098053)
[2024-12-12 02:24:38,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:38,489][root][INFO] - Training Epoch: 2/10, step 218/574 completed (loss: 1.782615303993225, acc: 0.5222222208976746)
[2024-12-12 02:24:38,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:38,802][root][INFO] - Training Epoch: 2/10, step 219/574 completed (loss: 0.8980193734169006, acc: 0.7272727489471436)
[2024-12-12 02:24:38,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:39,133][root][INFO] - Training Epoch: 2/10, step 220/574 completed (loss: 0.3665695786476135, acc: 0.9259259104728699)
[2024-12-12 02:24:39,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:39,486][root][INFO] - Training Epoch: 2/10, step 221/574 completed (loss: 0.9639562368392944, acc: 0.7200000286102295)
[2024-12-12 02:24:39,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:39,879][root][INFO] - Training Epoch: 2/10, step 222/574 completed (loss: 2.1060261726379395, acc: 0.4423076808452606)
[2024-12-12 02:24:40,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:40,644][root][INFO] - Training Epoch: 2/10, step 223/574 completed (loss: 1.811935544013977, acc: 0.5)
[2024-12-12 02:24:40,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:41,184][root][INFO] - Training Epoch: 2/10, step 224/574 completed (loss: 2.0844264030456543, acc: 0.40909090638160706)
[2024-12-12 02:24:41,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:41,620][root][INFO] - Training Epoch: 2/10, step 225/574 completed (loss: 2.2492246627807617, acc: 0.3617021143436432)
[2024-12-12 02:24:41,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:41,963][root][INFO] - Training Epoch: 2/10, step 226/574 completed (loss: 1.8070555925369263, acc: 0.43396225571632385)
[2024-12-12 02:24:42,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:42,310][root][INFO] - Training Epoch: 2/10, step 227/574 completed (loss: 2.025709390640259, acc: 0.44999998807907104)
[2024-12-12 02:24:42,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:42,645][root][INFO] - Training Epoch: 2/10, step 228/574 completed (loss: 1.3902719020843506, acc: 0.6279069781303406)
[2024-12-12 02:24:42,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:42,937][root][INFO] - Training Epoch: 2/10, step 229/574 completed (loss: 1.361335277557373, acc: 0.6333333253860474)
[2024-12-12 02:24:43,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:43,318][root][INFO] - Training Epoch: 2/10, step 230/574 completed (loss: 2.412978410720825, acc: 0.4000000059604645)
[2024-12-12 02:24:43,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:43,696][root][INFO] - Training Epoch: 2/10, step 231/574 completed (loss: 1.8577483892440796, acc: 0.5555555820465088)
[2024-12-12 02:24:43,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:44,125][root][INFO] - Training Epoch: 2/10, step 232/574 completed (loss: 1.6956939697265625, acc: 0.5388888716697693)
[2024-12-12 02:24:44,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:44,613][root][INFO] - Training Epoch: 2/10, step 233/574 completed (loss: 1.8272054195404053, acc: 0.5458715558052063)
[2024-12-12 02:24:44,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:45,084][root][INFO] - Training Epoch: 2/10, step 234/574 completed (loss: 1.7241333723068237, acc: 0.5384615659713745)
[2024-12-12 02:24:45,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:45,355][root][INFO] - Training Epoch: 2/10, step 235/574 completed (loss: 1.385604739189148, acc: 0.7368420958518982)
[2024-12-12 02:24:45,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:45,749][root][INFO] - Training Epoch: 2/10, step 236/574 completed (loss: 1.6147451400756836, acc: 0.5833333134651184)
[2024-12-12 02:24:45,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:46,118][root][INFO] - Training Epoch: 2/10, step 237/574 completed (loss: 2.912583112716675, acc: 0.3636363744735718)
[2024-12-12 02:24:46,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:46,476][root][INFO] - Training Epoch: 2/10, step 238/574 completed (loss: 1.9297800064086914, acc: 0.37037035822868347)
[2024-12-12 02:24:46,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:46,814][root][INFO] - Training Epoch: 2/10, step 239/574 completed (loss: 1.4882185459136963, acc: 0.6857143044471741)
[2024-12-12 02:24:46,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:47,155][root][INFO] - Training Epoch: 2/10, step 240/574 completed (loss: 1.6131027936935425, acc: 0.5681818127632141)
[2024-12-12 02:24:47,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:47,525][root][INFO] - Training Epoch: 2/10, step 241/574 completed (loss: 1.9220095872879028, acc: 0.47727271914482117)
[2024-12-12 02:24:47,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:48,108][root][INFO] - Training Epoch: 2/10, step 242/574 completed (loss: 2.0717127323150635, acc: 0.4032258093357086)
[2024-12-12 02:24:48,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:48,640][root][INFO] - Training Epoch: 2/10, step 243/574 completed (loss: 1.6674591302871704, acc: 0.5)
[2024-12-12 02:24:48,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:48,925][root][INFO] - Training Epoch: 2/10, step 244/574 completed (loss: 0.6084332466125488, acc: 0.8571428656578064)
[2024-12-12 02:24:49,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:49,257][root][INFO] - Training Epoch: 2/10, step 245/574 completed (loss: 1.2415229082107544, acc: 0.7307692170143127)
[2024-12-12 02:24:49,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:49,588][root][INFO] - Training Epoch: 2/10, step 246/574 completed (loss: 1.421837329864502, acc: 0.7096773982048035)
[2024-12-12 02:24:49,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:49,958][root][INFO] - Training Epoch: 2/10, step 247/574 completed (loss: 0.9892875552177429, acc: 0.6000000238418579)
[2024-12-12 02:24:50,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:50,357][root][INFO] - Training Epoch: 2/10, step 248/574 completed (loss: 1.5119494199752808, acc: 0.5405405163764954)
[2024-12-12 02:24:50,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:50,688][root][INFO] - Training Epoch: 2/10, step 249/574 completed (loss: 1.6518291234970093, acc: 0.5135135054588318)
[2024-12-12 02:24:50,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:51,011][root][INFO] - Training Epoch: 2/10, step 250/574 completed (loss: 1.4737441539764404, acc: 0.5675675868988037)
[2024-12-12 02:24:51,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:51,337][root][INFO] - Training Epoch: 2/10, step 251/574 completed (loss: 1.9491902589797974, acc: 0.44117647409439087)
[2024-12-12 02:24:51,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:51,680][root][INFO] - Training Epoch: 2/10, step 252/574 completed (loss: 0.7298977375030518, acc: 0.8048780560493469)
[2024-12-12 02:24:51,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:52,021][root][INFO] - Training Epoch: 2/10, step 253/574 completed (loss: 0.5518518686294556, acc: 0.8799999952316284)
[2024-12-12 02:24:52,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:52,313][root][INFO] - Training Epoch: 2/10, step 254/574 completed (loss: 0.2994215190410614, acc: 0.9200000166893005)
[2024-12-12 02:24:52,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:52,682][root][INFO] - Training Epoch: 2/10, step 255/574 completed (loss: 0.6909970641136169, acc: 0.8064516186714172)
[2024-12-12 02:24:52,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:53,013][root][INFO] - Training Epoch: 2/10, step 256/574 completed (loss: 1.7702336311340332, acc: 0.5263158082962036)
[2024-12-12 02:24:53,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:53,364][root][INFO] - Training Epoch: 2/10, step 257/574 completed (loss: 1.9157238006591797, acc: 0.5428571701049805)
[2024-12-12 02:24:53,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:53,725][root][INFO] - Training Epoch: 2/10, step 258/574 completed (loss: 1.563775658607483, acc: 0.5657894611358643)
[2024-12-12 02:24:53,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:54,294][root][INFO] - Training Epoch: 2/10, step 259/574 completed (loss: 1.8200581073760986, acc: 0.4433962404727936)
[2024-12-12 02:24:54,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:54,877][root][INFO] - Training Epoch: 2/10, step 260/574 completed (loss: 1.9879552125930786, acc: 0.4333333373069763)
[2024-12-12 02:24:54,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:55,178][root][INFO] - Training Epoch: 2/10, step 261/574 completed (loss: 1.3441051244735718, acc: 0.6666666865348816)
[2024-12-12 02:24:55,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:55,475][root][INFO] - Training Epoch: 2/10, step 262/574 completed (loss: 1.9355638027191162, acc: 0.5483871102333069)
[2024-12-12 02:24:55,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:55,823][root][INFO] - Training Epoch: 2/10, step 263/574 completed (loss: 2.956195831298828, acc: 0.30666667222976685)
[2024-12-12 02:24:55,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:56,173][root][INFO] - Training Epoch: 2/10, step 264/574 completed (loss: 2.435164451599121, acc: 0.375)
[2024-12-12 02:24:56,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:57,055][root][INFO] - Training Epoch: 2/10, step 265/574 completed (loss: 2.6082825660705566, acc: 0.328000009059906)
[2024-12-12 02:24:57,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:57,414][root][INFO] - Training Epoch: 2/10, step 266/574 completed (loss: 2.1712050437927246, acc: 0.3932584226131439)
[2024-12-12 02:24:57,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:57,777][root][INFO] - Training Epoch: 2/10, step 267/574 completed (loss: 2.220647096633911, acc: 0.45945945382118225)
[2024-12-12 02:24:57,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:58,232][root][INFO] - Training Epoch: 2/10, step 268/574 completed (loss: 1.4862393140792847, acc: 0.5862069129943848)
[2024-12-12 02:24:58,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:58,530][root][INFO] - Training Epoch: 2/10, step 269/574 completed (loss: 1.3612185716629028, acc: 0.6818181872367859)
[2024-12-12 02:24:58,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:58,882][root][INFO] - Training Epoch: 2/10, step 270/574 completed (loss: 1.3499540090560913, acc: 0.5454545617103577)
[2024-12-12 02:24:58,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:59,229][root][INFO] - Training Epoch: 2/10, step 271/574 completed (loss: 1.030239224433899, acc: 0.71875)
[2024-12-12 02:24:59,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:59,588][root][INFO] - Training Epoch: 2/10, step 272/574 completed (loss: 1.190875768661499, acc: 0.6666666865348816)
[2024-12-12 02:24:59,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:24:59,943][root][INFO] - Training Epoch: 2/10, step 273/574 completed (loss: 1.999206304550171, acc: 0.38333332538604736)
[2024-12-12 02:25:00,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:00,256][root][INFO] - Training Epoch: 2/10, step 274/574 completed (loss: 1.510056734085083, acc: 0.5)
[2024-12-12 02:25:00,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:00,621][root][INFO] - Training Epoch: 2/10, step 275/574 completed (loss: 1.0668128728866577, acc: 0.6333333253860474)
[2024-12-12 02:25:00,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:00,973][root][INFO] - Training Epoch: 2/10, step 276/574 completed (loss: 1.714333415031433, acc: 0.6551724076271057)
[2024-12-12 02:25:01,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:01,297][root][INFO] - Training Epoch: 2/10, step 277/574 completed (loss: 1.4383704662322998, acc: 0.6399999856948853)
[2024-12-12 02:25:01,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:01,566][root][INFO] - Training Epoch: 2/10, step 278/574 completed (loss: 2.355184316635132, acc: 0.38297873735427856)
[2024-12-12 02:25:01,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:01,940][root][INFO] - Training Epoch: 2/10, step 279/574 completed (loss: 1.854324460029602, acc: 0.5416666865348816)
[2024-12-12 02:25:02,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:02,318][root][INFO] - Training Epoch: 2/10, step 280/574 completed (loss: 1.7667804956436157, acc: 0.5909090638160706)
[2024-12-12 02:25:02,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:02,747][root][INFO] - Training Epoch: 2/10, step 281/574 completed (loss: 2.3350534439086914, acc: 0.42168673872947693)
[2024-12-12 02:25:02,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:03,134][root][INFO] - Training Epoch: 2/10, step 282/574 completed (loss: 2.1207263469696045, acc: 0.46296295523643494)
[2024-12-12 02:25:03,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:03,459][root][INFO] - Training Epoch: 2/10, step 283/574 completed (loss: 2.4659981727600098, acc: 0.2368421107530594)
[2024-12-12 02:25:04,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:04,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:04,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:05,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:05,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:06,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:06,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:06,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:07,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:07,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:07,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:08,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:08,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:08,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:09,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:09,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:09,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:10,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:10,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:10,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:11,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:11,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:12,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:12,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:12,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:12,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:13,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:13,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:13,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:14,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:14,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:15,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:15,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:16,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:16,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:16,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:16,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:17,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:17,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:18,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:18,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:18,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:19,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:19,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:19,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:20,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:20,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:20,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:21,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:21,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:21,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:22,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:22,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:22,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:23,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:23,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:23,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:24,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:24,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:24,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:25,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:25,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:25,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:26,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:26,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:26,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:27,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:27,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:28,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:28,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:28,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:29,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:29,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:29,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:30,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:30,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:30,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:31,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:31,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:31,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:32,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:32,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:32,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:33,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:33,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:34,121][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(6.6275, device='cuda:0') eval_epoch_loss=tensor(1.8912, device='cuda:0') eval_epoch_acc=tensor(0.5090, device='cuda:0')
[2024-12-12 02:25:34,122][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:25:34,122][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:25:34,313][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_2_step_284_loss_1.891224980354309/model.pt
[2024-12-12 02:25:34,317][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:25:34,317][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 1.891224980354309
[2024-12-12 02:25:34,318][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.5089747309684753
[2024-12-12 02:25:34,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:34,722][root][INFO] - Training Epoch: 2/10, step 284/574 completed (loss: 2.485301971435547, acc: 0.2647058963775635)
[2024-12-12 02:25:34,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:35,015][root][INFO] - Training Epoch: 2/10, step 285/574 completed (loss: 2.176262140274048, acc: 0.3499999940395355)
[2024-12-12 02:25:35,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:35,357][root][INFO] - Training Epoch: 2/10, step 286/574 completed (loss: 2.1968235969543457, acc: 0.375)
[2024-12-12 02:25:35,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:35,733][root][INFO] - Training Epoch: 2/10, step 287/574 completed (loss: 2.3787477016448975, acc: 0.36000001430511475)
[2024-12-12 02:25:35,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:36,046][root][INFO] - Training Epoch: 2/10, step 288/574 completed (loss: 1.964431881904602, acc: 0.4615384638309479)
[2024-12-12 02:25:36,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:36,398][root][INFO] - Training Epoch: 2/10, step 289/574 completed (loss: 2.449648857116699, acc: 0.34161490201950073)
[2024-12-12 02:25:36,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:36,745][root][INFO] - Training Epoch: 2/10, step 290/574 completed (loss: 2.4409022331237793, acc: 0.3711340129375458)
[2024-12-12 02:25:36,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:37,042][root][INFO] - Training Epoch: 2/10, step 291/574 completed (loss: 0.7780269980430603, acc: 0.7272727489471436)
[2024-12-12 02:25:37,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:37,385][root][INFO] - Training Epoch: 2/10, step 292/574 completed (loss: 2.068141222000122, acc: 0.4285714328289032)
[2024-12-12 02:25:37,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:37,807][root][INFO] - Training Epoch: 2/10, step 293/574 completed (loss: 1.67264723777771, acc: 0.6206896305084229)
[2024-12-12 02:25:37,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:38,269][root][INFO] - Training Epoch: 2/10, step 294/574 completed (loss: 1.3498094081878662, acc: 0.6181818246841431)
[2024-12-12 02:25:38,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:38,817][root][INFO] - Training Epoch: 2/10, step 295/574 completed (loss: 1.9033946990966797, acc: 0.5051546096801758)
[2024-12-12 02:25:38,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:39,111][root][INFO] - Training Epoch: 2/10, step 296/574 completed (loss: 2.1164612770080566, acc: 0.4482758641242981)
[2024-12-12 02:25:39,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:39,460][root][INFO] - Training Epoch: 2/10, step 297/574 completed (loss: 2.0886287689208984, acc: 0.40740740299224854)
[2024-12-12 02:25:39,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:39,829][root][INFO] - Training Epoch: 2/10, step 298/574 completed (loss: 1.9879158735275269, acc: 0.5526315569877625)
[2024-12-12 02:25:39,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:40,175][root][INFO] - Training Epoch: 2/10, step 299/574 completed (loss: 1.775166392326355, acc: 0.6071428656578064)
[2024-12-12 02:25:40,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:40,567][root][INFO] - Training Epoch: 2/10, step 300/574 completed (loss: 1.8221423625946045, acc: 0.53125)
[2024-12-12 02:25:40,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:40,937][root][INFO] - Training Epoch: 2/10, step 301/574 completed (loss: 2.0969395637512207, acc: 0.4716981053352356)
[2024-12-12 02:25:41,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:41,307][root][INFO] - Training Epoch: 2/10, step 302/574 completed (loss: 1.1986408233642578, acc: 0.6226415038108826)
[2024-12-12 02:25:41,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:41,679][root][INFO] - Training Epoch: 2/10, step 303/574 completed (loss: 1.3217471837997437, acc: 0.6470588445663452)
[2024-12-12 02:25:41,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:42,034][root][INFO] - Training Epoch: 2/10, step 304/574 completed (loss: 2.032285690307617, acc: 0.46875)
[2024-12-12 02:25:42,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:42,373][root][INFO] - Training Epoch: 2/10, step 305/574 completed (loss: 1.3956873416900635, acc: 0.6065573692321777)
[2024-12-12 02:25:42,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:42,737][root][INFO] - Training Epoch: 2/10, step 306/574 completed (loss: 0.7571701407432556, acc: 0.8666666746139526)
[2024-12-12 02:25:42,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:43,047][root][INFO] - Training Epoch: 2/10, step 307/574 completed (loss: 0.44708964228630066, acc: 0.8947368264198303)
[2024-12-12 02:25:43,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:43,408][root][INFO] - Training Epoch: 2/10, step 308/574 completed (loss: 2.114673137664795, acc: 0.4637681245803833)
[2024-12-12 02:25:43,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:43,838][root][INFO] - Training Epoch: 2/10, step 309/574 completed (loss: 1.8906694650650024, acc: 0.5)
[2024-12-12 02:25:43,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:44,211][root][INFO] - Training Epoch: 2/10, step 310/574 completed (loss: 1.7581908702850342, acc: 0.5421686768531799)
[2024-12-12 02:25:44,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:44,549][root][INFO] - Training Epoch: 2/10, step 311/574 completed (loss: 2.275954484939575, acc: 0.3333333432674408)
[2024-12-12 02:25:44,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:44,920][root][INFO] - Training Epoch: 2/10, step 312/574 completed (loss: 2.3670291900634766, acc: 0.40816327929496765)
[2024-12-12 02:25:45,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:45,269][root][INFO] - Training Epoch: 2/10, step 313/574 completed (loss: 0.20544908940792084, acc: 0.9583333134651184)
[2024-12-12 02:25:45,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:45,591][root][INFO] - Training Epoch: 2/10, step 314/574 completed (loss: 1.2179442644119263, acc: 0.7083333134651184)
[2024-12-12 02:25:45,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:45,901][root][INFO] - Training Epoch: 2/10, step 315/574 completed (loss: 0.9021090865135193, acc: 0.774193525314331)
[2024-12-12 02:25:45,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:46,196][root][INFO] - Training Epoch: 2/10, step 316/574 completed (loss: 1.1533739566802979, acc: 0.6129032373428345)
[2024-12-12 02:25:46,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:46,556][root][INFO] - Training Epoch: 2/10, step 317/574 completed (loss: 1.5192376375198364, acc: 0.5970149040222168)
[2024-12-12 02:25:46,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:46,903][root][INFO] - Training Epoch: 2/10, step 318/574 completed (loss: 1.4461967945098877, acc: 0.6153846383094788)
[2024-12-12 02:25:46,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:47,216][root][INFO] - Training Epoch: 2/10, step 319/574 completed (loss: 2.0694096088409424, acc: 0.42222222685813904)
[2024-12-12 02:25:47,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:47,600][root][INFO] - Training Epoch: 2/10, step 320/574 completed (loss: 1.6239879131317139, acc: 0.5645161271095276)
[2024-12-12 02:25:47,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:47,970][root][INFO] - Training Epoch: 2/10, step 321/574 completed (loss: 0.9602851867675781, acc: 0.7599999904632568)
[2024-12-12 02:25:48,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:48,351][root][INFO] - Training Epoch: 2/10, step 322/574 completed (loss: 2.325043201446533, acc: 0.37037035822868347)
[2024-12-12 02:25:48,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:48,692][root][INFO] - Training Epoch: 2/10, step 323/574 completed (loss: 3.165475845336914, acc: 0.1428571492433548)
[2024-12-12 02:25:48,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:49,000][root][INFO] - Training Epoch: 2/10, step 324/574 completed (loss: 2.3401970863342285, acc: 0.3333333432674408)
[2024-12-12 02:25:49,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:49,274][root][INFO] - Training Epoch: 2/10, step 325/574 completed (loss: 2.5291635990142822, acc: 0.39024388790130615)
[2024-12-12 02:25:49,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:49,630][root][INFO] - Training Epoch: 2/10, step 326/574 completed (loss: 2.1228160858154297, acc: 0.4736842215061188)
[2024-12-12 02:25:49,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:49,990][root][INFO] - Training Epoch: 2/10, step 327/574 completed (loss: 1.4678996801376343, acc: 0.5789473652839661)
[2024-12-12 02:25:50,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:50,336][root][INFO] - Training Epoch: 2/10, step 328/574 completed (loss: 0.996582567691803, acc: 0.7857142686843872)
[2024-12-12 02:25:50,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:50,677][root][INFO] - Training Epoch: 2/10, step 329/574 completed (loss: 2.0516583919525146, acc: 0.40740740299224854)
[2024-12-12 02:25:50,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:51,058][root][INFO] - Training Epoch: 2/10, step 330/574 completed (loss: 0.8840427994728088, acc: 0.71875)
[2024-12-12 02:25:51,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:51,457][root][INFO] - Training Epoch: 2/10, step 331/574 completed (loss: 1.9288517236709595, acc: 0.4838709533214569)
[2024-12-12 02:25:51,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:51,862][root][INFO] - Training Epoch: 2/10, step 332/574 completed (loss: 1.6845769882202148, acc: 0.4912280738353729)
[2024-12-12 02:25:51,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:52,201][root][INFO] - Training Epoch: 2/10, step 333/574 completed (loss: 2.372105598449707, acc: 0.375)
[2024-12-12 02:25:52,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:52,548][root][INFO] - Training Epoch: 2/10, step 334/574 completed (loss: 1.1963515281677246, acc: 0.6666666865348816)
[2024-12-12 02:25:52,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:52,938][root][INFO] - Training Epoch: 2/10, step 335/574 completed (loss: 1.9859354496002197, acc: 0.42105263471603394)
[2024-12-12 02:25:53,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:53,345][root][INFO] - Training Epoch: 2/10, step 336/574 completed (loss: 2.1782829761505127, acc: 0.4399999976158142)
[2024-12-12 02:25:53,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:53,716][root][INFO] - Training Epoch: 2/10, step 337/574 completed (loss: 2.2677948474884033, acc: 0.40229883790016174)
[2024-12-12 02:25:53,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:54,117][root][INFO] - Training Epoch: 2/10, step 338/574 completed (loss: 2.4531829357147217, acc: 0.3510638177394867)
[2024-12-12 02:25:54,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:54,498][root][INFO] - Training Epoch: 2/10, step 339/574 completed (loss: 2.494469404220581, acc: 0.34939759969711304)
[2024-12-12 02:25:54,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:54,875][root][INFO] - Training Epoch: 2/10, step 340/574 completed (loss: 1.0605483055114746, acc: 0.695652186870575)
[2024-12-12 02:25:55,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:55,253][root][INFO] - Training Epoch: 2/10, step 341/574 completed (loss: 2.0082218647003174, acc: 0.5384615659713745)
[2024-12-12 02:25:55,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:55,650][root][INFO] - Training Epoch: 2/10, step 342/574 completed (loss: 2.4551310539245605, acc: 0.3734939694404602)
[2024-12-12 02:25:55,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:56,034][root][INFO] - Training Epoch: 2/10, step 343/574 completed (loss: 1.901177167892456, acc: 0.5094339847564697)
[2024-12-12 02:25:56,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:56,377][root][INFO] - Training Epoch: 2/10, step 344/574 completed (loss: 2.0432519912719727, acc: 0.4683544337749481)
[2024-12-12 02:25:56,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:56,774][root][INFO] - Training Epoch: 2/10, step 345/574 completed (loss: 1.987778902053833, acc: 0.4901960790157318)
[2024-12-12 02:25:56,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:57,096][root][INFO] - Training Epoch: 2/10, step 346/574 completed (loss: 2.511566400527954, acc: 0.3283582031726837)
[2024-12-12 02:25:57,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:57,463][root][INFO] - Training Epoch: 2/10, step 347/574 completed (loss: 0.9226581454277039, acc: 0.699999988079071)
[2024-12-12 02:25:57,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:57,857][root][INFO] - Training Epoch: 2/10, step 348/574 completed (loss: 1.6218124628067017, acc: 0.6000000238418579)
[2024-12-12 02:25:58,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:58,279][root][INFO] - Training Epoch: 2/10, step 349/574 completed (loss: 1.3026652336120605, acc: 0.6666666865348816)
[2024-12-12 02:25:58,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:58,674][root][INFO] - Training Epoch: 2/10, step 350/574 completed (loss: 1.931139349937439, acc: 0.4883720874786377)
[2024-12-12 02:25:58,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:59,024][root][INFO] - Training Epoch: 2/10, step 351/574 completed (loss: 1.9997100830078125, acc: 0.43589743971824646)
[2024-12-12 02:25:59,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:59,410][root][INFO] - Training Epoch: 2/10, step 352/574 completed (loss: 1.931182861328125, acc: 0.4444444477558136)
[2024-12-12 02:25:59,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:25:59,774][root][INFO] - Training Epoch: 2/10, step 353/574 completed (loss: 0.8443244695663452, acc: 0.782608687877655)
[2024-12-12 02:25:59,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:00,080][root][INFO] - Training Epoch: 2/10, step 354/574 completed (loss: 2.416001319885254, acc: 0.5384615659713745)
[2024-12-12 02:26:00,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:00,440][root][INFO] - Training Epoch: 2/10, step 355/574 completed (loss: 2.5429275035858154, acc: 0.32967033982276917)
[2024-12-12 02:26:00,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:00,937][root][INFO] - Training Epoch: 2/10, step 356/574 completed (loss: 1.9627677202224731, acc: 0.5130434632301331)
[2024-12-12 02:26:01,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:01,321][root][INFO] - Training Epoch: 2/10, step 357/574 completed (loss: 2.0602967739105225, acc: 0.43478259444236755)
[2024-12-12 02:26:01,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:01,699][root][INFO] - Training Epoch: 2/10, step 358/574 completed (loss: 2.1761672496795654, acc: 0.3877550959587097)
[2024-12-12 02:26:01,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:02,053][root][INFO] - Training Epoch: 2/10, step 359/574 completed (loss: 0.3924354612827301, acc: 0.9166666865348816)
[2024-12-12 02:26:02,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:02,406][root][INFO] - Training Epoch: 2/10, step 360/574 completed (loss: 0.9099608063697815, acc: 0.6538461446762085)
[2024-12-12 02:26:02,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:02,736][root][INFO] - Training Epoch: 2/10, step 361/574 completed (loss: 1.5364247560501099, acc: 0.6341463327407837)
[2024-12-12 02:26:02,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:03,077][root][INFO] - Training Epoch: 2/10, step 362/574 completed (loss: 1.7405439615249634, acc: 0.5333333611488342)
[2024-12-12 02:26:03,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:03,463][root][INFO] - Training Epoch: 2/10, step 363/574 completed (loss: 2.151843309402466, acc: 0.40789473056793213)
[2024-12-12 02:26:03,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:03,842][root][INFO] - Training Epoch: 2/10, step 364/574 completed (loss: 1.9683735370635986, acc: 0.5365853905677795)
[2024-12-12 02:26:03,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:04,169][root][INFO] - Training Epoch: 2/10, step 365/574 completed (loss: 1.9112894535064697, acc: 0.4545454680919647)
[2024-12-12 02:26:04,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:04,515][root][INFO] - Training Epoch: 2/10, step 366/574 completed (loss: 0.8074789643287659, acc: 0.8333333134651184)
[2024-12-12 02:26:04,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:04,848][root][INFO] - Training Epoch: 2/10, step 367/574 completed (loss: 0.3392241895198822, acc: 0.9130434989929199)
[2024-12-12 02:26:04,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:05,156][root][INFO] - Training Epoch: 2/10, step 368/574 completed (loss: 0.7497830986976624, acc: 0.8214285969734192)
[2024-12-12 02:26:05,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:05,497][root][INFO] - Training Epoch: 2/10, step 369/574 completed (loss: 1.246963381767273, acc: 0.59375)
[2024-12-12 02:26:05,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:06,101][root][INFO] - Training Epoch: 2/10, step 370/574 completed (loss: 1.9545778036117554, acc: 0.4909090995788574)
[2024-12-12 02:26:06,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:06,963][root][INFO] - Training Epoch: 2/10, step 371/574 completed (loss: 1.5640132427215576, acc: 0.6132075190544128)
[2024-12-12 02:26:07,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:07,326][root][INFO] - Training Epoch: 2/10, step 372/574 completed (loss: 1.7737390995025635, acc: 0.5222222208976746)
[2024-12-12 02:26:07,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:07,694][root][INFO] - Training Epoch: 2/10, step 373/574 completed (loss: 1.7897461652755737, acc: 0.5178571343421936)
[2024-12-12 02:26:07,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:08,066][root][INFO] - Training Epoch: 2/10, step 374/574 completed (loss: 1.0993188619613647, acc: 0.7142857313156128)
[2024-12-12 02:26:08,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:08,407][root][INFO] - Training Epoch: 2/10, step 375/574 completed (loss: 0.19681544601917267, acc: 0.9599999785423279)
[2024-12-12 02:26:08,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:08,633][root][INFO] - Training Epoch: 2/10, step 376/574 completed (loss: 0.5794165134429932, acc: 0.782608687877655)
[2024-12-12 02:26:08,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:08,932][root][INFO] - Training Epoch: 2/10, step 377/574 completed (loss: 2.199234962463379, acc: 0.375)
[2024-12-12 02:26:09,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:09,251][root][INFO] - Training Epoch: 2/10, step 378/574 completed (loss: 1.934799313545227, acc: 0.4842105209827423)
[2024-12-12 02:26:09,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:09,824][root][INFO] - Training Epoch: 2/10, step 379/574 completed (loss: 1.9396164417266846, acc: 0.5508981943130493)
[2024-12-12 02:26:09,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:10,197][root][INFO] - Training Epoch: 2/10, step 380/574 completed (loss: 1.771136999130249, acc: 0.5413534045219421)
[2024-12-12 02:26:10,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:11,421][root][INFO] - Training Epoch: 2/10, step 381/574 completed (loss: 1.8290075063705444, acc: 0.5347593426704407)
[2024-12-12 02:26:11,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:11,985][root][INFO] - Training Epoch: 2/10, step 382/574 completed (loss: 1.5155551433563232, acc: 0.5585585832595825)
[2024-12-12 02:26:12,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:12,335][root][INFO] - Training Epoch: 2/10, step 383/574 completed (loss: 0.8632491827011108, acc: 0.8214285969734192)
[2024-12-12 02:26:12,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:12,705][root][INFO] - Training Epoch: 2/10, step 384/574 completed (loss: 0.9601320028305054, acc: 0.75)
[2024-12-12 02:26:12,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:13,046][root][INFO] - Training Epoch: 2/10, step 385/574 completed (loss: 1.4217034578323364, acc: 0.59375)
[2024-12-12 02:26:13,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:13,356][root][INFO] - Training Epoch: 2/10, step 386/574 completed (loss: 1.1896694898605347, acc: 0.6666666865348816)
[2024-12-12 02:26:13,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:13,688][root][INFO] - Training Epoch: 2/10, step 387/574 completed (loss: 1.1186537742614746, acc: 0.7105262875556946)
[2024-12-12 02:26:13,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:14,042][root][INFO] - Training Epoch: 2/10, step 388/574 completed (loss: 0.7952858209609985, acc: 0.7727272510528564)
[2024-12-12 02:26:14,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:14,405][root][INFO] - Training Epoch: 2/10, step 389/574 completed (loss: 1.3461138010025024, acc: 0.6000000238418579)
[2024-12-12 02:26:14,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:14,686][root][INFO] - Training Epoch: 2/10, step 390/574 completed (loss: 0.8641560673713684, acc: 0.8095238208770752)
[2024-12-12 02:26:14,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:14,981][root][INFO] - Training Epoch: 2/10, step 391/574 completed (loss: 2.5657408237457275, acc: 0.42592594027519226)
[2024-12-12 02:26:15,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:15,382][root][INFO] - Training Epoch: 2/10, step 392/574 completed (loss: 2.3178036212921143, acc: 0.43689319491386414)
[2024-12-12 02:26:15,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:15,904][root][INFO] - Training Epoch: 2/10, step 393/574 completed (loss: 1.9265336990356445, acc: 0.5)
[2024-12-12 02:26:16,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:16,275][root][INFO] - Training Epoch: 2/10, step 394/574 completed (loss: 2.291656255722046, acc: 0.4399999976158142)
[2024-12-12 02:26:16,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:16,681][root][INFO] - Training Epoch: 2/10, step 395/574 completed (loss: 2.159102439880371, acc: 0.4513888955116272)
[2024-12-12 02:26:16,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:17,022][root][INFO] - Training Epoch: 2/10, step 396/574 completed (loss: 2.160339117050171, acc: 0.5116279125213623)
[2024-12-12 02:26:17,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:17,373][root][INFO] - Training Epoch: 2/10, step 397/574 completed (loss: 1.133836269378662, acc: 0.6666666865348816)
[2024-12-12 02:26:17,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:17,786][root][INFO] - Training Epoch: 2/10, step 398/574 completed (loss: 1.803226351737976, acc: 0.5116279125213623)
[2024-12-12 02:26:17,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:18,201][root][INFO] - Training Epoch: 2/10, step 399/574 completed (loss: 1.6745260953903198, acc: 0.5199999809265137)
[2024-12-12 02:26:18,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:18,740][root][INFO] - Training Epoch: 2/10, step 400/574 completed (loss: 1.981853723526001, acc: 0.4852941036224365)
[2024-12-12 02:26:18,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:19,131][root][INFO] - Training Epoch: 2/10, step 401/574 completed (loss: 1.8297314643859863, acc: 0.5199999809265137)
[2024-12-12 02:26:19,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:19,475][root][INFO] - Training Epoch: 2/10, step 402/574 completed (loss: 1.3648979663848877, acc: 0.6666666865348816)
[2024-12-12 02:26:19,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:19,788][root][INFO] - Training Epoch: 2/10, step 403/574 completed (loss: 1.4852949380874634, acc: 0.5757575631141663)
[2024-12-12 02:26:19,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:20,091][root][INFO] - Training Epoch: 2/10, step 404/574 completed (loss: 0.9140767455101013, acc: 0.774193525314331)
[2024-12-12 02:26:20,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:20,427][root][INFO] - Training Epoch: 2/10, step 405/574 completed (loss: 1.4021199941635132, acc: 0.5925925970077515)
[2024-12-12 02:26:20,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:20,787][root][INFO] - Training Epoch: 2/10, step 406/574 completed (loss: 0.6934444904327393, acc: 0.8399999737739563)
[2024-12-12 02:26:20,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:21,137][root][INFO] - Training Epoch: 2/10, step 407/574 completed (loss: 0.8007725477218628, acc: 0.7777777910232544)
[2024-12-12 02:26:21,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:21,506][root][INFO] - Training Epoch: 2/10, step 408/574 completed (loss: 0.7478012442588806, acc: 0.8148148059844971)
[2024-12-12 02:26:21,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:21,848][root][INFO] - Training Epoch: 2/10, step 409/574 completed (loss: 1.0323727130889893, acc: 0.692307710647583)
[2024-12-12 02:26:21,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:22,182][root][INFO] - Training Epoch: 2/10, step 410/574 completed (loss: 1.3299051523208618, acc: 0.6724137663841248)
[2024-12-12 02:26:22,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:22,522][root][INFO] - Training Epoch: 2/10, step 411/574 completed (loss: 1.1911922693252563, acc: 0.75)
[2024-12-12 02:26:22,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:22,868][root][INFO] - Training Epoch: 2/10, step 412/574 completed (loss: 1.069284439086914, acc: 0.7333333492279053)
[2024-12-12 02:26:22,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:23,168][root][INFO] - Training Epoch: 2/10, step 413/574 completed (loss: 1.0922431945800781, acc: 0.6969696879386902)
[2024-12-12 02:26:23,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:23,501][root][INFO] - Training Epoch: 2/10, step 414/574 completed (loss: 1.099782109260559, acc: 0.6818181872367859)
[2024-12-12 02:26:23,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:23,829][root][INFO] - Training Epoch: 2/10, step 415/574 completed (loss: 2.2000160217285156, acc: 0.4901960790157318)
[2024-12-12 02:26:23,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:24,163][root][INFO] - Training Epoch: 2/10, step 416/574 completed (loss: 1.8763540983200073, acc: 0.5769230723381042)
[2024-12-12 02:26:24,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:24,534][root][INFO] - Training Epoch: 2/10, step 417/574 completed (loss: 1.5717949867248535, acc: 0.5555555820465088)
[2024-12-12 02:26:24,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:24,918][root][INFO] - Training Epoch: 2/10, step 418/574 completed (loss: 1.6002233028411865, acc: 0.574999988079071)
[2024-12-12 02:26:25,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:25,260][root][INFO] - Training Epoch: 2/10, step 419/574 completed (loss: 2.1498048305511475, acc: 0.4000000059604645)
[2024-12-12 02:26:25,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:25,525][root][INFO] - Training Epoch: 2/10, step 420/574 completed (loss: 0.33021169900894165, acc: 0.9523809552192688)
[2024-12-12 02:26:25,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:25,823][root][INFO] - Training Epoch: 2/10, step 421/574 completed (loss: 1.3177074193954468, acc: 0.6666666865348816)
[2024-12-12 02:26:25,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:26,114][root][INFO] - Training Epoch: 2/10, step 422/574 completed (loss: 1.2895314693450928, acc: 0.59375)
[2024-12-12 02:26:26,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:26,462][root][INFO] - Training Epoch: 2/10, step 423/574 completed (loss: 1.8512744903564453, acc: 0.4722222089767456)
[2024-12-12 02:26:26,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:26,812][root][INFO] - Training Epoch: 2/10, step 424/574 completed (loss: 1.4947762489318848, acc: 0.5555555820465088)
[2024-12-12 02:26:26,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:27,183][root][INFO] - Training Epoch: 2/10, step 425/574 completed (loss: 1.3443344831466675, acc: 0.6363636255264282)
[2024-12-12 02:26:27,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:27,515][root][INFO] - Training Epoch: 2/10, step 426/574 completed (loss: 1.0476418733596802, acc: 0.695652186870575)
[2024-12-12 02:26:28,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:28,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:28,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:29,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:29,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:30,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:30,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:30,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:31,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:31,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:31,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:32,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:32,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:33,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:33,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:33,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:34,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:34,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:34,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:35,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:35,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:36,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:36,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:36,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:36,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:37,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:37,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:38,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:38,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:38,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:39,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:39,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:39,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:40,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:40,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:40,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:41,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:41,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:42,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:42,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:42,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:43,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:43,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:43,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:43,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:44,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:44,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:45,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:45,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:45,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:46,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:46,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:46,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:47,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:47,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:47,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:48,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:48,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:48,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:49,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:49,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:49,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:50,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:50,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:51,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:51,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:51,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:52,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:52,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:53,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:53,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:54,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:54,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:54,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:54,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:55,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:55,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:56,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:56,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:56,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:57,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:57,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:57,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:57,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:58,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:58,894][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(7.3598, device='cuda:0') eval_epoch_loss=tensor(1.9960, device='cuda:0') eval_epoch_acc=tensor(0.4639, device='cuda:0')
[2024-12-12 02:26:58,895][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:26:58,896][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:26:59,179][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_2_step_427_loss_1.9960334300994873/model.pt
[2024-12-12 02:26:59,184][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:26:59,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:59,601][root][INFO] - Training Epoch: 2/10, step 427/574 completed (loss: 1.400384545326233, acc: 0.5405405163764954)
[2024-12-12 02:26:59,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:26:59,928][root][INFO] - Training Epoch: 2/10, step 428/574 completed (loss: 0.8878443837165833, acc: 0.7037037014961243)
[2024-12-12 02:27:00,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:00,232][root][INFO] - Training Epoch: 2/10, step 429/574 completed (loss: 1.3482118844985962, acc: 0.5652173757553101)
[2024-12-12 02:27:00,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:00,513][root][INFO] - Training Epoch: 2/10, step 430/574 completed (loss: 0.4930065870285034, acc: 0.8518518805503845)
[2024-12-12 02:27:00,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:00,849][root][INFO] - Training Epoch: 2/10, step 431/574 completed (loss: 0.476119726896286, acc: 0.8888888955116272)
[2024-12-12 02:27:00,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:01,207][root][INFO] - Training Epoch: 2/10, step 432/574 completed (loss: 0.9931975603103638, acc: 0.6521739363670349)
[2024-12-12 02:27:01,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:01,569][root][INFO] - Training Epoch: 2/10, step 433/574 completed (loss: 1.4293149709701538, acc: 0.5833333134651184)
[2024-12-12 02:27:01,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:01,929][root][INFO] - Training Epoch: 2/10, step 434/574 completed (loss: 0.5981038808822632, acc: 0.7599999904632568)
[2024-12-12 02:27:02,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:02,288][root][INFO] - Training Epoch: 2/10, step 435/574 completed (loss: 1.3471356630325317, acc: 0.6969696879386902)
[2024-12-12 02:27:02,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:02,622][root][INFO] - Training Epoch: 2/10, step 436/574 completed (loss: 1.5043901205062866, acc: 0.6388888955116272)
[2024-12-12 02:27:02,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:02,951][root][INFO] - Training Epoch: 2/10, step 437/574 completed (loss: 1.3752561807632446, acc: 0.6818181872367859)
[2024-12-12 02:27:03,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:03,243][root][INFO] - Training Epoch: 2/10, step 438/574 completed (loss: 0.12434031069278717, acc: 1.0)
[2024-12-12 02:27:03,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:03,538][root][INFO] - Training Epoch: 2/10, step 439/574 completed (loss: 2.084935188293457, acc: 0.4615384638309479)
[2024-12-12 02:27:03,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:04,000][root][INFO] - Training Epoch: 2/10, step 440/574 completed (loss: 2.1344189643859863, acc: 0.4848484992980957)
[2024-12-12 02:27:04,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:04,727][root][INFO] - Training Epoch: 2/10, step 441/574 completed (loss: 2.562481641769409, acc: 0.29600000381469727)
[2024-12-12 02:27:04,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:05,121][root][INFO] - Training Epoch: 2/10, step 442/574 completed (loss: 2.2982006072998047, acc: 0.3709677457809448)
[2024-12-12 02:27:05,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:05,773][root][INFO] - Training Epoch: 2/10, step 443/574 completed (loss: 2.2754902839660645, acc: 0.38805970549583435)
[2024-12-12 02:27:05,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:06,118][root][INFO] - Training Epoch: 2/10, step 444/574 completed (loss: 2.013366937637329, acc: 0.4716981053352356)
[2024-12-12 02:27:06,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:06,548][root][INFO] - Training Epoch: 2/10, step 445/574 completed (loss: 1.1523994207382202, acc: 0.7045454382896423)
[2024-12-12 02:27:06,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:06,926][root][INFO] - Training Epoch: 2/10, step 446/574 completed (loss: 1.436848521232605, acc: 0.739130437374115)
[2024-12-12 02:27:07,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:07,248][root][INFO] - Training Epoch: 2/10, step 447/574 completed (loss: 1.4368884563446045, acc: 0.5769230723381042)
[2024-12-12 02:27:07,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:07,586][root][INFO] - Training Epoch: 2/10, step 448/574 completed (loss: 1.0178985595703125, acc: 0.75)
[2024-12-12 02:27:07,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:07,902][root][INFO] - Training Epoch: 2/10, step 449/574 completed (loss: 2.1029014587402344, acc: 0.46268656849861145)
[2024-12-12 02:27:07,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:08,208][root][INFO] - Training Epoch: 2/10, step 450/574 completed (loss: 1.8465781211853027, acc: 0.5138888955116272)
[2024-12-12 02:27:08,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:08,570][root][INFO] - Training Epoch: 2/10, step 451/574 completed (loss: 1.984919786453247, acc: 0.43478259444236755)
[2024-12-12 02:27:08,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:08,948][root][INFO] - Training Epoch: 2/10, step 452/574 completed (loss: 2.125710964202881, acc: 0.43589743971824646)
[2024-12-12 02:27:09,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:09,282][root][INFO] - Training Epoch: 2/10, step 453/574 completed (loss: 2.299738645553589, acc: 0.44736841320991516)
[2024-12-12 02:27:09,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:09,575][root][INFO] - Training Epoch: 2/10, step 454/574 completed (loss: 1.8573700189590454, acc: 0.5102040767669678)
[2024-12-12 02:27:09,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:09,850][root][INFO] - Training Epoch: 2/10, step 455/574 completed (loss: 1.436842918395996, acc: 0.6363636255264282)
[2024-12-12 02:27:09,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:10,148][root][INFO] - Training Epoch: 2/10, step 456/574 completed (loss: 2.182011604309082, acc: 0.3814432919025421)
[2024-12-12 02:27:10,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:10,479][root][INFO] - Training Epoch: 2/10, step 457/574 completed (loss: 1.8907712697982788, acc: 0.5142857432365417)
[2024-12-12 02:27:10,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:10,851][root][INFO] - Training Epoch: 2/10, step 458/574 completed (loss: 2.004417657852173, acc: 0.4767441749572754)
[2024-12-12 02:27:10,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:11,178][root][INFO] - Training Epoch: 2/10, step 459/574 completed (loss: 2.446669816970825, acc: 0.375)
[2024-12-12 02:27:11,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:11,556][root][INFO] - Training Epoch: 2/10, step 460/574 completed (loss: 2.055359363555908, acc: 0.40740740299224854)
[2024-12-12 02:27:11,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:11,920][root][INFO] - Training Epoch: 2/10, step 461/574 completed (loss: 1.47649347782135, acc: 0.5277777910232544)
[2024-12-12 02:27:12,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:12,263][root][INFO] - Training Epoch: 2/10, step 462/574 completed (loss: 1.4048582315444946, acc: 0.625)
[2024-12-12 02:27:12,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:12,650][root][INFO] - Training Epoch: 2/10, step 463/574 completed (loss: 1.4422348737716675, acc: 0.6153846383094788)
[2024-12-12 02:27:12,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:13,053][root][INFO] - Training Epoch: 2/10, step 464/574 completed (loss: 1.9910451173782349, acc: 0.52173912525177)
[2024-12-12 02:27:13,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:13,419][root][INFO] - Training Epoch: 2/10, step 465/574 completed (loss: 2.0305683612823486, acc: 0.3571428656578064)
[2024-12-12 02:27:13,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:13,778][root][INFO] - Training Epoch: 2/10, step 466/574 completed (loss: 2.1107726097106934, acc: 0.4337349534034729)
[2024-12-12 02:27:13,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:14,133][root][INFO] - Training Epoch: 2/10, step 467/574 completed (loss: 1.8682454824447632, acc: 0.45045045018196106)
[2024-12-12 02:27:14,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:14,453][root][INFO] - Training Epoch: 2/10, step 468/574 completed (loss: 1.9280250072479248, acc: 0.5145630836486816)
[2024-12-12 02:27:14,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:14,785][root][INFO] - Training Epoch: 2/10, step 469/574 completed (loss: 1.7113524675369263, acc: 0.5609756112098694)
[2024-12-12 02:27:14,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:15,071][root][INFO] - Training Epoch: 2/10, step 470/574 completed (loss: 1.3251729011535645, acc: 0.7083333134651184)
[2024-12-12 02:27:15,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:15,422][root][INFO] - Training Epoch: 2/10, step 471/574 completed (loss: 2.3106002807617188, acc: 0.3571428656578064)
[2024-12-12 02:27:15,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:15,851][root][INFO] - Training Epoch: 2/10, step 472/574 completed (loss: 2.232475519180298, acc: 0.3529411852359772)
[2024-12-12 02:27:15,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:16,297][root][INFO] - Training Epoch: 2/10, step 473/574 completed (loss: 2.4017016887664795, acc: 0.35807859897613525)
[2024-12-12 02:27:16,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:16,667][root][INFO] - Training Epoch: 2/10, step 474/574 completed (loss: 2.260852098464966, acc: 0.3541666567325592)
[2024-12-12 02:27:16,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:17,021][root][INFO] - Training Epoch: 2/10, step 475/574 completed (loss: 2.226367473602295, acc: 0.3558282256126404)
[2024-12-12 02:27:17,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:17,407][root][INFO] - Training Epoch: 2/10, step 476/574 completed (loss: 2.260491132736206, acc: 0.38129496574401855)
[2024-12-12 02:27:17,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:17,755][root][INFO] - Training Epoch: 2/10, step 477/574 completed (loss: 2.1927425861358643, acc: 0.42211055755615234)
[2024-12-12 02:27:17,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:18,116][root][INFO] - Training Epoch: 2/10, step 478/574 completed (loss: 1.367443323135376, acc: 0.6666666865348816)
[2024-12-12 02:27:18,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:18,484][root][INFO] - Training Epoch: 2/10, step 479/574 completed (loss: 1.4240617752075195, acc: 0.6363636255264282)
[2024-12-12 02:27:18,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:18,785][root][INFO] - Training Epoch: 2/10, step 480/574 completed (loss: 1.2654054164886475, acc: 0.6666666865348816)
[2024-12-12 02:27:18,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:19,092][root][INFO] - Training Epoch: 2/10, step 481/574 completed (loss: 1.5852186679840088, acc: 0.550000011920929)
[2024-12-12 02:27:19,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:19,392][root][INFO] - Training Epoch: 2/10, step 482/574 completed (loss: 0.7916072607040405, acc: 0.8500000238418579)
[2024-12-12 02:27:19,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:19,780][root][INFO] - Training Epoch: 2/10, step 483/574 completed (loss: 1.5559210777282715, acc: 0.5344827771186829)
[2024-12-12 02:27:19,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:20,150][root][INFO] - Training Epoch: 2/10, step 484/574 completed (loss: 1.2317143678665161, acc: 0.7419354915618896)
[2024-12-12 02:27:20,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:20,464][root][INFO] - Training Epoch: 2/10, step 485/574 completed (loss: 0.8541831970214844, acc: 0.7894737124443054)
[2024-12-12 02:27:20,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:20,836][root][INFO] - Training Epoch: 2/10, step 486/574 completed (loss: 1.8342299461364746, acc: 0.5555555820465088)
[2024-12-12 02:27:20,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:21,214][root][INFO] - Training Epoch: 2/10, step 487/574 completed (loss: 2.046708345413208, acc: 0.4761904776096344)
[2024-12-12 02:27:21,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:21,565][root][INFO] - Training Epoch: 2/10, step 488/574 completed (loss: 1.239390254020691, acc: 0.5909090638160706)
[2024-12-12 02:27:21,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:21,950][root][INFO] - Training Epoch: 2/10, step 489/574 completed (loss: 1.9716830253601074, acc: 0.4769230782985687)
[2024-12-12 02:27:22,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:22,325][root][INFO] - Training Epoch: 2/10, step 490/574 completed (loss: 1.3655673265457153, acc: 0.699999988079071)
[2024-12-12 02:27:22,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:22,699][root][INFO] - Training Epoch: 2/10, step 491/574 completed (loss: 1.4184391498565674, acc: 0.6206896305084229)
[2024-12-12 02:27:22,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:23,034][root][INFO] - Training Epoch: 2/10, step 492/574 completed (loss: 1.7610331773757935, acc: 0.5686274766921997)
[2024-12-12 02:27:23,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:23,323][root][INFO] - Training Epoch: 2/10, step 493/574 completed (loss: 1.5331361293792725, acc: 0.5517241358757019)
[2024-12-12 02:27:23,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:23,625][root][INFO] - Training Epoch: 2/10, step 494/574 completed (loss: 0.809889018535614, acc: 0.8421052694320679)
[2024-12-12 02:27:23,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:23,997][root][INFO] - Training Epoch: 2/10, step 495/574 completed (loss: 2.329643726348877, acc: 0.42105263471603394)
[2024-12-12 02:27:24,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:24,397][root][INFO] - Training Epoch: 2/10, step 496/574 completed (loss: 1.9842008352279663, acc: 0.4910714328289032)
[2024-12-12 02:27:24,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:24,770][root][INFO] - Training Epoch: 2/10, step 497/574 completed (loss: 1.9334626197814941, acc: 0.483146071434021)
[2024-12-12 02:27:24,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:25,068][root][INFO] - Training Epoch: 2/10, step 498/574 completed (loss: 2.2544522285461426, acc: 0.40449437499046326)
[2024-12-12 02:27:25,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:25,433][root][INFO] - Training Epoch: 2/10, step 499/574 completed (loss: 2.3745532035827637, acc: 0.3617021143436432)
[2024-12-12 02:27:25,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:25,823][root][INFO] - Training Epoch: 2/10, step 500/574 completed (loss: 2.3794634342193604, acc: 0.3804347813129425)
[2024-12-12 02:27:25,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:26,150][root][INFO] - Training Epoch: 2/10, step 501/574 completed (loss: 0.5196908116340637, acc: 0.8399999737739563)
[2024-12-12 02:27:26,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:26,468][root][INFO] - Training Epoch: 2/10, step 502/574 completed (loss: 0.7605260610580444, acc: 0.7692307829856873)
[2024-12-12 02:27:26,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:26,856][root][INFO] - Training Epoch: 2/10, step 503/574 completed (loss: 0.6164757013320923, acc: 0.7407407164573669)
[2024-12-12 02:27:26,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:27,188][root][INFO] - Training Epoch: 2/10, step 504/574 completed (loss: 1.944374680519104, acc: 0.48148149251937866)
[2024-12-12 02:27:27,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:27,497][root][INFO] - Training Epoch: 2/10, step 505/574 completed (loss: 1.5202468633651733, acc: 0.5849056839942932)
[2024-12-12 02:27:27,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:27,879][root][INFO] - Training Epoch: 2/10, step 506/574 completed (loss: 1.2814431190490723, acc: 0.5862069129943848)
[2024-12-12 02:27:28,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:28,482][root][INFO] - Training Epoch: 2/10, step 507/574 completed (loss: 2.0638999938964844, acc: 0.45945945382118225)
[2024-12-12 02:27:28,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:28,919][root][INFO] - Training Epoch: 2/10, step 508/574 completed (loss: 1.8100392818450928, acc: 0.5070422291755676)
[2024-12-12 02:27:28,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:29,247][root][INFO] - Training Epoch: 2/10, step 509/574 completed (loss: 0.30165064334869385, acc: 0.949999988079071)
[2024-12-12 02:27:29,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:29,591][root][INFO] - Training Epoch: 2/10, step 510/574 completed (loss: 0.48978152871131897, acc: 0.8999999761581421)
[2024-12-12 02:27:29,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:29,882][root][INFO] - Training Epoch: 2/10, step 511/574 completed (loss: 0.9491721391677856, acc: 0.7692307829856873)
[2024-12-12 02:27:31,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:32,572][root][INFO] - Training Epoch: 2/10, step 512/574 completed (loss: 2.032886505126953, acc: 0.5071428418159485)
[2024-12-12 02:27:32,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:33,335][root][INFO] - Training Epoch: 2/10, step 513/574 completed (loss: 1.9609516859054565, acc: 0.5)
[2024-12-12 02:27:33,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:33,660][root][INFO] - Training Epoch: 2/10, step 514/574 completed (loss: 1.5045162439346313, acc: 0.6071428656578064)
[2024-12-12 02:27:33,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:34,050][root][INFO] - Training Epoch: 2/10, step 515/574 completed (loss: 1.4939308166503906, acc: 0.6000000238418579)
[2024-12-12 02:27:34,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:34,745][root][INFO] - Training Epoch: 2/10, step 516/574 completed (loss: 1.7703304290771484, acc: 0.5972222089767456)
[2024-12-12 02:27:34,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:35,110][root][INFO] - Training Epoch: 2/10, step 517/574 completed (loss: 0.42734527587890625, acc: 0.8846153616905212)
[2024-12-12 02:27:35,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:35,467][root][INFO] - Training Epoch: 2/10, step 518/574 completed (loss: 1.35150146484375, acc: 0.5806451439857483)
[2024-12-12 02:27:35,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:35,837][root][INFO] - Training Epoch: 2/10, step 519/574 completed (loss: 1.9611963033676147, acc: 0.6499999761581421)
[2024-12-12 02:27:35,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:36,140][root][INFO] - Training Epoch: 2/10, step 520/574 completed (loss: 1.9705876111984253, acc: 0.5925925970077515)
[2024-12-12 02:27:36,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:37,118][root][INFO] - Training Epoch: 2/10, step 521/574 completed (loss: 2.140669822692871, acc: 0.4449152648448944)
[2024-12-12 02:27:37,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:37,528][root][INFO] - Training Epoch: 2/10, step 522/574 completed (loss: 2.257026433944702, acc: 0.41044774651527405)
[2024-12-12 02:27:37,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:37,889][root][INFO] - Training Epoch: 2/10, step 523/574 completed (loss: 2.1804683208465576, acc: 0.39416059851646423)
[2024-12-12 02:27:38,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:38,457][root][INFO] - Training Epoch: 2/10, step 524/574 completed (loss: 1.970036268234253, acc: 0.46000000834465027)
[2024-12-12 02:27:38,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:38,756][root][INFO] - Training Epoch: 2/10, step 525/574 completed (loss: 1.902320384979248, acc: 0.48148149251937866)
[2024-12-12 02:27:38,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:39,054][root][INFO] - Training Epoch: 2/10, step 526/574 completed (loss: 1.7476400136947632, acc: 0.5384615659713745)
[2024-12-12 02:27:39,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:39,341][root][INFO] - Training Epoch: 2/10, step 527/574 completed (loss: 2.3592143058776855, acc: 0.2380952388048172)
[2024-12-12 02:27:39,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:39,681][root][INFO] - Training Epoch: 2/10, step 528/574 completed (loss: 2.903668165206909, acc: 0.2295081913471222)
[2024-12-12 02:27:39,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:40,017][root][INFO] - Training Epoch: 2/10, step 529/574 completed (loss: 1.7826358079910278, acc: 0.5423728823661804)
[2024-12-12 02:27:40,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:40,388][root][INFO] - Training Epoch: 2/10, step 530/574 completed (loss: 2.3895719051361084, acc: 0.41860464215278625)
[2024-12-12 02:27:40,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:40,722][root][INFO] - Training Epoch: 2/10, step 531/574 completed (loss: 2.058955192565918, acc: 0.47727271914482117)
[2024-12-12 02:27:40,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:41,044][root][INFO] - Training Epoch: 2/10, step 532/574 completed (loss: 2.327373743057251, acc: 0.3962264060974121)
[2024-12-12 02:27:41,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:41,325][root][INFO] - Training Epoch: 2/10, step 533/574 completed (loss: 1.7584056854248047, acc: 0.6136363744735718)
[2024-12-12 02:27:41,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:41,629][root][INFO] - Training Epoch: 2/10, step 534/574 completed (loss: 1.4624055624008179, acc: 0.6800000071525574)
[2024-12-12 02:27:41,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:42,014][root][INFO] - Training Epoch: 2/10, step 535/574 completed (loss: 1.475237250328064, acc: 0.6000000238418579)
[2024-12-12 02:27:42,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:42,410][root][INFO] - Training Epoch: 2/10, step 536/574 completed (loss: 1.242286205291748, acc: 0.6363636255264282)
[2024-12-12 02:27:42,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:42,804][root][INFO] - Training Epoch: 2/10, step 537/574 completed (loss: 1.81120765209198, acc: 0.5230769515037537)
[2024-12-12 02:27:42,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:43,127][root][INFO] - Training Epoch: 2/10, step 538/574 completed (loss: 1.6637921333312988, acc: 0.59375)
[2024-12-12 02:27:43,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:43,533][root][INFO] - Training Epoch: 2/10, step 539/574 completed (loss: 1.339143991470337, acc: 0.625)
[2024-12-12 02:27:43,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:43,861][root][INFO] - Training Epoch: 2/10, step 540/574 completed (loss: 1.6202789545059204, acc: 0.6363636255264282)
[2024-12-12 02:27:43,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:44,211][root][INFO] - Training Epoch: 2/10, step 541/574 completed (loss: 0.7183718085289001, acc: 0.8125)
[2024-12-12 02:27:44,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:44,560][root][INFO] - Training Epoch: 2/10, step 542/574 completed (loss: 0.8044835329055786, acc: 0.7419354915618896)
[2024-12-12 02:27:44,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:44,933][root][INFO] - Training Epoch: 2/10, step 543/574 completed (loss: 0.3539432883262634, acc: 0.95652174949646)
[2024-12-12 02:27:45,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:45,264][root][INFO] - Training Epoch: 2/10, step 544/574 completed (loss: 1.8988174200057983, acc: 0.5)
[2024-12-12 02:27:45,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:45,633][root][INFO] - Training Epoch: 2/10, step 545/574 completed (loss: 1.3485679626464844, acc: 0.5853658318519592)
[2024-12-12 02:27:45,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:45,971][root][INFO] - Training Epoch: 2/10, step 546/574 completed (loss: 0.8803689479827881, acc: 0.7428571581840515)
[2024-12-12 02:27:46,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:46,307][root][INFO] - Training Epoch: 2/10, step 547/574 completed (loss: 1.2343778610229492, acc: 0.6578947305679321)
[2024-12-12 02:27:46,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:46,649][root][INFO] - Training Epoch: 2/10, step 548/574 completed (loss: 1.43252694606781, acc: 0.6129032373428345)
[2024-12-12 02:27:46,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:47,038][root][INFO] - Training Epoch: 2/10, step 549/574 completed (loss: 0.41392093896865845, acc: 0.9200000166893005)
[2024-12-12 02:27:47,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:47,408][root][INFO] - Training Epoch: 2/10, step 550/574 completed (loss: 1.1737825870513916, acc: 0.5757575631141663)
[2024-12-12 02:27:47,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:47,825][root][INFO] - Training Epoch: 2/10, step 551/574 completed (loss: 0.971373438835144, acc: 0.75)
[2024-12-12 02:27:47,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:48,196][root][INFO] - Training Epoch: 2/10, step 552/574 completed (loss: 1.2968292236328125, acc: 0.6142857074737549)
[2024-12-12 02:27:48,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:48,595][root][INFO] - Training Epoch: 2/10, step 553/574 completed (loss: 2.288954496383667, acc: 0.38686132431030273)
[2024-12-12 02:27:48,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:48,993][root][INFO] - Training Epoch: 2/10, step 554/574 completed (loss: 1.8630019426345825, acc: 0.5103448033332825)
[2024-12-12 02:27:49,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:49,366][root][INFO] - Training Epoch: 2/10, step 555/574 completed (loss: 2.6421525478363037, acc: 0.33571428060531616)
[2024-12-12 02:27:49,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:49,757][root][INFO] - Training Epoch: 2/10, step 556/574 completed (loss: 2.414072036743164, acc: 0.34437087178230286)
[2024-12-12 02:27:49,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:50,131][root][INFO] - Training Epoch: 2/10, step 557/574 completed (loss: 1.9348382949829102, acc: 0.49572649598121643)
[2024-12-12 02:27:50,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:50,473][root][INFO] - Training Epoch: 2/10, step 558/574 completed (loss: 0.48204272985458374, acc: 0.8799999952316284)
[2024-12-12 02:27:50,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:50,814][root][INFO] - Training Epoch: 2/10, step 559/574 completed (loss: 1.278954267501831, acc: 0.6153846383094788)
[2024-12-12 02:27:50,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:51,192][root][INFO] - Training Epoch: 2/10, step 560/574 completed (loss: 0.6148694157600403, acc: 0.7692307829856873)
[2024-12-12 02:27:51,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:51,600][root][INFO] - Training Epoch: 2/10, step 561/574 completed (loss: 1.60515296459198, acc: 0.6153846383094788)
[2024-12-12 02:27:51,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:51,964][root][INFO] - Training Epoch: 2/10, step 562/574 completed (loss: 1.648029088973999, acc: 0.5111111402511597)
[2024-12-12 02:27:52,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:52,307][root][INFO] - Training Epoch: 2/10, step 563/574 completed (loss: 1.6637781858444214, acc: 0.5324675440788269)
[2024-12-12 02:27:52,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:52,672][root][INFO] - Training Epoch: 2/10, step 564/574 completed (loss: 1.7223249673843384, acc: 0.5416666865348816)
[2024-12-12 02:27:52,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:53,025][root][INFO] - Training Epoch: 2/10, step 565/574 completed (loss: 1.7016147375106812, acc: 0.517241358757019)
[2024-12-12 02:27:53,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:53,384][root][INFO] - Training Epoch: 2/10, step 566/574 completed (loss: 1.6502994298934937, acc: 0.5476190447807312)
[2024-12-12 02:27:53,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:53,754][root][INFO] - Training Epoch: 2/10, step 567/574 completed (loss: 1.6015743017196655, acc: 0.5789473652839661)
[2024-12-12 02:27:53,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:54,123][root][INFO] - Training Epoch: 2/10, step 568/574 completed (loss: 1.1510066986083984, acc: 0.6666666865348816)
[2024-12-12 02:27:54,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:54,545][root][INFO] - Training Epoch: 2/10, step 569/574 completed (loss: 1.9835429191589355, acc: 0.48128342628479004)
[2024-12-12 02:27:55,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:55,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:56,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:56,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:56,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:57,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:57,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:57,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:58,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:58,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:58,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:59,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:27:59,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:00,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:00,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:00,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:01,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:01,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:01,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:01,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:02,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:02,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:02,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:03,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:03,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:03,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:04,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:04,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:05,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:05,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:05,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:05,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:06,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:06,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:07,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:07,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:07,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:08,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:08,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:08,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:09,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:09,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:09,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:10,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:10,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:11,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:11,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:11,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:11,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:12,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:12,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:12,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:13,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:13,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:13,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:14,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:14,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:15,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:15,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:15,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:16,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:16,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:17,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:17,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:17,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:18,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:18,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:18,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:19,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:19,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:19,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:20,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:20,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:20,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:21,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:21,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:22,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:22,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:22,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:23,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:23,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:23,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:24,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:24,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:24,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:25,318][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(5.9561, device='cuda:0') eval_epoch_loss=tensor(1.7844, device='cuda:0') eval_epoch_acc=tensor(0.5351, device='cuda:0')
[2024-12-12 02:28:25,319][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:28:25,319][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:28:25,616][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_2_step_570_loss_1.7844241857528687/model.pt
[2024-12-12 02:28:25,623][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:28:25,624][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 1.7844241857528687
[2024-12-12 02:28:25,625][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.5351365804672241
[2024-12-12 02:28:25,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:26,049][root][INFO] - Training Epoch: 2/10, step 570/574 completed (loss: 1.3564467430114746, acc: 0.6774193644523621)
[2024-12-12 02:28:26,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:26,435][root][INFO] - Training Epoch: 2/10, step 571/574 completed (loss: 1.9362527132034302, acc: 0.49572649598121643)
[2024-12-12 02:28:26,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:26,754][root][INFO] - Training Epoch: 2/10, step 572/574 completed (loss: 2.301839828491211, acc: 0.3877550959587097)
[2024-12-12 02:28:26,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:27,090][root][INFO] - Training Epoch: 2/10, step 573/574 completed (loss: 2.192197561264038, acc: 0.4150943458080292)
[2024-12-12 02:28:27,473][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=5.7987, train_epoch_loss=1.7576, epoch time 355.32345424965024s
[2024-12-12 02:28:27,473][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-12-12 02:28:27,474][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2024-12-12 02:28:27,474][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-12-12 02:28:27,474][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2024-12-12 02:28:27,474][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:28:28,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:28,332][root][INFO] - Training Epoch: 3/10, step 0/574 completed (loss: 1.351304531097412, acc: 0.5925925970077515)
[2024-12-12 02:28:28,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:28,688][root][INFO] - Training Epoch: 3/10, step 1/574 completed (loss: 1.6194730997085571, acc: 0.6399999856948853)
[2024-12-12 02:28:28,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:29,053][root][INFO] - Training Epoch: 3/10, step 2/574 completed (loss: 2.3447470664978027, acc: 0.4324324429035187)
[2024-12-12 02:28:29,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:29,441][root][INFO] - Training Epoch: 3/10, step 3/574 completed (loss: 1.9904180765151978, acc: 0.42105263471603394)
[2024-12-12 02:28:29,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:29,819][root][INFO] - Training Epoch: 3/10, step 4/574 completed (loss: 1.9465888738632202, acc: 0.4324324429035187)
[2024-12-12 02:28:29,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:30,174][root][INFO] - Training Epoch: 3/10, step 5/574 completed (loss: 1.6992876529693604, acc: 0.5)
[2024-12-12 02:28:30,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:30,475][root][INFO] - Training Epoch: 3/10, step 6/574 completed (loss: 2.2735815048217773, acc: 0.4693877696990967)
[2024-12-12 02:28:30,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:30,787][root][INFO] - Training Epoch: 3/10, step 7/574 completed (loss: 1.5069634914398193, acc: 0.6000000238418579)
[2024-12-12 02:28:30,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:31,125][root][INFO] - Training Epoch: 3/10, step 8/574 completed (loss: 0.28610289096832275, acc: 0.9090909361839294)
[2024-12-12 02:28:31,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:31,429][root][INFO] - Training Epoch: 3/10, step 9/574 completed (loss: 0.4638058841228485, acc: 0.8846153616905212)
[2024-12-12 02:28:31,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:31,750][root][INFO] - Training Epoch: 3/10, step 10/574 completed (loss: 0.8059304356575012, acc: 0.7777777910232544)
[2024-12-12 02:28:31,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:32,107][root][INFO] - Training Epoch: 3/10, step 11/574 completed (loss: 1.4942126274108887, acc: 0.6410256624221802)
[2024-12-12 02:28:32,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:32,448][root][INFO] - Training Epoch: 3/10, step 12/574 completed (loss: 1.3873990774154663, acc: 0.5757575631141663)
[2024-12-12 02:28:32,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:32,770][root][INFO] - Training Epoch: 3/10, step 13/574 completed (loss: 1.5782225131988525, acc: 0.52173912525177)
[2024-12-12 02:28:32,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:33,127][root][INFO] - Training Epoch: 3/10, step 14/574 completed (loss: 2.001220464706421, acc: 0.529411792755127)
[2024-12-12 02:28:33,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:33,496][root][INFO] - Training Epoch: 3/10, step 15/574 completed (loss: 1.5820046663284302, acc: 0.5714285969734192)
[2024-12-12 02:28:33,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:33,861][root][INFO] - Training Epoch: 3/10, step 16/574 completed (loss: 0.7054739594459534, acc: 0.7894737124443054)
[2024-12-12 02:28:33,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:34,177][root][INFO] - Training Epoch: 3/10, step 17/574 completed (loss: 1.2466309070587158, acc: 0.5833333134651184)
[2024-12-12 02:28:34,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:34,538][root][INFO] - Training Epoch: 3/10, step 18/574 completed (loss: 1.9614211320877075, acc: 0.5277777910232544)
[2024-12-12 02:28:34,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:34,877][root][INFO] - Training Epoch: 3/10, step 19/574 completed (loss: 1.273403286933899, acc: 0.6842105388641357)
[2024-12-12 02:28:34,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:35,253][root][INFO] - Training Epoch: 3/10, step 20/574 completed (loss: 1.1980209350585938, acc: 0.6538461446762085)
[2024-12-12 02:28:35,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:35,600][root][INFO] - Training Epoch: 3/10, step 21/574 completed (loss: 1.4675052165985107, acc: 0.6206896305084229)
[2024-12-12 02:28:35,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:35,911][root][INFO] - Training Epoch: 3/10, step 22/574 completed (loss: 1.489383578300476, acc: 0.47999998927116394)
[2024-12-12 02:28:35,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:36,231][root][INFO] - Training Epoch: 3/10, step 23/574 completed (loss: 0.808678925037384, acc: 0.9047619104385376)
[2024-12-12 02:28:36,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:36,573][root][INFO] - Training Epoch: 3/10, step 24/574 completed (loss: 0.985428512096405, acc: 0.6875)
[2024-12-12 02:28:36,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:36,931][root][INFO] - Training Epoch: 3/10, step 25/574 completed (loss: 2.3134875297546387, acc: 0.3396226465702057)
[2024-12-12 02:28:37,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:37,315][root][INFO] - Training Epoch: 3/10, step 26/574 completed (loss: 2.1753041744232178, acc: 0.45205479860305786)
[2024-12-12 02:28:37,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:38,562][root][INFO] - Training Epoch: 3/10, step 27/574 completed (loss: 2.2006800174713135, acc: 0.4150197505950928)
[2024-12-12 02:28:38,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:38,862][root][INFO] - Training Epoch: 3/10, step 28/574 completed (loss: 1.8617037534713745, acc: 0.5116279125213623)
[2024-12-12 02:28:38,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:39,235][root][INFO] - Training Epoch: 3/10, step 29/574 completed (loss: 2.03446364402771, acc: 0.45783132314682007)
[2024-12-12 02:28:39,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:39,608][root][INFO] - Training Epoch: 3/10, step 30/574 completed (loss: 1.9590778350830078, acc: 0.4197530746459961)
[2024-12-12 02:28:39,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:39,955][root][INFO] - Training Epoch: 3/10, step 31/574 completed (loss: 2.129857063293457, acc: 0.3928571343421936)
[2024-12-12 02:28:40,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:40,289][root][INFO] - Training Epoch: 3/10, step 32/574 completed (loss: 1.2363591194152832, acc: 0.6666666865348816)
[2024-12-12 02:28:40,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:40,625][root][INFO] - Training Epoch: 3/10, step 33/574 completed (loss: 1.0498626232147217, acc: 0.6521739363670349)
[2024-12-12 02:28:40,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:41,010][root][INFO] - Training Epoch: 3/10, step 34/574 completed (loss: 2.0479888916015625, acc: 0.45378151535987854)
[2024-12-12 02:28:41,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:41,363][root][INFO] - Training Epoch: 3/10, step 35/574 completed (loss: 1.6624139547348022, acc: 0.5245901346206665)
[2024-12-12 02:28:41,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:41,774][root][INFO] - Training Epoch: 3/10, step 36/574 completed (loss: 1.9704951047897339, acc: 0.5079365372657776)
[2024-12-12 02:28:41,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:42,137][root][INFO] - Training Epoch: 3/10, step 37/574 completed (loss: 2.0170786380767822, acc: 0.4406779706478119)
[2024-12-12 02:28:42,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:42,532][root][INFO] - Training Epoch: 3/10, step 38/574 completed (loss: 1.4294819831848145, acc: 0.5977011322975159)
[2024-12-12 02:28:42,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:42,809][root][INFO] - Training Epoch: 3/10, step 39/574 completed (loss: 0.8588496446609497, acc: 0.761904776096344)
[2024-12-12 02:28:42,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:43,088][root][INFO] - Training Epoch: 3/10, step 40/574 completed (loss: 1.5335019826889038, acc: 0.5769230723381042)
[2024-12-12 02:28:43,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:43,453][root][INFO] - Training Epoch: 3/10, step 41/574 completed (loss: 2.4890799522399902, acc: 0.3513513505458832)
[2024-12-12 02:28:43,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:43,837][root][INFO] - Training Epoch: 3/10, step 42/574 completed (loss: 1.8784489631652832, acc: 0.5230769515037537)
[2024-12-12 02:28:43,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:44,260][root][INFO] - Training Epoch: 3/10, step 43/574 completed (loss: 2.1266798973083496, acc: 0.5656565427780151)
[2024-12-12 02:28:44,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:44,671][root][INFO] - Training Epoch: 3/10, step 44/574 completed (loss: 1.7239223718643188, acc: 0.5051546096801758)
[2024-12-12 02:28:44,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:45,084][root][INFO] - Training Epoch: 3/10, step 45/574 completed (loss: 2.0949878692626953, acc: 0.4779411852359772)
[2024-12-12 02:28:45,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:45,396][root][INFO] - Training Epoch: 3/10, step 46/574 completed (loss: 0.7142228484153748, acc: 0.8461538553237915)
[2024-12-12 02:28:45,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:45,756][root][INFO] - Training Epoch: 3/10, step 47/574 completed (loss: 0.5342656970024109, acc: 0.9259259104728699)
[2024-12-12 02:28:45,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:46,090][root][INFO] - Training Epoch: 3/10, step 48/574 completed (loss: 1.4306633472442627, acc: 0.6071428656578064)
[2024-12-12 02:28:46,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:46,476][root][INFO] - Training Epoch: 3/10, step 49/574 completed (loss: 0.827385663986206, acc: 0.7777777910232544)
[2024-12-12 02:28:46,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:46,883][root][INFO] - Training Epoch: 3/10, step 50/574 completed (loss: 1.2869277000427246, acc: 0.6315789222717285)
[2024-12-12 02:28:46,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:47,251][root][INFO] - Training Epoch: 3/10, step 51/574 completed (loss: 1.4043906927108765, acc: 0.682539701461792)
[2024-12-12 02:28:47,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:47,627][root][INFO] - Training Epoch: 3/10, step 52/574 completed (loss: 2.0057342052459717, acc: 0.4647887349128723)
[2024-12-12 02:28:47,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:48,073][root][INFO] - Training Epoch: 3/10, step 53/574 completed (loss: 2.325453281402588, acc: 0.4533333480358124)
[2024-12-12 02:28:48,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:48,390][root][INFO] - Training Epoch: 3/10, step 54/574 completed (loss: 1.1657894849777222, acc: 0.6486486196517944)
[2024-12-12 02:28:48,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:48,737][root][INFO] - Training Epoch: 3/10, step 55/574 completed (loss: 0.3558511435985565, acc: 0.8846153616905212)
[2024-12-12 02:28:50,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:51,764][root][INFO] - Training Epoch: 3/10, step 56/574 completed (loss: 1.8568859100341797, acc: 0.532423198223114)
[2024-12-12 02:28:52,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:53,111][root][INFO] - Training Epoch: 3/10, step 57/574 completed (loss: 2.440910816192627, acc: 0.38562092185020447)
[2024-12-12 02:28:53,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:53,736][root][INFO] - Training Epoch: 3/10, step 58/574 completed (loss: 1.9593491554260254, acc: 0.5170454382896423)
[2024-12-12 02:28:53,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:54,307][root][INFO] - Training Epoch: 3/10, step 59/574 completed (loss: 2.235095262527466, acc: 0.47058823704719543)
[2024-12-12 02:28:54,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:54,871][root][INFO] - Training Epoch: 3/10, step 60/574 completed (loss: 2.3366036415100098, acc: 0.36231884360313416)
[2024-12-12 02:28:54,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:55,277][root][INFO] - Training Epoch: 3/10, step 61/574 completed (loss: 1.6724815368652344, acc: 0.5874999761581421)
[2024-12-12 02:28:55,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:55,590][root][INFO] - Training Epoch: 3/10, step 62/574 completed (loss: 1.383832573890686, acc: 0.6764705777168274)
[2024-12-12 02:28:55,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:55,977][root][INFO] - Training Epoch: 3/10, step 63/574 completed (loss: 1.4189739227294922, acc: 0.6388888955116272)
[2024-12-12 02:28:56,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:56,330][root][INFO] - Training Epoch: 3/10, step 64/574 completed (loss: 1.578197717666626, acc: 0.59375)
[2024-12-12 02:28:56,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:56,727][root][INFO] - Training Epoch: 3/10, step 65/574 completed (loss: 0.6758238077163696, acc: 0.7931034564971924)
[2024-12-12 02:28:56,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:57,104][root][INFO] - Training Epoch: 3/10, step 66/574 completed (loss: 2.2296483516693115, acc: 0.4464285671710968)
[2024-12-12 02:28:57,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:57,491][root][INFO] - Training Epoch: 3/10, step 67/574 completed (loss: 2.005549907684326, acc: 0.4833333194255829)
[2024-12-12 02:28:57,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:57,837][root][INFO] - Training Epoch: 3/10, step 68/574 completed (loss: 0.5196205377578735, acc: 0.800000011920929)
[2024-12-12 02:28:57,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:58,153][root][INFO] - Training Epoch: 3/10, step 69/574 completed (loss: 1.1778281927108765, acc: 0.6388888955116272)
[2024-12-12 02:28:58,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:58,466][root][INFO] - Training Epoch: 3/10, step 70/574 completed (loss: 1.5163062810897827, acc: 0.6060606241226196)
[2024-12-12 02:28:58,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:58,825][root][INFO] - Training Epoch: 3/10, step 71/574 completed (loss: 1.9740725755691528, acc: 0.4779411852359772)
[2024-12-12 02:28:58,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:59,185][root][INFO] - Training Epoch: 3/10, step 72/574 completed (loss: 1.9254481792449951, acc: 0.4365079402923584)
[2024-12-12 02:28:59,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:59,568][root][INFO] - Training Epoch: 3/10, step 73/574 completed (loss: 2.221296787261963, acc: 0.3897435963153839)
[2024-12-12 02:28:59,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:28:59,974][root][INFO] - Training Epoch: 3/10, step 74/574 completed (loss: 1.8664129972457886, acc: 0.4693877696990967)
[2024-12-12 02:29:00,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:00,317][root][INFO] - Training Epoch: 3/10, step 75/574 completed (loss: 2.3794121742248535, acc: 0.31343284249305725)
[2024-12-12 02:29:00,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:00,701][root][INFO] - Training Epoch: 3/10, step 76/574 completed (loss: 2.1810173988342285, acc: 0.4197080433368683)
[2024-12-12 02:29:00,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:01,084][root][INFO] - Training Epoch: 3/10, step 77/574 completed (loss: 0.41019880771636963, acc: 0.8571428656578064)
[2024-12-12 02:29:01,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:01,464][root][INFO] - Training Epoch: 3/10, step 78/574 completed (loss: 0.429599791765213, acc: 0.9583333134651184)
[2024-12-12 02:29:01,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:01,860][root][INFO] - Training Epoch: 3/10, step 79/574 completed (loss: 1.0781282186508179, acc: 0.6969696879386902)
[2024-12-12 02:29:01,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:02,170][root][INFO] - Training Epoch: 3/10, step 80/574 completed (loss: 0.5599764585494995, acc: 0.8461538553237915)
[2024-12-12 02:29:02,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:02,493][root][INFO] - Training Epoch: 3/10, step 81/574 completed (loss: 1.6466208696365356, acc: 0.6346153616905212)
[2024-12-12 02:29:02,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:02,833][root][INFO] - Training Epoch: 3/10, step 82/574 completed (loss: 2.0291783809661865, acc: 0.5192307829856873)
[2024-12-12 02:29:02,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:03,230][root][INFO] - Training Epoch: 3/10, step 83/574 completed (loss: 1.362610101699829, acc: 0.53125)
[2024-12-12 02:29:03,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:03,619][root][INFO] - Training Epoch: 3/10, step 84/574 completed (loss: 1.9102540016174316, acc: 0.49275362491607666)
[2024-12-12 02:29:03,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:03,962][root][INFO] - Training Epoch: 3/10, step 85/574 completed (loss: 1.40049147605896, acc: 0.6000000238418579)
[2024-12-12 02:29:04,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:04,371][root][INFO] - Training Epoch: 3/10, step 86/574 completed (loss: 1.545142412185669, acc: 0.5652173757553101)
[2024-12-12 02:29:04,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:04,846][root][INFO] - Training Epoch: 3/10, step 87/574 completed (loss: 2.1649317741394043, acc: 0.4399999976158142)
[2024-12-12 02:29:04,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:05,184][root][INFO] - Training Epoch: 3/10, step 88/574 completed (loss: 1.9392871856689453, acc: 0.5436893105506897)
[2024-12-12 02:29:05,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:06,328][root][INFO] - Training Epoch: 3/10, step 89/574 completed (loss: 1.8496904373168945, acc: 0.5388349294662476)
[2024-12-12 02:29:06,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:07,159][root][INFO] - Training Epoch: 3/10, step 90/574 completed (loss: 2.0440468788146973, acc: 0.42473119497299194)
[2024-12-12 02:29:07,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:07,967][root][INFO] - Training Epoch: 3/10, step 91/574 completed (loss: 1.816591501235962, acc: 0.5301724076271057)
[2024-12-12 02:29:08,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:08,717][root][INFO] - Training Epoch: 3/10, step 92/574 completed (loss: 1.5029939413070679, acc: 0.5894736647605896)
[2024-12-12 02:29:08,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:09,716][root][INFO] - Training Epoch: 3/10, step 93/574 completed (loss: 2.264194965362549, acc: 0.31683167815208435)
[2024-12-12 02:29:09,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:10,084][root][INFO] - Training Epoch: 3/10, step 94/574 completed (loss: 2.240664482116699, acc: 0.4032258093357086)
[2024-12-12 02:29:10,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:10,412][root][INFO] - Training Epoch: 3/10, step 95/574 completed (loss: 2.2932424545288086, acc: 0.3478260934352875)
[2024-12-12 02:29:10,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:10,795][root][INFO] - Training Epoch: 3/10, step 96/574 completed (loss: 2.3477354049682617, acc: 0.29411765933036804)
[2024-12-12 02:29:10,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:11,160][root][INFO] - Training Epoch: 3/10, step 97/574 completed (loss: 2.404938220977783, acc: 0.26923078298568726)
[2024-12-12 02:29:11,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:11,565][root][INFO] - Training Epoch: 3/10, step 98/574 completed (loss: 2.320897340774536, acc: 0.40875911712646484)
[2024-12-12 02:29:11,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:11,927][root][INFO] - Training Epoch: 3/10, step 99/574 completed (loss: 2.285053014755249, acc: 0.3731343150138855)
[2024-12-12 02:29:11,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:12,223][root][INFO] - Training Epoch: 3/10, step 100/574 completed (loss: 0.842010498046875, acc: 0.800000011920929)
[2024-12-12 02:29:12,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:12,609][root][INFO] - Training Epoch: 3/10, step 101/574 completed (loss: 0.5300291180610657, acc: 0.8636363744735718)
[2024-12-12 02:29:12,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:13,014][root][INFO] - Training Epoch: 3/10, step 102/574 completed (loss: 0.707426130771637, acc: 0.8695651888847351)
[2024-12-12 02:29:13,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:13,362][root][INFO] - Training Epoch: 3/10, step 103/574 completed (loss: 1.2573927640914917, acc: 0.5909090638160706)
[2024-12-12 02:29:13,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:13,693][root][INFO] - Training Epoch: 3/10, step 104/574 completed (loss: 1.860655426979065, acc: 0.5344827771186829)
[2024-12-12 02:29:13,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:14,001][root][INFO] - Training Epoch: 3/10, step 105/574 completed (loss: 1.4725055694580078, acc: 0.6511628031730652)
[2024-12-12 02:29:14,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:14,317][root][INFO] - Training Epoch: 3/10, step 106/574 completed (loss: 1.0343308448791504, acc: 0.7200000286102295)
[2024-12-12 02:29:14,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:14,634][root][INFO] - Training Epoch: 3/10, step 107/574 completed (loss: 0.3113875091075897, acc: 0.8823529481887817)
[2024-12-12 02:29:14,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:14,909][root][INFO] - Training Epoch: 3/10, step 108/574 completed (loss: 0.27583277225494385, acc: 0.9230769276618958)
[2024-12-12 02:29:14,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:15,269][root][INFO] - Training Epoch: 3/10, step 109/574 completed (loss: 1.472144603729248, acc: 0.5952380895614624)
[2024-12-12 02:29:15,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:15,656][root][INFO] - Training Epoch: 3/10, step 110/574 completed (loss: 1.7846425771713257, acc: 0.5230769515037537)
[2024-12-12 02:29:15,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:16,047][root][INFO] - Training Epoch: 3/10, step 111/574 completed (loss: 1.722535252571106, acc: 0.5614035129547119)
[2024-12-12 02:29:16,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:16,431][root][INFO] - Training Epoch: 3/10, step 112/574 completed (loss: 1.560106873512268, acc: 0.5964912176132202)
[2024-12-12 02:29:16,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:16,764][root][INFO] - Training Epoch: 3/10, step 113/574 completed (loss: 1.9169628620147705, acc: 0.5384615659713745)
[2024-12-12 02:29:16,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:17,130][root][INFO] - Training Epoch: 3/10, step 114/574 completed (loss: 1.331787109375, acc: 0.6938775777816772)
[2024-12-12 02:29:17,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:17,507][root][INFO] - Training Epoch: 3/10, step 115/574 completed (loss: 0.21255312860012054, acc: 0.9545454382896423)
[2024-12-12 02:29:17,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:17,918][root][INFO] - Training Epoch: 3/10, step 116/574 completed (loss: 1.9499516487121582, acc: 0.4761904776096344)
[2024-12-12 02:29:18,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:18,257][root][INFO] - Training Epoch: 3/10, step 117/574 completed (loss: 1.9511334896087646, acc: 0.5609756112098694)
[2024-12-12 02:29:18,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:18,671][root][INFO] - Training Epoch: 3/10, step 118/574 completed (loss: 1.6010782718658447, acc: 0.5483871102333069)
[2024-12-12 02:29:18,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:19,591][root][INFO] - Training Epoch: 3/10, step 119/574 completed (loss: 2.0403592586517334, acc: 0.4752851724624634)
[2024-12-12 02:29:19,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:19,924][root][INFO] - Training Epoch: 3/10, step 120/574 completed (loss: 1.4307711124420166, acc: 0.6000000238418579)
[2024-12-12 02:29:20,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:20,322][root][INFO] - Training Epoch: 3/10, step 121/574 completed (loss: 1.4215959310531616, acc: 0.6538461446762085)
[2024-12-12 02:29:20,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:20,702][root][INFO] - Training Epoch: 3/10, step 122/574 completed (loss: 0.7197532653808594, acc: 0.875)
[2024-12-12 02:29:20,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:21,074][root][INFO] - Training Epoch: 3/10, step 123/574 completed (loss: 1.1808338165283203, acc: 0.7894737124443054)
[2024-12-12 02:29:21,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:21,491][root][INFO] - Training Epoch: 3/10, step 124/574 completed (loss: 2.1340794563293457, acc: 0.4171779155731201)
[2024-12-12 02:29:21,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:21,912][root][INFO] - Training Epoch: 3/10, step 125/574 completed (loss: 1.7611567974090576, acc: 0.5347222089767456)
[2024-12-12 02:29:22,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:22,276][root][INFO] - Training Epoch: 3/10, step 126/574 completed (loss: 2.244459867477417, acc: 0.38333332538604736)
[2024-12-12 02:29:22,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:22,598][root][INFO] - Training Epoch: 3/10, step 127/574 completed (loss: 2.0921437740325928, acc: 0.3988095223903656)
[2024-12-12 02:29:22,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:22,888][root][INFO] - Training Epoch: 3/10, step 128/574 completed (loss: 1.9481552839279175, acc: 0.4923076927661896)
[2024-12-12 02:29:22,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:23,308][root][INFO] - Training Epoch: 3/10, step 129/574 completed (loss: 1.8648886680603027, acc: 0.5)
[2024-12-12 02:29:23,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:23,660][root][INFO] - Training Epoch: 3/10, step 130/574 completed (loss: 0.9796819090843201, acc: 0.7307692170143127)
[2024-12-12 02:29:23,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:24,038][root][INFO] - Training Epoch: 3/10, step 131/574 completed (loss: 0.4114551246166229, acc: 0.8695651888847351)
[2024-12-12 02:29:24,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:24,363][root][INFO] - Training Epoch: 3/10, step 132/574 completed (loss: 1.3919973373413086, acc: 0.625)
[2024-12-12 02:29:24,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:24,718][root][INFO] - Training Epoch: 3/10, step 133/574 completed (loss: 1.6207427978515625, acc: 0.52173912525177)
[2024-12-12 02:29:24,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:25,072][root][INFO] - Training Epoch: 3/10, step 134/574 completed (loss: 1.2587171792984009, acc: 0.6285714507102966)
[2024-12-12 02:29:25,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:25,379][root][INFO] - Training Epoch: 3/10, step 135/574 completed (loss: 1.135480284690857, acc: 0.6153846383094788)
[2024-12-12 02:29:25,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:25,721][root][INFO] - Training Epoch: 3/10, step 136/574 completed (loss: 1.9045146703720093, acc: 0.5)
[2024-12-12 02:29:25,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:26,037][root][INFO] - Training Epoch: 3/10, step 137/574 completed (loss: 1.2318998575210571, acc: 0.6333333253860474)
[2024-12-12 02:29:26,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:26,394][root][INFO] - Training Epoch: 3/10, step 138/574 completed (loss: 1.2918776273727417, acc: 0.695652186870575)
[2024-12-12 02:29:27,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:27,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:27,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:28,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:28,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:29,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:29,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:29,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:30,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:30,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:30,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:31,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:31,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:32,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:32,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:32,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:33,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:33,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:33,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:34,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:34,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:35,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:35,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:35,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:36,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:36,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:36,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:37,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:37,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:37,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:38,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:38,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:38,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:39,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:39,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:40,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:40,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:40,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:41,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:41,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:41,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:42,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:42,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:42,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:43,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:43,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:43,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:44,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:44,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:44,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:44,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:45,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:45,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:45,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:46,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:46,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:46,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:47,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:47,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:47,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:48,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:48,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:48,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:49,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:49,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:49,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:50,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:50,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:51,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:51,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:51,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:52,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:52,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:52,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:53,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:53,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:53,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:54,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:54,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:54,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:55,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:55,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:55,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:56,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:56,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:57,028][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(5.8069, device='cuda:0') eval_epoch_loss=tensor(1.7591, device='cuda:0') eval_epoch_acc=tensor(0.5241, device='cuda:0')
[2024-12-12 02:29:57,029][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:29:57,029][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:29:57,254][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_3_step_139_loss_1.7590519189834595/model.pt
[2024-12-12 02:29:57,257][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:29:57,258][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 1.7590519189834595
[2024-12-12 02:29:57,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:57,579][root][INFO] - Training Epoch: 3/10, step 139/574 completed (loss: 1.6803526878356934, acc: 0.5714285969734192)
[2024-12-12 02:29:57,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:57,866][root][INFO] - Training Epoch: 3/10, step 140/574 completed (loss: 1.8159970045089722, acc: 0.4615384638309479)
[2024-12-12 02:29:57,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:58,214][root][INFO] - Training Epoch: 3/10, step 141/574 completed (loss: 2.5499041080474854, acc: 0.19354838132858276)
[2024-12-12 02:29:58,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:58,554][root][INFO] - Training Epoch: 3/10, step 142/574 completed (loss: 2.2330222129821777, acc: 0.4324324429035187)
[2024-12-12 02:29:58,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:59,077][root][INFO] - Training Epoch: 3/10, step 143/574 completed (loss: 1.9290862083435059, acc: 0.41228070855140686)
[2024-12-12 02:29:59,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:59,469][root][INFO] - Training Epoch: 3/10, step 144/574 completed (loss: 1.7620506286621094, acc: 0.5447761416435242)
[2024-12-12 02:29:59,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:29:59,881][root][INFO] - Training Epoch: 3/10, step 145/574 completed (loss: 2.3135621547698975, acc: 0.3469387888908386)
[2024-12-12 02:30:00,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:00,323][root][INFO] - Training Epoch: 3/10, step 146/574 completed (loss: 2.0550458431243896, acc: 0.38297873735427856)
[2024-12-12 02:30:00,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:00,689][root][INFO] - Training Epoch: 3/10, step 147/574 completed (loss: 1.6507444381713867, acc: 0.5428571701049805)
[2024-12-12 02:30:00,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:01,060][root][INFO] - Training Epoch: 3/10, step 148/574 completed (loss: 1.9983288049697876, acc: 0.5)
[2024-12-12 02:30:01,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:01,385][root][INFO] - Training Epoch: 3/10, step 149/574 completed (loss: 1.3055446147918701, acc: 0.6521739363670349)
[2024-12-12 02:30:01,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:01,782][root][INFO] - Training Epoch: 3/10, step 150/574 completed (loss: 1.7531405687332153, acc: 0.48275861144065857)
[2024-12-12 02:30:01,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:02,177][root][INFO] - Training Epoch: 3/10, step 151/574 completed (loss: 1.9319041967391968, acc: 0.54347825050354)
[2024-12-12 02:30:02,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:02,526][root][INFO] - Training Epoch: 3/10, step 152/574 completed (loss: 1.8338226079940796, acc: 0.49152541160583496)
[2024-12-12 02:30:02,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:02,883][root][INFO] - Training Epoch: 3/10, step 153/574 completed (loss: 2.207040786743164, acc: 0.4385964870452881)
[2024-12-12 02:30:02,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:03,226][root][INFO] - Training Epoch: 3/10, step 154/574 completed (loss: 1.8718527555465698, acc: 0.4864864945411682)
[2024-12-12 02:30:03,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:03,578][root][INFO] - Training Epoch: 3/10, step 155/574 completed (loss: 1.511840581893921, acc: 0.6785714030265808)
[2024-12-12 02:30:03,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:03,903][root][INFO] - Training Epoch: 3/10, step 156/574 completed (loss: 1.1436845064163208, acc: 0.739130437374115)
[2024-12-12 02:30:03,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:04,280][root][INFO] - Training Epoch: 3/10, step 157/574 completed (loss: 1.9350732564926147, acc: 0.42105263471603394)
[2024-12-12 02:30:04,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:05,939][root][INFO] - Training Epoch: 3/10, step 158/574 completed (loss: 1.5789521932601929, acc: 0.5270270109176636)
[2024-12-12 02:30:06,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:06,308][root][INFO] - Training Epoch: 3/10, step 159/574 completed (loss: 2.0438969135284424, acc: 0.42592594027519226)
[2024-12-12 02:30:06,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:06,734][root][INFO] - Training Epoch: 3/10, step 160/574 completed (loss: 1.7420361042022705, acc: 0.4883720874786377)
[2024-12-12 02:30:06,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:07,323][root][INFO] - Training Epoch: 3/10, step 161/574 completed (loss: 1.6243821382522583, acc: 0.5058823823928833)
[2024-12-12 02:30:07,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:07,879][root][INFO] - Training Epoch: 3/10, step 162/574 completed (loss: 2.082935333251953, acc: 0.4606741666793823)
[2024-12-12 02:30:07,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:08,183][root][INFO] - Training Epoch: 3/10, step 163/574 completed (loss: 1.6493544578552246, acc: 0.5681818127632141)
[2024-12-12 02:30:08,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:08,549][root][INFO] - Training Epoch: 3/10, step 164/574 completed (loss: 1.4843250513076782, acc: 0.5714285969734192)
[2024-12-12 02:30:08,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:08,827][root][INFO] - Training Epoch: 3/10, step 165/574 completed (loss: 1.4232592582702637, acc: 0.5862069129943848)
[2024-12-12 02:30:08,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:09,217][root][INFO] - Training Epoch: 3/10, step 166/574 completed (loss: 1.368262767791748, acc: 0.5102040767669678)
[2024-12-12 02:30:09,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:09,586][root][INFO] - Training Epoch: 3/10, step 167/574 completed (loss: 1.6666911840438843, acc: 0.5600000023841858)
[2024-12-12 02:30:09,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:10,011][root][INFO] - Training Epoch: 3/10, step 168/574 completed (loss: 1.615179181098938, acc: 0.5416666865348816)
[2024-12-12 02:30:10,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:10,351][root][INFO] - Training Epoch: 3/10, step 169/574 completed (loss: 1.830487608909607, acc: 0.5196078419685364)
[2024-12-12 02:30:10,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:11,382][root][INFO] - Training Epoch: 3/10, step 170/574 completed (loss: 2.2485511302948, acc: 0.45890411734580994)
[2024-12-12 02:30:11,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:11,733][root][INFO] - Training Epoch: 3/10, step 171/574 completed (loss: 0.8001001477241516, acc: 0.8333333134651184)
[2024-12-12 02:30:11,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:12,062][root][INFO] - Training Epoch: 3/10, step 172/574 completed (loss: 0.8849146962165833, acc: 0.8148148059844971)
[2024-12-12 02:30:12,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:12,423][root][INFO] - Training Epoch: 3/10, step 173/574 completed (loss: 1.2658361196517944, acc: 0.6785714030265808)
[2024-12-12 02:30:12,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:12,964][root][INFO] - Training Epoch: 3/10, step 174/574 completed (loss: 1.8571765422821045, acc: 0.5132743120193481)
[2024-12-12 02:30:13,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:13,279][root][INFO] - Training Epoch: 3/10, step 175/574 completed (loss: 1.6341310739517212, acc: 0.52173912525177)
[2024-12-12 02:30:13,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:13,646][root][INFO] - Training Epoch: 3/10, step 176/574 completed (loss: 1.6822088956832886, acc: 0.5681818127632141)
[2024-12-12 02:30:13,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:14,565][root][INFO] - Training Epoch: 3/10, step 177/574 completed (loss: 2.3789846897125244, acc: 0.3435114622116089)
[2024-12-12 02:30:14,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:15,243][root][INFO] - Training Epoch: 3/10, step 178/574 completed (loss: 2.12636137008667, acc: 0.40740740299224854)
[2024-12-12 02:30:15,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:15,573][root][INFO] - Training Epoch: 3/10, step 179/574 completed (loss: 1.4321649074554443, acc: 0.6065573692321777)
[2024-12-12 02:30:15,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:15,926][root][INFO] - Training Epoch: 3/10, step 180/574 completed (loss: 0.6703196167945862, acc: 0.7916666865348816)
[2024-12-12 02:30:16,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:16,285][root][INFO] - Training Epoch: 3/10, step 181/574 completed (loss: 0.9658440351486206, acc: 0.7200000286102295)
[2024-12-12 02:30:16,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:16,653][root][INFO] - Training Epoch: 3/10, step 182/574 completed (loss: 0.8493040204048157, acc: 0.75)
[2024-12-12 02:30:16,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:17,034][root][INFO] - Training Epoch: 3/10, step 183/574 completed (loss: 1.968156337738037, acc: 0.4390243887901306)
[2024-12-12 02:30:17,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:17,389][root][INFO] - Training Epoch: 3/10, step 184/574 completed (loss: 2.2451698780059814, acc: 0.395770400762558)
[2024-12-12 02:30:17,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:17,808][root][INFO] - Training Epoch: 3/10, step 185/574 completed (loss: 2.301243543624878, acc: 0.38616713881492615)
[2024-12-12 02:30:17,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:18,290][root][INFO] - Training Epoch: 3/10, step 186/574 completed (loss: 2.364846706390381, acc: 0.4000000059604645)
[2024-12-12 02:30:18,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:18,824][root][INFO] - Training Epoch: 3/10, step 187/574 completed (loss: 2.1731719970703125, acc: 0.3939962387084961)
[2024-12-12 02:30:18,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:19,221][root][INFO] - Training Epoch: 3/10, step 188/574 completed (loss: 2.137673854827881, acc: 0.4056939482688904)
[2024-12-12 02:30:19,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:19,610][root][INFO] - Training Epoch: 3/10, step 189/574 completed (loss: 1.9360299110412598, acc: 0.5199999809265137)
[2024-12-12 02:30:19,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:20,162][root][INFO] - Training Epoch: 3/10, step 190/574 completed (loss: 2.1439669132232666, acc: 0.45348837971687317)
[2024-12-12 02:30:20,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:20,960][root][INFO] - Training Epoch: 3/10, step 191/574 completed (loss: 1.8971102237701416, acc: 0.5396825671195984)
[2024-12-12 02:30:21,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:21,876][root][INFO] - Training Epoch: 3/10, step 192/574 completed (loss: 2.026808977127075, acc: 0.5)
[2024-12-12 02:30:22,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:22,628][root][INFO] - Training Epoch: 3/10, step 193/574 completed (loss: 1.7251965999603271, acc: 0.5764706134796143)
[2024-12-12 02:30:22,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:23,711][root][INFO] - Training Epoch: 3/10, step 194/574 completed (loss: 1.695426106452942, acc: 0.5679012537002563)
[2024-12-12 02:30:23,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:24,665][root][INFO] - Training Epoch: 3/10, step 195/574 completed (loss: 1.3408710956573486, acc: 0.6129032373428345)
[2024-12-12 02:30:24,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:24,990][root][INFO] - Training Epoch: 3/10, step 196/574 completed (loss: 0.6998905539512634, acc: 0.8571428656578064)
[2024-12-12 02:30:25,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:25,303][root][INFO] - Training Epoch: 3/10, step 197/574 completed (loss: 1.4688903093338013, acc: 0.625)
[2024-12-12 02:30:25,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:25,644][root][INFO] - Training Epoch: 3/10, step 198/574 completed (loss: 1.86672842502594, acc: 0.5441176295280457)
[2024-12-12 02:30:25,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:26,036][root][INFO] - Training Epoch: 3/10, step 199/574 completed (loss: 1.9773032665252686, acc: 0.4779411852359772)
[2024-12-12 02:30:26,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:26,411][root][INFO] - Training Epoch: 3/10, step 200/574 completed (loss: 2.2713611125946045, acc: 0.4237288236618042)
[2024-12-12 02:30:26,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:26,791][root][INFO] - Training Epoch: 3/10, step 201/574 completed (loss: 2.2289466857910156, acc: 0.4552238881587982)
[2024-12-12 02:30:26,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:27,194][root][INFO] - Training Epoch: 3/10, step 202/574 completed (loss: 2.163646936416626, acc: 0.43689319491386414)
[2024-12-12 02:30:27,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:27,544][root][INFO] - Training Epoch: 3/10, step 203/574 completed (loss: 1.7292178869247437, acc: 0.5079365372657776)
[2024-12-12 02:30:27,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:27,907][root][INFO] - Training Epoch: 3/10, step 204/574 completed (loss: 1.7842812538146973, acc: 0.5384615659713745)
[2024-12-12 02:30:28,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:28,261][root][INFO] - Training Epoch: 3/10, step 205/574 completed (loss: 2.084115982055664, acc: 0.42152467370033264)
[2024-12-12 02:30:28,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:28,666][root][INFO] - Training Epoch: 3/10, step 206/574 completed (loss: 2.028883457183838, acc: 0.4881889820098877)
[2024-12-12 02:30:28,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:29,062][root][INFO] - Training Epoch: 3/10, step 207/574 completed (loss: 1.9834179878234863, acc: 0.4655172526836395)
[2024-12-12 02:30:29,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:29,465][root][INFO] - Training Epoch: 3/10, step 208/574 completed (loss: 1.9701029062271118, acc: 0.48188406229019165)
[2024-12-12 02:30:29,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:29,861][root][INFO] - Training Epoch: 3/10, step 209/574 completed (loss: 2.150376081466675, acc: 0.43579766154289246)
[2024-12-12 02:30:29,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:30,250][root][INFO] - Training Epoch: 3/10, step 210/574 completed (loss: 2.123457908630371, acc: 0.42391303181648254)
[2024-12-12 02:30:30,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:30,570][root][INFO] - Training Epoch: 3/10, step 211/574 completed (loss: 0.9880719184875488, acc: 0.695652186870575)
[2024-12-12 02:30:30,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:30,914][root][INFO] - Training Epoch: 3/10, step 212/574 completed (loss: 1.3041112422943115, acc: 0.6428571343421936)
[2024-12-12 02:30:31,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:31,282][root][INFO] - Training Epoch: 3/10, step 213/574 completed (loss: 1.1217215061187744, acc: 0.7234042286872864)
[2024-12-12 02:30:31,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:31,964][root][INFO] - Training Epoch: 3/10, step 214/574 completed (loss: 1.6800521612167358, acc: 0.5769230723381042)
[2024-12-12 02:30:32,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:32,339][root][INFO] - Training Epoch: 3/10, step 215/574 completed (loss: 1.4169825315475464, acc: 0.6081081032752991)
[2024-12-12 02:30:32,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:32,722][root][INFO] - Training Epoch: 3/10, step 216/574 completed (loss: 1.4008318185806274, acc: 0.6162790656089783)
[2024-12-12 02:30:32,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:33,256][root][INFO] - Training Epoch: 3/10, step 217/574 completed (loss: 1.6548281908035278, acc: 0.5675675868988037)
[2024-12-12 02:30:33,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:33,688][root][INFO] - Training Epoch: 3/10, step 218/574 completed (loss: 1.5675715208053589, acc: 0.5222222208976746)
[2024-12-12 02:30:33,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:34,029][root][INFO] - Training Epoch: 3/10, step 219/574 completed (loss: 0.791843056678772, acc: 0.7575757503509521)
[2024-12-12 02:30:34,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:34,379][root][INFO] - Training Epoch: 3/10, step 220/574 completed (loss: 0.17759856581687927, acc: 0.9629629850387573)
[2024-12-12 02:30:34,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:34,761][root][INFO] - Training Epoch: 3/10, step 221/574 completed (loss: 0.38836535811424255, acc: 0.9200000166893005)
[2024-12-12 02:30:34,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:35,121][root][INFO] - Training Epoch: 3/10, step 222/574 completed (loss: 1.801646113395691, acc: 0.5)
[2024-12-12 02:30:35,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:35,888][root][INFO] - Training Epoch: 3/10, step 223/574 completed (loss: 1.5568242073059082, acc: 0.5760869383811951)
[2024-12-12 02:30:36,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:36,428][root][INFO] - Training Epoch: 3/10, step 224/574 completed (loss: 1.8350257873535156, acc: 0.4943181872367859)
[2024-12-12 02:30:36,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:36,865][root][INFO] - Training Epoch: 3/10, step 225/574 completed (loss: 2.094846725463867, acc: 0.40425533056259155)
[2024-12-12 02:30:36,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:37,242][root][INFO] - Training Epoch: 3/10, step 226/574 completed (loss: 1.3885554075241089, acc: 0.6037735939025879)
[2024-12-12 02:30:37,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:37,569][root][INFO] - Training Epoch: 3/10, step 227/574 completed (loss: 1.7417219877243042, acc: 0.5)
[2024-12-12 02:30:37,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:37,932][root][INFO] - Training Epoch: 3/10, step 228/574 completed (loss: 1.0850253105163574, acc: 0.7674418687820435)
[2024-12-12 02:30:38,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:38,320][root][INFO] - Training Epoch: 3/10, step 229/574 completed (loss: 1.0604721307754517, acc: 0.7333333492279053)
[2024-12-12 02:30:38,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:38,734][root][INFO] - Training Epoch: 3/10, step 230/574 completed (loss: 2.2761523723602295, acc: 0.4421052634716034)
[2024-12-12 02:30:38,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:39,087][root][INFO] - Training Epoch: 3/10, step 231/574 completed (loss: 1.7035807371139526, acc: 0.5888888835906982)
[2024-12-12 02:30:39,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:39,506][root][INFO] - Training Epoch: 3/10, step 232/574 completed (loss: 1.4665676355361938, acc: 0.5888888835906982)
[2024-12-12 02:30:39,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:39,994][root][INFO] - Training Epoch: 3/10, step 233/574 completed (loss: 1.6982014179229736, acc: 0.5963302850723267)
[2024-12-12 02:30:40,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:40,467][root][INFO] - Training Epoch: 3/10, step 234/574 completed (loss: 1.5411680936813354, acc: 0.5615384578704834)
[2024-12-12 02:30:40,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:40,764][root][INFO] - Training Epoch: 3/10, step 235/574 completed (loss: 0.8347501158714294, acc: 0.7894737124443054)
[2024-12-12 02:30:40,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:41,075][root][INFO] - Training Epoch: 3/10, step 236/574 completed (loss: 1.0921556949615479, acc: 0.7916666865348816)
[2024-12-12 02:30:41,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:41,420][root][INFO] - Training Epoch: 3/10, step 237/574 completed (loss: 2.2740015983581543, acc: 0.3181818127632141)
[2024-12-12 02:30:41,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:41,839][root][INFO] - Training Epoch: 3/10, step 238/574 completed (loss: 1.7053899765014648, acc: 0.40740740299224854)
[2024-12-12 02:30:41,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:42,159][root][INFO] - Training Epoch: 3/10, step 239/574 completed (loss: 1.062419056892395, acc: 0.6857143044471741)
[2024-12-12 02:30:42,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:42,491][root][INFO] - Training Epoch: 3/10, step 240/574 completed (loss: 1.4749127626419067, acc: 0.5909090638160706)
[2024-12-12 02:30:42,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:42,881][root][INFO] - Training Epoch: 3/10, step 241/574 completed (loss: 1.8723711967468262, acc: 0.5227272510528564)
[2024-12-12 02:30:43,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:43,461][root][INFO] - Training Epoch: 3/10, step 242/574 completed (loss: 2.00606107711792, acc: 0.4516128897666931)
[2024-12-12 02:30:43,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:43,988][root][INFO] - Training Epoch: 3/10, step 243/574 completed (loss: 1.4501676559448242, acc: 0.6136363744735718)
[2024-12-12 02:30:44,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:44,341][root][INFO] - Training Epoch: 3/10, step 244/574 completed (loss: 0.21407417953014374, acc: 0.9523809552192688)
[2024-12-12 02:30:44,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:44,671][root][INFO] - Training Epoch: 3/10, step 245/574 completed (loss: 0.7320578694343567, acc: 0.8461538553237915)
[2024-12-12 02:30:44,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:45,030][root][INFO] - Training Epoch: 3/10, step 246/574 completed (loss: 0.8997127413749695, acc: 0.774193525314331)
[2024-12-12 02:30:45,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:45,389][root][INFO] - Training Epoch: 3/10, step 247/574 completed (loss: 0.6037604808807373, acc: 0.75)
[2024-12-12 02:30:45,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:45,770][root][INFO] - Training Epoch: 3/10, step 248/574 completed (loss: 1.335618257522583, acc: 0.5945945978164673)
[2024-12-12 02:30:45,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:46,094][root][INFO] - Training Epoch: 3/10, step 249/574 completed (loss: 1.2650139331817627, acc: 0.5945945978164673)
[2024-12-12 02:30:46,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:46,436][root][INFO] - Training Epoch: 3/10, step 250/574 completed (loss: 1.1765564680099487, acc: 0.6486486196517944)
[2024-12-12 02:30:46,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:46,792][root][INFO] - Training Epoch: 3/10, step 251/574 completed (loss: 1.6149930953979492, acc: 0.5588235259056091)
[2024-12-12 02:30:46,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:47,177][root][INFO] - Training Epoch: 3/10, step 252/574 completed (loss: 0.5793860554695129, acc: 0.8048780560493469)
[2024-12-12 02:30:47,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:47,521][root][INFO] - Training Epoch: 3/10, step 253/574 completed (loss: 0.2890429198741913, acc: 0.9599999785423279)
[2024-12-12 02:30:47,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:47,823][root][INFO] - Training Epoch: 3/10, step 254/574 completed (loss: 0.16599828004837036, acc: 0.9200000166893005)
[2024-12-12 02:30:47,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:48,137][root][INFO] - Training Epoch: 3/10, step 255/574 completed (loss: 0.2709500193595886, acc: 0.9677419066429138)
[2024-12-12 02:30:48,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:48,528][root][INFO] - Training Epoch: 3/10, step 256/574 completed (loss: 1.2759008407592773, acc: 0.6666666865348816)
[2024-12-12 02:30:48,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:48,905][root][INFO] - Training Epoch: 3/10, step 257/574 completed (loss: 1.5095493793487549, acc: 0.6428571343421936)
[2024-12-12 02:30:49,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:49,241][root][INFO] - Training Epoch: 3/10, step 258/574 completed (loss: 1.2403472661972046, acc: 0.6315789222717285)
[2024-12-12 02:30:49,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:49,807][root][INFO] - Training Epoch: 3/10, step 259/574 completed (loss: 1.6632412672042847, acc: 0.5283018946647644)
[2024-12-12 02:30:49,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:50,391][root][INFO] - Training Epoch: 3/10, step 260/574 completed (loss: 1.730104923248291, acc: 0.5333333611488342)
[2024-12-12 02:30:50,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:50,758][root][INFO] - Training Epoch: 3/10, step 261/574 completed (loss: 1.014326572418213, acc: 0.6944444179534912)
[2024-12-12 02:30:50,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:51,080][root][INFO] - Training Epoch: 3/10, step 262/574 completed (loss: 1.3840148448944092, acc: 0.6129032373428345)
[2024-12-12 02:30:51,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:51,402][root][INFO] - Training Epoch: 3/10, step 263/574 completed (loss: 2.777834415435791, acc: 0.36000001430511475)
[2024-12-12 02:30:51,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:51,708][root][INFO] - Training Epoch: 3/10, step 264/574 completed (loss: 2.0650734901428223, acc: 0.5)
[2024-12-12 02:30:51,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:52,530][root][INFO] - Training Epoch: 3/10, step 265/574 completed (loss: 2.378389596939087, acc: 0.35199999809265137)
[2024-12-12 02:30:52,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:52,906][root][INFO] - Training Epoch: 3/10, step 266/574 completed (loss: 2.137874126434326, acc: 0.3820224702358246)
[2024-12-12 02:30:53,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:53,272][root][INFO] - Training Epoch: 3/10, step 267/574 completed (loss: 2.158566474914551, acc: 0.45945945382118225)
[2024-12-12 02:30:53,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:53,719][root][INFO] - Training Epoch: 3/10, step 268/574 completed (loss: 1.3106557130813599, acc: 0.6206896305084229)
[2024-12-12 02:30:53,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:54,076][root][INFO] - Training Epoch: 3/10, step 269/574 completed (loss: 0.7864828109741211, acc: 0.8181818127632141)
[2024-12-12 02:30:54,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:54,439][root][INFO] - Training Epoch: 3/10, step 270/574 completed (loss: 0.7006588578224182, acc: 0.7727272510528564)
[2024-12-12 02:30:54,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:54,805][root][INFO] - Training Epoch: 3/10, step 271/574 completed (loss: 0.7263789772987366, acc: 0.78125)
[2024-12-12 02:30:54,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:55,145][root][INFO] - Training Epoch: 3/10, step 272/574 completed (loss: 1.0211710929870605, acc: 0.699999988079071)
[2024-12-12 02:30:55,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:55,535][root][INFO] - Training Epoch: 3/10, step 273/574 completed (loss: 1.7531808614730835, acc: 0.5)
[2024-12-12 02:30:55,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:55,885][root][INFO] - Training Epoch: 3/10, step 274/574 completed (loss: 1.0896501541137695, acc: 0.65625)
[2024-12-12 02:30:56,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:56,299][root][INFO] - Training Epoch: 3/10, step 275/574 completed (loss: 0.6963834762573242, acc: 0.800000011920929)
[2024-12-12 02:30:56,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:56,674][root][INFO] - Training Epoch: 3/10, step 276/574 completed (loss: 1.182167410850525, acc: 0.7241379022598267)
[2024-12-12 02:30:56,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:57,060][root][INFO] - Training Epoch: 3/10, step 277/574 completed (loss: 0.8595573306083679, acc: 0.7200000286102295)
[2024-12-12 02:30:57,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:57,393][root][INFO] - Training Epoch: 3/10, step 278/574 completed (loss: 2.0752241611480713, acc: 0.40425533056259155)
[2024-12-12 02:30:57,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:57,760][root][INFO] - Training Epoch: 3/10, step 279/574 completed (loss: 1.5465445518493652, acc: 0.5416666865348816)
[2024-12-12 02:30:57,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:58,106][root][INFO] - Training Epoch: 3/10, step 280/574 completed (loss: 1.4714274406433105, acc: 0.6363636255264282)
[2024-12-12 02:30:58,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:58,523][root][INFO] - Training Epoch: 3/10, step 281/574 completed (loss: 2.0946929454803467, acc: 0.4939759075641632)
[2024-12-12 02:30:59,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:30:59,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:00,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:00,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:00,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:01,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:01,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:01,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:02,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:02,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:02,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:03,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:03,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:03,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:04,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:04,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:04,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:05,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:05,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:05,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:06,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:06,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:06,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:07,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:07,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:07,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:08,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:08,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:09,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:09,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:09,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:10,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:10,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:10,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:11,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:11,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:11,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:12,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:12,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:12,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:13,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:13,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:14,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:14,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:14,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:15,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:15,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:15,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:16,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:16,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:16,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:17,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:17,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:17,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:18,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:18,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:18,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:19,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:19,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:19,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:20,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:20,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:21,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:21,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:21,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:22,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:22,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:22,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:23,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:23,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:24,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:24,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:24,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:25,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:25,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:25,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:26,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:26,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:26,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:27,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:27,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:27,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:28,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:28,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:28,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:29,496][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(5.7529, device='cuda:0') eval_epoch_loss=tensor(1.7497, device='cuda:0') eval_epoch_acc=tensor(0.5525, device='cuda:0')
[2024-12-12 02:31:29,497][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:31:29,497][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:31:29,708][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_3_step_282_loss_1.7497124671936035/model.pt
[2024-12-12 02:31:29,714][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:31:29,715][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 1.7497124671936035
[2024-12-12 02:31:29,716][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.552504301071167
[2024-12-12 02:31:29,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:30,135][root][INFO] - Training Epoch: 3/10, step 282/574 completed (loss: 1.8975937366485596, acc: 0.5185185074806213)
[2024-12-12 02:31:30,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:30,476][root][INFO] - Training Epoch: 3/10, step 283/574 completed (loss: 1.9395344257354736, acc: 0.3947368562221527)
[2024-12-12 02:31:30,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:30,776][root][INFO] - Training Epoch: 3/10, step 284/574 completed (loss: 2.017819404602051, acc: 0.47058823704719543)
[2024-12-12 02:31:30,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:31,135][root][INFO] - Training Epoch: 3/10, step 285/574 completed (loss: 1.6998023986816406, acc: 0.4749999940395355)
[2024-12-12 02:31:31,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:31,484][root][INFO] - Training Epoch: 3/10, step 286/574 completed (loss: 2.050370216369629, acc: 0.4296875)
[2024-12-12 02:31:31,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:31,814][root][INFO] - Training Epoch: 3/10, step 287/574 completed (loss: 2.1944992542266846, acc: 0.4000000059604645)
[2024-12-12 02:31:31,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:32,108][root][INFO] - Training Epoch: 3/10, step 288/574 completed (loss: 1.743985891342163, acc: 0.5604395866394043)
[2024-12-12 02:31:32,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:32,453][root][INFO] - Training Epoch: 3/10, step 289/574 completed (loss: 2.2459051609039307, acc: 0.4285714328289032)
[2024-12-12 02:31:32,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:32,802][root][INFO] - Training Epoch: 3/10, step 290/574 completed (loss: 2.345120429992676, acc: 0.3659793734550476)
[2024-12-12 02:31:32,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:33,165][root][INFO] - Training Epoch: 3/10, step 291/574 completed (loss: 0.4013197720050812, acc: 0.8636363744735718)
[2024-12-12 02:31:33,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:33,527][root][INFO] - Training Epoch: 3/10, step 292/574 completed (loss: 1.8113679885864258, acc: 0.5714285969734192)
[2024-12-12 02:31:33,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:33,902][root][INFO] - Training Epoch: 3/10, step 293/574 completed (loss: 1.3358147144317627, acc: 0.6379310488700867)
[2024-12-12 02:31:34,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:34,381][root][INFO] - Training Epoch: 3/10, step 294/574 completed (loss: 1.1645318269729614, acc: 0.6909090876579285)
[2024-12-12 02:31:34,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:34,932][root][INFO] - Training Epoch: 3/10, step 295/574 completed (loss: 1.773443341255188, acc: 0.5257731676101685)
[2024-12-12 02:31:35,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:35,280][root][INFO] - Training Epoch: 3/10, step 296/574 completed (loss: 1.9268171787261963, acc: 0.517241358757019)
[2024-12-12 02:31:35,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:35,659][root][INFO] - Training Epoch: 3/10, step 297/574 completed (loss: 1.5941739082336426, acc: 0.5555555820465088)
[2024-12-12 02:31:35,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:36,004][root][INFO] - Training Epoch: 3/10, step 298/574 completed (loss: 1.7126761674880981, acc: 0.5263158082962036)
[2024-12-12 02:31:36,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:36,369][root][INFO] - Training Epoch: 3/10, step 299/574 completed (loss: 1.4921481609344482, acc: 0.6607142686843872)
[2024-12-12 02:31:36,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:36,747][root][INFO] - Training Epoch: 3/10, step 300/574 completed (loss: 1.4615583419799805, acc: 0.65625)
[2024-12-12 02:31:36,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:37,121][root][INFO] - Training Epoch: 3/10, step 301/574 completed (loss: 1.595863938331604, acc: 0.5849056839942932)
[2024-12-12 02:31:37,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:37,504][root][INFO] - Training Epoch: 3/10, step 302/574 completed (loss: 0.9166821837425232, acc: 0.7169811129570007)
[2024-12-12 02:31:37,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:37,877][root][INFO] - Training Epoch: 3/10, step 303/574 completed (loss: 1.0369430780410767, acc: 0.7352941036224365)
[2024-12-12 02:31:37,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:38,264][root][INFO] - Training Epoch: 3/10, step 304/574 completed (loss: 1.5764555931091309, acc: 0.625)
[2024-12-12 02:31:38,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:38,608][root][INFO] - Training Epoch: 3/10, step 305/574 completed (loss: 1.187355637550354, acc: 0.6557376980781555)
[2024-12-12 02:31:38,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:38,893][root][INFO] - Training Epoch: 3/10, step 306/574 completed (loss: 0.6138385534286499, acc: 0.8333333134651184)
[2024-12-12 02:31:38,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:39,189][root][INFO] - Training Epoch: 3/10, step 307/574 completed (loss: 0.21132637560367584, acc: 0.9473684430122375)
[2024-12-12 02:31:39,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:39,522][root][INFO] - Training Epoch: 3/10, step 308/574 completed (loss: 1.8882588148117065, acc: 0.5072463750839233)
[2024-12-12 02:31:39,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:39,931][root][INFO] - Training Epoch: 3/10, step 309/574 completed (loss: 1.5691685676574707, acc: 0.5555555820465088)
[2024-12-12 02:31:40,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:40,316][root][INFO] - Training Epoch: 3/10, step 310/574 completed (loss: 1.4843127727508545, acc: 0.5421686768531799)
[2024-12-12 02:31:40,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:40,694][root][INFO] - Training Epoch: 3/10, step 311/574 completed (loss: 2.0381152629852295, acc: 0.41025641560554504)
[2024-12-12 02:31:40,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:41,086][root][INFO] - Training Epoch: 3/10, step 312/574 completed (loss: 2.082143545150757, acc: 0.44897958636283875)
[2024-12-12 02:31:41,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:41,445][root][INFO] - Training Epoch: 3/10, step 313/574 completed (loss: 0.07066886126995087, acc: 1.0)
[2024-12-12 02:31:41,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:41,792][root][INFO] - Training Epoch: 3/10, step 314/574 completed (loss: 0.5915074944496155, acc: 0.8333333134651184)
[2024-12-12 02:31:41,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:42,127][root][INFO] - Training Epoch: 3/10, step 315/574 completed (loss: 0.5467926263809204, acc: 0.8709677457809448)
[2024-12-12 02:31:42,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:42,504][root][INFO] - Training Epoch: 3/10, step 316/574 completed (loss: 0.7301197648048401, acc: 0.8064516186714172)
[2024-12-12 02:31:42,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:42,890][root][INFO] - Training Epoch: 3/10, step 317/574 completed (loss: 1.2533800601959229, acc: 0.6567164063453674)
[2024-12-12 02:31:42,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:43,253][root][INFO] - Training Epoch: 3/10, step 318/574 completed (loss: 1.3067049980163574, acc: 0.6634615659713745)
[2024-12-12 02:31:43,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:43,589][root][INFO] - Training Epoch: 3/10, step 319/574 completed (loss: 1.7914377450942993, acc: 0.42222222685813904)
[2024-12-12 02:31:43,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:43,929][root][INFO] - Training Epoch: 3/10, step 320/574 completed (loss: 1.2905596494674683, acc: 0.6290322542190552)
[2024-12-12 02:31:44,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:44,305][root][INFO] - Training Epoch: 3/10, step 321/574 completed (loss: 0.7127472758293152, acc: 0.800000011920929)
[2024-12-12 02:31:44,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:44,674][root][INFO] - Training Epoch: 3/10, step 322/574 completed (loss: 1.9647408723831177, acc: 0.40740740299224854)
[2024-12-12 02:31:44,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:45,000][root][INFO] - Training Epoch: 3/10, step 323/574 completed (loss: 2.6642167568206787, acc: 0.2857142984867096)
[2024-12-12 02:31:45,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:45,380][root][INFO] - Training Epoch: 3/10, step 324/574 completed (loss: 2.008025646209717, acc: 0.4871794879436493)
[2024-12-12 02:31:45,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:45,720][root][INFO] - Training Epoch: 3/10, step 325/574 completed (loss: 2.4282095432281494, acc: 0.4146341383457184)
[2024-12-12 02:31:45,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:46,042][root][INFO] - Training Epoch: 3/10, step 326/574 completed (loss: 1.8065787553787231, acc: 0.5)
[2024-12-12 02:31:46,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:46,343][root][INFO] - Training Epoch: 3/10, step 327/574 completed (loss: 0.8371224403381348, acc: 0.6842105388641357)
[2024-12-12 02:31:46,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:46,675][root][INFO] - Training Epoch: 3/10, step 328/574 completed (loss: 0.28830578923225403, acc: 0.9642857313156128)
[2024-12-12 02:31:46,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:47,031][root][INFO] - Training Epoch: 3/10, step 329/574 completed (loss: 1.2355157136917114, acc: 0.5555555820465088)
[2024-12-12 02:31:47,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:47,385][root][INFO] - Training Epoch: 3/10, step 330/574 completed (loss: 0.6267641186714172, acc: 0.84375)
[2024-12-12 02:31:47,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:47,737][root][INFO] - Training Epoch: 3/10, step 331/574 completed (loss: 1.6360900402069092, acc: 0.5806451439857483)
[2024-12-12 02:31:47,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:48,135][root][INFO] - Training Epoch: 3/10, step 332/574 completed (loss: 1.3934969902038574, acc: 0.5263158082962036)
[2024-12-12 02:31:48,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:48,478][root][INFO] - Training Epoch: 3/10, step 333/574 completed (loss: 2.1863107681274414, acc: 0.4375)
[2024-12-12 02:31:48,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:48,820][root][INFO] - Training Epoch: 3/10, step 334/574 completed (loss: 0.8320803046226501, acc: 0.699999988079071)
[2024-12-12 02:31:48,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:49,192][root][INFO] - Training Epoch: 3/10, step 335/574 completed (loss: 1.326934576034546, acc: 0.5789473652839661)
[2024-12-12 02:31:49,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:49,585][root][INFO] - Training Epoch: 3/10, step 336/574 completed (loss: 1.9971297979354858, acc: 0.47999998927116394)
[2024-12-12 02:31:49,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:49,984][root][INFO] - Training Epoch: 3/10, step 337/574 completed (loss: 2.1861929893493652, acc: 0.37931033968925476)
[2024-12-12 02:31:50,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:50,366][root][INFO] - Training Epoch: 3/10, step 338/574 completed (loss: 2.3253183364868164, acc: 0.38297873735427856)
[2024-12-12 02:31:50,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:50,750][root][INFO] - Training Epoch: 3/10, step 339/574 completed (loss: 2.3699424266815186, acc: 0.3855421543121338)
[2024-12-12 02:31:50,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:51,092][root][INFO] - Training Epoch: 3/10, step 340/574 completed (loss: 0.6099544763565063, acc: 0.8260869383811951)
[2024-12-12 02:31:51,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:51,451][root][INFO] - Training Epoch: 3/10, step 341/574 completed (loss: 1.5466042757034302, acc: 0.5384615659713745)
[2024-12-12 02:31:51,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:51,818][root][INFO] - Training Epoch: 3/10, step 342/574 completed (loss: 2.1697728633880615, acc: 0.4457831382751465)
[2024-12-12 02:31:51,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:52,151][root][INFO] - Training Epoch: 3/10, step 343/574 completed (loss: 1.7599960565567017, acc: 0.5660377144813538)
[2024-12-12 02:31:52,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:52,515][root][INFO] - Training Epoch: 3/10, step 344/574 completed (loss: 1.725233793258667, acc: 0.607594907283783)
[2024-12-12 02:31:52,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:52,872][root][INFO] - Training Epoch: 3/10, step 345/574 completed (loss: 1.6048802137374878, acc: 0.5490196347236633)
[2024-12-12 02:31:52,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:53,230][root][INFO] - Training Epoch: 3/10, step 346/574 completed (loss: 2.1761248111724854, acc: 0.3731343150138855)
[2024-12-12 02:31:53,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:53,609][root][INFO] - Training Epoch: 3/10, step 347/574 completed (loss: 0.4915989935398102, acc: 0.8500000238418579)
[2024-12-12 02:31:53,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:53,927][root][INFO] - Training Epoch: 3/10, step 348/574 completed (loss: 0.9567280411720276, acc: 0.7599999904632568)
[2024-12-12 02:31:54,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:54,310][root][INFO] - Training Epoch: 3/10, step 349/574 completed (loss: 1.1278303861618042, acc: 0.7222222089767456)
[2024-12-12 02:31:54,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:54,670][root][INFO] - Training Epoch: 3/10, step 350/574 completed (loss: 1.8830105066299438, acc: 0.4651162922382355)
[2024-12-12 02:31:54,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:55,026][root][INFO] - Training Epoch: 3/10, step 351/574 completed (loss: 1.664637804031372, acc: 0.5128205418586731)
[2024-12-12 02:31:55,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:55,386][root][INFO] - Training Epoch: 3/10, step 352/574 completed (loss: 1.7868918180465698, acc: 0.5777778029441833)
[2024-12-12 02:31:55,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:55,747][root][INFO] - Training Epoch: 3/10, step 353/574 completed (loss: 0.4547652304172516, acc: 0.782608687877655)
[2024-12-12 02:31:55,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:56,092][root][INFO] - Training Epoch: 3/10, step 354/574 completed (loss: 1.5423651933670044, acc: 0.6153846383094788)
[2024-12-12 02:31:56,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:56,487][root][INFO] - Training Epoch: 3/10, step 355/574 completed (loss: 2.3330183029174805, acc: 0.3956044018268585)
[2024-12-12 02:31:56,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:56,984][root][INFO] - Training Epoch: 3/10, step 356/574 completed (loss: 1.9143121242523193, acc: 0.5478261113166809)
[2024-12-12 02:31:57,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:57,305][root][INFO] - Training Epoch: 3/10, step 357/574 completed (loss: 1.800129771232605, acc: 0.54347825050354)
[2024-12-12 02:31:57,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:57,685][root][INFO] - Training Epoch: 3/10, step 358/574 completed (loss: 1.835413932800293, acc: 0.4693877696990967)
[2024-12-12 02:31:57,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:57,982][root][INFO] - Training Epoch: 3/10, step 359/574 completed (loss: 0.1793494075536728, acc: 0.9583333134651184)
[2024-12-12 02:31:58,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:58,317][root][INFO] - Training Epoch: 3/10, step 360/574 completed (loss: 0.573013186454773, acc: 0.807692289352417)
[2024-12-12 02:31:58,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:58,683][root][INFO] - Training Epoch: 3/10, step 361/574 completed (loss: 1.287356972694397, acc: 0.6829268336296082)
[2024-12-12 02:31:58,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:59,029][root][INFO] - Training Epoch: 3/10, step 362/574 completed (loss: 1.56619393825531, acc: 0.644444465637207)
[2024-12-12 02:31:59,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:59,345][root][INFO] - Training Epoch: 3/10, step 363/574 completed (loss: 1.8054901361465454, acc: 0.46052631735801697)
[2024-12-12 02:31:59,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:31:59,710][root][INFO] - Training Epoch: 3/10, step 364/574 completed (loss: 1.5784186124801636, acc: 0.5121951103210449)
[2024-12-12 02:31:59,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:00,076][root][INFO] - Training Epoch: 3/10, step 365/574 completed (loss: 1.5007809400558472, acc: 0.5757575631141663)
[2024-12-12 02:32:00,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:00,390][root][INFO] - Training Epoch: 3/10, step 366/574 completed (loss: 0.4745961129665375, acc: 0.8333333134651184)
[2024-12-12 02:32:00,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:00,685][root][INFO] - Training Epoch: 3/10, step 367/574 completed (loss: 0.09445099532604218, acc: 1.0)
[2024-12-12 02:32:00,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:01,075][root][INFO] - Training Epoch: 3/10, step 368/574 completed (loss: 0.5180441737174988, acc: 0.8571428656578064)
[2024-12-12 02:32:01,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:01,431][root][INFO] - Training Epoch: 3/10, step 369/574 completed (loss: 0.7128294706344604, acc: 0.84375)
[2024-12-12 02:32:01,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:02,044][root][INFO] - Training Epoch: 3/10, step 370/574 completed (loss: 1.8296362161636353, acc: 0.539393961429596)
[2024-12-12 02:32:02,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:02,931][root][INFO] - Training Epoch: 3/10, step 371/574 completed (loss: 1.3641269207000732, acc: 0.6698113083839417)
[2024-12-12 02:32:03,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:03,254][root][INFO] - Training Epoch: 3/10, step 372/574 completed (loss: 1.4949918985366821, acc: 0.6000000238418579)
[2024-12-12 02:32:03,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:03,644][root][INFO] - Training Epoch: 3/10, step 373/574 completed (loss: 1.4080315828323364, acc: 0.6071428656578064)
[2024-12-12 02:32:03,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:04,032][root][INFO] - Training Epoch: 3/10, step 374/574 completed (loss: 0.7867568135261536, acc: 0.7142857313156128)
[2024-12-12 02:32:04,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:04,416][root][INFO] - Training Epoch: 3/10, step 375/574 completed (loss: 0.08861774206161499, acc: 1.0)
[2024-12-12 02:32:04,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:04,731][root][INFO] - Training Epoch: 3/10, step 376/574 completed (loss: 0.20380866527557373, acc: 0.95652174949646)
[2024-12-12 02:32:04,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:05,089][root][INFO] - Training Epoch: 3/10, step 377/574 completed (loss: 1.4315247535705566, acc: 0.6458333134651184)
[2024-12-12 02:32:05,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:05,447][root][INFO] - Training Epoch: 3/10, step 378/574 completed (loss: 1.6120975017547607, acc: 0.5684210658073425)
[2024-12-12 02:32:05,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:06,021][root][INFO] - Training Epoch: 3/10, step 379/574 completed (loss: 1.7311127185821533, acc: 0.57485032081604)
[2024-12-12 02:32:06,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:06,433][root][INFO] - Training Epoch: 3/10, step 380/574 completed (loss: 1.64823579788208, acc: 0.5263158082962036)
[2024-12-12 02:32:06,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:07,674][root][INFO] - Training Epoch: 3/10, step 381/574 completed (loss: 1.659937858581543, acc: 0.5561497211456299)
[2024-12-12 02:32:07,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:08,238][root][INFO] - Training Epoch: 3/10, step 382/574 completed (loss: 1.3242530822753906, acc: 0.6486486196517944)
[2024-12-12 02:32:08,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:08,596][root][INFO] - Training Epoch: 3/10, step 383/574 completed (loss: 0.6717982292175293, acc: 0.7857142686843872)
[2024-12-12 02:32:08,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:08,942][root][INFO] - Training Epoch: 3/10, step 384/574 completed (loss: 0.3956827223300934, acc: 0.8928571343421936)
[2024-12-12 02:32:09,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:09,312][root][INFO] - Training Epoch: 3/10, step 385/574 completed (loss: 1.0203299522399902, acc: 0.71875)
[2024-12-12 02:32:09,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:09,673][root][INFO] - Training Epoch: 3/10, step 386/574 completed (loss: 0.7753347158432007, acc: 0.7777777910232544)
[2024-12-12 02:32:09,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:10,013][root][INFO] - Training Epoch: 3/10, step 387/574 completed (loss: 0.9511309266090393, acc: 0.7631579041481018)
[2024-12-12 02:32:10,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:10,340][root][INFO] - Training Epoch: 3/10, step 388/574 completed (loss: 0.2977276146411896, acc: 0.9090909361839294)
[2024-12-12 02:32:10,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:10,694][root][INFO] - Training Epoch: 3/10, step 389/574 completed (loss: 0.7028094530105591, acc: 0.75)
[2024-12-12 02:32:10,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:11,078][root][INFO] - Training Epoch: 3/10, step 390/574 completed (loss: 0.6006089448928833, acc: 0.9047619104385376)
[2024-12-12 02:32:11,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:11,397][root][INFO] - Training Epoch: 3/10, step 391/574 completed (loss: 2.391373872756958, acc: 0.37037035822868347)
[2024-12-12 02:32:11,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:11,721][root][INFO] - Training Epoch: 3/10, step 392/574 completed (loss: 2.229785203933716, acc: 0.41747573018074036)
[2024-12-12 02:32:11,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:12,249][root][INFO] - Training Epoch: 3/10, step 393/574 completed (loss: 1.7730871438980103, acc: 0.529411792755127)
[2024-12-12 02:32:12,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:12,628][root][INFO] - Training Epoch: 3/10, step 394/574 completed (loss: 2.1265780925750732, acc: 0.4533333480358124)
[2024-12-12 02:32:12,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:13,029][root][INFO] - Training Epoch: 3/10, step 395/574 completed (loss: 2.102633237838745, acc: 0.4444444477558136)
[2024-12-12 02:32:13,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:13,440][root][INFO] - Training Epoch: 3/10, step 396/574 completed (loss: 1.4361779689788818, acc: 0.6744186282157898)
[2024-12-12 02:32:13,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:13,784][root][INFO] - Training Epoch: 3/10, step 397/574 completed (loss: 0.6113433837890625, acc: 0.75)
[2024-12-12 02:32:13,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:14,111][root][INFO] - Training Epoch: 3/10, step 398/574 completed (loss: 1.363523244857788, acc: 0.6279069781303406)
[2024-12-12 02:32:14,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:14,493][root][INFO] - Training Epoch: 3/10, step 399/574 completed (loss: 1.1085129976272583, acc: 0.6399999856948853)
[2024-12-12 02:32:14,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:15,026][root][INFO] - Training Epoch: 3/10, step 400/574 completed (loss: 1.6335030794143677, acc: 0.5588235259056091)
[2024-12-12 02:32:15,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:15,386][root][INFO] - Training Epoch: 3/10, step 401/574 completed (loss: 1.5922061204910278, acc: 0.5733333230018616)
[2024-12-12 02:32:15,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:15,722][root][INFO] - Training Epoch: 3/10, step 402/574 completed (loss: 0.9827715754508972, acc: 0.7272727489471436)
[2024-12-12 02:32:15,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:16,088][root][INFO] - Training Epoch: 3/10, step 403/574 completed (loss: 1.0747747421264648, acc: 0.7575757503509521)
[2024-12-12 02:32:16,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:16,470][root][INFO] - Training Epoch: 3/10, step 404/574 completed (loss: 0.4932326078414917, acc: 0.8709677457809448)
[2024-12-12 02:32:16,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:16,791][root][INFO] - Training Epoch: 3/10, step 405/574 completed (loss: 0.871763288974762, acc: 0.7407407164573669)
[2024-12-12 02:32:16,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:17,137][root][INFO] - Training Epoch: 3/10, step 406/574 completed (loss: 0.4034082889556885, acc: 0.9599999785423279)
[2024-12-12 02:32:17,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:17,516][root][INFO] - Training Epoch: 3/10, step 407/574 completed (loss: 0.6286032199859619, acc: 0.8333333134651184)
[2024-12-12 02:32:17,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:17,894][root][INFO] - Training Epoch: 3/10, step 408/574 completed (loss: 0.555107593536377, acc: 0.8888888955116272)
[2024-12-12 02:32:18,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:18,253][root][INFO] - Training Epoch: 3/10, step 409/574 completed (loss: 0.43366605043411255, acc: 0.8846153616905212)
[2024-12-12 02:32:18,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:18,617][root][INFO] - Training Epoch: 3/10, step 410/574 completed (loss: 1.1006678342819214, acc: 0.6896551847457886)
[2024-12-12 02:32:18,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:18,945][root][INFO] - Training Epoch: 3/10, step 411/574 completed (loss: 0.6351121068000793, acc: 0.7857142686843872)
[2024-12-12 02:32:19,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:19,233][root][INFO] - Training Epoch: 3/10, step 412/574 completed (loss: 0.7452724575996399, acc: 0.8666666746139526)
[2024-12-12 02:32:19,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:19,575][root][INFO] - Training Epoch: 3/10, step 413/574 completed (loss: 0.6050053834915161, acc: 0.8484848737716675)
[2024-12-12 02:32:19,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:19,914][root][INFO] - Training Epoch: 3/10, step 414/574 completed (loss: 0.16009916365146637, acc: 0.9545454382896423)
[2024-12-12 02:32:20,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:20,265][root][INFO] - Training Epoch: 3/10, step 415/574 completed (loss: 1.8332821130752563, acc: 0.47058823704719543)
[2024-12-12 02:32:20,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:20,640][root][INFO] - Training Epoch: 3/10, step 416/574 completed (loss: 1.637107014656067, acc: 0.5384615659713745)
[2024-12-12 02:32:20,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:20,984][root][INFO] - Training Epoch: 3/10, step 417/574 completed (loss: 0.8939232230186462, acc: 0.6666666865348816)
[2024-12-12 02:32:21,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:21,350][root][INFO] - Training Epoch: 3/10, step 418/574 completed (loss: 1.4266998767852783, acc: 0.6499999761581421)
[2024-12-12 02:32:21,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:21,717][root][INFO] - Training Epoch: 3/10, step 419/574 completed (loss: 1.114327311515808, acc: 0.6499999761581421)
[2024-12-12 02:32:21,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:22,081][root][INFO] - Training Epoch: 3/10, step 420/574 completed (loss: 0.1548113375902176, acc: 0.9523809552192688)
[2024-12-12 02:32:22,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:22,395][root][INFO] - Training Epoch: 3/10, step 421/574 completed (loss: 0.8160672783851624, acc: 0.7333333492279053)
[2024-12-12 02:32:22,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:22,772][root][INFO] - Training Epoch: 3/10, step 422/574 completed (loss: 0.7836447954177856, acc: 0.8125)
[2024-12-12 02:32:22,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:23,166][root][INFO] - Training Epoch: 3/10, step 423/574 completed (loss: 1.669803500175476, acc: 0.5)
[2024-12-12 02:32:23,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:23,503][root][INFO] - Training Epoch: 3/10, step 424/574 completed (loss: 1.1725552082061768, acc: 0.6666666865348816)
[2024-12-12 02:32:24,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:24,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:24,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:25,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:25,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:25,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:26,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:26,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:26,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:27,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:27,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:27,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:28,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:28,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:28,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:29,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:29,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:29,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:30,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:30,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:30,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:31,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:31,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:31,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:32,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:32,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:32,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:33,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:33,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:33,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:34,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:34,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:34,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:35,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:35,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:35,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:36,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:36,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:37,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:37,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:37,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:38,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:38,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:38,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:39,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:39,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:39,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:40,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:40,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:40,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:41,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:41,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:41,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:42,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:42,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:42,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:43,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:43,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:43,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:44,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:44,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:44,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:45,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:45,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:45,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:46,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:46,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:46,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:47,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:47,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:47,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:48,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:48,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:48,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:49,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:49,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:50,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:50,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:50,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:51,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:51,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:51,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:52,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:52,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:52,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:53,382][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(6.3120, device='cuda:0') eval_epoch_loss=tensor(1.8425, device='cuda:0') eval_epoch_acc=tensor(0.5292, device='cuda:0')
[2024-12-12 02:32:53,384][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:32:53,384][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:32:53,608][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_3_step_425_loss_1.842451810836792/model.pt
[2024-12-12 02:32:53,611][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:32:53,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:54,000][root][INFO] - Training Epoch: 3/10, step 425/574 completed (loss: 1.109177589416504, acc: 0.7272727489471436)
[2024-12-12 02:32:54,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:54,327][root][INFO] - Training Epoch: 3/10, step 426/574 completed (loss: 0.6168930530548096, acc: 0.9130434989929199)
[2024-12-12 02:32:54,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:54,640][root][INFO] - Training Epoch: 3/10, step 427/574 completed (loss: 0.9684286117553711, acc: 0.7837837934494019)
[2024-12-12 02:32:54,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:54,992][root][INFO] - Training Epoch: 3/10, step 428/574 completed (loss: 0.9561636447906494, acc: 0.7037037014961243)
[2024-12-12 02:32:55,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:55,309][root][INFO] - Training Epoch: 3/10, step 429/574 completed (loss: 1.2865824699401855, acc: 0.5652173757553101)
[2024-12-12 02:32:55,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:55,606][root][INFO] - Training Epoch: 3/10, step 430/574 completed (loss: 0.8861991763114929, acc: 0.7777777910232544)
[2024-12-12 02:32:55,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:55,982][root][INFO] - Training Epoch: 3/10, step 431/574 completed (loss: 0.46455511450767517, acc: 0.8518518805503845)
[2024-12-12 02:32:56,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:56,298][root][INFO] - Training Epoch: 3/10, step 432/574 completed (loss: 0.7315242886543274, acc: 0.8260869383811951)
[2024-12-12 02:32:56,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:56,679][root][INFO] - Training Epoch: 3/10, step 433/574 completed (loss: 1.1270098686218262, acc: 0.6944444179534912)
[2024-12-12 02:32:56,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:57,069][root][INFO] - Training Epoch: 3/10, step 434/574 completed (loss: 0.3005184233188629, acc: 0.8799999952316284)
[2024-12-12 02:32:57,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:57,466][root][INFO] - Training Epoch: 3/10, step 435/574 completed (loss: 0.9944284558296204, acc: 0.6969696879386902)
[2024-12-12 02:32:57,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:57,866][root][INFO] - Training Epoch: 3/10, step 436/574 completed (loss: 1.1295366287231445, acc: 0.6388888955116272)
[2024-12-12 02:32:57,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:58,167][root][INFO] - Training Epoch: 3/10, step 437/574 completed (loss: 1.1105198860168457, acc: 0.7272727489471436)
[2024-12-12 02:32:58,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:58,520][root][INFO] - Training Epoch: 3/10, step 438/574 completed (loss: 0.2733272314071655, acc: 0.9047619104385376)
[2024-12-12 02:32:58,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:58,892][root][INFO] - Training Epoch: 3/10, step 439/574 completed (loss: 1.5807374715805054, acc: 0.6410256624221802)
[2024-12-12 02:32:59,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:32:59,352][root][INFO] - Training Epoch: 3/10, step 440/574 completed (loss: 1.821943759918213, acc: 0.5303030014038086)
[2024-12-12 02:32:59,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:00,087][root][INFO] - Training Epoch: 3/10, step 441/574 completed (loss: 2.2830052375793457, acc: 0.3919999897480011)
[2024-12-12 02:33:00,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:00,500][root][INFO] - Training Epoch: 3/10, step 442/574 completed (loss: 2.1191210746765137, acc: 0.45967742800712585)
[2024-12-12 02:33:00,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:01,151][root][INFO] - Training Epoch: 3/10, step 443/574 completed (loss: 2.0524308681488037, acc: 0.45771142840385437)
[2024-12-12 02:33:01,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:01,451][root][INFO] - Training Epoch: 3/10, step 444/574 completed (loss: 1.6771833896636963, acc: 0.5849056839942932)
[2024-12-12 02:33:01,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:01,869][root][INFO] - Training Epoch: 3/10, step 445/574 completed (loss: 0.8608047366142273, acc: 0.75)
[2024-12-12 02:33:01,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:02,237][root][INFO] - Training Epoch: 3/10, step 446/574 completed (loss: 0.9554640650749207, acc: 0.8260869383811951)
[2024-12-12 02:33:02,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:02,558][root][INFO] - Training Epoch: 3/10, step 447/574 completed (loss: 0.6818576455116272, acc: 0.7692307829856873)
[2024-12-12 02:33:02,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:02,937][root][INFO] - Training Epoch: 3/10, step 448/574 completed (loss: 0.73650062084198, acc: 0.75)
[2024-12-12 02:33:03,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:03,320][root][INFO] - Training Epoch: 3/10, step 449/574 completed (loss: 1.7443349361419678, acc: 0.6268656849861145)
[2024-12-12 02:33:03,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:03,700][root][INFO] - Training Epoch: 3/10, step 450/574 completed (loss: 1.50920569896698, acc: 0.5972222089767456)
[2024-12-12 02:33:03,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:04,072][root][INFO] - Training Epoch: 3/10, step 451/574 completed (loss: 1.6326990127563477, acc: 0.532608687877655)
[2024-12-12 02:33:04,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:04,466][root][INFO] - Training Epoch: 3/10, step 452/574 completed (loss: 1.7565866708755493, acc: 0.5384615659713745)
[2024-12-12 02:33:04,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:04,800][root][INFO] - Training Epoch: 3/10, step 453/574 completed (loss: 1.9858131408691406, acc: 0.4736842215061188)
[2024-12-12 02:33:04,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:05,120][root][INFO] - Training Epoch: 3/10, step 454/574 completed (loss: 1.6769168376922607, acc: 0.5510203838348389)
[2024-12-12 02:33:05,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:05,420][root][INFO] - Training Epoch: 3/10, step 455/574 completed (loss: 1.0983543395996094, acc: 0.6969696879386902)
[2024-12-12 02:33:05,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:05,748][root][INFO] - Training Epoch: 3/10, step 456/574 completed (loss: 2.075798511505127, acc: 0.4329896867275238)
[2024-12-12 02:33:05,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:06,109][root][INFO] - Training Epoch: 3/10, step 457/574 completed (loss: 1.6203233003616333, acc: 0.5285714268684387)
[2024-12-12 02:33:06,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:06,506][root][INFO] - Training Epoch: 3/10, step 458/574 completed (loss: 1.8732575178146362, acc: 0.4883720874786377)
[2024-12-12 02:33:06,597][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:06,830][root][INFO] - Training Epoch: 3/10, step 459/574 completed (loss: 2.072805404663086, acc: 0.4107142984867096)
[2024-12-12 02:33:06,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:07,182][root][INFO] - Training Epoch: 3/10, step 460/574 completed (loss: 1.8615522384643555, acc: 0.48148149251937866)
[2024-12-12 02:33:07,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:07,513][root][INFO] - Training Epoch: 3/10, step 461/574 completed (loss: 1.3985352516174316, acc: 0.6111111044883728)
[2024-12-12 02:33:07,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:07,846][root][INFO] - Training Epoch: 3/10, step 462/574 completed (loss: 1.0387953519821167, acc: 0.6875)
[2024-12-12 02:33:07,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:08,207][root][INFO] - Training Epoch: 3/10, step 463/574 completed (loss: 1.0481200218200684, acc: 0.807692289352417)
[2024-12-12 02:33:08,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:08,538][root][INFO] - Training Epoch: 3/10, step 464/574 completed (loss: 1.3467700481414795, acc: 0.6304348111152649)
[2024-12-12 02:33:08,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:08,908][root][INFO] - Training Epoch: 3/10, step 465/574 completed (loss: 1.7442047595977783, acc: 0.4285714328289032)
[2024-12-12 02:33:09,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:09,298][root][INFO] - Training Epoch: 3/10, step 466/574 completed (loss: 2.017333507537842, acc: 0.4457831382751465)
[2024-12-12 02:33:09,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:09,722][root][INFO] - Training Epoch: 3/10, step 467/574 completed (loss: 1.600198745727539, acc: 0.5405405163764954)
[2024-12-12 02:33:09,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:10,070][root][INFO] - Training Epoch: 3/10, step 468/574 completed (loss: 1.7551065683364868, acc: 0.5242718458175659)
[2024-12-12 02:33:10,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:10,451][root][INFO] - Training Epoch: 3/10, step 469/574 completed (loss: 1.492566466331482, acc: 0.6178861856460571)
[2024-12-12 02:33:10,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:10,801][root][INFO] - Training Epoch: 3/10, step 470/574 completed (loss: 0.70351642370224, acc: 0.75)
[2024-12-12 02:33:10,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:11,091][root][INFO] - Training Epoch: 3/10, step 471/574 completed (loss: 1.384769320487976, acc: 0.5)
[2024-12-12 02:33:11,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:11,483][root][INFO] - Training Epoch: 3/10, step 472/574 completed (loss: 2.053920030593872, acc: 0.4313725531101227)
[2024-12-12 02:33:11,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:11,832][root][INFO] - Training Epoch: 3/10, step 473/574 completed (loss: 2.320274591445923, acc: 0.4017467200756073)
[2024-12-12 02:33:11,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:12,191][root][INFO] - Training Epoch: 3/10, step 474/574 completed (loss: 2.1498539447784424, acc: 0.3854166567325592)
[2024-12-12 02:33:12,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:12,607][root][INFO] - Training Epoch: 3/10, step 475/574 completed (loss: 2.1440131664276123, acc: 0.4049079716205597)
[2024-12-12 02:33:12,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:12,982][root][INFO] - Training Epoch: 3/10, step 476/574 completed (loss: 2.085190534591675, acc: 0.4316546618938446)
[2024-12-12 02:33:13,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:13,374][root][INFO] - Training Epoch: 3/10, step 477/574 completed (loss: 2.133985996246338, acc: 0.43718594312667847)
[2024-12-12 02:33:13,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:13,731][root][INFO] - Training Epoch: 3/10, step 478/574 completed (loss: 0.9807475209236145, acc: 0.75)
[2024-12-12 02:33:13,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:14,076][root][INFO] - Training Epoch: 3/10, step 479/574 completed (loss: 1.240999698638916, acc: 0.6969696879386902)
[2024-12-12 02:33:14,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:14,443][root][INFO] - Training Epoch: 3/10, step 480/574 completed (loss: 1.1766812801361084, acc: 0.7407407164573669)
[2024-12-12 02:33:14,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:14,824][root][INFO] - Training Epoch: 3/10, step 481/574 completed (loss: 1.1304881572723389, acc: 0.6499999761581421)
[2024-12-12 02:33:14,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:15,122][root][INFO] - Training Epoch: 3/10, step 482/574 completed (loss: 0.6095438003540039, acc: 0.8500000238418579)
[2024-12-12 02:33:15,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:15,496][root][INFO] - Training Epoch: 3/10, step 483/574 completed (loss: 1.3367547988891602, acc: 0.568965494632721)
[2024-12-12 02:33:15,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:15,872][root][INFO] - Training Epoch: 3/10, step 484/574 completed (loss: 0.7107141613960266, acc: 0.8064516186714172)
[2024-12-12 02:33:15,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:16,241][root][INFO] - Training Epoch: 3/10, step 485/574 completed (loss: 0.5103990435600281, acc: 0.8947368264198303)
[2024-12-12 02:33:16,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:16,614][root][INFO] - Training Epoch: 3/10, step 486/574 completed (loss: 1.4007893800735474, acc: 0.6296296119689941)
[2024-12-12 02:33:16,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:16,956][root][INFO] - Training Epoch: 3/10, step 487/574 completed (loss: 1.0876210927963257, acc: 0.6666666865348816)
[2024-12-12 02:33:17,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:17,316][root][INFO] - Training Epoch: 3/10, step 488/574 completed (loss: 1.081729769706726, acc: 0.5909090638160706)
[2024-12-12 02:33:17,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:17,681][root][INFO] - Training Epoch: 3/10, step 489/574 completed (loss: 1.880462884902954, acc: 0.4769230782985687)
[2024-12-12 02:33:17,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:18,048][root][INFO] - Training Epoch: 3/10, step 490/574 completed (loss: 0.9606485962867737, acc: 0.7333333492279053)
[2024-12-12 02:33:18,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:18,432][root][INFO] - Training Epoch: 3/10, step 491/574 completed (loss: 1.05326509475708, acc: 0.6551724076271057)
[2024-12-12 02:33:18,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:18,795][root][INFO] - Training Epoch: 3/10, step 492/574 completed (loss: 1.5308170318603516, acc: 0.5686274766921997)
[2024-12-12 02:33:18,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:19,179][root][INFO] - Training Epoch: 3/10, step 493/574 completed (loss: 1.126305341720581, acc: 0.6551724076271057)
[2024-12-12 02:33:19,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:19,506][root][INFO] - Training Epoch: 3/10, step 494/574 completed (loss: 0.47780147194862366, acc: 0.9473684430122375)
[2024-12-12 02:33:19,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:19,871][root][INFO] - Training Epoch: 3/10, step 495/574 completed (loss: 1.410395622253418, acc: 0.5789473652839661)
[2024-12-12 02:33:19,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:20,282][root][INFO] - Training Epoch: 3/10, step 496/574 completed (loss: 1.7429617643356323, acc: 0.5267857313156128)
[2024-12-12 02:33:20,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:20,706][root][INFO] - Training Epoch: 3/10, step 497/574 completed (loss: 1.7558417320251465, acc: 0.49438202381134033)
[2024-12-12 02:33:20,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:21,051][root][INFO] - Training Epoch: 3/10, step 498/574 completed (loss: 2.084778070449829, acc: 0.47191011905670166)
[2024-12-12 02:33:21,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:21,421][root][INFO] - Training Epoch: 3/10, step 499/574 completed (loss: 2.2334771156311035, acc: 0.39007091522216797)
[2024-12-12 02:33:21,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:21,810][root][INFO] - Training Epoch: 3/10, step 500/574 completed (loss: 2.2567811012268066, acc: 0.4021739065647125)
[2024-12-12 02:33:21,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:22,147][root][INFO] - Training Epoch: 3/10, step 501/574 completed (loss: 0.4517348110675812, acc: 0.8799999952316284)
[2024-12-12 02:33:22,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:22,483][root][INFO] - Training Epoch: 3/10, step 502/574 completed (loss: 0.41676002740859985, acc: 0.8846153616905212)
[2024-12-12 02:33:22,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:22,821][root][INFO] - Training Epoch: 3/10, step 503/574 completed (loss: 0.43551430106163025, acc: 0.8888888955116272)
[2024-12-12 02:33:22,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:23,153][root][INFO] - Training Epoch: 3/10, step 504/574 completed (loss: 1.8296178579330444, acc: 0.5555555820465088)
[2024-12-12 02:33:23,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:23,539][root][INFO] - Training Epoch: 3/10, step 505/574 completed (loss: 1.3862743377685547, acc: 0.6037735939025879)
[2024-12-12 02:33:23,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:23,875][root][INFO] - Training Epoch: 3/10, step 506/574 completed (loss: 1.0381429195404053, acc: 0.6551724076271057)
[2024-12-12 02:33:24,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:24,460][root][INFO] - Training Epoch: 3/10, step 507/574 completed (loss: 1.8971058130264282, acc: 0.44144144654273987)
[2024-12-12 02:33:24,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:24,894][root][INFO] - Training Epoch: 3/10, step 508/574 completed (loss: 1.6166865825653076, acc: 0.5633803009986877)
[2024-12-12 02:33:25,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:25,269][root][INFO] - Training Epoch: 3/10, step 509/574 completed (loss: 0.34470364451408386, acc: 0.8999999761581421)
[2024-12-12 02:33:25,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:25,647][root][INFO] - Training Epoch: 3/10, step 510/574 completed (loss: 0.36524778604507446, acc: 0.8999999761581421)
[2024-12-12 02:33:25,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:25,981][root][INFO] - Training Epoch: 3/10, step 511/574 completed (loss: 0.8526233434677124, acc: 0.807692289352417)
[2024-12-12 02:33:27,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:28,669][root][INFO] - Training Epoch: 3/10, step 512/574 completed (loss: 1.9308534860610962, acc: 0.48571428656578064)
[2024-12-12 02:33:28,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:29,434][root][INFO] - Training Epoch: 3/10, step 513/574 completed (loss: 1.7583070993423462, acc: 0.5396825671195984)
[2024-12-12 02:33:29,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:29,789][root][INFO] - Training Epoch: 3/10, step 514/574 completed (loss: 0.9965559244155884, acc: 0.7142857313156128)
[2024-12-12 02:33:29,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:30,172][root][INFO] - Training Epoch: 3/10, step 515/574 completed (loss: 1.219610571861267, acc: 0.6333333253860474)
[2024-12-12 02:33:30,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:30,870][root][INFO] - Training Epoch: 3/10, step 516/574 completed (loss: 1.4075335264205933, acc: 0.6527777910232544)
[2024-12-12 02:33:30,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:31,159][root][INFO] - Training Epoch: 3/10, step 517/574 completed (loss: 0.2540116012096405, acc: 0.9615384340286255)
[2024-12-12 02:33:31,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:31,492][root][INFO] - Training Epoch: 3/10, step 518/574 completed (loss: 0.9182205200195312, acc: 0.6774193644523621)
[2024-12-12 02:33:31,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:31,798][root][INFO] - Training Epoch: 3/10, step 519/574 completed (loss: 1.2018358707427979, acc: 0.699999988079071)
[2024-12-12 02:33:31,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:32,161][root][INFO] - Training Epoch: 3/10, step 520/574 completed (loss: 1.3310155868530273, acc: 0.6296296119689941)
[2024-12-12 02:33:32,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:33,150][root][INFO] - Training Epoch: 3/10, step 521/574 completed (loss: 1.9917080402374268, acc: 0.4237288236618042)
[2024-12-12 02:33:33,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:33,534][root][INFO] - Training Epoch: 3/10, step 522/574 completed (loss: 2.006547212600708, acc: 0.48507463932037354)
[2024-12-12 02:33:33,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:33,930][root][INFO] - Training Epoch: 3/10, step 523/574 completed (loss: 2.000696897506714, acc: 0.45985400676727295)
[2024-12-12 02:33:34,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:34,501][root][INFO] - Training Epoch: 3/10, step 524/574 completed (loss: 1.8493565320968628, acc: 0.5099999904632568)
[2024-12-12 02:33:34,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:34,846][root][INFO] - Training Epoch: 3/10, step 525/574 completed (loss: 1.5972137451171875, acc: 0.5925925970077515)
[2024-12-12 02:33:34,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:35,222][root][INFO] - Training Epoch: 3/10, step 526/574 completed (loss: 1.535266637802124, acc: 0.557692289352417)
[2024-12-12 02:33:35,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:35,603][root][INFO] - Training Epoch: 3/10, step 527/574 completed (loss: 1.3660999536514282, acc: 0.523809552192688)
[2024-12-12 02:33:35,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:35,996][root][INFO] - Training Epoch: 3/10, step 528/574 completed (loss: 2.5988497734069824, acc: 0.26229506731033325)
[2024-12-12 02:33:36,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:36,296][root][INFO] - Training Epoch: 3/10, step 529/574 completed (loss: 1.4296927452087402, acc: 0.5932203531265259)
[2024-12-12 02:33:36,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:36,634][root][INFO] - Training Epoch: 3/10, step 530/574 completed (loss: 2.0679080486297607, acc: 0.5116279125213623)
[2024-12-12 02:33:36,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:36,972][root][INFO] - Training Epoch: 3/10, step 531/574 completed (loss: 1.8135814666748047, acc: 0.5454545617103577)
[2024-12-12 02:33:37,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:37,350][root][INFO] - Training Epoch: 3/10, step 532/574 completed (loss: 2.1000430583953857, acc: 0.43396225571632385)
[2024-12-12 02:33:37,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:37,731][root][INFO] - Training Epoch: 3/10, step 533/574 completed (loss: 1.4445956945419312, acc: 0.6363636255264282)
[2024-12-12 02:33:37,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:38,085][root][INFO] - Training Epoch: 3/10, step 534/574 completed (loss: 0.8506665825843811, acc: 0.7599999904632568)
[2024-12-12 02:33:38,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:38,489][root][INFO] - Training Epoch: 3/10, step 535/574 completed (loss: 0.9399285316467285, acc: 0.699999988079071)
[2024-12-12 02:33:38,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:38,860][root][INFO] - Training Epoch: 3/10, step 536/574 completed (loss: 0.5258237719535828, acc: 0.8181818127632141)
[2024-12-12 02:33:39,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:39,291][root][INFO] - Training Epoch: 3/10, step 537/574 completed (loss: 1.4651339054107666, acc: 0.6769230961799622)
[2024-12-12 02:33:39,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:39,680][root][INFO] - Training Epoch: 3/10, step 538/574 completed (loss: 1.4582494497299194, acc: 0.640625)
[2024-12-12 02:33:39,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:40,099][root][INFO] - Training Epoch: 3/10, step 539/574 completed (loss: 0.9050000905990601, acc: 0.84375)
[2024-12-12 02:33:40,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:40,437][root][INFO] - Training Epoch: 3/10, step 540/574 completed (loss: 1.2336370944976807, acc: 0.6969696879386902)
[2024-12-12 02:33:40,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:40,719][root][INFO] - Training Epoch: 3/10, step 541/574 completed (loss: 0.5197606682777405, acc: 0.8125)
[2024-12-12 02:33:40,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:41,059][root][INFO] - Training Epoch: 3/10, step 542/574 completed (loss: 0.602149486541748, acc: 0.8064516186714172)
[2024-12-12 02:33:41,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:41,399][root][INFO] - Training Epoch: 3/10, step 543/574 completed (loss: 0.2594897747039795, acc: 0.9130434989929199)
[2024-12-12 02:33:41,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:41,760][root][INFO] - Training Epoch: 3/10, step 544/574 completed (loss: 1.370120882987976, acc: 0.6666666865348816)
[2024-12-12 02:33:41,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:42,140][root][INFO] - Training Epoch: 3/10, step 545/574 completed (loss: 0.979267418384552, acc: 0.7317073345184326)
[2024-12-12 02:33:42,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:42,511][root][INFO] - Training Epoch: 3/10, step 546/574 completed (loss: 0.601312518119812, acc: 0.8571428656578064)
[2024-12-12 02:33:42,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:42,847][root][INFO] - Training Epoch: 3/10, step 547/574 completed (loss: 0.903383731842041, acc: 0.8157894611358643)
[2024-12-12 02:33:42,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:43,146][root][INFO] - Training Epoch: 3/10, step 548/574 completed (loss: 0.8288153409957886, acc: 0.8387096524238586)
[2024-12-12 02:33:43,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:43,462][root][INFO] - Training Epoch: 3/10, step 549/574 completed (loss: 0.2592218816280365, acc: 0.9200000166893005)
[2024-12-12 02:33:43,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:43,828][root][INFO] - Training Epoch: 3/10, step 550/574 completed (loss: 0.6175714135169983, acc: 0.7272727489471436)
[2024-12-12 02:33:43,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:44,203][root][INFO] - Training Epoch: 3/10, step 551/574 completed (loss: 0.6965540647506714, acc: 0.8500000238418579)
[2024-12-12 02:33:44,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:44,594][root][INFO] - Training Epoch: 3/10, step 552/574 completed (loss: 1.089271903038025, acc: 0.6285714507102966)
[2024-12-12 02:33:44,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:44,912][root][INFO] - Training Epoch: 3/10, step 553/574 completed (loss: 1.9771817922592163, acc: 0.47445255517959595)
[2024-12-12 02:33:44,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:45,263][root][INFO] - Training Epoch: 3/10, step 554/574 completed (loss: 1.5341731309890747, acc: 0.5862069129943848)
[2024-12-12 02:33:45,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:45,594][root][INFO] - Training Epoch: 3/10, step 555/574 completed (loss: 2.266310691833496, acc: 0.4357142746448517)
[2024-12-12 02:33:45,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:45,965][root][INFO] - Training Epoch: 3/10, step 556/574 completed (loss: 2.085265874862671, acc: 0.3973509967327118)
[2024-12-12 02:33:46,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:46,343][root][INFO] - Training Epoch: 3/10, step 557/574 completed (loss: 1.6519371271133423, acc: 0.5384615659713745)
[2024-12-12 02:33:46,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:46,659][root][INFO] - Training Epoch: 3/10, step 558/574 completed (loss: 0.21561037003993988, acc: 0.9200000166893005)
[2024-12-12 02:33:46,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:47,035][root][INFO] - Training Epoch: 3/10, step 559/574 completed (loss: 0.59637051820755, acc: 0.807692289352417)
[2024-12-12 02:33:47,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:47,400][root][INFO] - Training Epoch: 3/10, step 560/574 completed (loss: 0.30307891964912415, acc: 0.9230769276618958)
[2024-12-12 02:33:47,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:47,775][root][INFO] - Training Epoch: 3/10, step 561/574 completed (loss: 1.4387868642807007, acc: 0.6410256624221802)
[2024-12-12 02:33:47,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:48,166][root][INFO] - Training Epoch: 3/10, step 562/574 completed (loss: 1.366876482963562, acc: 0.5888888835906982)
[2024-12-12 02:33:48,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:48,502][root][INFO] - Training Epoch: 3/10, step 563/574 completed (loss: 1.2322427034378052, acc: 0.649350643157959)
[2024-12-12 02:33:48,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:48,885][root][INFO] - Training Epoch: 3/10, step 564/574 completed (loss: 1.1889054775238037, acc: 0.625)
[2024-12-12 02:33:48,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:49,195][root][INFO] - Training Epoch: 3/10, step 565/574 completed (loss: 1.2015042304992676, acc: 0.6896551847457886)
[2024-12-12 02:33:49,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:49,548][root][INFO] - Training Epoch: 3/10, step 566/574 completed (loss: 1.5237973928451538, acc: 0.511904776096344)
[2024-12-12 02:33:49,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:49,870][root][INFO] - Training Epoch: 3/10, step 567/574 completed (loss: 0.9159888029098511, acc: 0.7631579041481018)
[2024-12-12 02:33:50,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:50,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:51,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:51,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:52,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:52,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:52,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:53,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:53,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:54,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:54,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:54,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:55,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:55,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:56,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:56,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:56,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:57,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:57,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:57,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:57,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:58,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:58,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:59,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:59,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:59,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:33:59,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:00,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:00,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:01,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:01,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:01,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:02,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:02,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:02,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:03,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:03,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:04,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:04,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:04,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:05,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:05,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:05,773][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:06,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:06,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:06,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:07,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:07,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:07,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:08,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:08,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:08,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:09,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:09,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:09,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:10,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:10,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:10,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:10,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:11,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:11,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:12,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:12,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:12,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:13,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:13,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:13,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:14,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:14,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:14,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:15,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:15,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:15,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:16,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:16,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:16,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:17,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:17,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:18,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:18,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:18,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:19,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:19,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:19,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:19,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:20,575][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(5.8667, device='cuda:0') eval_epoch_loss=tensor(1.7693, device='cuda:0') eval_epoch_acc=tensor(0.5660, device='cuda:0')
[2024-12-12 02:34:20,576][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:34:20,576][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:34:20,788][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_3_step_568_loss_1.769289493560791/model.pt
[2024-12-12 02:34:20,791][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:34:20,792][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.5660360455513
[2024-12-12 02:34:20,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:21,172][root][INFO] - Training Epoch: 3/10, step 568/574 completed (loss: 0.6048653721809387, acc: 0.7407407164573669)
[2024-12-12 02:34:21,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:21,590][root][INFO] - Training Epoch: 3/10, step 569/574 completed (loss: 1.7726023197174072, acc: 0.529411792755127)
[2024-12-12 02:34:21,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:21,981][root][INFO] - Training Epoch: 3/10, step 570/574 completed (loss: 1.1161167621612549, acc: 0.6612903475761414)
[2024-12-12 02:34:22,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:22,387][root][INFO] - Training Epoch: 3/10, step 571/574 completed (loss: 1.6285663843154907, acc: 0.5641025900840759)
[2024-12-12 02:34:22,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:22,707][root][INFO] - Training Epoch: 3/10, step 572/574 completed (loss: 2.1842939853668213, acc: 0.3928571343421936)
[2024-12-12 02:34:22,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:23,017][root][INFO] - Training Epoch: 3/10, step 573/574 completed (loss: 1.999459147453308, acc: 0.43396225571632385)
[2024-12-12 02:34:23,414][slam_llm.utils.train_utils][INFO] - Epoch 3: train_perplexity=4.2136, train_epoch_loss=1.4383, epoch time 355.9390222392976s
[2024-12-12 02:34:23,414][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 02:34:23,414][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 16 GB
[2024-12-12 02:34:23,415][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 02:34:23,415][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 9
[2024-12-12 02:34:23,415][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:34:23,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:24,245][root][INFO] - Training Epoch: 4/10, step 0/574 completed (loss: 0.7918134331703186, acc: 0.7777777910232544)
[2024-12-12 02:34:24,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:24,565][root][INFO] - Training Epoch: 4/10, step 1/574 completed (loss: 1.2849512100219727, acc: 0.6800000071525574)
[2024-12-12 02:34:24,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:24,898][root][INFO] - Training Epoch: 4/10, step 2/574 completed (loss: 1.8718208074569702, acc: 0.5945945978164673)
[2024-12-12 02:34:25,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:25,297][root][INFO] - Training Epoch: 4/10, step 3/574 completed (loss: 1.664145827293396, acc: 0.5263158082962036)
[2024-12-12 02:34:25,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:25,676][root][INFO] - Training Epoch: 4/10, step 4/574 completed (loss: 1.6676157712936401, acc: 0.5135135054588318)
[2024-12-12 02:34:25,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:26,045][root][INFO] - Training Epoch: 4/10, step 5/574 completed (loss: 1.652450680732727, acc: 0.4642857015132904)
[2024-12-12 02:34:26,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:26,377][root][INFO] - Training Epoch: 4/10, step 6/574 completed (loss: 1.9497305154800415, acc: 0.5510203838348389)
[2024-12-12 02:34:26,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:26,696][root][INFO] - Training Epoch: 4/10, step 7/574 completed (loss: 1.1872851848602295, acc: 0.699999988079071)
[2024-12-12 02:34:26,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:27,053][root][INFO] - Training Epoch: 4/10, step 8/574 completed (loss: 0.15964531898498535, acc: 0.9545454382896423)
[2024-12-12 02:34:27,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:27,410][root][INFO] - Training Epoch: 4/10, step 9/574 completed (loss: 0.40671810507774353, acc: 0.8846153616905212)
[2024-12-12 02:34:27,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:27,751][root][INFO] - Training Epoch: 4/10, step 10/574 completed (loss: 0.5914086103439331, acc: 0.8888888955116272)
[2024-12-12 02:34:27,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:28,106][root][INFO] - Training Epoch: 4/10, step 11/574 completed (loss: 1.5228848457336426, acc: 0.6666666865348816)
[2024-12-12 02:34:28,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:28,524][root][INFO] - Training Epoch: 4/10, step 12/574 completed (loss: 1.1917340755462646, acc: 0.7272727489471436)
[2024-12-12 02:34:28,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:28,892][root][INFO] - Training Epoch: 4/10, step 13/574 completed (loss: 1.4116332530975342, acc: 0.6086956262588501)
[2024-12-12 02:34:28,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:29,210][root][INFO] - Training Epoch: 4/10, step 14/574 completed (loss: 1.6961225271224976, acc: 0.5098039507865906)
[2024-12-12 02:34:29,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:29,518][root][INFO] - Training Epoch: 4/10, step 15/574 completed (loss: 1.4408823251724243, acc: 0.5510203838348389)
[2024-12-12 02:34:29,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:29,876][root][INFO] - Training Epoch: 4/10, step 16/574 completed (loss: 0.45181670784950256, acc: 0.8421052694320679)
[2024-12-12 02:34:29,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:30,229][root][INFO] - Training Epoch: 4/10, step 17/574 completed (loss: 0.7125501036643982, acc: 0.7083333134651184)
[2024-12-12 02:34:30,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:30,542][root][INFO] - Training Epoch: 4/10, step 18/574 completed (loss: 1.5698704719543457, acc: 0.5)
[2024-12-12 02:34:30,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:30,834][root][INFO] - Training Epoch: 4/10, step 19/574 completed (loss: 0.6967771649360657, acc: 0.7894737124443054)
[2024-12-12 02:34:30,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:31,196][root][INFO] - Training Epoch: 4/10, step 20/574 completed (loss: 0.9051523208618164, acc: 0.7692307829856873)
[2024-12-12 02:34:31,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:31,550][root][INFO] - Training Epoch: 4/10, step 21/574 completed (loss: 0.9707574248313904, acc: 0.6896551847457886)
[2024-12-12 02:34:31,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:31,929][root][INFO] - Training Epoch: 4/10, step 22/574 completed (loss: 0.741678774356842, acc: 0.6399999856948853)
[2024-12-12 02:34:32,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:32,308][root][INFO] - Training Epoch: 4/10, step 23/574 completed (loss: 0.4902186691761017, acc: 0.8571428656578064)
[2024-12-12 02:34:32,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:32,692][root][INFO] - Training Epoch: 4/10, step 24/574 completed (loss: 0.43961530923843384, acc: 0.8125)
[2024-12-12 02:34:32,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:33,090][root][INFO] - Training Epoch: 4/10, step 25/574 completed (loss: 2.0935754776000977, acc: 0.3962264060974121)
[2024-12-12 02:34:33,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:33,446][root][INFO] - Training Epoch: 4/10, step 26/574 completed (loss: 1.9791065454483032, acc: 0.4109589159488678)
[2024-12-12 02:34:33,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:34,686][root][INFO] - Training Epoch: 4/10, step 27/574 completed (loss: 2.0587921142578125, acc: 0.4387351870536804)
[2024-12-12 02:34:34,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:35,046][root][INFO] - Training Epoch: 4/10, step 28/574 completed (loss: 1.4422857761383057, acc: 0.6279069781303406)
[2024-12-12 02:34:35,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:35,386][root][INFO] - Training Epoch: 4/10, step 29/574 completed (loss: 1.8637217283248901, acc: 0.5060241222381592)
[2024-12-12 02:34:35,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:35,741][root][INFO] - Training Epoch: 4/10, step 30/574 completed (loss: 1.6944472789764404, acc: 0.5061728358268738)
[2024-12-12 02:34:35,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:36,106][root][INFO] - Training Epoch: 4/10, step 31/574 completed (loss: 1.8928669691085815, acc: 0.5357142686843872)
[2024-12-12 02:34:36,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:36,485][root][INFO] - Training Epoch: 4/10, step 32/574 completed (loss: 0.9055222272872925, acc: 0.7037037014961243)
[2024-12-12 02:34:36,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:36,887][root][INFO] - Training Epoch: 4/10, step 33/574 completed (loss: 0.5086367130279541, acc: 0.8260869383811951)
[2024-12-12 02:34:37,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:37,285][root][INFO] - Training Epoch: 4/10, step 34/574 completed (loss: 1.817368984222412, acc: 0.5210084319114685)
[2024-12-12 02:34:37,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:37,692][root][INFO] - Training Epoch: 4/10, step 35/574 completed (loss: 1.3451281785964966, acc: 0.6393442749977112)
[2024-12-12 02:34:37,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:38,072][root][INFO] - Training Epoch: 4/10, step 36/574 completed (loss: 1.867897391319275, acc: 0.4920634925365448)
[2024-12-12 02:34:38,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:38,416][root][INFO] - Training Epoch: 4/10, step 37/574 completed (loss: 1.6826902627944946, acc: 0.49152541160583496)
[2024-12-12 02:34:38,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:38,799][root][INFO] - Training Epoch: 4/10, step 38/574 completed (loss: 1.3369454145431519, acc: 0.5977011322975159)
[2024-12-12 02:34:38,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:39,161][root][INFO] - Training Epoch: 4/10, step 39/574 completed (loss: 0.4491645395755768, acc: 0.8571428656578064)
[2024-12-12 02:34:39,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:39,529][root][INFO] - Training Epoch: 4/10, step 40/574 completed (loss: 0.9763678908348083, acc: 0.692307710647583)
[2024-12-12 02:34:39,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:39,925][root][INFO] - Training Epoch: 4/10, step 41/574 completed (loss: 2.1358871459960938, acc: 0.4864864945411682)
[2024-12-12 02:34:40,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:40,276][root][INFO] - Training Epoch: 4/10, step 42/574 completed (loss: 1.7053940296173096, acc: 0.4769230782985687)
[2024-12-12 02:34:40,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:40,715][root][INFO] - Training Epoch: 4/10, step 43/574 completed (loss: 1.9606629610061646, acc: 0.5555555820465088)
[2024-12-12 02:34:40,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:41,135][root][INFO] - Training Epoch: 4/10, step 44/574 completed (loss: 1.5141090154647827, acc: 0.5773195624351501)
[2024-12-12 02:34:41,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:41,529][root][INFO] - Training Epoch: 4/10, step 45/574 completed (loss: 1.9211876392364502, acc: 0.5220588445663452)
[2024-12-12 02:34:41,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:41,867][root][INFO] - Training Epoch: 4/10, step 46/574 completed (loss: 0.5563750863075256, acc: 0.8461538553237915)
[2024-12-12 02:34:41,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:42,193][root][INFO] - Training Epoch: 4/10, step 47/574 completed (loss: 0.3701826333999634, acc: 0.9629629850387573)
[2024-12-12 02:34:42,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:42,516][root][INFO] - Training Epoch: 4/10, step 48/574 completed (loss: 0.800932765007019, acc: 0.75)
[2024-12-12 02:34:42,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:42,859][root][INFO] - Training Epoch: 4/10, step 49/574 completed (loss: 0.6057037115097046, acc: 0.8333333134651184)
[2024-12-12 02:34:42,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:43,254][root][INFO] - Training Epoch: 4/10, step 50/574 completed (loss: 1.0322681665420532, acc: 0.6666666865348816)
[2024-12-12 02:34:43,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:43,588][root][INFO] - Training Epoch: 4/10, step 51/574 completed (loss: 1.244757056236267, acc: 0.682539701461792)
[2024-12-12 02:34:43,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:43,939][root][INFO] - Training Epoch: 4/10, step 52/574 completed (loss: 1.8897199630737305, acc: 0.49295774102211)
[2024-12-12 02:34:44,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:44,383][root][INFO] - Training Epoch: 4/10, step 53/574 completed (loss: 2.1488029956817627, acc: 0.46000000834465027)
[2024-12-12 02:34:44,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:44,718][root][INFO] - Training Epoch: 4/10, step 54/574 completed (loss: 1.025991439819336, acc: 0.7297297120094299)
[2024-12-12 02:34:44,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:45,127][root][INFO] - Training Epoch: 4/10, step 55/574 completed (loss: 0.18845565617084503, acc: 0.9615384340286255)
[2024-12-12 02:34:46,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:48,110][root][INFO] - Training Epoch: 4/10, step 56/574 completed (loss: 1.7699894905090332, acc: 0.5358361601829529)
[2024-12-12 02:34:48,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:49,468][root][INFO] - Training Epoch: 4/10, step 57/574 completed (loss: 2.3495311737060547, acc: 0.40305009484291077)
[2024-12-12 02:34:49,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:50,109][root][INFO] - Training Epoch: 4/10, step 58/574 completed (loss: 1.8775042295455933, acc: 0.5227272510528564)
[2024-12-12 02:34:50,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:50,687][root][INFO] - Training Epoch: 4/10, step 59/574 completed (loss: 2.0523571968078613, acc: 0.5147058963775635)
[2024-12-12 02:34:50,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:51,261][root][INFO] - Training Epoch: 4/10, step 60/574 completed (loss: 2.2427306175231934, acc: 0.4275362193584442)
[2024-12-12 02:34:51,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:51,686][root][INFO] - Training Epoch: 4/10, step 61/574 completed (loss: 1.4547227621078491, acc: 0.637499988079071)
[2024-12-12 02:34:51,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:52,040][root][INFO] - Training Epoch: 4/10, step 62/574 completed (loss: 0.8913236260414124, acc: 0.7352941036224365)
[2024-12-12 02:34:52,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:52,362][root][INFO] - Training Epoch: 4/10, step 63/574 completed (loss: 1.2181966304779053, acc: 0.6666666865348816)
[2024-12-12 02:34:52,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:52,740][root][INFO] - Training Epoch: 4/10, step 64/574 completed (loss: 1.2321635484695435, acc: 0.640625)
[2024-12-12 02:34:52,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:53,124][root][INFO] - Training Epoch: 4/10, step 65/574 completed (loss: 0.41321471333503723, acc: 0.931034505367279)
[2024-12-12 02:34:53,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:53,479][root][INFO] - Training Epoch: 4/10, step 66/574 completed (loss: 2.0160105228424072, acc: 0.5)
[2024-12-12 02:34:53,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:53,867][root][INFO] - Training Epoch: 4/10, step 67/574 completed (loss: 1.8046375513076782, acc: 0.5)
[2024-12-12 02:34:53,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:54,263][root][INFO] - Training Epoch: 4/10, step 68/574 completed (loss: 0.24957071244716644, acc: 0.8799999952316284)
[2024-12-12 02:34:54,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:54,642][root][INFO] - Training Epoch: 4/10, step 69/574 completed (loss: 0.9872349500656128, acc: 0.7222222089767456)
[2024-12-12 02:34:54,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:55,023][root][INFO] - Training Epoch: 4/10, step 70/574 completed (loss: 0.9725983142852783, acc: 0.7575757503509521)
[2024-12-12 02:34:55,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:55,403][root][INFO] - Training Epoch: 4/10, step 71/574 completed (loss: 1.868436336517334, acc: 0.4852941036224365)
[2024-12-12 02:34:55,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:55,752][root][INFO] - Training Epoch: 4/10, step 72/574 completed (loss: 1.9084445238113403, acc: 0.4682539701461792)
[2024-12-12 02:34:55,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:56,139][root][INFO] - Training Epoch: 4/10, step 73/574 completed (loss: 2.162668466567993, acc: 0.4000000059604645)
[2024-12-12 02:34:56,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:56,480][root][INFO] - Training Epoch: 4/10, step 74/574 completed (loss: 1.7738401889801025, acc: 0.5)
[2024-12-12 02:34:56,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:56,840][root][INFO] - Training Epoch: 4/10, step 75/574 completed (loss: 2.2707948684692383, acc: 0.35820895433425903)
[2024-12-12 02:34:56,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:57,228][root][INFO] - Training Epoch: 4/10, step 76/574 completed (loss: 2.1410343647003174, acc: 0.43795621395111084)
[2024-12-12 02:34:57,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:57,614][root][INFO] - Training Epoch: 4/10, step 77/574 completed (loss: 0.4649375379085541, acc: 0.8571428656578064)
[2024-12-12 02:34:57,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:57,991][root][INFO] - Training Epoch: 4/10, step 78/574 completed (loss: 0.26386553049087524, acc: 0.9583333134651184)
[2024-12-12 02:34:58,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:58,375][root][INFO] - Training Epoch: 4/10, step 79/574 completed (loss: 0.6735379099845886, acc: 0.8484848737716675)
[2024-12-12 02:34:58,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:58,720][root][INFO] - Training Epoch: 4/10, step 80/574 completed (loss: 0.45815977454185486, acc: 0.8461538553237915)
[2024-12-12 02:34:58,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:59,072][root][INFO] - Training Epoch: 4/10, step 81/574 completed (loss: 1.5829355716705322, acc: 0.5769230723381042)
[2024-12-12 02:34:59,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:59,469][root][INFO] - Training Epoch: 4/10, step 82/574 completed (loss: 1.8972358703613281, acc: 0.4615384638309479)
[2024-12-12 02:34:59,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:34:59,814][root][INFO] - Training Epoch: 4/10, step 83/574 completed (loss: 0.9766577482223511, acc: 0.75)
[2024-12-12 02:34:59,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:00,137][root][INFO] - Training Epoch: 4/10, step 84/574 completed (loss: 1.663474202156067, acc: 0.5797101259231567)
[2024-12-12 02:35:00,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:00,501][root][INFO] - Training Epoch: 4/10, step 85/574 completed (loss: 1.1949104070663452, acc: 0.6200000047683716)
[2024-12-12 02:35:00,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:00,837][root][INFO] - Training Epoch: 4/10, step 86/574 completed (loss: 0.7992295026779175, acc: 0.8260869383811951)
[2024-12-12 02:35:00,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:01,305][root][INFO] - Training Epoch: 4/10, step 87/574 completed (loss: 1.7327525615692139, acc: 0.5600000023841858)
[2024-12-12 02:35:01,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:01,687][root][INFO] - Training Epoch: 4/10, step 88/574 completed (loss: 1.8783643245697021, acc: 0.4757281541824341)
[2024-12-12 02:35:02,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:02,784][root][INFO] - Training Epoch: 4/10, step 89/574 completed (loss: 1.7732200622558594, acc: 0.553398072719574)
[2024-12-12 02:35:02,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:03,606][root][INFO] - Training Epoch: 4/10, step 90/574 completed (loss: 1.9764429330825806, acc: 0.4516128897666931)
[2024-12-12 02:35:03,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:04,411][root][INFO] - Training Epoch: 4/10, step 91/574 completed (loss: 1.7771496772766113, acc: 0.5603448152542114)
[2024-12-12 02:35:04,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:05,156][root][INFO] - Training Epoch: 4/10, step 92/574 completed (loss: 1.4214106798171997, acc: 0.6000000238418579)
[2024-12-12 02:35:05,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:06,146][root][INFO] - Training Epoch: 4/10, step 93/574 completed (loss: 2.1858444213867188, acc: 0.42574256658554077)
[2024-12-12 02:35:06,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:06,477][root][INFO] - Training Epoch: 4/10, step 94/574 completed (loss: 2.0901873111724854, acc: 0.4354838728904724)
[2024-12-12 02:35:06,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:06,902][root][INFO] - Training Epoch: 4/10, step 95/574 completed (loss: 2.1640737056732178, acc: 0.4492753744125366)
[2024-12-12 02:35:07,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:07,269][root][INFO] - Training Epoch: 4/10, step 96/574 completed (loss: 2.2028844356536865, acc: 0.38655462861061096)
[2024-12-12 02:35:07,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:07,598][root][INFO] - Training Epoch: 4/10, step 97/574 completed (loss: 2.2300608158111572, acc: 0.3461538553237915)
[2024-12-12 02:35:07,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:07,968][root][INFO] - Training Epoch: 4/10, step 98/574 completed (loss: 2.196073532104492, acc: 0.38686132431030273)
[2024-12-12 02:35:08,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:08,324][root][INFO] - Training Epoch: 4/10, step 99/574 completed (loss: 2.099790573120117, acc: 0.4029850661754608)
[2024-12-12 02:35:08,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:08,705][root][INFO] - Training Epoch: 4/10, step 100/574 completed (loss: 0.6296564340591431, acc: 0.800000011920929)
[2024-12-12 02:35:08,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:09,023][root][INFO] - Training Epoch: 4/10, step 101/574 completed (loss: 0.14845797419548035, acc: 1.0)
[2024-12-12 02:35:09,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:09,417][root][INFO] - Training Epoch: 4/10, step 102/574 completed (loss: 0.43464818596839905, acc: 0.95652174949646)
[2024-12-12 02:35:09,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:09,754][root][INFO] - Training Epoch: 4/10, step 103/574 completed (loss: 0.9647922515869141, acc: 0.75)
[2024-12-12 02:35:09,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:10,053][root][INFO] - Training Epoch: 4/10, step 104/574 completed (loss: 1.6327944993972778, acc: 0.5517241358757019)
[2024-12-12 02:35:10,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:10,356][root][INFO] - Training Epoch: 4/10, step 105/574 completed (loss: 1.0420500040054321, acc: 0.6744186282157898)
[2024-12-12 02:35:10,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:10,749][root][INFO] - Training Epoch: 4/10, step 106/574 completed (loss: 0.6860328912734985, acc: 0.7599999904632568)
[2024-12-12 02:35:10,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:11,129][root][INFO] - Training Epoch: 4/10, step 107/574 completed (loss: 0.1856718212366104, acc: 0.8823529481887817)
[2024-12-12 02:35:11,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:11,510][root][INFO] - Training Epoch: 4/10, step 108/574 completed (loss: 0.020658234134316444, acc: 1.0)
[2024-12-12 02:35:11,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:11,848][root][INFO] - Training Epoch: 4/10, step 109/574 completed (loss: 1.1086336374282837, acc: 0.6904761791229248)
[2024-12-12 02:35:11,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:12,200][root][INFO] - Training Epoch: 4/10, step 110/574 completed (loss: 1.6090576648712158, acc: 0.5538461804389954)
[2024-12-12 02:35:12,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:12,594][root][INFO] - Training Epoch: 4/10, step 111/574 completed (loss: 1.5450210571289062, acc: 0.6491228342056274)
[2024-12-12 02:35:12,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:12,974][root][INFO] - Training Epoch: 4/10, step 112/574 completed (loss: 1.3238494396209717, acc: 0.6140350699424744)
[2024-12-12 02:35:13,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:13,344][root][INFO] - Training Epoch: 4/10, step 113/574 completed (loss: 1.6808136701583862, acc: 0.5384615659713745)
[2024-12-12 02:35:13,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:13,713][root][INFO] - Training Epoch: 4/10, step 114/574 completed (loss: 1.101101279258728, acc: 0.6938775777816772)
[2024-12-12 02:35:13,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:14,081][root][INFO] - Training Epoch: 4/10, step 115/574 completed (loss: 0.05852032080292702, acc: 1.0)
[2024-12-12 02:35:14,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:14,480][root][INFO] - Training Epoch: 4/10, step 116/574 completed (loss: 1.6048883199691772, acc: 0.6190476417541504)
[2024-12-12 02:35:14,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:14,856][root][INFO] - Training Epoch: 4/10, step 117/574 completed (loss: 1.7148205041885376, acc: 0.5447154641151428)
[2024-12-12 02:35:14,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:15,210][root][INFO] - Training Epoch: 4/10, step 118/574 completed (loss: 1.3785489797592163, acc: 0.6290322542190552)
[2024-12-12 02:35:15,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:16,078][root][INFO] - Training Epoch: 4/10, step 119/574 completed (loss: 1.9536718130111694, acc: 0.4790874421596527)
[2024-12-12 02:35:16,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:16,423][root][INFO] - Training Epoch: 4/10, step 120/574 completed (loss: 1.2833836078643799, acc: 0.6666666865348816)
[2024-12-12 02:35:16,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:16,829][root][INFO] - Training Epoch: 4/10, step 121/574 completed (loss: 1.2493579387664795, acc: 0.6346153616905212)
[2024-12-12 02:35:16,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:17,214][root][INFO] - Training Epoch: 4/10, step 122/574 completed (loss: 0.5559481978416443, acc: 0.875)
[2024-12-12 02:35:17,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:17,578][root][INFO] - Training Epoch: 4/10, step 123/574 completed (loss: 0.4424903392791748, acc: 0.7894737124443054)
[2024-12-12 02:35:17,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:17,948][root][INFO] - Training Epoch: 4/10, step 124/574 completed (loss: 1.9598782062530518, acc: 0.4601227045059204)
[2024-12-12 02:35:18,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:18,328][root][INFO] - Training Epoch: 4/10, step 125/574 completed (loss: 1.662520170211792, acc: 0.5416666865348816)
[2024-12-12 02:35:18,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:18,701][root][INFO] - Training Epoch: 4/10, step 126/574 completed (loss: 2.0339784622192383, acc: 0.42500001192092896)
[2024-12-12 02:35:18,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:19,080][root][INFO] - Training Epoch: 4/10, step 127/574 completed (loss: 1.9646985530853271, acc: 0.4523809552192688)
[2024-12-12 02:35:19,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:19,439][root][INFO] - Training Epoch: 4/10, step 128/574 completed (loss: 1.8411105871200562, acc: 0.5025641322135925)
[2024-12-12 02:35:19,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:19,838][root][INFO] - Training Epoch: 4/10, step 129/574 completed (loss: 1.6834360361099243, acc: 0.5073529481887817)
[2024-12-12 02:35:19,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:20,183][root][INFO] - Training Epoch: 4/10, step 130/574 completed (loss: 0.8187245726585388, acc: 0.7692307829856873)
[2024-12-12 02:35:20,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:20,506][root][INFO] - Training Epoch: 4/10, step 131/574 completed (loss: 0.3516942858695984, acc: 0.9130434989929199)
[2024-12-12 02:35:20,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:20,862][root][INFO] - Training Epoch: 4/10, step 132/574 completed (loss: 0.8656947612762451, acc: 0.75)
[2024-12-12 02:35:20,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:21,175][root][INFO] - Training Epoch: 4/10, step 133/574 completed (loss: 1.1348133087158203, acc: 0.6086956262588501)
[2024-12-12 02:35:21,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:21,493][root][INFO] - Training Epoch: 4/10, step 134/574 completed (loss: 1.1340067386627197, acc: 0.6857143044471741)
[2024-12-12 02:35:21,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:21,802][root][INFO] - Training Epoch: 4/10, step 135/574 completed (loss: 0.6996886730194092, acc: 0.8461538553237915)
[2024-12-12 02:35:21,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:22,137][root][INFO] - Training Epoch: 4/10, step 136/574 completed (loss: 1.4436750411987305, acc: 0.5714285969734192)
[2024-12-12 02:35:22,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:23,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:23,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:23,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:24,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:24,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:25,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:25,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:25,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:26,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:26,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:26,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:27,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:27,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:27,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:28,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:28,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:28,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:29,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:29,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:29,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:30,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:30,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:30,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:31,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:31,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:31,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:32,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:32,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:32,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:33,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:33,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:33,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:34,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:34,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:34,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:35,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:35,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:35,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:36,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:36,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:37,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:37,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:37,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:37,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:38,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:38,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:38,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:39,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:39,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:39,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:40,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:40,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:40,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:41,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:41,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:41,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:41,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:42,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:42,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:42,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:43,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:43,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:43,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:44,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:44,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:44,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:45,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:45,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:46,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:46,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:46,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:47,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:47,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:47,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:47,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:48,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:48,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:48,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:49,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:49,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:49,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:50,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:50,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:50,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:51,298][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(5.1581, device='cuda:0') eval_epoch_loss=tensor(1.6406, device='cuda:0') eval_epoch_acc=tensor(0.5711, device='cuda:0')
[2024-12-12 02:35:51,300][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:35:51,300][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:35:51,529][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_4_step_137_loss_1.6405664682388306/model.pt
[2024-12-12 02:35:51,534][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:35:51,535][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 4 is 1.6405664682388306
[2024-12-12 02:35:51,535][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.5711145997047424
[2024-12-12 02:35:51,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:51,932][root][INFO] - Training Epoch: 4/10, step 137/574 completed (loss: 0.9025463461875916, acc: 0.699999988079071)
[2024-12-12 02:35:52,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:52,316][root][INFO] - Training Epoch: 4/10, step 138/574 completed (loss: 0.8495012521743774, acc: 0.782608687877655)
[2024-12-12 02:35:52,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:52,651][root][INFO] - Training Epoch: 4/10, step 139/574 completed (loss: 0.9953708648681641, acc: 0.761904776096344)
[2024-12-12 02:35:52,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:53,033][root][INFO] - Training Epoch: 4/10, step 140/574 completed (loss: 1.1089003086090088, acc: 0.5769230723381042)
[2024-12-12 02:35:53,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:53,392][root][INFO] - Training Epoch: 4/10, step 141/574 completed (loss: 1.6192811727523804, acc: 0.4838709533214569)
[2024-12-12 02:35:53,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:53,742][root][INFO] - Training Epoch: 4/10, step 142/574 completed (loss: 1.5513105392456055, acc: 0.5675675868988037)
[2024-12-12 02:35:53,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:54,266][root][INFO] - Training Epoch: 4/10, step 143/574 completed (loss: 1.7114287614822388, acc: 0.41228070855140686)
[2024-12-12 02:35:54,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:54,683][root][INFO] - Training Epoch: 4/10, step 144/574 completed (loss: 1.551275372505188, acc: 0.5671641826629639)
[2024-12-12 02:35:54,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:55,084][root][INFO] - Training Epoch: 4/10, step 145/574 completed (loss: 2.106421709060669, acc: 0.37755101919174194)
[2024-12-12 02:35:55,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:55,518][root][INFO] - Training Epoch: 4/10, step 146/574 completed (loss: 1.8567149639129639, acc: 0.42553192377090454)
[2024-12-12 02:35:55,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:55,881][root][INFO] - Training Epoch: 4/10, step 147/574 completed (loss: 1.532763957977295, acc: 0.5714285969734192)
[2024-12-12 02:35:55,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:56,244][root][INFO] - Training Epoch: 4/10, step 148/574 completed (loss: 1.9949758052825928, acc: 0.4642857015132904)
[2024-12-12 02:35:56,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:56,616][root][INFO] - Training Epoch: 4/10, step 149/574 completed (loss: 1.4677789211273193, acc: 0.739130437374115)
[2024-12-12 02:35:56,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:56,957][root][INFO] - Training Epoch: 4/10, step 150/574 completed (loss: 1.3178069591522217, acc: 0.6206896305084229)
[2024-12-12 02:35:57,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:57,303][root][INFO] - Training Epoch: 4/10, step 151/574 completed (loss: 1.7397868633270264, acc: 0.54347825050354)
[2024-12-12 02:35:57,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:57,648][root][INFO] - Training Epoch: 4/10, step 152/574 completed (loss: 1.5888614654541016, acc: 0.5254237055778503)
[2024-12-12 02:35:57,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:58,011][root][INFO] - Training Epoch: 4/10, step 153/574 completed (loss: 1.9661173820495605, acc: 0.4385964870452881)
[2024-12-12 02:35:58,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:58,386][root][INFO] - Training Epoch: 4/10, step 154/574 completed (loss: 1.6872800588607788, acc: 0.5810810923576355)
[2024-12-12 02:35:58,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:58,731][root][INFO] - Training Epoch: 4/10, step 155/574 completed (loss: 1.2047134637832642, acc: 0.7142857313156128)
[2024-12-12 02:35:58,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:59,087][root][INFO] - Training Epoch: 4/10, step 156/574 completed (loss: 1.0267966985702515, acc: 0.739130437374115)
[2024-12-12 02:35:59,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:35:59,422][root][INFO] - Training Epoch: 4/10, step 157/574 completed (loss: 1.2380157709121704, acc: 0.6842105388641357)
[2024-12-12 02:36:00,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:01,056][root][INFO] - Training Epoch: 4/10, step 158/574 completed (loss: 1.3450216054916382, acc: 0.6216216087341309)
[2024-12-12 02:36:01,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:01,369][root][INFO] - Training Epoch: 4/10, step 159/574 completed (loss: 1.7637536525726318, acc: 0.5)
[2024-12-12 02:36:01,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:01,770][root][INFO] - Training Epoch: 4/10, step 160/574 completed (loss: 1.4572594165802002, acc: 0.604651153087616)
[2024-12-12 02:36:01,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:02,357][root][INFO] - Training Epoch: 4/10, step 161/574 completed (loss: 1.3665021657943726, acc: 0.5764706134796143)
[2024-12-12 02:36:02,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:02,911][root][INFO] - Training Epoch: 4/10, step 162/574 completed (loss: 1.8542231321334839, acc: 0.483146071434021)
[2024-12-12 02:36:02,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:03,223][root][INFO] - Training Epoch: 4/10, step 163/574 completed (loss: 1.1865965127944946, acc: 0.6590909361839294)
[2024-12-12 02:36:03,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:03,585][root][INFO] - Training Epoch: 4/10, step 164/574 completed (loss: 1.0603195428848267, acc: 0.7142857313156128)
[2024-12-12 02:36:03,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:03,918][root][INFO] - Training Epoch: 4/10, step 165/574 completed (loss: 1.16248619556427, acc: 0.6551724076271057)
[2024-12-12 02:36:04,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:04,281][root][INFO] - Training Epoch: 4/10, step 166/574 completed (loss: 1.0370149612426758, acc: 0.6938775777816772)
[2024-12-12 02:36:04,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:04,620][root][INFO] - Training Epoch: 4/10, step 167/574 completed (loss: 1.3227136135101318, acc: 0.5799999833106995)
[2024-12-12 02:36:04,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:05,016][root][INFO] - Training Epoch: 4/10, step 168/574 completed (loss: 1.470664143562317, acc: 0.6111111044883728)
[2024-12-12 02:36:05,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:05,343][root][INFO] - Training Epoch: 4/10, step 169/574 completed (loss: 1.6843879222869873, acc: 0.5784313678741455)
[2024-12-12 02:36:05,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:06,374][root][INFO] - Training Epoch: 4/10, step 170/574 completed (loss: 2.2358932495117188, acc: 0.45205479860305786)
[2024-12-12 02:36:06,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:06,631][root][INFO] - Training Epoch: 4/10, step 171/574 completed (loss: 0.6911935210227966, acc: 0.8333333134651184)
[2024-12-12 02:36:06,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:07,023][root][INFO] - Training Epoch: 4/10, step 172/574 completed (loss: 0.7719595432281494, acc: 0.7407407164573669)
[2024-12-12 02:36:07,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:07,360][root][INFO] - Training Epoch: 4/10, step 173/574 completed (loss: 0.7099981904029846, acc: 0.7857142686843872)
[2024-12-12 02:36:07,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:07,904][root][INFO] - Training Epoch: 4/10, step 174/574 completed (loss: 1.5713914632797241, acc: 0.6017699241638184)
[2024-12-12 02:36:07,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:08,258][root][INFO] - Training Epoch: 4/10, step 175/574 completed (loss: 1.5161552429199219, acc: 0.5797101259231567)
[2024-12-12 02:36:08,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:08,611][root][INFO] - Training Epoch: 4/10, step 176/574 completed (loss: 1.4751147031784058, acc: 0.5681818127632141)
[2024-12-12 02:36:08,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:09,523][root][INFO] - Training Epoch: 4/10, step 177/574 completed (loss: 2.164961576461792, acc: 0.4351145029067993)
[2024-12-12 02:36:09,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:10,196][root][INFO] - Training Epoch: 4/10, step 178/574 completed (loss: 1.9658879041671753, acc: 0.4592592716217041)
[2024-12-12 02:36:10,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:10,515][root][INFO] - Training Epoch: 4/10, step 179/574 completed (loss: 1.3258981704711914, acc: 0.6393442749977112)
[2024-12-12 02:36:10,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:10,804][root][INFO] - Training Epoch: 4/10, step 180/574 completed (loss: 0.41244983673095703, acc: 0.9166666865348816)
[2024-12-12 02:36:10,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:11,101][root][INFO] - Training Epoch: 4/10, step 181/574 completed (loss: 0.6204104423522949, acc: 0.8399999737739563)
[2024-12-12 02:36:11,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:11,446][root][INFO] - Training Epoch: 4/10, step 182/574 completed (loss: 0.4884508550167084, acc: 0.8214285969734192)
[2024-12-12 02:36:11,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:11,780][root][INFO] - Training Epoch: 4/10, step 183/574 completed (loss: 1.7613136768341064, acc: 0.47560974955558777)
[2024-12-12 02:36:11,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:12,118][root][INFO] - Training Epoch: 4/10, step 184/574 completed (loss: 2.076779842376709, acc: 0.43202418088912964)
[2024-12-12 02:36:12,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:12,476][root][INFO] - Training Epoch: 4/10, step 185/574 completed (loss: 2.1730737686157227, acc: 0.39193084836006165)
[2024-12-12 02:36:12,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:12,956][root][INFO] - Training Epoch: 4/10, step 186/574 completed (loss: 2.2239632606506348, acc: 0.4156250059604645)
[2024-12-12 02:36:13,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:13,484][root][INFO] - Training Epoch: 4/10, step 187/574 completed (loss: 2.086557388305664, acc: 0.42964354157447815)
[2024-12-12 02:36:13,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:13,881][root][INFO] - Training Epoch: 4/10, step 188/574 completed (loss: 2.051767349243164, acc: 0.44483986496925354)
[2024-12-12 02:36:13,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:14,188][root][INFO] - Training Epoch: 4/10, step 189/574 completed (loss: 1.1321876049041748, acc: 0.6800000071525574)
[2024-12-12 02:36:14,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:14,738][root][INFO] - Training Epoch: 4/10, step 190/574 completed (loss: 2.09438419342041, acc: 0.39534884691238403)
[2024-12-12 02:36:14,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:15,546][root][INFO] - Training Epoch: 4/10, step 191/574 completed (loss: 1.8318581581115723, acc: 0.5396825671195984)
[2024-12-12 02:36:15,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:16,464][root][INFO] - Training Epoch: 4/10, step 192/574 completed (loss: 1.9263279438018799, acc: 0.46212121844291687)
[2024-12-12 02:36:16,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:17,206][root][INFO] - Training Epoch: 4/10, step 193/574 completed (loss: 1.504348874092102, acc: 0.6000000238418579)
[2024-12-12 02:36:17,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:18,284][root][INFO] - Training Epoch: 4/10, step 194/574 completed (loss: 1.5540777444839478, acc: 0.5864197611808777)
[2024-12-12 02:36:18,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:19,243][root][INFO] - Training Epoch: 4/10, step 195/574 completed (loss: 1.2197279930114746, acc: 0.6935483813285828)
[2024-12-12 02:36:19,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:19,617][root][INFO] - Training Epoch: 4/10, step 196/574 completed (loss: 0.37510058283805847, acc: 0.8571428656578064)
[2024-12-12 02:36:19,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:19,989][root][INFO] - Training Epoch: 4/10, step 197/574 completed (loss: 1.2291213274002075, acc: 0.699999988079071)
[2024-12-12 02:36:20,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:20,382][root][INFO] - Training Epoch: 4/10, step 198/574 completed (loss: 1.5364855527877808, acc: 0.6176470518112183)
[2024-12-12 02:36:20,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:20,717][root][INFO] - Training Epoch: 4/10, step 199/574 completed (loss: 1.8204964399337769, acc: 0.5073529481887817)
[2024-12-12 02:36:20,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:21,047][root][INFO] - Training Epoch: 4/10, step 200/574 completed (loss: 2.010202646255493, acc: 0.4830508530139923)
[2024-12-12 02:36:21,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:21,398][root][INFO] - Training Epoch: 4/10, step 201/574 completed (loss: 2.0929670333862305, acc: 0.44029849767684937)
[2024-12-12 02:36:21,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:21,786][root][INFO] - Training Epoch: 4/10, step 202/574 completed (loss: 1.9026895761489868, acc: 0.4757281541824341)
[2024-12-12 02:36:21,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:22,169][root][INFO] - Training Epoch: 4/10, step 203/574 completed (loss: 1.468744158744812, acc: 0.6190476417541504)
[2024-12-12 02:36:22,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:22,570][root][INFO] - Training Epoch: 4/10, step 204/574 completed (loss: 1.5580077171325684, acc: 0.5714285969734192)
[2024-12-12 02:36:22,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:22,972][root][INFO] - Training Epoch: 4/10, step 205/574 completed (loss: 1.9638547897338867, acc: 0.46188339591026306)
[2024-12-12 02:36:23,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:23,371][root][INFO] - Training Epoch: 4/10, step 206/574 completed (loss: 1.9636460542678833, acc: 0.4881889820098877)
[2024-12-12 02:36:23,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:23,716][root][INFO] - Training Epoch: 4/10, step 207/574 completed (loss: 1.8739535808563232, acc: 0.4784482717514038)
[2024-12-12 02:36:23,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:24,128][root][INFO] - Training Epoch: 4/10, step 208/574 completed (loss: 1.877084493637085, acc: 0.47826087474823)
[2024-12-12 02:36:24,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:24,520][root][INFO] - Training Epoch: 4/10, step 209/574 completed (loss: 2.0121583938598633, acc: 0.43579766154289246)
[2024-12-12 02:36:24,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:24,860][root][INFO] - Training Epoch: 4/10, step 210/574 completed (loss: 1.9825057983398438, acc: 0.5)
[2024-12-12 02:36:24,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:25,211][root][INFO] - Training Epoch: 4/10, step 211/574 completed (loss: 0.4724145531654358, acc: 0.8260869383811951)
[2024-12-12 02:36:25,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:25,574][root][INFO] - Training Epoch: 4/10, step 212/574 completed (loss: 0.8542355895042419, acc: 0.6785714030265808)
[2024-12-12 02:36:25,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:25,942][root][INFO] - Training Epoch: 4/10, step 213/574 completed (loss: 0.8486778736114502, acc: 0.7659574747085571)
[2024-12-12 02:36:26,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:26,622][root][INFO] - Training Epoch: 4/10, step 214/574 completed (loss: 1.5118526220321655, acc: 0.5923076868057251)
[2024-12-12 02:36:26,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:26,945][root][INFO] - Training Epoch: 4/10, step 215/574 completed (loss: 1.2406431436538696, acc: 0.6351351141929626)
[2024-12-12 02:36:27,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:27,257][root][INFO] - Training Epoch: 4/10, step 216/574 completed (loss: 1.215986967086792, acc: 0.6976743936538696)
[2024-12-12 02:36:27,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:27,789][root][INFO] - Training Epoch: 4/10, step 217/574 completed (loss: 1.4629814624786377, acc: 0.6036036014556885)
[2024-12-12 02:36:27,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:28,173][root][INFO] - Training Epoch: 4/10, step 218/574 completed (loss: 1.3945387601852417, acc: 0.5888888835906982)
[2024-12-12 02:36:28,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:28,481][root][INFO] - Training Epoch: 4/10, step 219/574 completed (loss: 0.6535314321517944, acc: 0.7878788113594055)
[2024-12-12 02:36:28,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:28,781][root][INFO] - Training Epoch: 4/10, step 220/574 completed (loss: 0.26395368576049805, acc: 0.9629629850387573)
[2024-12-12 02:36:28,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:29,087][root][INFO] - Training Epoch: 4/10, step 221/574 completed (loss: 0.26852190494537354, acc: 0.9200000166893005)
[2024-12-12 02:36:29,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:29,406][root][INFO] - Training Epoch: 4/10, step 222/574 completed (loss: 1.3149867057800293, acc: 0.6346153616905212)
[2024-12-12 02:36:29,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:30,172][root][INFO] - Training Epoch: 4/10, step 223/574 completed (loss: 1.3844430446624756, acc: 0.592391312122345)
[2024-12-12 02:36:30,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:30,712][root][INFO] - Training Epoch: 4/10, step 224/574 completed (loss: 1.805250644683838, acc: 0.5397727489471436)
[2024-12-12 02:36:30,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:31,143][root][INFO] - Training Epoch: 4/10, step 225/574 completed (loss: 1.8887073993682861, acc: 0.44680851697921753)
[2024-12-12 02:36:31,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:31,495][root][INFO] - Training Epoch: 4/10, step 226/574 completed (loss: 1.139378547668457, acc: 0.6415094137191772)
[2024-12-12 02:36:31,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:31,869][root][INFO] - Training Epoch: 4/10, step 227/574 completed (loss: 1.3899095058441162, acc: 0.6000000238418579)
[2024-12-12 02:36:31,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:32,208][root][INFO] - Training Epoch: 4/10, step 228/574 completed (loss: 0.7267974615097046, acc: 0.8139534592628479)
[2024-12-12 02:36:32,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:32,566][root][INFO] - Training Epoch: 4/10, step 229/574 completed (loss: 0.8989945650100708, acc: 0.7666666507720947)
[2024-12-12 02:36:32,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:32,931][root][INFO] - Training Epoch: 4/10, step 230/574 completed (loss: 2.0346744060516357, acc: 0.4842105209827423)
[2024-12-12 02:36:33,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:33,301][root][INFO] - Training Epoch: 4/10, step 231/574 completed (loss: 1.5767345428466797, acc: 0.6000000238418579)
[2024-12-12 02:36:33,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:33,730][root][INFO] - Training Epoch: 4/10, step 232/574 completed (loss: 1.3597736358642578, acc: 0.6111111044883728)
[2024-12-12 02:36:33,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:34,224][root][INFO] - Training Epoch: 4/10, step 233/574 completed (loss: 1.640723705291748, acc: 0.6009174585342407)
[2024-12-12 02:36:34,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:34,705][root][INFO] - Training Epoch: 4/10, step 234/574 completed (loss: 1.5363807678222656, acc: 0.5692307949066162)
[2024-12-12 02:36:34,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:35,031][root][INFO] - Training Epoch: 4/10, step 235/574 completed (loss: 0.527744472026825, acc: 0.7894737124443054)
[2024-12-12 02:36:35,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:35,364][root][INFO] - Training Epoch: 4/10, step 236/574 completed (loss: 0.39581236243247986, acc: 0.9166666865348816)
[2024-12-12 02:36:35,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:35,718][root][INFO] - Training Epoch: 4/10, step 237/574 completed (loss: 1.1831215620040894, acc: 0.5454545617103577)
[2024-12-12 02:36:35,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:36,056][root][INFO] - Training Epoch: 4/10, step 238/574 completed (loss: 1.4398539066314697, acc: 0.5185185074806213)
[2024-12-12 02:36:36,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:36,439][root][INFO] - Training Epoch: 4/10, step 239/574 completed (loss: 0.7250074744224548, acc: 0.7714285850524902)
[2024-12-12 02:36:36,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:36,795][root][INFO] - Training Epoch: 4/10, step 240/574 completed (loss: 1.2006900310516357, acc: 0.6363636255264282)
[2024-12-12 02:36:36,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:37,107][root][INFO] - Training Epoch: 4/10, step 241/574 completed (loss: 1.3874952793121338, acc: 0.6818181872367859)
[2024-12-12 02:36:37,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:37,685][root][INFO] - Training Epoch: 4/10, step 242/574 completed (loss: 1.802712321281433, acc: 0.5161290168762207)
[2024-12-12 02:36:37,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:38,211][root][INFO] - Training Epoch: 4/10, step 243/574 completed (loss: 1.1021863222122192, acc: 0.6818181872367859)
[2024-12-12 02:36:38,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:38,549][root][INFO] - Training Epoch: 4/10, step 244/574 completed (loss: 0.04987560585141182, acc: 1.0)
[2024-12-12 02:36:38,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:38,884][root][INFO] - Training Epoch: 4/10, step 245/574 completed (loss: 0.49816781282424927, acc: 0.8461538553237915)
[2024-12-12 02:36:38,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:39,242][root][INFO] - Training Epoch: 4/10, step 246/574 completed (loss: 0.6962392926216125, acc: 0.774193525314331)
[2024-12-12 02:36:39,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:39,575][root][INFO] - Training Epoch: 4/10, step 247/574 completed (loss: 0.490278959274292, acc: 0.8500000238418579)
[2024-12-12 02:36:39,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:39,918][root][INFO] - Training Epoch: 4/10, step 248/574 completed (loss: 1.04905366897583, acc: 0.7027027010917664)
[2024-12-12 02:36:40,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:40,283][root][INFO] - Training Epoch: 4/10, step 249/574 completed (loss: 1.179832100868225, acc: 0.6756756901741028)
[2024-12-12 02:36:40,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:40,627][root][INFO] - Training Epoch: 4/10, step 250/574 completed (loss: 1.0246247053146362, acc: 0.7027027010917664)
[2024-12-12 02:36:40,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:40,958][root][INFO] - Training Epoch: 4/10, step 251/574 completed (loss: 1.5321111679077148, acc: 0.6470588445663452)
[2024-12-12 02:36:41,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:41,334][root][INFO] - Training Epoch: 4/10, step 252/574 completed (loss: 0.40458282828330994, acc: 0.8292682766914368)
[2024-12-12 02:36:41,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:41,673][root][INFO] - Training Epoch: 4/10, step 253/574 completed (loss: 0.056433066725730896, acc: 1.0)
[2024-12-12 02:36:41,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:42,043][root][INFO] - Training Epoch: 4/10, step 254/574 completed (loss: 0.06203572452068329, acc: 1.0)
[2024-12-12 02:36:42,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:42,400][root][INFO] - Training Epoch: 4/10, step 255/574 completed (loss: 0.1939212828874588, acc: 0.9677419066429138)
[2024-12-12 02:36:42,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:42,743][root][INFO] - Training Epoch: 4/10, step 256/574 completed (loss: 1.0319781303405762, acc: 0.7368420958518982)
[2024-12-12 02:36:42,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:43,107][root][INFO] - Training Epoch: 4/10, step 257/574 completed (loss: 1.358853816986084, acc: 0.5714285969734192)
[2024-12-12 02:36:43,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:43,428][root][INFO] - Training Epoch: 4/10, step 258/574 completed (loss: 1.1596055030822754, acc: 0.6710526347160339)
[2024-12-12 02:36:43,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:43,992][root][INFO] - Training Epoch: 4/10, step 259/574 completed (loss: 1.4892237186431885, acc: 0.5849056839942932)
[2024-12-12 02:36:44,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:44,574][root][INFO] - Training Epoch: 4/10, step 260/574 completed (loss: 1.5914415121078491, acc: 0.550000011920929)
[2024-12-12 02:36:44,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:44,867][root][INFO] - Training Epoch: 4/10, step 261/574 completed (loss: 0.6958404183387756, acc: 0.8055555820465088)
[2024-12-12 02:36:44,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:45,225][root][INFO] - Training Epoch: 4/10, step 262/574 completed (loss: 0.8050991892814636, acc: 0.8064516186714172)
[2024-12-12 02:36:45,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:45,632][root][INFO] - Training Epoch: 4/10, step 263/574 completed (loss: 2.320817708969116, acc: 0.4000000059604645)
[2024-12-12 02:36:45,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:45,971][root][INFO] - Training Epoch: 4/10, step 264/574 completed (loss: 1.7701562643051147, acc: 0.5)
[2024-12-12 02:36:46,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:46,793][root][INFO] - Training Epoch: 4/10, step 265/574 completed (loss: 2.2575747966766357, acc: 0.42399999499320984)
[2024-12-12 02:36:46,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:47,114][root][INFO] - Training Epoch: 4/10, step 266/574 completed (loss: 2.0214617252349854, acc: 0.40449437499046326)
[2024-12-12 02:36:47,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:47,512][root][INFO] - Training Epoch: 4/10, step 267/574 completed (loss: 2.0267715454101562, acc: 0.4864864945411682)
[2024-12-12 02:36:47,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:47,979][root][INFO] - Training Epoch: 4/10, step 268/574 completed (loss: 1.2217860221862793, acc: 0.6379310488700867)
[2024-12-12 02:36:48,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:48,336][root][INFO] - Training Epoch: 4/10, step 269/574 completed (loss: 0.3684651255607605, acc: 0.8636363744735718)
[2024-12-12 02:36:48,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:48,712][root][INFO] - Training Epoch: 4/10, step 270/574 completed (loss: 0.3240261971950531, acc: 0.8636363744735718)
[2024-12-12 02:36:48,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:49,085][root][INFO] - Training Epoch: 4/10, step 271/574 completed (loss: 0.5604132413864136, acc: 0.84375)
[2024-12-12 02:36:49,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:49,452][root][INFO] - Training Epoch: 4/10, step 272/574 completed (loss: 0.6402578949928284, acc: 0.8333333134651184)
[2024-12-12 02:36:49,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:49,849][root][INFO] - Training Epoch: 4/10, step 273/574 completed (loss: 1.5169724225997925, acc: 0.6000000238418579)
[2024-12-12 02:36:49,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:50,218][root][INFO] - Training Epoch: 4/10, step 274/574 completed (loss: 0.5931927561759949, acc: 0.78125)
[2024-12-12 02:36:50,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:50,571][root][INFO] - Training Epoch: 4/10, step 275/574 completed (loss: 0.595798909664154, acc: 0.800000011920929)
[2024-12-12 02:36:50,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:50,921][root][INFO] - Training Epoch: 4/10, step 276/574 completed (loss: 0.9681658148765564, acc: 0.7241379022598267)
[2024-12-12 02:36:51,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:51,283][root][INFO] - Training Epoch: 4/10, step 277/574 completed (loss: 0.41888853907585144, acc: 0.8799999952316284)
[2024-12-12 02:36:51,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:51,666][root][INFO] - Training Epoch: 4/10, step 278/574 completed (loss: 1.4954313039779663, acc: 0.5531914830207825)
[2024-12-12 02:36:51,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:52,016][root][INFO] - Training Epoch: 4/10, step 279/574 completed (loss: 1.2115565538406372, acc: 0.625)
[2024-12-12 02:36:52,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:53,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:53,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:53,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:53,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:54,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:54,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:54,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:55,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:55,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:56,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:56,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:56,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:57,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:57,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:57,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:58,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:58,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:58,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:59,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:59,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:36:59,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:00,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:00,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:01,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:01,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:01,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:02,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:02,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:02,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:03,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:03,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:03,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:04,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:04,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:05,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:05,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:05,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:06,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:06,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:06,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:06,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:07,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:07,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:07,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:08,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:08,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:08,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:09,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:09,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:09,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:10,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:10,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:10,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:11,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:11,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:11,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:12,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:12,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:12,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:13,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:13,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:14,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:14,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:14,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:14,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:15,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:15,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:16,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:16,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:16,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:17,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:17,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:17,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:18,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:18,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:18,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:19,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:19,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:19,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:20,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:20,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:20,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:21,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:21,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:22,300][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(5.5556, device='cuda:0') eval_epoch_loss=tensor(1.7148, device='cuda:0') eval_epoch_acc=tensor(0.5778, device='cuda:0')
[2024-12-12 02:37:22,301][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:37:22,301][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:37:22,544][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_4_step_280_loss_1.7148125171661377/model.pt
[2024-12-12 02:37:22,554][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:37:22,556][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.5777506232261658
[2024-12-12 02:37:22,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:23,028][root][INFO] - Training Epoch: 4/10, step 280/574 completed (loss: 1.1477662324905396, acc: 0.6590909361839294)
[2024-12-12 02:37:23,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:23,457][root][INFO] - Training Epoch: 4/10, step 281/574 completed (loss: 1.9415192604064941, acc: 0.5301204919815063)
[2024-12-12 02:37:23,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:23,844][root][INFO] - Training Epoch: 4/10, step 282/574 completed (loss: 1.8038935661315918, acc: 0.5277777910232544)
[2024-12-12 02:37:23,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:24,178][root][INFO] - Training Epoch: 4/10, step 283/574 completed (loss: 0.9910803437232971, acc: 0.7368420958518982)
[2024-12-12 02:37:24,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:24,507][root][INFO] - Training Epoch: 4/10, step 284/574 completed (loss: 1.143925666809082, acc: 0.6764705777168274)
[2024-12-12 02:37:24,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:24,885][root][INFO] - Training Epoch: 4/10, step 285/574 completed (loss: 0.9580732583999634, acc: 0.7250000238418579)
[2024-12-12 02:37:24,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:25,261][root][INFO] - Training Epoch: 4/10, step 286/574 completed (loss: 1.9685847759246826, acc: 0.4375)
[2024-12-12 02:37:25,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:25,649][root][INFO] - Training Epoch: 4/10, step 287/574 completed (loss: 2.026379346847534, acc: 0.40799999237060547)
[2024-12-12 02:37:25,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:26,053][root][INFO] - Training Epoch: 4/10, step 288/574 completed (loss: 1.4329025745391846, acc: 0.6263736486434937)
[2024-12-12 02:37:26,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:26,432][root][INFO] - Training Epoch: 4/10, step 289/574 completed (loss: 2.0882668495178223, acc: 0.44720497727394104)
[2024-12-12 02:37:26,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:26,811][root][INFO] - Training Epoch: 4/10, step 290/574 completed (loss: 2.2184746265411377, acc: 0.3865979313850403)
[2024-12-12 02:37:26,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:27,209][root][INFO] - Training Epoch: 4/10, step 291/574 completed (loss: 0.1573951244354248, acc: 0.9090909361839294)
[2024-12-12 02:37:27,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:27,536][root][INFO] - Training Epoch: 4/10, step 292/574 completed (loss: 1.5923672914505005, acc: 0.5)
[2024-12-12 02:37:27,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:27,936][root][INFO] - Training Epoch: 4/10, step 293/574 completed (loss: 1.052112102508545, acc: 0.6896551847457886)
[2024-12-12 02:37:28,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:28,415][root][INFO] - Training Epoch: 4/10, step 294/574 completed (loss: 1.258569359779358, acc: 0.6181818246841431)
[2024-12-12 02:37:28,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:28,963][root][INFO] - Training Epoch: 4/10, step 295/574 completed (loss: 1.7294416427612305, acc: 0.5876288414001465)
[2024-12-12 02:37:29,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:29,309][root][INFO] - Training Epoch: 4/10, step 296/574 completed (loss: 1.670074701309204, acc: 0.5344827771186829)
[2024-12-12 02:37:29,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:29,684][root][INFO] - Training Epoch: 4/10, step 297/574 completed (loss: 1.0374430418014526, acc: 0.7407407164573669)
[2024-12-12 02:37:29,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:30,045][root][INFO] - Training Epoch: 4/10, step 298/574 completed (loss: 1.340012550354004, acc: 0.6842105388641357)
[2024-12-12 02:37:30,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:30,419][root][INFO] - Training Epoch: 4/10, step 299/574 completed (loss: 1.3337489366531372, acc: 0.7321428656578064)
[2024-12-12 02:37:30,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:30,779][root][INFO] - Training Epoch: 4/10, step 300/574 completed (loss: 0.979274332523346, acc: 0.78125)
[2024-12-12 02:37:30,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:31,144][root][INFO] - Training Epoch: 4/10, step 301/574 completed (loss: 1.1716039180755615, acc: 0.7169811129570007)
[2024-12-12 02:37:31,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:31,523][root][INFO] - Training Epoch: 4/10, step 302/574 completed (loss: 0.7524922490119934, acc: 0.7358490824699402)
[2024-12-12 02:37:31,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:31,881][root][INFO] - Training Epoch: 4/10, step 303/574 completed (loss: 0.7197239995002747, acc: 0.7941176295280457)
[2024-12-12 02:37:31,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:32,221][root][INFO] - Training Epoch: 4/10, step 304/574 completed (loss: 1.1826399564743042, acc: 0.6875)
[2024-12-12 02:37:32,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:32,559][root][INFO] - Training Epoch: 4/10, step 305/574 completed (loss: 1.0113799571990967, acc: 0.7213114500045776)
[2024-12-12 02:37:32,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:32,857][root][INFO] - Training Epoch: 4/10, step 306/574 completed (loss: 0.5626009702682495, acc: 0.8666666746139526)
[2024-12-12 02:37:32,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:33,184][root][INFO] - Training Epoch: 4/10, step 307/574 completed (loss: 0.354926198720932, acc: 0.9473684430122375)
[2024-12-12 02:37:33,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:33,545][root][INFO] - Training Epoch: 4/10, step 308/574 completed (loss: 1.720229983329773, acc: 0.4637681245803833)
[2024-12-12 02:37:33,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:33,980][root][INFO] - Training Epoch: 4/10, step 309/574 completed (loss: 1.365833044052124, acc: 0.6527777910232544)
[2024-12-12 02:37:34,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:34,366][root][INFO] - Training Epoch: 4/10, step 310/574 completed (loss: 1.2651597261428833, acc: 0.6144578456878662)
[2024-12-12 02:37:34,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:34,748][root][INFO] - Training Epoch: 4/10, step 311/574 completed (loss: 1.7106209993362427, acc: 0.5128205418586731)
[2024-12-12 02:37:34,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:35,098][root][INFO] - Training Epoch: 4/10, step 312/574 completed (loss: 1.7880079746246338, acc: 0.5204081535339355)
[2024-12-12 02:37:35,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:35,363][root][INFO] - Training Epoch: 4/10, step 313/574 completed (loss: 0.11570658534765244, acc: 0.9583333134651184)
[2024-12-12 02:37:35,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:35,655][root][INFO] - Training Epoch: 4/10, step 314/574 completed (loss: 0.35086655616760254, acc: 0.875)
[2024-12-12 02:37:35,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:36,036][root][INFO] - Training Epoch: 4/10, step 315/574 completed (loss: 0.8684371113777161, acc: 0.8387096524238586)
[2024-12-12 02:37:36,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:36,376][root][INFO] - Training Epoch: 4/10, step 316/574 completed (loss: 0.56974858045578, acc: 0.8709677457809448)
[2024-12-12 02:37:36,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:36,710][root][INFO] - Training Epoch: 4/10, step 317/574 completed (loss: 1.095996618270874, acc: 0.7014925479888916)
[2024-12-12 02:37:36,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:37,091][root][INFO] - Training Epoch: 4/10, step 318/574 completed (loss: 1.2885526418685913, acc: 0.6634615659713745)
[2024-12-12 02:37:37,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:37,424][root][INFO] - Training Epoch: 4/10, step 319/574 completed (loss: 1.1342586278915405, acc: 0.644444465637207)
[2024-12-12 02:37:37,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:37,746][root][INFO] - Training Epoch: 4/10, step 320/574 completed (loss: 1.1304439306259155, acc: 0.6774193644523621)
[2024-12-12 02:37:37,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:38,099][root][INFO] - Training Epoch: 4/10, step 321/574 completed (loss: 0.589739203453064, acc: 0.800000011920929)
[2024-12-12 02:37:38,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:38,473][root][INFO] - Training Epoch: 4/10, step 322/574 completed (loss: 1.4263120889663696, acc: 0.48148149251937866)
[2024-12-12 02:37:38,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:38,828][root][INFO] - Training Epoch: 4/10, step 323/574 completed (loss: 1.9468564987182617, acc: 0.4571428596973419)
[2024-12-12 02:37:38,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:39,147][root][INFO] - Training Epoch: 4/10, step 324/574 completed (loss: 1.5748801231384277, acc: 0.5641025900840759)
[2024-12-12 02:37:39,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:39,501][root][INFO] - Training Epoch: 4/10, step 325/574 completed (loss: 1.965436577796936, acc: 0.39024388790130615)
[2024-12-12 02:37:39,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:39,875][root][INFO] - Training Epoch: 4/10, step 326/574 completed (loss: 1.6104953289031982, acc: 0.5526315569877625)
[2024-12-12 02:37:39,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:40,194][root][INFO] - Training Epoch: 4/10, step 327/574 completed (loss: 0.29066330194473267, acc: 0.8947368264198303)
[2024-12-12 02:37:40,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:40,491][root][INFO] - Training Epoch: 4/10, step 328/574 completed (loss: 0.17399784922599792, acc: 0.9642857313156128)
[2024-12-12 02:37:40,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:40,878][root][INFO] - Training Epoch: 4/10, step 329/574 completed (loss: 0.8344610929489136, acc: 0.6666666865348816)
[2024-12-12 02:37:40,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:41,219][root][INFO] - Training Epoch: 4/10, step 330/574 completed (loss: 0.6188492774963379, acc: 0.8125)
[2024-12-12 02:37:41,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:41,506][root][INFO] - Training Epoch: 4/10, step 331/574 completed (loss: 1.6654136180877686, acc: 0.5967742204666138)
[2024-12-12 02:37:41,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:41,904][root][INFO] - Training Epoch: 4/10, step 332/574 completed (loss: 1.1630605459213257, acc: 0.6842105388641357)
[2024-12-12 02:37:42,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:42,281][root][INFO] - Training Epoch: 4/10, step 333/574 completed (loss: 1.3452965021133423, acc: 0.59375)
[2024-12-12 02:37:42,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:42,663][root][INFO] - Training Epoch: 4/10, step 334/574 completed (loss: 0.641700029373169, acc: 0.7666666507720947)
[2024-12-12 02:37:42,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:43,051][root][INFO] - Training Epoch: 4/10, step 335/574 completed (loss: 0.8433527946472168, acc: 0.7894737124443054)
[2024-12-12 02:37:43,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:43,430][root][INFO] - Training Epoch: 4/10, step 336/574 completed (loss: 1.873850703239441, acc: 0.47999998927116394)
[2024-12-12 02:37:43,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:43,786][root][INFO] - Training Epoch: 4/10, step 337/574 completed (loss: 2.218247652053833, acc: 0.37931033968925476)
[2024-12-12 02:37:43,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:44,113][root][INFO] - Training Epoch: 4/10, step 338/574 completed (loss: 2.1945433616638184, acc: 0.41489362716674805)
[2024-12-12 02:37:44,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:44,461][root][INFO] - Training Epoch: 4/10, step 339/574 completed (loss: 2.208407163619995, acc: 0.3855421543121338)
[2024-12-12 02:37:44,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:44,838][root][INFO] - Training Epoch: 4/10, step 340/574 completed (loss: 0.36747002601623535, acc: 0.9130434989929199)
[2024-12-12 02:37:44,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:45,196][root][INFO] - Training Epoch: 4/10, step 341/574 completed (loss: 1.090015172958374, acc: 0.692307710647583)
[2024-12-12 02:37:45,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:45,520][root][INFO] - Training Epoch: 4/10, step 342/574 completed (loss: 2.075634002685547, acc: 0.4457831382751465)
[2024-12-12 02:37:45,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:45,819][root][INFO] - Training Epoch: 4/10, step 343/574 completed (loss: 1.4847229719161987, acc: 0.6226415038108826)
[2024-12-12 02:37:45,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:46,181][root][INFO] - Training Epoch: 4/10, step 344/574 completed (loss: 1.4805452823638916, acc: 0.6582278609275818)
[2024-12-12 02:37:46,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:46,533][root][INFO] - Training Epoch: 4/10, step 345/574 completed (loss: 1.387791395187378, acc: 0.6274510025978088)
[2024-12-12 02:37:46,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:46,931][root][INFO] - Training Epoch: 4/10, step 346/574 completed (loss: 1.917697787284851, acc: 0.46268656849861145)
[2024-12-12 02:37:47,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:47,230][root][INFO] - Training Epoch: 4/10, step 347/574 completed (loss: 0.26789405941963196, acc: 0.949999988079071)
[2024-12-12 02:37:47,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:47,585][root][INFO] - Training Epoch: 4/10, step 348/574 completed (loss: 0.5544597506523132, acc: 0.800000011920929)
[2024-12-12 02:37:47,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:47,965][root][INFO] - Training Epoch: 4/10, step 349/574 completed (loss: 1.0262479782104492, acc: 0.7222222089767456)
[2024-12-12 02:37:48,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:48,272][root][INFO] - Training Epoch: 4/10, step 350/574 completed (loss: 1.7554965019226074, acc: 0.5813953280448914)
[2024-12-12 02:37:48,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:48,609][root][INFO] - Training Epoch: 4/10, step 351/574 completed (loss: 1.467031478881836, acc: 0.6153846383094788)
[2024-12-12 02:37:48,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:48,976][root][INFO] - Training Epoch: 4/10, step 352/574 completed (loss: 1.637887954711914, acc: 0.6000000238418579)
[2024-12-12 02:37:49,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:49,350][root][INFO] - Training Epoch: 4/10, step 353/574 completed (loss: 0.3566289246082306, acc: 0.8260869383811951)
[2024-12-12 02:37:49,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:49,711][root][INFO] - Training Epoch: 4/10, step 354/574 completed (loss: 0.9941051006317139, acc: 0.6153846383094788)
[2024-12-12 02:37:49,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:50,112][root][INFO] - Training Epoch: 4/10, step 355/574 completed (loss: 2.30971097946167, acc: 0.37362638115882874)
[2024-12-12 02:37:50,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:50,609][root][INFO] - Training Epoch: 4/10, step 356/574 completed (loss: 1.8937946557998657, acc: 0.469565212726593)
[2024-12-12 02:37:50,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:50,977][root][INFO] - Training Epoch: 4/10, step 357/574 completed (loss: 1.6444005966186523, acc: 0.54347825050354)
[2024-12-12 02:37:51,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:51,336][root][INFO] - Training Epoch: 4/10, step 358/574 completed (loss: 1.650141716003418, acc: 0.4897959232330322)
[2024-12-12 02:37:51,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:51,650][root][INFO] - Training Epoch: 4/10, step 359/574 completed (loss: 0.22683556377887726, acc: 0.9583333134651184)
[2024-12-12 02:37:51,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:51,945][root][INFO] - Training Epoch: 4/10, step 360/574 completed (loss: 0.16444066166877747, acc: 1.0)
[2024-12-12 02:37:52,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:52,313][root][INFO] - Training Epoch: 4/10, step 361/574 completed (loss: 0.9175416231155396, acc: 0.7804877758026123)
[2024-12-12 02:37:52,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:52,680][root][INFO] - Training Epoch: 4/10, step 362/574 completed (loss: 1.4553163051605225, acc: 0.6666666865348816)
[2024-12-12 02:37:52,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:53,025][root][INFO] - Training Epoch: 4/10, step 363/574 completed (loss: 1.5734105110168457, acc: 0.5526315569877625)
[2024-12-12 02:37:53,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:53,371][root][INFO] - Training Epoch: 4/10, step 364/574 completed (loss: 1.1700176000595093, acc: 0.6585366129875183)
[2024-12-12 02:37:53,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:53,717][root][INFO] - Training Epoch: 4/10, step 365/574 completed (loss: 1.017820119857788, acc: 0.7272727489471436)
[2024-12-12 02:37:53,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:54,059][root][INFO] - Training Epoch: 4/10, step 366/574 completed (loss: 0.16373766958713531, acc: 0.9583333134651184)
[2024-12-12 02:37:54,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:54,409][root][INFO] - Training Epoch: 4/10, step 367/574 completed (loss: 0.1173432394862175, acc: 0.95652174949646)
[2024-12-12 02:37:54,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:54,765][root][INFO] - Training Epoch: 4/10, step 368/574 completed (loss: 0.6694793105125427, acc: 0.8571428656578064)
[2024-12-12 02:37:54,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:55,107][root][INFO] - Training Epoch: 4/10, step 369/574 completed (loss: 0.5846438407897949, acc: 0.8125)
[2024-12-12 02:37:55,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:55,707][root][INFO] - Training Epoch: 4/10, step 370/574 completed (loss: 1.7854204177856445, acc: 0.539393961429596)
[2024-12-12 02:37:55,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:56,569][root][INFO] - Training Epoch: 4/10, step 371/574 completed (loss: 1.2854739427566528, acc: 0.6698113083839417)
[2024-12-12 02:37:56,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:56,900][root][INFO] - Training Epoch: 4/10, step 372/574 completed (loss: 1.4832265377044678, acc: 0.644444465637207)
[2024-12-12 02:37:57,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:57,253][root][INFO] - Training Epoch: 4/10, step 373/574 completed (loss: 1.1766606569290161, acc: 0.6785714030265808)
[2024-12-12 02:37:57,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:57,583][root][INFO] - Training Epoch: 4/10, step 374/574 completed (loss: 0.6453050374984741, acc: 0.800000011920929)
[2024-12-12 02:37:57,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:57,887][root][INFO] - Training Epoch: 4/10, step 375/574 completed (loss: 0.05087423324584961, acc: 1.0)
[2024-12-12 02:37:57,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:58,225][root][INFO] - Training Epoch: 4/10, step 376/574 completed (loss: 0.046595294028520584, acc: 1.0)
[2024-12-12 02:37:58,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:58,547][root][INFO] - Training Epoch: 4/10, step 377/574 completed (loss: 1.0957499742507935, acc: 0.6458333134651184)
[2024-12-12 02:37:58,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:58,926][root][INFO] - Training Epoch: 4/10, step 378/574 completed (loss: 1.2912617921829224, acc: 0.6105263233184814)
[2024-12-12 02:37:59,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:59,500][root][INFO] - Training Epoch: 4/10, step 379/574 completed (loss: 1.696353554725647, acc: 0.56886225938797)
[2024-12-12 02:37:59,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:37:59,898][root][INFO] - Training Epoch: 4/10, step 380/574 completed (loss: 1.45947265625, acc: 0.5789473652839661)
[2024-12-12 02:38:00,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:01,135][root][INFO] - Training Epoch: 4/10, step 381/574 completed (loss: 1.5760313272476196, acc: 0.5614973306655884)
[2024-12-12 02:38:01,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:01,700][root][INFO] - Training Epoch: 4/10, step 382/574 completed (loss: 1.103852391242981, acc: 0.684684693813324)
[2024-12-12 02:38:01,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:02,055][root][INFO] - Training Epoch: 4/10, step 383/574 completed (loss: 0.39400336146354675, acc: 0.9285714030265808)
[2024-12-12 02:38:02,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:02,393][root][INFO] - Training Epoch: 4/10, step 384/574 completed (loss: 0.15255138278007507, acc: 0.9642857313156128)
[2024-12-12 02:38:02,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:02,713][root][INFO] - Training Epoch: 4/10, step 385/574 completed (loss: 0.3312221169471741, acc: 0.875)
[2024-12-12 02:38:02,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:03,015][root][INFO] - Training Epoch: 4/10, step 386/574 completed (loss: 0.3488824963569641, acc: 0.8611111044883728)
[2024-12-12 02:38:03,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:03,317][root][INFO] - Training Epoch: 4/10, step 387/574 completed (loss: 0.5282506346702576, acc: 0.8421052694320679)
[2024-12-12 02:38:03,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:03,658][root][INFO] - Training Epoch: 4/10, step 388/574 completed (loss: 0.061680231243371964, acc: 1.0)
[2024-12-12 02:38:03,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:04,042][root][INFO] - Training Epoch: 4/10, step 389/574 completed (loss: 0.3837431073188782, acc: 0.8999999761581421)
[2024-12-12 02:38:04,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:04,423][root][INFO] - Training Epoch: 4/10, step 390/574 completed (loss: 0.3645608425140381, acc: 0.8571428656578064)
[2024-12-12 02:38:04,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:04,832][root][INFO] - Training Epoch: 4/10, step 391/574 completed (loss: 2.010376453399658, acc: 0.5)
[2024-12-12 02:38:04,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:05,231][root][INFO] - Training Epoch: 4/10, step 392/574 completed (loss: 2.076767683029175, acc: 0.48543688654899597)
[2024-12-12 02:38:05,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:05,753][root][INFO] - Training Epoch: 4/10, step 393/574 completed (loss: 1.807650089263916, acc: 0.5588235259056091)
[2024-12-12 02:38:05,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:06,144][root][INFO] - Training Epoch: 4/10, step 394/574 completed (loss: 2.1689751148223877, acc: 0.4333333373069763)
[2024-12-12 02:38:06,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:06,522][root][INFO] - Training Epoch: 4/10, step 395/574 completed (loss: 2.039130449295044, acc: 0.4722222089767456)
[2024-12-12 02:38:06,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:06,866][root][INFO] - Training Epoch: 4/10, step 396/574 completed (loss: 1.1951336860656738, acc: 0.7209302186965942)
[2024-12-12 02:38:06,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:07,234][root][INFO] - Training Epoch: 4/10, step 397/574 completed (loss: 0.41522035002708435, acc: 0.9583333134651184)
[2024-12-12 02:38:07,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:07,642][root][INFO] - Training Epoch: 4/10, step 398/574 completed (loss: 1.054104208946228, acc: 0.6976743936538696)
[2024-12-12 02:38:07,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:07,987][root][INFO] - Training Epoch: 4/10, step 399/574 completed (loss: 0.9278649091720581, acc: 0.7599999904632568)
[2024-12-12 02:38:08,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:08,527][root][INFO] - Training Epoch: 4/10, step 400/574 completed (loss: 1.4761353731155396, acc: 0.6029411554336548)
[2024-12-12 02:38:08,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:08,895][root][INFO] - Training Epoch: 4/10, step 401/574 completed (loss: 1.4144951105117798, acc: 0.653333306312561)
[2024-12-12 02:38:08,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:09,198][root][INFO] - Training Epoch: 4/10, step 402/574 completed (loss: 1.1289656162261963, acc: 0.7272727489471436)
[2024-12-12 02:38:09,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:09,545][root][INFO] - Training Epoch: 4/10, step 403/574 completed (loss: 0.7183444499969482, acc: 0.8484848737716675)
[2024-12-12 02:38:09,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:09,930][root][INFO] - Training Epoch: 4/10, step 404/574 completed (loss: 0.352361798286438, acc: 0.9354838728904724)
[2024-12-12 02:38:10,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:10,305][root][INFO] - Training Epoch: 4/10, step 405/574 completed (loss: 0.6490959525108337, acc: 0.8148148059844971)
[2024-12-12 02:38:10,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:10,660][root][INFO] - Training Epoch: 4/10, step 406/574 completed (loss: 0.2950052320957184, acc: 0.9599999785423279)
[2024-12-12 02:38:10,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:11,018][root][INFO] - Training Epoch: 4/10, step 407/574 completed (loss: 0.5069166421890259, acc: 0.8333333134651184)
[2024-12-12 02:38:11,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:11,342][root][INFO] - Training Epoch: 4/10, step 408/574 completed (loss: 0.4495336413383484, acc: 0.8518518805503845)
[2024-12-12 02:38:11,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:11,700][root][INFO] - Training Epoch: 4/10, step 409/574 completed (loss: 0.19233328104019165, acc: 0.9615384340286255)
[2024-12-12 02:38:11,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:12,067][root][INFO] - Training Epoch: 4/10, step 410/574 completed (loss: 0.9275522828102112, acc: 0.7413793206214905)
[2024-12-12 02:38:12,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:12,388][root][INFO] - Training Epoch: 4/10, step 411/574 completed (loss: 0.3333428204059601, acc: 0.9642857313156128)
[2024-12-12 02:38:12,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:12,721][root][INFO] - Training Epoch: 4/10, step 412/574 completed (loss: 0.6005750894546509, acc: 0.8666666746139526)
[2024-12-12 02:38:12,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:13,046][root][INFO] - Training Epoch: 4/10, step 413/574 completed (loss: 0.42602571845054626, acc: 0.8181818127632141)
[2024-12-12 02:38:13,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:13,353][root][INFO] - Training Epoch: 4/10, step 414/574 completed (loss: 0.23159855604171753, acc: 0.9090909361839294)
[2024-12-12 02:38:13,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:13,663][root][INFO] - Training Epoch: 4/10, step 415/574 completed (loss: 1.4578418731689453, acc: 0.6078431606292725)
[2024-12-12 02:38:13,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:14,015][root][INFO] - Training Epoch: 4/10, step 416/574 completed (loss: 1.092604160308838, acc: 0.6538461446762085)
[2024-12-12 02:38:14,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:14,368][root][INFO] - Training Epoch: 4/10, step 417/574 completed (loss: 0.9969256520271301, acc: 0.6666666865348816)
[2024-12-12 02:38:14,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:14,739][root][INFO] - Training Epoch: 4/10, step 418/574 completed (loss: 1.0740910768508911, acc: 0.699999988079071)
[2024-12-12 02:38:14,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:15,083][root][INFO] - Training Epoch: 4/10, step 419/574 completed (loss: 0.20973487198352814, acc: 0.949999988079071)
[2024-12-12 02:38:15,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:15,392][root][INFO] - Training Epoch: 4/10, step 420/574 completed (loss: 0.12951672077178955, acc: 0.9523809552192688)
[2024-12-12 02:38:15,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:15,685][root][INFO] - Training Epoch: 4/10, step 421/574 completed (loss: 0.5208220481872559, acc: 0.7666666507720947)
[2024-12-12 02:38:15,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:15,971][root][INFO] - Training Epoch: 4/10, step 422/574 completed (loss: 0.6906068325042725, acc: 0.84375)
[2024-12-12 02:38:16,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:17,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:17,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:17,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:18,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:18,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:18,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:19,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:19,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:19,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:20,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:20,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:20,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:21,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:21,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:21,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:22,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:22,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:22,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:23,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:23,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:23,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:24,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:24,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:24,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:25,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:25,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:25,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:26,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:26,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:26,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:27,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:27,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:28,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:28,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:28,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:29,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:29,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:29,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:30,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:30,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:30,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:31,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:31,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:31,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:32,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:32,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:33,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:33,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:33,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:33,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:34,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:34,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:35,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:35,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:35,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:36,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:36,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:36,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:37,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:37,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:37,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:38,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:38,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:38,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:39,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:39,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:39,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:40,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:40,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:41,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:41,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:41,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:42,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:42,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:42,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:43,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:43,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:43,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:44,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:44,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:44,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:45,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:45,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:45,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:46,407][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(10.9956, device='cuda:0') eval_epoch_loss=tensor(2.3975, device='cuda:0') eval_epoch_acc=tensor(0.5241, device='cuda:0')
[2024-12-12 02:38:46,408][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:38:46,408][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:38:46,622][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_4_step_423_loss_2.397496223449707/model.pt
[2024-12-12 02:38:46,627][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:38:46,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:47,030][root][INFO] - Training Epoch: 4/10, step 423/574 completed (loss: 1.0351687669754028, acc: 0.8055555820465088)
[2024-12-12 02:38:47,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:47,354][root][INFO] - Training Epoch: 4/10, step 424/574 completed (loss: 0.7071022391319275, acc: 0.7777777910232544)
[2024-12-12 02:38:47,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:47,685][root][INFO] - Training Epoch: 4/10, step 425/574 completed (loss: 1.4861501455307007, acc: 0.6969696879386902)
[2024-12-12 02:38:47,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:48,055][root][INFO] - Training Epoch: 4/10, step 426/574 completed (loss: 1.1755218505859375, acc: 0.782608687877655)
[2024-12-12 02:38:48,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:48,407][root][INFO] - Training Epoch: 4/10, step 427/574 completed (loss: 0.741401195526123, acc: 0.7837837934494019)
[2024-12-12 02:38:48,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:48,760][root][INFO] - Training Epoch: 4/10, step 428/574 completed (loss: 0.33528992533683777, acc: 0.8888888955116272)
[2024-12-12 02:38:48,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:49,078][root][INFO] - Training Epoch: 4/10, step 429/574 completed (loss: 0.6048051714897156, acc: 0.9130434989929199)
[2024-12-12 02:38:49,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:49,437][root][INFO] - Training Epoch: 4/10, step 430/574 completed (loss: 0.13865308463573456, acc: 0.9629629850387573)
[2024-12-12 02:38:49,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:49,803][root][INFO] - Training Epoch: 4/10, step 431/574 completed (loss: 0.16996799409389496, acc: 1.0)
[2024-12-12 02:38:49,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:50,164][root][INFO] - Training Epoch: 4/10, step 432/574 completed (loss: 0.6375147700309753, acc: 0.8260869383811951)
[2024-12-12 02:38:50,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:50,568][root][INFO] - Training Epoch: 4/10, step 433/574 completed (loss: 1.3501969575881958, acc: 0.5555555820465088)
[2024-12-12 02:38:50,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:50,903][root][INFO] - Training Epoch: 4/10, step 434/574 completed (loss: 0.6606258153915405, acc: 0.7599999904632568)
[2024-12-12 02:38:50,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:51,217][root][INFO] - Training Epoch: 4/10, step 435/574 completed (loss: 0.9587773680686951, acc: 0.7272727489471436)
[2024-12-12 02:38:51,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:51,574][root][INFO] - Training Epoch: 4/10, step 436/574 completed (loss: 0.8148800134658813, acc: 0.75)
[2024-12-12 02:38:51,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:51,969][root][INFO] - Training Epoch: 4/10, step 437/574 completed (loss: 1.120347261428833, acc: 0.6818181872367859)
[2024-12-12 02:38:52,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:52,308][root][INFO] - Training Epoch: 4/10, step 438/574 completed (loss: 0.22227434813976288, acc: 0.9523809552192688)
[2024-12-12 02:38:52,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:52,611][root][INFO] - Training Epoch: 4/10, step 439/574 completed (loss: 1.33767569065094, acc: 0.6666666865348816)
[2024-12-12 02:38:52,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:53,074][root][INFO] - Training Epoch: 4/10, step 440/574 completed (loss: 1.5993595123291016, acc: 0.6060606241226196)
[2024-12-12 02:38:53,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:53,810][root][INFO] - Training Epoch: 4/10, step 441/574 completed (loss: 2.1921894550323486, acc: 0.41600000858306885)
[2024-12-12 02:38:53,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:54,208][root][INFO] - Training Epoch: 4/10, step 442/574 completed (loss: 2.0884196758270264, acc: 0.45967742800712585)
[2024-12-12 02:38:54,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:54,866][root][INFO] - Training Epoch: 4/10, step 443/574 completed (loss: 2.011509418487549, acc: 0.46268656849861145)
[2024-12-12 02:38:54,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:55,243][root][INFO] - Training Epoch: 4/10, step 444/574 completed (loss: 1.680851936340332, acc: 0.5849056839942932)
[2024-12-12 02:38:55,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:55,640][root][INFO] - Training Epoch: 4/10, step 445/574 completed (loss: 0.7965224981307983, acc: 0.75)
[2024-12-12 02:38:55,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:55,979][root][INFO] - Training Epoch: 4/10, step 446/574 completed (loss: 0.632337749004364, acc: 0.782608687877655)
[2024-12-12 02:38:56,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:56,354][root][INFO] - Training Epoch: 4/10, step 447/574 completed (loss: 0.6792017221450806, acc: 0.807692289352417)
[2024-12-12 02:38:56,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:56,748][root][INFO] - Training Epoch: 4/10, step 448/574 completed (loss: 0.417082816362381, acc: 0.8928571343421936)
[2024-12-12 02:38:56,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:57,135][root][INFO] - Training Epoch: 4/10, step 449/574 completed (loss: 1.5009254217147827, acc: 0.611940324306488)
[2024-12-12 02:38:57,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:57,495][root][INFO] - Training Epoch: 4/10, step 450/574 completed (loss: 1.2331955432891846, acc: 0.6527777910232544)
[2024-12-12 02:38:57,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:57,884][root][INFO] - Training Epoch: 4/10, step 451/574 completed (loss: 1.4877781867980957, acc: 0.6195651888847351)
[2024-12-12 02:38:57,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:58,215][root][INFO] - Training Epoch: 4/10, step 452/574 completed (loss: 1.4650863409042358, acc: 0.5641025900840759)
[2024-12-12 02:38:58,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:58,522][root][INFO] - Training Epoch: 4/10, step 453/574 completed (loss: 1.5863547325134277, acc: 0.5789473652839661)
[2024-12-12 02:38:58,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:58,867][root][INFO] - Training Epoch: 4/10, step 454/574 completed (loss: 1.2978211641311646, acc: 0.5510203838348389)
[2024-12-12 02:38:58,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:59,214][root][INFO] - Training Epoch: 4/10, step 455/574 completed (loss: 0.5475891828536987, acc: 0.8787878751754761)
[2024-12-12 02:38:59,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:38:59,625][root][INFO] - Training Epoch: 4/10, step 456/574 completed (loss: 1.9153882265090942, acc: 0.4845360815525055)
[2024-12-12 02:38:59,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:00,011][root][INFO] - Training Epoch: 4/10, step 457/574 completed (loss: 1.2952117919921875, acc: 0.6142857074737549)
[2024-12-12 02:39:00,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:00,414][root][INFO] - Training Epoch: 4/10, step 458/574 completed (loss: 1.785357117652893, acc: 0.5)
[2024-12-12 02:39:00,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:00,734][root][INFO] - Training Epoch: 4/10, step 459/574 completed (loss: 1.7827513217926025, acc: 0.5178571343421936)
[2024-12-12 02:39:00,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:01,037][root][INFO] - Training Epoch: 4/10, step 460/574 completed (loss: 1.852476954460144, acc: 0.5061728358268738)
[2024-12-12 02:39:01,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:01,316][root][INFO] - Training Epoch: 4/10, step 461/574 completed (loss: 1.0808931589126587, acc: 0.6388888955116272)
[2024-12-12 02:39:01,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:01,608][root][INFO] - Training Epoch: 4/10, step 462/574 completed (loss: 1.018173336982727, acc: 0.75)
[2024-12-12 02:39:01,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:01,968][root][INFO] - Training Epoch: 4/10, step 463/574 completed (loss: 0.6772117018699646, acc: 0.807692289352417)
[2024-12-12 02:39:02,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:02,280][root][INFO] - Training Epoch: 4/10, step 464/574 completed (loss: 0.8603227138519287, acc: 0.739130437374115)
[2024-12-12 02:39:02,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:02,646][root][INFO] - Training Epoch: 4/10, step 465/574 completed (loss: 1.522695779800415, acc: 0.5)
[2024-12-12 02:39:02,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:02,977][root][INFO] - Training Epoch: 4/10, step 466/574 completed (loss: 1.723044753074646, acc: 0.5301204919815063)
[2024-12-12 02:39:03,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:03,349][root][INFO] - Training Epoch: 4/10, step 467/574 completed (loss: 1.3930206298828125, acc: 0.5855855941772461)
[2024-12-12 02:39:03,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:03,676][root][INFO] - Training Epoch: 4/10, step 468/574 completed (loss: 1.6217132806777954, acc: 0.5631067752838135)
[2024-12-12 02:39:03,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:03,960][root][INFO] - Training Epoch: 4/10, step 469/574 completed (loss: 1.3740891218185425, acc: 0.6260162591934204)
[2024-12-12 02:39:04,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:04,295][root][INFO] - Training Epoch: 4/10, step 470/574 completed (loss: 0.3108407258987427, acc: 0.9166666865348816)
[2024-12-12 02:39:04,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:04,601][root][INFO] - Training Epoch: 4/10, step 471/574 completed (loss: 0.8208805322647095, acc: 0.6785714030265808)
[2024-12-12 02:39:04,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:04,990][root][INFO] - Training Epoch: 4/10, step 472/574 completed (loss: 1.8905597925186157, acc: 0.47058823704719543)
[2024-12-12 02:39:05,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:05,404][root][INFO] - Training Epoch: 4/10, step 473/574 completed (loss: 2.092658519744873, acc: 0.43668121099472046)
[2024-12-12 02:39:05,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:05,780][root][INFO] - Training Epoch: 4/10, step 474/574 completed (loss: 1.816344141960144, acc: 0.5)
[2024-12-12 02:39:05,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:06,173][root][INFO] - Training Epoch: 4/10, step 475/574 completed (loss: 1.9482437372207642, acc: 0.4969325065612793)
[2024-12-12 02:39:06,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:06,510][root][INFO] - Training Epoch: 4/10, step 476/574 completed (loss: 1.9176828861236572, acc: 0.43884891271591187)
[2024-12-12 02:39:06,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:06,897][root][INFO] - Training Epoch: 4/10, step 477/574 completed (loss: 1.9610577821731567, acc: 0.4673366844654083)
[2024-12-12 02:39:06,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:07,220][root][INFO] - Training Epoch: 4/10, step 478/574 completed (loss: 1.0047199726104736, acc: 0.6944444179534912)
[2024-12-12 02:39:07,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:07,583][root][INFO] - Training Epoch: 4/10, step 479/574 completed (loss: 0.8577795028686523, acc: 0.6969696879386902)
[2024-12-12 02:39:07,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:07,951][root][INFO] - Training Epoch: 4/10, step 480/574 completed (loss: 0.7305301427841187, acc: 0.7407407164573669)
[2024-12-12 02:39:08,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:08,319][root][INFO] - Training Epoch: 4/10, step 481/574 completed (loss: 0.7113513350486755, acc: 0.75)
[2024-12-12 02:39:08,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:08,654][root][INFO] - Training Epoch: 4/10, step 482/574 completed (loss: 0.5840592980384827, acc: 0.800000011920929)
[2024-12-12 02:39:08,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:09,028][root][INFO] - Training Epoch: 4/10, step 483/574 completed (loss: 1.2271043062210083, acc: 0.5862069129943848)
[2024-12-12 02:39:09,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:09,361][root][INFO] - Training Epoch: 4/10, step 484/574 completed (loss: 0.46571433544158936, acc: 0.8709677457809448)
[2024-12-12 02:39:09,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:09,714][root][INFO] - Training Epoch: 4/10, step 485/574 completed (loss: 0.41471853852272034, acc: 0.8947368264198303)
[2024-12-12 02:39:09,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:10,081][root][INFO] - Training Epoch: 4/10, step 486/574 completed (loss: 1.2981888055801392, acc: 0.6296296119689941)
[2024-12-12 02:39:10,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:10,417][root][INFO] - Training Epoch: 4/10, step 487/574 completed (loss: 0.9395250678062439, acc: 0.7142857313156128)
[2024-12-12 02:39:10,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:10,785][root][INFO] - Training Epoch: 4/10, step 488/574 completed (loss: 0.5805610418319702, acc: 0.7727272510528564)
[2024-12-12 02:39:10,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:11,134][root][INFO] - Training Epoch: 4/10, step 489/574 completed (loss: 1.6309170722961426, acc: 0.5692307949066162)
[2024-12-12 02:39:11,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:11,438][root][INFO] - Training Epoch: 4/10, step 490/574 completed (loss: 0.9116365313529968, acc: 0.7666666507720947)
[2024-12-12 02:39:11,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:11,749][root][INFO] - Training Epoch: 4/10, step 491/574 completed (loss: 0.6821183562278748, acc: 0.7586206793785095)
[2024-12-12 02:39:11,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:12,067][root][INFO] - Training Epoch: 4/10, step 492/574 completed (loss: 1.386505126953125, acc: 0.5882353186607361)
[2024-12-12 02:39:12,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:12,374][root][INFO] - Training Epoch: 4/10, step 493/574 completed (loss: 0.7842996120452881, acc: 0.7931034564971924)
[2024-12-12 02:39:12,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:12,671][root][INFO] - Training Epoch: 4/10, step 494/574 completed (loss: 0.440177857875824, acc: 0.8947368264198303)
[2024-12-12 02:39:12,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:13,018][root][INFO] - Training Epoch: 4/10, step 495/574 completed (loss: 0.9629512429237366, acc: 0.7368420958518982)
[2024-12-12 02:39:13,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:13,370][root][INFO] - Training Epoch: 4/10, step 496/574 completed (loss: 1.4576655626296997, acc: 0.6160714030265808)
[2024-12-12 02:39:13,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:13,718][root][INFO] - Training Epoch: 4/10, step 497/574 completed (loss: 1.4872338771820068, acc: 0.5617977380752563)
[2024-12-12 02:39:13,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:14,104][root][INFO] - Training Epoch: 4/10, step 498/574 completed (loss: 1.8849514722824097, acc: 0.516853928565979)
[2024-12-12 02:39:14,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:14,450][root][INFO] - Training Epoch: 4/10, step 499/574 completed (loss: 2.210928440093994, acc: 0.42553192377090454)
[2024-12-12 02:39:14,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:14,842][root][INFO] - Training Epoch: 4/10, step 500/574 completed (loss: 2.193197727203369, acc: 0.43478259444236755)
[2024-12-12 02:39:14,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:15,167][root][INFO] - Training Epoch: 4/10, step 501/574 completed (loss: 0.1714392900466919, acc: 1.0)
[2024-12-12 02:39:15,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:15,494][root][INFO] - Training Epoch: 4/10, step 502/574 completed (loss: 0.3548271358013153, acc: 0.8846153616905212)
[2024-12-12 02:39:15,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:15,834][root][INFO] - Training Epoch: 4/10, step 503/574 completed (loss: 0.588853120803833, acc: 0.8518518805503845)
[2024-12-12 02:39:15,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:16,193][root][INFO] - Training Epoch: 4/10, step 504/574 completed (loss: 1.0790904760360718, acc: 0.7037037014961243)
[2024-12-12 02:39:16,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:16,542][root][INFO] - Training Epoch: 4/10, step 505/574 completed (loss: 1.032928228378296, acc: 0.6792452931404114)
[2024-12-12 02:39:16,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:16,862][root][INFO] - Training Epoch: 4/10, step 506/574 completed (loss: 0.8007668852806091, acc: 0.7586206793785095)
[2024-12-12 02:39:17,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:17,449][root][INFO] - Training Epoch: 4/10, step 507/574 completed (loss: 1.7374870777130127, acc: 0.522522509098053)
[2024-12-12 02:39:17,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:17,886][root][INFO] - Training Epoch: 4/10, step 508/574 completed (loss: 1.413979172706604, acc: 0.6619718074798584)
[2024-12-12 02:39:17,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:18,178][root][INFO] - Training Epoch: 4/10, step 509/574 completed (loss: 0.08949410915374756, acc: 1.0)
[2024-12-12 02:39:18,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:18,500][root][INFO] - Training Epoch: 4/10, step 510/574 completed (loss: 0.24740462005138397, acc: 0.8999999761581421)
[2024-12-12 02:39:18,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:18,864][root][INFO] - Training Epoch: 4/10, step 511/574 completed (loss: 0.5300108790397644, acc: 0.9230769276618958)
[2024-12-12 02:39:20,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:21,572][root][INFO] - Training Epoch: 4/10, step 512/574 completed (loss: 1.8692415952682495, acc: 0.5142857432365417)
[2024-12-12 02:39:21,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:22,335][root][INFO] - Training Epoch: 4/10, step 513/574 completed (loss: 1.6519978046417236, acc: 0.5317460298538208)
[2024-12-12 02:39:22,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:22,701][root][INFO] - Training Epoch: 4/10, step 514/574 completed (loss: 1.1239303350448608, acc: 0.7142857313156128)
[2024-12-12 02:39:22,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:23,033][root][INFO] - Training Epoch: 4/10, step 515/574 completed (loss: 1.2296785116195679, acc: 0.6833333373069763)
[2024-12-12 02:39:23,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:23,729][root][INFO] - Training Epoch: 4/10, step 516/574 completed (loss: 1.3120931386947632, acc: 0.6805555820465088)
[2024-12-12 02:39:23,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:24,115][root][INFO] - Training Epoch: 4/10, step 517/574 completed (loss: 0.33540967106819153, acc: 0.9230769276618958)
[2024-12-12 02:39:24,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:24,432][root][INFO] - Training Epoch: 4/10, step 518/574 completed (loss: 0.7473958134651184, acc: 0.8064516186714172)
[2024-12-12 02:39:24,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:24,750][root][INFO] - Training Epoch: 4/10, step 519/574 completed (loss: 1.1506496667861938, acc: 0.6499999761581421)
[2024-12-12 02:39:24,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:25,120][root][INFO] - Training Epoch: 4/10, step 520/574 completed (loss: 0.9547363519668579, acc: 0.7407407164573669)
[2024-12-12 02:39:25,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:26,115][root][INFO] - Training Epoch: 4/10, step 521/574 completed (loss: 1.9271855354309082, acc: 0.4661017060279846)
[2024-12-12 02:39:26,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:26,495][root][INFO] - Training Epoch: 4/10, step 522/574 completed (loss: 1.781378149986267, acc: 0.5223880410194397)
[2024-12-12 02:39:26,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:26,861][root][INFO] - Training Epoch: 4/10, step 523/574 completed (loss: 1.8465068340301514, acc: 0.49635037779808044)
[2024-12-12 02:39:26,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:27,427][root][INFO] - Training Epoch: 4/10, step 524/574 completed (loss: 1.7738120555877686, acc: 0.5049999952316284)
[2024-12-12 02:39:27,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:27,731][root][INFO] - Training Epoch: 4/10, step 525/574 completed (loss: 1.3690346479415894, acc: 0.5925925970077515)
[2024-12-12 02:39:27,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:28,038][root][INFO] - Training Epoch: 4/10, step 526/574 completed (loss: 1.4188106060028076, acc: 0.5961538553237915)
[2024-12-12 02:39:28,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:28,313][root][INFO] - Training Epoch: 4/10, step 527/574 completed (loss: 0.6322574615478516, acc: 0.8571428656578064)
[2024-12-12 02:39:28,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:28,686][root][INFO] - Training Epoch: 4/10, step 528/574 completed (loss: 2.136132001876831, acc: 0.37704917788505554)
[2024-12-12 02:39:28,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:29,067][root][INFO] - Training Epoch: 4/10, step 529/574 completed (loss: 1.2879724502563477, acc: 0.6440678238868713)
[2024-12-12 02:39:29,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:29,406][root][INFO] - Training Epoch: 4/10, step 530/574 completed (loss: 1.9402799606323242, acc: 0.5116279125213623)
[2024-12-12 02:39:29,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:29,772][root][INFO] - Training Epoch: 4/10, step 531/574 completed (loss: 1.3590412139892578, acc: 0.6136363744735718)
[2024-12-12 02:39:29,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:30,108][root][INFO] - Training Epoch: 4/10, step 532/574 completed (loss: 1.8003202676773071, acc: 0.49056604504585266)
[2024-12-12 02:39:30,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:30,458][root][INFO] - Training Epoch: 4/10, step 533/574 completed (loss: 1.220076084136963, acc: 0.7272727489471436)
[2024-12-12 02:39:30,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:30,799][root][INFO] - Training Epoch: 4/10, step 534/574 completed (loss: 0.5167802572250366, acc: 0.800000011920929)
[2024-12-12 02:39:30,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:31,132][root][INFO] - Training Epoch: 4/10, step 535/574 completed (loss: 0.8319970965385437, acc: 0.800000011920929)
[2024-12-12 02:39:31,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:31,440][root][INFO] - Training Epoch: 4/10, step 536/574 completed (loss: 0.2066984325647354, acc: 1.0)
[2024-12-12 02:39:31,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:31,829][root][INFO] - Training Epoch: 4/10, step 537/574 completed (loss: 1.3441897630691528, acc: 0.6615384817123413)
[2024-12-12 02:39:31,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:32,198][root][INFO] - Training Epoch: 4/10, step 538/574 completed (loss: 1.227454662322998, acc: 0.6875)
[2024-12-12 02:39:32,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:32,578][root][INFO] - Training Epoch: 4/10, step 539/574 completed (loss: 0.8747283220291138, acc: 0.875)
[2024-12-12 02:39:32,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:32,903][root][INFO] - Training Epoch: 4/10, step 540/574 completed (loss: 1.0629777908325195, acc: 0.7272727489471436)
[2024-12-12 02:39:33,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:33,226][root][INFO] - Training Epoch: 4/10, step 541/574 completed (loss: 0.47539255023002625, acc: 0.75)
[2024-12-12 02:39:33,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:33,547][root][INFO] - Training Epoch: 4/10, step 542/574 completed (loss: 0.2871285080909729, acc: 0.9354838728904724)
[2024-12-12 02:39:33,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:33,874][root][INFO] - Training Epoch: 4/10, step 543/574 completed (loss: 0.10280675441026688, acc: 0.95652174949646)
[2024-12-12 02:39:33,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:34,218][root][INFO] - Training Epoch: 4/10, step 544/574 completed (loss: 1.0068975687026978, acc: 0.7333333492279053)
[2024-12-12 02:39:34,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:34,567][root][INFO] - Training Epoch: 4/10, step 545/574 completed (loss: 0.9774910807609558, acc: 0.707317054271698)
[2024-12-12 02:39:34,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:34,938][root][INFO] - Training Epoch: 4/10, step 546/574 completed (loss: 0.3914429247379303, acc: 0.8857142925262451)
[2024-12-12 02:39:35,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:35,255][root][INFO] - Training Epoch: 4/10, step 547/574 completed (loss: 0.7590939402580261, acc: 0.8421052694320679)
[2024-12-12 02:39:35,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:35,607][root][INFO] - Training Epoch: 4/10, step 548/574 completed (loss: 0.4982525110244751, acc: 0.9032257795333862)
[2024-12-12 02:39:35,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:35,962][root][INFO] - Training Epoch: 4/10, step 549/574 completed (loss: 0.18397104740142822, acc: 0.9200000166893005)
[2024-12-12 02:39:36,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:36,292][root][INFO] - Training Epoch: 4/10, step 550/574 completed (loss: 0.6718291640281677, acc: 0.8787878751754761)
[2024-12-12 02:39:36,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:36,636][root][INFO] - Training Epoch: 4/10, step 551/574 completed (loss: 0.5802017450332642, acc: 0.824999988079071)
[2024-12-12 02:39:36,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:36,985][root][INFO] - Training Epoch: 4/10, step 552/574 completed (loss: 0.7820512056350708, acc: 0.699999988079071)
[2024-12-12 02:39:37,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:37,369][root][INFO] - Training Epoch: 4/10, step 553/574 completed (loss: 1.8676800727844238, acc: 0.510948896408081)
[2024-12-12 02:39:37,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:37,708][root][INFO] - Training Epoch: 4/10, step 554/574 completed (loss: 1.39594566822052, acc: 0.6137930750846863)
[2024-12-12 02:39:37,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:38,093][root][INFO] - Training Epoch: 4/10, step 555/574 completed (loss: 2.1417975425720215, acc: 0.44285714626312256)
[2024-12-12 02:39:38,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:38,437][root][INFO] - Training Epoch: 4/10, step 556/574 completed (loss: 1.9375519752502441, acc: 0.503311276435852)
[2024-12-12 02:39:38,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:38,796][root][INFO] - Training Epoch: 4/10, step 557/574 completed (loss: 1.2970274686813354, acc: 0.6068376302719116)
[2024-12-12 02:39:38,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:39,147][root][INFO] - Training Epoch: 4/10, step 558/574 completed (loss: 0.1395692229270935, acc: 0.9200000166893005)
[2024-12-12 02:39:39,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:39,473][root][INFO] - Training Epoch: 4/10, step 559/574 completed (loss: 0.27909722924232483, acc: 0.9230769276618958)
[2024-12-12 02:39:39,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:39,837][root][INFO] - Training Epoch: 4/10, step 560/574 completed (loss: 0.20922771096229553, acc: 0.9615384340286255)
[2024-12-12 02:39:39,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:40,217][root][INFO] - Training Epoch: 4/10, step 561/574 completed (loss: 0.9023988246917725, acc: 0.7435897588729858)
[2024-12-12 02:39:40,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:40,557][root][INFO] - Training Epoch: 4/10, step 562/574 completed (loss: 1.13291597366333, acc: 0.644444465637207)
[2024-12-12 02:39:40,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:40,890][root][INFO] - Training Epoch: 4/10, step 563/574 completed (loss: 1.0203968286514282, acc: 0.7272727489471436)
[2024-12-12 02:39:40,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:41,240][root][INFO] - Training Epoch: 4/10, step 564/574 completed (loss: 0.8569719791412354, acc: 0.7291666865348816)
[2024-12-12 02:39:41,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:41,580][root][INFO] - Training Epoch: 4/10, step 565/574 completed (loss: 0.7619614005088806, acc: 0.7586206793785095)
[2024-12-12 02:39:42,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:42,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:42,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:43,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:43,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:44,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:44,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:44,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:45,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:45,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:45,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:46,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:46,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:46,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:47,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:47,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:47,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:48,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:48,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:49,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:49,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:49,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:50,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:50,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:50,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:51,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:51,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:51,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:52,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:52,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:53,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:53,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:53,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:53,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:54,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:54,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:54,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:55,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:55,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:55,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:56,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:56,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:56,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:57,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:57,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:57,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:58,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:58,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:58,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:59,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:59,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:59,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:39:59,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:00,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:00,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:01,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:01,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:01,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:02,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:02,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:02,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:03,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:03,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:04,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:04,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:04,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:05,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:05,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:05,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:06,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:06,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:06,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:07,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:07,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:07,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:08,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:08,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:08,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:09,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:09,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:09,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:09,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:10,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:10,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:11,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:11,734][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(5.7149, device='cuda:0') eval_epoch_loss=tensor(1.7431, device='cuda:0') eval_epoch_acc=tensor(0.5874, device='cuda:0')
[2024-12-12 02:40:11,735][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:40:11,735][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:40:11,938][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_4_step_566_loss_1.7430756092071533/model.pt
[2024-12-12 02:40:11,943][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:40:11,944][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.5873726606369019
[2024-12-12 02:40:12,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:12,336][root][INFO] - Training Epoch: 4/10, step 566/574 completed (loss: 1.3033257722854614, acc: 0.5833333134651184)
[2024-12-12 02:40:12,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:12,688][root][INFO] - Training Epoch: 4/10, step 567/574 completed (loss: 0.7623708248138428, acc: 0.7894737124443054)
[2024-12-12 02:40:12,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:13,064][root][INFO] - Training Epoch: 4/10, step 568/574 completed (loss: 0.060707900673151016, acc: 1.0)
[2024-12-12 02:40:13,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:13,438][root][INFO] - Training Epoch: 4/10, step 569/574 completed (loss: 1.5782394409179688, acc: 0.5668449401855469)
[2024-12-12 02:40:13,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:13,726][root][INFO] - Training Epoch: 4/10, step 570/574 completed (loss: 0.8142751455307007, acc: 0.7419354915618896)
[2024-12-12 02:40:13,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:14,031][root][INFO] - Training Epoch: 4/10, step 571/574 completed (loss: 1.2214432954788208, acc: 0.632478654384613)
[2024-12-12 02:40:14,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:14,355][root][INFO] - Training Epoch: 4/10, step 572/574 completed (loss: 2.0006191730499268, acc: 0.4642857015132904)
[2024-12-12 02:40:14,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:14,695][root][INFO] - Training Epoch: 4/10, step 573/574 completed (loss: 1.8133128881454468, acc: 0.5031446814537048)
[2024-12-12 02:40:15,088][slam_llm.utils.train_utils][INFO] - Epoch 4: train_perplexity=3.3386, train_epoch_loss=1.2056, epoch time 351.6724964790046s
[2024-12-12 02:40:15,089][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 02:40:15,089][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 14 GB
[2024-12-12 02:40:15,089][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 02:40:15,089][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 12
[2024-12-12 02:40:15,089][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:40:15,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:15,901][root][INFO] - Training Epoch: 5/10, step 0/574 completed (loss: 0.5198867321014404, acc: 0.8518518805503845)
[2024-12-12 02:40:16,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:16,240][root][INFO] - Training Epoch: 5/10, step 1/574 completed (loss: 0.5296828150749207, acc: 0.8799999952316284)
[2024-12-12 02:40:16,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:16,568][root][INFO] - Training Epoch: 5/10, step 2/574 completed (loss: 1.6102362871170044, acc: 0.5945945978164673)
[2024-12-12 02:40:16,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:16,974][root][INFO] - Training Epoch: 5/10, step 3/574 completed (loss: 1.1723592281341553, acc: 0.6315789222717285)
[2024-12-12 02:40:17,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:17,305][root][INFO] - Training Epoch: 5/10, step 4/574 completed (loss: 1.2092466354370117, acc: 0.5945945978164673)
[2024-12-12 02:40:17,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:17,672][root][INFO] - Training Epoch: 5/10, step 5/574 completed (loss: 0.7949391007423401, acc: 0.7142857313156128)
[2024-12-12 02:40:17,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:18,032][root][INFO] - Training Epoch: 5/10, step 6/574 completed (loss: 1.4512410163879395, acc: 0.6122449040412903)
[2024-12-12 02:40:18,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:18,422][root][INFO] - Training Epoch: 5/10, step 7/574 completed (loss: 0.7886921167373657, acc: 0.8333333134651184)
[2024-12-12 02:40:18,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:18,804][root][INFO] - Training Epoch: 5/10, step 8/574 completed (loss: 0.1545601189136505, acc: 0.9545454382896423)
[2024-12-12 02:40:18,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:19,169][root][INFO] - Training Epoch: 5/10, step 9/574 completed (loss: 0.2593640983104706, acc: 0.8846153616905212)
[2024-12-12 02:40:19,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:19,531][root][INFO] - Training Epoch: 5/10, step 10/574 completed (loss: 0.5249078869819641, acc: 0.8888888955116272)
[2024-12-12 02:40:19,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:19,894][root][INFO] - Training Epoch: 5/10, step 11/574 completed (loss: 1.0174628496170044, acc: 0.7435897588729858)
[2024-12-12 02:40:20,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:20,246][root][INFO] - Training Epoch: 5/10, step 12/574 completed (loss: 0.6955249905586243, acc: 0.8181818127632141)
[2024-12-12 02:40:20,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:20,664][root][INFO] - Training Epoch: 5/10, step 13/574 completed (loss: 0.9046476483345032, acc: 0.782608687877655)
[2024-12-12 02:40:20,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:21,011][root][INFO] - Training Epoch: 5/10, step 14/574 completed (loss: 1.5410983562469482, acc: 0.5686274766921997)
[2024-12-12 02:40:21,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:21,352][root][INFO] - Training Epoch: 5/10, step 15/574 completed (loss: 1.392601490020752, acc: 0.6530612111091614)
[2024-12-12 02:40:21,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:21,673][root][INFO] - Training Epoch: 5/10, step 16/574 completed (loss: 0.2516394853591919, acc: 0.8947368264198303)
[2024-12-12 02:40:21,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:21,985][root][INFO] - Training Epoch: 5/10, step 17/574 completed (loss: 0.5720160603523254, acc: 0.875)
[2024-12-12 02:40:22,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:22,305][root][INFO] - Training Epoch: 5/10, step 18/574 completed (loss: 1.191340446472168, acc: 0.6666666865348816)
[2024-12-12 02:40:22,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:22,617][root][INFO] - Training Epoch: 5/10, step 19/574 completed (loss: 0.3165554106235504, acc: 0.8947368264198303)
[2024-12-12 02:40:22,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:22,976][root][INFO] - Training Epoch: 5/10, step 20/574 completed (loss: 0.4764404296875, acc: 0.8461538553237915)
[2024-12-12 02:40:23,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:23,327][root][INFO] - Training Epoch: 5/10, step 21/574 completed (loss: 0.7378718852996826, acc: 0.7586206793785095)
[2024-12-12 02:40:23,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:23,701][root][INFO] - Training Epoch: 5/10, step 22/574 completed (loss: 0.46369850635528564, acc: 0.8399999737739563)
[2024-12-12 02:40:23,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:24,102][root][INFO] - Training Epoch: 5/10, step 23/574 completed (loss: 0.17230141162872314, acc: 0.9523809552192688)
[2024-12-12 02:40:24,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:24,473][root][INFO] - Training Epoch: 5/10, step 24/574 completed (loss: 0.7832671403884888, acc: 0.75)
[2024-12-12 02:40:24,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:24,826][root][INFO] - Training Epoch: 5/10, step 25/574 completed (loss: 1.6741446256637573, acc: 0.4528301954269409)
[2024-12-12 02:40:24,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:25,184][root][INFO] - Training Epoch: 5/10, step 26/574 completed (loss: 1.7310293912887573, acc: 0.4931506812572479)
[2024-12-12 02:40:25,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:26,459][root][INFO] - Training Epoch: 5/10, step 27/574 completed (loss: 2.035015821456909, acc: 0.4466403126716614)
[2024-12-12 02:40:26,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:26,805][root][INFO] - Training Epoch: 5/10, step 28/574 completed (loss: 1.3535399436950684, acc: 0.604651153087616)
[2024-12-12 02:40:26,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:27,118][root][INFO] - Training Epoch: 5/10, step 29/574 completed (loss: 1.6387128829956055, acc: 0.5060241222381592)
[2024-12-12 02:40:27,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:27,486][root][INFO] - Training Epoch: 5/10, step 30/574 completed (loss: 1.56243097782135, acc: 0.5679012537002563)
[2024-12-12 02:40:27,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:27,832][root][INFO] - Training Epoch: 5/10, step 31/574 completed (loss: 0.9835538864135742, acc: 0.75)
[2024-12-12 02:40:27,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:28,145][root][INFO] - Training Epoch: 5/10, step 32/574 completed (loss: 0.39042752981185913, acc: 0.9259259104728699)
[2024-12-12 02:40:28,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:28,507][root][INFO] - Training Epoch: 5/10, step 33/574 completed (loss: 0.11101173609495163, acc: 1.0)
[2024-12-12 02:40:28,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:28,876][root][INFO] - Training Epoch: 5/10, step 34/574 completed (loss: 1.6422284841537476, acc: 0.529411792755127)
[2024-12-12 02:40:28,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:29,200][root][INFO] - Training Epoch: 5/10, step 35/574 completed (loss: 1.2746400833129883, acc: 0.688524603843689)
[2024-12-12 02:40:29,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:29,602][root][INFO] - Training Epoch: 5/10, step 36/574 completed (loss: 1.6384142637252808, acc: 0.4920634925365448)
[2024-12-12 02:40:29,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:29,977][root][INFO] - Training Epoch: 5/10, step 37/574 completed (loss: 1.4720007181167603, acc: 0.5593220591545105)
[2024-12-12 02:40:30,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:30,374][root][INFO] - Training Epoch: 5/10, step 38/574 completed (loss: 1.136532187461853, acc: 0.6436781883239746)
[2024-12-12 02:40:30,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:30,689][root][INFO] - Training Epoch: 5/10, step 39/574 completed (loss: 0.5609174370765686, acc: 0.8571428656578064)
[2024-12-12 02:40:30,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:30,996][root][INFO] - Training Epoch: 5/10, step 40/574 completed (loss: 1.0595186948776245, acc: 0.807692289352417)
[2024-12-12 02:40:31,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:31,357][root][INFO] - Training Epoch: 5/10, step 41/574 completed (loss: 1.8097814321517944, acc: 0.5675675868988037)
[2024-12-12 02:40:31,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:31,788][root][INFO] - Training Epoch: 5/10, step 42/574 completed (loss: 1.5736069679260254, acc: 0.5384615659713745)
[2024-12-12 02:40:31,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:32,209][root][INFO] - Training Epoch: 5/10, step 43/574 completed (loss: 1.8178151845932007, acc: 0.5252525210380554)
[2024-12-12 02:40:32,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:32,613][root][INFO] - Training Epoch: 5/10, step 44/574 completed (loss: 1.491485595703125, acc: 0.5979381203651428)
[2024-12-12 02:40:32,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:33,038][root][INFO] - Training Epoch: 5/10, step 45/574 completed (loss: 1.8661541938781738, acc: 0.5073529481887817)
[2024-12-12 02:40:33,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:33,410][root][INFO] - Training Epoch: 5/10, step 46/574 completed (loss: 0.36672157049179077, acc: 0.9615384340286255)
[2024-12-12 02:40:33,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:33,750][root][INFO] - Training Epoch: 5/10, step 47/574 completed (loss: 0.2881472110748291, acc: 0.9259259104728699)
[2024-12-12 02:40:33,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:34,099][root][INFO] - Training Epoch: 5/10, step 48/574 completed (loss: 0.7475711703300476, acc: 0.75)
[2024-12-12 02:40:34,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:34,423][root][INFO] - Training Epoch: 5/10, step 49/574 completed (loss: 0.4710760712623596, acc: 0.8055555820465088)
[2024-12-12 02:40:34,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:34,747][root][INFO] - Training Epoch: 5/10, step 50/574 completed (loss: 1.0020878314971924, acc: 0.7017543911933899)
[2024-12-12 02:40:34,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:35,098][root][INFO] - Training Epoch: 5/10, step 51/574 completed (loss: 1.0785104036331177, acc: 0.7142857313156128)
[2024-12-12 02:40:35,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:35,441][root][INFO] - Training Epoch: 5/10, step 52/574 completed (loss: 1.6497067213058472, acc: 0.5352112650871277)
[2024-12-12 02:40:35,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:35,886][root][INFO] - Training Epoch: 5/10, step 53/574 completed (loss: 2.0398788452148438, acc: 0.47333332896232605)
[2024-12-12 02:40:35,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:36,247][root][INFO] - Training Epoch: 5/10, step 54/574 completed (loss: 0.7566275000572205, acc: 0.7837837934494019)
[2024-12-12 02:40:36,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:36,651][root][INFO] - Training Epoch: 5/10, step 55/574 completed (loss: 0.17921286821365356, acc: 0.9615384340286255)
[2024-12-12 02:40:38,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:39,626][root][INFO] - Training Epoch: 5/10, step 56/574 completed (loss: 1.699584722518921, acc: 0.5631399154663086)
[2024-12-12 02:40:40,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:40,959][root][INFO] - Training Epoch: 5/10, step 57/574 completed (loss: 2.2644901275634766, acc: 0.4161219894886017)
[2024-12-12 02:40:41,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:41,585][root][INFO] - Training Epoch: 5/10, step 58/574 completed (loss: 1.8593212366104126, acc: 0.5454545617103577)
[2024-12-12 02:40:41,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:42,158][root][INFO] - Training Epoch: 5/10, step 59/574 completed (loss: 1.8185417652130127, acc: 0.5588235259056091)
[2024-12-12 02:40:42,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:42,721][root][INFO] - Training Epoch: 5/10, step 60/574 completed (loss: 1.9910056591033936, acc: 0.4275362193584442)
[2024-12-12 02:40:42,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:43,129][root][INFO] - Training Epoch: 5/10, step 61/574 completed (loss: 1.4275906085968018, acc: 0.6499999761581421)
[2024-12-12 02:40:43,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:43,504][root][INFO] - Training Epoch: 5/10, step 62/574 completed (loss: 0.41164422035217285, acc: 0.8529411554336548)
[2024-12-12 02:40:43,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:43,902][root][INFO] - Training Epoch: 5/10, step 63/574 completed (loss: 1.0163216590881348, acc: 0.75)
[2024-12-12 02:40:44,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:44,260][root][INFO] - Training Epoch: 5/10, step 64/574 completed (loss: 0.9890211820602417, acc: 0.75)
[2024-12-12 02:40:44,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:44,560][root][INFO] - Training Epoch: 5/10, step 65/574 completed (loss: 0.30027833580970764, acc: 0.931034505367279)
[2024-12-12 02:40:44,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:44,908][root][INFO] - Training Epoch: 5/10, step 66/574 completed (loss: 1.4160865545272827, acc: 0.6071428656578064)
[2024-12-12 02:40:45,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:45,258][root][INFO] - Training Epoch: 5/10, step 67/574 completed (loss: 1.4493449926376343, acc: 0.5166666507720947)
[2024-12-12 02:40:45,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:45,638][root][INFO] - Training Epoch: 5/10, step 68/574 completed (loss: 0.13880248367786407, acc: 1.0)
[2024-12-12 02:40:45,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:45,994][root][INFO] - Training Epoch: 5/10, step 69/574 completed (loss: 0.8460016250610352, acc: 0.75)
[2024-12-12 02:40:46,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:46,380][root][INFO] - Training Epoch: 5/10, step 70/574 completed (loss: 0.575760543346405, acc: 0.8484848737716675)
[2024-12-12 02:40:46,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:46,771][root][INFO] - Training Epoch: 5/10, step 71/574 completed (loss: 1.6730737686157227, acc: 0.5808823704719543)
[2024-12-12 02:40:46,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:47,130][root][INFO] - Training Epoch: 5/10, step 72/574 completed (loss: 1.7371902465820312, acc: 0.5158730149269104)
[2024-12-12 02:40:47,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:47,504][root][INFO] - Training Epoch: 5/10, step 73/574 completed (loss: 2.010190725326538, acc: 0.4307692348957062)
[2024-12-12 02:40:47,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:47,860][root][INFO] - Training Epoch: 5/10, step 74/574 completed (loss: 1.3322505950927734, acc: 0.6122449040412903)
[2024-12-12 02:40:47,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:48,188][root][INFO] - Training Epoch: 5/10, step 75/574 completed (loss: 2.1417183876037598, acc: 0.41791045665740967)
[2024-12-12 02:40:48,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:48,584][root][INFO] - Training Epoch: 5/10, step 76/574 completed (loss: 2.122533082962036, acc: 0.4562043845653534)
[2024-12-12 02:40:48,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:48,940][root][INFO] - Training Epoch: 5/10, step 77/574 completed (loss: 0.13423149287700653, acc: 0.9523809552192688)
[2024-12-12 02:40:49,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:49,323][root][INFO] - Training Epoch: 5/10, step 78/574 completed (loss: 0.3152541220188141, acc: 0.9583333134651184)
[2024-12-12 02:40:49,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:49,662][root][INFO] - Training Epoch: 5/10, step 79/574 completed (loss: 0.4593409299850464, acc: 0.8787878751754761)
[2024-12-12 02:40:49,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:49,998][root][INFO] - Training Epoch: 5/10, step 80/574 completed (loss: 0.15596461296081543, acc: 0.9615384340286255)
[2024-12-12 02:40:50,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:50,313][root][INFO] - Training Epoch: 5/10, step 81/574 completed (loss: 1.339590072631836, acc: 0.6153846383094788)
[2024-12-12 02:40:50,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:50,681][root][INFO] - Training Epoch: 5/10, step 82/574 completed (loss: 1.4192657470703125, acc: 0.6153846383094788)
[2024-12-12 02:40:50,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:51,045][root][INFO] - Training Epoch: 5/10, step 83/574 completed (loss: 0.6179696321487427, acc: 0.78125)
[2024-12-12 02:40:51,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:51,427][root][INFO] - Training Epoch: 5/10, step 84/574 completed (loss: 1.435543417930603, acc: 0.6231883764266968)
[2024-12-12 02:40:51,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:51,770][root][INFO] - Training Epoch: 5/10, step 85/574 completed (loss: 1.1056325435638428, acc: 0.6800000071525574)
[2024-12-12 02:40:51,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:52,117][root][INFO] - Training Epoch: 5/10, step 86/574 completed (loss: 0.2538042366504669, acc: 0.95652174949646)
[2024-12-12 02:40:52,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:52,570][root][INFO] - Training Epoch: 5/10, step 87/574 completed (loss: 1.4905890226364136, acc: 0.5600000023841858)
[2024-12-12 02:40:52,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:52,905][root][INFO] - Training Epoch: 5/10, step 88/574 completed (loss: 1.5641467571258545, acc: 0.5631067752838135)
[2024-12-12 02:40:53,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:54,070][root][INFO] - Training Epoch: 5/10, step 89/574 completed (loss: 1.6928236484527588, acc: 0.5631067752838135)
[2024-12-12 02:40:54,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:54,892][root][INFO] - Training Epoch: 5/10, step 90/574 completed (loss: 1.8122162818908691, acc: 0.49462366104125977)
[2024-12-12 02:40:55,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:55,704][root][INFO] - Training Epoch: 5/10, step 91/574 completed (loss: 1.6902121305465698, acc: 0.5603448152542114)
[2024-12-12 02:40:55,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:56,453][root][INFO] - Training Epoch: 5/10, step 92/574 completed (loss: 1.2525827884674072, acc: 0.621052622795105)
[2024-12-12 02:40:56,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:57,446][root][INFO] - Training Epoch: 5/10, step 93/574 completed (loss: 1.9231466054916382, acc: 0.4455445408821106)
[2024-12-12 02:40:57,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:57,791][root][INFO] - Training Epoch: 5/10, step 94/574 completed (loss: 1.87833833694458, acc: 0.4677419364452362)
[2024-12-12 02:40:57,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:58,186][root][INFO] - Training Epoch: 5/10, step 95/574 completed (loss: 1.837960958480835, acc: 0.5072463750839233)
[2024-12-12 02:40:58,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:58,527][root][INFO] - Training Epoch: 5/10, step 96/574 completed (loss: 2.1041131019592285, acc: 0.38655462861061096)
[2024-12-12 02:40:58,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:58,847][root][INFO] - Training Epoch: 5/10, step 97/574 completed (loss: 1.934153437614441, acc: 0.45192307233810425)
[2024-12-12 02:40:58,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:59,225][root][INFO] - Training Epoch: 5/10, step 98/574 completed (loss: 2.0596399307250977, acc: 0.43795621395111084)
[2024-12-12 02:40:59,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:59,601][root][INFO] - Training Epoch: 5/10, step 99/574 completed (loss: 1.7914925813674927, acc: 0.5074626803398132)
[2024-12-12 02:40:59,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:40:59,947][root][INFO] - Training Epoch: 5/10, step 100/574 completed (loss: 0.46198874711990356, acc: 0.949999988079071)
[2024-12-12 02:41:00,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:00,266][root][INFO] - Training Epoch: 5/10, step 101/574 completed (loss: 0.1035642921924591, acc: 1.0)
[2024-12-12 02:41:00,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:00,636][root][INFO] - Training Epoch: 5/10, step 102/574 completed (loss: 0.14746932685375214, acc: 1.0)
[2024-12-12 02:41:00,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:00,974][root][INFO] - Training Epoch: 5/10, step 103/574 completed (loss: 0.58909010887146, acc: 0.8863636255264282)
[2024-12-12 02:41:01,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:01,341][root][INFO] - Training Epoch: 5/10, step 104/574 completed (loss: 1.4766260385513306, acc: 0.568965494632721)
[2024-12-12 02:41:01,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:01,700][root][INFO] - Training Epoch: 5/10, step 105/574 completed (loss: 0.7038975954055786, acc: 0.7906976938247681)
[2024-12-12 02:41:01,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:02,062][root][INFO] - Training Epoch: 5/10, step 106/574 completed (loss: 0.511405348777771, acc: 0.8799999952316284)
[2024-12-12 02:41:02,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:02,423][root][INFO] - Training Epoch: 5/10, step 107/574 completed (loss: 0.07740576565265656, acc: 1.0)
[2024-12-12 02:41:02,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:02,777][root][INFO] - Training Epoch: 5/10, step 108/574 completed (loss: 0.03742204234004021, acc: 1.0)
[2024-12-12 02:41:02,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:03,156][root][INFO] - Training Epoch: 5/10, step 109/574 completed (loss: 0.8631728887557983, acc: 0.7142857313156128)
[2024-12-12 02:41:03,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:03,501][root][INFO] - Training Epoch: 5/10, step 110/574 completed (loss: 1.4413872957229614, acc: 0.6153846383094788)
[2024-12-12 02:41:03,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:03,893][root][INFO] - Training Epoch: 5/10, step 111/574 completed (loss: 1.4118001461029053, acc: 0.6491228342056274)
[2024-12-12 02:41:03,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:04,277][root][INFO] - Training Epoch: 5/10, step 112/574 completed (loss: 1.1662535667419434, acc: 0.6491228342056274)
[2024-12-12 02:41:04,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:04,648][root][INFO] - Training Epoch: 5/10, step 113/574 completed (loss: 1.2570699453353882, acc: 0.692307710647583)
[2024-12-12 02:41:04,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:05,010][root][INFO] - Training Epoch: 5/10, step 114/574 completed (loss: 0.9733591675758362, acc: 0.7142857313156128)
[2024-12-12 02:41:05,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:05,407][root][INFO] - Training Epoch: 5/10, step 115/574 completed (loss: 0.04271933063864708, acc: 1.0)
[2024-12-12 02:41:05,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:05,787][root][INFO] - Training Epoch: 5/10, step 116/574 completed (loss: 1.3978643417358398, acc: 0.6666666865348816)
[2024-12-12 02:41:05,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:06,166][root][INFO] - Training Epoch: 5/10, step 117/574 completed (loss: 1.5433976650238037, acc: 0.642276406288147)
[2024-12-12 02:41:06,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:06,580][root][INFO] - Training Epoch: 5/10, step 118/574 completed (loss: 1.1291276216506958, acc: 0.6774193644523621)
[2024-12-12 02:41:06,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:07,450][root][INFO] - Training Epoch: 5/10, step 119/574 completed (loss: 1.834515929222107, acc: 0.5057034492492676)
[2024-12-12 02:41:07,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:07,799][root][INFO] - Training Epoch: 5/10, step 120/574 completed (loss: 1.073803424835205, acc: 0.6666666865348816)
[2024-12-12 02:41:07,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:08,221][root][INFO] - Training Epoch: 5/10, step 121/574 completed (loss: 1.174499750137329, acc: 0.7115384340286255)
[2024-12-12 02:41:08,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:08,547][root][INFO] - Training Epoch: 5/10, step 122/574 completed (loss: 0.4494796097278595, acc: 0.9166666865348816)
[2024-12-12 02:41:08,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:08,936][root][INFO] - Training Epoch: 5/10, step 123/574 completed (loss: 0.09351861476898193, acc: 1.0)
[2024-12-12 02:41:09,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:09,341][root][INFO] - Training Epoch: 5/10, step 124/574 completed (loss: 1.7195230722427368, acc: 0.5276073813438416)
[2024-12-12 02:41:09,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:09,711][root][INFO] - Training Epoch: 5/10, step 125/574 completed (loss: 1.574586272239685, acc: 0.5833333134651184)
[2024-12-12 02:41:09,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:10,098][root][INFO] - Training Epoch: 5/10, step 126/574 completed (loss: 1.8359262943267822, acc: 0.4583333432674408)
[2024-12-12 02:41:10,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:10,495][root][INFO] - Training Epoch: 5/10, step 127/574 completed (loss: 1.778581142425537, acc: 0.4821428656578064)
[2024-12-12 02:41:10,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:10,879][root][INFO] - Training Epoch: 5/10, step 128/574 completed (loss: 1.6545168161392212, acc: 0.5435897707939148)
[2024-12-12 02:41:11,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:11,309][root][INFO] - Training Epoch: 5/10, step 129/574 completed (loss: 1.6038689613342285, acc: 0.5220588445663452)
[2024-12-12 02:41:11,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:11,658][root][INFO] - Training Epoch: 5/10, step 130/574 completed (loss: 0.5684478282928467, acc: 0.807692289352417)
[2024-12-12 02:41:11,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:12,014][root][INFO] - Training Epoch: 5/10, step 131/574 completed (loss: 0.2103787362575531, acc: 0.9130434989929199)
[2024-12-12 02:41:12,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:12,327][root][INFO] - Training Epoch: 5/10, step 132/574 completed (loss: 0.34607797861099243, acc: 0.90625)
[2024-12-12 02:41:12,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:12,581][root][INFO] - Training Epoch: 5/10, step 133/574 completed (loss: 0.42328932881355286, acc: 0.8695651888847351)
[2024-12-12 02:41:12,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:12,908][root][INFO] - Training Epoch: 5/10, step 134/574 completed (loss: 0.8270713686943054, acc: 0.800000011920929)
[2024-12-12 02:41:13,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:14,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:14,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:14,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:15,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:15,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:15,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:16,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:16,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:16,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:17,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:17,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:18,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:18,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:18,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:19,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:19,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:19,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:20,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:20,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:20,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:21,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:21,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:21,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:22,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:22,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:22,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:23,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:23,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:24,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:24,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:24,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:25,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:25,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:25,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:26,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:26,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:26,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:27,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:27,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:27,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:28,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:28,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:28,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:29,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:29,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:29,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:30,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:30,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:30,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:31,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:31,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:32,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:32,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:32,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:32,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:33,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:33,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:34,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:34,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:34,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:35,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:35,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:35,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:36,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:36,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:37,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:37,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:37,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:38,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:38,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:38,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:39,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:39,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:39,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:40,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:40,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:40,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:41,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:41,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:41,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:42,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:42,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:42,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:43,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:43,730][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.8796, device='cuda:0') eval_epoch_loss=tensor(1.5851, device='cuda:0') eval_epoch_acc=tensor(0.6108, device='cuda:0')
[2024-12-12 02:41:43,731][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:41:43,731][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:41:43,936][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_5_step_135_loss_1.5850577354431152/model.pt
[2024-12-12 02:41:43,940][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:41:43,940][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 5 is 1.5850577354431152
[2024-12-12 02:41:43,941][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 5 is 0.6107640266418457
[2024-12-12 02:41:44,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:44,263][root][INFO] - Training Epoch: 5/10, step 135/574 completed (loss: 0.5034550428390503, acc: 0.7692307829856873)
[2024-12-12 02:41:44,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:44,564][root][INFO] - Training Epoch: 5/10, step 136/574 completed (loss: 0.9518404603004456, acc: 0.738095223903656)
[2024-12-12 02:41:44,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:44,870][root][INFO] - Training Epoch: 5/10, step 137/574 completed (loss: 0.8864548802375793, acc: 0.7333333492279053)
[2024-12-12 02:41:44,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:45,172][root][INFO] - Training Epoch: 5/10, step 138/574 completed (loss: 0.4997902810573578, acc: 0.8260869383811951)
[2024-12-12 02:41:45,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:45,510][root][INFO] - Training Epoch: 5/10, step 139/574 completed (loss: 0.5226678848266602, acc: 0.8571428656578064)
[2024-12-12 02:41:45,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:45,881][root][INFO] - Training Epoch: 5/10, step 140/574 completed (loss: 0.5475645661354065, acc: 0.807692289352417)
[2024-12-12 02:41:45,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:46,262][root][INFO] - Training Epoch: 5/10, step 141/574 completed (loss: 1.0862542390823364, acc: 0.6774193644523621)
[2024-12-12 02:41:46,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:46,605][root][INFO] - Training Epoch: 5/10, step 142/574 completed (loss: 1.3314495086669922, acc: 0.6216216087341309)
[2024-12-12 02:41:46,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:47,132][root][INFO] - Training Epoch: 5/10, step 143/574 completed (loss: 1.6242852210998535, acc: 0.4736842215061188)
[2024-12-12 02:41:47,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:47,472][root][INFO] - Training Epoch: 5/10, step 144/574 completed (loss: 1.4432166814804077, acc: 0.6268656849861145)
[2024-12-12 02:41:47,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:47,833][root][INFO] - Training Epoch: 5/10, step 145/574 completed (loss: 1.808989405632019, acc: 0.4693877696990967)
[2024-12-12 02:41:47,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:48,263][root][INFO] - Training Epoch: 5/10, step 146/574 completed (loss: 1.7482420206069946, acc: 0.4893617033958435)
[2024-12-12 02:41:48,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:48,595][root][INFO] - Training Epoch: 5/10, step 147/574 completed (loss: 1.4009666442871094, acc: 0.6142857074737549)
[2024-12-12 02:41:48,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:48,955][root][INFO] - Training Epoch: 5/10, step 148/574 completed (loss: 1.2499598264694214, acc: 0.6428571343421936)
[2024-12-12 02:41:49,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:49,255][root][INFO] - Training Epoch: 5/10, step 149/574 completed (loss: 0.8044715523719788, acc: 0.739130437374115)
[2024-12-12 02:41:49,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:49,606][root][INFO] - Training Epoch: 5/10, step 150/574 completed (loss: 0.8726792335510254, acc: 0.7931034564971924)
[2024-12-12 02:41:49,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:49,971][root][INFO] - Training Epoch: 5/10, step 151/574 completed (loss: 1.5693992376327515, acc: 0.6521739363670349)
[2024-12-12 02:41:50,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:50,339][root][INFO] - Training Epoch: 5/10, step 152/574 completed (loss: 1.4704886674880981, acc: 0.5762711763381958)
[2024-12-12 02:41:50,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:50,707][root][INFO] - Training Epoch: 5/10, step 153/574 completed (loss: 1.5132627487182617, acc: 0.4912280738353729)
[2024-12-12 02:41:50,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:51,087][root][INFO] - Training Epoch: 5/10, step 154/574 completed (loss: 1.5379884243011475, acc: 0.5810810923576355)
[2024-12-12 02:41:51,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:51,447][root][INFO] - Training Epoch: 5/10, step 155/574 completed (loss: 0.9473627805709839, acc: 0.7142857313156128)
[2024-12-12 02:41:51,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:51,771][root][INFO] - Training Epoch: 5/10, step 156/574 completed (loss: 0.6360015869140625, acc: 0.782608687877655)
[2024-12-12 02:41:51,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:52,042][root][INFO] - Training Epoch: 5/10, step 157/574 completed (loss: 1.1720819473266602, acc: 0.6842105388641357)
[2024-12-12 02:41:52,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:53,673][root][INFO] - Training Epoch: 5/10, step 158/574 completed (loss: 1.3579022884368896, acc: 0.6351351141929626)
[2024-12-12 02:41:53,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:54,075][root][INFO] - Training Epoch: 5/10, step 159/574 completed (loss: 1.5879689455032349, acc: 0.5740740895271301)
[2024-12-12 02:41:54,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:54,501][root][INFO] - Training Epoch: 5/10, step 160/574 completed (loss: 1.480109691619873, acc: 0.604651153087616)
[2024-12-12 02:41:54,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:55,090][root][INFO] - Training Epoch: 5/10, step 161/574 completed (loss: 1.2179163694381714, acc: 0.6117647290229797)
[2024-12-12 02:41:55,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:55,650][root][INFO] - Training Epoch: 5/10, step 162/574 completed (loss: 1.7358441352844238, acc: 0.5056179761886597)
[2024-12-12 02:41:55,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:55,991][root][INFO] - Training Epoch: 5/10, step 163/574 completed (loss: 0.99430251121521, acc: 0.6818181872367859)
[2024-12-12 02:41:56,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:56,345][root][INFO] - Training Epoch: 5/10, step 164/574 completed (loss: 0.5324320793151855, acc: 0.8095238208770752)
[2024-12-12 02:41:56,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:56,721][root][INFO] - Training Epoch: 5/10, step 165/574 completed (loss: 0.8115168809890747, acc: 0.6551724076271057)
[2024-12-12 02:41:56,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:57,101][root][INFO] - Training Epoch: 5/10, step 166/574 completed (loss: 0.5132126808166504, acc: 0.8367347121238708)
[2024-12-12 02:41:57,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:57,483][root][INFO] - Training Epoch: 5/10, step 167/574 completed (loss: 0.7583730816841125, acc: 0.7400000095367432)
[2024-12-12 02:41:57,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:57,908][root][INFO] - Training Epoch: 5/10, step 168/574 completed (loss: 1.241532802581787, acc: 0.6388888955116272)
[2024-12-12 02:41:58,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:58,284][root][INFO] - Training Epoch: 5/10, step 169/574 completed (loss: 1.4936147928237915, acc: 0.6176470518112183)
[2024-12-12 02:41:58,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:59,320][root][INFO] - Training Epoch: 5/10, step 170/574 completed (loss: 2.120204448699951, acc: 0.4726027250289917)
[2024-12-12 02:41:59,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:59,627][root][INFO] - Training Epoch: 5/10, step 171/574 completed (loss: 0.3192048668861389, acc: 0.9166666865348816)
[2024-12-12 02:41:59,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:41:59,945][root][INFO] - Training Epoch: 5/10, step 172/574 completed (loss: 0.5684584379196167, acc: 0.8148148059844971)
[2024-12-12 02:42:00,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:00,320][root][INFO] - Training Epoch: 5/10, step 173/574 completed (loss: 0.5244544148445129, acc: 0.8214285969734192)
[2024-12-12 02:42:00,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:00,862][root][INFO] - Training Epoch: 5/10, step 174/574 completed (loss: 1.4330867528915405, acc: 0.6460176706314087)
[2024-12-12 02:42:00,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:01,134][root][INFO] - Training Epoch: 5/10, step 175/574 completed (loss: 1.1459779739379883, acc: 0.6811594367027283)
[2024-12-12 02:42:01,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:01,517][root][INFO] - Training Epoch: 5/10, step 176/574 completed (loss: 1.3685914278030396, acc: 0.6477272510528564)
[2024-12-12 02:42:01,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:02,427][root][INFO] - Training Epoch: 5/10, step 177/574 completed (loss: 2.1267271041870117, acc: 0.45038166642189026)
[2024-12-12 02:42:02,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:03,097][root][INFO] - Training Epoch: 5/10, step 178/574 completed (loss: 1.9323707818984985, acc: 0.4888888895511627)
[2024-12-12 02:42:03,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:03,456][root][INFO] - Training Epoch: 5/10, step 179/574 completed (loss: 1.0804715156555176, acc: 0.6721311211585999)
[2024-12-12 02:42:03,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:03,821][root][INFO] - Training Epoch: 5/10, step 180/574 completed (loss: 0.33087942004203796, acc: 0.9583333134651184)
[2024-12-12 02:42:03,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:04,157][root][INFO] - Training Epoch: 5/10, step 181/574 completed (loss: 0.5751820206642151, acc: 0.8399999737739563)
[2024-12-12 02:42:04,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:04,536][root][INFO] - Training Epoch: 5/10, step 182/574 completed (loss: 0.32874050736427307, acc: 0.8928571343421936)
[2024-12-12 02:42:04,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:04,924][root][INFO] - Training Epoch: 5/10, step 183/574 completed (loss: 1.4829318523406982, acc: 0.5609756112098694)
[2024-12-12 02:42:05,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:05,302][root][INFO] - Training Epoch: 5/10, step 184/574 completed (loss: 1.9381743669509888, acc: 0.4441087543964386)
[2024-12-12 02:42:05,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:05,726][root][INFO] - Training Epoch: 5/10, step 185/574 completed (loss: 2.1189465522766113, acc: 0.42363113164901733)
[2024-12-12 02:42:05,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:06,214][root][INFO] - Training Epoch: 5/10, step 186/574 completed (loss: 2.1528663635253906, acc: 0.4281249940395355)
[2024-12-12 02:42:06,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:06,741][root][INFO] - Training Epoch: 5/10, step 187/574 completed (loss: 2.090268850326538, acc: 0.4352720379829407)
[2024-12-12 02:42:06,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:07,155][root][INFO] - Training Epoch: 5/10, step 188/574 completed (loss: 1.9291905164718628, acc: 0.4661921560764313)
[2024-12-12 02:42:07,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:07,503][root][INFO] - Training Epoch: 5/10, step 189/574 completed (loss: 0.3521283268928528, acc: 0.9200000166893005)
[2024-12-12 02:42:07,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:08,055][root][INFO] - Training Epoch: 5/10, step 190/574 completed (loss: 1.957120656967163, acc: 0.43023255467414856)
[2024-12-12 02:42:08,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:08,853][root][INFO] - Training Epoch: 5/10, step 191/574 completed (loss: 1.8786530494689941, acc: 0.5634920597076416)
[2024-12-12 02:42:09,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:09,771][root][INFO] - Training Epoch: 5/10, step 192/574 completed (loss: 1.8211876153945923, acc: 0.5303030014038086)
[2024-12-12 02:42:09,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:10,516][root][INFO] - Training Epoch: 5/10, step 193/574 completed (loss: 1.361289381980896, acc: 0.6235294342041016)
[2024-12-12 02:42:10,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:11,596][root][INFO] - Training Epoch: 5/10, step 194/574 completed (loss: 1.4414305686950684, acc: 0.5987654328346252)
[2024-12-12 02:42:11,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:12,553][root][INFO] - Training Epoch: 5/10, step 195/574 completed (loss: 1.1233396530151367, acc: 0.6935483813285828)
[2024-12-12 02:42:12,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:12,912][root][INFO] - Training Epoch: 5/10, step 196/574 completed (loss: 0.36830493807792664, acc: 0.8928571343421936)
[2024-12-12 02:42:13,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:13,250][root][INFO] - Training Epoch: 5/10, step 197/574 completed (loss: 0.6332830786705017, acc: 0.824999988079071)
[2024-12-12 02:42:13,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:13,660][root][INFO] - Training Epoch: 5/10, step 198/574 completed (loss: 1.2074273824691772, acc: 0.6764705777168274)
[2024-12-12 02:42:13,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:14,001][root][INFO] - Training Epoch: 5/10, step 199/574 completed (loss: 1.5978413820266724, acc: 0.595588207244873)
[2024-12-12 02:42:14,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:14,350][root][INFO] - Training Epoch: 5/10, step 200/574 completed (loss: 1.678253173828125, acc: 0.5338982939720154)
[2024-12-12 02:42:14,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:14,725][root][INFO] - Training Epoch: 5/10, step 201/574 completed (loss: 1.839621901512146, acc: 0.4701492488384247)
[2024-12-12 02:42:14,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:15,117][root][INFO] - Training Epoch: 5/10, step 202/574 completed (loss: 1.7722994089126587, acc: 0.5145630836486816)
[2024-12-12 02:42:15,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:15,500][root][INFO] - Training Epoch: 5/10, step 203/574 completed (loss: 1.148350715637207, acc: 0.682539701461792)
[2024-12-12 02:42:15,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:15,818][root][INFO] - Training Epoch: 5/10, step 204/574 completed (loss: 1.1874264478683472, acc: 0.6703296899795532)
[2024-12-12 02:42:15,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:16,159][root][INFO] - Training Epoch: 5/10, step 205/574 completed (loss: 1.7678678035736084, acc: 0.5291479825973511)
[2024-12-12 02:42:16,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:16,570][root][INFO] - Training Epoch: 5/10, step 206/574 completed (loss: 1.8620073795318604, acc: 0.4881889820098877)
[2024-12-12 02:42:16,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:16,977][root][INFO] - Training Epoch: 5/10, step 207/574 completed (loss: 1.7081509828567505, acc: 0.5344827771186829)
[2024-12-12 02:42:17,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:17,333][root][INFO] - Training Epoch: 5/10, step 208/574 completed (loss: 1.7598661184310913, acc: 0.532608687877655)
[2024-12-12 02:42:17,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:17,730][root][INFO] - Training Epoch: 5/10, step 209/574 completed (loss: 1.8574397563934326, acc: 0.49805447459220886)
[2024-12-12 02:42:17,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:18,111][root][INFO] - Training Epoch: 5/10, step 210/574 completed (loss: 1.854039192199707, acc: 0.54347825050354)
[2024-12-12 02:42:18,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:18,474][root][INFO] - Training Epoch: 5/10, step 211/574 completed (loss: 0.3538529872894287, acc: 0.8260869383811951)
[2024-12-12 02:42:18,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:18,801][root][INFO] - Training Epoch: 5/10, step 212/574 completed (loss: 0.45510023832321167, acc: 0.8571428656578064)
[2024-12-12 02:42:18,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:19,138][root][INFO] - Training Epoch: 5/10, step 213/574 completed (loss: 0.6194406747817993, acc: 0.8723404407501221)
[2024-12-12 02:42:19,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:19,820][root][INFO] - Training Epoch: 5/10, step 214/574 completed (loss: 1.330419659614563, acc: 0.6384615302085876)
[2024-12-12 02:42:19,903][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:20,153][root][INFO] - Training Epoch: 5/10, step 215/574 completed (loss: 0.9221212863922119, acc: 0.7162162065505981)
[2024-12-12 02:42:20,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:20,573][root][INFO] - Training Epoch: 5/10, step 216/574 completed (loss: 1.0830082893371582, acc: 0.7209302186965942)
[2024-12-12 02:42:20,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:21,108][root][INFO] - Training Epoch: 5/10, step 217/574 completed (loss: 1.2007781267166138, acc: 0.6936936974525452)
[2024-12-12 02:42:21,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:21,492][root][INFO] - Training Epoch: 5/10, step 218/574 completed (loss: 1.0393306016921997, acc: 0.7111111283302307)
[2024-12-12 02:42:21,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:21,838][root][INFO] - Training Epoch: 5/10, step 219/574 completed (loss: 0.3722946345806122, acc: 0.9090909361839294)
[2024-12-12 02:42:21,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:22,148][root][INFO] - Training Epoch: 5/10, step 220/574 completed (loss: 0.21435756981372833, acc: 0.9629629850387573)
[2024-12-12 02:42:22,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:22,513][root][INFO] - Training Epoch: 5/10, step 221/574 completed (loss: 0.25029581785202026, acc: 0.8399999737739563)
[2024-12-12 02:42:22,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:22,903][root][INFO] - Training Epoch: 5/10, step 222/574 completed (loss: 1.034796953201294, acc: 0.7115384340286255)
[2024-12-12 02:42:23,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:23,665][root][INFO] - Training Epoch: 5/10, step 223/574 completed (loss: 1.2673871517181396, acc: 0.6086956262588501)
[2024-12-12 02:42:23,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:24,205][root][INFO] - Training Epoch: 5/10, step 224/574 completed (loss: 1.659665584564209, acc: 0.5795454382896423)
[2024-12-12 02:42:24,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:24,643][root][INFO] - Training Epoch: 5/10, step 225/574 completed (loss: 1.6836920976638794, acc: 0.4893617033958435)
[2024-12-12 02:42:24,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:25,047][root][INFO] - Training Epoch: 5/10, step 226/574 completed (loss: 0.8260658979415894, acc: 0.7547169923782349)
[2024-12-12 02:42:25,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:25,423][root][INFO] - Training Epoch: 5/10, step 227/574 completed (loss: 1.0869861841201782, acc: 0.6499999761581421)
[2024-12-12 02:42:25,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:25,777][root][INFO] - Training Epoch: 5/10, step 228/574 completed (loss: 0.612313985824585, acc: 0.8139534592628479)
[2024-12-12 02:42:25,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:26,141][root][INFO] - Training Epoch: 5/10, step 229/574 completed (loss: 0.6755359768867493, acc: 0.800000011920929)
[2024-12-12 02:42:26,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:26,546][root][INFO] - Training Epoch: 5/10, step 230/574 completed (loss: 1.9545061588287354, acc: 0.5052631497383118)
[2024-12-12 02:42:26,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:26,937][root][INFO] - Training Epoch: 5/10, step 231/574 completed (loss: 1.3649797439575195, acc: 0.6111111044883728)
[2024-12-12 02:42:27,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:27,350][root][INFO] - Training Epoch: 5/10, step 232/574 completed (loss: 1.283889889717102, acc: 0.6499999761581421)
[2024-12-12 02:42:27,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:27,840][root][INFO] - Training Epoch: 5/10, step 233/574 completed (loss: 1.6691807508468628, acc: 0.5688073635101318)
[2024-12-12 02:42:27,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:28,311][root][INFO] - Training Epoch: 5/10, step 234/574 completed (loss: 1.2991206645965576, acc: 0.6384615302085876)
[2024-12-12 02:42:28,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:28,610][root][INFO] - Training Epoch: 5/10, step 235/574 completed (loss: 0.1727360486984253, acc: 0.9473684430122375)
[2024-12-12 02:42:28,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:28,936][root][INFO] - Training Epoch: 5/10, step 236/574 completed (loss: 0.34566715359687805, acc: 0.9166666865348816)
[2024-12-12 02:42:29,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:29,284][root][INFO] - Training Epoch: 5/10, step 237/574 completed (loss: 0.7780552506446838, acc: 0.7272727489471436)
[2024-12-12 02:42:29,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:29,637][root][INFO] - Training Epoch: 5/10, step 238/574 completed (loss: 1.0780562162399292, acc: 0.5925925970077515)
[2024-12-12 02:42:29,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:29,967][root][INFO] - Training Epoch: 5/10, step 239/574 completed (loss: 0.6188360452651978, acc: 0.7142857313156128)
[2024-12-12 02:42:30,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:30,313][root][INFO] - Training Epoch: 5/10, step 240/574 completed (loss: 0.8105722665786743, acc: 0.7727272510528564)
[2024-12-12 02:42:30,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:30,646][root][INFO] - Training Epoch: 5/10, step 241/574 completed (loss: 0.7979822754859924, acc: 0.75)
[2024-12-12 02:42:30,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:31,229][root][INFO] - Training Epoch: 5/10, step 242/574 completed (loss: 1.6508734226226807, acc: 0.5161290168762207)
[2024-12-12 02:42:31,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:31,757][root][INFO] - Training Epoch: 5/10, step 243/574 completed (loss: 0.9418737292289734, acc: 0.6818181872367859)
[2024-12-12 02:42:31,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:32,100][root][INFO] - Training Epoch: 5/10, step 244/574 completed (loss: 0.028305038809776306, acc: 1.0)
[2024-12-12 02:42:32,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:32,391][root][INFO] - Training Epoch: 5/10, step 245/574 completed (loss: 0.37858450412750244, acc: 0.9230769276618958)
[2024-12-12 02:42:32,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:32,737][root][INFO] - Training Epoch: 5/10, step 246/574 completed (loss: 0.14172843098640442, acc: 0.9677419066429138)
[2024-12-12 02:42:32,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:33,063][root][INFO] - Training Epoch: 5/10, step 247/574 completed (loss: 0.16528278589248657, acc: 0.949999988079071)
[2024-12-12 02:42:33,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:33,404][root][INFO] - Training Epoch: 5/10, step 248/574 completed (loss: 0.5012862682342529, acc: 0.8648648858070374)
[2024-12-12 02:42:33,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:33,741][root][INFO] - Training Epoch: 5/10, step 249/574 completed (loss: 0.6810972690582275, acc: 0.8108108043670654)
[2024-12-12 02:42:33,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:34,107][root][INFO] - Training Epoch: 5/10, step 250/574 completed (loss: 0.7325711250305176, acc: 0.8108108043670654)
[2024-12-12 02:42:34,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:34,476][root][INFO] - Training Epoch: 5/10, step 251/574 completed (loss: 1.2378933429718018, acc: 0.6323529481887817)
[2024-12-12 02:42:34,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:34,804][root][INFO] - Training Epoch: 5/10, step 252/574 completed (loss: 0.3062460124492645, acc: 0.8780487775802612)
[2024-12-12 02:42:34,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:35,159][root][INFO] - Training Epoch: 5/10, step 253/574 completed (loss: 0.043481286615133286, acc: 1.0)
[2024-12-12 02:42:35,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:35,481][root][INFO] - Training Epoch: 5/10, step 254/574 completed (loss: 0.11350491642951965, acc: 0.9599999785423279)
[2024-12-12 02:42:35,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:35,846][root][INFO] - Training Epoch: 5/10, step 255/574 completed (loss: 0.22053466737270355, acc: 0.9032257795333862)
[2024-12-12 02:42:35,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:36,181][root][INFO] - Training Epoch: 5/10, step 256/574 completed (loss: 0.8112420439720154, acc: 0.7894737124443054)
[2024-12-12 02:42:36,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:36,543][root][INFO] - Training Epoch: 5/10, step 257/574 completed (loss: 1.0587126016616821, acc: 0.699999988079071)
[2024-12-12 02:42:36,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:36,945][root][INFO] - Training Epoch: 5/10, step 258/574 completed (loss: 0.8495753407478333, acc: 0.75)
[2024-12-12 02:42:37,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:37,512][root][INFO] - Training Epoch: 5/10, step 259/574 completed (loss: 1.3278449773788452, acc: 0.6037735939025879)
[2024-12-12 02:42:37,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:38,095][root][INFO] - Training Epoch: 5/10, step 260/574 completed (loss: 1.3680063486099243, acc: 0.6416666507720947)
[2024-12-12 02:42:38,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:38,461][root][INFO] - Training Epoch: 5/10, step 261/574 completed (loss: 0.4560052156448364, acc: 0.9166666865348816)
[2024-12-12 02:42:38,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:38,840][root][INFO] - Training Epoch: 5/10, step 262/574 completed (loss: 0.5539234280586243, acc: 0.8064516186714172)
[2024-12-12 02:42:38,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:39,233][root][INFO] - Training Epoch: 5/10, step 263/574 completed (loss: 2.057075262069702, acc: 0.4933333396911621)
[2024-12-12 02:42:39,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:39,601][root][INFO] - Training Epoch: 5/10, step 264/574 completed (loss: 1.6334376335144043, acc: 0.5833333134651184)
[2024-12-12 02:42:39,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:40,498][root][INFO] - Training Epoch: 5/10, step 265/574 completed (loss: 2.152601718902588, acc: 0.3919999897480011)
[2024-12-12 02:42:40,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:40,860][root][INFO] - Training Epoch: 5/10, step 266/574 completed (loss: 1.8673721551895142, acc: 0.449438214302063)
[2024-12-12 02:42:40,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:41,212][root][INFO] - Training Epoch: 5/10, step 267/574 completed (loss: 1.8671464920043945, acc: 0.4864864945411682)
[2024-12-12 02:42:41,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:41,671][root][INFO] - Training Epoch: 5/10, step 268/574 completed (loss: 1.0758816003799438, acc: 0.6379310488700867)
[2024-12-12 02:42:41,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:42,004][root][INFO] - Training Epoch: 5/10, step 269/574 completed (loss: 0.27224433422088623, acc: 0.9090909361839294)
[2024-12-12 02:42:42,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:42,334][root][INFO] - Training Epoch: 5/10, step 270/574 completed (loss: 0.34098494052886963, acc: 0.8636363744735718)
[2024-12-12 02:42:42,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:42,696][root][INFO] - Training Epoch: 5/10, step 271/574 completed (loss: 0.38513702154159546, acc: 0.875)
[2024-12-12 02:42:42,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:43,044][root][INFO] - Training Epoch: 5/10, step 272/574 completed (loss: 0.28154876828193665, acc: 0.8999999761581421)
[2024-12-12 02:42:43,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:43,415][root][INFO] - Training Epoch: 5/10, step 273/574 completed (loss: 1.2939773797988892, acc: 0.6499999761581421)
[2024-12-12 02:42:43,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:43,768][root][INFO] - Training Epoch: 5/10, step 274/574 completed (loss: 0.5157800912857056, acc: 0.8125)
[2024-12-12 02:42:43,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:44,129][root][INFO] - Training Epoch: 5/10, step 275/574 completed (loss: 0.3336934447288513, acc: 0.8999999761581421)
[2024-12-12 02:42:44,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:44,498][root][INFO] - Training Epoch: 5/10, step 276/574 completed (loss: 0.6686161756515503, acc: 0.8620689511299133)
[2024-12-12 02:42:44,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:44,789][root][INFO] - Training Epoch: 5/10, step 277/574 completed (loss: 0.4324336349964142, acc: 0.8799999952316284)
[2024-12-12 02:42:45,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:45,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:46,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:46,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:46,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:47,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:47,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:47,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:48,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:48,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:48,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:49,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:49,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:49,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:50,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:50,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:50,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:51,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:51,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:52,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:52,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:52,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:52,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:53,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:53,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:54,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:54,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:54,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:55,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:55,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:55,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:56,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:56,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:56,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:57,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:57,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:58,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:58,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:58,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:59,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:59,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:42:59,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:00,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:00,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:00,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:01,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:01,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:01,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:02,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:02,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:02,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:03,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:03,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:03,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:04,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:04,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:04,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:05,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:05,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:05,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:06,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:06,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:07,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:07,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:07,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:07,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:08,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:08,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:09,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:09,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:09,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:10,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:10,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:10,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:11,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:11,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:11,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:12,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:12,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:12,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:13,225][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:13,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:13,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:14,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:14,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:15,222][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.9690, device='cuda:0') eval_epoch_loss=tensor(1.6032, device='cuda:0') eval_epoch_acc=tensor(0.6093, device='cuda:0')
[2024-12-12 02:43:15,223][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:43:15,223][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:43:15,423][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_5_step_278_loss_1.6032086610794067/model.pt
[2024-12-12 02:43:15,426][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:43:15,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:15,833][root][INFO] - Training Epoch: 5/10, step 278/574 completed (loss: 1.0019844770431519, acc: 0.7234042286872864)
[2024-12-12 02:43:15,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:16,219][root][INFO] - Training Epoch: 5/10, step 279/574 completed (loss: 0.947270393371582, acc: 0.7083333134651184)
[2024-12-12 02:43:16,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:16,576][root][INFO] - Training Epoch: 5/10, step 280/574 completed (loss: 0.6518650650978088, acc: 0.8409090638160706)
[2024-12-12 02:43:16,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:16,987][root][INFO] - Training Epoch: 5/10, step 281/574 completed (loss: 1.6216996908187866, acc: 0.5903614163398743)
[2024-12-12 02:43:17,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:17,349][root][INFO] - Training Epoch: 5/10, step 282/574 completed (loss: 1.6112066507339478, acc: 0.5277777910232544)
[2024-12-12 02:43:17,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:17,704][root][INFO] - Training Epoch: 5/10, step 283/574 completed (loss: 0.8541021943092346, acc: 0.7105262875556946)
[2024-12-12 02:43:17,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:18,037][root][INFO] - Training Epoch: 5/10, step 284/574 completed (loss: 0.4264857769012451, acc: 0.8529411554336548)
[2024-12-12 02:43:18,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:18,407][root][INFO] - Training Epoch: 5/10, step 285/574 completed (loss: 0.3974868953227997, acc: 0.8500000238418579)
[2024-12-12 02:43:18,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:18,739][root][INFO] - Training Epoch: 5/10, step 286/574 completed (loss: 1.7159467935562134, acc: 0.4921875)
[2024-12-12 02:43:18,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:19,069][root][INFO] - Training Epoch: 5/10, step 287/574 completed (loss: 1.8152419328689575, acc: 0.5360000133514404)
[2024-12-12 02:43:19,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:19,364][root][INFO] - Training Epoch: 5/10, step 288/574 completed (loss: 1.1772794723510742, acc: 0.6593406796455383)
[2024-12-12 02:43:19,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:19,707][root][INFO] - Training Epoch: 5/10, step 289/574 completed (loss: 1.9449479579925537, acc: 0.5031055808067322)
[2024-12-12 02:43:19,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:20,053][root][INFO] - Training Epoch: 5/10, step 290/574 completed (loss: 2.0252580642700195, acc: 0.4639175236225128)
[2024-12-12 02:43:20,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:20,363][root][INFO] - Training Epoch: 5/10, step 291/574 completed (loss: 0.0525471568107605, acc: 1.0)
[2024-12-12 02:43:20,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:20,674][root][INFO] - Training Epoch: 5/10, step 292/574 completed (loss: 1.0197042226791382, acc: 0.6904761791229248)
[2024-12-12 02:43:20,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:21,069][root][INFO] - Training Epoch: 5/10, step 293/574 completed (loss: 1.1038334369659424, acc: 0.6724137663841248)
[2024-12-12 02:43:21,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:21,527][root][INFO] - Training Epoch: 5/10, step 294/574 completed (loss: 0.928032398223877, acc: 0.7272727489471436)
[2024-12-12 02:43:21,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:22,075][root][INFO] - Training Epoch: 5/10, step 295/574 completed (loss: 1.550233006477356, acc: 0.6082473993301392)
[2024-12-12 02:43:22,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:22,379][root][INFO] - Training Epoch: 5/10, step 296/574 completed (loss: 1.4145804643630981, acc: 0.6206896305084229)
[2024-12-12 02:43:22,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:22,766][root][INFO] - Training Epoch: 5/10, step 297/574 completed (loss: 0.4335745573043823, acc: 0.9259259104728699)
[2024-12-12 02:43:22,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:23,123][root][INFO] - Training Epoch: 5/10, step 298/574 completed (loss: 0.8740800619125366, acc: 0.8157894611358643)
[2024-12-12 02:43:23,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:23,501][root][INFO] - Training Epoch: 5/10, step 299/574 completed (loss: 0.7890152335166931, acc: 0.8035714030265808)
[2024-12-12 02:43:23,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:23,865][root][INFO] - Training Epoch: 5/10, step 300/574 completed (loss: 0.397708535194397, acc: 0.875)
[2024-12-12 02:43:23,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:24,222][root][INFO] - Training Epoch: 5/10, step 301/574 completed (loss: 0.9706853032112122, acc: 0.7547169923782349)
[2024-12-12 02:43:24,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:24,579][root][INFO] - Training Epoch: 5/10, step 302/574 completed (loss: 0.46447110176086426, acc: 0.8301886916160583)
[2024-12-12 02:43:24,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:24,966][root][INFO] - Training Epoch: 5/10, step 303/574 completed (loss: 0.23515963554382324, acc: 0.9411764740943909)
[2024-12-12 02:43:25,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:25,334][root][INFO] - Training Epoch: 5/10, step 304/574 completed (loss: 0.82276451587677, acc: 0.75)
[2024-12-12 02:43:25,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:25,677][root][INFO] - Training Epoch: 5/10, step 305/574 completed (loss: 0.9045799374580383, acc: 0.7377049326896667)
[2024-12-12 02:43:25,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:25,984][root][INFO] - Training Epoch: 5/10, step 306/574 completed (loss: 0.3729572892189026, acc: 0.8999999761581421)
[2024-12-12 02:43:26,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:26,323][root][INFO] - Training Epoch: 5/10, step 307/574 completed (loss: 0.032418590039014816, acc: 1.0)
[2024-12-12 02:43:26,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:26,626][root][INFO] - Training Epoch: 5/10, step 308/574 completed (loss: 1.3992810249328613, acc: 0.5797101259231567)
[2024-12-12 02:43:26,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:27,036][root][INFO] - Training Epoch: 5/10, step 309/574 completed (loss: 1.1399788856506348, acc: 0.6666666865348816)
[2024-12-12 02:43:27,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:27,338][root][INFO] - Training Epoch: 5/10, step 310/574 completed (loss: 0.9949085116386414, acc: 0.6746987700462341)
[2024-12-12 02:43:27,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:27,696][root][INFO] - Training Epoch: 5/10, step 311/574 completed (loss: 1.4024567604064941, acc: 0.5512820482254028)
[2024-12-12 02:43:27,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:28,091][root][INFO] - Training Epoch: 5/10, step 312/574 completed (loss: 1.4288409948349, acc: 0.6530612111091614)
[2024-12-12 02:43:28,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:28,455][root][INFO] - Training Epoch: 5/10, step 313/574 completed (loss: 0.017217641696333885, acc: 1.0)
[2024-12-12 02:43:28,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:28,836][root][INFO] - Training Epoch: 5/10, step 314/574 completed (loss: 0.1510963886976242, acc: 0.9166666865348816)
[2024-12-12 02:43:28,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:29,203][root][INFO] - Training Epoch: 5/10, step 315/574 completed (loss: 0.39832234382629395, acc: 0.8709677457809448)
[2024-12-12 02:43:29,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:29,570][root][INFO] - Training Epoch: 5/10, step 316/574 completed (loss: 0.3972369134426117, acc: 0.9032257795333862)
[2024-12-12 02:43:29,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:29,934][root][INFO] - Training Epoch: 5/10, step 317/574 completed (loss: 0.8634833097457886, acc: 0.7164179086685181)
[2024-12-12 02:43:30,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:30,284][root][INFO] - Training Epoch: 5/10, step 318/574 completed (loss: 0.947108805179596, acc: 0.7211538553237915)
[2024-12-12 02:43:30,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:30,632][root][INFO] - Training Epoch: 5/10, step 319/574 completed (loss: 0.5588964223861694, acc: 0.8222222328186035)
[2024-12-12 02:43:30,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:31,002][root][INFO] - Training Epoch: 5/10, step 320/574 completed (loss: 0.707904040813446, acc: 0.8387096524238586)
[2024-12-12 02:43:31,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:31,382][root][INFO] - Training Epoch: 5/10, step 321/574 completed (loss: 0.3276336193084717, acc: 0.8999999761581421)
[2024-12-12 02:43:31,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:31,756][root][INFO] - Training Epoch: 5/10, step 322/574 completed (loss: 1.0550297498703003, acc: 0.6296296119689941)
[2024-12-12 02:43:31,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:32,096][root][INFO] - Training Epoch: 5/10, step 323/574 completed (loss: 1.570830225944519, acc: 0.6285714507102966)
[2024-12-12 02:43:32,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:32,417][root][INFO] - Training Epoch: 5/10, step 324/574 completed (loss: 1.0085484981536865, acc: 0.7435897588729858)
[2024-12-12 02:43:32,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:32,772][root][INFO] - Training Epoch: 5/10, step 325/574 completed (loss: 1.4474506378173828, acc: 0.6585366129875183)
[2024-12-12 02:43:32,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:33,104][root][INFO] - Training Epoch: 5/10, step 326/574 completed (loss: 1.1898791790008545, acc: 0.6842105388641357)
[2024-12-12 02:43:33,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:33,409][root][INFO] - Training Epoch: 5/10, step 327/574 completed (loss: 0.21829891204833984, acc: 0.9473684430122375)
[2024-12-12 02:43:33,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:33,744][root][INFO] - Training Epoch: 5/10, step 328/574 completed (loss: 0.08817946910858154, acc: 1.0)
[2024-12-12 02:43:33,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:34,150][root][INFO] - Training Epoch: 5/10, step 329/574 completed (loss: 0.6360853910446167, acc: 0.8518518805503845)
[2024-12-12 02:43:34,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:34,509][root][INFO] - Training Epoch: 5/10, step 330/574 completed (loss: 0.6964026093482971, acc: 0.8125)
[2024-12-12 02:43:34,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:34,878][root][INFO] - Training Epoch: 5/10, step 331/574 completed (loss: 1.317706823348999, acc: 0.6129032373428345)
[2024-12-12 02:43:34,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:35,248][root][INFO] - Training Epoch: 5/10, step 332/574 completed (loss: 0.8688703179359436, acc: 0.7017543911933899)
[2024-12-12 02:43:35,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:35,605][root][INFO] - Training Epoch: 5/10, step 333/574 completed (loss: 1.032434344291687, acc: 0.6875)
[2024-12-12 02:43:35,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:35,984][root][INFO] - Training Epoch: 5/10, step 334/574 completed (loss: 0.37351685762405396, acc: 0.8999999761581421)
[2024-12-12 02:43:36,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:36,349][root][INFO] - Training Epoch: 5/10, step 335/574 completed (loss: 0.3162587583065033, acc: 0.8421052694320679)
[2024-12-12 02:43:36,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:36,704][root][INFO] - Training Epoch: 5/10, step 336/574 completed (loss: 1.5294396877288818, acc: 0.6399999856948853)
[2024-12-12 02:43:36,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:37,088][root][INFO] - Training Epoch: 5/10, step 337/574 completed (loss: 1.973829984664917, acc: 0.49425286054611206)
[2024-12-12 02:43:37,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:37,440][root][INFO] - Training Epoch: 5/10, step 338/574 completed (loss: 2.065558910369873, acc: 0.478723406791687)
[2024-12-12 02:43:37,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:37,805][root][INFO] - Training Epoch: 5/10, step 339/574 completed (loss: 2.0247554779052734, acc: 0.4457831382751465)
[2024-12-12 02:43:37,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:38,178][root][INFO] - Training Epoch: 5/10, step 340/574 completed (loss: 0.1513948142528534, acc: 1.0)
[2024-12-12 02:43:38,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:38,530][root][INFO] - Training Epoch: 5/10, step 341/574 completed (loss: 0.8443923592567444, acc: 0.6666666865348816)
[2024-12-12 02:43:38,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:38,912][root][INFO] - Training Epoch: 5/10, step 342/574 completed (loss: 2.0101916790008545, acc: 0.4819277226924896)
[2024-12-12 02:43:39,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:39,256][root][INFO] - Training Epoch: 5/10, step 343/574 completed (loss: 1.2823196649551392, acc: 0.6415094137191772)
[2024-12-12 02:43:39,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:39,649][root][INFO] - Training Epoch: 5/10, step 344/574 completed (loss: 1.2923933267593384, acc: 0.607594907283783)
[2024-12-12 02:43:39,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:39,993][root][INFO] - Training Epoch: 5/10, step 345/574 completed (loss: 0.9347317814826965, acc: 0.7058823704719543)
[2024-12-12 02:43:40,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:40,336][root][INFO] - Training Epoch: 5/10, step 346/574 completed (loss: 1.7299385070800781, acc: 0.46268656849861145)
[2024-12-12 02:43:40,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:40,701][root][INFO] - Training Epoch: 5/10, step 347/574 completed (loss: 0.14395961165428162, acc: 0.949999988079071)
[2024-12-12 02:43:40,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:41,021][root][INFO] - Training Epoch: 5/10, step 348/574 completed (loss: 0.17173969745635986, acc: 0.9599999785423279)
[2024-12-12 02:43:41,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:41,432][root][INFO] - Training Epoch: 5/10, step 349/574 completed (loss: 0.7182042002677917, acc: 0.8055555820465088)
[2024-12-12 02:43:41,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:41,792][root][INFO] - Training Epoch: 5/10, step 350/574 completed (loss: 1.512780785560608, acc: 0.5813953280448914)
[2024-12-12 02:43:41,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:42,164][root][INFO] - Training Epoch: 5/10, step 351/574 completed (loss: 0.8157200813293457, acc: 0.6666666865348816)
[2024-12-12 02:43:42,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:42,572][root][INFO] - Training Epoch: 5/10, step 352/574 completed (loss: 1.2973663806915283, acc: 0.6222222447395325)
[2024-12-12 02:43:42,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:42,954][root][INFO] - Training Epoch: 5/10, step 353/574 completed (loss: 0.439740389585495, acc: 0.8695651888847351)
[2024-12-12 02:43:43,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:43,315][root][INFO] - Training Epoch: 5/10, step 354/574 completed (loss: 0.6146824359893799, acc: 0.8461538553237915)
[2024-12-12 02:43:43,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:43,705][root][INFO] - Training Epoch: 5/10, step 355/574 completed (loss: 1.9285117387771606, acc: 0.49450549483299255)
[2024-12-12 02:43:43,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:44,213][root][INFO] - Training Epoch: 5/10, step 356/574 completed (loss: 1.6178275346755981, acc: 0.6086956262588501)
[2024-12-12 02:43:44,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:44,617][root][INFO] - Training Epoch: 5/10, step 357/574 completed (loss: 1.4108171463012695, acc: 0.6195651888847351)
[2024-12-12 02:43:44,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:44,995][root][INFO] - Training Epoch: 5/10, step 358/574 completed (loss: 1.0630956888198853, acc: 0.6326530575752258)
[2024-12-12 02:43:45,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:45,321][root][INFO] - Training Epoch: 5/10, step 359/574 completed (loss: 0.02893999218940735, acc: 1.0)
[2024-12-12 02:43:45,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:45,664][root][INFO] - Training Epoch: 5/10, step 360/574 completed (loss: 0.13771189749240875, acc: 1.0)
[2024-12-12 02:43:45,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:46,023][root][INFO] - Training Epoch: 5/10, step 361/574 completed (loss: 0.8160709142684937, acc: 0.8536585569381714)
[2024-12-12 02:43:46,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:46,412][root][INFO] - Training Epoch: 5/10, step 362/574 completed (loss: 1.1032578945159912, acc: 0.6666666865348816)
[2024-12-12 02:43:46,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:46,772][root][INFO] - Training Epoch: 5/10, step 363/574 completed (loss: 1.3156741857528687, acc: 0.5921052694320679)
[2024-12-12 02:43:46,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:47,084][root][INFO] - Training Epoch: 5/10, step 364/574 completed (loss: 0.8256436586380005, acc: 0.6829268336296082)
[2024-12-12 02:43:47,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:47,440][root][INFO] - Training Epoch: 5/10, step 365/574 completed (loss: 0.5030681490898132, acc: 0.939393937587738)
[2024-12-12 02:43:47,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:47,812][root][INFO] - Training Epoch: 5/10, step 366/574 completed (loss: 0.13690567016601562, acc: 0.9583333134651184)
[2024-12-12 02:43:47,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:48,182][root][INFO] - Training Epoch: 5/10, step 367/574 completed (loss: 0.1077112928032875, acc: 1.0)
[2024-12-12 02:43:48,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:48,569][root][INFO] - Training Epoch: 5/10, step 368/574 completed (loss: 0.3286868631839752, acc: 0.9642857313156128)
[2024-12-12 02:43:48,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:48,897][root][INFO] - Training Epoch: 5/10, step 369/574 completed (loss: 0.2436867654323578, acc: 0.96875)
[2024-12-12 02:43:49,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:49,496][root][INFO] - Training Epoch: 5/10, step 370/574 completed (loss: 1.6441936492919922, acc: 0.5575757622718811)
[2024-12-12 02:43:49,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:50,354][root][INFO] - Training Epoch: 5/10, step 371/574 completed (loss: 1.1757292747497559, acc: 0.698113203048706)
[2024-12-12 02:43:50,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:50,656][root][INFO] - Training Epoch: 5/10, step 372/574 completed (loss: 1.2049826383590698, acc: 0.7333333492279053)
[2024-12-12 02:43:50,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:51,018][root][INFO] - Training Epoch: 5/10, step 373/574 completed (loss: 0.9155458807945251, acc: 0.7321428656578064)
[2024-12-12 02:43:51,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:51,367][root][INFO] - Training Epoch: 5/10, step 374/574 completed (loss: 0.6196695566177368, acc: 0.800000011920929)
[2024-12-12 02:43:51,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:51,746][root][INFO] - Training Epoch: 5/10, step 375/574 completed (loss: 0.05728258192539215, acc: 1.0)
[2024-12-12 02:43:51,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:52,074][root][INFO] - Training Epoch: 5/10, step 376/574 completed (loss: 0.01365079265087843, acc: 1.0)
[2024-12-12 02:43:52,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:52,436][root][INFO] - Training Epoch: 5/10, step 377/574 completed (loss: 0.7994717955589294, acc: 0.7916666865348816)
[2024-12-12 02:43:52,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:52,829][root][INFO] - Training Epoch: 5/10, step 378/574 completed (loss: 1.0416067838668823, acc: 0.6947368383407593)
[2024-12-12 02:43:52,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:53,416][root][INFO] - Training Epoch: 5/10, step 379/574 completed (loss: 1.402312994003296, acc: 0.658682644367218)
[2024-12-12 02:43:53,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:53,815][root][INFO] - Training Epoch: 5/10, step 380/574 completed (loss: 1.2462403774261475, acc: 0.6616541147232056)
[2024-12-12 02:43:54,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:54,909][root][INFO] - Training Epoch: 5/10, step 381/574 completed (loss: 1.3399137258529663, acc: 0.614973247051239)
[2024-12-12 02:43:55,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:55,476][root][INFO] - Training Epoch: 5/10, step 382/574 completed (loss: 0.870441734790802, acc: 0.7657657861709595)
[2024-12-12 02:43:55,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:55,804][root][INFO] - Training Epoch: 5/10, step 383/574 completed (loss: 0.21637704968452454, acc: 0.9285714030265808)
[2024-12-12 02:43:55,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:56,137][root][INFO] - Training Epoch: 5/10, step 384/574 completed (loss: 0.02023276314139366, acc: 1.0)
[2024-12-12 02:43:56,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:56,459][root][INFO] - Training Epoch: 5/10, step 385/574 completed (loss: 0.3742311894893646, acc: 0.875)
[2024-12-12 02:43:56,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:56,810][root][INFO] - Training Epoch: 5/10, step 386/574 completed (loss: 0.13016612827777863, acc: 0.9722222089767456)
[2024-12-12 02:43:56,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:57,144][root][INFO] - Training Epoch: 5/10, step 387/574 completed (loss: 0.1236695721745491, acc: 0.9736841917037964)
[2024-12-12 02:43:57,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:57,487][root][INFO] - Training Epoch: 5/10, step 388/574 completed (loss: 0.0159307848662138, acc: 1.0)
[2024-12-12 02:43:57,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:57,851][root][INFO] - Training Epoch: 5/10, step 389/574 completed (loss: 0.19437244534492493, acc: 0.949999988079071)
[2024-12-12 02:43:57,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:58,209][root][INFO] - Training Epoch: 5/10, step 390/574 completed (loss: 0.44515863060951233, acc: 0.9047619104385376)
[2024-12-12 02:43:58,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:58,599][root][INFO] - Training Epoch: 5/10, step 391/574 completed (loss: 1.7789467573165894, acc: 0.5555555820465088)
[2024-12-12 02:43:58,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:58,991][root][INFO] - Training Epoch: 5/10, step 392/574 completed (loss: 1.8494806289672852, acc: 0.5048543810844421)
[2024-12-12 02:43:59,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:59,517][root][INFO] - Training Epoch: 5/10, step 393/574 completed (loss: 1.658132553100586, acc: 0.5588235259056091)
[2024-12-12 02:43:59,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:43:59,934][root][INFO] - Training Epoch: 5/10, step 394/574 completed (loss: 1.8122605085372925, acc: 0.47999998927116394)
[2024-12-12 02:44:00,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:00,338][root][INFO] - Training Epoch: 5/10, step 395/574 completed (loss: 1.9817161560058594, acc: 0.5069444179534912)
[2024-12-12 02:44:00,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:00,718][root][INFO] - Training Epoch: 5/10, step 396/574 completed (loss: 0.8585644960403442, acc: 0.7441860437393188)
[2024-12-12 02:44:00,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:01,093][root][INFO] - Training Epoch: 5/10, step 397/574 completed (loss: 0.22247280180454254, acc: 0.9583333134651184)
[2024-12-12 02:44:01,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:01,442][root][INFO] - Training Epoch: 5/10, step 398/574 completed (loss: 1.0196703672409058, acc: 0.6744186282157898)
[2024-12-12 02:44:01,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:01,793][root][INFO] - Training Epoch: 5/10, step 399/574 completed (loss: 0.2503635883331299, acc: 0.9200000166893005)
[2024-12-12 02:44:01,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:02,326][root][INFO] - Training Epoch: 5/10, step 400/574 completed (loss: 1.2681987285614014, acc: 0.6176470518112183)
[2024-12-12 02:44:02,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:02,681][root][INFO] - Training Epoch: 5/10, step 401/574 completed (loss: 1.256352186203003, acc: 0.653333306312561)
[2024-12-12 02:44:02,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:03,057][root][INFO] - Training Epoch: 5/10, step 402/574 completed (loss: 0.9699599742889404, acc: 0.6969696879386902)
[2024-12-12 02:44:03,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:03,427][root][INFO] - Training Epoch: 5/10, step 403/574 completed (loss: 0.5231077075004578, acc: 0.8787878751754761)
[2024-12-12 02:44:03,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:03,767][root][INFO] - Training Epoch: 5/10, step 404/574 completed (loss: 0.17023906111717224, acc: 0.9032257795333862)
[2024-12-12 02:44:03,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:04,085][root][INFO] - Training Epoch: 5/10, step 405/574 completed (loss: 0.29662132263183594, acc: 0.8888888955116272)
[2024-12-12 02:44:04,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:04,434][root][INFO] - Training Epoch: 5/10, step 406/574 completed (loss: 0.18207596242427826, acc: 0.9599999785423279)
[2024-12-12 02:44:04,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:04,797][root][INFO] - Training Epoch: 5/10, step 407/574 completed (loss: 0.4152074456214905, acc: 0.9166666865348816)
[2024-12-12 02:44:04,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:05,179][root][INFO] - Training Epoch: 5/10, step 408/574 completed (loss: 0.2815133035182953, acc: 0.9259259104728699)
[2024-12-12 02:44:05,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:05,547][root][INFO] - Training Epoch: 5/10, step 409/574 completed (loss: 0.22908976674079895, acc: 0.8846153616905212)
[2024-12-12 02:44:05,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:05,939][root][INFO] - Training Epoch: 5/10, step 410/574 completed (loss: 0.6621206998825073, acc: 0.8448275923728943)
[2024-12-12 02:44:06,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:06,282][root][INFO] - Training Epoch: 5/10, step 411/574 completed (loss: 0.23954899609088898, acc: 0.9285714030265808)
[2024-12-12 02:44:06,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:06,639][root][INFO] - Training Epoch: 5/10, step 412/574 completed (loss: 0.384550005197525, acc: 0.8999999761581421)
[2024-12-12 02:44:06,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:06,985][root][INFO] - Training Epoch: 5/10, step 413/574 completed (loss: 0.36388033628463745, acc: 0.939393937587738)
[2024-12-12 02:44:07,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:07,293][root][INFO] - Training Epoch: 5/10, step 414/574 completed (loss: 0.10094740241765976, acc: 0.9545454382896423)
[2024-12-12 02:44:07,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:07,674][root][INFO] - Training Epoch: 5/10, step 415/574 completed (loss: 1.1645786762237549, acc: 0.6078431606292725)
[2024-12-12 02:44:07,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:08,047][root][INFO] - Training Epoch: 5/10, step 416/574 completed (loss: 0.778862714767456, acc: 0.7307692170143127)
[2024-12-12 02:44:08,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:08,407][root][INFO] - Training Epoch: 5/10, step 417/574 completed (loss: 0.554459810256958, acc: 0.7222222089767456)
[2024-12-12 02:44:08,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:08,794][root][INFO] - Training Epoch: 5/10, step 418/574 completed (loss: 0.7010823488235474, acc: 0.7749999761581421)
[2024-12-12 02:44:08,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:09,141][root][INFO] - Training Epoch: 5/10, step 419/574 completed (loss: 0.2898789048194885, acc: 0.8999999761581421)
[2024-12-12 02:44:09,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:09,477][root][INFO] - Training Epoch: 5/10, step 420/574 completed (loss: 0.07002214342355728, acc: 1.0)
[2024-12-12 02:44:10,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:10,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:10,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:11,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:11,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:11,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:12,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:12,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:12,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:13,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:13,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:13,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:14,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:14,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:14,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:15,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:15,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:15,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:16,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:16,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:16,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:17,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:17,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:17,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:18,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:18,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:19,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:19,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:19,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:20,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:20,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:20,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:21,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:21,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:21,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:22,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:22,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:22,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:23,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:23,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:24,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:24,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:24,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:25,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:25,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:26,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:26,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:26,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:26,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:27,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:27,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:27,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:28,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:28,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:28,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:29,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:29,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:29,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:30,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:30,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:31,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:31,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:31,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:32,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:32,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:32,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:33,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:33,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:34,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:34,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:34,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:35,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:35,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:35,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:36,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:36,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:36,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:37,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:37,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:37,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:38,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:38,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:38,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:38,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:39,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:39,946][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(5.3506, device='cuda:0') eval_epoch_loss=tensor(1.6772, device='cuda:0') eval_epoch_acc=tensor(0.6039, device='cuda:0')
[2024-12-12 02:44:39,947][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:44:39,947][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:44:40,173][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_5_step_421_loss_1.67720627784729/model.pt
[2024-12-12 02:44:40,183][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:44:40,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:40,579][root][INFO] - Training Epoch: 5/10, step 421/574 completed (loss: 0.3133181631565094, acc: 0.8333333134651184)
[2024-12-12 02:44:40,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:40,926][root][INFO] - Training Epoch: 5/10, step 422/574 completed (loss: 0.5687482953071594, acc: 0.8125)
[2024-12-12 02:44:41,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:41,316][root][INFO] - Training Epoch: 5/10, step 423/574 completed (loss: 0.2990429699420929, acc: 0.9444444179534912)
[2024-12-12 02:44:41,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:41,664][root][INFO] - Training Epoch: 5/10, step 424/574 completed (loss: 0.16439448297023773, acc: 0.9259259104728699)
[2024-12-12 02:44:41,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:41,995][root][INFO] - Training Epoch: 5/10, step 425/574 completed (loss: 0.27874669432640076, acc: 0.9090909361839294)
[2024-12-12 02:44:42,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:42,316][root][INFO] - Training Epoch: 5/10, step 426/574 completed (loss: 0.5110396146774292, acc: 0.8695651888847351)
[2024-12-12 02:44:42,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:42,625][root][INFO] - Training Epoch: 5/10, step 427/574 completed (loss: 0.6231406927108765, acc: 0.7297297120094299)
[2024-12-12 02:44:42,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:42,979][root][INFO] - Training Epoch: 5/10, step 428/574 completed (loss: 0.3730640113353729, acc: 0.9259259104728699)
[2024-12-12 02:44:43,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:43,321][root][INFO] - Training Epoch: 5/10, step 429/574 completed (loss: 0.2947903871536255, acc: 0.8695651888847351)
[2024-12-12 02:44:43,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:43,655][root][INFO] - Training Epoch: 5/10, step 430/574 completed (loss: 0.1162005066871643, acc: 0.9629629850387573)
[2024-12-12 02:44:43,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:43,987][root][INFO] - Training Epoch: 5/10, step 431/574 completed (loss: 0.0743514820933342, acc: 1.0)
[2024-12-12 02:44:44,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:44,297][root][INFO] - Training Epoch: 5/10, step 432/574 completed (loss: 0.2264796793460846, acc: 0.9130434989929199)
[2024-12-12 02:44:44,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:44,674][root][INFO] - Training Epoch: 5/10, step 433/574 completed (loss: 0.8445141315460205, acc: 0.75)
[2024-12-12 02:44:44,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:45,001][root][INFO] - Training Epoch: 5/10, step 434/574 completed (loss: 0.05245181545615196, acc: 1.0)
[2024-12-12 02:44:45,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:45,360][root][INFO] - Training Epoch: 5/10, step 435/574 completed (loss: 0.589858889579773, acc: 0.7878788113594055)
[2024-12-12 02:44:45,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:45,638][root][INFO] - Training Epoch: 5/10, step 436/574 completed (loss: 0.5689870715141296, acc: 0.7777777910232544)
[2024-12-12 02:44:45,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:45,987][root][INFO] - Training Epoch: 5/10, step 437/574 completed (loss: 0.6583847403526306, acc: 0.8181818127632141)
[2024-12-12 02:44:46,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:46,314][root][INFO] - Training Epoch: 5/10, step 438/574 completed (loss: 0.08290912955999374, acc: 0.9523809552192688)
[2024-12-12 02:44:46,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:46,717][root][INFO] - Training Epoch: 5/10, step 439/574 completed (loss: 0.8376269340515137, acc: 0.692307710647583)
[2024-12-12 02:44:46,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:47,172][root][INFO] - Training Epoch: 5/10, step 440/574 completed (loss: 1.3743754625320435, acc: 0.6212121248245239)
[2024-12-12 02:44:47,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:47,898][root][INFO] - Training Epoch: 5/10, step 441/574 completed (loss: 2.0981385707855225, acc: 0.42399999499320984)
[2024-12-12 02:44:48,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:48,344][root][INFO] - Training Epoch: 5/10, step 442/574 completed (loss: 1.8434820175170898, acc: 0.5)
[2024-12-12 02:44:48,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:48,997][root][INFO] - Training Epoch: 5/10, step 443/574 completed (loss: 1.861183762550354, acc: 0.5074626803398132)
[2024-12-12 02:44:49,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:49,296][root][INFO] - Training Epoch: 5/10, step 444/574 completed (loss: 1.1700693368911743, acc: 0.7169811129570007)
[2024-12-12 02:44:49,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:49,716][root][INFO] - Training Epoch: 5/10, step 445/574 completed (loss: 0.653175413608551, acc: 0.8181818127632141)
[2024-12-12 02:44:49,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:50,059][root][INFO] - Training Epoch: 5/10, step 446/574 completed (loss: 0.4154881536960602, acc: 0.8695651888847351)
[2024-12-12 02:44:50,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:50,409][root][INFO] - Training Epoch: 5/10, step 447/574 completed (loss: 0.3761727213859558, acc: 0.8461538553237915)
[2024-12-12 02:44:50,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:50,747][root][INFO] - Training Epoch: 5/10, step 448/574 completed (loss: 0.5647675395011902, acc: 0.8571428656578064)
[2024-12-12 02:44:50,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:51,129][root][INFO] - Training Epoch: 5/10, step 449/574 completed (loss: 0.9814400672912598, acc: 0.7164179086685181)
[2024-12-12 02:44:51,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:51,470][root][INFO] - Training Epoch: 5/10, step 450/574 completed (loss: 0.923778772354126, acc: 0.7777777910232544)
[2024-12-12 02:44:51,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:51,775][root][INFO] - Training Epoch: 5/10, step 451/574 completed (loss: 1.0793876647949219, acc: 0.6739130616188049)
[2024-12-12 02:44:51,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:52,130][root][INFO] - Training Epoch: 5/10, step 452/574 completed (loss: 1.2919646501541138, acc: 0.6282051205635071)
[2024-12-12 02:44:52,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:52,450][root][INFO] - Training Epoch: 5/10, step 453/574 completed (loss: 1.2863348722457886, acc: 0.6973684430122375)
[2024-12-12 02:44:52,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:52,826][root][INFO] - Training Epoch: 5/10, step 454/574 completed (loss: 0.8319029808044434, acc: 0.7346938848495483)
[2024-12-12 02:44:52,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:53,202][root][INFO] - Training Epoch: 5/10, step 455/574 completed (loss: 0.2917071282863617, acc: 0.9090909361839294)
[2024-12-12 02:44:53,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:53,576][root][INFO] - Training Epoch: 5/10, step 456/574 completed (loss: 1.7008111476898193, acc: 0.5154638886451721)
[2024-12-12 02:44:53,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:53,972][root][INFO] - Training Epoch: 5/10, step 457/574 completed (loss: 0.8216065168380737, acc: 0.800000011920929)
[2024-12-12 02:44:54,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:54,375][root][INFO] - Training Epoch: 5/10, step 458/574 completed (loss: 1.5547255277633667, acc: 0.569767415523529)
[2024-12-12 02:44:54,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:54,754][root][INFO] - Training Epoch: 5/10, step 459/574 completed (loss: 1.2618685960769653, acc: 0.6785714030265808)
[2024-12-12 02:44:54,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:55,135][root][INFO] - Training Epoch: 5/10, step 460/574 completed (loss: 1.3272390365600586, acc: 0.5925925970077515)
[2024-12-12 02:44:55,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:55,461][root][INFO] - Training Epoch: 5/10, step 461/574 completed (loss: 0.7757560014724731, acc: 0.7222222089767456)
[2024-12-12 02:44:55,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:55,759][root][INFO] - Training Epoch: 5/10, step 462/574 completed (loss: 0.3232716917991638, acc: 0.9375)
[2024-12-12 02:44:55,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:56,111][root][INFO] - Training Epoch: 5/10, step 463/574 completed (loss: 0.26223036646842957, acc: 0.9615384340286255)
[2024-12-12 02:44:56,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:56,486][root][INFO] - Training Epoch: 5/10, step 464/574 completed (loss: 0.5672260522842407, acc: 0.804347813129425)
[2024-12-12 02:44:56,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:56,822][root][INFO] - Training Epoch: 5/10, step 465/574 completed (loss: 1.0137994289398193, acc: 0.6666666865348816)
[2024-12-12 02:44:56,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:57,192][root][INFO] - Training Epoch: 5/10, step 466/574 completed (loss: 1.3179562091827393, acc: 0.6746987700462341)
[2024-12-12 02:44:57,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:57,554][root][INFO] - Training Epoch: 5/10, step 467/574 completed (loss: 1.093017339706421, acc: 0.6666666865348816)
[2024-12-12 02:44:57,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:57,910][root][INFO] - Training Epoch: 5/10, step 468/574 completed (loss: 1.255678653717041, acc: 0.6699029207229614)
[2024-12-12 02:44:58,021][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:58,261][root][INFO] - Training Epoch: 5/10, step 469/574 completed (loss: 1.0881551504135132, acc: 0.6829268336296082)
[2024-12-12 02:44:58,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:58,627][root][INFO] - Training Epoch: 5/10, step 470/574 completed (loss: 0.1014450415968895, acc: 0.9583333134651184)
[2024-12-12 02:44:58,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:59,004][root][INFO] - Training Epoch: 5/10, step 471/574 completed (loss: 0.42257553339004517, acc: 0.8928571343421936)
[2024-12-12 02:44:59,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:59,422][root][INFO] - Training Epoch: 5/10, step 472/574 completed (loss: 1.5251803398132324, acc: 0.5882353186607361)
[2024-12-12 02:44:59,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:44:59,808][root][INFO] - Training Epoch: 5/10, step 473/574 completed (loss: 1.9061359167099, acc: 0.4628821015357971)
[2024-12-12 02:44:59,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:00,183][root][INFO] - Training Epoch: 5/10, step 474/574 completed (loss: 1.5052732229232788, acc: 0.5416666865348816)
[2024-12-12 02:45:00,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:00,568][root][INFO] - Training Epoch: 5/10, step 475/574 completed (loss: 1.6550053358078003, acc: 0.546012282371521)
[2024-12-12 02:45:00,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:00,926][root][INFO] - Training Epoch: 5/10, step 476/574 completed (loss: 1.7097500562667847, acc: 0.5611510872840881)
[2024-12-12 02:45:01,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:01,296][root][INFO] - Training Epoch: 5/10, step 477/574 completed (loss: 1.6985315084457397, acc: 0.5025125741958618)
[2024-12-12 02:45:01,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:01,616][root][INFO] - Training Epoch: 5/10, step 478/574 completed (loss: 0.809935986995697, acc: 0.8055555820465088)
[2024-12-12 02:45:01,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:01,904][root][INFO] - Training Epoch: 5/10, step 479/574 completed (loss: 0.7209280133247375, acc: 0.8181818127632141)
[2024-12-12 02:45:01,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:02,213][root][INFO] - Training Epoch: 5/10, step 480/574 completed (loss: 0.3603422939777374, acc: 0.8518518805503845)
[2024-12-12 02:45:02,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:02,603][root][INFO] - Training Epoch: 5/10, step 481/574 completed (loss: 0.5256665945053101, acc: 0.8500000238418579)
[2024-12-12 02:45:02,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:02,927][root][INFO] - Training Epoch: 5/10, step 482/574 completed (loss: 0.37319883704185486, acc: 0.8999999761581421)
[2024-12-12 02:45:03,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:03,292][root][INFO] - Training Epoch: 5/10, step 483/574 completed (loss: 1.0789532661437988, acc: 0.6724137663841248)
[2024-12-12 02:45:03,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:03,660][root][INFO] - Training Epoch: 5/10, step 484/574 completed (loss: 0.560617983341217, acc: 0.9354838728904724)
[2024-12-12 02:45:03,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:04,021][root][INFO] - Training Epoch: 5/10, step 485/574 completed (loss: 0.16678526997566223, acc: 0.9473684430122375)
[2024-12-12 02:45:04,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:04,349][root][INFO] - Training Epoch: 5/10, step 486/574 completed (loss: 0.6308709383010864, acc: 0.8148148059844971)
[2024-12-12 02:45:04,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:04,680][root][INFO] - Training Epoch: 5/10, step 487/574 completed (loss: 0.4302498400211334, acc: 0.9047619104385376)
[2024-12-12 02:45:04,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:05,052][root][INFO] - Training Epoch: 5/10, step 488/574 completed (loss: 0.39560872316360474, acc: 0.8636363744735718)
[2024-12-12 02:45:05,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:05,406][root][INFO] - Training Epoch: 5/10, step 489/574 completed (loss: 1.5407252311706543, acc: 0.6000000238418579)
[2024-12-12 02:45:05,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:05,722][root][INFO] - Training Epoch: 5/10, step 490/574 completed (loss: 0.5930784344673157, acc: 0.800000011920929)
[2024-12-12 02:45:05,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:06,018][root][INFO] - Training Epoch: 5/10, step 491/574 completed (loss: 0.45111992955207825, acc: 0.8620689511299133)
[2024-12-12 02:45:06,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:06,372][root][INFO] - Training Epoch: 5/10, step 492/574 completed (loss: 0.9691298007965088, acc: 0.7254902124404907)
[2024-12-12 02:45:06,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:06,737][root][INFO] - Training Epoch: 5/10, step 493/574 completed (loss: 0.5650147795677185, acc: 0.8620689511299133)
[2024-12-12 02:45:06,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:07,104][root][INFO] - Training Epoch: 5/10, step 494/574 completed (loss: 0.23784945905208588, acc: 0.9473684430122375)
[2024-12-12 02:45:07,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:07,421][root][INFO] - Training Epoch: 5/10, step 495/574 completed (loss: 0.6973533034324646, acc: 0.8421052694320679)
[2024-12-12 02:45:07,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:07,813][root][INFO] - Training Epoch: 5/10, step 496/574 completed (loss: 1.3837329149246216, acc: 0.6071428656578064)
[2024-12-12 02:45:07,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:08,244][root][INFO] - Training Epoch: 5/10, step 497/574 completed (loss: 1.2378698587417603, acc: 0.6516854166984558)
[2024-12-12 02:45:08,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:08,632][root][INFO] - Training Epoch: 5/10, step 498/574 completed (loss: 1.4791477918624878, acc: 0.6292135119438171)
[2024-12-12 02:45:08,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:09,006][root][INFO] - Training Epoch: 5/10, step 499/574 completed (loss: 1.972497820854187, acc: 0.44680851697921753)
[2024-12-12 02:45:09,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:09,340][root][INFO] - Training Epoch: 5/10, step 500/574 completed (loss: 1.8967406749725342, acc: 0.54347825050354)
[2024-12-12 02:45:09,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:09,684][root][INFO] - Training Epoch: 5/10, step 501/574 completed (loss: 0.42801105976104736, acc: 0.9599999785423279)
[2024-12-12 02:45:09,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:10,024][root][INFO] - Training Epoch: 5/10, step 502/574 completed (loss: 0.14482785761356354, acc: 0.9615384340286255)
[2024-12-12 02:45:10,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:10,337][root][INFO] - Training Epoch: 5/10, step 503/574 completed (loss: 0.527489960193634, acc: 0.8518518805503845)
[2024-12-12 02:45:10,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:10,622][root][INFO] - Training Epoch: 5/10, step 504/574 completed (loss: 0.4283764362335205, acc: 0.8888888955116272)
[2024-12-12 02:45:10,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:10,975][root][INFO] - Training Epoch: 5/10, step 505/574 completed (loss: 0.8064940571784973, acc: 0.7358490824699402)
[2024-12-12 02:45:11,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:11,317][root][INFO] - Training Epoch: 5/10, step 506/574 completed (loss: 0.6274515986442566, acc: 0.7586206793785095)
[2024-12-12 02:45:11,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:11,934][root][INFO] - Training Epoch: 5/10, step 507/574 completed (loss: 1.6235376596450806, acc: 0.5315315127372742)
[2024-12-12 02:45:12,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:12,360][root][INFO] - Training Epoch: 5/10, step 508/574 completed (loss: 1.2562178373336792, acc: 0.6619718074798584)
[2024-12-12 02:45:12,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:12,671][root][INFO] - Training Epoch: 5/10, step 509/574 completed (loss: 0.06528599560260773, acc: 1.0)
[2024-12-12 02:45:12,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:12,956][root][INFO] - Training Epoch: 5/10, step 510/574 completed (loss: 0.09910885244607925, acc: 1.0)
[2024-12-12 02:45:13,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:13,301][root][INFO] - Training Epoch: 5/10, step 511/574 completed (loss: 0.21406742930412292, acc: 0.9615384340286255)
[2024-12-12 02:45:14,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:16,025][root][INFO] - Training Epoch: 5/10, step 512/574 completed (loss: 1.8759499788284302, acc: 0.4571428596973419)
[2024-12-12 02:45:16,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:16,788][root][INFO] - Training Epoch: 5/10, step 513/574 completed (loss: 1.3700790405273438, acc: 0.60317462682724)
[2024-12-12 02:45:16,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:17,058][root][INFO] - Training Epoch: 5/10, step 514/574 completed (loss: 0.5874590873718262, acc: 0.8571428656578064)
[2024-12-12 02:45:17,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:17,353][root][INFO] - Training Epoch: 5/10, step 515/574 completed (loss: 0.838407576084137, acc: 0.7333333492279053)
[2024-12-12 02:45:17,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:18,042][root][INFO] - Training Epoch: 5/10, step 516/574 completed (loss: 1.1341358423233032, acc: 0.6944444179534912)
[2024-12-12 02:45:18,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:18,399][root][INFO] - Training Epoch: 5/10, step 517/574 completed (loss: 0.043305084109306335, acc: 1.0)
[2024-12-12 02:45:18,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:18,780][root][INFO] - Training Epoch: 5/10, step 518/574 completed (loss: 0.38160082697868347, acc: 0.8709677457809448)
[2024-12-12 02:45:18,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:19,167][root][INFO] - Training Epoch: 5/10, step 519/574 completed (loss: 0.971581757068634, acc: 0.699999988079071)
[2024-12-12 02:45:19,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:19,538][root][INFO] - Training Epoch: 5/10, step 520/574 completed (loss: 0.36953815817832947, acc: 0.8888888955116272)
[2024-12-12 02:45:19,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:20,524][root][INFO] - Training Epoch: 5/10, step 521/574 completed (loss: 1.8631352186203003, acc: 0.4788135588169098)
[2024-12-12 02:45:20,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:20,903][root][INFO] - Training Epoch: 5/10, step 522/574 completed (loss: 1.5107673406600952, acc: 0.5970149040222168)
[2024-12-12 02:45:21,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:21,272][root][INFO] - Training Epoch: 5/10, step 523/574 completed (loss: 1.7520602941513062, acc: 0.5182482004165649)
[2024-12-12 02:45:21,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:21,845][root][INFO] - Training Epoch: 5/10, step 524/574 completed (loss: 1.6846494674682617, acc: 0.5400000214576721)
[2024-12-12 02:45:21,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:22,179][root][INFO] - Training Epoch: 5/10, step 525/574 completed (loss: 0.8063440918922424, acc: 0.7222222089767456)
[2024-12-12 02:45:22,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:22,495][root][INFO] - Training Epoch: 5/10, step 526/574 completed (loss: 1.03432297706604, acc: 0.6538461446762085)
[2024-12-12 02:45:22,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:22,796][root][INFO] - Training Epoch: 5/10, step 527/574 completed (loss: 0.17743341624736786, acc: 1.0)
[2024-12-12 02:45:22,882][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:23,125][root][INFO] - Training Epoch: 5/10, step 528/574 completed (loss: 1.8538724184036255, acc: 0.44262295961380005)
[2024-12-12 02:45:23,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:23,548][root][INFO] - Training Epoch: 5/10, step 529/574 completed (loss: 1.1014591455459595, acc: 0.694915235042572)
[2024-12-12 02:45:23,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:23,928][root][INFO] - Training Epoch: 5/10, step 530/574 completed (loss: 1.649303674697876, acc: 0.5581395626068115)
[2024-12-12 02:45:24,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:24,272][root][INFO] - Training Epoch: 5/10, step 531/574 completed (loss: 1.10629141330719, acc: 0.7272727489471436)
[2024-12-12 02:45:24,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:24,591][root][INFO] - Training Epoch: 5/10, step 532/574 completed (loss: 1.4778261184692383, acc: 0.6037735939025879)
[2024-12-12 02:45:24,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:24,949][root][INFO] - Training Epoch: 5/10, step 533/574 completed (loss: 0.8715853095054626, acc: 0.7954545617103577)
[2024-12-12 02:45:25,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:25,325][root][INFO] - Training Epoch: 5/10, step 534/574 completed (loss: 0.16563086211681366, acc: 0.9599999785423279)
[2024-12-12 02:45:25,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:25,711][root][INFO] - Training Epoch: 5/10, step 535/574 completed (loss: 0.21336665749549866, acc: 0.949999988079071)
[2024-12-12 02:45:25,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:26,081][root][INFO] - Training Epoch: 5/10, step 536/574 completed (loss: 0.4664967358112335, acc: 0.9090909361839294)
[2024-12-12 02:45:26,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:26,484][root][INFO] - Training Epoch: 5/10, step 537/574 completed (loss: 0.8790863156318665, acc: 0.7846153974533081)
[2024-12-12 02:45:26,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:26,869][root][INFO] - Training Epoch: 5/10, step 538/574 completed (loss: 0.977293074131012, acc: 0.6875)
[2024-12-12 02:45:27,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:27,258][root][INFO] - Training Epoch: 5/10, step 539/574 completed (loss: 0.4930136799812317, acc: 0.8125)
[2024-12-12 02:45:27,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:27,543][root][INFO] - Training Epoch: 5/10, step 540/574 completed (loss: 0.7457466125488281, acc: 0.7878788113594055)
[2024-12-12 02:45:27,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:27,854][root][INFO] - Training Epoch: 5/10, step 541/574 completed (loss: 0.170768603682518, acc: 0.9375)
[2024-12-12 02:45:27,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:28,163][root][INFO] - Training Epoch: 5/10, step 542/574 completed (loss: 0.16960497200489044, acc: 0.9677419066429138)
[2024-12-12 02:45:28,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:28,503][root][INFO] - Training Epoch: 5/10, step 543/574 completed (loss: 0.09275457262992859, acc: 0.95652174949646)
[2024-12-12 02:45:28,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:28,841][root][INFO] - Training Epoch: 5/10, step 544/574 completed (loss: 1.1159018278121948, acc: 0.7666666507720947)
[2024-12-12 02:45:28,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:29,152][root][INFO] - Training Epoch: 5/10, step 545/574 completed (loss: 0.6989813446998596, acc: 0.8536585569381714)
[2024-12-12 02:45:29,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:29,536][root][INFO] - Training Epoch: 5/10, step 546/574 completed (loss: 0.20330850780010223, acc: 0.9142857193946838)
[2024-12-12 02:45:29,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:29,893][root][INFO] - Training Epoch: 5/10, step 547/574 completed (loss: 0.2702362537384033, acc: 0.9210526347160339)
[2024-12-12 02:45:30,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:30,281][root][INFO] - Training Epoch: 5/10, step 548/574 completed (loss: 0.24291765689849854, acc: 0.9032257795333862)
[2024-12-12 02:45:30,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:30,615][root][INFO] - Training Epoch: 5/10, step 549/574 completed (loss: 0.11615580320358276, acc: 1.0)
[2024-12-12 02:45:30,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:30,949][root][INFO] - Training Epoch: 5/10, step 550/574 completed (loss: 0.3379457890987396, acc: 0.9090909361839294)
[2024-12-12 02:45:31,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:31,287][root][INFO] - Training Epoch: 5/10, step 551/574 completed (loss: 0.355351984500885, acc: 0.8500000238418579)
[2024-12-12 02:45:31,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:31,659][root][INFO] - Training Epoch: 5/10, step 552/574 completed (loss: 0.45071282982826233, acc: 0.8714285492897034)
[2024-12-12 02:45:31,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:31,988][root][INFO] - Training Epoch: 5/10, step 553/574 completed (loss: 1.4176768064498901, acc: 0.5985401272773743)
[2024-12-12 02:45:32,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:32,390][root][INFO] - Training Epoch: 5/10, step 554/574 completed (loss: 1.100545883178711, acc: 0.6689655184745789)
[2024-12-12 02:45:32,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:32,796][root][INFO] - Training Epoch: 5/10, step 555/574 completed (loss: 1.8901393413543701, acc: 0.4714285731315613)
[2024-12-12 02:45:32,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:33,105][root][INFO] - Training Epoch: 5/10, step 556/574 completed (loss: 1.6440446376800537, acc: 0.5099337697029114)
[2024-12-12 02:45:33,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:33,449][root][INFO] - Training Epoch: 5/10, step 557/574 completed (loss: 1.0601531267166138, acc: 0.7179487347602844)
[2024-12-12 02:45:33,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:33,779][root][INFO] - Training Epoch: 5/10, step 558/574 completed (loss: 0.15677793323993683, acc: 0.9200000166893005)
[2024-12-12 02:45:33,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:34,140][root][INFO] - Training Epoch: 5/10, step 559/574 completed (loss: 0.36374109983444214, acc: 0.8461538553237915)
[2024-12-12 02:45:34,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:34,512][root][INFO] - Training Epoch: 5/10, step 560/574 completed (loss: 0.1187291294336319, acc: 0.9615384340286255)
[2024-12-12 02:45:34,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:34,867][root][INFO] - Training Epoch: 5/10, step 561/574 completed (loss: 0.8697595596313477, acc: 0.7692307829856873)
[2024-12-12 02:45:34,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:35,264][root][INFO] - Training Epoch: 5/10, step 562/574 completed (loss: 0.8605601191520691, acc: 0.7111111283302307)
[2024-12-12 02:45:35,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:35,591][root][INFO] - Training Epoch: 5/10, step 563/574 completed (loss: 0.741187572479248, acc: 0.7662337422370911)
[2024-12-12 02:45:36,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:36,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:37,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:37,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:37,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:38,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:38,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:38,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:39,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:39,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:40,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:40,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:40,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:41,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:41,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:42,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:42,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:42,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:43,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:43,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:43,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:44,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:44,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:44,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:45,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:45,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:45,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:46,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:46,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:46,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:47,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:47,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:47,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:48,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:48,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:48,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:49,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:49,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:49,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:50,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:50,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:50,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:51,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:51,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:52,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:52,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:52,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:53,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:53,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:53,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:54,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:54,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:54,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:54,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:55,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:55,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:55,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:56,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:56,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:56,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:57,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:57,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:58,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:58,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:58,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:59,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:59,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:45:59,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:00,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:00,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:00,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:01,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:01,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:01,908][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:02,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:02,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:02,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:03,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:03,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:03,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:04,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:04,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:04,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:05,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:05,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:06,131][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.7231, device='cuda:0') eval_epoch_loss=tensor(1.5525, device='cuda:0') eval_epoch_acc=tensor(0.6263, device='cuda:0')
[2024-12-12 02:46:06,133][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:46:06,133][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:46:06,323][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_5_step_564_loss_1.552459955215454/model.pt
[2024-12-12 02:46:06,326][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:46:06,327][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 5 is 1.552459955215454
[2024-12-12 02:46:06,327][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 5 is 0.6262530088424683
[2024-12-12 02:46:06,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:06,647][root][INFO] - Training Epoch: 5/10, step 564/574 completed (loss: 0.45647427439689636, acc: 0.8541666865348816)
[2024-12-12 02:46:06,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:06,945][root][INFO] - Training Epoch: 5/10, step 565/574 completed (loss: 0.36931201815605164, acc: 0.8793103694915771)
[2024-12-12 02:46:07,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:07,308][root][INFO] - Training Epoch: 5/10, step 566/574 completed (loss: 0.986504316329956, acc: 0.7142857313156128)
[2024-12-12 02:46:07,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:07,692][root][INFO] - Training Epoch: 5/10, step 567/574 completed (loss: 0.23425787687301636, acc: 0.9210526347160339)
[2024-12-12 02:46:07,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:08,060][root][INFO] - Training Epoch: 5/10, step 568/574 completed (loss: 0.08100948482751846, acc: 0.9629629850387573)
[2024-12-12 02:46:08,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:08,439][root][INFO] - Training Epoch: 5/10, step 569/574 completed (loss: 1.5085848569869995, acc: 0.6096256971359253)
[2024-12-12 02:46:08,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:08,746][root][INFO] - Training Epoch: 5/10, step 570/574 completed (loss: 0.5068531632423401, acc: 0.8387096524238586)
[2024-12-12 02:46:08,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:09,063][root][INFO] - Training Epoch: 5/10, step 571/574 completed (loss: 0.9766101837158203, acc: 0.7008547186851501)
[2024-12-12 02:46:09,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:09,353][root][INFO] - Training Epoch: 5/10, step 572/574 completed (loss: 1.809713363647461, acc: 0.5102040767669678)
[2024-12-12 02:46:09,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:09,691][root][INFO] - Training Epoch: 5/10, step 573/574 completed (loss: 1.6006176471710205, acc: 0.5283018946647644)
[2024-12-12 02:46:10,079][slam_llm.utils.train_utils][INFO] - Epoch 5: train_perplexity=2.6099, train_epoch_loss=0.9593, epoch time 354.98850847408175s
[2024-12-12 02:46:10,079][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 02:46:10,079][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 16 GB
[2024-12-12 02:46:10,079][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 02:46:10,079][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 14
[2024-12-12 02:46:10,080][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:46:10,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:10,950][root][INFO] - Training Epoch: 6/10, step 0/574 completed (loss: 0.5467652678489685, acc: 0.8888888955116272)
[2024-12-12 02:46:11,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:11,289][root][INFO] - Training Epoch: 6/10, step 1/574 completed (loss: 0.5065137147903442, acc: 0.8799999952316284)
[2024-12-12 02:46:11,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:11,614][root][INFO] - Training Epoch: 6/10, step 2/574 completed (loss: 0.5752250552177429, acc: 0.7837837934494019)
[2024-12-12 02:46:11,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:11,943][root][INFO] - Training Epoch: 6/10, step 3/574 completed (loss: 0.8141182065010071, acc: 0.7368420958518982)
[2024-12-12 02:46:12,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:12,275][root][INFO] - Training Epoch: 6/10, step 4/574 completed (loss: 0.7540389895439148, acc: 0.7567567825317383)
[2024-12-12 02:46:12,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:12,688][root][INFO] - Training Epoch: 6/10, step 5/574 completed (loss: 0.3577430248260498, acc: 0.8571428656578064)
[2024-12-12 02:46:12,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:13,047][root][INFO] - Training Epoch: 6/10, step 6/574 completed (loss: 1.0121172666549683, acc: 0.6938775777816772)
[2024-12-12 02:46:13,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:13,352][root][INFO] - Training Epoch: 6/10, step 7/574 completed (loss: 0.1331920027732849, acc: 1.0)
[2024-12-12 02:46:13,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:13,730][root][INFO] - Training Epoch: 6/10, step 8/574 completed (loss: 0.007008914370089769, acc: 1.0)
[2024-12-12 02:46:13,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:14,115][root][INFO] - Training Epoch: 6/10, step 9/574 completed (loss: 0.1619015336036682, acc: 0.9615384340286255)
[2024-12-12 02:46:14,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:14,434][root][INFO] - Training Epoch: 6/10, step 10/574 completed (loss: 0.29896411299705505, acc: 0.8888888955116272)
[2024-12-12 02:46:14,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:14,812][root][INFO] - Training Epoch: 6/10, step 11/574 completed (loss: 0.7549775242805481, acc: 0.7179487347602844)
[2024-12-12 02:46:14,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:15,201][root][INFO] - Training Epoch: 6/10, step 12/574 completed (loss: 0.4309890568256378, acc: 0.8484848737716675)
[2024-12-12 02:46:15,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:15,584][root][INFO] - Training Epoch: 6/10, step 13/574 completed (loss: 0.8299887180328369, acc: 0.739130437374115)
[2024-12-12 02:46:15,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:15,955][root][INFO] - Training Epoch: 6/10, step 14/574 completed (loss: 0.8650265336036682, acc: 0.7254902124404907)
[2024-12-12 02:46:16,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:16,293][root][INFO] - Training Epoch: 6/10, step 15/574 completed (loss: 1.0644797086715698, acc: 0.6734693646430969)
[2024-12-12 02:46:16,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:16,605][root][INFO] - Training Epoch: 6/10, step 16/574 completed (loss: 0.09580563753843307, acc: 0.9473684430122375)
[2024-12-12 02:46:16,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:16,927][root][INFO] - Training Epoch: 6/10, step 17/574 completed (loss: 0.21210642158985138, acc: 0.9583333134651184)
[2024-12-12 02:46:17,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:17,296][root][INFO] - Training Epoch: 6/10, step 18/574 completed (loss: 0.448178768157959, acc: 0.8888888955116272)
[2024-12-12 02:46:17,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:17,631][root][INFO] - Training Epoch: 6/10, step 19/574 completed (loss: 0.6895089149475098, acc: 0.8421052694320679)
[2024-12-12 02:46:17,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:17,938][root][INFO] - Training Epoch: 6/10, step 20/574 completed (loss: 0.43378713726997375, acc: 0.8461538553237915)
[2024-12-12 02:46:18,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:18,294][root][INFO] - Training Epoch: 6/10, step 21/574 completed (loss: 0.21349330246448517, acc: 0.931034505367279)
[2024-12-12 02:46:18,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:18,683][root][INFO] - Training Epoch: 6/10, step 22/574 completed (loss: 0.8770727515220642, acc: 0.7599999904632568)
[2024-12-12 02:46:18,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:19,033][root][INFO] - Training Epoch: 6/10, step 23/574 completed (loss: 0.4075092077255249, acc: 0.9047619104385376)
[2024-12-12 02:46:19,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:19,359][root][INFO] - Training Epoch: 6/10, step 24/574 completed (loss: 0.3049185276031494, acc: 0.875)
[2024-12-12 02:46:19,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:19,698][root][INFO] - Training Epoch: 6/10, step 25/574 completed (loss: 1.4902844429016113, acc: 0.5283018946647644)
[2024-12-12 02:46:19,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:20,036][root][INFO] - Training Epoch: 6/10, step 26/574 completed (loss: 1.507667064666748, acc: 0.6027397513389587)
[2024-12-12 02:46:20,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:21,289][root][INFO] - Training Epoch: 6/10, step 27/574 completed (loss: 2.024062156677246, acc: 0.4308300316333771)
[2024-12-12 02:46:21,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:21,608][root][INFO] - Training Epoch: 6/10, step 28/574 completed (loss: 0.8001874089241028, acc: 0.7209302186965942)
[2024-12-12 02:46:21,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:21,933][root][INFO] - Training Epoch: 6/10, step 29/574 completed (loss: 1.2958238124847412, acc: 0.6144578456878662)
[2024-12-12 02:46:22,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:22,268][root][INFO] - Training Epoch: 6/10, step 30/574 completed (loss: 1.3221123218536377, acc: 0.5802469253540039)
[2024-12-12 02:46:22,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:22,626][root][INFO] - Training Epoch: 6/10, step 31/574 completed (loss: 0.5814728140830994, acc: 0.7857142686843872)
[2024-12-12 02:46:22,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:22,966][root][INFO] - Training Epoch: 6/10, step 32/574 completed (loss: 0.4416457712650299, acc: 0.8148148059844971)
[2024-12-12 02:46:23,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:23,312][root][INFO] - Training Epoch: 6/10, step 33/574 completed (loss: 0.1288912445306778, acc: 0.95652174949646)
[2024-12-12 02:46:23,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:23,714][root][INFO] - Training Epoch: 6/10, step 34/574 completed (loss: 1.359810471534729, acc: 0.6470588445663452)
[2024-12-12 02:46:23,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:24,050][root][INFO] - Training Epoch: 6/10, step 35/574 completed (loss: 0.9567856788635254, acc: 0.7049180269241333)
[2024-12-12 02:46:24,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:24,437][root][INFO] - Training Epoch: 6/10, step 36/574 completed (loss: 1.2942911386489868, acc: 0.6349206566810608)
[2024-12-12 02:46:24,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:24,770][root][INFO] - Training Epoch: 6/10, step 37/574 completed (loss: 1.0168768167495728, acc: 0.6779661178588867)
[2024-12-12 02:46:24,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:25,170][root][INFO] - Training Epoch: 6/10, step 38/574 completed (loss: 0.9451863169670105, acc: 0.7356321811676025)
[2024-12-12 02:46:25,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:25,549][root][INFO] - Training Epoch: 6/10, step 39/574 completed (loss: 0.3553817868232727, acc: 0.8571428656578064)
[2024-12-12 02:46:25,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:25,957][root][INFO] - Training Epoch: 6/10, step 40/574 completed (loss: 0.5508846640586853, acc: 0.9230769276618958)
[2024-12-12 02:46:26,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:26,325][root][INFO] - Training Epoch: 6/10, step 41/574 completed (loss: 1.52573823928833, acc: 0.5405405163764954)
[2024-12-12 02:46:26,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:26,656][root][INFO] - Training Epoch: 6/10, step 42/574 completed (loss: 1.3746907711029053, acc: 0.5846154093742371)
[2024-12-12 02:46:26,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:27,059][root][INFO] - Training Epoch: 6/10, step 43/574 completed (loss: 1.4377589225769043, acc: 0.6161616444587708)
[2024-12-12 02:46:27,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:27,460][root][INFO] - Training Epoch: 6/10, step 44/574 completed (loss: 1.2608060836791992, acc: 0.6597937941551208)
[2024-12-12 02:46:27,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:27,856][root][INFO] - Training Epoch: 6/10, step 45/574 completed (loss: 1.6443052291870117, acc: 0.5441176295280457)
[2024-12-12 02:46:27,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:28,174][root][INFO] - Training Epoch: 6/10, step 46/574 completed (loss: 0.12076394259929657, acc: 0.9615384340286255)
[2024-12-12 02:46:28,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:28,542][root][INFO] - Training Epoch: 6/10, step 47/574 completed (loss: 0.14392250776290894, acc: 0.9629629850387573)
[2024-12-12 02:46:28,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:28,946][root][INFO] - Training Epoch: 6/10, step 48/574 completed (loss: 0.3381533920764923, acc: 0.8928571343421936)
[2024-12-12 02:46:29,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:29,331][root][INFO] - Training Epoch: 6/10, step 49/574 completed (loss: 0.4598659873008728, acc: 0.8888888955116272)
[2024-12-12 02:46:29,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:29,737][root][INFO] - Training Epoch: 6/10, step 50/574 completed (loss: 1.0268547534942627, acc: 0.7368420958518982)
[2024-12-12 02:46:29,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:30,138][root][INFO] - Training Epoch: 6/10, step 51/574 completed (loss: 1.0242573022842407, acc: 0.6984127163887024)
[2024-12-12 02:46:30,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:30,509][root][INFO] - Training Epoch: 6/10, step 52/574 completed (loss: 1.5155295133590698, acc: 0.6760563254356384)
[2024-12-12 02:46:30,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:30,978][root][INFO] - Training Epoch: 6/10, step 53/574 completed (loss: 1.8849962949752808, acc: 0.47999998927116394)
[2024-12-12 02:46:31,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:31,380][root][INFO] - Training Epoch: 6/10, step 54/574 completed (loss: 0.6393032073974609, acc: 0.837837815284729)
[2024-12-12 02:46:31,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:31,716][root][INFO] - Training Epoch: 6/10, step 55/574 completed (loss: 0.04652469605207443, acc: 1.0)
[2024-12-12 02:46:33,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:34,697][root][INFO] - Training Epoch: 6/10, step 56/574 completed (loss: 1.6718480587005615, acc: 0.5563139915466309)
[2024-12-12 02:46:35,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:36,044][root][INFO] - Training Epoch: 6/10, step 57/574 completed (loss: 2.1473705768585205, acc: 0.429193913936615)
[2024-12-12 02:46:36,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:36,667][root][INFO] - Training Epoch: 6/10, step 58/574 completed (loss: 1.604308009147644, acc: 0.5738636255264282)
[2024-12-12 02:46:36,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:37,237][root][INFO] - Training Epoch: 6/10, step 59/574 completed (loss: 1.6016674041748047, acc: 0.5661764740943909)
[2024-12-12 02:46:37,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:37,801][root][INFO] - Training Epoch: 6/10, step 60/574 completed (loss: 1.746680498123169, acc: 0.5)
[2024-12-12 02:46:37,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:38,225][root][INFO] - Training Epoch: 6/10, step 61/574 completed (loss: 1.2932453155517578, acc: 0.6625000238418579)
[2024-12-12 02:46:38,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:38,618][root][INFO] - Training Epoch: 6/10, step 62/574 completed (loss: 0.5264328718185425, acc: 0.8529411554336548)
[2024-12-12 02:46:38,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:39,010][root][INFO] - Training Epoch: 6/10, step 63/574 completed (loss: 0.8773947954177856, acc: 0.7222222089767456)
[2024-12-12 02:46:39,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:39,373][root][INFO] - Training Epoch: 6/10, step 64/574 completed (loss: 0.632724940776825, acc: 0.765625)
[2024-12-12 02:46:39,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:39,705][root][INFO] - Training Epoch: 6/10, step 65/574 completed (loss: 0.14017626643180847, acc: 0.9655172228813171)
[2024-12-12 02:46:39,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:40,089][root][INFO] - Training Epoch: 6/10, step 66/574 completed (loss: 1.0489317178726196, acc: 0.6964285969734192)
[2024-12-12 02:46:40,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:40,495][root][INFO] - Training Epoch: 6/10, step 67/574 completed (loss: 1.0571943521499634, acc: 0.6333333253860474)
[2024-12-12 02:46:40,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:40,877][root][INFO] - Training Epoch: 6/10, step 68/574 completed (loss: 0.10746315866708755, acc: 0.9599999785423279)
[2024-12-12 02:46:40,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:41,250][root][INFO] - Training Epoch: 6/10, step 69/574 completed (loss: 0.5452913641929626, acc: 0.8611111044883728)
[2024-12-12 02:46:41,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:41,646][root][INFO] - Training Epoch: 6/10, step 70/574 completed (loss: 0.38204118609428406, acc: 0.939393937587738)
[2024-12-12 02:46:41,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:42,004][root][INFO] - Training Epoch: 6/10, step 71/574 completed (loss: 1.4747895002365112, acc: 0.5808823704719543)
[2024-12-12 02:46:42,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:42,410][root][INFO] - Training Epoch: 6/10, step 72/574 completed (loss: 1.4597177505493164, acc: 0.5634920597076416)
[2024-12-12 02:46:42,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:42,813][root][INFO] - Training Epoch: 6/10, step 73/574 completed (loss: 1.851420283317566, acc: 0.5076923370361328)
[2024-12-12 02:46:42,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:43,214][root][INFO] - Training Epoch: 6/10, step 74/574 completed (loss: 0.9921309947967529, acc: 0.7448979616165161)
[2024-12-12 02:46:43,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:43,549][root][INFO] - Training Epoch: 6/10, step 75/574 completed (loss: 1.8180400133132935, acc: 0.48507463932037354)
[2024-12-12 02:46:43,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:43,949][root][INFO] - Training Epoch: 6/10, step 76/574 completed (loss: 2.0356743335723877, acc: 0.43795621395111084)
[2024-12-12 02:46:44,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:44,264][root][INFO] - Training Epoch: 6/10, step 77/574 completed (loss: 0.08850463479757309, acc: 0.9523809552192688)
[2024-12-12 02:46:44,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:44,618][root][INFO] - Training Epoch: 6/10, step 78/574 completed (loss: 0.09643208980560303, acc: 0.9583333134651184)
[2024-12-12 02:46:44,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:44,957][root][INFO] - Training Epoch: 6/10, step 79/574 completed (loss: 0.19624461233615875, acc: 0.939393937587738)
[2024-12-12 02:46:45,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:45,344][root][INFO] - Training Epoch: 6/10, step 80/574 completed (loss: 0.14753474295139313, acc: 0.9230769276618958)
[2024-12-12 02:46:45,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:45,725][root][INFO] - Training Epoch: 6/10, step 81/574 completed (loss: 0.8684006333351135, acc: 0.7692307829856873)
[2024-12-12 02:46:45,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:46,103][root][INFO] - Training Epoch: 6/10, step 82/574 completed (loss: 0.9778897762298584, acc: 0.6730769276618958)
[2024-12-12 02:46:46,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:46,467][root][INFO] - Training Epoch: 6/10, step 83/574 completed (loss: 0.35984402894973755, acc: 0.875)
[2024-12-12 02:46:46,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:46,797][root][INFO] - Training Epoch: 6/10, step 84/574 completed (loss: 1.0896304845809937, acc: 0.739130437374115)
[2024-12-12 02:46:46,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:47,127][root][INFO] - Training Epoch: 6/10, step 85/574 completed (loss: 0.9259576201438904, acc: 0.699999988079071)
[2024-12-12 02:46:47,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:47,447][root][INFO] - Training Epoch: 6/10, step 86/574 completed (loss: 0.21462783217430115, acc: 0.9130434989929199)
[2024-12-12 02:46:47,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:47,900][root][INFO] - Training Epoch: 6/10, step 87/574 completed (loss: 0.835814356803894, acc: 0.7200000286102295)
[2024-12-12 02:46:47,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:48,287][root][INFO] - Training Epoch: 6/10, step 88/574 completed (loss: 1.3364380598068237, acc: 0.6407766938209534)
[2024-12-12 02:46:48,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:49,418][root][INFO] - Training Epoch: 6/10, step 89/574 completed (loss: 1.4282795190811157, acc: 0.6165048480033875)
[2024-12-12 02:46:49,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:50,240][root][INFO] - Training Epoch: 6/10, step 90/574 completed (loss: 1.6947530508041382, acc: 0.5430107712745667)
[2024-12-12 02:46:50,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:51,049][root][INFO] - Training Epoch: 6/10, step 91/574 completed (loss: 1.5698974132537842, acc: 0.6206896305084229)
[2024-12-12 02:46:51,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:51,794][root][INFO] - Training Epoch: 6/10, step 92/574 completed (loss: 1.1180901527404785, acc: 0.6526315808296204)
[2024-12-12 02:46:52,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:52,823][root][INFO] - Training Epoch: 6/10, step 93/574 completed (loss: 1.7966477870941162, acc: 0.5247524976730347)
[2024-12-12 02:46:52,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:53,188][root][INFO] - Training Epoch: 6/10, step 94/574 completed (loss: 1.4830750226974487, acc: 0.6290322542190552)
[2024-12-12 02:46:53,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:53,593][root][INFO] - Training Epoch: 6/10, step 95/574 completed (loss: 1.3704057931900024, acc: 0.6086956262588501)
[2024-12-12 02:46:53,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:53,954][root][INFO] - Training Epoch: 6/10, step 96/574 completed (loss: 1.884246587753296, acc: 0.4789915978908539)
[2024-12-12 02:46:54,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:54,282][root][INFO] - Training Epoch: 6/10, step 97/574 completed (loss: 1.7378565073013306, acc: 0.49038460850715637)
[2024-12-12 02:46:54,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:54,692][root][INFO] - Training Epoch: 6/10, step 98/574 completed (loss: 1.9511590003967285, acc: 0.45255473256111145)
[2024-12-12 02:46:54,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:55,077][root][INFO] - Training Epoch: 6/10, step 99/574 completed (loss: 1.2443305253982544, acc: 0.6567164063453674)
[2024-12-12 02:46:55,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:55,452][root][INFO] - Training Epoch: 6/10, step 100/574 completed (loss: 0.23911449313163757, acc: 0.949999988079071)
[2024-12-12 02:46:55,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:55,803][root][INFO] - Training Epoch: 6/10, step 101/574 completed (loss: 0.08711162954568863, acc: 0.9545454382896423)
[2024-12-12 02:46:55,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:56,181][root][INFO] - Training Epoch: 6/10, step 102/574 completed (loss: 0.04296664893627167, acc: 1.0)
[2024-12-12 02:46:56,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:56,545][root][INFO] - Training Epoch: 6/10, step 103/574 completed (loss: 0.2208050787448883, acc: 0.9545454382896423)
[2024-12-12 02:46:56,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:56,885][root][INFO] - Training Epoch: 6/10, step 104/574 completed (loss: 1.0379425287246704, acc: 0.6896551847457886)
[2024-12-12 02:46:57,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:57,267][root][INFO] - Training Epoch: 6/10, step 105/574 completed (loss: 0.3580344021320343, acc: 0.8604651093482971)
[2024-12-12 02:46:57,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:57,623][root][INFO] - Training Epoch: 6/10, step 106/574 completed (loss: 0.2235890030860901, acc: 0.9200000166893005)
[2024-12-12 02:46:57,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:57,915][root][INFO] - Training Epoch: 6/10, step 107/574 completed (loss: 0.05193326622247696, acc: 1.0)
[2024-12-12 02:46:58,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:58,287][root][INFO] - Training Epoch: 6/10, step 108/574 completed (loss: 0.007909498177468777, acc: 1.0)
[2024-12-12 02:46:58,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:58,669][root][INFO] - Training Epoch: 6/10, step 109/574 completed (loss: 0.361342191696167, acc: 0.9047619104385376)
[2024-12-12 02:46:58,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:59,041][root][INFO] - Training Epoch: 6/10, step 110/574 completed (loss: 1.0620896816253662, acc: 0.7076923251152039)
[2024-12-12 02:46:59,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:59,474][root][INFO] - Training Epoch: 6/10, step 111/574 completed (loss: 1.171116590499878, acc: 0.719298243522644)
[2024-12-12 02:46:59,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:46:59,861][root][INFO] - Training Epoch: 6/10, step 112/574 completed (loss: 1.1608747243881226, acc: 0.6842105388641357)
[2024-12-12 02:46:59,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:00,175][root][INFO] - Training Epoch: 6/10, step 113/574 completed (loss: 0.8498299717903137, acc: 0.7435897588729858)
[2024-12-12 02:47:00,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:00,536][root][INFO] - Training Epoch: 6/10, step 114/574 completed (loss: 0.6822331547737122, acc: 0.795918345451355)
[2024-12-12 02:47:00,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:00,899][root][INFO] - Training Epoch: 6/10, step 115/574 completed (loss: 0.013426094315946102, acc: 1.0)
[2024-12-12 02:47:01,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:01,294][root][INFO] - Training Epoch: 6/10, step 116/574 completed (loss: 1.1565483808517456, acc: 0.682539701461792)
[2024-12-12 02:47:01,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:01,680][root][INFO] - Training Epoch: 6/10, step 117/574 completed (loss: 1.4332016706466675, acc: 0.6585366129875183)
[2024-12-12 02:47:01,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:02,003][root][INFO] - Training Epoch: 6/10, step 118/574 completed (loss: 0.9303451776504517, acc: 0.725806474685669)
[2024-12-12 02:47:02,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:02,877][root][INFO] - Training Epoch: 6/10, step 119/574 completed (loss: 1.7470886707305908, acc: 0.5247148275375366)
[2024-12-12 02:47:02,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:03,251][root][INFO] - Training Epoch: 6/10, step 120/574 completed (loss: 0.7884935736656189, acc: 0.8133333325386047)
[2024-12-12 02:47:03,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:03,664][root][INFO] - Training Epoch: 6/10, step 121/574 completed (loss: 0.8170253038406372, acc: 0.7692307829856873)
[2024-12-12 02:47:03,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:04,038][root][INFO] - Training Epoch: 6/10, step 122/574 completed (loss: 0.17590613663196564, acc: 0.9583333134651184)
[2024-12-12 02:47:04,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:04,412][root][INFO] - Training Epoch: 6/10, step 123/574 completed (loss: 0.15421929955482483, acc: 0.9473684430122375)
[2024-12-12 02:47:04,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:04,836][root][INFO] - Training Epoch: 6/10, step 124/574 completed (loss: 1.5658931732177734, acc: 0.5521472096443176)
[2024-12-12 02:47:04,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:05,252][root][INFO] - Training Epoch: 6/10, step 125/574 completed (loss: 1.4584020376205444, acc: 0.5694444179534912)
[2024-12-12 02:47:05,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:05,612][root][INFO] - Training Epoch: 6/10, step 126/574 completed (loss: 1.6795485019683838, acc: 0.550000011920929)
[2024-12-12 02:47:05,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:05,995][root][INFO] - Training Epoch: 6/10, step 127/574 completed (loss: 1.5557371377944946, acc: 0.5833333134651184)
[2024-12-12 02:47:06,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:06,357][root][INFO] - Training Epoch: 6/10, step 128/574 completed (loss: 1.4359592199325562, acc: 0.5948718190193176)
[2024-12-12 02:47:06,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:06,762][root][INFO] - Training Epoch: 6/10, step 129/574 completed (loss: 1.4347339868545532, acc: 0.6029411554336548)
[2024-12-12 02:47:06,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:07,128][root][INFO] - Training Epoch: 6/10, step 130/574 completed (loss: 0.5873696208000183, acc: 0.8846153616905212)
[2024-12-12 02:47:07,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:07,490][root][INFO] - Training Epoch: 6/10, step 131/574 completed (loss: 0.18486937880516052, acc: 0.95652174949646)
[2024-12-12 02:47:07,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:07,851][root][INFO] - Training Epoch: 6/10, step 132/574 completed (loss: 0.4743911623954773, acc: 0.875)
[2024-12-12 02:47:08,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:08,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:09,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:09,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:09,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:10,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:10,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:10,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:11,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:11,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:11,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:12,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:12,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:12,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:13,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:13,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:13,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:14,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:14,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:14,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:15,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:15,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:15,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:16,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:16,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:16,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:17,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:17,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:17,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:18,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:18,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:18,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:19,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:19,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:19,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:20,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:20,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:20,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:21,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:21,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:22,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:22,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:22,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:23,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:23,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:23,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:24,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:24,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:24,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:25,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:25,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:25,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:26,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:26,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:26,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:27,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:27,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:27,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:28,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:28,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:28,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:29,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:29,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:29,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:30,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:30,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:31,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:31,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:31,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:32,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:32,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:33,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:33,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:33,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:33,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:34,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:34,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:35,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:35,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:35,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:36,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:36,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:36,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:37,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:37,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:37,985][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.2012, device='cuda:0') eval_epoch_loss=tensor(1.4354, device='cuda:0') eval_epoch_acc=tensor(0.6474, device='cuda:0')
[2024-12-12 02:47:37,986][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:47:37,986][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:47:38,175][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_6_step_133_loss_1.4353593587875366/model.pt
[2024-12-12 02:47:38,179][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:47:38,179][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 6 is 1.4353593587875366
[2024-12-12 02:47:38,180][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 6 is 0.6473864912986755
[2024-12-12 02:47:38,281][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:38,572][root][INFO] - Training Epoch: 6/10, step 133/574 completed (loss: 0.39399197697639465, acc: 0.95652174949646)
[2024-12-12 02:47:38,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:38,913][root][INFO] - Training Epoch: 6/10, step 134/574 completed (loss: 0.6625014543533325, acc: 0.8857142925262451)
[2024-12-12 02:47:39,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:39,278][root][INFO] - Training Epoch: 6/10, step 135/574 completed (loss: 0.12787608802318573, acc: 0.9615384340286255)
[2024-12-12 02:47:39,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:39,614][root][INFO] - Training Epoch: 6/10, step 136/574 completed (loss: 0.7361326217651367, acc: 0.7857142686843872)
[2024-12-12 02:47:39,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:39,959][root][INFO] - Training Epoch: 6/10, step 137/574 completed (loss: 0.49855539202690125, acc: 0.8666666746139526)
[2024-12-12 02:47:40,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:40,298][root][INFO] - Training Epoch: 6/10, step 138/574 completed (loss: 0.2928200662136078, acc: 0.95652174949646)
[2024-12-12 02:47:40,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:40,609][root][INFO] - Training Epoch: 6/10, step 139/574 completed (loss: 0.2441658228635788, acc: 0.9047619104385376)
[2024-12-12 02:47:40,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:40,921][root][INFO] - Training Epoch: 6/10, step 140/574 completed (loss: 0.3090582489967346, acc: 0.9230769276618958)
[2024-12-12 02:47:40,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:41,272][root][INFO] - Training Epoch: 6/10, step 141/574 completed (loss: 0.34069380164146423, acc: 0.8709677457809448)
[2024-12-12 02:47:41,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:41,621][root][INFO] - Training Epoch: 6/10, step 142/574 completed (loss: 0.6305302381515503, acc: 0.7837837934494019)
[2024-12-12 02:47:41,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:42,144][root][INFO] - Training Epoch: 6/10, step 143/574 completed (loss: 1.3690465688705444, acc: 0.5438596606254578)
[2024-12-12 02:47:42,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:42,517][root][INFO] - Training Epoch: 6/10, step 144/574 completed (loss: 1.3337904214859009, acc: 0.5970149040222168)
[2024-12-12 02:47:42,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:42,879][root][INFO] - Training Epoch: 6/10, step 145/574 completed (loss: 1.5391916036605835, acc: 0.5510203838348389)
[2024-12-12 02:47:42,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:43,310][root][INFO] - Training Epoch: 6/10, step 146/574 completed (loss: 1.6542603969573975, acc: 0.5)
[2024-12-12 02:47:43,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:43,657][root][INFO] - Training Epoch: 6/10, step 147/574 completed (loss: 1.1731756925582886, acc: 0.699999988079071)
[2024-12-12 02:47:43,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:44,017][root][INFO] - Training Epoch: 6/10, step 148/574 completed (loss: 0.7135875821113586, acc: 0.7142857313156128)
[2024-12-12 02:47:44,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:44,343][root][INFO] - Training Epoch: 6/10, step 149/574 completed (loss: 0.6444501876831055, acc: 0.8260869383811951)
[2024-12-12 02:47:44,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:44,680][root][INFO] - Training Epoch: 6/10, step 150/574 completed (loss: 0.28937238454818726, acc: 0.931034505367279)
[2024-12-12 02:47:44,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:45,079][root][INFO] - Training Epoch: 6/10, step 151/574 completed (loss: 1.166024923324585, acc: 0.695652186870575)
[2024-12-12 02:47:45,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:45,480][root][INFO] - Training Epoch: 6/10, step 152/574 completed (loss: 1.1364473104476929, acc: 0.6779661178588867)
[2024-12-12 02:47:45,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:45,837][root][INFO] - Training Epoch: 6/10, step 153/574 completed (loss: 1.1909087896347046, acc: 0.6315789222717285)
[2024-12-12 02:47:45,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:46,178][root][INFO] - Training Epoch: 6/10, step 154/574 completed (loss: 1.178444266319275, acc: 0.7027027010917664)
[2024-12-12 02:47:46,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:46,468][root][INFO] - Training Epoch: 6/10, step 155/574 completed (loss: 0.2866635024547577, acc: 0.9642857313156128)
[2024-12-12 02:47:46,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:46,855][root][INFO] - Training Epoch: 6/10, step 156/574 completed (loss: 0.3495652675628662, acc: 0.8695651888847351)
[2024-12-12 02:47:46,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:47,185][root][INFO] - Training Epoch: 6/10, step 157/574 completed (loss: 0.7277128100395203, acc: 0.8421052694320679)
[2024-12-12 02:47:47,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:48,822][root][INFO] - Training Epoch: 6/10, step 158/574 completed (loss: 1.1207308769226074, acc: 0.6486486196517944)
[2024-12-12 02:47:48,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:49,109][root][INFO] - Training Epoch: 6/10, step 159/574 completed (loss: 1.5587443113327026, acc: 0.5555555820465088)
[2024-12-12 02:47:49,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:49,494][root][INFO] - Training Epoch: 6/10, step 160/574 completed (loss: 1.0500881671905518, acc: 0.7558139562606812)
[2024-12-12 02:47:49,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:50,081][root][INFO] - Training Epoch: 6/10, step 161/574 completed (loss: 1.1900469064712524, acc: 0.6117647290229797)
[2024-12-12 02:47:50,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:50,635][root][INFO] - Training Epoch: 6/10, step 162/574 completed (loss: 1.6356855630874634, acc: 0.5393258333206177)
[2024-12-12 02:47:50,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:50,987][root][INFO] - Training Epoch: 6/10, step 163/574 completed (loss: 0.7800687551498413, acc: 0.8181818127632141)
[2024-12-12 02:47:51,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:51,356][root][INFO] - Training Epoch: 6/10, step 164/574 completed (loss: 0.6168456673622131, acc: 0.761904776096344)
[2024-12-12 02:47:51,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:51,688][root][INFO] - Training Epoch: 6/10, step 165/574 completed (loss: 0.8457064032554626, acc: 0.7586206793785095)
[2024-12-12 02:47:51,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:52,053][root][INFO] - Training Epoch: 6/10, step 166/574 completed (loss: 0.45999377965927124, acc: 0.8775510191917419)
[2024-12-12 02:47:52,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:52,383][root][INFO] - Training Epoch: 6/10, step 167/574 completed (loss: 0.4875842332839966, acc: 0.8600000143051147)
[2024-12-12 02:47:52,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:52,787][root][INFO] - Training Epoch: 6/10, step 168/574 completed (loss: 0.9007278084754944, acc: 0.7916666865348816)
[2024-12-12 02:47:52,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:53,171][root][INFO] - Training Epoch: 6/10, step 169/574 completed (loss: 1.4396275281906128, acc: 0.6078431606292725)
[2024-12-12 02:47:53,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:54,201][root][INFO] - Training Epoch: 6/10, step 170/574 completed (loss: 1.82401704788208, acc: 0.5136986374855042)
[2024-12-12 02:47:54,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:54,494][root][INFO] - Training Epoch: 6/10, step 171/574 completed (loss: 0.2704288065433502, acc: 0.9583333134651184)
[2024-12-12 02:47:54,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:54,831][root][INFO] - Training Epoch: 6/10, step 172/574 completed (loss: 0.6152952909469604, acc: 0.8518518805503845)
[2024-12-12 02:47:54,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:55,139][root][INFO] - Training Epoch: 6/10, step 173/574 completed (loss: 0.2436961680650711, acc: 0.9642857313156128)
[2024-12-12 02:47:55,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:55,678][root][INFO] - Training Epoch: 6/10, step 174/574 completed (loss: 1.1371667385101318, acc: 0.7079645991325378)
[2024-12-12 02:47:55,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:56,026][root][INFO] - Training Epoch: 6/10, step 175/574 completed (loss: 0.8994447588920593, acc: 0.782608687877655)
[2024-12-12 02:47:56,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:56,374][root][INFO] - Training Epoch: 6/10, step 176/574 completed (loss: 1.0088638067245483, acc: 0.7159090638160706)
[2024-12-12 02:47:56,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:57,284][root][INFO] - Training Epoch: 6/10, step 177/574 completed (loss: 1.9111976623535156, acc: 0.49618321657180786)
[2024-12-12 02:47:57,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:57,956][root][INFO] - Training Epoch: 6/10, step 178/574 completed (loss: 1.6121267080307007, acc: 0.5333333611488342)
[2024-12-12 02:47:58,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:58,312][root][INFO] - Training Epoch: 6/10, step 179/574 completed (loss: 0.8035533428192139, acc: 0.8032786846160889)
[2024-12-12 02:47:58,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:58,661][root][INFO] - Training Epoch: 6/10, step 180/574 completed (loss: 0.1383896917104721, acc: 0.9583333134651184)
[2024-12-12 02:47:58,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:59,003][root][INFO] - Training Epoch: 6/10, step 181/574 completed (loss: 0.54923015832901, acc: 0.8399999737739563)
[2024-12-12 02:47:59,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:59,322][root][INFO] - Training Epoch: 6/10, step 182/574 completed (loss: 0.15618740022182465, acc: 0.9285714030265808)
[2024-12-12 02:47:59,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:59,600][root][INFO] - Training Epoch: 6/10, step 183/574 completed (loss: 1.108479380607605, acc: 0.6097561120986938)
[2024-12-12 02:47:59,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:47:59,926][root][INFO] - Training Epoch: 6/10, step 184/574 completed (loss: 1.6891448497772217, acc: 0.5226585865020752)
[2024-12-12 02:48:00,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:00,288][root][INFO] - Training Epoch: 6/10, step 185/574 completed (loss: 1.8599804639816284, acc: 0.47838616371154785)
[2024-12-12 02:48:00,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:00,767][root][INFO] - Training Epoch: 6/10, step 186/574 completed (loss: 1.8954932689666748, acc: 0.5)
[2024-12-12 02:48:00,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:01,295][root][INFO] - Training Epoch: 6/10, step 187/574 completed (loss: 1.9492771625518799, acc: 0.4746716618537903)
[2024-12-12 02:48:01,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:01,710][root][INFO] - Training Epoch: 6/10, step 188/574 completed (loss: 1.6891855001449585, acc: 0.5373665690422058)
[2024-12-12 02:48:01,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:02,058][root][INFO] - Training Epoch: 6/10, step 189/574 completed (loss: 0.3407604694366455, acc: 0.8799999952316284)
[2024-12-12 02:48:02,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:02,614][root][INFO] - Training Epoch: 6/10, step 190/574 completed (loss: 1.703816294670105, acc: 0.5232558250427246)
[2024-12-12 02:48:02,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:03,413][root][INFO] - Training Epoch: 6/10, step 191/574 completed (loss: 1.8324788808822632, acc: 0.5476190447807312)
[2024-12-12 02:48:03,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:04,339][root][INFO] - Training Epoch: 6/10, step 192/574 completed (loss: 1.718250036239624, acc: 0.5530303120613098)
[2024-12-12 02:48:04,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:05,089][root][INFO] - Training Epoch: 6/10, step 193/574 completed (loss: 1.2368688583374023, acc: 0.6941176652908325)
[2024-12-12 02:48:05,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:06,168][root][INFO] - Training Epoch: 6/10, step 194/574 completed (loss: 1.2572782039642334, acc: 0.6296296119689941)
[2024-12-12 02:48:06,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:07,127][root][INFO] - Training Epoch: 6/10, step 195/574 completed (loss: 0.9652897119522095, acc: 0.6935483813285828)
[2024-12-12 02:48:07,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:07,492][root][INFO] - Training Epoch: 6/10, step 196/574 completed (loss: 0.13033680617809296, acc: 0.9642857313156128)
[2024-12-12 02:48:07,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:07,864][root][INFO] - Training Epoch: 6/10, step 197/574 completed (loss: 0.345457524061203, acc: 0.875)
[2024-12-12 02:48:07,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:08,215][root][INFO] - Training Epoch: 6/10, step 198/574 completed (loss: 0.8448914289474487, acc: 0.779411792755127)
[2024-12-12 02:48:08,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:08,605][root][INFO] - Training Epoch: 6/10, step 199/574 completed (loss: 1.3456703424453735, acc: 0.625)
[2024-12-12 02:48:08,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:08,973][root][INFO] - Training Epoch: 6/10, step 200/574 completed (loss: 1.3370776176452637, acc: 0.5847457647323608)
[2024-12-12 02:48:09,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:09,295][root][INFO] - Training Epoch: 6/10, step 201/574 completed (loss: 1.5918943881988525, acc: 0.5522388219833374)
[2024-12-12 02:48:09,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:09,626][root][INFO] - Training Epoch: 6/10, step 202/574 completed (loss: 1.3008946180343628, acc: 0.6407766938209534)
[2024-12-12 02:48:09,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:09,926][root][INFO] - Training Epoch: 6/10, step 203/574 completed (loss: 1.1513444185256958, acc: 0.6349206566810608)
[2024-12-12 02:48:10,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:10,297][root][INFO] - Training Epoch: 6/10, step 204/574 completed (loss: 0.7835429310798645, acc: 0.7692307829856873)
[2024-12-12 02:48:10,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:10,656][root][INFO] - Training Epoch: 6/10, step 205/574 completed (loss: 1.5427024364471436, acc: 0.5784753561019897)
[2024-12-12 02:48:10,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:11,045][root][INFO] - Training Epoch: 6/10, step 206/574 completed (loss: 1.664517879486084, acc: 0.5354330539703369)
[2024-12-12 02:48:11,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:11,384][root][INFO] - Training Epoch: 6/10, step 207/574 completed (loss: 1.4031023979187012, acc: 0.5991379022598267)
[2024-12-12 02:48:11,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:11,772][root][INFO] - Training Epoch: 6/10, step 208/574 completed (loss: 1.5100129842758179, acc: 0.5942028760910034)
[2024-12-12 02:48:11,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:12,167][root][INFO] - Training Epoch: 6/10, step 209/574 completed (loss: 1.4482554197311401, acc: 0.5953307151794434)
[2024-12-12 02:48:12,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:12,503][root][INFO] - Training Epoch: 6/10, step 210/574 completed (loss: 1.5099984407424927, acc: 0.554347813129425)
[2024-12-12 02:48:12,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:12,804][root][INFO] - Training Epoch: 6/10, step 211/574 completed (loss: 0.19417059421539307, acc: 0.9130434989929199)
[2024-12-12 02:48:12,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:13,123][root][INFO] - Training Epoch: 6/10, step 212/574 completed (loss: 0.08031988143920898, acc: 1.0)
[2024-12-12 02:48:13,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:13,511][root][INFO] - Training Epoch: 6/10, step 213/574 completed (loss: 0.49941813945770264, acc: 0.8936170339584351)
[2024-12-12 02:48:13,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:14,195][root][INFO] - Training Epoch: 6/10, step 214/574 completed (loss: 1.0090688467025757, acc: 0.7076923251152039)
[2024-12-12 02:48:14,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:14,528][root][INFO] - Training Epoch: 6/10, step 215/574 completed (loss: 0.6894286870956421, acc: 0.7972972989082336)
[2024-12-12 02:48:14,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:14,844][root][INFO] - Training Epoch: 6/10, step 216/574 completed (loss: 0.7214743494987488, acc: 0.8255813717842102)
[2024-12-12 02:48:14,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:15,376][root][INFO] - Training Epoch: 6/10, step 217/574 completed (loss: 0.9599981904029846, acc: 0.7387387156486511)
[2024-12-12 02:48:15,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:15,786][root][INFO] - Training Epoch: 6/10, step 218/574 completed (loss: 0.7312456965446472, acc: 0.8222222328186035)
[2024-12-12 02:48:15,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:16,139][root][INFO] - Training Epoch: 6/10, step 219/574 completed (loss: 0.2973606288433075, acc: 0.8787878751754761)
[2024-12-12 02:48:16,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:16,472][root][INFO] - Training Epoch: 6/10, step 220/574 completed (loss: 0.03325559198856354, acc: 1.0)
[2024-12-12 02:48:16,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:16,785][root][INFO] - Training Epoch: 6/10, step 221/574 completed (loss: 0.26331645250320435, acc: 0.9599999785423279)
[2024-12-12 02:48:16,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:17,102][root][INFO] - Training Epoch: 6/10, step 222/574 completed (loss: 0.675770103931427, acc: 0.75)
[2024-12-12 02:48:17,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:17,860][root][INFO] - Training Epoch: 6/10, step 223/574 completed (loss: 1.0508222579956055, acc: 0.6739130616188049)
[2024-12-12 02:48:18,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:18,400][root][INFO] - Training Epoch: 6/10, step 224/574 completed (loss: 1.392029881477356, acc: 0.6079545617103577)
[2024-12-12 02:48:18,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:18,838][root][INFO] - Training Epoch: 6/10, step 225/574 completed (loss: 1.3424978256225586, acc: 0.6170212626457214)
[2024-12-12 02:48:18,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:19,196][root][INFO] - Training Epoch: 6/10, step 226/574 completed (loss: 0.6124951243400574, acc: 0.8113207817077637)
[2024-12-12 02:48:19,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:19,559][root][INFO] - Training Epoch: 6/10, step 227/574 completed (loss: 0.6380597949028015, acc: 0.75)
[2024-12-12 02:48:19,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:19,948][root][INFO] - Training Epoch: 6/10, step 228/574 completed (loss: 0.26609787344932556, acc: 0.8837209343910217)
[2024-12-12 02:48:20,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:20,339][root][INFO] - Training Epoch: 6/10, step 229/574 completed (loss: 0.48086827993392944, acc: 0.8666666746139526)
[2024-12-12 02:48:20,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:20,711][root][INFO] - Training Epoch: 6/10, step 230/574 completed (loss: 1.5897902250289917, acc: 0.5263158082962036)
[2024-12-12 02:48:20,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:21,073][root][INFO] - Training Epoch: 6/10, step 231/574 completed (loss: 1.0959151983261108, acc: 0.6777777671813965)
[2024-12-12 02:48:21,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:21,495][root][INFO] - Training Epoch: 6/10, step 232/574 completed (loss: 1.0916707515716553, acc: 0.7111111283302307)
[2024-12-12 02:48:21,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:21,986][root][INFO] - Training Epoch: 6/10, step 233/574 completed (loss: 1.5277647972106934, acc: 0.5917431116104126)
[2024-12-12 02:48:22,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:22,463][root][INFO] - Training Epoch: 6/10, step 234/574 completed (loss: 1.159257411956787, acc: 0.6615384817123413)
[2024-12-12 02:48:22,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:22,799][root][INFO] - Training Epoch: 6/10, step 235/574 completed (loss: 0.16344697773456573, acc: 0.9473684430122375)
[2024-12-12 02:48:22,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:23,116][root][INFO] - Training Epoch: 6/10, step 236/574 completed (loss: 0.2885325253009796, acc: 0.875)
[2024-12-12 02:48:23,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:23,423][root][INFO] - Training Epoch: 6/10, step 237/574 completed (loss: 0.37550684809684753, acc: 0.8636363744735718)
[2024-12-12 02:48:23,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:23,772][root][INFO] - Training Epoch: 6/10, step 238/574 completed (loss: 0.6894961595535278, acc: 0.7777777910232544)
[2024-12-12 02:48:23,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:24,092][root][INFO] - Training Epoch: 6/10, step 239/574 completed (loss: 0.23457665741443634, acc: 0.9428571462631226)
[2024-12-12 02:48:24,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:24,427][root][INFO] - Training Epoch: 6/10, step 240/574 completed (loss: 0.6807201504707336, acc: 0.8409090638160706)
[2024-12-12 02:48:24,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:24,742][root][INFO] - Training Epoch: 6/10, step 241/574 completed (loss: 0.48968765139579773, acc: 0.8181818127632141)
[2024-12-12 02:48:24,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:25,327][root][INFO] - Training Epoch: 6/10, step 242/574 completed (loss: 1.3139148950576782, acc: 0.6290322542190552)
[2024-12-12 02:48:25,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:25,863][root][INFO] - Training Epoch: 6/10, step 243/574 completed (loss: 0.9038391709327698, acc: 0.7272727489471436)
[2024-12-12 02:48:25,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:26,170][root][INFO] - Training Epoch: 6/10, step 244/574 completed (loss: 0.009015624411404133, acc: 1.0)
[2024-12-12 02:48:26,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:26,522][root][INFO] - Training Epoch: 6/10, step 245/574 completed (loss: 0.14263933897018433, acc: 0.9615384340286255)
[2024-12-12 02:48:26,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:26,899][root][INFO] - Training Epoch: 6/10, step 246/574 completed (loss: 0.17359033226966858, acc: 0.9677419066429138)
[2024-12-12 02:48:26,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:27,239][root][INFO] - Training Epoch: 6/10, step 247/574 completed (loss: 0.21516764163970947, acc: 0.949999988079071)
[2024-12-12 02:48:27,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:27,582][root][INFO] - Training Epoch: 6/10, step 248/574 completed (loss: 0.4360295832157135, acc: 0.8648648858070374)
[2024-12-12 02:48:27,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:27,923][root][INFO] - Training Epoch: 6/10, step 249/574 completed (loss: 0.43996477127075195, acc: 0.8648648858070374)
[2024-12-12 02:48:28,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:28,285][root][INFO] - Training Epoch: 6/10, step 250/574 completed (loss: 0.5183202624320984, acc: 0.8918918967247009)
[2024-12-12 02:48:28,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:28,674][root][INFO] - Training Epoch: 6/10, step 251/574 completed (loss: 0.8479512929916382, acc: 0.720588207244873)
[2024-12-12 02:48:28,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:29,027][root][INFO] - Training Epoch: 6/10, step 252/574 completed (loss: 0.17169694602489471, acc: 0.9512194991111755)
[2024-12-12 02:48:29,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:29,376][root][INFO] - Training Epoch: 6/10, step 253/574 completed (loss: 0.0266854427754879, acc: 1.0)
[2024-12-12 02:48:29,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:29,756][root][INFO] - Training Epoch: 6/10, step 254/574 completed (loss: 0.04770227521657944, acc: 1.0)
[2024-12-12 02:48:29,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:30,084][root][INFO] - Training Epoch: 6/10, step 255/574 completed (loss: 0.053431905806064606, acc: 1.0)
[2024-12-12 02:48:30,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:30,445][root][INFO] - Training Epoch: 6/10, step 256/574 completed (loss: 0.4175538718700409, acc: 0.8771929740905762)
[2024-12-12 02:48:30,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:30,788][root][INFO] - Training Epoch: 6/10, step 257/574 completed (loss: 0.7304800152778625, acc: 0.8285714387893677)
[2024-12-12 02:48:30,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:31,169][root][INFO] - Training Epoch: 6/10, step 258/574 completed (loss: 0.6133461594581604, acc: 0.8289473652839661)
[2024-12-12 02:48:31,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:31,736][root][INFO] - Training Epoch: 6/10, step 259/574 completed (loss: 1.0132777690887451, acc: 0.7075471878051758)
[2024-12-12 02:48:31,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:32,320][root][INFO] - Training Epoch: 6/10, step 260/574 completed (loss: 1.243346095085144, acc: 0.675000011920929)
[2024-12-12 02:48:32,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:32,661][root][INFO] - Training Epoch: 6/10, step 261/574 completed (loss: 0.30873844027519226, acc: 0.9444444179534912)
[2024-12-12 02:48:32,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:33,029][root][INFO] - Training Epoch: 6/10, step 262/574 completed (loss: 0.41378501057624817, acc: 0.9032257795333862)
[2024-12-12 02:48:33,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:33,414][root][INFO] - Training Epoch: 6/10, step 263/574 completed (loss: 1.721052646636963, acc: 0.54666668176651)
[2024-12-12 02:48:33,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:33,778][root][INFO] - Training Epoch: 6/10, step 264/574 completed (loss: 1.3353286981582642, acc: 0.6458333134651184)
[2024-12-12 02:48:33,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:34,602][root][INFO] - Training Epoch: 6/10, step 265/574 completed (loss: 1.9961563348770142, acc: 0.46399998664855957)
[2024-12-12 02:48:34,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:34,966][root][INFO] - Training Epoch: 6/10, step 266/574 completed (loss: 1.7504860162734985, acc: 0.516853928565979)
[2024-12-12 02:48:35,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:35,310][root][INFO] - Training Epoch: 6/10, step 267/574 completed (loss: 1.4974178075790405, acc: 0.5135135054588318)
[2024-12-12 02:48:35,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:35,773][root][INFO] - Training Epoch: 6/10, step 268/574 completed (loss: 1.0341118574142456, acc: 0.7068965435028076)
[2024-12-12 02:48:35,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:36,139][root][INFO] - Training Epoch: 6/10, step 269/574 completed (loss: 0.03368350490927696, acc: 1.0)
[2024-12-12 02:48:36,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:36,515][root][INFO] - Training Epoch: 6/10, step 270/574 completed (loss: 0.068965844810009, acc: 1.0)
[2024-12-12 02:48:36,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:36,883][root][INFO] - Training Epoch: 6/10, step 271/574 completed (loss: 0.22194206714630127, acc: 0.9375)
[2024-12-12 02:48:37,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:37,257][root][INFO] - Training Epoch: 6/10, step 272/574 completed (loss: 0.37154844403266907, acc: 0.8999999761581421)
[2024-12-12 02:48:37,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:37,656][root][INFO] - Training Epoch: 6/10, step 273/574 completed (loss: 0.9804669618606567, acc: 0.7333333492279053)
[2024-12-12 02:48:37,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:38,046][root][INFO] - Training Epoch: 6/10, step 274/574 completed (loss: 0.4461587965488434, acc: 0.875)
[2024-12-12 02:48:38,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:38,401][root][INFO] - Training Epoch: 6/10, step 275/574 completed (loss: 0.3104066848754883, acc: 0.9333333373069763)
[2024-12-12 02:48:39,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:39,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:39,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:40,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:40,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:40,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:41,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:41,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:42,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:42,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:42,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:43,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:43,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:43,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:44,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:44,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:44,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:45,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:45,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:45,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:46,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:46,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:46,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:47,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:47,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:47,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:48,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:48,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:49,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:49,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:49,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:50,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:50,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:50,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:51,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:51,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:51,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:52,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:52,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:52,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:53,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:53,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:53,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:54,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:54,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:54,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:55,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:55,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:56,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:56,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:56,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:57,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:57,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:57,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:58,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:58,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:58,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:59,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:59,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:48:59,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:00,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:00,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:01,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:01,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:01,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:02,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:02,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:02,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:03,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:03,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:03,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:04,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:04,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:04,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:05,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:05,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:05,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:05,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:06,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:06,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:06,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:07,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:07,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:07,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:08,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:08,857][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.1593, device='cuda:0') eval_epoch_loss=tensor(1.4254, device='cuda:0') eval_epoch_acc=tensor(0.6532, device='cuda:0')
[2024-12-12 02:49:08,859][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:49:08,859][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:49:09,055][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_6_step_276_loss_1.4253522157669067/model.pt
[2024-12-12 02:49:09,058][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:49:09,059][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 6 is 1.4253522157669067
[2024-12-12 02:49:09,059][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 6 is 0.6532231569290161
[2024-12-12 02:49:09,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:09,489][root][INFO] - Training Epoch: 6/10, step 276/574 completed (loss: 0.6072306036949158, acc: 0.8275862336158752)
[2024-12-12 02:49:09,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:09,822][root][INFO] - Training Epoch: 6/10, step 277/574 completed (loss: 0.25886857509613037, acc: 0.9200000166893005)
[2024-12-12 02:49:09,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:10,084][root][INFO] - Training Epoch: 6/10, step 278/574 completed (loss: 0.8594492077827454, acc: 0.6808510422706604)
[2024-12-12 02:49:10,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:10,436][root][INFO] - Training Epoch: 6/10, step 279/574 completed (loss: 0.6557544469833374, acc: 0.8333333134651184)
[2024-12-12 02:49:10,540][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:10,784][root][INFO] - Training Epoch: 6/10, step 280/574 completed (loss: 0.3891657888889313, acc: 0.9090909361839294)
[2024-12-12 02:49:10,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:11,200][root][INFO] - Training Epoch: 6/10, step 281/574 completed (loss: 1.275158166885376, acc: 0.6626505851745605)
[2024-12-12 02:49:11,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:11,561][root][INFO] - Training Epoch: 6/10, step 282/574 completed (loss: 1.5758038759231567, acc: 0.6388888955116272)
[2024-12-12 02:49:11,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:11,872][root][INFO] - Training Epoch: 6/10, step 283/574 completed (loss: 0.34962424635887146, acc: 0.8947368264198303)
[2024-12-12 02:49:11,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:12,234][root][INFO] - Training Epoch: 6/10, step 284/574 completed (loss: 0.2769777774810791, acc: 0.9117646813392639)
[2024-12-12 02:49:12,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:12,579][root][INFO] - Training Epoch: 6/10, step 285/574 completed (loss: 0.2005360871553421, acc: 0.949999988079071)
[2024-12-12 02:49:12,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:12,964][root][INFO] - Training Epoch: 6/10, step 286/574 completed (loss: 1.2015153169631958, acc: 0.6015625)
[2024-12-12 02:49:13,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:13,296][root][INFO] - Training Epoch: 6/10, step 287/574 completed (loss: 1.390371322631836, acc: 0.6159999966621399)
[2024-12-12 02:49:13,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:13,648][root][INFO] - Training Epoch: 6/10, step 288/574 completed (loss: 0.7397661209106445, acc: 0.8351648449897766)
[2024-12-12 02:49:13,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:13,995][root][INFO] - Training Epoch: 6/10, step 289/574 completed (loss: 1.6037957668304443, acc: 0.5652173757553101)
[2024-12-12 02:49:14,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:14,310][root][INFO] - Training Epoch: 6/10, step 290/574 completed (loss: 1.6185805797576904, acc: 0.5412371158599854)
[2024-12-12 02:49:14,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:14,683][root][INFO] - Training Epoch: 6/10, step 291/574 completed (loss: 0.24556118249893188, acc: 0.9090909361839294)
[2024-12-12 02:49:14,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:15,076][root][INFO] - Training Epoch: 6/10, step 292/574 completed (loss: 0.4998706579208374, acc: 0.8809523582458496)
[2024-12-12 02:49:15,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:15,486][root][INFO] - Training Epoch: 6/10, step 293/574 completed (loss: 0.917121410369873, acc: 0.7758620977401733)
[2024-12-12 02:49:15,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:15,937][root][INFO] - Training Epoch: 6/10, step 294/574 completed (loss: 0.7140412926673889, acc: 0.7818182110786438)
[2024-12-12 02:49:16,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:16,485][root][INFO] - Training Epoch: 6/10, step 295/574 completed (loss: 1.3198035955429077, acc: 0.623711347579956)
[2024-12-12 02:49:16,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:16,835][root][INFO] - Training Epoch: 6/10, step 296/574 completed (loss: 0.9742462635040283, acc: 0.7068965435028076)
[2024-12-12 02:49:16,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:17,213][root][INFO] - Training Epoch: 6/10, step 297/574 completed (loss: 0.5105132460594177, acc: 0.8148148059844971)
[2024-12-12 02:49:17,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:17,554][root][INFO] - Training Epoch: 6/10, step 298/574 completed (loss: 0.6005993485450745, acc: 0.8421052694320679)
[2024-12-12 02:49:17,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:17,925][root][INFO] - Training Epoch: 6/10, step 299/574 completed (loss: 0.5347405076026917, acc: 0.8392857313156128)
[2024-12-12 02:49:18,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:18,303][root][INFO] - Training Epoch: 6/10, step 300/574 completed (loss: 0.33851468563079834, acc: 0.90625)
[2024-12-12 02:49:18,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:18,645][root][INFO] - Training Epoch: 6/10, step 301/574 completed (loss: 0.6913300156593323, acc: 0.7735849022865295)
[2024-12-12 02:49:18,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:18,968][root][INFO] - Training Epoch: 6/10, step 302/574 completed (loss: 0.23760950565338135, acc: 0.9245283007621765)
[2024-12-12 02:49:19,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:19,286][root][INFO] - Training Epoch: 6/10, step 303/574 completed (loss: 0.15197008848190308, acc: 0.9411764740943909)
[2024-12-12 02:49:19,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:19,606][root][INFO] - Training Epoch: 6/10, step 304/574 completed (loss: 0.2708694636821747, acc: 0.90625)
[2024-12-12 02:49:19,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:19,929][root][INFO] - Training Epoch: 6/10, step 305/574 completed (loss: 0.746153712272644, acc: 0.7704917788505554)
[2024-12-12 02:49:20,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:20,263][root][INFO] - Training Epoch: 6/10, step 306/574 completed (loss: 0.10690407454967499, acc: 0.9666666388511658)
[2024-12-12 02:49:20,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:20,596][root][INFO] - Training Epoch: 6/10, step 307/574 completed (loss: 0.009588979184627533, acc: 1.0)
[2024-12-12 02:49:20,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:20,977][root][INFO] - Training Epoch: 6/10, step 308/574 completed (loss: 0.8113515377044678, acc: 0.7681159377098083)
[2024-12-12 02:49:21,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:21,385][root][INFO] - Training Epoch: 6/10, step 309/574 completed (loss: 0.8154861927032471, acc: 0.7638888955116272)
[2024-12-12 02:49:21,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:21,747][root][INFO] - Training Epoch: 6/10, step 310/574 completed (loss: 0.9313559532165527, acc: 0.6987951993942261)
[2024-12-12 02:49:21,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:22,060][root][INFO] - Training Epoch: 6/10, step 311/574 completed (loss: 1.1208932399749756, acc: 0.6794871687889099)
[2024-12-12 02:49:22,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:22,424][root][INFO] - Training Epoch: 6/10, step 312/574 completed (loss: 1.197606086730957, acc: 0.6428571343421936)
[2024-12-12 02:49:22,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:22,729][root][INFO] - Training Epoch: 6/10, step 313/574 completed (loss: 0.018010035157203674, acc: 1.0)
[2024-12-12 02:49:22,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:23,038][root][INFO] - Training Epoch: 6/10, step 314/574 completed (loss: 0.07381264120340347, acc: 1.0)
[2024-12-12 02:49:23,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:23,419][root][INFO] - Training Epoch: 6/10, step 315/574 completed (loss: 0.14611439406871796, acc: 0.9354838728904724)
[2024-12-12 02:49:23,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:23,787][root][INFO] - Training Epoch: 6/10, step 316/574 completed (loss: 0.36601823568344116, acc: 0.9032257795333862)
[2024-12-12 02:49:23,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:24,133][root][INFO] - Training Epoch: 6/10, step 317/574 completed (loss: 0.5892935991287231, acc: 0.8208954930305481)
[2024-12-12 02:49:24,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:24,508][root][INFO] - Training Epoch: 6/10, step 318/574 completed (loss: 0.7990948557853699, acc: 0.7692307829856873)
[2024-12-12 02:49:24,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:24,806][root][INFO] - Training Epoch: 6/10, step 319/574 completed (loss: 0.3370197117328644, acc: 0.9111111164093018)
[2024-12-12 02:49:24,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:25,095][root][INFO] - Training Epoch: 6/10, step 320/574 completed (loss: 0.38300755620002747, acc: 0.8870967626571655)
[2024-12-12 02:49:25,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:25,382][root][INFO] - Training Epoch: 6/10, step 321/574 completed (loss: 0.21136625111103058, acc: 0.9399999976158142)
[2024-12-12 02:49:25,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:25,680][root][INFO] - Training Epoch: 6/10, step 322/574 completed (loss: 0.5978392362594604, acc: 0.8148148059844971)
[2024-12-12 02:49:25,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:26,036][root][INFO] - Training Epoch: 6/10, step 323/574 completed (loss: 0.6954365372657776, acc: 0.800000011920929)
[2024-12-12 02:49:26,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:26,383][root][INFO] - Training Epoch: 6/10, step 324/574 completed (loss: 0.7660197019577026, acc: 0.7435897588729858)
[2024-12-12 02:49:26,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:26,696][root][INFO] - Training Epoch: 6/10, step 325/574 completed (loss: 1.069246768951416, acc: 0.6829268336296082)
[2024-12-12 02:49:26,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:27,051][root][INFO] - Training Epoch: 6/10, step 326/574 completed (loss: 0.9998055696487427, acc: 0.7368420958518982)
[2024-12-12 02:49:27,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:27,363][root][INFO] - Training Epoch: 6/10, step 327/574 completed (loss: 0.10281667858362198, acc: 1.0)
[2024-12-12 02:49:27,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:27,725][root][INFO] - Training Epoch: 6/10, step 328/574 completed (loss: 0.03363766893744469, acc: 0.9642857313156128)
[2024-12-12 02:49:27,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:28,077][root][INFO] - Training Epoch: 6/10, step 329/574 completed (loss: 0.4729073941707611, acc: 0.8518518805503845)
[2024-12-12 02:49:28,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:28,424][root][INFO] - Training Epoch: 6/10, step 330/574 completed (loss: 0.16475242376327515, acc: 0.96875)
[2024-12-12 02:49:28,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:28,793][root][INFO] - Training Epoch: 6/10, step 331/574 completed (loss: 0.8468626737594604, acc: 0.725806474685669)
[2024-12-12 02:49:28,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:29,176][root][INFO] - Training Epoch: 6/10, step 332/574 completed (loss: 0.5542387366294861, acc: 0.8771929740905762)
[2024-12-12 02:49:29,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:29,481][root][INFO] - Training Epoch: 6/10, step 333/574 completed (loss: 0.7344707250595093, acc: 0.8125)
[2024-12-12 02:49:29,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:29,855][root][INFO] - Training Epoch: 6/10, step 334/574 completed (loss: 0.5517768859863281, acc: 0.8333333134651184)
[2024-12-12 02:49:29,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:30,226][root][INFO] - Training Epoch: 6/10, step 335/574 completed (loss: 0.2104768306016922, acc: 0.9473684430122375)
[2024-12-12 02:49:30,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:30,568][root][INFO] - Training Epoch: 6/10, step 336/574 completed (loss: 0.9244735240936279, acc: 0.7799999713897705)
[2024-12-12 02:49:30,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:30,937][root][INFO] - Training Epoch: 6/10, step 337/574 completed (loss: 1.408538818359375, acc: 0.6436781883239746)
[2024-12-12 02:49:31,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:31,366][root][INFO] - Training Epoch: 6/10, step 338/574 completed (loss: 1.6420248746871948, acc: 0.5531914830207825)
[2024-12-12 02:49:31,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:31,753][root][INFO] - Training Epoch: 6/10, step 339/574 completed (loss: 1.7875593900680542, acc: 0.5301204919815063)
[2024-12-12 02:49:31,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:32,118][root][INFO] - Training Epoch: 6/10, step 340/574 completed (loss: 0.04391728341579437, acc: 1.0)
[2024-12-12 02:49:32,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:32,460][root][INFO] - Training Epoch: 6/10, step 341/574 completed (loss: 0.288120299577713, acc: 0.8717948794364929)
[2024-12-12 02:49:32,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:32,856][root][INFO] - Training Epoch: 6/10, step 342/574 completed (loss: 1.4870752096176147, acc: 0.5783132314682007)
[2024-12-12 02:49:32,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:33,233][root][INFO] - Training Epoch: 6/10, step 343/574 completed (loss: 0.9174898266792297, acc: 0.7358490824699402)
[2024-12-12 02:49:33,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:33,613][root][INFO] - Training Epoch: 6/10, step 344/574 completed (loss: 0.9445686936378479, acc: 0.7721518874168396)
[2024-12-12 02:49:33,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:33,952][root][INFO] - Training Epoch: 6/10, step 345/574 completed (loss: 0.4844092130661011, acc: 0.9019607901573181)
[2024-12-12 02:49:34,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:34,325][root][INFO] - Training Epoch: 6/10, step 346/574 completed (loss: 1.2682325839996338, acc: 0.6268656849861145)
[2024-12-12 02:49:34,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:34,666][root][INFO] - Training Epoch: 6/10, step 347/574 completed (loss: 0.020530086010694504, acc: 1.0)
[2024-12-12 02:49:34,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:35,039][root][INFO] - Training Epoch: 6/10, step 348/574 completed (loss: 0.09580817818641663, acc: 1.0)
[2024-12-12 02:49:35,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:35,448][root][INFO] - Training Epoch: 6/10, step 349/574 completed (loss: 0.644536554813385, acc: 0.8055555820465088)
[2024-12-12 02:49:35,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:35,782][root][INFO] - Training Epoch: 6/10, step 350/574 completed (loss: 1.2502739429473877, acc: 0.6279069781303406)
[2024-12-12 02:49:35,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:36,126][root][INFO] - Training Epoch: 6/10, step 351/574 completed (loss: 0.5314297080039978, acc: 0.8974359035491943)
[2024-12-12 02:49:36,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:36,516][root][INFO] - Training Epoch: 6/10, step 352/574 completed (loss: 1.1275185346603394, acc: 0.6000000238418579)
[2024-12-12 02:49:36,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:36,833][root][INFO] - Training Epoch: 6/10, step 353/574 completed (loss: 0.04148859158158302, acc: 1.0)
[2024-12-12 02:49:36,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:37,163][root][INFO] - Training Epoch: 6/10, step 354/574 completed (loss: 0.7334862351417542, acc: 0.7307692170143127)
[2024-12-12 02:49:37,296][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:37,562][root][INFO] - Training Epoch: 6/10, step 355/574 completed (loss: 1.6781941652297974, acc: 0.49450549483299255)
[2024-12-12 02:49:37,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:38,060][root][INFO] - Training Epoch: 6/10, step 356/574 completed (loss: 1.3999472856521606, acc: 0.6000000238418579)
[2024-12-12 02:49:38,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:38,387][root][INFO] - Training Epoch: 6/10, step 357/574 completed (loss: 1.1185953617095947, acc: 0.6521739363670349)
[2024-12-12 02:49:38,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:38,712][root][INFO] - Training Epoch: 6/10, step 358/574 completed (loss: 0.8259099125862122, acc: 0.7551020383834839)
[2024-12-12 02:49:38,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:39,075][root][INFO] - Training Epoch: 6/10, step 359/574 completed (loss: 0.009905990213155746, acc: 1.0)
[2024-12-12 02:49:39,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:39,392][root][INFO] - Training Epoch: 6/10, step 360/574 completed (loss: 0.09598541259765625, acc: 0.9615384340286255)
[2024-12-12 02:49:39,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:39,775][root][INFO] - Training Epoch: 6/10, step 361/574 completed (loss: 0.6849793791770935, acc: 0.8292682766914368)
[2024-12-12 02:49:39,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:40,122][root][INFO] - Training Epoch: 6/10, step 362/574 completed (loss: 0.616534411907196, acc: 0.8666666746139526)
[2024-12-12 02:49:40,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:40,502][root][INFO] - Training Epoch: 6/10, step 363/574 completed (loss: 0.8884723782539368, acc: 0.75)
[2024-12-12 02:49:40,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:40,851][root][INFO] - Training Epoch: 6/10, step 364/574 completed (loss: 0.29239141941070557, acc: 0.8780487775802612)
[2024-12-12 02:49:40,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:41,165][root][INFO] - Training Epoch: 6/10, step 365/574 completed (loss: 0.3475188612937927, acc: 0.939393937587738)
[2024-12-12 02:49:41,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:41,483][root][INFO] - Training Epoch: 6/10, step 366/574 completed (loss: 0.008781882002949715, acc: 1.0)
[2024-12-12 02:49:41,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:41,816][root][INFO] - Training Epoch: 6/10, step 367/574 completed (loss: 0.09898877888917923, acc: 0.95652174949646)
[2024-12-12 02:49:41,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:42,134][root][INFO] - Training Epoch: 6/10, step 368/574 completed (loss: 0.19640518724918365, acc: 0.9285714030265808)
[2024-12-12 02:49:42,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:42,465][root][INFO] - Training Epoch: 6/10, step 369/574 completed (loss: 0.23593902587890625, acc: 0.9375)
[2024-12-12 02:49:42,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:43,066][root][INFO] - Training Epoch: 6/10, step 370/574 completed (loss: 1.6390066146850586, acc: 0.5515151619911194)
[2024-12-12 02:49:43,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:43,927][root][INFO] - Training Epoch: 6/10, step 371/574 completed (loss: 1.1146818399429321, acc: 0.6603773832321167)
[2024-12-12 02:49:44,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:44,257][root][INFO] - Training Epoch: 6/10, step 372/574 completed (loss: 0.8243891596794128, acc: 0.7444444298744202)
[2024-12-12 02:49:44,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:44,624][root][INFO] - Training Epoch: 6/10, step 373/574 completed (loss: 0.5137385725975037, acc: 0.8214285969734192)
[2024-12-12 02:49:44,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:45,006][root][INFO] - Training Epoch: 6/10, step 374/574 completed (loss: 0.4548995792865753, acc: 0.8285714387893677)
[2024-12-12 02:49:45,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:45,398][root][INFO] - Training Epoch: 6/10, step 375/574 completed (loss: 0.012320122681558132, acc: 1.0)
[2024-12-12 02:49:45,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:45,772][root][INFO] - Training Epoch: 6/10, step 376/574 completed (loss: 0.05722843483090401, acc: 1.0)
[2024-12-12 02:49:45,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:46,156][root][INFO] - Training Epoch: 6/10, step 377/574 completed (loss: 0.368326336145401, acc: 0.875)
[2024-12-12 02:49:46,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:46,537][root][INFO] - Training Epoch: 6/10, step 378/574 completed (loss: 0.7666406631469727, acc: 0.7684210538864136)
[2024-12-12 02:49:46,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:47,125][root][INFO] - Training Epoch: 6/10, step 379/574 completed (loss: 1.1645581722259521, acc: 0.688622772693634)
[2024-12-12 02:49:47,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:47,539][root][INFO] - Training Epoch: 6/10, step 380/574 completed (loss: 0.9614347219467163, acc: 0.7443609237670898)
[2024-12-12 02:49:47,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:48,784][root][INFO] - Training Epoch: 6/10, step 381/574 completed (loss: 1.1326416730880737, acc: 0.6898396015167236)
[2024-12-12 02:49:48,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:49,350][root][INFO] - Training Epoch: 6/10, step 382/574 completed (loss: 0.7611212730407715, acc: 0.792792797088623)
[2024-12-12 02:49:49,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:49,720][root][INFO] - Training Epoch: 6/10, step 383/574 completed (loss: 0.4343700408935547, acc: 0.9285714030265808)
[2024-12-12 02:49:49,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:50,044][root][INFO] - Training Epoch: 6/10, step 384/574 completed (loss: 0.11478967219591141, acc: 0.9285714030265808)
[2024-12-12 02:49:50,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:50,310][root][INFO] - Training Epoch: 6/10, step 385/574 completed (loss: 0.07985576242208481, acc: 0.96875)
[2024-12-12 02:49:50,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:50,616][root][INFO] - Training Epoch: 6/10, step 386/574 completed (loss: 0.05333395302295685, acc: 1.0)
[2024-12-12 02:49:50,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:50,942][root][INFO] - Training Epoch: 6/10, step 387/574 completed (loss: 0.06884845346212387, acc: 0.9736841917037964)
[2024-12-12 02:49:51,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:51,283][root][INFO] - Training Epoch: 6/10, step 388/574 completed (loss: 0.013701730407774448, acc: 1.0)
[2024-12-12 02:49:51,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:51,598][root][INFO] - Training Epoch: 6/10, step 389/574 completed (loss: 0.054031919687986374, acc: 1.0)
[2024-12-12 02:49:51,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:51,977][root][INFO] - Training Epoch: 6/10, step 390/574 completed (loss: 0.15130923688411713, acc: 0.9523809552192688)
[2024-12-12 02:49:52,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:52,350][root][INFO] - Training Epoch: 6/10, step 391/574 completed (loss: 1.2238484621047974, acc: 0.6666666865348816)
[2024-12-12 02:49:52,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:52,706][root][INFO] - Training Epoch: 6/10, step 392/574 completed (loss: 1.4194012880325317, acc: 0.6407766938209534)
[2024-12-12 02:49:52,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:53,241][root][INFO] - Training Epoch: 6/10, step 393/574 completed (loss: 1.481429100036621, acc: 0.6470588445663452)
[2024-12-12 02:49:53,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:53,642][root][INFO] - Training Epoch: 6/10, step 394/574 completed (loss: 1.510669231414795, acc: 0.5733333230018616)
[2024-12-12 02:49:53,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:54,051][root][INFO] - Training Epoch: 6/10, step 395/574 completed (loss: 1.7063649892807007, acc: 0.5902777910232544)
[2024-12-12 02:49:54,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:54,409][root][INFO] - Training Epoch: 6/10, step 396/574 completed (loss: 0.7720718383789062, acc: 0.7674418687820435)
[2024-12-12 02:49:54,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:54,747][root][INFO] - Training Epoch: 6/10, step 397/574 completed (loss: 0.15842404961585999, acc: 0.9583333134651184)
[2024-12-12 02:49:54,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:55,107][root][INFO] - Training Epoch: 6/10, step 398/574 completed (loss: 0.6070061326026917, acc: 0.8372092843055725)
[2024-12-12 02:49:55,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:55,469][root][INFO] - Training Epoch: 6/10, step 399/574 completed (loss: 0.27725011110305786, acc: 0.8399999737739563)
[2024-12-12 02:49:55,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:56,003][root][INFO] - Training Epoch: 6/10, step 400/574 completed (loss: 0.8565808534622192, acc: 0.7647058963775635)
[2024-12-12 02:49:56,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:56,370][root][INFO] - Training Epoch: 6/10, step 401/574 completed (loss: 0.8313557505607605, acc: 0.7599999904632568)
[2024-12-12 02:49:56,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:56,750][root][INFO] - Training Epoch: 6/10, step 402/574 completed (loss: 0.4275134801864624, acc: 0.8181818127632141)
[2024-12-12 02:49:56,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:57,071][root][INFO] - Training Epoch: 6/10, step 403/574 completed (loss: 0.1887984573841095, acc: 0.939393937587738)
[2024-12-12 02:49:57,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:57,372][root][INFO] - Training Epoch: 6/10, step 404/574 completed (loss: 0.03385311737656593, acc: 1.0)
[2024-12-12 02:49:57,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:57,701][root][INFO] - Training Epoch: 6/10, step 405/574 completed (loss: 0.08748307824134827, acc: 0.9629629850387573)
[2024-12-12 02:49:57,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:58,051][root][INFO] - Training Epoch: 6/10, step 406/574 completed (loss: 0.07396315783262253, acc: 0.9599999785423279)
[2024-12-12 02:49:58,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:58,393][root][INFO] - Training Epoch: 6/10, step 407/574 completed (loss: 0.18110798299312592, acc: 0.9444444179534912)
[2024-12-12 02:49:58,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:58,715][root][INFO] - Training Epoch: 6/10, step 408/574 completed (loss: 0.1758049875497818, acc: 0.8888888955116272)
[2024-12-12 02:49:58,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:59,031][root][INFO] - Training Epoch: 6/10, step 409/574 completed (loss: 0.09940849244594574, acc: 0.9615384340286255)
[2024-12-12 02:49:59,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:59,401][root][INFO] - Training Epoch: 6/10, step 410/574 completed (loss: 0.3780076205730438, acc: 0.9137930870056152)
[2024-12-12 02:49:59,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:49:59,739][root][INFO] - Training Epoch: 6/10, step 411/574 completed (loss: 0.11719483137130737, acc: 0.9642857313156128)
[2024-12-12 02:49:59,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:00,123][root][INFO] - Training Epoch: 6/10, step 412/574 completed (loss: 0.22954997420310974, acc: 0.9666666388511658)
[2024-12-12 02:50:00,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:00,491][root][INFO] - Training Epoch: 6/10, step 413/574 completed (loss: 0.13444717228412628, acc: 0.9696969985961914)
[2024-12-12 02:50:00,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:00,883][root][INFO] - Training Epoch: 6/10, step 414/574 completed (loss: 0.07271593064069748, acc: 1.0)
[2024-12-12 02:50:00,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:01,217][root][INFO] - Training Epoch: 6/10, step 415/574 completed (loss: 0.7577855587005615, acc: 0.7450980544090271)
[2024-12-12 02:50:01,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:01,580][root][INFO] - Training Epoch: 6/10, step 416/574 completed (loss: 0.26000136137008667, acc: 0.9230769276618958)
[2024-12-12 02:50:01,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:01,949][root][INFO] - Training Epoch: 6/10, step 417/574 completed (loss: 0.2737019658088684, acc: 0.9444444179534912)
[2024-12-12 02:50:02,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:02,293][root][INFO] - Training Epoch: 6/10, step 418/574 completed (loss: 0.587428867816925, acc: 0.800000011920929)
[2024-12-12 02:50:02,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:03,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:03,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:03,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:04,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:04,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:05,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:05,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:05,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:06,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:06,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:06,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:07,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:07,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:07,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:08,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:08,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:09,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:09,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:09,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:09,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:10,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:10,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:10,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:11,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:11,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:11,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:12,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:12,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:12,893][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:13,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:13,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:13,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:14,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:14,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:14,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:15,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:15,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:16,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:16,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:16,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:17,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:17,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:17,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:18,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:18,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:18,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:19,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:19,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:19,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:20,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:20,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:20,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:21,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:21,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:21,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:22,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:22,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:22,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:23,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:23,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:23,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:24,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:24,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:25,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:25,457][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:25,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:26,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:26,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:26,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:27,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:27,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:27,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:28,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:28,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:28,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:28,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:29,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:29,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:29,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:30,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:30,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:30,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:31,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:31,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:32,196][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.1930, device='cuda:0') eval_epoch_loss=tensor(1.4334, device='cuda:0') eval_epoch_acc=tensor(0.6601, device='cuda:0')
[2024-12-12 02:50:32,197][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:50:32,197][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:50:32,389][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_6_step_419_loss_1.4334280490875244/model.pt
[2024-12-12 02:50:32,392][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:50:32,393][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 6 is 0.6601085662841797
[2024-12-12 02:50:32,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:32,823][root][INFO] - Training Epoch: 6/10, step 419/574 completed (loss: 0.16406069695949554, acc: 0.949999988079071)
[2024-12-12 02:50:32,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:33,201][root][INFO] - Training Epoch: 6/10, step 420/574 completed (loss: 0.02711520902812481, acc: 1.0)
[2024-12-12 02:50:33,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:33,536][root][INFO] - Training Epoch: 6/10, step 421/574 completed (loss: 0.2518845498561859, acc: 0.9333333373069763)
[2024-12-12 02:50:33,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:33,849][root][INFO] - Training Epoch: 6/10, step 422/574 completed (loss: 0.141778364777565, acc: 0.96875)
[2024-12-12 02:50:33,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:34,182][root][INFO] - Training Epoch: 6/10, step 423/574 completed (loss: 0.4342203438282013, acc: 0.9166666865348816)
[2024-12-12 02:50:34,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:34,523][root][INFO] - Training Epoch: 6/10, step 424/574 completed (loss: 0.24847330152988434, acc: 0.8888888955116272)
[2024-12-12 02:50:34,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:34,911][root][INFO] - Training Epoch: 6/10, step 425/574 completed (loss: 0.14433608949184418, acc: 0.9696969985961914)
[2024-12-12 02:50:35,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:35,268][root][INFO] - Training Epoch: 6/10, step 426/574 completed (loss: 0.019619083032011986, acc: 1.0)
[2024-12-12 02:50:35,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:35,602][root][INFO] - Training Epoch: 6/10, step 427/574 completed (loss: 0.16912837326526642, acc: 0.9459459185600281)
[2024-12-12 02:50:35,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:35,964][root][INFO] - Training Epoch: 6/10, step 428/574 completed (loss: 0.11931867897510529, acc: 0.9629629850387573)
[2024-12-12 02:50:36,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:36,313][root][INFO] - Training Epoch: 6/10, step 429/574 completed (loss: 0.03998170047998428, acc: 1.0)
[2024-12-12 02:50:36,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:36,688][root][INFO] - Training Epoch: 6/10, step 430/574 completed (loss: 0.015369068831205368, acc: 1.0)
[2024-12-12 02:50:36,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:37,031][root][INFO] - Training Epoch: 6/10, step 431/574 completed (loss: 0.006134497933089733, acc: 1.0)
[2024-12-12 02:50:37,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:37,354][root][INFO] - Training Epoch: 6/10, step 432/574 completed (loss: 0.049372147768735886, acc: 1.0)
[2024-12-12 02:50:37,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:37,711][root][INFO] - Training Epoch: 6/10, step 433/574 completed (loss: 0.45916905999183655, acc: 0.8888888955116272)
[2024-12-12 02:50:37,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:38,054][root][INFO] - Training Epoch: 6/10, step 434/574 completed (loss: 0.1325768381357193, acc: 0.9599999785423279)
[2024-12-12 02:50:38,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:38,401][root][INFO] - Training Epoch: 6/10, step 435/574 completed (loss: 0.3060876131057739, acc: 0.8181818127632141)
[2024-12-12 02:50:38,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:38,758][root][INFO] - Training Epoch: 6/10, step 436/574 completed (loss: 0.3181615471839905, acc: 0.8611111044883728)
[2024-12-12 02:50:38,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:39,126][root][INFO] - Training Epoch: 6/10, step 437/574 completed (loss: 0.5327188968658447, acc: 0.8409090638160706)
[2024-12-12 02:50:39,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:39,412][root][INFO] - Training Epoch: 6/10, step 438/574 completed (loss: 0.0694059506058693, acc: 0.9523809552192688)
[2024-12-12 02:50:39,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:39,718][root][INFO] - Training Epoch: 6/10, step 439/574 completed (loss: 0.37995877861976624, acc: 0.8974359035491943)
[2024-12-12 02:50:39,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:40,194][root][INFO] - Training Epoch: 6/10, step 440/574 completed (loss: 1.0654648542404175, acc: 0.7121211886405945)
[2024-12-12 02:50:40,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:40,942][root][INFO] - Training Epoch: 6/10, step 441/574 completed (loss: 1.800667405128479, acc: 0.5120000243186951)
[2024-12-12 02:50:41,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:41,338][root][INFO] - Training Epoch: 6/10, step 442/574 completed (loss: 1.5167995691299438, acc: 0.5564516186714172)
[2024-12-12 02:50:41,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:41,989][root][INFO] - Training Epoch: 6/10, step 443/574 completed (loss: 1.5932238101959229, acc: 0.5621890425682068)
[2024-12-12 02:50:42,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:42,280][root][INFO] - Training Epoch: 6/10, step 444/574 completed (loss: 0.5079991221427917, acc: 0.8867924809455872)
[2024-12-12 02:50:42,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:42,694][root][INFO] - Training Epoch: 6/10, step 445/574 completed (loss: 0.4843599498271942, acc: 0.8863636255264282)
[2024-12-12 02:50:42,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:42,972][root][INFO] - Training Epoch: 6/10, step 446/574 completed (loss: 0.32524052262306213, acc: 0.9130434989929199)
[2024-12-12 02:50:43,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:43,278][root][INFO] - Training Epoch: 6/10, step 447/574 completed (loss: 0.09756874293088913, acc: 0.9615384340286255)
[2024-12-12 02:50:43,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:43,638][root][INFO] - Training Epoch: 6/10, step 448/574 completed (loss: 0.21573278307914734, acc: 0.9285714030265808)
[2024-12-12 02:50:43,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:43,954][root][INFO] - Training Epoch: 6/10, step 449/574 completed (loss: 0.560826301574707, acc: 0.8208954930305481)
[2024-12-12 02:50:44,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:44,275][root][INFO] - Training Epoch: 6/10, step 450/574 completed (loss: 0.5228409171104431, acc: 0.8472222089767456)
[2024-12-12 02:50:44,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:44,640][root][INFO] - Training Epoch: 6/10, step 451/574 completed (loss: 0.6897069811820984, acc: 0.8152173757553101)
[2024-12-12 02:50:44,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:44,968][root][INFO] - Training Epoch: 6/10, step 452/574 completed (loss: 0.862883448600769, acc: 0.7179487347602844)
[2024-12-12 02:50:45,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:45,285][root][INFO] - Training Epoch: 6/10, step 453/574 completed (loss: 0.863120436668396, acc: 0.75)
[2024-12-12 02:50:45,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:45,652][root][INFO] - Training Epoch: 6/10, step 454/574 completed (loss: 0.6325193643569946, acc: 0.8163265585899353)
[2024-12-12 02:50:45,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:46,047][root][INFO] - Training Epoch: 6/10, step 455/574 completed (loss: 0.10882484912872314, acc: 1.0)
[2024-12-12 02:50:46,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:46,423][root][INFO] - Training Epoch: 6/10, step 456/574 completed (loss: 1.46841299533844, acc: 0.5979381203651428)
[2024-12-12 02:50:46,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:46,781][root][INFO] - Training Epoch: 6/10, step 457/574 completed (loss: 0.6326299905776978, acc: 0.8714285492897034)
[2024-12-12 02:50:46,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:47,159][root][INFO] - Training Epoch: 6/10, step 458/574 completed (loss: 1.3159228563308716, acc: 0.6279069781303406)
[2024-12-12 02:50:47,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:47,511][root][INFO] - Training Epoch: 6/10, step 459/574 completed (loss: 0.7612428069114685, acc: 0.7142857313156128)
[2024-12-12 02:50:47,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:47,870][root][INFO] - Training Epoch: 6/10, step 460/574 completed (loss: 1.1248728036880493, acc: 0.7160493731498718)
[2024-12-12 02:50:47,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:48,243][root][INFO] - Training Epoch: 6/10, step 461/574 completed (loss: 0.5152801275253296, acc: 0.8611111044883728)
[2024-12-12 02:50:48,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:48,591][root][INFO] - Training Epoch: 6/10, step 462/574 completed (loss: 0.12489322572946548, acc: 0.96875)
[2024-12-12 02:50:48,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:48,951][root][INFO] - Training Epoch: 6/10, step 463/574 completed (loss: 0.03806007653474808, acc: 1.0)
[2024-12-12 02:50:49,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:49,324][root][INFO] - Training Epoch: 6/10, step 464/574 completed (loss: 0.24309676885604858, acc: 0.9347826242446899)
[2024-12-12 02:50:49,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:49,675][root][INFO] - Training Epoch: 6/10, step 465/574 completed (loss: 0.6508142352104187, acc: 0.773809552192688)
[2024-12-12 02:50:49,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:50,025][root][INFO] - Training Epoch: 6/10, step 466/574 completed (loss: 1.0402143001556396, acc: 0.7108433842658997)
[2024-12-12 02:50:50,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:50,393][root][INFO] - Training Epoch: 6/10, step 467/574 completed (loss: 0.9195529818534851, acc: 0.7477477192878723)
[2024-12-12 02:50:50,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:50,721][root][INFO] - Training Epoch: 6/10, step 468/574 completed (loss: 1.0815969705581665, acc: 0.6990291476249695)
[2024-12-12 02:50:50,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:51,094][root][INFO] - Training Epoch: 6/10, step 469/574 completed (loss: 0.900501549243927, acc: 0.7560975551605225)
[2024-12-12 02:50:51,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:51,435][root][INFO] - Training Epoch: 6/10, step 470/574 completed (loss: 0.2094244360923767, acc: 0.9166666865348816)
[2024-12-12 02:50:51,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:51,729][root][INFO] - Training Epoch: 6/10, step 471/574 completed (loss: 0.09379071742296219, acc: 0.9642857313156128)
[2024-12-12 02:50:51,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:52,122][root][INFO] - Training Epoch: 6/10, step 472/574 completed (loss: 1.4630218744277954, acc: 0.5980392098426819)
[2024-12-12 02:50:52,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:52,507][root][INFO] - Training Epoch: 6/10, step 473/574 completed (loss: 1.598663568496704, acc: 0.5414847135543823)
[2024-12-12 02:50:52,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:52,813][root][INFO] - Training Epoch: 6/10, step 474/574 completed (loss: 1.047575831413269, acc: 0.6666666865348816)
[2024-12-12 02:50:52,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:53,118][root][INFO] - Training Epoch: 6/10, step 475/574 completed (loss: 1.3774253129959106, acc: 0.6196318864822388)
[2024-12-12 02:50:53,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:53,407][root][INFO] - Training Epoch: 6/10, step 476/574 completed (loss: 1.3694283962249756, acc: 0.6258992552757263)
[2024-12-12 02:50:53,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:53,750][root][INFO] - Training Epoch: 6/10, step 477/574 completed (loss: 1.5496408939361572, acc: 0.5879396796226501)
[2024-12-12 02:50:53,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:54,063][root][INFO] - Training Epoch: 6/10, step 478/574 completed (loss: 0.5244213342666626, acc: 0.8055555820465088)
[2024-12-12 02:50:54,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:54,412][root][INFO] - Training Epoch: 6/10, step 479/574 completed (loss: 0.18349547684192657, acc: 0.9696969985961914)
[2024-12-12 02:50:54,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:54,755][root][INFO] - Training Epoch: 6/10, step 480/574 completed (loss: 0.1491577923297882, acc: 0.9629629850387573)
[2024-12-12 02:50:54,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:55,103][root][INFO] - Training Epoch: 6/10, step 481/574 completed (loss: 0.3646736741065979, acc: 0.949999988079071)
[2024-12-12 02:50:55,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:55,404][root][INFO] - Training Epoch: 6/10, step 482/574 completed (loss: 0.1548803746700287, acc: 0.949999988079071)
[2024-12-12 02:50:55,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:55,774][root][INFO] - Training Epoch: 6/10, step 483/574 completed (loss: 0.8124876618385315, acc: 0.7241379022598267)
[2024-12-12 02:50:55,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:56,149][root][INFO] - Training Epoch: 6/10, step 484/574 completed (loss: 0.06515678763389587, acc: 1.0)
[2024-12-12 02:50:56,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:56,477][root][INFO] - Training Epoch: 6/10, step 485/574 completed (loss: 0.09181945025920868, acc: 0.9473684430122375)
[2024-12-12 02:50:56,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:56,831][root][INFO] - Training Epoch: 6/10, step 486/574 completed (loss: 0.43858030438423157, acc: 0.8888888955116272)
[2024-12-12 02:50:56,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:57,184][root][INFO] - Training Epoch: 6/10, step 487/574 completed (loss: 0.5517529249191284, acc: 0.8095238208770752)
[2024-12-12 02:50:57,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:57,544][root][INFO] - Training Epoch: 6/10, step 488/574 completed (loss: 0.09702892601490021, acc: 0.9545454382896423)
[2024-12-12 02:50:57,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:57,914][root][INFO] - Training Epoch: 6/10, step 489/574 completed (loss: 1.3447591066360474, acc: 0.6153846383094788)
[2024-12-12 02:50:58,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:58,286][root][INFO] - Training Epoch: 6/10, step 490/574 completed (loss: 0.4136742949485779, acc: 0.800000011920929)
[2024-12-12 02:50:58,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:58,635][root][INFO] - Training Epoch: 6/10, step 491/574 completed (loss: 0.19795647263526917, acc: 0.931034505367279)
[2024-12-12 02:50:58,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:59,006][root][INFO] - Training Epoch: 6/10, step 492/574 completed (loss: 0.7366094589233398, acc: 0.7843137383460999)
[2024-12-12 02:50:59,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:59,375][root][INFO] - Training Epoch: 6/10, step 493/574 completed (loss: 0.38140198588371277, acc: 0.8620689511299133)
[2024-12-12 02:50:59,476][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:59,703][root][INFO] - Training Epoch: 6/10, step 494/574 completed (loss: 0.38147178292274475, acc: 0.8947368264198303)
[2024-12-12 02:50:59,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:50:59,993][root][INFO] - Training Epoch: 6/10, step 495/574 completed (loss: 0.5598134994506836, acc: 0.8421052694320679)
[2024-12-12 02:51:00,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:00,359][root][INFO] - Training Epoch: 6/10, step 496/574 completed (loss: 1.3208249807357788, acc: 0.6517857313156128)
[2024-12-12 02:51:00,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:00,771][root][INFO] - Training Epoch: 6/10, step 497/574 completed (loss: 1.0944714546203613, acc: 0.7078651785850525)
[2024-12-12 02:51:00,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:01,133][root][INFO] - Training Epoch: 6/10, step 498/574 completed (loss: 1.2679743766784668, acc: 0.6853932738304138)
[2024-12-12 02:51:01,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:01,506][root][INFO] - Training Epoch: 6/10, step 499/574 completed (loss: 1.816995620727539, acc: 0.5177304744720459)
[2024-12-12 02:51:01,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:01,887][root][INFO] - Training Epoch: 6/10, step 500/574 completed (loss: 1.6905262470245361, acc: 0.52173912525177)
[2024-12-12 02:51:01,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:02,215][root][INFO] - Training Epoch: 6/10, step 501/574 completed (loss: 0.05000258609652519, acc: 1.0)
[2024-12-12 02:51:02,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:02,523][root][INFO] - Training Epoch: 6/10, step 502/574 completed (loss: 0.04134637117385864, acc: 1.0)
[2024-12-12 02:51:02,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:02,832][root][INFO] - Training Epoch: 6/10, step 503/574 completed (loss: 0.06467436999082565, acc: 1.0)
[2024-12-12 02:51:02,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:03,196][root][INFO] - Training Epoch: 6/10, step 504/574 completed (loss: 0.1799098402261734, acc: 0.9259259104728699)
[2024-12-12 02:51:03,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:03,576][root][INFO] - Training Epoch: 6/10, step 505/574 completed (loss: 0.28960439562797546, acc: 0.9245283007621765)
[2024-12-12 02:51:03,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:03,945][root][INFO] - Training Epoch: 6/10, step 506/574 completed (loss: 0.24921388924121857, acc: 0.931034505367279)
[2024-12-12 02:51:04,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:04,539][root][INFO] - Training Epoch: 6/10, step 507/574 completed (loss: 1.4448953866958618, acc: 0.6126126050949097)
[2024-12-12 02:51:04,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:04,983][root][INFO] - Training Epoch: 6/10, step 508/574 completed (loss: 0.9107128977775574, acc: 0.7887324094772339)
[2024-12-12 02:51:05,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:05,294][root][INFO] - Training Epoch: 6/10, step 509/574 completed (loss: 0.2650109529495239, acc: 0.8999999761581421)
[2024-12-12 02:51:05,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:05,637][root][INFO] - Training Epoch: 6/10, step 510/574 completed (loss: 0.08771421015262604, acc: 0.9666666388511658)
[2024-12-12 02:51:05,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:05,954][root][INFO] - Training Epoch: 6/10, step 511/574 completed (loss: 0.2004701942205429, acc: 0.9230769276618958)
[2024-12-12 02:51:07,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:08,650][root][INFO] - Training Epoch: 6/10, step 512/574 completed (loss: 1.6262481212615967, acc: 0.5785714387893677)
[2024-12-12 02:51:08,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:09,412][root][INFO] - Training Epoch: 6/10, step 513/574 completed (loss: 1.3178993463516235, acc: 0.6190476417541504)
[2024-12-12 02:51:09,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:09,758][root][INFO] - Training Epoch: 6/10, step 514/574 completed (loss: 0.27840277552604675, acc: 0.9642857313156128)
[2024-12-12 02:51:09,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:10,122][root][INFO] - Training Epoch: 6/10, step 515/574 completed (loss: 0.8728399872779846, acc: 0.800000011920929)
[2024-12-12 02:51:10,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:10,813][root][INFO] - Training Epoch: 6/10, step 516/574 completed (loss: 0.9042528867721558, acc: 0.75)
[2024-12-12 02:51:10,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:11,126][root][INFO] - Training Epoch: 6/10, step 517/574 completed (loss: 0.0064450716599822044, acc: 1.0)
[2024-12-12 02:51:11,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:11,429][root][INFO] - Training Epoch: 6/10, step 518/574 completed (loss: 0.1005626693367958, acc: 0.9677419066429138)
[2024-12-12 02:51:11,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:11,716][root][INFO] - Training Epoch: 6/10, step 519/574 completed (loss: 0.43122243881225586, acc: 0.8500000238418579)
[2024-12-12 02:51:11,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:12,021][root][INFO] - Training Epoch: 6/10, step 520/574 completed (loss: 0.1553633064031601, acc: 0.9259259104728699)
[2024-12-12 02:51:12,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:12,999][root][INFO] - Training Epoch: 6/10, step 521/574 completed (loss: 1.7329189777374268, acc: 0.508474588394165)
[2024-12-12 02:51:13,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:13,341][root][INFO] - Training Epoch: 6/10, step 522/574 completed (loss: 1.137820839881897, acc: 0.7238805890083313)
[2024-12-12 02:51:13,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:13,702][root][INFO] - Training Epoch: 6/10, step 523/574 completed (loss: 1.3276746273040771, acc: 0.6058394312858582)
[2024-12-12 02:51:13,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:14,262][root][INFO] - Training Epoch: 6/10, step 524/574 completed (loss: 1.451592206954956, acc: 0.5799999833106995)
[2024-12-12 02:51:14,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:14,576][root][INFO] - Training Epoch: 6/10, step 525/574 completed (loss: 0.3613591492176056, acc: 0.8518518805503845)
[2024-12-12 02:51:14,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:14,889][root][INFO] - Training Epoch: 6/10, step 526/574 completed (loss: 0.46683600544929504, acc: 0.8269230723381042)
[2024-12-12 02:51:14,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:15,216][root][INFO] - Training Epoch: 6/10, step 527/574 completed (loss: 0.14982382953166962, acc: 1.0)
[2024-12-12 02:51:15,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:15,570][root][INFO] - Training Epoch: 6/10, step 528/574 completed (loss: 1.317110538482666, acc: 0.6393442749977112)
[2024-12-12 02:51:15,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:15,867][root][INFO] - Training Epoch: 6/10, step 529/574 completed (loss: 0.77852463722229, acc: 0.7966101765632629)
[2024-12-12 02:51:15,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:16,219][root][INFO] - Training Epoch: 6/10, step 530/574 completed (loss: 1.3870314359664917, acc: 0.6511628031730652)
[2024-12-12 02:51:16,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:16,597][root][INFO] - Training Epoch: 6/10, step 531/574 completed (loss: 0.876634418964386, acc: 0.7727272510528564)
[2024-12-12 02:51:16,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:16,960][root][INFO] - Training Epoch: 6/10, step 532/574 completed (loss: 0.9781505465507507, acc: 0.7169811129570007)
[2024-12-12 02:51:17,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:17,322][root][INFO] - Training Epoch: 6/10, step 533/574 completed (loss: 0.6113253235816956, acc: 0.8409090638160706)
[2024-12-12 02:51:17,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:17,676][root][INFO] - Training Epoch: 6/10, step 534/574 completed (loss: 0.34314286708831787, acc: 0.8399999737739563)
[2024-12-12 02:51:17,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:18,043][root][INFO] - Training Epoch: 6/10, step 535/574 completed (loss: 0.13149121403694153, acc: 1.0)
[2024-12-12 02:51:18,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:18,371][root][INFO] - Training Epoch: 6/10, step 536/574 completed (loss: 0.19927896559238434, acc: 0.9090909361839294)
[2024-12-12 02:51:18,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:18,763][root][INFO] - Training Epoch: 6/10, step 537/574 completed (loss: 0.6224234700202942, acc: 0.8307692408561707)
[2024-12-12 02:51:18,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:19,162][root][INFO] - Training Epoch: 6/10, step 538/574 completed (loss: 0.7118583917617798, acc: 0.765625)
[2024-12-12 02:51:19,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:19,555][root][INFO] - Training Epoch: 6/10, step 539/574 completed (loss: 0.38507434725761414, acc: 0.9375)
[2024-12-12 02:51:19,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:19,898][root][INFO] - Training Epoch: 6/10, step 540/574 completed (loss: 0.37667202949523926, acc: 0.9090909361839294)
[2024-12-12 02:51:19,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:20,199][root][INFO] - Training Epoch: 6/10, step 541/574 completed (loss: 0.24265672266483307, acc: 0.9375)
[2024-12-12 02:51:20,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:20,491][root][INFO] - Training Epoch: 6/10, step 542/574 completed (loss: 0.06190832704305649, acc: 1.0)
[2024-12-12 02:51:20,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:20,833][root][INFO] - Training Epoch: 6/10, step 543/574 completed (loss: 0.14595824480056763, acc: 0.95652174949646)
[2024-12-12 02:51:20,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:21,204][root][INFO] - Training Epoch: 6/10, step 544/574 completed (loss: 0.11286256462335587, acc: 1.0)
[2024-12-12 02:51:21,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:21,605][root][INFO] - Training Epoch: 6/10, step 545/574 completed (loss: 0.5544496774673462, acc: 0.8292682766914368)
[2024-12-12 02:51:21,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:21,981][root][INFO] - Training Epoch: 6/10, step 546/574 completed (loss: 0.18913228809833527, acc: 0.8857142925262451)
[2024-12-12 02:51:22,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:22,320][root][INFO] - Training Epoch: 6/10, step 547/574 completed (loss: 0.22085949778556824, acc: 0.9473684430122375)
[2024-12-12 02:51:22,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:22,612][root][INFO] - Training Epoch: 6/10, step 548/574 completed (loss: 0.14689476788043976, acc: 0.9677419066429138)
[2024-12-12 02:51:22,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:22,851][root][INFO] - Training Epoch: 6/10, step 549/574 completed (loss: 0.018370246514678, acc: 1.0)
[2024-12-12 02:51:22,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:23,142][root][INFO] - Training Epoch: 6/10, step 550/574 completed (loss: 0.12649454176425934, acc: 0.9696969985961914)
[2024-12-12 02:51:23,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:23,423][root][INFO] - Training Epoch: 6/10, step 551/574 completed (loss: 0.2384430468082428, acc: 0.8999999761581421)
[2024-12-12 02:51:23,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:23,778][root][INFO] - Training Epoch: 6/10, step 552/574 completed (loss: 0.23833294212818146, acc: 0.9285714030265808)
[2024-12-12 02:51:23,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:24,116][root][INFO] - Training Epoch: 6/10, step 553/574 completed (loss: 1.0717705488204956, acc: 0.6861313581466675)
[2024-12-12 02:51:24,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:24,497][root][INFO] - Training Epoch: 6/10, step 554/574 completed (loss: 0.9988908171653748, acc: 0.7172414064407349)
[2024-12-12 02:51:24,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:24,870][root][INFO] - Training Epoch: 6/10, step 555/574 completed (loss: 1.4974805116653442, acc: 0.550000011920929)
[2024-12-12 02:51:24,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:25,234][root][INFO] - Training Epoch: 6/10, step 556/574 completed (loss: 1.255232334136963, acc: 0.6357616186141968)
[2024-12-12 02:51:25,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:25,556][root][INFO] - Training Epoch: 6/10, step 557/574 completed (loss: 0.6569795608520508, acc: 0.8205128312110901)
[2024-12-12 02:51:25,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:25,890][root][INFO] - Training Epoch: 6/10, step 558/574 completed (loss: 0.23976175487041473, acc: 0.9599999785423279)
[2024-12-12 02:51:25,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:26,222][root][INFO] - Training Epoch: 6/10, step 559/574 completed (loss: 0.052259866148233414, acc: 1.0)
[2024-12-12 02:51:26,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:26,574][root][INFO] - Training Epoch: 6/10, step 560/574 completed (loss: 0.013981552794575691, acc: 1.0)
[2024-12-12 02:51:26,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:26,930][root][INFO] - Training Epoch: 6/10, step 561/574 completed (loss: 0.5523985028266907, acc: 0.8717948794364929)
[2024-12-12 02:51:27,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:27,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:28,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:28,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:28,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:29,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:29,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:30,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:30,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:30,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:31,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:31,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:31,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:32,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:32,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:33,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:33,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:33,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:34,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:34,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:34,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:35,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:35,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:35,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:35,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:36,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:36,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:37,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:37,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:37,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:38,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:38,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:38,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:39,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:39,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:39,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:40,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:40,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:40,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:41,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:41,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:42,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:42,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:42,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:43,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:43,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:43,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:44,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:44,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:44,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:45,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:45,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:45,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:46,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:46,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:46,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:47,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:47,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:47,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:48,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:48,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:48,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:49,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:49,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:50,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:50,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:50,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:51,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:51,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:52,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:52,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:52,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:53,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:53,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:53,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:54,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:54,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:54,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:55,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:55,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:55,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:56,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:56,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:57,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:57,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:58,083][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.4745, device='cuda:0') eval_epoch_loss=tensor(1.4984, device='cuda:0') eval_epoch_acc=tensor(0.6569, device='cuda:0')
[2024-12-12 02:51:58,084][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:51:58,085][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:51:58,278][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_6_step_562_loss_1.4984021186828613/model.pt
[2024-12-12 02:51:58,281][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:51:58,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:58,697][root][INFO] - Training Epoch: 6/10, step 562/574 completed (loss: 0.8364653587341309, acc: 0.6888889074325562)
[2024-12-12 02:51:58,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:59,035][root][INFO] - Training Epoch: 6/10, step 563/574 completed (loss: 0.4567374587059021, acc: 0.8571428656578064)
[2024-12-12 02:51:59,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:59,400][root][INFO] - Training Epoch: 6/10, step 564/574 completed (loss: 0.3672961890697479, acc: 0.875)
[2024-12-12 02:51:59,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:51:59,696][root][INFO] - Training Epoch: 6/10, step 565/574 completed (loss: 0.4227370023727417, acc: 0.8103448152542114)
[2024-12-12 02:51:59,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:00,063][root][INFO] - Training Epoch: 6/10, step 566/574 completed (loss: 0.7292268872261047, acc: 0.773809552192688)
[2024-12-12 02:52:00,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:00,454][root][INFO] - Training Epoch: 6/10, step 567/574 completed (loss: 0.08011394739151001, acc: 0.9736841917037964)
[2024-12-12 02:52:00,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:00,818][root][INFO] - Training Epoch: 6/10, step 568/574 completed (loss: 0.07768677175045013, acc: 1.0)
[2024-12-12 02:52:00,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:01,190][root][INFO] - Training Epoch: 6/10, step 569/574 completed (loss: 1.248007893562317, acc: 0.6684492230415344)
[2024-12-12 02:52:01,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:01,485][root][INFO] - Training Epoch: 6/10, step 570/574 completed (loss: 0.3572438061237335, acc: 0.8870967626571655)
[2024-12-12 02:52:01,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:01,865][root][INFO] - Training Epoch: 6/10, step 571/574 completed (loss: 0.6513015031814575, acc: 0.7777777910232544)
[2024-12-12 02:52:01,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:02,198][root][INFO] - Training Epoch: 6/10, step 572/574 completed (loss: 1.4939393997192383, acc: 0.5510203838348389)
[2024-12-12 02:52:02,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:02,532][root][INFO] - Training Epoch: 6/10, step 573/574 completed (loss: 1.4430683851242065, acc: 0.5471698045730591)
[2024-12-12 02:52:02,924][slam_llm.utils.train_utils][INFO] - Epoch 6: train_perplexity=2.0763, train_epoch_loss=0.7306, epoch time 352.8437786102295s
[2024-12-12 02:52:02,925][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 02:52:02,925][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2024-12-12 02:52:02,925][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 02:52:02,925][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 17
[2024-12-12 02:52:02,925][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:52:03,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:03,772][root][INFO] - Training Epoch: 7/10, step 0/574 completed (loss: 0.11954999715089798, acc: 0.9629629850387573)
[2024-12-12 02:52:03,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:04,066][root][INFO] - Training Epoch: 7/10, step 1/574 completed (loss: 0.2548345625400543, acc: 0.9200000166893005)
[2024-12-12 02:52:04,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:04,393][root][INFO] - Training Epoch: 7/10, step 2/574 completed (loss: 0.5097939968109131, acc: 0.8108108043670654)
[2024-12-12 02:52:04,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:04,774][root][INFO] - Training Epoch: 7/10, step 3/574 completed (loss: 0.251169890165329, acc: 0.9473684430122375)
[2024-12-12 02:52:04,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:05,183][root][INFO] - Training Epoch: 7/10, step 4/574 completed (loss: 0.22737601399421692, acc: 0.9459459185600281)
[2024-12-12 02:52:05,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:05,562][root][INFO] - Training Epoch: 7/10, step 5/574 completed (loss: 0.11108564585447311, acc: 1.0)
[2024-12-12 02:52:05,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:05,923][root][INFO] - Training Epoch: 7/10, step 6/574 completed (loss: 0.7230929732322693, acc: 0.7551020383834839)
[2024-12-12 02:52:06,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:06,270][root][INFO] - Training Epoch: 7/10, step 7/574 completed (loss: 0.20459583401679993, acc: 0.9333333373069763)
[2024-12-12 02:52:06,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:06,654][root][INFO] - Training Epoch: 7/10, step 8/574 completed (loss: 0.025927776470780373, acc: 1.0)
[2024-12-12 02:52:06,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:07,004][root][INFO] - Training Epoch: 7/10, step 9/574 completed (loss: 0.031684573739767075, acc: 1.0)
[2024-12-12 02:52:07,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:07,329][root][INFO] - Training Epoch: 7/10, step 10/574 completed (loss: 0.18012021481990814, acc: 0.9629629850387573)
[2024-12-12 02:52:07,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:07,654][root][INFO] - Training Epoch: 7/10, step 11/574 completed (loss: 0.4164563715457916, acc: 0.8461538553237915)
[2024-12-12 02:52:07,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:07,987][root][INFO] - Training Epoch: 7/10, step 12/574 completed (loss: 0.20806153118610382, acc: 0.8787878751754761)
[2024-12-12 02:52:08,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:08,402][root][INFO] - Training Epoch: 7/10, step 13/574 completed (loss: 0.4168027341365814, acc: 0.8478260636329651)
[2024-12-12 02:52:08,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:08,748][root][INFO] - Training Epoch: 7/10, step 14/574 completed (loss: 0.6028873324394226, acc: 0.8235294222831726)
[2024-12-12 02:52:08,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:09,078][root][INFO] - Training Epoch: 7/10, step 15/574 completed (loss: 0.671178936958313, acc: 0.795918345451355)
[2024-12-12 02:52:09,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:09,448][root][INFO] - Training Epoch: 7/10, step 16/574 completed (loss: 0.030650490894913673, acc: 1.0)
[2024-12-12 02:52:09,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:09,815][root][INFO] - Training Epoch: 7/10, step 17/574 completed (loss: 0.02320113033056259, acc: 1.0)
[2024-12-12 02:52:09,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:10,149][root][INFO] - Training Epoch: 7/10, step 18/574 completed (loss: 0.28381869196891785, acc: 0.9166666865348816)
[2024-12-12 02:52:10,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:10,532][root][INFO] - Training Epoch: 7/10, step 19/574 completed (loss: 0.025343909859657288, acc: 1.0)
[2024-12-12 02:52:10,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:10,864][root][INFO] - Training Epoch: 7/10, step 20/574 completed (loss: 0.2361392080783844, acc: 0.9230769276618958)
[2024-12-12 02:52:10,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:11,254][root][INFO] - Training Epoch: 7/10, step 21/574 completed (loss: 0.36692574620246887, acc: 0.931034505367279)
[2024-12-12 02:52:11,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:11,600][root][INFO] - Training Epoch: 7/10, step 22/574 completed (loss: 0.14151710271835327, acc: 0.9200000166893005)
[2024-12-12 02:52:11,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:11,986][root][INFO] - Training Epoch: 7/10, step 23/574 completed (loss: 0.12128359079360962, acc: 0.9047619104385376)
[2024-12-12 02:52:12,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:12,356][root][INFO] - Training Epoch: 7/10, step 24/574 completed (loss: 0.17425113916397095, acc: 0.9375)
[2024-12-12 02:52:12,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:12,683][root][INFO] - Training Epoch: 7/10, step 25/574 completed (loss: 0.6973045468330383, acc: 0.7735849022865295)
[2024-12-12 02:52:12,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:13,047][root][INFO] - Training Epoch: 7/10, step 26/574 completed (loss: 0.9588664770126343, acc: 0.7123287916183472)
[2024-12-12 02:52:13,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:14,294][root][INFO] - Training Epoch: 7/10, step 27/574 completed (loss: 1.7950184345245361, acc: 0.47035571932792664)
[2024-12-12 02:52:14,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:14,600][root][INFO] - Training Epoch: 7/10, step 28/574 completed (loss: 0.2364787459373474, acc: 0.930232584476471)
[2024-12-12 02:52:14,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:14,944][root][INFO] - Training Epoch: 7/10, step 29/574 completed (loss: 0.7610329389572144, acc: 0.7710843086242676)
[2024-12-12 02:52:15,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:15,346][root][INFO] - Training Epoch: 7/10, step 30/574 completed (loss: 1.1986796855926514, acc: 0.654321014881134)
[2024-12-12 02:52:15,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:15,690][root][INFO] - Training Epoch: 7/10, step 31/574 completed (loss: 0.12938864529132843, acc: 0.9642857313156128)
[2024-12-12 02:52:15,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:16,037][root][INFO] - Training Epoch: 7/10, step 32/574 completed (loss: 0.5113050937652588, acc: 0.8888888955116272)
[2024-12-12 02:52:16,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:16,407][root][INFO] - Training Epoch: 7/10, step 33/574 completed (loss: 0.020718950778245926, acc: 1.0)
[2024-12-12 02:52:16,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:16,802][root][INFO] - Training Epoch: 7/10, step 34/574 completed (loss: 1.1943968534469604, acc: 0.6974790096282959)
[2024-12-12 02:52:16,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:17,183][root][INFO] - Training Epoch: 7/10, step 35/574 completed (loss: 0.8346391320228577, acc: 0.8032786846160889)
[2024-12-12 02:52:17,291][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:17,544][root][INFO] - Training Epoch: 7/10, step 36/574 completed (loss: 1.0942754745483398, acc: 0.7460317611694336)
[2024-12-12 02:52:17,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:17,867][root][INFO] - Training Epoch: 7/10, step 37/574 completed (loss: 0.8605862259864807, acc: 0.7288135886192322)
[2024-12-12 02:52:17,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:18,209][root][INFO] - Training Epoch: 7/10, step 38/574 completed (loss: 0.9260516166687012, acc: 0.7356321811676025)
[2024-12-12 02:52:18,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:18,562][root][INFO] - Training Epoch: 7/10, step 39/574 completed (loss: 0.061831213533878326, acc: 1.0)
[2024-12-12 02:52:18,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:18,909][root][INFO] - Training Epoch: 7/10, step 40/574 completed (loss: 0.6555256247520447, acc: 0.8461538553237915)
[2024-12-12 02:52:19,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:19,325][root][INFO] - Training Epoch: 7/10, step 41/574 completed (loss: 0.9970587491989136, acc: 0.7432432174682617)
[2024-12-12 02:52:19,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:19,685][root][INFO] - Training Epoch: 7/10, step 42/574 completed (loss: 0.9961126446723938, acc: 0.7384615540504456)
[2024-12-12 02:52:19,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:20,130][root][INFO] - Training Epoch: 7/10, step 43/574 completed (loss: 1.4149858951568604, acc: 0.6666666865348816)
[2024-12-12 02:52:20,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:20,539][root][INFO] - Training Epoch: 7/10, step 44/574 completed (loss: 1.1336791515350342, acc: 0.6907216310501099)
[2024-12-12 02:52:20,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:20,935][root][INFO] - Training Epoch: 7/10, step 45/574 completed (loss: 1.3159066438674927, acc: 0.6029411554336548)
[2024-12-12 02:52:21,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:21,291][root][INFO] - Training Epoch: 7/10, step 46/574 completed (loss: 0.12013170123100281, acc: 0.9615384340286255)
[2024-12-12 02:52:21,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:21,667][root][INFO] - Training Epoch: 7/10, step 47/574 completed (loss: 0.12429846823215485, acc: 0.9629629850387573)
[2024-12-12 02:52:21,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:22,046][root][INFO] - Training Epoch: 7/10, step 48/574 completed (loss: 0.3320099413394928, acc: 0.8928571343421936)
[2024-12-12 02:52:22,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:22,387][root][INFO] - Training Epoch: 7/10, step 49/574 completed (loss: 0.2287890613079071, acc: 0.9444444179534912)
[2024-12-12 02:52:22,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:22,782][root][INFO] - Training Epoch: 7/10, step 50/574 completed (loss: 0.7622565031051636, acc: 0.7894737124443054)
[2024-12-12 02:52:22,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:23,126][root][INFO] - Training Epoch: 7/10, step 51/574 completed (loss: 0.7259669303894043, acc: 0.7936508059501648)
[2024-12-12 02:52:23,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:23,448][root][INFO] - Training Epoch: 7/10, step 52/574 completed (loss: 1.1516293287277222, acc: 0.6056337952613831)
[2024-12-12 02:52:23,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:23,902][root][INFO] - Training Epoch: 7/10, step 53/574 completed (loss: 1.7311418056488037, acc: 0.4866666793823242)
[2024-12-12 02:52:24,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:24,291][root][INFO] - Training Epoch: 7/10, step 54/574 completed (loss: 0.4258732795715332, acc: 0.8918918967247009)
[2024-12-12 02:52:24,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:24,689][root][INFO] - Training Epoch: 7/10, step 55/574 completed (loss: 0.1342875361442566, acc: 0.9615384340286255)
[2024-12-12 02:52:26,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:27,659][root][INFO] - Training Epoch: 7/10, step 56/574 completed (loss: 1.5811254978179932, acc: 0.5699658989906311)
[2024-12-12 02:52:28,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:28,987][root][INFO] - Training Epoch: 7/10, step 57/574 completed (loss: 2.04144549369812, acc: 0.4596949815750122)
[2024-12-12 02:52:29,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:29,612][root][INFO] - Training Epoch: 7/10, step 58/574 completed (loss: 1.4744590520858765, acc: 0.5738636255264282)
[2024-12-12 02:52:29,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:30,212][root][INFO] - Training Epoch: 7/10, step 59/574 completed (loss: 1.2601141929626465, acc: 0.654411792755127)
[2024-12-12 02:52:30,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:30,817][root][INFO] - Training Epoch: 7/10, step 60/574 completed (loss: 1.5998121500015259, acc: 0.5072463750839233)
[2024-12-12 02:52:30,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:31,232][root][INFO] - Training Epoch: 7/10, step 61/574 completed (loss: 1.1814937591552734, acc: 0.6875)
[2024-12-12 02:52:31,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:31,545][root][INFO] - Training Epoch: 7/10, step 62/574 completed (loss: 0.05021242797374725, acc: 1.0)
[2024-12-12 02:52:31,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:31,867][root][INFO] - Training Epoch: 7/10, step 63/574 completed (loss: 0.5076029896736145, acc: 0.8055555820465088)
[2024-12-12 02:52:31,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:32,211][root][INFO] - Training Epoch: 7/10, step 64/574 completed (loss: 0.31980830430984497, acc: 0.921875)
[2024-12-12 02:52:32,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:32,558][root][INFO] - Training Epoch: 7/10, step 65/574 completed (loss: 0.09343484044075012, acc: 1.0)
[2024-12-12 02:52:32,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:32,906][root][INFO] - Training Epoch: 7/10, step 66/574 completed (loss: 0.6782746911048889, acc: 0.8392857313156128)
[2024-12-12 02:52:33,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:33,251][root][INFO] - Training Epoch: 7/10, step 67/574 completed (loss: 0.510627031326294, acc: 0.7833333611488342)
[2024-12-12 02:52:33,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:33,586][root][INFO] - Training Epoch: 7/10, step 68/574 completed (loss: 0.02166328951716423, acc: 1.0)
[2024-12-12 02:52:33,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:33,924][root][INFO] - Training Epoch: 7/10, step 69/574 completed (loss: 0.263807088136673, acc: 0.8888888955116272)
[2024-12-12 02:52:34,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:34,267][root][INFO] - Training Epoch: 7/10, step 70/574 completed (loss: 0.37588486075401306, acc: 0.8181818127632141)
[2024-12-12 02:52:34,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:34,619][root][INFO] - Training Epoch: 7/10, step 71/574 completed (loss: 1.4317021369934082, acc: 0.595588207244873)
[2024-12-12 02:52:34,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:34,961][root][INFO] - Training Epoch: 7/10, step 72/574 completed (loss: 1.2546498775482178, acc: 0.5952380895614624)
[2024-12-12 02:52:35,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:35,328][root][INFO] - Training Epoch: 7/10, step 73/574 completed (loss: 1.7774699926376343, acc: 0.5076923370361328)
[2024-12-12 02:52:35,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:35,678][root][INFO] - Training Epoch: 7/10, step 74/574 completed (loss: 0.8684759140014648, acc: 0.6836734414100647)
[2024-12-12 02:52:35,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:36,060][root][INFO] - Training Epoch: 7/10, step 75/574 completed (loss: 1.528187870979309, acc: 0.5447761416435242)
[2024-12-12 02:52:36,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:36,454][root][INFO] - Training Epoch: 7/10, step 76/574 completed (loss: 1.7937779426574707, acc: 0.5218977928161621)
[2024-12-12 02:52:36,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:36,788][root][INFO] - Training Epoch: 7/10, step 77/574 completed (loss: 0.015650693327188492, acc: 1.0)
[2024-12-12 02:52:36,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:37,143][root][INFO] - Training Epoch: 7/10, step 78/574 completed (loss: 0.01911683939397335, acc: 1.0)
[2024-12-12 02:52:37,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:37,516][root][INFO] - Training Epoch: 7/10, step 79/574 completed (loss: 0.13009166717529297, acc: 0.9696969985961914)
[2024-12-12 02:52:37,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:37,889][root][INFO] - Training Epoch: 7/10, step 80/574 completed (loss: 0.043980177491903305, acc: 1.0)
[2024-12-12 02:52:37,995][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:38,235][root][INFO] - Training Epoch: 7/10, step 81/574 completed (loss: 0.6313477754592896, acc: 0.7884615659713745)
[2024-12-12 02:52:38,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:38,624][root][INFO] - Training Epoch: 7/10, step 82/574 completed (loss: 0.8249789476394653, acc: 0.7115384340286255)
[2024-12-12 02:52:38,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:39,002][root][INFO] - Training Epoch: 7/10, step 83/574 completed (loss: 0.11915621161460876, acc: 0.96875)
[2024-12-12 02:52:39,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:39,385][root][INFO] - Training Epoch: 7/10, step 84/574 completed (loss: 0.7630642652511597, acc: 0.782608687877655)
[2024-12-12 02:52:39,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:39,755][root][INFO] - Training Epoch: 7/10, step 85/574 completed (loss: 0.5786373019218445, acc: 0.7799999713897705)
[2024-12-12 02:52:39,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:40,123][root][INFO] - Training Epoch: 7/10, step 86/574 completed (loss: 0.038347695022821426, acc: 1.0)
[2024-12-12 02:52:40,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:40,574][root][INFO] - Training Epoch: 7/10, step 87/574 completed (loss: 0.5332470536231995, acc: 0.8199999928474426)
[2024-12-12 02:52:40,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:40,905][root][INFO] - Training Epoch: 7/10, step 88/574 completed (loss: 0.9659278392791748, acc: 0.7572815418243408)
[2024-12-12 02:52:41,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:41,993][root][INFO] - Training Epoch: 7/10, step 89/574 completed (loss: 1.2364451885223389, acc: 0.6941747665405273)
[2024-12-12 02:52:42,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:42,815][root][INFO] - Training Epoch: 7/10, step 90/574 completed (loss: 1.4883993864059448, acc: 0.6075268983840942)
[2024-12-12 02:52:43,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:43,620][root][INFO] - Training Epoch: 7/10, step 91/574 completed (loss: 1.3937360048294067, acc: 0.6163793206214905)
[2024-12-12 02:52:43,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:44,364][root][INFO] - Training Epoch: 7/10, step 92/574 completed (loss: 0.9136549830436707, acc: 0.6842105388641357)
[2024-12-12 02:52:44,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:45,354][root][INFO] - Training Epoch: 7/10, step 93/574 completed (loss: 1.647566556930542, acc: 0.5148515105247498)
[2024-12-12 02:52:45,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:45,746][root][INFO] - Training Epoch: 7/10, step 94/574 completed (loss: 1.239543080329895, acc: 0.6290322542190552)
[2024-12-12 02:52:45,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:46,153][root][INFO] - Training Epoch: 7/10, step 95/574 completed (loss: 0.7340993881225586, acc: 0.8260869383811951)
[2024-12-12 02:52:46,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:46,578][root][INFO] - Training Epoch: 7/10, step 96/574 completed (loss: 1.505711317062378, acc: 0.5378151535987854)
[2024-12-12 02:52:46,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:46,998][root][INFO] - Training Epoch: 7/10, step 97/574 completed (loss: 1.4057756662368774, acc: 0.6057692170143127)
[2024-12-12 02:52:47,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:47,380][root][INFO] - Training Epoch: 7/10, step 98/574 completed (loss: 1.6425808668136597, acc: 0.55474454164505)
[2024-12-12 02:52:47,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:47,687][root][INFO] - Training Epoch: 7/10, step 99/574 completed (loss: 1.0234428644180298, acc: 0.6865671873092651)
[2024-12-12 02:52:47,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:48,003][root][INFO] - Training Epoch: 7/10, step 100/574 completed (loss: 0.23599493503570557, acc: 0.949999988079071)
[2024-12-12 02:52:48,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:48,275][root][INFO] - Training Epoch: 7/10, step 101/574 completed (loss: 0.017814423888921738, acc: 1.0)
[2024-12-12 02:52:48,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:48,674][root][INFO] - Training Epoch: 7/10, step 102/574 completed (loss: 0.03056042641401291, acc: 1.0)
[2024-12-12 02:52:48,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:49,046][root][INFO] - Training Epoch: 7/10, step 103/574 completed (loss: 0.06815502792596817, acc: 1.0)
[2024-12-12 02:52:49,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:49,405][root][INFO] - Training Epoch: 7/10, step 104/574 completed (loss: 0.6673969626426697, acc: 0.8275862336158752)
[2024-12-12 02:52:49,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:49,723][root][INFO] - Training Epoch: 7/10, step 105/574 completed (loss: 0.22784487903118134, acc: 0.9069767594337463)
[2024-12-12 02:52:49,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:50,043][root][INFO] - Training Epoch: 7/10, step 106/574 completed (loss: 0.11538143455982208, acc: 0.9599999785423279)
[2024-12-12 02:52:50,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:50,403][root][INFO] - Training Epoch: 7/10, step 107/574 completed (loss: 0.005287250969558954, acc: 1.0)
[2024-12-12 02:52:50,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:50,776][root][INFO] - Training Epoch: 7/10, step 108/574 completed (loss: 0.04121438413858414, acc: 0.9615384340286255)
[2024-12-12 02:52:50,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:51,159][root][INFO] - Training Epoch: 7/10, step 109/574 completed (loss: 0.24150225520133972, acc: 0.9047619104385376)
[2024-12-12 02:52:51,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:51,555][root][INFO] - Training Epoch: 7/10, step 110/574 completed (loss: 0.7203713059425354, acc: 0.8307692408561707)
[2024-12-12 02:52:51,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:51,977][root][INFO] - Training Epoch: 7/10, step 111/574 completed (loss: 0.7116268873214722, acc: 0.8245614171028137)
[2024-12-12 02:52:52,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:52,347][root][INFO] - Training Epoch: 7/10, step 112/574 completed (loss: 0.815335214138031, acc: 0.7719298005104065)
[2024-12-12 02:52:52,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:52,705][root][INFO] - Training Epoch: 7/10, step 113/574 completed (loss: 0.5222204923629761, acc: 0.8205128312110901)
[2024-12-12 02:52:52,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:53,089][root][INFO] - Training Epoch: 7/10, step 114/574 completed (loss: 0.6076999306678772, acc: 0.8367347121238708)
[2024-12-12 02:52:53,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:53,414][root][INFO] - Training Epoch: 7/10, step 115/574 completed (loss: 0.004150408320128918, acc: 1.0)
[2024-12-12 02:52:53,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:53,747][root][INFO] - Training Epoch: 7/10, step 116/574 completed (loss: 0.7437395453453064, acc: 0.8095238208770752)
[2024-12-12 02:52:53,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:54,127][root][INFO] - Training Epoch: 7/10, step 117/574 completed (loss: 1.0194374322891235, acc: 0.7154471278190613)
[2024-12-12 02:52:54,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:54,514][root][INFO] - Training Epoch: 7/10, step 118/574 completed (loss: 0.5398623943328857, acc: 0.7903226017951965)
[2024-12-12 02:52:54,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:55,406][root][INFO] - Training Epoch: 7/10, step 119/574 completed (loss: 1.6230775117874146, acc: 0.517110288143158)
[2024-12-12 02:52:55,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:55,811][root][INFO] - Training Epoch: 7/10, step 120/574 completed (loss: 0.6454658508300781, acc: 0.7866666913032532)
[2024-12-12 02:52:55,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:56,228][root][INFO] - Training Epoch: 7/10, step 121/574 completed (loss: 0.5924047231674194, acc: 0.8461538553237915)
[2024-12-12 02:52:56,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:56,589][root][INFO] - Training Epoch: 7/10, step 122/574 completed (loss: 0.039039719849824905, acc: 1.0)
[2024-12-12 02:52:56,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:56,946][root][INFO] - Training Epoch: 7/10, step 123/574 completed (loss: 0.15147294104099274, acc: 0.9473684430122375)
[2024-12-12 02:52:57,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:57,294][root][INFO] - Training Epoch: 7/10, step 124/574 completed (loss: 1.431786060333252, acc: 0.5889570713043213)
[2024-12-12 02:52:57,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:57,705][root][INFO] - Training Epoch: 7/10, step 125/574 completed (loss: 1.2671552896499634, acc: 0.6180555820465088)
[2024-12-12 02:52:57,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:58,082][root][INFO] - Training Epoch: 7/10, step 126/574 completed (loss: 1.2920258045196533, acc: 0.6166666746139526)
[2024-12-12 02:52:58,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:58,459][root][INFO] - Training Epoch: 7/10, step 127/574 completed (loss: 1.324099063873291, acc: 0.601190447807312)
[2024-12-12 02:52:58,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:58,785][root][INFO] - Training Epoch: 7/10, step 128/574 completed (loss: 1.2323366403579712, acc: 0.656410276889801)
[2024-12-12 02:52:58,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:59,187][root][INFO] - Training Epoch: 7/10, step 129/574 completed (loss: 1.301781415939331, acc: 0.6102941036224365)
[2024-12-12 02:52:59,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:52:59,508][root][INFO] - Training Epoch: 7/10, step 130/574 completed (loss: 0.22773104906082153, acc: 0.9615384340286255)
[2024-12-12 02:53:00,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:00,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:01,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:01,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:01,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:02,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:02,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:02,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:02,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:03,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:03,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:04,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:04,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:04,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:05,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:05,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:05,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:06,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:06,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:06,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:07,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:07,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:07,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:08,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:08,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:09,020][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:09,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:09,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:09,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:10,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:10,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:10,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:11,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:11,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:11,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:12,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:12,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:12,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:13,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:13,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:14,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:14,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:14,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:15,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:15,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:15,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:16,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:16,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:16,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:16,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:17,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:17,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:17,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:18,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:18,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:18,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:19,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:19,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:19,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:20,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:20,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:20,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:21,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:21,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:21,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:22,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:22,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:23,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:23,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:24,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:24,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:24,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:24,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:25,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:25,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:25,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:26,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:26,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:27,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:27,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:27,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:28,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:28,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:28,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:28,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:29,575][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.8339, device='cuda:0') eval_epoch_loss=tensor(1.3439, device='cuda:0') eval_epoch_acc=tensor(0.6802, device='cuda:0')
[2024-12-12 02:53:29,576][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:53:29,576][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:53:30,027][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_7_step_131_loss_1.3438856601715088/model.pt
[2024-12-12 02:53:30,030][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:53:30,030][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 7 is 1.3438856601715088
[2024-12-12 02:53:30,031][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 7 is 0.6802414059638977
[2024-12-12 02:53:30,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:30,421][root][INFO] - Training Epoch: 7/10, step 131/574 completed (loss: 0.15535590052604675, acc: 0.95652174949646)
[2024-12-12 02:53:30,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:30,738][root][INFO] - Training Epoch: 7/10, step 132/574 completed (loss: 0.1688748151063919, acc: 0.9375)
[2024-12-12 02:53:30,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:31,037][root][INFO] - Training Epoch: 7/10, step 133/574 completed (loss: 0.2601320743560791, acc: 0.9130434989929199)
[2024-12-12 02:53:31,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:31,394][root][INFO] - Training Epoch: 7/10, step 134/574 completed (loss: 0.13685502111911774, acc: 0.9714285731315613)
[2024-12-12 02:53:31,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:31,730][root][INFO] - Training Epoch: 7/10, step 135/574 completed (loss: 0.1477038413286209, acc: 0.9615384340286255)
[2024-12-12 02:53:31,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:32,031][root][INFO] - Training Epoch: 7/10, step 136/574 completed (loss: 0.32946643233299255, acc: 0.8809523582458496)
[2024-12-12 02:53:32,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:32,318][root][INFO] - Training Epoch: 7/10, step 137/574 completed (loss: 0.4157024621963501, acc: 0.8333333134651184)
[2024-12-12 02:53:32,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:32,614][root][INFO] - Training Epoch: 7/10, step 138/574 completed (loss: 0.16362564265727997, acc: 0.9130434989929199)
[2024-12-12 02:53:32,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:32,927][root][INFO] - Training Epoch: 7/10, step 139/574 completed (loss: 0.0475461408495903, acc: 1.0)
[2024-12-12 02:53:33,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:33,307][root][INFO] - Training Epoch: 7/10, step 140/574 completed (loss: 0.06318274885416031, acc: 0.9615384340286255)
[2024-12-12 02:53:33,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:33,681][root][INFO] - Training Epoch: 7/10, step 141/574 completed (loss: 0.24959954619407654, acc: 0.9032257795333862)
[2024-12-12 02:53:33,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:34,025][root][INFO] - Training Epoch: 7/10, step 142/574 completed (loss: 0.34787583351135254, acc: 0.9189189076423645)
[2024-12-12 02:53:34,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:34,548][root][INFO] - Training Epoch: 7/10, step 143/574 completed (loss: 1.0349998474121094, acc: 0.6578947305679321)
[2024-12-12 02:53:34,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:34,879][root][INFO] - Training Epoch: 7/10, step 144/574 completed (loss: 0.980930507183075, acc: 0.7238805890083313)
[2024-12-12 02:53:34,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:35,198][root][INFO] - Training Epoch: 7/10, step 145/574 completed (loss: 1.1139830350875854, acc: 0.6122449040412903)
[2024-12-12 02:53:35,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:35,635][root][INFO] - Training Epoch: 7/10, step 146/574 completed (loss: 1.5259264707565308, acc: 0.5744680762290955)
[2024-12-12 02:53:35,721][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:35,952][root][INFO] - Training Epoch: 7/10, step 147/574 completed (loss: 0.9116763472557068, acc: 0.7285714149475098)
[2024-12-12 02:53:36,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:36,254][root][INFO] - Training Epoch: 7/10, step 148/574 completed (loss: 0.3527636229991913, acc: 0.8571428656578064)
[2024-12-12 02:53:36,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:36,556][root][INFO] - Training Epoch: 7/10, step 149/574 completed (loss: 0.41981008648872375, acc: 0.9130434989929199)
[2024-12-12 02:53:36,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:36,857][root][INFO] - Training Epoch: 7/10, step 150/574 completed (loss: 0.1333197057247162, acc: 0.931034505367279)
[2024-12-12 02:53:36,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:37,159][root][INFO] - Training Epoch: 7/10, step 151/574 completed (loss: 0.5445446372032166, acc: 0.804347813129425)
[2024-12-12 02:53:37,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:37,503][root][INFO] - Training Epoch: 7/10, step 152/574 completed (loss: 0.9237406849861145, acc: 0.7796609997749329)
[2024-12-12 02:53:37,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:37,849][root][INFO] - Training Epoch: 7/10, step 153/574 completed (loss: 0.6732589602470398, acc: 0.7894737124443054)
[2024-12-12 02:53:37,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:38,223][root][INFO] - Training Epoch: 7/10, step 154/574 completed (loss: 0.9168182611465454, acc: 0.7432432174682617)
[2024-12-12 02:53:38,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:38,555][root][INFO] - Training Epoch: 7/10, step 155/574 completed (loss: 0.11065006256103516, acc: 0.9642857313156128)
[2024-12-12 02:53:38,640][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:38,908][root][INFO] - Training Epoch: 7/10, step 156/574 completed (loss: 0.42542630434036255, acc: 0.8695651888847351)
[2024-12-12 02:53:38,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:39,256][root][INFO] - Training Epoch: 7/10, step 157/574 completed (loss: 0.7084638476371765, acc: 0.7894737124443054)
[2024-12-12 02:53:39,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:40,915][root][INFO] - Training Epoch: 7/10, step 158/574 completed (loss: 1.007419228553772, acc: 0.7432432174682617)
[2024-12-12 02:53:41,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:41,346][root][INFO] - Training Epoch: 7/10, step 159/574 completed (loss: 1.2530171871185303, acc: 0.5740740895271301)
[2024-12-12 02:53:41,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:41,750][root][INFO] - Training Epoch: 7/10, step 160/574 completed (loss: 0.9645397067070007, acc: 0.7441860437393188)
[2024-12-12 02:53:41,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:42,337][root][INFO] - Training Epoch: 7/10, step 161/574 completed (loss: 0.9688323736190796, acc: 0.7176470756530762)
[2024-12-12 02:53:42,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:42,891][root][INFO] - Training Epoch: 7/10, step 162/574 completed (loss: 1.3521983623504639, acc: 0.6067415475845337)
[2024-12-12 02:53:42,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:43,238][root][INFO] - Training Epoch: 7/10, step 163/574 completed (loss: 0.6892601251602173, acc: 0.7727272510528564)
[2024-12-12 02:53:43,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:43,626][root][INFO] - Training Epoch: 7/10, step 164/574 completed (loss: 0.39439740777015686, acc: 0.9047619104385376)
[2024-12-12 02:53:43,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:44,004][root][INFO] - Training Epoch: 7/10, step 165/574 completed (loss: 0.4171822965145111, acc: 0.931034505367279)
[2024-12-12 02:53:44,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:44,344][root][INFO] - Training Epoch: 7/10, step 166/574 completed (loss: 0.22585244476795197, acc: 0.9387755393981934)
[2024-12-12 02:53:44,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:44,745][root][INFO] - Training Epoch: 7/10, step 167/574 completed (loss: 0.40228256583213806, acc: 0.8999999761581421)
[2024-12-12 02:53:44,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:45,147][root][INFO] - Training Epoch: 7/10, step 168/574 completed (loss: 0.8255760669708252, acc: 0.7638888955116272)
[2024-12-12 02:53:45,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:45,504][root][INFO] - Training Epoch: 7/10, step 169/574 completed (loss: 1.2620021104812622, acc: 0.5980392098426819)
[2024-12-12 02:53:45,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:46,533][root][INFO] - Training Epoch: 7/10, step 170/574 completed (loss: 1.7978129386901855, acc: 0.534246563911438)
[2024-12-12 02:53:46,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:46,878][root][INFO] - Training Epoch: 7/10, step 171/574 completed (loss: 0.12918612360954285, acc: 0.9583333134651184)
[2024-12-12 02:53:47,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:47,261][root][INFO] - Training Epoch: 7/10, step 172/574 completed (loss: 0.4305681884288788, acc: 0.8888888955116272)
[2024-12-12 02:53:47,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:47,598][root][INFO] - Training Epoch: 7/10, step 173/574 completed (loss: 0.05868622660636902, acc: 1.0)
[2024-12-12 02:53:47,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:48,145][root][INFO] - Training Epoch: 7/10, step 174/574 completed (loss: 1.0797874927520752, acc: 0.7168141603469849)
[2024-12-12 02:53:48,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:48,487][root][INFO] - Training Epoch: 7/10, step 175/574 completed (loss: 0.8018840551376343, acc: 0.7971014380455017)
[2024-12-12 02:53:48,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:48,854][root][INFO] - Training Epoch: 7/10, step 176/574 completed (loss: 0.8654358386993408, acc: 0.7386363744735718)
[2024-12-12 02:53:49,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:49,773][root][INFO] - Training Epoch: 7/10, step 177/574 completed (loss: 1.8319141864776611, acc: 0.5648854970932007)
[2024-12-12 02:53:49,941][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:50,456][root][INFO] - Training Epoch: 7/10, step 178/574 completed (loss: 1.4066027402877808, acc: 0.5703703761100769)
[2024-12-12 02:53:50,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:50,822][root][INFO] - Training Epoch: 7/10, step 179/574 completed (loss: 0.637321412563324, acc: 0.7868852615356445)
[2024-12-12 02:53:50,920][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:51,140][root][INFO] - Training Epoch: 7/10, step 180/574 completed (loss: 0.3251287043094635, acc: 0.9166666865348816)
[2024-12-12 02:53:51,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:51,432][root][INFO] - Training Epoch: 7/10, step 181/574 completed (loss: 0.37176117300987244, acc: 0.8799999952316284)
[2024-12-12 02:53:51,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:51,759][root][INFO] - Training Epoch: 7/10, step 182/574 completed (loss: 0.2574516236782074, acc: 0.9285714030265808)
[2024-12-12 02:53:51,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:52,123][root][INFO] - Training Epoch: 7/10, step 183/574 completed (loss: 0.6349467635154724, acc: 0.792682945728302)
[2024-12-12 02:53:52,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:52,552][root][INFO] - Training Epoch: 7/10, step 184/574 completed (loss: 1.3514361381530762, acc: 0.6132930517196655)
[2024-12-12 02:53:52,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:52,937][root][INFO] - Training Epoch: 7/10, step 185/574 completed (loss: 1.6085914373397827, acc: 0.5360230803489685)
[2024-12-12 02:53:53,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:53,427][root][INFO] - Training Epoch: 7/10, step 186/574 completed (loss: 1.6118818521499634, acc: 0.5687500238418579)
[2024-12-12 02:53:53,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:53,955][root][INFO] - Training Epoch: 7/10, step 187/574 completed (loss: 1.772894024848938, acc: 0.5159474611282349)
[2024-12-12 02:53:54,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:54,370][root][INFO] - Training Epoch: 7/10, step 188/574 completed (loss: 1.4036012887954712, acc: 0.5907473564147949)
[2024-12-12 02:53:54,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:54,699][root][INFO] - Training Epoch: 7/10, step 189/574 completed (loss: 0.32843825221061707, acc: 0.8799999952316284)
[2024-12-12 02:53:54,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:55,249][root][INFO] - Training Epoch: 7/10, step 190/574 completed (loss: 1.2667491436004639, acc: 0.569767415523529)
[2024-12-12 02:53:55,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:56,046][root][INFO] - Training Epoch: 7/10, step 191/574 completed (loss: 1.5868780612945557, acc: 0.5634920597076416)
[2024-12-12 02:53:56,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:56,970][root][INFO] - Training Epoch: 7/10, step 192/574 completed (loss: 1.3987706899642944, acc: 0.5681818127632141)
[2024-12-12 02:53:57,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:57,726][root][INFO] - Training Epoch: 7/10, step 193/574 completed (loss: 0.8532480597496033, acc: 0.7764706015586853)
[2024-12-12 02:53:58,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:58,813][root][INFO] - Training Epoch: 7/10, step 194/574 completed (loss: 1.1110774278640747, acc: 0.6851851940155029)
[2024-12-12 02:53:59,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:53:59,771][root][INFO] - Training Epoch: 7/10, step 195/574 completed (loss: 0.6677395701408386, acc: 0.8064516186714172)
[2024-12-12 02:53:59,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:00,056][root][INFO] - Training Epoch: 7/10, step 196/574 completed (loss: 0.38240566849708557, acc: 0.8928571343421936)
[2024-12-12 02:54:00,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:00,389][root][INFO] - Training Epoch: 7/10, step 197/574 completed (loss: 0.24057824909687042, acc: 0.8999999761581421)
[2024-12-12 02:54:00,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:00,733][root][INFO] - Training Epoch: 7/10, step 198/574 completed (loss: 0.6393362283706665, acc: 0.7941176295280457)
[2024-12-12 02:54:00,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:01,116][root][INFO] - Training Epoch: 7/10, step 199/574 completed (loss: 1.2055014371871948, acc: 0.654411792755127)
[2024-12-12 02:54:01,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:01,538][root][INFO] - Training Epoch: 7/10, step 200/574 completed (loss: 1.0629844665527344, acc: 0.694915235042572)
[2024-12-12 02:54:01,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:01,917][root][INFO] - Training Epoch: 7/10, step 201/574 completed (loss: 1.306481122970581, acc: 0.6492537260055542)
[2024-12-12 02:54:02,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:02,339][root][INFO] - Training Epoch: 7/10, step 202/574 completed (loss: 1.0071220397949219, acc: 0.708737850189209)
[2024-12-12 02:54:02,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:02,678][root][INFO] - Training Epoch: 7/10, step 203/574 completed (loss: 0.7112655639648438, acc: 0.761904776096344)
[2024-12-12 02:54:02,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:03,034][root][INFO] - Training Epoch: 7/10, step 204/574 completed (loss: 0.5106256604194641, acc: 0.8461538553237915)
[2024-12-12 02:54:03,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:03,457][root][INFO] - Training Epoch: 7/10, step 205/574 completed (loss: 1.2433415651321411, acc: 0.6547085046768188)
[2024-12-12 02:54:03,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:03,854][root][INFO] - Training Epoch: 7/10, step 206/574 completed (loss: 1.4351640939712524, acc: 0.6299212574958801)
[2024-12-12 02:54:03,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:04,188][root][INFO] - Training Epoch: 7/10, step 207/574 completed (loss: 1.265400767326355, acc: 0.6594827771186829)
[2024-12-12 02:54:04,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:04,555][root][INFO] - Training Epoch: 7/10, step 208/574 completed (loss: 1.3304111957550049, acc: 0.6739130616188049)
[2024-12-12 02:54:04,671][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:04,905][root][INFO] - Training Epoch: 7/10, step 209/574 completed (loss: 1.1343408823013306, acc: 0.661478579044342)
[2024-12-12 02:54:05,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:05,281][root][INFO] - Training Epoch: 7/10, step 210/574 completed (loss: 1.071648359298706, acc: 0.6739130616188049)
[2024-12-12 02:54:05,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:05,662][root][INFO] - Training Epoch: 7/10, step 211/574 completed (loss: 0.039988283067941666, acc: 1.0)
[2024-12-12 02:54:05,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:06,057][root][INFO] - Training Epoch: 7/10, step 212/574 completed (loss: 0.3472658097743988, acc: 0.8571428656578064)
[2024-12-12 02:54:06,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:06,384][root][INFO] - Training Epoch: 7/10, step 213/574 completed (loss: 0.31897446513175964, acc: 0.914893627166748)
[2024-12-12 02:54:06,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:07,069][root][INFO] - Training Epoch: 7/10, step 214/574 completed (loss: 0.7775400280952454, acc: 0.7846153974533081)
[2024-12-12 02:54:07,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:07,462][root][INFO] - Training Epoch: 7/10, step 215/574 completed (loss: 0.49268999695777893, acc: 0.837837815284729)
[2024-12-12 02:54:07,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:07,871][root][INFO] - Training Epoch: 7/10, step 216/574 completed (loss: 0.5311315059661865, acc: 0.8604651093482971)
[2024-12-12 02:54:08,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:08,408][root][INFO] - Training Epoch: 7/10, step 217/574 completed (loss: 0.7129625678062439, acc: 0.8288288116455078)
[2024-12-12 02:54:08,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:08,793][root][INFO] - Training Epoch: 7/10, step 218/574 completed (loss: 0.4931992292404175, acc: 0.8666666746139526)
[2024-12-12 02:54:08,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:09,164][root][INFO] - Training Epoch: 7/10, step 219/574 completed (loss: 0.17697399854660034, acc: 0.939393937587738)
[2024-12-12 02:54:09,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:09,496][root][INFO] - Training Epoch: 7/10, step 220/574 completed (loss: 0.029886873438954353, acc: 1.0)
[2024-12-12 02:54:09,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:09,806][root][INFO] - Training Epoch: 7/10, step 221/574 completed (loss: 0.12030796706676483, acc: 0.9599999785423279)
[2024-12-12 02:54:09,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:10,158][root][INFO] - Training Epoch: 7/10, step 222/574 completed (loss: 0.5078266859054565, acc: 0.8653846383094788)
[2024-12-12 02:54:10,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:10,950][root][INFO] - Training Epoch: 7/10, step 223/574 completed (loss: 0.8769236207008362, acc: 0.717391312122345)
[2024-12-12 02:54:11,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:11,502][root][INFO] - Training Epoch: 7/10, step 224/574 completed (loss: 1.164272427558899, acc: 0.6931818127632141)
[2024-12-12 02:54:11,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:11,949][root][INFO] - Training Epoch: 7/10, step 225/574 completed (loss: 0.9814438223838806, acc: 0.6914893388748169)
[2024-12-12 02:54:12,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:12,344][root][INFO] - Training Epoch: 7/10, step 226/574 completed (loss: 0.40119460225105286, acc: 0.8867924809455872)
[2024-12-12 02:54:12,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:12,740][root][INFO] - Training Epoch: 7/10, step 227/574 completed (loss: 0.5021122694015503, acc: 0.8166666626930237)
[2024-12-12 02:54:12,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:13,089][root][INFO] - Training Epoch: 7/10, step 228/574 completed (loss: 0.198792964220047, acc: 0.9534883499145508)
[2024-12-12 02:54:13,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:13,393][root][INFO] - Training Epoch: 7/10, step 229/574 completed (loss: 0.3806489408016205, acc: 0.8999999761581421)
[2024-12-12 02:54:13,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:13,810][root][INFO] - Training Epoch: 7/10, step 230/574 completed (loss: 1.514330267906189, acc: 0.5789473652839661)
[2024-12-12 02:54:13,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:14,175][root][INFO] - Training Epoch: 7/10, step 231/574 completed (loss: 1.0595698356628418, acc: 0.6888889074325562)
[2024-12-12 02:54:14,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:14,621][root][INFO] - Training Epoch: 7/10, step 232/574 completed (loss: 1.1093765497207642, acc: 0.7055555582046509)
[2024-12-12 02:54:14,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:15,109][root][INFO] - Training Epoch: 7/10, step 233/574 completed (loss: 1.6007063388824463, acc: 0.5871559381484985)
[2024-12-12 02:54:15,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:15,575][root][INFO] - Training Epoch: 7/10, step 234/574 completed (loss: 0.9861887097358704, acc: 0.7461538314819336)
[2024-12-12 02:54:15,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:15,934][root][INFO] - Training Epoch: 7/10, step 235/574 completed (loss: 0.31167855858802795, acc: 0.8947368264198303)
[2024-12-12 02:54:16,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:16,256][root][INFO] - Training Epoch: 7/10, step 236/574 completed (loss: 0.125579372048378, acc: 0.9166666865348816)
[2024-12-12 02:54:16,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:16,614][root][INFO] - Training Epoch: 7/10, step 237/574 completed (loss: 0.46001651883125305, acc: 0.8181818127632141)
[2024-12-12 02:54:16,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:16,999][root][INFO] - Training Epoch: 7/10, step 238/574 completed (loss: 0.4937126040458679, acc: 0.8888888955116272)
[2024-12-12 02:54:17,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:17,393][root][INFO] - Training Epoch: 7/10, step 239/574 completed (loss: 0.19899383187294006, acc: 0.9428571462631226)
[2024-12-12 02:54:17,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:17,755][root][INFO] - Training Epoch: 7/10, step 240/574 completed (loss: 0.5075399279594421, acc: 0.8409090638160706)
[2024-12-12 02:54:17,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:18,052][root][INFO] - Training Epoch: 7/10, step 241/574 completed (loss: 0.24936890602111816, acc: 0.9318181872367859)
[2024-12-12 02:54:18,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:18,629][root][INFO] - Training Epoch: 7/10, step 242/574 completed (loss: 0.9378576874732971, acc: 0.7580645084381104)
[2024-12-12 02:54:18,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:19,165][root][INFO] - Training Epoch: 7/10, step 243/574 completed (loss: 0.5838508009910583, acc: 0.8181818127632141)
[2024-12-12 02:54:19,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:19,526][root][INFO] - Training Epoch: 7/10, step 244/574 completed (loss: 0.011419771239161491, acc: 1.0)
[2024-12-12 02:54:19,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:19,870][root][INFO] - Training Epoch: 7/10, step 245/574 completed (loss: 0.25446704030036926, acc: 0.9230769276618958)
[2024-12-12 02:54:19,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:20,231][root][INFO] - Training Epoch: 7/10, step 246/574 completed (loss: 0.03845949470996857, acc: 1.0)
[2024-12-12 02:54:20,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:20,606][root][INFO] - Training Epoch: 7/10, step 247/574 completed (loss: 0.04687195271253586, acc: 1.0)
[2024-12-12 02:54:20,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:20,960][root][INFO] - Training Epoch: 7/10, step 248/574 completed (loss: 0.35078686475753784, acc: 0.8648648858070374)
[2024-12-12 02:54:21,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:21,286][root][INFO] - Training Epoch: 7/10, step 249/574 completed (loss: 0.31450772285461426, acc: 0.8918918967247009)
[2024-12-12 02:54:21,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:21,606][root][INFO] - Training Epoch: 7/10, step 250/574 completed (loss: 0.27637574076652527, acc: 0.9189189076423645)
[2024-12-12 02:54:21,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:21,974][root][INFO] - Training Epoch: 7/10, step 251/574 completed (loss: 0.798357367515564, acc: 0.779411792755127)
[2024-12-12 02:54:22,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:22,322][root][INFO] - Training Epoch: 7/10, step 252/574 completed (loss: 0.24224157631397247, acc: 0.8536585569381714)
[2024-12-12 02:54:22,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:22,663][root][INFO] - Training Epoch: 7/10, step 253/574 completed (loss: 0.019937507808208466, acc: 1.0)
[2024-12-12 02:54:22,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:23,043][root][INFO] - Training Epoch: 7/10, step 254/574 completed (loss: 0.018555372953414917, acc: 1.0)
[2024-12-12 02:54:23,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:23,425][root][INFO] - Training Epoch: 7/10, step 255/574 completed (loss: 0.027273761108517647, acc: 1.0)
[2024-12-12 02:54:23,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:23,804][root][INFO] - Training Epoch: 7/10, step 256/574 completed (loss: 0.1349666565656662, acc: 0.9824561476707458)
[2024-12-12 02:54:23,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:24,141][root][INFO] - Training Epoch: 7/10, step 257/574 completed (loss: 0.3872554302215576, acc: 0.9142857193946838)
[2024-12-12 02:54:24,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:24,483][root][INFO] - Training Epoch: 7/10, step 258/574 completed (loss: 0.4518987834453583, acc: 0.8815789222717285)
[2024-12-12 02:54:24,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:25,047][root][INFO] - Training Epoch: 7/10, step 259/574 completed (loss: 0.8378163576126099, acc: 0.7735849022865295)
[2024-12-12 02:54:25,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:25,629][root][INFO] - Training Epoch: 7/10, step 260/574 completed (loss: 1.0522297620773315, acc: 0.7166666388511658)
[2024-12-12 02:54:25,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:25,991][root][INFO] - Training Epoch: 7/10, step 261/574 completed (loss: 0.22579018771648407, acc: 0.8888888955116272)
[2024-12-12 02:54:26,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:26,334][root][INFO] - Training Epoch: 7/10, step 262/574 completed (loss: 0.141091451048851, acc: 0.9677419066429138)
[2024-12-12 02:54:26,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:26,655][root][INFO] - Training Epoch: 7/10, step 263/574 completed (loss: 1.2100039720535278, acc: 0.7066666483879089)
[2024-12-12 02:54:26,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:26,960][root][INFO] - Training Epoch: 7/10, step 264/574 completed (loss: 0.9169685244560242, acc: 0.7291666865348816)
[2024-12-12 02:54:27,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:27,782][root][INFO] - Training Epoch: 7/10, step 265/574 completed (loss: 1.7428354024887085, acc: 0.5440000295639038)
[2024-12-12 02:54:27,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:28,104][root][INFO] - Training Epoch: 7/10, step 266/574 completed (loss: 1.4188718795776367, acc: 0.584269642829895)
[2024-12-12 02:54:28,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:28,467][root][INFO] - Training Epoch: 7/10, step 267/574 completed (loss: 1.2392476797103882, acc: 0.6081081032752991)
[2024-12-12 02:54:28,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:28,930][root][INFO] - Training Epoch: 7/10, step 268/574 completed (loss: 0.8552517294883728, acc: 0.7758620977401733)
[2024-12-12 02:54:29,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:29,286][root][INFO] - Training Epoch: 7/10, step 269/574 completed (loss: 0.03667020797729492, acc: 1.0)
[2024-12-12 02:54:29,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:29,604][root][INFO] - Training Epoch: 7/10, step 270/574 completed (loss: 0.017382774502038956, acc: 1.0)
[2024-12-12 02:54:29,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:29,942][root][INFO] - Training Epoch: 7/10, step 271/574 completed (loss: 0.1434839814901352, acc: 0.9375)
[2024-12-12 02:54:30,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:30,291][root][INFO] - Training Epoch: 7/10, step 272/574 completed (loss: 0.22157038748264313, acc: 0.9333333373069763)
[2024-12-12 02:54:30,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:30,681][root][INFO] - Training Epoch: 7/10, step 273/574 completed (loss: 0.5634779334068298, acc: 0.8166666626930237)
[2024-12-12 02:54:31,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:31,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:32,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:32,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:32,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:33,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:33,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:33,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:34,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:34,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:34,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:35,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:35,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:36,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:36,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:36,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:37,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:37,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:37,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:38,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:38,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:38,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:39,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:39,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:39,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:40,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:40,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:40,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:41,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:41,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:41,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:42,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:42,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:42,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:43,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:43,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:43,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:44,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:44,515][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:44,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:45,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:45,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:45,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:46,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:46,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:46,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:47,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:47,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:47,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:48,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:48,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:48,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:49,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:49,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:49,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:50,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:50,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:50,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:51,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:51,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:51,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:52,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:52,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:52,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:53,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:53,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:53,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:54,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:54,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:55,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:55,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:55,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:56,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:56,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:56,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:56,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:57,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:57,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:58,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:58,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:58,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:59,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:59,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:54:59,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:00,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:00,675][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.8662, device='cuda:0') eval_epoch_loss=tensor(1.3523, device='cuda:0') eval_epoch_acc=tensor(0.6789, device='cuda:0')
[2024-12-12 02:55:00,676][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:55:00,676][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:55:00,864][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_7_step_274_loss_1.3522841930389404/model.pt
[2024-12-12 02:55:00,868][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:55:00,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:01,262][root][INFO] - Training Epoch: 7/10, step 274/574 completed (loss: 0.3225717842578888, acc: 0.90625)
[2024-12-12 02:55:01,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:01,618][root][INFO] - Training Epoch: 7/10, step 275/574 completed (loss: 0.2767062485218048, acc: 0.9666666388511658)
[2024-12-12 02:55:01,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:01,990][root][INFO] - Training Epoch: 7/10, step 276/574 completed (loss: 0.2485731542110443, acc: 0.931034505367279)
[2024-12-12 02:55:02,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:02,340][root][INFO] - Training Epoch: 7/10, step 277/574 completed (loss: 0.03794103115797043, acc: 1.0)
[2024-12-12 02:55:02,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:02,686][root][INFO] - Training Epoch: 7/10, step 278/574 completed (loss: 0.39818423986434937, acc: 0.8936170339584351)
[2024-12-12 02:55:02,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:03,032][root][INFO] - Training Epoch: 7/10, step 279/574 completed (loss: 0.5368499159812927, acc: 0.8958333134651184)
[2024-12-12 02:55:03,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:03,419][root][INFO] - Training Epoch: 7/10, step 280/574 completed (loss: 0.296083003282547, acc: 0.9318181872367859)
[2024-12-12 02:55:03,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:03,842][root][INFO] - Training Epoch: 7/10, step 281/574 completed (loss: 1.0030690431594849, acc: 0.6987951993942261)
[2024-12-12 02:55:03,974][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:04,234][root][INFO] - Training Epoch: 7/10, step 282/574 completed (loss: 1.286103367805481, acc: 0.6388888955116272)
[2024-12-12 02:55:04,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:04,598][root][INFO] - Training Epoch: 7/10, step 283/574 completed (loss: 0.2078893631696701, acc: 0.9473684430122375)
[2024-12-12 02:55:04,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:04,941][root][INFO] - Training Epoch: 7/10, step 284/574 completed (loss: 0.270378977060318, acc: 0.9411764740943909)
[2024-12-12 02:55:05,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:05,263][root][INFO] - Training Epoch: 7/10, step 285/574 completed (loss: 0.09074221551418304, acc: 0.9750000238418579)
[2024-12-12 02:55:05,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:05,559][root][INFO] - Training Epoch: 7/10, step 286/574 completed (loss: 0.8774463534355164, acc: 0.734375)
[2024-12-12 02:55:05,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:05,856][root][INFO] - Training Epoch: 7/10, step 287/574 completed (loss: 0.967122495174408, acc: 0.7039999961853027)
[2024-12-12 02:55:05,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:06,175][root][INFO] - Training Epoch: 7/10, step 288/574 completed (loss: 0.5802677869796753, acc: 0.8241758346557617)
[2024-12-12 02:55:06,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:06,537][root][INFO] - Training Epoch: 7/10, step 289/574 completed (loss: 1.2412179708480835, acc: 0.6832298040390015)
[2024-12-12 02:55:06,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:06,932][root][INFO] - Training Epoch: 7/10, step 290/574 completed (loss: 1.3095256090164185, acc: 0.6391752362251282)
[2024-12-12 02:55:07,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:07,260][root][INFO] - Training Epoch: 7/10, step 291/574 completed (loss: 0.006170375272631645, acc: 1.0)
[2024-12-12 02:55:07,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:07,630][root][INFO] - Training Epoch: 7/10, step 292/574 completed (loss: 0.25492584705352783, acc: 0.9285714030265808)
[2024-12-12 02:55:07,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:07,987][root][INFO] - Training Epoch: 7/10, step 293/574 completed (loss: 0.5508685111999512, acc: 0.8965517282485962)
[2024-12-12 02:55:08,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:08,453][root][INFO] - Training Epoch: 7/10, step 294/574 completed (loss: 0.39815831184387207, acc: 0.9090909361839294)
[2024-12-12 02:55:08,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:09,001][root][INFO] - Training Epoch: 7/10, step 295/574 completed (loss: 1.1952927112579346, acc: 0.6855670213699341)
[2024-12-12 02:55:09,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:09,337][root][INFO] - Training Epoch: 7/10, step 296/574 completed (loss: 0.4906972348690033, acc: 0.8275862336158752)
[2024-12-12 02:55:09,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:09,724][root][INFO] - Training Epoch: 7/10, step 297/574 completed (loss: 0.218500554561615, acc: 0.9629629850387573)
[2024-12-12 02:55:09,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:10,062][root][INFO] - Training Epoch: 7/10, step 298/574 completed (loss: 0.3963216543197632, acc: 0.9210526347160339)
[2024-12-12 02:55:10,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:10,416][root][INFO] - Training Epoch: 7/10, step 299/574 completed (loss: 0.18494121730327606, acc: 0.9464285969734192)
[2024-12-12 02:55:10,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:10,747][root][INFO] - Training Epoch: 7/10, step 300/574 completed (loss: 0.08561213314533234, acc: 1.0)
[2024-12-12 02:55:10,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:11,062][root][INFO] - Training Epoch: 7/10, step 301/574 completed (loss: 0.3031792640686035, acc: 0.9245283007621765)
[2024-12-12 02:55:11,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:11,377][root][INFO] - Training Epoch: 7/10, step 302/574 completed (loss: 0.2894531786441803, acc: 0.9056603908538818)
[2024-12-12 02:55:11,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:11,663][root][INFO] - Training Epoch: 7/10, step 303/574 completed (loss: 0.1714342087507248, acc: 0.970588207244873)
[2024-12-12 02:55:11,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:12,013][root][INFO] - Training Epoch: 7/10, step 304/574 completed (loss: 0.12897703051567078, acc: 0.9375)
[2024-12-12 02:55:12,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:12,352][root][INFO] - Training Epoch: 7/10, step 305/574 completed (loss: 0.46580201387405396, acc: 0.8360655903816223)
[2024-12-12 02:55:12,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:12,735][root][INFO] - Training Epoch: 7/10, step 306/574 completed (loss: 0.21562416851520538, acc: 0.9333333373069763)
[2024-12-12 02:55:12,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:13,066][root][INFO] - Training Epoch: 7/10, step 307/574 completed (loss: 0.44937339425086975, acc: 0.9473684430122375)
[2024-12-12 02:55:13,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:13,422][root][INFO] - Training Epoch: 7/10, step 308/574 completed (loss: 0.7573688626289368, acc: 0.7681159377098083)
[2024-12-12 02:55:13,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:13,847][root][INFO] - Training Epoch: 7/10, step 309/574 completed (loss: 0.6770131587982178, acc: 0.8055555820465088)
[2024-12-12 02:55:13,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:14,225][root][INFO] - Training Epoch: 7/10, step 310/574 completed (loss: 0.7058767080307007, acc: 0.7710843086242676)
[2024-12-12 02:55:14,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:14,589][root][INFO] - Training Epoch: 7/10, step 311/574 completed (loss: 0.9250591993331909, acc: 0.7307692170143127)
[2024-12-12 02:55:14,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:14,944][root][INFO] - Training Epoch: 7/10, step 312/574 completed (loss: 1.045382022857666, acc: 0.6836734414100647)
[2024-12-12 02:55:15,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:15,246][root][INFO] - Training Epoch: 7/10, step 313/574 completed (loss: 0.00870581716299057, acc: 1.0)
[2024-12-12 02:55:15,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:15,539][root][INFO] - Training Epoch: 7/10, step 314/574 completed (loss: 0.033765289932489395, acc: 1.0)
[2024-12-12 02:55:15,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:15,908][root][INFO] - Training Epoch: 7/10, step 315/574 completed (loss: 0.07697343081235886, acc: 0.9677419066429138)
[2024-12-12 02:55:16,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:16,284][root][INFO] - Training Epoch: 7/10, step 316/574 completed (loss: 0.44796594977378845, acc: 0.9354838728904724)
[2024-12-12 02:55:16,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:16,632][root][INFO] - Training Epoch: 7/10, step 317/574 completed (loss: 0.37048497796058655, acc: 0.9253731369972229)
[2024-12-12 02:55:16,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:16,995][root][INFO] - Training Epoch: 7/10, step 318/574 completed (loss: 0.639143705368042, acc: 0.8365384340286255)
[2024-12-12 02:55:17,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:17,367][root][INFO] - Training Epoch: 7/10, step 319/574 completed (loss: 0.2146044373512268, acc: 0.9333333373069763)
[2024-12-12 02:55:17,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:17,720][root][INFO] - Training Epoch: 7/10, step 320/574 completed (loss: 0.12174954265356064, acc: 0.9838709831237793)
[2024-12-12 02:55:17,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:18,057][root][INFO] - Training Epoch: 7/10, step 321/574 completed (loss: 0.12134092301130295, acc: 0.9599999785423279)
[2024-12-12 02:55:18,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:18,391][root][INFO] - Training Epoch: 7/10, step 322/574 completed (loss: 0.32697540521621704, acc: 0.8888888955116272)
[2024-12-12 02:55:18,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:18,775][root][INFO] - Training Epoch: 7/10, step 323/574 completed (loss: 0.4715966284275055, acc: 0.8571428656578064)
[2024-12-12 02:55:18,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:19,152][root][INFO] - Training Epoch: 7/10, step 324/574 completed (loss: 0.4437252879142761, acc: 0.8461538553237915)
[2024-12-12 02:55:19,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:19,521][root][INFO] - Training Epoch: 7/10, step 325/574 completed (loss: 0.689254641532898, acc: 0.7560975551605225)
[2024-12-12 02:55:19,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:19,828][root][INFO] - Training Epoch: 7/10, step 326/574 completed (loss: 0.4964217245578766, acc: 0.7894737124443054)
[2024-12-12 02:55:19,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:20,125][root][INFO] - Training Epoch: 7/10, step 327/574 completed (loss: 0.07358409464359283, acc: 1.0)
[2024-12-12 02:55:20,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:20,424][root][INFO] - Training Epoch: 7/10, step 328/574 completed (loss: 0.04408147186040878, acc: 0.9642857313156128)
[2024-12-12 02:55:20,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:20,774][root][INFO] - Training Epoch: 7/10, step 329/574 completed (loss: 0.14040035009384155, acc: 0.9629629850387573)
[2024-12-12 02:55:20,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:21,164][root][INFO] - Training Epoch: 7/10, step 330/574 completed (loss: 0.1501065045595169, acc: 0.96875)
[2024-12-12 02:55:21,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:21,510][root][INFO] - Training Epoch: 7/10, step 331/574 completed (loss: 0.6944584846496582, acc: 0.8548387289047241)
[2024-12-12 02:55:21,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:21,905][root][INFO] - Training Epoch: 7/10, step 332/574 completed (loss: 0.4532241225242615, acc: 0.8771929740905762)
[2024-12-12 02:55:22,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:22,234][root][INFO] - Training Epoch: 7/10, step 333/574 completed (loss: 0.35000187158584595, acc: 0.90625)
[2024-12-12 02:55:22,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:22,582][root][INFO] - Training Epoch: 7/10, step 334/574 completed (loss: 0.13334441184997559, acc: 0.9666666388511658)
[2024-12-12 02:55:22,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:22,920][root][INFO] - Training Epoch: 7/10, step 335/574 completed (loss: 0.04692845419049263, acc: 1.0)
[2024-12-12 02:55:23,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:23,282][root][INFO] - Training Epoch: 7/10, step 336/574 completed (loss: 0.6082260608673096, acc: 0.8399999737739563)
[2024-12-12 02:55:23,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:23,621][root][INFO] - Training Epoch: 7/10, step 337/574 completed (loss: 1.1896004676818848, acc: 0.6666666865348816)
[2024-12-12 02:55:23,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:23,992][root][INFO] - Training Epoch: 7/10, step 338/574 completed (loss: 1.1531020402908325, acc: 0.6170212626457214)
[2024-12-12 02:55:24,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:24,316][root][INFO] - Training Epoch: 7/10, step 339/574 completed (loss: 1.3134937286376953, acc: 0.6144578456878662)
[2024-12-12 02:55:24,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:24,680][root][INFO] - Training Epoch: 7/10, step 340/574 completed (loss: 0.0034134851302951574, acc: 1.0)
[2024-12-12 02:55:24,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:24,983][root][INFO] - Training Epoch: 7/10, step 341/574 completed (loss: 0.136891171336174, acc: 0.9487179517745972)
[2024-12-12 02:55:25,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:25,348][root][INFO] - Training Epoch: 7/10, step 342/574 completed (loss: 0.8502932786941528, acc: 0.7228915691375732)
[2024-12-12 02:55:25,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:25,727][root][INFO] - Training Epoch: 7/10, step 343/574 completed (loss: 0.7335253357887268, acc: 0.7735849022865295)
[2024-12-12 02:55:25,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:26,098][root][INFO] - Training Epoch: 7/10, step 344/574 completed (loss: 0.3789319694042206, acc: 0.8860759735107422)
[2024-12-12 02:55:26,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:26,438][root][INFO] - Training Epoch: 7/10, step 345/574 completed (loss: 0.18116635084152222, acc: 0.9607843160629272)
[2024-12-12 02:55:26,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:26,793][root][INFO] - Training Epoch: 7/10, step 346/574 completed (loss: 0.8746359348297119, acc: 0.7313432693481445)
[2024-12-12 02:55:26,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:27,125][root][INFO] - Training Epoch: 7/10, step 347/574 completed (loss: 0.18851269781589508, acc: 0.949999988079071)
[2024-12-12 02:55:27,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:27,483][root][INFO] - Training Epoch: 7/10, step 348/574 completed (loss: 0.24101054668426514, acc: 0.9200000166893005)
[2024-12-12 02:55:27,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:27,891][root][INFO] - Training Epoch: 7/10, step 349/574 completed (loss: 0.4594977796077728, acc: 0.8611111044883728)
[2024-12-12 02:55:28,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:28,273][root][INFO] - Training Epoch: 7/10, step 350/574 completed (loss: 0.8624352812767029, acc: 0.6511628031730652)
[2024-12-12 02:55:28,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:28,654][root][INFO] - Training Epoch: 7/10, step 351/574 completed (loss: 0.38126689195632935, acc: 0.8717948794364929)
[2024-12-12 02:55:28,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:29,028][root][INFO] - Training Epoch: 7/10, step 352/574 completed (loss: 0.9698323011398315, acc: 0.7111111283302307)
[2024-12-12 02:55:29,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:29,369][root][INFO] - Training Epoch: 7/10, step 353/574 completed (loss: 0.010050429962575436, acc: 1.0)
[2024-12-12 02:55:29,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:29,725][root][INFO] - Training Epoch: 7/10, step 354/574 completed (loss: 0.1475674957036972, acc: 0.9230769276618958)
[2024-12-12 02:55:29,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:30,090][root][INFO] - Training Epoch: 7/10, step 355/574 completed (loss: 1.3588080406188965, acc: 0.6153846383094788)
[2024-12-12 02:55:30,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:30,586][root][INFO] - Training Epoch: 7/10, step 356/574 completed (loss: 1.1739556789398193, acc: 0.686956524848938)
[2024-12-12 02:55:30,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:30,954][root][INFO] - Training Epoch: 7/10, step 357/574 completed (loss: 0.7688248753547668, acc: 0.72826087474823)
[2024-12-12 02:55:31,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:31,298][root][INFO] - Training Epoch: 7/10, step 358/574 completed (loss: 0.592377781867981, acc: 0.7755101919174194)
[2024-12-12 02:55:31,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:31,623][root][INFO] - Training Epoch: 7/10, step 359/574 completed (loss: 0.09681984037160873, acc: 0.9583333134651184)
[2024-12-12 02:55:31,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:31,973][root][INFO] - Training Epoch: 7/10, step 360/574 completed (loss: 0.056254252791404724, acc: 1.0)
[2024-12-12 02:55:32,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:32,327][root][INFO] - Training Epoch: 7/10, step 361/574 completed (loss: 0.13814584910869598, acc: 0.9756097793579102)
[2024-12-12 02:55:32,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:32,698][root][INFO] - Training Epoch: 7/10, step 362/574 completed (loss: 0.560559868812561, acc: 0.8666666746139526)
[2024-12-12 02:55:32,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:33,073][root][INFO] - Training Epoch: 7/10, step 363/574 completed (loss: 0.4651353657245636, acc: 0.8157894611358643)
[2024-12-12 02:55:33,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:33,420][root][INFO] - Training Epoch: 7/10, step 364/574 completed (loss: 0.29680657386779785, acc: 0.9268292784690857)
[2024-12-12 02:55:33,503][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:33,737][root][INFO] - Training Epoch: 7/10, step 365/574 completed (loss: 0.27373218536376953, acc: 0.9090909361839294)
[2024-12-12 02:55:33,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:34,089][root][INFO] - Training Epoch: 7/10, step 366/574 completed (loss: 0.008994849398732185, acc: 1.0)
[2024-12-12 02:55:34,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:34,456][root][INFO] - Training Epoch: 7/10, step 367/574 completed (loss: 0.06828291714191437, acc: 1.0)
[2024-12-12 02:55:34,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:34,830][root][INFO] - Training Epoch: 7/10, step 368/574 completed (loss: 0.1168198436498642, acc: 0.9642857313156128)
[2024-12-12 02:55:34,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:35,131][root][INFO] - Training Epoch: 7/10, step 369/574 completed (loss: 0.1260560154914856, acc: 0.96875)
[2024-12-12 02:55:35,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:35,745][root][INFO] - Training Epoch: 7/10, step 370/574 completed (loss: 1.2342891693115234, acc: 0.6545454263687134)
[2024-12-12 02:55:36,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:36,632][root][INFO] - Training Epoch: 7/10, step 371/574 completed (loss: 0.7135726809501648, acc: 0.8301886916160583)
[2024-12-12 02:55:36,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:36,983][root][INFO] - Training Epoch: 7/10, step 372/574 completed (loss: 0.5439780354499817, acc: 0.8222222328186035)
[2024-12-12 02:55:37,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:37,349][root][INFO] - Training Epoch: 7/10, step 373/574 completed (loss: 0.4284243881702423, acc: 0.9107142686843872)
[2024-12-12 02:55:37,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:37,728][root][INFO] - Training Epoch: 7/10, step 374/574 completed (loss: 0.2715575397014618, acc: 0.9142857193946838)
[2024-12-12 02:55:37,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:38,099][root][INFO] - Training Epoch: 7/10, step 375/574 completed (loss: 0.014482816681265831, acc: 1.0)
[2024-12-12 02:55:38,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:38,399][root][INFO] - Training Epoch: 7/10, step 376/574 completed (loss: 0.03433218598365784, acc: 1.0)
[2024-12-12 02:55:38,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:38,761][root][INFO] - Training Epoch: 7/10, step 377/574 completed (loss: 0.3988361060619354, acc: 0.8125)
[2024-12-12 02:55:38,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:39,156][root][INFO] - Training Epoch: 7/10, step 378/574 completed (loss: 0.6954964995384216, acc: 0.8315789699554443)
[2024-12-12 02:55:39,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:39,739][root][INFO] - Training Epoch: 7/10, step 379/574 completed (loss: 1.1382031440734863, acc: 0.71856290102005)
[2024-12-12 02:55:39,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:40,114][root][INFO] - Training Epoch: 7/10, step 380/574 completed (loss: 0.8611336350440979, acc: 0.7819548845291138)
[2024-12-12 02:55:40,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:41,195][root][INFO] - Training Epoch: 7/10, step 381/574 completed (loss: 0.9899014830589294, acc: 0.7379679083824158)
[2024-12-12 02:55:41,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:41,768][root][INFO] - Training Epoch: 7/10, step 382/574 completed (loss: 0.7540302872657776, acc: 0.8108108043670654)
[2024-12-12 02:55:41,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:42,128][root][INFO] - Training Epoch: 7/10, step 383/574 completed (loss: 0.20709310472011566, acc: 0.9642857313156128)
[2024-12-12 02:55:42,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:42,497][root][INFO] - Training Epoch: 7/10, step 384/574 completed (loss: 0.08444612473249435, acc: 0.9642857313156128)
[2024-12-12 02:55:42,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:42,836][root][INFO] - Training Epoch: 7/10, step 385/574 completed (loss: 0.08433026075363159, acc: 0.96875)
[2024-12-12 02:55:42,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:43,136][root][INFO] - Training Epoch: 7/10, step 386/574 completed (loss: 0.028464078903198242, acc: 1.0)
[2024-12-12 02:55:43,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:43,451][root][INFO] - Training Epoch: 7/10, step 387/574 completed (loss: 0.349719762802124, acc: 0.9473684430122375)
[2024-12-12 02:55:43,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:43,780][root][INFO] - Training Epoch: 7/10, step 388/574 completed (loss: 0.011401423253118992, acc: 1.0)
[2024-12-12 02:55:43,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:44,147][root][INFO] - Training Epoch: 7/10, step 389/574 completed (loss: 0.030566131696105003, acc: 1.0)
[2024-12-12 02:55:44,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:44,486][root][INFO] - Training Epoch: 7/10, step 390/574 completed (loss: 0.24875229597091675, acc: 0.9523809552192688)
[2024-12-12 02:55:44,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:44,811][root][INFO] - Training Epoch: 7/10, step 391/574 completed (loss: 0.6539717316627502, acc: 0.7592592835426331)
[2024-12-12 02:55:44,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:45,162][root][INFO] - Training Epoch: 7/10, step 392/574 completed (loss: 1.197142243385315, acc: 0.6699029207229614)
[2024-12-12 02:55:45,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:45,723][root][INFO] - Training Epoch: 7/10, step 393/574 completed (loss: 1.1113462448120117, acc: 0.6764705777168274)
[2024-12-12 02:55:45,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:46,132][root][INFO] - Training Epoch: 7/10, step 394/574 completed (loss: 1.2260311841964722, acc: 0.6733333468437195)
[2024-12-12 02:55:46,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:46,532][root][INFO] - Training Epoch: 7/10, step 395/574 completed (loss: 1.3248926401138306, acc: 0.6388888955116272)
[2024-12-12 02:55:46,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:46,909][root][INFO] - Training Epoch: 7/10, step 396/574 completed (loss: 0.3738400638103485, acc: 0.930232584476471)
[2024-12-12 02:55:47,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:47,243][root][INFO] - Training Epoch: 7/10, step 397/574 completed (loss: 0.02597145549952984, acc: 1.0)
[2024-12-12 02:55:47,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:47,606][root][INFO] - Training Epoch: 7/10, step 398/574 completed (loss: 0.6750810146331787, acc: 0.7906976938247681)
[2024-12-12 02:55:47,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:47,969][root][INFO] - Training Epoch: 7/10, step 399/574 completed (loss: 0.15863172709941864, acc: 0.9599999785423279)
[2024-12-12 02:55:48,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:48,524][root][INFO] - Training Epoch: 7/10, step 400/574 completed (loss: 0.6581416130065918, acc: 0.8088235259056091)
[2024-12-12 02:55:48,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:48,854][root][INFO] - Training Epoch: 7/10, step 401/574 completed (loss: 0.6859114766120911, acc: 0.7599999904632568)
[2024-12-12 02:55:48,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:49,249][root][INFO] - Training Epoch: 7/10, step 402/574 completed (loss: 0.35472768545150757, acc: 0.8484848737716675)
[2024-12-12 02:55:49,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:49,611][root][INFO] - Training Epoch: 7/10, step 403/574 completed (loss: 0.12391733378171921, acc: 0.9696969985961914)
[2024-12-12 02:55:49,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:49,979][root][INFO] - Training Epoch: 7/10, step 404/574 completed (loss: 0.015330658294260502, acc: 1.0)
[2024-12-12 02:55:50,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:50,348][root][INFO] - Training Epoch: 7/10, step 405/574 completed (loss: 0.07596517354249954, acc: 0.9629629850387573)
[2024-12-12 02:55:50,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:50,711][root][INFO] - Training Epoch: 7/10, step 406/574 completed (loss: 0.03217669203877449, acc: 1.0)
[2024-12-12 02:55:50,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:51,086][root][INFO] - Training Epoch: 7/10, step 407/574 completed (loss: 0.11544454097747803, acc: 0.9722222089767456)
[2024-12-12 02:55:51,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:51,404][root][INFO] - Training Epoch: 7/10, step 408/574 completed (loss: 0.03780175745487213, acc: 1.0)
[2024-12-12 02:55:51,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:51,748][root][INFO] - Training Epoch: 7/10, step 409/574 completed (loss: 0.016583487391471863, acc: 1.0)
[2024-12-12 02:55:51,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:52,111][root][INFO] - Training Epoch: 7/10, step 410/574 completed (loss: 0.25618499517440796, acc: 0.9482758641242981)
[2024-12-12 02:55:52,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:52,508][root][INFO] - Training Epoch: 7/10, step 411/574 completed (loss: 0.05618984252214432, acc: 1.0)
[2024-12-12 02:55:52,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:52,879][root][INFO] - Training Epoch: 7/10, step 412/574 completed (loss: 0.4490490257740021, acc: 0.9333333373069763)
[2024-12-12 02:55:52,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:53,254][root][INFO] - Training Epoch: 7/10, step 413/574 completed (loss: 0.3230955898761749, acc: 0.8787878751754761)
[2024-12-12 02:55:53,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:53,595][root][INFO] - Training Epoch: 7/10, step 414/574 completed (loss: 0.008361663669347763, acc: 1.0)
[2024-12-12 02:55:53,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:53,928][root][INFO] - Training Epoch: 7/10, step 415/574 completed (loss: 0.4730253517627716, acc: 0.8627451062202454)
[2024-12-12 02:55:54,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:54,291][root][INFO] - Training Epoch: 7/10, step 416/574 completed (loss: 0.0726347416639328, acc: 1.0)
[2024-12-12 02:55:55,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:55,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:55,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:55,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:56,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:56,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:56,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:57,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:57,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:58,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:58,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:58,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:59,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:59,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:55:59,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:00,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:00,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:00,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:01,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:01,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:01,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:02,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:02,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:02,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:03,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:03,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:03,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:04,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:04,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:04,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:05,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:05,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:05,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:06,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:06,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:07,087][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:07,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:07,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:08,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:08,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:08,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:09,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:09,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:09,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:10,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:10,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:10,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:11,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:11,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:11,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:12,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:12,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:12,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:13,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:13,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:13,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:14,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:14,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:14,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:15,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:15,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:15,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:16,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:16,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:17,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:17,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:17,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:18,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:18,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:18,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:19,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:19,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:19,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:20,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:20,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:20,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:21,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:21,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:21,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:22,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:22,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:22,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:23,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:23,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:24,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:24,573][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.0616, device='cuda:0') eval_epoch_loss=tensor(1.4016, device='cuda:0') eval_epoch_acc=tensor(0.6710, device='cuda:0')
[2024-12-12 02:56:24,574][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:56:24,574][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:56:24,763][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_7_step_417_loss_1.4015816450119019/model.pt
[2024-12-12 02:56:24,766][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:56:24,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:25,143][root][INFO] - Training Epoch: 7/10, step 417/574 completed (loss: 0.16647176444530487, acc: 0.9444444179534912)
[2024-12-12 02:56:25,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:25,531][root][INFO] - Training Epoch: 7/10, step 418/574 completed (loss: 0.34092962741851807, acc: 0.8999999761581421)
[2024-12-12 02:56:25,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:25,897][root][INFO] - Training Epoch: 7/10, step 419/574 completed (loss: 0.044217657297849655, acc: 1.0)
[2024-12-12 02:56:26,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:26,253][root][INFO] - Training Epoch: 7/10, step 420/574 completed (loss: 0.03463069349527359, acc: 1.0)
[2024-12-12 02:56:26,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:26,537][root][INFO] - Training Epoch: 7/10, step 421/574 completed (loss: 0.42883941531181335, acc: 0.8999999761581421)
[2024-12-12 02:56:26,639][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:26,880][root][INFO] - Training Epoch: 7/10, step 422/574 completed (loss: 0.12270523607730865, acc: 0.9375)
[2024-12-12 02:56:26,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:27,241][root][INFO] - Training Epoch: 7/10, step 423/574 completed (loss: 0.20958667993545532, acc: 0.9722222089767456)
[2024-12-12 02:56:27,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:27,603][root][INFO] - Training Epoch: 7/10, step 424/574 completed (loss: 0.07098687440156937, acc: 1.0)
[2024-12-12 02:56:27,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:27,944][root][INFO] - Training Epoch: 7/10, step 425/574 completed (loss: 0.11539682745933533, acc: 0.9696969985961914)
[2024-12-12 02:56:28,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:28,245][root][INFO] - Training Epoch: 7/10, step 426/574 completed (loss: 0.00547728780657053, acc: 1.0)
[2024-12-12 02:56:28,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:28,603][root][INFO] - Training Epoch: 7/10, step 427/574 completed (loss: 0.12265021353960037, acc: 0.9729729890823364)
[2024-12-12 02:56:28,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:28,930][root][INFO] - Training Epoch: 7/10, step 428/574 completed (loss: 0.007256361655890942, acc: 1.0)
[2024-12-12 02:56:29,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:29,246][root][INFO] - Training Epoch: 7/10, step 429/574 completed (loss: 0.0758528932929039, acc: 0.95652174949646)
[2024-12-12 02:56:29,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:29,566][root][INFO] - Training Epoch: 7/10, step 430/574 completed (loss: 0.03496536612510681, acc: 1.0)
[2024-12-12 02:56:29,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:29,916][root][INFO] - Training Epoch: 7/10, step 431/574 completed (loss: 0.09250794351100922, acc: 0.9629629850387573)
[2024-12-12 02:56:30,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:30,236][root][INFO] - Training Epoch: 7/10, step 432/574 completed (loss: 0.0024557719007134438, acc: 1.0)
[2024-12-12 02:56:30,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:30,591][root][INFO] - Training Epoch: 7/10, step 433/574 completed (loss: 0.3399043381214142, acc: 0.8888888955116272)
[2024-12-12 02:56:30,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:30,951][root][INFO] - Training Epoch: 7/10, step 434/574 completed (loss: 0.006816238164901733, acc: 1.0)
[2024-12-12 02:56:31,061][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:31,313][root][INFO] - Training Epoch: 7/10, step 435/574 completed (loss: 0.22209207713603973, acc: 0.939393937587738)
[2024-12-12 02:56:31,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:31,668][root][INFO] - Training Epoch: 7/10, step 436/574 completed (loss: 0.105316661298275, acc: 0.9722222089767456)
[2024-12-12 02:56:31,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:32,053][root][INFO] - Training Epoch: 7/10, step 437/574 completed (loss: 0.23824165761470795, acc: 0.9090909361839294)
[2024-12-12 02:56:32,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:32,419][root][INFO] - Training Epoch: 7/10, step 438/574 completed (loss: 0.08635422587394714, acc: 0.9523809552192688)
[2024-12-12 02:56:32,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:32,730][root][INFO] - Training Epoch: 7/10, step 439/574 completed (loss: 0.32564088702201843, acc: 0.8974359035491943)
[2024-12-12 02:56:32,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:33,187][root][INFO] - Training Epoch: 7/10, step 440/574 completed (loss: 0.6643385291099548, acc: 0.8030303120613098)
[2024-12-12 02:56:33,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:33,913][root][INFO] - Training Epoch: 7/10, step 441/574 completed (loss: 1.3434686660766602, acc: 0.656000018119812)
[2024-12-12 02:56:34,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:34,310][root][INFO] - Training Epoch: 7/10, step 442/574 completed (loss: 1.2037297487258911, acc: 0.6935483813285828)
[2024-12-12 02:56:34,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:34,972][root][INFO] - Training Epoch: 7/10, step 443/574 completed (loss: 1.3310314416885376, acc: 0.6019900441169739)
[2024-12-12 02:56:35,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:35,335][root][INFO] - Training Epoch: 7/10, step 444/574 completed (loss: 0.4255131483078003, acc: 0.849056601524353)
[2024-12-12 02:56:35,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:35,751][root][INFO] - Training Epoch: 7/10, step 445/574 completed (loss: 0.19677262008190155, acc: 0.9772727489471436)
[2024-12-12 02:56:35,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:36,092][root][INFO] - Training Epoch: 7/10, step 446/574 completed (loss: 0.01644907332956791, acc: 1.0)
[2024-12-12 02:56:36,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:36,482][root][INFO] - Training Epoch: 7/10, step 447/574 completed (loss: 0.1614047735929489, acc: 0.9615384340286255)
[2024-12-12 02:56:36,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:36,852][root][INFO] - Training Epoch: 7/10, step 448/574 completed (loss: 0.01881508342921734, acc: 1.0)
[2024-12-12 02:56:36,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:37,229][root][INFO] - Training Epoch: 7/10, step 449/574 completed (loss: 0.2705497145652771, acc: 0.9253731369972229)
[2024-12-12 02:56:37,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:37,615][root][INFO] - Training Epoch: 7/10, step 450/574 completed (loss: 0.31667548418045044, acc: 0.875)
[2024-12-12 02:56:37,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:37,961][root][INFO] - Training Epoch: 7/10, step 451/574 completed (loss: 0.453253835439682, acc: 0.8260869383811951)
[2024-12-12 02:56:38,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:38,311][root][INFO] - Training Epoch: 7/10, step 452/574 completed (loss: 0.5833997130393982, acc: 0.8589743375778198)
[2024-12-12 02:56:38,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:38,632][root][INFO] - Training Epoch: 7/10, step 453/574 completed (loss: 0.6611500978469849, acc: 0.7763158082962036)
[2024-12-12 02:56:38,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:38,996][root][INFO] - Training Epoch: 7/10, step 454/574 completed (loss: 0.5439563393592834, acc: 0.8163265585899353)
[2024-12-12 02:56:39,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:39,336][root][INFO] - Training Epoch: 7/10, step 455/574 completed (loss: 0.12786079943180084, acc: 0.9696969985961914)
[2024-12-12 02:56:39,419][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:39,686][root][INFO] - Training Epoch: 7/10, step 456/574 completed (loss: 1.249098539352417, acc: 0.6082473993301392)
[2024-12-12 02:56:39,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:40,032][root][INFO] - Training Epoch: 7/10, step 457/574 completed (loss: 0.40876132249832153, acc: 0.8857142925262451)
[2024-12-12 02:56:40,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:40,426][root][INFO] - Training Epoch: 7/10, step 458/574 completed (loss: 1.086073398590088, acc: 0.6976743936538696)
[2024-12-12 02:56:40,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:40,793][root][INFO] - Training Epoch: 7/10, step 459/574 completed (loss: 0.3978862464427948, acc: 0.8571428656578064)
[2024-12-12 02:56:40,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:41,134][root][INFO] - Training Epoch: 7/10, step 460/574 completed (loss: 0.7073251008987427, acc: 0.7777777910232544)
[2024-12-12 02:56:41,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:41,452][root][INFO] - Training Epoch: 7/10, step 461/574 completed (loss: 0.15828388929367065, acc: 1.0)
[2024-12-12 02:56:41,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:41,757][root][INFO] - Training Epoch: 7/10, step 462/574 completed (loss: 0.07147017866373062, acc: 1.0)
[2024-12-12 02:56:41,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:42,095][root][INFO] - Training Epoch: 7/10, step 463/574 completed (loss: 0.01960107870399952, acc: 1.0)
[2024-12-12 02:56:42,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:42,424][root][INFO] - Training Epoch: 7/10, step 464/574 completed (loss: 0.14401842653751373, acc: 0.97826087474823)
[2024-12-12 02:56:42,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:42,749][root][INFO] - Training Epoch: 7/10, step 465/574 completed (loss: 0.5172078609466553, acc: 0.8452380895614624)
[2024-12-12 02:56:42,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:43,060][root][INFO] - Training Epoch: 7/10, step 466/574 completed (loss: 0.818946361541748, acc: 0.7831325531005859)
[2024-12-12 02:56:43,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:43,447][root][INFO] - Training Epoch: 7/10, step 467/574 completed (loss: 0.6837278008460999, acc: 0.8108108043670654)
[2024-12-12 02:56:43,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:43,821][root][INFO] - Training Epoch: 7/10, step 468/574 completed (loss: 0.9903081655502319, acc: 0.7184466123580933)
[2024-12-12 02:56:43,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:44,191][root][INFO] - Training Epoch: 7/10, step 469/574 completed (loss: 0.6969768404960632, acc: 0.8292682766914368)
[2024-12-12 02:56:44,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:44,553][root][INFO] - Training Epoch: 7/10, step 470/574 completed (loss: 0.021333828568458557, acc: 1.0)
[2024-12-12 02:56:44,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:44,921][root][INFO] - Training Epoch: 7/10, step 471/574 completed (loss: 0.09281878173351288, acc: 1.0)
[2024-12-12 02:56:45,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:45,326][root][INFO] - Training Epoch: 7/10, step 472/574 completed (loss: 1.1115676164627075, acc: 0.6470588445663452)
[2024-12-12 02:56:45,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:45,736][root][INFO] - Training Epoch: 7/10, step 473/574 completed (loss: 1.2837918996810913, acc: 0.624454140663147)
[2024-12-12 02:56:45,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:46,142][root][INFO] - Training Epoch: 7/10, step 474/574 completed (loss: 0.7013909816741943, acc: 0.7604166865348816)
[2024-12-12 02:56:46,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:46,529][root][INFO] - Training Epoch: 7/10, step 475/574 completed (loss: 1.0289779901504517, acc: 0.7116564512252808)
[2024-12-12 02:56:46,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:46,868][root][INFO] - Training Epoch: 7/10, step 476/574 completed (loss: 0.9845818281173706, acc: 0.7266187071800232)
[2024-12-12 02:56:46,968][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:47,218][root][INFO] - Training Epoch: 7/10, step 477/574 completed (loss: 1.298927664756775, acc: 0.6532663106918335)
[2024-12-12 02:56:47,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:47,561][root][INFO] - Training Epoch: 7/10, step 478/574 completed (loss: 0.2498655915260315, acc: 0.9444444179534912)
[2024-12-12 02:56:47,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:47,870][root][INFO] - Training Epoch: 7/10, step 479/574 completed (loss: 0.19112388789653778, acc: 0.939393937587738)
[2024-12-12 02:56:47,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:48,163][root][INFO] - Training Epoch: 7/10, step 480/574 completed (loss: 0.09177538007497787, acc: 0.9629629850387573)
[2024-12-12 02:56:48,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:48,476][root][INFO] - Training Epoch: 7/10, step 481/574 completed (loss: 0.12329313904047012, acc: 0.8999999761581421)
[2024-12-12 02:56:48,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:48,827][root][INFO] - Training Epoch: 7/10, step 482/574 completed (loss: 0.06121676415205002, acc: 1.0)
[2024-12-12 02:56:48,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:49,208][root][INFO] - Training Epoch: 7/10, step 483/574 completed (loss: 0.7144660949707031, acc: 0.7758620977401733)
[2024-12-12 02:56:49,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:49,561][root][INFO] - Training Epoch: 7/10, step 484/574 completed (loss: 0.042029231786727905, acc: 1.0)
[2024-12-12 02:56:49,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:49,926][root][INFO] - Training Epoch: 7/10, step 485/574 completed (loss: 0.08898843079805374, acc: 0.9473684430122375)
[2024-12-12 02:56:50,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:50,309][root][INFO] - Training Epoch: 7/10, step 486/574 completed (loss: 0.191180020570755, acc: 0.9629629850387573)
[2024-12-12 02:56:50,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:50,666][root][INFO] - Training Epoch: 7/10, step 487/574 completed (loss: 0.1503954380750656, acc: 0.9523809552192688)
[2024-12-12 02:56:50,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:51,043][root][INFO] - Training Epoch: 7/10, step 488/574 completed (loss: 0.0609872080385685, acc: 0.9545454382896423)
[2024-12-12 02:56:51,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:51,419][root][INFO] - Training Epoch: 7/10, step 489/574 completed (loss: 0.9511474967002869, acc: 0.7538461685180664)
[2024-12-12 02:56:51,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:51,758][root][INFO] - Training Epoch: 7/10, step 490/574 completed (loss: 0.16313938796520233, acc: 0.9333333373069763)
[2024-12-12 02:56:51,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:52,137][root][INFO] - Training Epoch: 7/10, step 491/574 completed (loss: 0.07024730741977692, acc: 1.0)
[2024-12-12 02:56:52,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:52,511][root][INFO] - Training Epoch: 7/10, step 492/574 completed (loss: 0.46316200494766235, acc: 0.8627451062202454)
[2024-12-12 02:56:52,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:52,866][root][INFO] - Training Epoch: 7/10, step 493/574 completed (loss: 0.3814602792263031, acc: 0.9655172228813171)
[2024-12-12 02:56:52,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:53,237][root][INFO] - Training Epoch: 7/10, step 494/574 completed (loss: 0.07103466242551804, acc: 1.0)
[2024-12-12 02:56:53,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:53,570][root][INFO] - Training Epoch: 7/10, step 495/574 completed (loss: 0.41590604186058044, acc: 0.8421052694320679)
[2024-12-12 02:56:53,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:53,922][root][INFO] - Training Epoch: 7/10, step 496/574 completed (loss: 1.1336334943771362, acc: 0.7321428656578064)
[2024-12-12 02:56:54,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:54,293][root][INFO] - Training Epoch: 7/10, step 497/574 completed (loss: 0.883975088596344, acc: 0.7415730357170105)
[2024-12-12 02:56:54,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:54,667][root][INFO] - Training Epoch: 7/10, step 498/574 completed (loss: 1.0958131551742554, acc: 0.6966292262077332)
[2024-12-12 02:56:54,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:54,996][root][INFO] - Training Epoch: 7/10, step 499/574 completed (loss: 1.7038394212722778, acc: 0.5177304744720459)
[2024-12-12 02:56:55,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:55,303][root][INFO] - Training Epoch: 7/10, step 500/574 completed (loss: 1.3538100719451904, acc: 0.6630434989929199)
[2024-12-12 02:56:55,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:55,594][root][INFO] - Training Epoch: 7/10, step 501/574 completed (loss: 0.005850243382155895, acc: 1.0)
[2024-12-12 02:56:55,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:55,888][root][INFO] - Training Epoch: 7/10, step 502/574 completed (loss: 0.014875072985887527, acc: 1.0)
[2024-12-12 02:56:56,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:56,252][root][INFO] - Training Epoch: 7/10, step 503/574 completed (loss: 0.09036126732826233, acc: 0.9629629850387573)
[2024-12-12 02:56:56,372][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:56,596][root][INFO] - Training Epoch: 7/10, step 504/574 completed (loss: 0.10117674618959427, acc: 1.0)
[2024-12-12 02:56:56,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:56,940][root][INFO] - Training Epoch: 7/10, step 505/574 completed (loss: 0.2740423381328583, acc: 0.8867924809455872)
[2024-12-12 02:56:57,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:57,291][root][INFO] - Training Epoch: 7/10, step 506/574 completed (loss: 0.44406768679618835, acc: 0.8620689511299133)
[2024-12-12 02:56:57,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:57,876][root][INFO] - Training Epoch: 7/10, step 507/574 completed (loss: 1.345496654510498, acc: 0.630630612373352)
[2024-12-12 02:56:57,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:58,315][root][INFO] - Training Epoch: 7/10, step 508/574 completed (loss: 0.7804844379425049, acc: 0.7746478915214539)
[2024-12-12 02:56:58,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:58,670][root][INFO] - Training Epoch: 7/10, step 509/574 completed (loss: 0.0032162461429834366, acc: 1.0)
[2024-12-12 02:56:58,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:59,008][root][INFO] - Training Epoch: 7/10, step 510/574 completed (loss: 0.06485798209905624, acc: 1.0)
[2024-12-12 02:56:59,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:56:59,334][root][INFO] - Training Epoch: 7/10, step 511/574 completed (loss: 0.15063722431659698, acc: 0.9230769276618958)
[2024-12-12 02:57:00,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:02,163][root][INFO] - Training Epoch: 7/10, step 512/574 completed (loss: 1.5822514295578003, acc: 0.550000011920929)
[2024-12-12 02:57:02,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:02,927][root][INFO] - Training Epoch: 7/10, step 513/574 completed (loss: 1.0384999513626099, acc: 0.6746031641960144)
[2024-12-12 02:57:03,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:03,291][root][INFO] - Training Epoch: 7/10, step 514/574 completed (loss: 0.39783191680908203, acc: 0.9285714030265808)
[2024-12-12 02:57:03,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:03,651][root][INFO] - Training Epoch: 7/10, step 515/574 completed (loss: 0.4774910509586334, acc: 0.8333333134651184)
[2024-12-12 02:57:03,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:04,346][root][INFO] - Training Epoch: 7/10, step 516/574 completed (loss: 0.7414780259132385, acc: 0.7777777910232544)
[2024-12-12 02:57:04,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:04,713][root][INFO] - Training Epoch: 7/10, step 517/574 completed (loss: 0.003966913092881441, acc: 1.0)
[2024-12-12 02:57:04,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:05,062][root][INFO] - Training Epoch: 7/10, step 518/574 completed (loss: 0.03777683898806572, acc: 1.0)
[2024-12-12 02:57:05,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:05,446][root][INFO] - Training Epoch: 7/10, step 519/574 completed (loss: 0.08191101998090744, acc: 1.0)
[2024-12-12 02:57:05,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:05,811][root][INFO] - Training Epoch: 7/10, step 520/574 completed (loss: 0.20149536430835724, acc: 0.9259259104728699)
[2024-12-12 02:57:06,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:06,803][root][INFO] - Training Epoch: 7/10, step 521/574 completed (loss: 1.6775063276290894, acc: 0.5338982939720154)
[2024-12-12 02:57:06,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:07,196][root][INFO] - Training Epoch: 7/10, step 522/574 completed (loss: 0.8792089819908142, acc: 0.7388059496879578)
[2024-12-12 02:57:07,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:07,610][root][INFO] - Training Epoch: 7/10, step 523/574 completed (loss: 1.0423027276992798, acc: 0.7007299065589905)
[2024-12-12 02:57:07,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:08,213][root][INFO] - Training Epoch: 7/10, step 524/574 completed (loss: 1.313586711883545, acc: 0.6299999952316284)
[2024-12-12 02:57:08,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:08,544][root][INFO] - Training Epoch: 7/10, step 525/574 completed (loss: 0.12773871421813965, acc: 0.9629629850387573)
[2024-12-12 02:57:08,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:08,928][root][INFO] - Training Epoch: 7/10, step 526/574 completed (loss: 0.2959792912006378, acc: 0.9038461446762085)
[2024-12-12 02:57:09,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:09,257][root][INFO] - Training Epoch: 7/10, step 527/574 completed (loss: 0.16611382365226746, acc: 0.9523809552192688)
[2024-12-12 02:57:09,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:09,640][root][INFO] - Training Epoch: 7/10, step 528/574 completed (loss: 0.6229463815689087, acc: 0.7868852615356445)
[2024-12-12 02:57:09,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:09,968][root][INFO] - Training Epoch: 7/10, step 529/574 completed (loss: 0.5680257678031921, acc: 0.8305084705352783)
[2024-12-12 02:57:10,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:10,320][root][INFO] - Training Epoch: 7/10, step 530/574 completed (loss: 0.7160971760749817, acc: 0.7906976938247681)
[2024-12-12 02:57:10,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:10,708][root][INFO] - Training Epoch: 7/10, step 531/574 completed (loss: 0.33557623624801636, acc: 0.8863636255264282)
[2024-12-12 02:57:10,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:11,052][root][INFO] - Training Epoch: 7/10, step 532/574 completed (loss: 0.44995883107185364, acc: 0.8301886916160583)
[2024-12-12 02:57:11,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:11,370][root][INFO] - Training Epoch: 7/10, step 533/574 completed (loss: 0.27907586097717285, acc: 0.9772727489471436)
[2024-12-12 02:57:11,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:11,738][root][INFO] - Training Epoch: 7/10, step 534/574 completed (loss: 0.07144804298877716, acc: 0.9599999785423279)
[2024-12-12 02:57:11,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:12,080][root][INFO] - Training Epoch: 7/10, step 535/574 completed (loss: 0.29428988695144653, acc: 0.8500000238418579)
[2024-12-12 02:57:12,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:12,378][root][INFO] - Training Epoch: 7/10, step 536/574 completed (loss: 0.028635285794734955, acc: 1.0)
[2024-12-12 02:57:12,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:12,796][root][INFO] - Training Epoch: 7/10, step 537/574 completed (loss: 0.42365288734436035, acc: 0.8615384697914124)
[2024-12-12 02:57:12,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:13,181][root][INFO] - Training Epoch: 7/10, step 538/574 completed (loss: 0.6800920367240906, acc: 0.828125)
[2024-12-12 02:57:13,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:13,579][root][INFO] - Training Epoch: 7/10, step 539/574 completed (loss: 0.191380575299263, acc: 0.90625)
[2024-12-12 02:57:13,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:13,933][root][INFO] - Training Epoch: 7/10, step 540/574 completed (loss: 0.4083271324634552, acc: 0.8787878751754761)
[2024-12-12 02:57:14,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:14,274][root][INFO] - Training Epoch: 7/10, step 541/574 completed (loss: 0.028086217120289803, acc: 1.0)
[2024-12-12 02:57:14,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:14,581][root][INFO] - Training Epoch: 7/10, step 542/574 completed (loss: 0.03540920838713646, acc: 1.0)
[2024-12-12 02:57:14,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:14,956][root][INFO] - Training Epoch: 7/10, step 543/574 completed (loss: 0.008794030174612999, acc: 1.0)
[2024-12-12 02:57:15,088][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:15,354][root][INFO] - Training Epoch: 7/10, step 544/574 completed (loss: 0.1611892580986023, acc: 0.9333333373069763)
[2024-12-12 02:57:15,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:15,734][root][INFO] - Training Epoch: 7/10, step 545/574 completed (loss: 0.10445068031549454, acc: 0.9756097793579102)
[2024-12-12 02:57:15,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:16,094][root][INFO] - Training Epoch: 7/10, step 546/574 completed (loss: 0.04598857834935188, acc: 1.0)
[2024-12-12 02:57:16,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:16,459][root][INFO] - Training Epoch: 7/10, step 547/574 completed (loss: 0.2921852171421051, acc: 0.9473684430122375)
[2024-12-12 02:57:16,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:16,839][root][INFO] - Training Epoch: 7/10, step 548/574 completed (loss: 0.030583392828702927, acc: 1.0)
[2024-12-12 02:57:16,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:17,203][root][INFO] - Training Epoch: 7/10, step 549/574 completed (loss: 0.04404694214463234, acc: 1.0)
[2024-12-12 02:57:17,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:17,543][root][INFO] - Training Epoch: 7/10, step 550/574 completed (loss: 0.12393680959939957, acc: 0.939393937587738)
[2024-12-12 02:57:17,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:17,900][root][INFO] - Training Epoch: 7/10, step 551/574 completed (loss: 0.29707202315330505, acc: 0.949999988079071)
[2024-12-12 02:57:18,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:18,248][root][INFO] - Training Epoch: 7/10, step 552/574 completed (loss: 0.1755836009979248, acc: 0.9428571462631226)
[2024-12-12 02:57:18,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:18,610][root][INFO] - Training Epoch: 7/10, step 553/574 completed (loss: 0.6935498118400574, acc: 0.8029196858406067)
[2024-12-12 02:57:18,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:18,948][root][INFO] - Training Epoch: 7/10, step 554/574 completed (loss: 0.7057398557662964, acc: 0.7793103456497192)
[2024-12-12 02:57:19,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:19,321][root][INFO] - Training Epoch: 7/10, step 555/574 completed (loss: 0.9830807447433472, acc: 0.7428571581840515)
[2024-12-12 02:57:19,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:19,693][root][INFO] - Training Epoch: 7/10, step 556/574 completed (loss: 0.9130026698112488, acc: 0.7682119011878967)
[2024-12-12 02:57:19,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:19,998][root][INFO] - Training Epoch: 7/10, step 557/574 completed (loss: 0.4371989369392395, acc: 0.8717948794364929)
[2024-12-12 02:57:20,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:20,348][root][INFO] - Training Epoch: 7/10, step 558/574 completed (loss: 0.009376107715070248, acc: 1.0)
[2024-12-12 02:57:20,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:20,683][root][INFO] - Training Epoch: 7/10, step 559/574 completed (loss: 0.17905983328819275, acc: 0.9230769276618958)
[2024-12-12 02:57:21,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:21,878][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:22,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:22,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:23,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:23,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:23,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:24,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:24,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:25,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:25,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:25,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:26,093][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:26,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:26,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:27,183][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:27,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:27,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:28,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:28,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:28,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:28,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:29,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:29,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:30,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:30,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:30,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:31,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:31,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:31,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:31,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:32,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:32,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:33,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:33,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:33,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:34,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:34,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:34,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:35,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:35,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:35,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:36,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:36,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:36,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:37,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:37,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:38,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:38,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:38,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:39,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:39,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:39,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:40,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:40,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:40,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:41,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:41,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:42,042][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:42,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:42,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:43,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:43,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:43,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:44,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:44,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:44,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:45,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:45,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:46,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:46,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:46,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:47,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:47,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:47,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:48,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:48,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:48,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:49,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:49,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:49,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:50,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:50,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:50,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:51,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:51,696][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.8902, device='cuda:0') eval_epoch_loss=tensor(1.3585, device='cuda:0') eval_epoch_acc=tensor(0.6927, device='cuda:0')
[2024-12-12 02:57:51,698][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:57:51,698][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:57:51,973][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_7_step_560_loss_1.3584641218185425/model.pt
[2024-12-12 02:57:51,976][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:57:51,977][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 7 is 0.6926641464233398
[2024-12-12 02:57:52,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:52,287][root][INFO] - Training Epoch: 7/10, step 560/574 completed (loss: 0.07527877390384674, acc: 0.9615384340286255)
[2024-12-12 02:57:52,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:52,657][root][INFO] - Training Epoch: 7/10, step 561/574 completed (loss: 0.10684499889612198, acc: 1.0)
[2024-12-12 02:57:52,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:53,003][root][INFO] - Training Epoch: 7/10, step 562/574 completed (loss: 0.5711147785186768, acc: 0.8222222328186035)
[2024-12-12 02:57:53,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:53,362][root][INFO] - Training Epoch: 7/10, step 563/574 completed (loss: 0.37421366572380066, acc: 0.8961039185523987)
[2024-12-12 02:57:53,462][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:53,742][root][INFO] - Training Epoch: 7/10, step 564/574 completed (loss: 0.18983519077301025, acc: 0.9583333134651184)
[2024-12-12 02:57:53,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:54,106][root][INFO] - Training Epoch: 7/10, step 565/574 completed (loss: 0.08348139375448227, acc: 0.9655172228813171)
[2024-12-12 02:57:54,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:54,459][root][INFO] - Training Epoch: 7/10, step 566/574 completed (loss: 0.5182317495346069, acc: 0.8690476417541504)
[2024-12-12 02:57:54,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:54,791][root][INFO] - Training Epoch: 7/10, step 567/574 completed (loss: 0.049280568957328796, acc: 0.9736841917037964)
[2024-12-12 02:57:54,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:55,150][root][INFO] - Training Epoch: 7/10, step 568/574 completed (loss: 0.1724633276462555, acc: 0.9259259104728699)
[2024-12-12 02:57:55,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:55,546][root][INFO] - Training Epoch: 7/10, step 569/574 completed (loss: 0.9164091944694519, acc: 0.7540106773376465)
[2024-12-12 02:57:55,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:55,831][root][INFO] - Training Epoch: 7/10, step 570/574 completed (loss: 0.26896852254867554, acc: 0.9193548560142517)
[2024-12-12 02:57:55,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:56,202][root][INFO] - Training Epoch: 7/10, step 571/574 completed (loss: 0.4508408010005951, acc: 0.8717948794364929)
[2024-12-12 02:57:56,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:56,532][root][INFO] - Training Epoch: 7/10, step 572/574 completed (loss: 1.219607949256897, acc: 0.6785714030265808)
[2024-12-12 02:57:56,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:56,869][root][INFO] - Training Epoch: 7/10, step 573/574 completed (loss: 1.1109333038330078, acc: 0.6540880799293518)
[2024-12-12 02:57:57,271][slam_llm.utils.train_utils][INFO] - Epoch 7: train_perplexity=1.7265, train_epoch_loss=0.5461, epoch time 354.34492379426956s
[2024-12-12 02:57:57,271][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 02:57:57,272][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2024-12-12 02:57:57,272][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 02:57:57,272][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 19
[2024-12-12 02:57:57,272][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 02:57:57,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:58,204][root][INFO] - Training Epoch: 8/10, step 0/574 completed (loss: 0.07800019532442093, acc: 0.9629629850387573)
[2024-12-12 02:57:58,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:58,618][root][INFO] - Training Epoch: 8/10, step 1/574 completed (loss: 0.1337835192680359, acc: 1.0)
[2024-12-12 02:57:58,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:58,991][root][INFO] - Training Epoch: 8/10, step 2/574 completed (loss: 0.21456590294837952, acc: 0.9459459185600281)
[2024-12-12 02:57:59,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:59,413][root][INFO] - Training Epoch: 8/10, step 3/574 completed (loss: 0.07844742387533188, acc: 0.9736841917037964)
[2024-12-12 02:57:59,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:57:59,755][root][INFO] - Training Epoch: 8/10, step 4/574 completed (loss: 0.25610673427581787, acc: 0.9459459185600281)
[2024-12-12 02:57:59,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:00,131][root][INFO] - Training Epoch: 8/10, step 5/574 completed (loss: 0.04767323285341263, acc: 1.0)
[2024-12-12 02:58:00,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:00,488][root][INFO] - Training Epoch: 8/10, step 6/574 completed (loss: 0.300403892993927, acc: 0.8979591727256775)
[2024-12-12 02:58:00,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:00,795][root][INFO] - Training Epoch: 8/10, step 7/574 completed (loss: 0.23863662779331207, acc: 0.8666666746139526)
[2024-12-12 02:58:00,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:01,134][root][INFO] - Training Epoch: 8/10, step 8/574 completed (loss: 0.006024478003382683, acc: 1.0)
[2024-12-12 02:58:01,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:01,478][root][INFO] - Training Epoch: 8/10, step 9/574 completed (loss: 0.030830394476652145, acc: 1.0)
[2024-12-12 02:58:01,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:01,814][root][INFO] - Training Epoch: 8/10, step 10/574 completed (loss: 0.03982184827327728, acc: 0.9629629850387573)
[2024-12-12 02:58:01,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:02,200][root][INFO] - Training Epoch: 8/10, step 11/574 completed (loss: 0.21097706258296967, acc: 0.9743589758872986)
[2024-12-12 02:58:02,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:02,538][root][INFO] - Training Epoch: 8/10, step 12/574 completed (loss: 0.18426655232906342, acc: 0.939393937587738)
[2024-12-12 02:58:02,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:02,883][root][INFO] - Training Epoch: 8/10, step 13/574 completed (loss: 0.10081354528665543, acc: 1.0)
[2024-12-12 02:58:03,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:03,289][root][INFO] - Training Epoch: 8/10, step 14/574 completed (loss: 0.24609027802944183, acc: 0.9411764740943909)
[2024-12-12 02:58:03,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:03,643][root][INFO] - Training Epoch: 8/10, step 15/574 completed (loss: 0.21661612391471863, acc: 0.918367326259613)
[2024-12-12 02:58:03,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:03,989][root][INFO] - Training Epoch: 8/10, step 16/574 completed (loss: 0.07138218730688095, acc: 0.9473684430122375)
[2024-12-12 02:58:04,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:04,324][root][INFO] - Training Epoch: 8/10, step 17/574 completed (loss: 0.01772436872124672, acc: 1.0)
[2024-12-12 02:58:04,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:04,662][root][INFO] - Training Epoch: 8/10, step 18/574 completed (loss: 0.17915338277816772, acc: 0.9166666865348816)
[2024-12-12 02:58:04,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:05,040][root][INFO] - Training Epoch: 8/10, step 19/574 completed (loss: 0.29799121618270874, acc: 0.8947368264198303)
[2024-12-12 02:58:05,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:05,377][root][INFO] - Training Epoch: 8/10, step 20/574 completed (loss: 0.13760915398597717, acc: 0.9615384340286255)
[2024-12-12 02:58:05,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:05,752][root][INFO] - Training Epoch: 8/10, step 21/574 completed (loss: 0.04446134716272354, acc: 1.0)
[2024-12-12 02:58:05,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:06,106][root][INFO] - Training Epoch: 8/10, step 22/574 completed (loss: 0.05944950133562088, acc: 1.0)
[2024-12-12 02:58:06,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:06,491][root][INFO] - Training Epoch: 8/10, step 23/574 completed (loss: 0.007823443040251732, acc: 1.0)
[2024-12-12 02:58:06,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:06,873][root][INFO] - Training Epoch: 8/10, step 24/574 completed (loss: 0.0777597427368164, acc: 0.9375)
[2024-12-12 02:58:07,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:07,274][root][INFO] - Training Epoch: 8/10, step 25/574 completed (loss: 0.4428812563419342, acc: 0.8867924809455872)
[2024-12-12 02:58:07,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:07,676][root][INFO] - Training Epoch: 8/10, step 26/574 completed (loss: 0.6597363948822021, acc: 0.8219178318977356)
[2024-12-12 02:58:08,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:08,988][root][INFO] - Training Epoch: 8/10, step 27/574 completed (loss: 1.6778072118759155, acc: 0.529644250869751)
[2024-12-12 02:58:09,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:09,316][root][INFO] - Training Epoch: 8/10, step 28/574 completed (loss: 0.17277298867702484, acc: 0.930232584476471)
[2024-12-12 02:58:09,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:09,677][root][INFO] - Training Epoch: 8/10, step 29/574 completed (loss: 0.6077905297279358, acc: 0.7951807379722595)
[2024-12-12 02:58:09,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:10,051][root][INFO] - Training Epoch: 8/10, step 30/574 completed (loss: 0.7411860823631287, acc: 0.7777777910232544)
[2024-12-12 02:58:10,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:10,410][root][INFO] - Training Epoch: 8/10, step 31/574 completed (loss: 0.1299201250076294, acc: 0.9642857313156128)
[2024-12-12 02:58:10,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:10,752][root][INFO] - Training Epoch: 8/10, step 32/574 completed (loss: 0.05834410712122917, acc: 0.9629629850387573)
[2024-12-12 02:58:10,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:11,136][root][INFO] - Training Epoch: 8/10, step 33/574 completed (loss: 0.5051589012145996, acc: 0.95652174949646)
[2024-12-12 02:58:11,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:11,497][root][INFO] - Training Epoch: 8/10, step 34/574 completed (loss: 0.7841854691505432, acc: 0.7394958138465881)
[2024-12-12 02:58:11,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:11,840][root][INFO] - Training Epoch: 8/10, step 35/574 completed (loss: 0.5398181080818176, acc: 0.8360655903816223)
[2024-12-12 02:58:11,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:12,208][root][INFO] - Training Epoch: 8/10, step 36/574 completed (loss: 0.5712552666664124, acc: 0.8571428656578064)
[2024-12-12 02:58:12,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:12,588][root][INFO] - Training Epoch: 8/10, step 37/574 completed (loss: 0.3146252930164337, acc: 0.8983050584793091)
[2024-12-12 02:58:12,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:12,987][root][INFO] - Training Epoch: 8/10, step 38/574 completed (loss: 0.6044374108314514, acc: 0.8045976758003235)
[2024-12-12 02:58:13,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:13,370][root][INFO] - Training Epoch: 8/10, step 39/574 completed (loss: 0.4629635512828827, acc: 0.9047619104385376)
[2024-12-12 02:58:13,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:13,702][root][INFO] - Training Epoch: 8/10, step 40/574 completed (loss: 0.1513313204050064, acc: 0.9615384340286255)
[2024-12-12 02:58:13,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:14,075][root][INFO] - Training Epoch: 8/10, step 41/574 completed (loss: 0.6750233769416809, acc: 0.8108108043670654)
[2024-12-12 02:58:14,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:14,478][root][INFO] - Training Epoch: 8/10, step 42/574 completed (loss: 0.7397979497909546, acc: 0.7538461685180664)
[2024-12-12 02:58:14,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:14,946][root][INFO] - Training Epoch: 8/10, step 43/574 completed (loss: 0.9493722915649414, acc: 0.7272727489471436)
[2024-12-12 02:58:15,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:15,387][root][INFO] - Training Epoch: 8/10, step 44/574 completed (loss: 0.7187182903289795, acc: 0.8144329786300659)
[2024-12-12 02:58:15,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:15,796][root][INFO] - Training Epoch: 8/10, step 45/574 completed (loss: 1.0837671756744385, acc: 0.625)
[2024-12-12 02:58:15,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:16,178][root][INFO] - Training Epoch: 8/10, step 46/574 completed (loss: 0.0687425434589386, acc: 0.9615384340286255)
[2024-12-12 02:58:16,286][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:16,557][root][INFO] - Training Epoch: 8/10, step 47/574 completed (loss: 0.00522169703617692, acc: 1.0)
[2024-12-12 02:58:16,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:16,927][root][INFO] - Training Epoch: 8/10, step 48/574 completed (loss: 0.07012810558080673, acc: 1.0)
[2024-12-12 02:58:17,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:17,323][root][INFO] - Training Epoch: 8/10, step 49/574 completed (loss: 0.3240288496017456, acc: 0.9444444179534912)
[2024-12-12 02:58:17,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:17,712][root][INFO] - Training Epoch: 8/10, step 50/574 completed (loss: 0.6848457455635071, acc: 0.7719298005104065)
[2024-12-12 02:58:17,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:18,120][root][INFO] - Training Epoch: 8/10, step 51/574 completed (loss: 0.5496615171432495, acc: 0.841269850730896)
[2024-12-12 02:58:18,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:18,503][root][INFO] - Training Epoch: 8/10, step 52/574 completed (loss: 0.8266646862030029, acc: 0.7605633735656738)
[2024-12-12 02:58:18,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:18,962][root][INFO] - Training Epoch: 8/10, step 53/574 completed (loss: 1.5922809839248657, acc: 0.54666668176651)
[2024-12-12 02:58:19,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:19,288][root][INFO] - Training Epoch: 8/10, step 54/574 completed (loss: 0.11187136918306351, acc: 0.9729729890823364)
[2024-12-12 02:58:19,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:19,630][root][INFO] - Training Epoch: 8/10, step 55/574 completed (loss: 0.045180048793554306, acc: 1.0)
[2024-12-12 02:58:21,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:22,634][root][INFO] - Training Epoch: 8/10, step 56/574 completed (loss: 1.5409562587738037, acc: 0.6040955781936646)
[2024-12-12 02:58:23,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:23,972][root][INFO] - Training Epoch: 8/10, step 57/574 completed (loss: 1.9789049625396729, acc: 0.45315903425216675)
[2024-12-12 02:58:24,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:24,597][root][INFO] - Training Epoch: 8/10, step 58/574 completed (loss: 1.397932529449463, acc: 0.6363636255264282)
[2024-12-12 02:58:24,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:25,172][root][INFO] - Training Epoch: 8/10, step 59/574 completed (loss: 1.1580840349197388, acc: 0.6102941036224365)
[2024-12-12 02:58:25,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:25,758][root][INFO] - Training Epoch: 8/10, step 60/574 completed (loss: 1.4034810066223145, acc: 0.5652173757553101)
[2024-12-12 02:58:25,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:26,166][root][INFO] - Training Epoch: 8/10, step 61/574 completed (loss: 0.9183155298233032, acc: 0.737500011920929)
[2024-12-12 02:58:26,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:26,570][root][INFO] - Training Epoch: 8/10, step 62/574 completed (loss: 0.12892022728919983, acc: 0.970588207244873)
[2024-12-12 02:58:26,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:26,878][root][INFO] - Training Epoch: 8/10, step 63/574 completed (loss: 0.6311278343200684, acc: 0.8888888955116272)
[2024-12-12 02:58:26,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:27,312][root][INFO] - Training Epoch: 8/10, step 64/574 completed (loss: 0.15122978389263153, acc: 0.96875)
[2024-12-12 02:58:27,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:27,672][root][INFO] - Training Epoch: 8/10, step 65/574 completed (loss: 0.49674510955810547, acc: 0.931034505367279)
[2024-12-12 02:58:27,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:28,013][root][INFO] - Training Epoch: 8/10, step 66/574 completed (loss: 0.43393322825431824, acc: 0.8571428656578064)
[2024-12-12 02:58:28,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:28,356][root][INFO] - Training Epoch: 8/10, step 67/574 completed (loss: 0.3990626931190491, acc: 0.8999999761581421)
[2024-12-12 02:58:28,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:28,739][root][INFO] - Training Epoch: 8/10, step 68/574 completed (loss: 0.034744635224342346, acc: 1.0)
[2024-12-12 02:58:28,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:29,019][root][INFO] - Training Epoch: 8/10, step 69/574 completed (loss: 0.29365748167037964, acc: 0.8611111044883728)
[2024-12-12 02:58:29,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:29,449][root][INFO] - Training Epoch: 8/10, step 70/574 completed (loss: 0.1950547844171524, acc: 0.939393937587738)
[2024-12-12 02:58:29,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:29,848][root][INFO] - Training Epoch: 8/10, step 71/574 completed (loss: 1.054966688156128, acc: 0.6691176295280457)
[2024-12-12 02:58:29,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:30,250][root][INFO] - Training Epoch: 8/10, step 72/574 completed (loss: 1.0277279615402222, acc: 0.7142857313156128)
[2024-12-12 02:58:30,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:30,562][root][INFO] - Training Epoch: 8/10, step 73/574 completed (loss: 1.533029556274414, acc: 0.5743589997291565)
[2024-12-12 02:58:30,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:30,865][root][INFO] - Training Epoch: 8/10, step 74/574 completed (loss: 0.616270124912262, acc: 0.795918345451355)
[2024-12-12 02:58:30,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:31,225][root][INFO] - Training Epoch: 8/10, step 75/574 completed (loss: 1.3244705200195312, acc: 0.6268656849861145)
[2024-12-12 02:58:31,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:31,648][root][INFO] - Training Epoch: 8/10, step 76/574 completed (loss: 1.679319143295288, acc: 0.55474454164505)
[2024-12-12 02:58:31,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:31,993][root][INFO] - Training Epoch: 8/10, step 77/574 completed (loss: 0.003948016092181206, acc: 1.0)
[2024-12-12 02:58:32,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:32,310][root][INFO] - Training Epoch: 8/10, step 78/574 completed (loss: 0.10716772824525833, acc: 0.9583333134651184)
[2024-12-12 02:58:32,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:32,627][root][INFO] - Training Epoch: 8/10, step 79/574 completed (loss: 0.042662493884563446, acc: 1.0)
[2024-12-12 02:58:32,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:33,009][root][INFO] - Training Epoch: 8/10, step 80/574 completed (loss: 0.018030090257525444, acc: 1.0)
[2024-12-12 02:58:33,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:33,416][root][INFO] - Training Epoch: 8/10, step 81/574 completed (loss: 0.47300881147384644, acc: 0.8653846383094788)
[2024-12-12 02:58:33,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:33,768][root][INFO] - Training Epoch: 8/10, step 82/574 completed (loss: 0.3814263939857483, acc: 0.9038461446762085)
[2024-12-12 02:58:33,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:34,096][root][INFO] - Training Epoch: 8/10, step 83/574 completed (loss: 0.05582653358578682, acc: 1.0)
[2024-12-12 02:58:34,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:34,502][root][INFO] - Training Epoch: 8/10, step 84/574 completed (loss: 0.5524806380271912, acc: 0.8260869383811951)
[2024-12-12 02:58:34,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:34,876][root][INFO] - Training Epoch: 8/10, step 85/574 completed (loss: 0.26915213465690613, acc: 0.9200000166893005)
[2024-12-12 02:58:34,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:35,217][root][INFO] - Training Epoch: 8/10, step 86/574 completed (loss: 0.2289327085018158, acc: 0.9130434989929199)
[2024-12-12 02:58:35,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:35,673][root][INFO] - Training Epoch: 8/10, step 87/574 completed (loss: 0.46723872423171997, acc: 0.8799999952316284)
[2024-12-12 02:58:35,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:36,013][root][INFO] - Training Epoch: 8/10, step 88/574 completed (loss: 0.8228020668029785, acc: 0.7475728392601013)
[2024-12-12 02:58:36,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:37,120][root][INFO] - Training Epoch: 8/10, step 89/574 completed (loss: 1.0989378690719604, acc: 0.708737850189209)
[2024-12-12 02:58:37,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:37,943][root][INFO] - Training Epoch: 8/10, step 90/574 completed (loss: 1.3909540176391602, acc: 0.6451612710952759)
[2024-12-12 02:58:38,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:38,752][root][INFO] - Training Epoch: 8/10, step 91/574 completed (loss: 1.3128424882888794, acc: 0.6508620977401733)
[2024-12-12 02:58:38,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:39,504][root][INFO] - Training Epoch: 8/10, step 92/574 completed (loss: 0.8422219753265381, acc: 0.7263157963752747)
[2024-12-12 02:58:39,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:40,504][root][INFO] - Training Epoch: 8/10, step 93/574 completed (loss: 1.21151602268219, acc: 0.5841584205627441)
[2024-12-12 02:58:40,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:40,833][root][INFO] - Training Epoch: 8/10, step 94/574 completed (loss: 0.8796687722206116, acc: 0.774193525314331)
[2024-12-12 02:58:40,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:41,188][root][INFO] - Training Epoch: 8/10, step 95/574 completed (loss: 0.46869105100631714, acc: 0.8405796885490417)
[2024-12-12 02:58:41,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:41,541][root][INFO] - Training Epoch: 8/10, step 96/574 completed (loss: 1.0494450330734253, acc: 0.6722689270973206)
[2024-12-12 02:58:41,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:41,961][root][INFO] - Training Epoch: 8/10, step 97/574 completed (loss: 1.0533027648925781, acc: 0.692307710647583)
[2024-12-12 02:58:42,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:42,344][root][INFO] - Training Epoch: 8/10, step 98/574 completed (loss: 1.3033790588378906, acc: 0.5620437860488892)
[2024-12-12 02:58:42,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:42,705][root][INFO] - Training Epoch: 8/10, step 99/574 completed (loss: 0.5690720677375793, acc: 0.8358209133148193)
[2024-12-12 02:58:42,827][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:43,078][root][INFO] - Training Epoch: 8/10, step 100/574 completed (loss: 0.2055833637714386, acc: 0.949999988079071)
[2024-12-12 02:58:43,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:43,409][root][INFO] - Training Epoch: 8/10, step 101/574 completed (loss: 0.013453901745378971, acc: 1.0)
[2024-12-12 02:58:43,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:43,707][root][INFO] - Training Epoch: 8/10, step 102/574 completed (loss: 0.018027789890766144, acc: 1.0)
[2024-12-12 02:58:43,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:44,045][root][INFO] - Training Epoch: 8/10, step 103/574 completed (loss: 0.06209784373641014, acc: 0.9772727489471436)
[2024-12-12 02:58:44,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:44,381][root][INFO] - Training Epoch: 8/10, step 104/574 completed (loss: 0.5119636058807373, acc: 0.8448275923728943)
[2024-12-12 02:58:44,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:44,762][root][INFO] - Training Epoch: 8/10, step 105/574 completed (loss: 0.05281541496515274, acc: 1.0)
[2024-12-12 02:58:44,860][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:45,104][root][INFO] - Training Epoch: 8/10, step 106/574 completed (loss: 0.15710797905921936, acc: 0.9200000166893005)
[2024-12-12 02:58:45,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:45,454][root][INFO] - Training Epoch: 8/10, step 107/574 completed (loss: 0.00646239472553134, acc: 1.0)
[2024-12-12 02:58:45,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:45,863][root][INFO] - Training Epoch: 8/10, step 108/574 completed (loss: 0.003284037346020341, acc: 1.0)
[2024-12-12 02:58:45,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:46,207][root][INFO] - Training Epoch: 8/10, step 109/574 completed (loss: 0.11587189137935638, acc: 0.976190447807312)
[2024-12-12 02:58:46,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:46,571][root][INFO] - Training Epoch: 8/10, step 110/574 completed (loss: 0.4916090667247772, acc: 0.8615384697914124)
[2024-12-12 02:58:46,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:47,032][root][INFO] - Training Epoch: 8/10, step 111/574 completed (loss: 0.4756166338920593, acc: 0.8421052694320679)
[2024-12-12 02:58:47,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:47,455][root][INFO] - Training Epoch: 8/10, step 112/574 completed (loss: 0.44939103722572327, acc: 0.859649121761322)
[2024-12-12 02:58:47,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:47,824][root][INFO] - Training Epoch: 8/10, step 113/574 completed (loss: 0.3738354444503784, acc: 0.8717948794364929)
[2024-12-12 02:58:47,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:48,199][root][INFO] - Training Epoch: 8/10, step 114/574 completed (loss: 0.47216469049453735, acc: 0.8571428656578064)
[2024-12-12 02:58:48,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:48,499][root][INFO] - Training Epoch: 8/10, step 115/574 completed (loss: 0.011142287403345108, acc: 1.0)
[2024-12-12 02:58:48,587][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:48,813][root][INFO] - Training Epoch: 8/10, step 116/574 completed (loss: 0.5871633291244507, acc: 0.8095238208770752)
[2024-12-12 02:58:48,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:49,113][root][INFO] - Training Epoch: 8/10, step 117/574 completed (loss: 0.6526486277580261, acc: 0.8130081295967102)
[2024-12-12 02:58:49,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:49,426][root][INFO] - Training Epoch: 8/10, step 118/574 completed (loss: 0.19784051179885864, acc: 0.9354838728904724)
[2024-12-12 02:58:49,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:50,313][root][INFO] - Training Epoch: 8/10, step 119/574 completed (loss: 1.4244880676269531, acc: 0.6121672987937927)
[2024-12-12 02:58:50,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:50,647][root][INFO] - Training Epoch: 8/10, step 120/574 completed (loss: 0.306031197309494, acc: 0.9333333373069763)
[2024-12-12 02:58:50,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:51,086][root][INFO] - Training Epoch: 8/10, step 121/574 completed (loss: 0.4573516547679901, acc: 0.8653846383094788)
[2024-12-12 02:58:51,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:51,476][root][INFO] - Training Epoch: 8/10, step 122/574 completed (loss: 0.01456362009048462, acc: 1.0)
[2024-12-12 02:58:51,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:51,864][root][INFO] - Training Epoch: 8/10, step 123/574 completed (loss: 0.04457777366042137, acc: 1.0)
[2024-12-12 02:58:51,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:52,209][root][INFO] - Training Epoch: 8/10, step 124/574 completed (loss: 1.2232189178466797, acc: 0.6380367875099182)
[2024-12-12 02:58:52,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:52,576][root][INFO] - Training Epoch: 8/10, step 125/574 completed (loss: 1.1102052927017212, acc: 0.6805555820465088)
[2024-12-12 02:58:52,657][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:52,886][root][INFO] - Training Epoch: 8/10, step 126/574 completed (loss: 0.8931967616081238, acc: 0.7416666746139526)
[2024-12-12 02:58:53,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:53,275][root][INFO] - Training Epoch: 8/10, step 127/574 completed (loss: 1.2246441841125488, acc: 0.636904776096344)
[2024-12-12 02:58:53,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:53,671][root][INFO] - Training Epoch: 8/10, step 128/574 completed (loss: 1.024999737739563, acc: 0.7128205299377441)
[2024-12-12 02:58:54,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:54,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:55,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:55,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:55,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:56,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:56,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:56,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:57,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:57,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:58,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:58,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:58,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:59,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:58:59,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:00,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:00,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:00,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:01,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:01,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:01,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:02,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:02,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:02,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:03,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:03,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:03,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:04,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:04,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:04,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:04,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:05,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:05,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:05,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:06,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:06,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:07,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:07,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:08,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:08,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:08,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:09,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:09,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:09,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:10,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:10,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:10,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:11,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:11,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:11,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:12,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:12,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:13,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:13,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:13,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:14,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:14,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:14,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:15,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:15,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:15,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:16,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:16,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:16,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:17,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:17,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:18,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:18,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:18,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:19,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:19,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:20,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:20,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:20,724][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:20,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:21,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:21,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:21,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:22,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:22,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:23,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:23,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:23,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:24,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:24,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:25,163][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.0008, device='cuda:0') eval_epoch_loss=tensor(1.3865, device='cuda:0') eval_epoch_acc=tensor(0.6955, device='cuda:0')
[2024-12-12 02:59:25,165][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 02:59:25,166][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 02:59:25,481][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_8_step_129_loss_1.3864881992340088/model.pt
[2024-12-12 02:59:25,484][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 02:59:25,485][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 8 is 0.6954719424247742
[2024-12-12 02:59:25,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:25,906][root][INFO] - Training Epoch: 8/10, step 129/574 completed (loss: 1.041152834892273, acc: 0.7132353186607361)
[2024-12-12 02:59:26,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:26,271][root][INFO] - Training Epoch: 8/10, step 130/574 completed (loss: 0.2431885302066803, acc: 0.9615384340286255)
[2024-12-12 02:59:26,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:26,558][root][INFO] - Training Epoch: 8/10, step 131/574 completed (loss: 0.07221600413322449, acc: 1.0)
[2024-12-12 02:59:26,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:26,856][root][INFO] - Training Epoch: 8/10, step 132/574 completed (loss: 0.11729788035154343, acc: 0.96875)
[2024-12-12 02:59:26,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:27,167][root][INFO] - Training Epoch: 8/10, step 133/574 completed (loss: 0.15533822774887085, acc: 0.9130434989929199)
[2024-12-12 02:59:27,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:27,525][root][INFO] - Training Epoch: 8/10, step 134/574 completed (loss: 0.20193758606910706, acc: 0.9142857193946838)
[2024-12-12 02:59:27,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:27,843][root][INFO] - Training Epoch: 8/10, step 135/574 completed (loss: 0.051954686641693115, acc: 1.0)
[2024-12-12 02:59:27,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:28,147][root][INFO] - Training Epoch: 8/10, step 136/574 completed (loss: 0.15579181909561157, acc: 0.9523809552192688)
[2024-12-12 02:59:28,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:28,486][root][INFO] - Training Epoch: 8/10, step 137/574 completed (loss: 0.18536092340946198, acc: 0.9666666388511658)
[2024-12-12 02:59:28,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:28,815][root][INFO] - Training Epoch: 8/10, step 138/574 completed (loss: 0.17868755757808685, acc: 0.95652174949646)
[2024-12-12 02:59:28,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:29,105][root][INFO] - Training Epoch: 8/10, step 139/574 completed (loss: 0.09980326890945435, acc: 0.9523809552192688)
[2024-12-12 02:59:29,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:29,370][root][INFO] - Training Epoch: 8/10, step 140/574 completed (loss: 0.05956452712416649, acc: 1.0)
[2024-12-12 02:59:29,449][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:29,684][root][INFO] - Training Epoch: 8/10, step 141/574 completed (loss: 0.1390523910522461, acc: 0.9677419066429138)
[2024-12-12 02:59:29,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:29,993][root][INFO] - Training Epoch: 8/10, step 142/574 completed (loss: 0.18359224498271942, acc: 0.9459459185600281)
[2024-12-12 02:59:30,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:30,525][root][INFO] - Training Epoch: 8/10, step 143/574 completed (loss: 0.7646084427833557, acc: 0.7631579041481018)
[2024-12-12 02:59:30,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:30,877][root][INFO] - Training Epoch: 8/10, step 144/574 completed (loss: 0.8095623850822449, acc: 0.7761194109916687)
[2024-12-12 02:59:30,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:31,224][root][INFO] - Training Epoch: 8/10, step 145/574 completed (loss: 0.7214850783348083, acc: 0.7755101919174194)
[2024-12-12 02:59:31,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:31,676][root][INFO] - Training Epoch: 8/10, step 146/574 completed (loss: 1.037742257118225, acc: 0.6382978558540344)
[2024-12-12 02:59:31,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:32,055][root][INFO] - Training Epoch: 8/10, step 147/574 completed (loss: 0.5793600082397461, acc: 0.8142856955528259)
[2024-12-12 02:59:32,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:32,372][root][INFO] - Training Epoch: 8/10, step 148/574 completed (loss: 0.29535937309265137, acc: 0.8214285969734192)
[2024-12-12 02:59:32,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:32,723][root][INFO] - Training Epoch: 8/10, step 149/574 completed (loss: 0.11166904121637344, acc: 0.95652174949646)
[2024-12-12 02:59:32,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:33,098][root][INFO] - Training Epoch: 8/10, step 150/574 completed (loss: 0.22802862524986267, acc: 0.931034505367279)
[2024-12-12 02:59:33,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:33,481][root][INFO] - Training Epoch: 8/10, step 151/574 completed (loss: 0.48343920707702637, acc: 0.8913043737411499)
[2024-12-12 02:59:33,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:33,825][root][INFO] - Training Epoch: 8/10, step 152/574 completed (loss: 0.4248131215572357, acc: 0.8644067645072937)
[2024-12-12 02:59:33,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:34,159][root][INFO] - Training Epoch: 8/10, step 153/574 completed (loss: 0.3253028988838196, acc: 0.8947368264198303)
[2024-12-12 02:59:34,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:34,493][root][INFO] - Training Epoch: 8/10, step 154/574 completed (loss: 0.7107986807823181, acc: 0.7702702879905701)
[2024-12-12 02:59:34,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:34,856][root][INFO] - Training Epoch: 8/10, step 155/574 completed (loss: 0.03959118202328682, acc: 1.0)
[2024-12-12 02:59:34,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:35,221][root][INFO] - Training Epoch: 8/10, step 156/574 completed (loss: 0.02988395281136036, acc: 1.0)
[2024-12-12 02:59:35,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:35,594][root][INFO] - Training Epoch: 8/10, step 157/574 completed (loss: 0.15086011588573456, acc: 1.0)
[2024-12-12 02:59:36,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:37,258][root][INFO] - Training Epoch: 8/10, step 158/574 completed (loss: 0.6660057902336121, acc: 0.7837837934494019)
[2024-12-12 02:59:37,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:37,555][root][INFO] - Training Epoch: 8/10, step 159/574 completed (loss: 0.5246613025665283, acc: 0.8888888955116272)
[2024-12-12 02:59:37,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:37,993][root][INFO] - Training Epoch: 8/10, step 160/574 completed (loss: 0.7848799228668213, acc: 0.7790697813034058)
[2024-12-12 02:59:38,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:38,583][root][INFO] - Training Epoch: 8/10, step 161/574 completed (loss: 0.7049820423126221, acc: 0.800000011920929)
[2024-12-12 02:59:38,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:39,147][root][INFO] - Training Epoch: 8/10, step 162/574 completed (loss: 1.0980665683746338, acc: 0.6853932738304138)
[2024-12-12 02:59:39,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:39,482][root][INFO] - Training Epoch: 8/10, step 163/574 completed (loss: 0.2858316898345947, acc: 0.9090909361839294)
[2024-12-12 02:59:39,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:39,812][root][INFO] - Training Epoch: 8/10, step 164/574 completed (loss: 0.2595939636230469, acc: 0.9523809552192688)
[2024-12-12 02:59:39,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:40,193][root][INFO] - Training Epoch: 8/10, step 165/574 completed (loss: 0.47030866146087646, acc: 0.8275862336158752)
[2024-12-12 02:59:40,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:40,513][root][INFO] - Training Epoch: 8/10, step 166/574 completed (loss: 0.17510488629341125, acc: 0.8979591727256775)
[2024-12-12 02:59:40,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:40,894][root][INFO] - Training Epoch: 8/10, step 167/574 completed (loss: 0.21654388308525085, acc: 0.9599999785423279)
[2024-12-12 02:59:41,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:41,352][root][INFO] - Training Epoch: 8/10, step 168/574 completed (loss: 0.5156840682029724, acc: 0.8194444179534912)
[2024-12-12 02:59:41,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:41,734][root][INFO] - Training Epoch: 8/10, step 169/574 completed (loss: 1.0850132703781128, acc: 0.686274528503418)
[2024-12-12 02:59:42,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:42,766][root][INFO] - Training Epoch: 8/10, step 170/574 completed (loss: 1.6631563901901245, acc: 0.5410959124565125)
[2024-12-12 02:59:42,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:43,069][root][INFO] - Training Epoch: 8/10, step 171/574 completed (loss: 0.045250535011291504, acc: 1.0)
[2024-12-12 02:59:43,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:43,406][root][INFO] - Training Epoch: 8/10, step 172/574 completed (loss: 0.08186302334070206, acc: 0.9629629850387573)
[2024-12-12 02:59:43,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:43,754][root][INFO] - Training Epoch: 8/10, step 173/574 completed (loss: 0.017373204231262207, acc: 1.0)
[2024-12-12 02:59:43,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:44,298][root][INFO] - Training Epoch: 8/10, step 174/574 completed (loss: 0.9661307334899902, acc: 0.7345132827758789)
[2024-12-12 02:59:44,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:44,625][root][INFO] - Training Epoch: 8/10, step 175/574 completed (loss: 0.6894041299819946, acc: 0.8115941882133484)
[2024-12-12 02:59:44,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:45,001][root][INFO] - Training Epoch: 8/10, step 176/574 completed (loss: 0.6908977031707764, acc: 0.8181818127632141)
[2024-12-12 02:59:45,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:45,912][root][INFO] - Training Epoch: 8/10, step 177/574 completed (loss: 1.7489973306655884, acc: 0.5419847369194031)
[2024-12-12 02:59:46,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:46,584][root][INFO] - Training Epoch: 8/10, step 178/574 completed (loss: 1.26690673828125, acc: 0.6370370388031006)
[2024-12-12 02:59:46,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:46,934][root][INFO] - Training Epoch: 8/10, step 179/574 completed (loss: 0.40067294239997864, acc: 0.9016393423080444)
[2024-12-12 02:59:47,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:47,301][root][INFO] - Training Epoch: 8/10, step 180/574 completed (loss: 0.11963105201721191, acc: 0.9583333134651184)
[2024-12-12 02:59:47,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:47,641][root][INFO] - Training Epoch: 8/10, step 181/574 completed (loss: 0.1215660348534584, acc: 0.9599999785423279)
[2024-12-12 02:59:47,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:47,938][root][INFO] - Training Epoch: 8/10, step 182/574 completed (loss: 0.19001425802707672, acc: 0.9642857313156128)
[2024-12-12 02:59:48,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:48,289][root][INFO] - Training Epoch: 8/10, step 183/574 completed (loss: 0.449055552482605, acc: 0.8780487775802612)
[2024-12-12 02:59:48,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:48,695][root][INFO] - Training Epoch: 8/10, step 184/574 completed (loss: 1.0851808786392212, acc: 0.69486403465271)
[2024-12-12 02:59:48,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:49,034][root][INFO] - Training Epoch: 8/10, step 185/574 completed (loss: 1.34710693359375, acc: 0.619596540927887)
[2024-12-12 02:59:49,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:49,515][root][INFO] - Training Epoch: 8/10, step 186/574 completed (loss: 1.3364578485488892, acc: 0.6343749761581421)
[2024-12-12 02:59:49,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:50,041][root][INFO] - Training Epoch: 8/10, step 187/574 completed (loss: 1.545899510383606, acc: 0.5741088390350342)
[2024-12-12 02:59:50,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:50,475][root][INFO] - Training Epoch: 8/10, step 188/574 completed (loss: 1.236708402633667, acc: 0.6476868391036987)
[2024-12-12 02:59:50,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:50,800][root][INFO] - Training Epoch: 8/10, step 189/574 completed (loss: 0.02837398834526539, acc: 1.0)
[2024-12-12 02:59:50,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:51,349][root][INFO] - Training Epoch: 8/10, step 190/574 completed (loss: 0.8637368679046631, acc: 0.7325581312179565)
[2024-12-12 02:59:51,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:52,145][root][INFO] - Training Epoch: 8/10, step 191/574 completed (loss: 1.2389132976531982, acc: 0.6349206566810608)
[2024-12-12 02:59:52,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:53,063][root][INFO] - Training Epoch: 8/10, step 192/574 completed (loss: 1.0003490447998047, acc: 0.7045454382896423)
[2024-12-12 02:59:53,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:53,812][root][INFO] - Training Epoch: 8/10, step 193/574 completed (loss: 0.737873375415802, acc: 0.7411764860153198)
[2024-12-12 02:59:54,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:54,893][root][INFO] - Training Epoch: 8/10, step 194/574 completed (loss: 0.909725546836853, acc: 0.7407407164573669)
[2024-12-12 02:59:55,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:55,847][root][INFO] - Training Epoch: 8/10, step 195/574 completed (loss: 0.5831283330917358, acc: 0.8709677457809448)
[2024-12-12 02:59:55,915][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:56,177][root][INFO] - Training Epoch: 8/10, step 196/574 completed (loss: 0.03698921948671341, acc: 1.0)
[2024-12-12 02:59:56,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:56,568][root][INFO] - Training Epoch: 8/10, step 197/574 completed (loss: 0.22543692588806152, acc: 0.925000011920929)
[2024-12-12 02:59:56,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:56,857][root][INFO] - Training Epoch: 8/10, step 198/574 completed (loss: 0.45609790086746216, acc: 0.8676470518112183)
[2024-12-12 02:59:56,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:57,216][root][INFO] - Training Epoch: 8/10, step 199/574 completed (loss: 1.0034431219100952, acc: 0.75)
[2024-12-12 02:59:57,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:57,607][root][INFO] - Training Epoch: 8/10, step 200/574 completed (loss: 0.7430614829063416, acc: 0.805084764957428)
[2024-12-12 02:59:57,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:57,984][root][INFO] - Training Epoch: 8/10, step 201/574 completed (loss: 0.9565113186836243, acc: 0.746268630027771)
[2024-12-12 02:59:58,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:58,409][root][INFO] - Training Epoch: 8/10, step 202/574 completed (loss: 0.7358804941177368, acc: 0.7961165308952332)
[2024-12-12 02:59:58,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:58,775][root][INFO] - Training Epoch: 8/10, step 203/574 completed (loss: 0.39645975828170776, acc: 0.920634925365448)
[2024-12-12 02:59:58,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:59,140][root][INFO] - Training Epoch: 8/10, step 204/574 completed (loss: 0.20955228805541992, acc: 0.9450549483299255)
[2024-12-12 02:59:59,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:59,546][root][INFO] - Training Epoch: 8/10, step 205/574 completed (loss: 0.9444513916969299, acc: 0.7399103045463562)
[2024-12-12 02:59:59,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 02:59:59,936][root][INFO] - Training Epoch: 8/10, step 206/574 completed (loss: 1.107517123222351, acc: 0.6968504190444946)
[2024-12-12 03:00:00,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:00,303][root][INFO] - Training Epoch: 8/10, step 207/574 completed (loss: 0.9105560183525085, acc: 0.7715517282485962)
[2024-12-12 03:00:00,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:00,705][root][INFO] - Training Epoch: 8/10, step 208/574 completed (loss: 1.1189498901367188, acc: 0.695652186870575)
[2024-12-12 03:00:00,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:01,111][root][INFO] - Training Epoch: 8/10, step 209/574 completed (loss: 0.9337737560272217, acc: 0.731517493724823)
[2024-12-12 03:00:01,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:01,472][root][INFO] - Training Epoch: 8/10, step 210/574 completed (loss: 0.8661094307899475, acc: 0.739130437374115)
[2024-12-12 03:00:01,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:01,810][root][INFO] - Training Epoch: 8/10, step 211/574 completed (loss: 0.08329059928655624, acc: 0.95652174949646)
[2024-12-12 03:00:01,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:02,179][root][INFO] - Training Epoch: 8/10, step 212/574 completed (loss: 0.03653886914253235, acc: 1.0)
[2024-12-12 03:00:02,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:02,583][root][INFO] - Training Epoch: 8/10, step 213/574 completed (loss: 0.16961385309696198, acc: 0.978723406791687)
[2024-12-12 03:00:02,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:03,272][root][INFO] - Training Epoch: 8/10, step 214/574 completed (loss: 0.5577791929244995, acc: 0.8230769038200378)
[2024-12-12 03:00:03,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:03,681][root][INFO] - Training Epoch: 8/10, step 215/574 completed (loss: 0.3552230894565582, acc: 0.8783783912658691)
[2024-12-12 03:00:03,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:04,052][root][INFO] - Training Epoch: 8/10, step 216/574 completed (loss: 0.3080461621284485, acc: 0.9069767594337463)
[2024-12-12 03:00:04,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:04,586][root][INFO] - Training Epoch: 8/10, step 217/574 completed (loss: 0.5035730600357056, acc: 0.837837815284729)
[2024-12-12 03:00:04,684][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:04,970][root][INFO] - Training Epoch: 8/10, step 218/574 completed (loss: 0.3361111581325531, acc: 0.8999999761581421)
[2024-12-12 03:00:05,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:05,330][root][INFO] - Training Epoch: 8/10, step 219/574 completed (loss: 0.087516650557518, acc: 1.0)
[2024-12-12 03:00:05,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:05,682][root][INFO] - Training Epoch: 8/10, step 220/574 completed (loss: 0.003861847100779414, acc: 1.0)
[2024-12-12 03:00:05,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:06,080][root][INFO] - Training Epoch: 8/10, step 221/574 completed (loss: 0.03889138624072075, acc: 0.9599999785423279)
[2024-12-12 03:00:06,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:06,478][root][INFO] - Training Epoch: 8/10, step 222/574 completed (loss: 0.36624258756637573, acc: 0.8653846383094788)
[2024-12-12 03:00:06,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:07,244][root][INFO] - Training Epoch: 8/10, step 223/574 completed (loss: 0.7023565769195557, acc: 0.782608687877655)
[2024-12-12 03:00:07,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:07,796][root][INFO] - Training Epoch: 8/10, step 224/574 completed (loss: 0.9350168704986572, acc: 0.7386363744735718)
[2024-12-12 03:00:07,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:08,272][root][INFO] - Training Epoch: 8/10, step 225/574 completed (loss: 0.6809651255607605, acc: 0.7978723645210266)
[2024-12-12 03:00:08,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:08,695][root][INFO] - Training Epoch: 8/10, step 226/574 completed (loss: 0.1925039142370224, acc: 0.9433962106704712)
[2024-12-12 03:00:08,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:09,110][root][INFO] - Training Epoch: 8/10, step 227/574 completed (loss: 0.25349196791648865, acc: 0.8999999761581421)
[2024-12-12 03:00:09,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:09,472][root][INFO] - Training Epoch: 8/10, step 228/574 completed (loss: 0.21023745834827423, acc: 0.9534883499145508)
[2024-12-12 03:00:09,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:09,829][root][INFO] - Training Epoch: 8/10, step 229/574 completed (loss: 0.35702869296073914, acc: 0.8666666746139526)
[2024-12-12 03:00:09,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:10,256][root][INFO] - Training Epoch: 8/10, step 230/574 completed (loss: 0.84632408618927, acc: 0.7263157963752747)
[2024-12-12 03:00:10,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:10,595][root][INFO] - Training Epoch: 8/10, step 231/574 completed (loss: 0.5780551433563232, acc: 0.8444444537162781)
[2024-12-12 03:00:10,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:11,023][root][INFO] - Training Epoch: 8/10, step 232/574 completed (loss: 0.9866021275520325, acc: 0.7277777791023254)
[2024-12-12 03:00:11,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:11,517][root][INFO] - Training Epoch: 8/10, step 233/574 completed (loss: 1.52433443069458, acc: 0.6100917458534241)
[2024-12-12 03:00:11,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:11,987][root][INFO] - Training Epoch: 8/10, step 234/574 completed (loss: 0.7356142997741699, acc: 0.7538461685180664)
[2024-12-12 03:00:12,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:12,259][root][INFO] - Training Epoch: 8/10, step 235/574 completed (loss: 0.02498825266957283, acc: 1.0)
[2024-12-12 03:00:12,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:12,550][root][INFO] - Training Epoch: 8/10, step 236/574 completed (loss: 0.07431859523057938, acc: 0.9583333134651184)
[2024-12-12 03:00:12,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:12,918][root][INFO] - Training Epoch: 8/10, step 237/574 completed (loss: 0.11319389194250107, acc: 0.9545454382896423)
[2024-12-12 03:00:13,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:13,328][root][INFO] - Training Epoch: 8/10, step 238/574 completed (loss: 0.24210502207279205, acc: 0.8888888955116272)
[2024-12-12 03:00:13,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:13,673][root][INFO] - Training Epoch: 8/10, step 239/574 completed (loss: 0.3233207166194916, acc: 0.8857142925262451)
[2024-12-12 03:00:13,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:14,077][root][INFO] - Training Epoch: 8/10, step 240/574 completed (loss: 0.3975604772567749, acc: 0.8863636255264282)
[2024-12-12 03:00:14,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:14,388][root][INFO] - Training Epoch: 8/10, step 241/574 completed (loss: 0.14612875878810883, acc: 0.9772727489471436)
[2024-12-12 03:00:14,531][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:14,971][root][INFO] - Training Epoch: 8/10, step 242/574 completed (loss: 0.812191367149353, acc: 0.7419354915618896)
[2024-12-12 03:00:15,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:15,500][root][INFO] - Training Epoch: 8/10, step 243/574 completed (loss: 0.3812345266342163, acc: 0.8863636255264282)
[2024-12-12 03:00:15,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:15,836][root][INFO] - Training Epoch: 8/10, step 244/574 completed (loss: 0.005317621398717165, acc: 1.0)
[2024-12-12 03:00:15,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:16,194][root][INFO] - Training Epoch: 8/10, step 245/574 completed (loss: 0.09302723407745361, acc: 1.0)
[2024-12-12 03:00:16,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:16,528][root][INFO] - Training Epoch: 8/10, step 246/574 completed (loss: 0.027399491518735886, acc: 1.0)
[2024-12-12 03:00:16,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:16,835][root][INFO] - Training Epoch: 8/10, step 247/574 completed (loss: 0.1723242700099945, acc: 0.949999988079071)
[2024-12-12 03:00:16,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:17,229][root][INFO] - Training Epoch: 8/10, step 248/574 completed (loss: 0.21702787280082703, acc: 0.9189189076423645)
[2024-12-12 03:00:17,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:17,598][root][INFO] - Training Epoch: 8/10, step 249/574 completed (loss: 0.43059390783309937, acc: 0.8648648858070374)
[2024-12-12 03:00:17,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:17,936][root][INFO] - Training Epoch: 8/10, step 250/574 completed (loss: 0.32452815771102905, acc: 0.8918918967247009)
[2024-12-12 03:00:18,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:18,270][root][INFO] - Training Epoch: 8/10, step 251/574 completed (loss: 0.4410388469696045, acc: 0.8529411554336548)
[2024-12-12 03:00:18,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:18,633][root][INFO] - Training Epoch: 8/10, step 252/574 completed (loss: 0.07876507192850113, acc: 0.9756097793579102)
[2024-12-12 03:00:18,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:18,954][root][INFO] - Training Epoch: 8/10, step 253/574 completed (loss: 0.023023003712296486, acc: 1.0)
[2024-12-12 03:00:19,032][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:19,270][root][INFO] - Training Epoch: 8/10, step 254/574 completed (loss: 0.06948575377464294, acc: 0.9599999785423279)
[2024-12-12 03:00:19,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:19,649][root][INFO] - Training Epoch: 8/10, step 255/574 completed (loss: 0.03077809140086174, acc: 1.0)
[2024-12-12 03:00:19,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:20,036][root][INFO] - Training Epoch: 8/10, step 256/574 completed (loss: 0.051646485924720764, acc: 1.0)
[2024-12-12 03:00:20,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:20,417][root][INFO] - Training Epoch: 8/10, step 257/574 completed (loss: 0.4267500340938568, acc: 0.8428571224212646)
[2024-12-12 03:00:20,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:20,791][root][INFO] - Training Epoch: 8/10, step 258/574 completed (loss: 0.39330345392227173, acc: 0.8947368264198303)
[2024-12-12 03:00:20,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:21,382][root][INFO] - Training Epoch: 8/10, step 259/574 completed (loss: 0.7972949743270874, acc: 0.7641509175300598)
[2024-12-12 03:00:21,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:21,966][root][INFO] - Training Epoch: 8/10, step 260/574 completed (loss: 0.978448212146759, acc: 0.7333333492279053)
[2024-12-12 03:00:22,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:22,223][root][INFO] - Training Epoch: 8/10, step 261/574 completed (loss: 0.16813941299915314, acc: 0.9722222089767456)
[2024-12-12 03:00:22,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:22,516][root][INFO] - Training Epoch: 8/10, step 262/574 completed (loss: 0.14313939213752747, acc: 0.9354838728904724)
[2024-12-12 03:00:22,614][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:22,871][root][INFO] - Training Epoch: 8/10, step 263/574 completed (loss: 0.8351444602012634, acc: 0.746666669845581)
[2024-12-12 03:00:23,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:23,260][root][INFO] - Training Epoch: 8/10, step 264/574 completed (loss: 0.7749228477478027, acc: 0.75)
[2024-12-12 03:00:23,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:24,100][root][INFO] - Training Epoch: 8/10, step 265/574 completed (loss: 1.6297463178634644, acc: 0.527999997138977)
[2024-12-12 03:00:24,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:24,425][root][INFO] - Training Epoch: 8/10, step 266/574 completed (loss: 0.9131414890289307, acc: 0.7191011309623718)
[2024-12-12 03:00:24,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:24,777][root][INFO] - Training Epoch: 8/10, step 267/574 completed (loss: 0.7735185623168945, acc: 0.662162184715271)
[2024-12-12 03:00:24,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:25,235][root][INFO] - Training Epoch: 8/10, step 268/574 completed (loss: 0.8420149087905884, acc: 0.8103448152542114)
[2024-12-12 03:00:25,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:25,624][root][INFO] - Training Epoch: 8/10, step 269/574 completed (loss: 0.01981702446937561, acc: 1.0)
[2024-12-12 03:00:25,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:26,001][root][INFO] - Training Epoch: 8/10, step 270/574 completed (loss: 0.04948270320892334, acc: 1.0)
[2024-12-12 03:00:26,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:26,335][root][INFO] - Training Epoch: 8/10, step 271/574 completed (loss: 0.14524546265602112, acc: 0.96875)
[2024-12-12 03:00:27,141][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:27,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:27,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:28,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:28,682][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:28,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:29,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:29,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:30,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:30,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:30,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:31,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:31,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:32,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:32,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:32,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:33,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:33,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:33,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:34,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:34,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:34,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:35,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:35,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:35,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:36,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:36,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:36,892][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:37,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:37,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:37,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:38,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:38,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:38,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:39,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:39,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:39,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:40,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:40,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:40,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:41,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:41,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:41,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:42,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:42,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:43,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:43,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:43,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:44,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:44,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:44,836][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:45,252][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:45,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:45,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:46,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:46,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:46,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:47,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:47,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:47,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:48,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:48,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:49,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:49,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:49,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:50,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:50,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:50,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:51,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:51,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:52,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:52,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:52,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:53,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:53,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:53,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:54,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:54,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:54,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:55,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:55,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:55,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:56,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:56,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:57,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:57,611][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.9098, device='cuda:0') eval_epoch_loss=tensor(1.3635, device='cuda:0') eval_epoch_acc=tensor(0.7026, device='cuda:0')
[2024-12-12 03:00:57,612][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:00:57,613][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:00:57,840][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_8_step_272_loss_1.363477110862732/model.pt
[2024-12-12 03:00:57,843][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 03:00:57,844][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 8 is 0.7025531530380249
[2024-12-12 03:00:57,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:58,249][root][INFO] - Training Epoch: 8/10, step 272/574 completed (loss: 0.057348039001226425, acc: 0.9666666388511658)
[2024-12-12 03:00:58,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:58,680][root][INFO] - Training Epoch: 8/10, step 273/574 completed (loss: 0.352296382188797, acc: 0.9166666865348816)
[2024-12-12 03:00:58,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:59,021][root][INFO] - Training Epoch: 8/10, step 274/574 completed (loss: 0.2413577437400818, acc: 0.90625)
[2024-12-12 03:00:59,113][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:59,355][root][INFO] - Training Epoch: 8/10, step 275/574 completed (loss: 0.15976768732070923, acc: 0.9333333373069763)
[2024-12-12 03:00:59,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:00:59,717][root][INFO] - Training Epoch: 8/10, step 276/574 completed (loss: 0.13858434557914734, acc: 0.931034505367279)
[2024-12-12 03:00:59,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:00,116][root][INFO] - Training Epoch: 8/10, step 277/574 completed (loss: 0.17334610223770142, acc: 0.9200000166893005)
[2024-12-12 03:01:00,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:00,453][root][INFO] - Training Epoch: 8/10, step 278/574 completed (loss: 0.2644420564174652, acc: 0.936170220375061)
[2024-12-12 03:01:00,545][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:00,757][root][INFO] - Training Epoch: 8/10, step 279/574 completed (loss: 0.3905707895755768, acc: 0.875)
[2024-12-12 03:01:00,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:01,074][root][INFO] - Training Epoch: 8/10, step 280/574 completed (loss: 0.11709799617528915, acc: 0.9772727489471436)
[2024-12-12 03:01:01,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:01,494][root][INFO] - Training Epoch: 8/10, step 281/574 completed (loss: 0.870685875415802, acc: 0.6987951993942261)
[2024-12-12 03:01:01,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:01,899][root][INFO] - Training Epoch: 8/10, step 282/574 completed (loss: 1.166414737701416, acc: 0.7129629850387573)
[2024-12-12 03:01:02,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:02,284][root][INFO] - Training Epoch: 8/10, step 283/574 completed (loss: 0.18086130917072296, acc: 0.9473684430122375)
[2024-12-12 03:01:02,397][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:02,645][root][INFO] - Training Epoch: 8/10, step 284/574 completed (loss: 0.08832424879074097, acc: 1.0)
[2024-12-12 03:01:02,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:02,970][root][INFO] - Training Epoch: 8/10, step 285/574 completed (loss: 0.1777484118938446, acc: 0.949999988079071)
[2024-12-12 03:01:03,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:03,298][root][INFO] - Training Epoch: 8/10, step 286/574 completed (loss: 0.6535706520080566, acc: 0.765625)
[2024-12-12 03:01:03,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:03,669][root][INFO] - Training Epoch: 8/10, step 287/574 completed (loss: 0.8006286025047302, acc: 0.7839999794960022)
[2024-12-12 03:01:03,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:04,025][root][INFO] - Training Epoch: 8/10, step 288/574 completed (loss: 0.4372769892215729, acc: 0.8681318759918213)
[2024-12-12 03:01:04,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:04,377][root][INFO] - Training Epoch: 8/10, step 289/574 completed (loss: 0.9759034514427185, acc: 0.7267080545425415)
[2024-12-12 03:01:04,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:04,738][root][INFO] - Training Epoch: 8/10, step 290/574 completed (loss: 1.104072093963623, acc: 0.6855670213699341)
[2024-12-12 03:01:04,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:05,031][root][INFO] - Training Epoch: 8/10, step 291/574 completed (loss: 0.0072128549218177795, acc: 1.0)
[2024-12-12 03:01:05,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:05,386][root][INFO] - Training Epoch: 8/10, step 292/574 completed (loss: 0.11078710854053497, acc: 0.976190447807312)
[2024-12-12 03:01:05,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:05,805][root][INFO] - Training Epoch: 8/10, step 293/574 completed (loss: 0.6888042688369751, acc: 0.8103448152542114)
[2024-12-12 03:01:05,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:06,276][root][INFO] - Training Epoch: 8/10, step 294/574 completed (loss: 0.3735089600086212, acc: 0.8909090757369995)
[2024-12-12 03:01:06,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:06,827][root][INFO] - Training Epoch: 8/10, step 295/574 completed (loss: 1.0737950801849365, acc: 0.7319587469100952)
[2024-12-12 03:01:06,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:07,140][root][INFO] - Training Epoch: 8/10, step 296/574 completed (loss: 0.29531851410865784, acc: 0.931034505367279)
[2024-12-12 03:01:07,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:07,518][root][INFO] - Training Epoch: 8/10, step 297/574 completed (loss: 0.1706867218017578, acc: 0.9629629850387573)
[2024-12-12 03:01:07,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:07,846][root][INFO] - Training Epoch: 8/10, step 298/574 completed (loss: 0.45595088601112366, acc: 0.8947368264198303)
[2024-12-12 03:01:07,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:08,202][root][INFO] - Training Epoch: 8/10, step 299/574 completed (loss: 0.1550227850675583, acc: 0.9821428656578064)
[2024-12-12 03:01:08,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:08,470][root][INFO] - Training Epoch: 8/10, step 300/574 completed (loss: 0.22296962141990662, acc: 0.9375)
[2024-12-12 03:01:08,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:08,786][root][INFO] - Training Epoch: 8/10, step 301/574 completed (loss: 0.21505595743656158, acc: 0.9433962106704712)
[2024-12-12 03:01:08,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:09,115][root][INFO] - Training Epoch: 8/10, step 302/574 completed (loss: 0.09896893799304962, acc: 0.9811320900917053)
[2024-12-12 03:01:09,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:09,446][root][INFO] - Training Epoch: 8/10, step 303/574 completed (loss: 0.09369011968374252, acc: 0.970588207244873)
[2024-12-12 03:01:09,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:09,833][root][INFO] - Training Epoch: 8/10, step 304/574 completed (loss: 0.20762856304645538, acc: 0.9375)
[2024-12-12 03:01:09,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:10,187][root][INFO] - Training Epoch: 8/10, step 305/574 completed (loss: 0.41377073526382446, acc: 0.8524590134620667)
[2024-12-12 03:01:10,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:10,492][root][INFO] - Training Epoch: 8/10, step 306/574 completed (loss: 0.09392883628606796, acc: 1.0)
[2024-12-12 03:01:10,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:10,789][root][INFO] - Training Epoch: 8/10, step 307/574 completed (loss: 0.15614035725593567, acc: 0.9473684430122375)
[2024-12-12 03:01:10,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:11,137][root][INFO] - Training Epoch: 8/10, step 308/574 completed (loss: 0.45797455310821533, acc: 0.8405796885490417)
[2024-12-12 03:01:11,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:11,557][root][INFO] - Training Epoch: 8/10, step 309/574 completed (loss: 0.44538629055023193, acc: 0.9027777910232544)
[2024-12-12 03:01:11,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:11,890][root][INFO] - Training Epoch: 8/10, step 310/574 completed (loss: 0.47243237495422363, acc: 0.8313252925872803)
[2024-12-12 03:01:12,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:12,252][root][INFO] - Training Epoch: 8/10, step 311/574 completed (loss: 0.47556543350219727, acc: 0.8589743375778198)
[2024-12-12 03:01:12,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:12,647][root][INFO] - Training Epoch: 8/10, step 312/574 completed (loss: 0.6535220146179199, acc: 0.7755101919174194)
[2024-12-12 03:01:12,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:13,014][root][INFO] - Training Epoch: 8/10, step 313/574 completed (loss: 0.005608818028122187, acc: 1.0)
[2024-12-12 03:01:13,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:13,356][root][INFO] - Training Epoch: 8/10, step 314/574 completed (loss: 0.05820126459002495, acc: 0.9583333134651184)
[2024-12-12 03:01:13,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:13,690][root][INFO] - Training Epoch: 8/10, step 315/574 completed (loss: 0.025988029316067696, acc: 1.0)
[2024-12-12 03:01:13,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:14,044][root][INFO] - Training Epoch: 8/10, step 316/574 completed (loss: 0.44964274764060974, acc: 0.9354838728904724)
[2024-12-12 03:01:14,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:14,397][root][INFO] - Training Epoch: 8/10, step 317/574 completed (loss: 0.29787489771842957, acc: 0.9104477763175964)
[2024-12-12 03:01:14,478][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:14,758][root][INFO] - Training Epoch: 8/10, step 318/574 completed (loss: 0.6138662695884705, acc: 0.817307710647583)
[2024-12-12 03:01:14,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:15,126][root][INFO] - Training Epoch: 8/10, step 319/574 completed (loss: 0.22847679257392883, acc: 0.9333333373069763)
[2024-12-12 03:01:15,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:15,504][root][INFO] - Training Epoch: 8/10, step 320/574 completed (loss: 0.07766695320606232, acc: 0.9838709831237793)
[2024-12-12 03:01:15,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:15,897][root][INFO] - Training Epoch: 8/10, step 321/574 completed (loss: 0.24923741817474365, acc: 0.9599999785423279)
[2024-12-12 03:01:16,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:16,241][root][INFO] - Training Epoch: 8/10, step 322/574 completed (loss: 0.3187658190727234, acc: 0.9259259104728699)
[2024-12-12 03:01:16,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:16,551][root][INFO] - Training Epoch: 8/10, step 323/574 completed (loss: 0.27223265171051025, acc: 0.9428571462631226)
[2024-12-12 03:01:16,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:16,850][root][INFO] - Training Epoch: 8/10, step 324/574 completed (loss: 0.3051246106624603, acc: 0.8717948794364929)
[2024-12-12 03:01:16,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:17,152][root][INFO] - Training Epoch: 8/10, step 325/574 completed (loss: 0.2844531238079071, acc: 0.9024389982223511)
[2024-12-12 03:01:17,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:17,461][root][INFO] - Training Epoch: 8/10, step 326/574 completed (loss: 0.2082349807024002, acc: 0.9210526347160339)
[2024-12-12 03:01:17,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:17,836][root][INFO] - Training Epoch: 8/10, step 327/574 completed (loss: 0.08210068941116333, acc: 0.9473684430122375)
[2024-12-12 03:01:17,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:18,158][root][INFO] - Training Epoch: 8/10, step 328/574 completed (loss: 0.09688783437013626, acc: 0.9642857313156128)
[2024-12-12 03:01:18,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:18,477][root][INFO] - Training Epoch: 8/10, step 329/574 completed (loss: 0.15016944706439972, acc: 0.9629629850387573)
[2024-12-12 03:01:18,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:18,839][root][INFO] - Training Epoch: 8/10, step 330/574 completed (loss: 0.2065710723400116, acc: 0.9375)
[2024-12-12 03:01:18,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:19,189][root][INFO] - Training Epoch: 8/10, step 331/574 completed (loss: 0.33790215849876404, acc: 0.9032257795333862)
[2024-12-12 03:01:19,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:19,613][root][INFO] - Training Epoch: 8/10, step 332/574 completed (loss: 0.36466631293296814, acc: 0.9122806787490845)
[2024-12-12 03:01:19,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:19,967][root][INFO] - Training Epoch: 8/10, step 333/574 completed (loss: 0.30866414308547974, acc: 0.90625)
[2024-12-12 03:01:20,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:20,367][root][INFO] - Training Epoch: 8/10, step 334/574 completed (loss: 0.18180114030838013, acc: 0.9333333373069763)
[2024-12-12 03:01:20,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:20,674][root][INFO] - Training Epoch: 8/10, step 335/574 completed (loss: 0.08074291795492172, acc: 0.9473684430122375)
[2024-12-12 03:01:20,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:20,977][root][INFO] - Training Epoch: 8/10, step 336/574 completed (loss: 0.32050877809524536, acc: 0.9200000166893005)
[2024-12-12 03:01:21,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:21,356][root][INFO] - Training Epoch: 8/10, step 337/574 completed (loss: 0.9612050652503967, acc: 0.7356321811676025)
[2024-12-12 03:01:21,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:21,729][root][INFO] - Training Epoch: 8/10, step 338/574 completed (loss: 0.9815880656242371, acc: 0.7021276354789734)
[2024-12-12 03:01:21,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:22,066][root][INFO] - Training Epoch: 8/10, step 339/574 completed (loss: 1.1060594320297241, acc: 0.6746987700462341)
[2024-12-12 03:01:22,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:22,440][root][INFO] - Training Epoch: 8/10, step 340/574 completed (loss: 0.009479600004851818, acc: 1.0)
[2024-12-12 03:01:22,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:22,822][root][INFO] - Training Epoch: 8/10, step 341/574 completed (loss: 0.036354079842567444, acc: 1.0)
[2024-12-12 03:01:22,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:23,214][root][INFO] - Training Epoch: 8/10, step 342/574 completed (loss: 0.5760748386383057, acc: 0.8433734774589539)
[2024-12-12 03:01:23,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:23,547][root][INFO] - Training Epoch: 8/10, step 343/574 completed (loss: 0.4192167818546295, acc: 0.9056603908538818)
[2024-12-12 03:01:23,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:23,814][root][INFO] - Training Epoch: 8/10, step 344/574 completed (loss: 0.28776997327804565, acc: 0.9113923907279968)
[2024-12-12 03:01:23,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:24,181][root][INFO] - Training Epoch: 8/10, step 345/574 completed (loss: 0.2787236273288727, acc: 0.9215686321258545)
[2024-12-12 03:01:24,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:24,533][root][INFO] - Training Epoch: 8/10, step 346/574 completed (loss: 0.5965093374252319, acc: 0.8059701323509216)
[2024-12-12 03:01:24,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:24,900][root][INFO] - Training Epoch: 8/10, step 347/574 completed (loss: 0.006799527909606695, acc: 1.0)
[2024-12-12 03:01:25,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:25,269][root][INFO] - Training Epoch: 8/10, step 348/574 completed (loss: 0.11157172173261642, acc: 0.9599999785423279)
[2024-12-12 03:01:25,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:25,649][root][INFO] - Training Epoch: 8/10, step 349/574 completed (loss: 0.1805938184261322, acc: 0.9722222089767456)
[2024-12-12 03:01:25,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:26,029][root][INFO] - Training Epoch: 8/10, step 350/574 completed (loss: 0.49526816606521606, acc: 0.8604651093482971)
[2024-12-12 03:01:26,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:26,427][root][INFO] - Training Epoch: 8/10, step 351/574 completed (loss: 0.1516771912574768, acc: 0.9743589758872986)
[2024-12-12 03:01:26,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:26,799][root][INFO] - Training Epoch: 8/10, step 352/574 completed (loss: 0.45553576946258545, acc: 0.8666666746139526)
[2024-12-12 03:01:26,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:27,140][root][INFO] - Training Epoch: 8/10, step 353/574 completed (loss: 0.052522923797369, acc: 0.95652174949646)
[2024-12-12 03:01:27,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:27,519][root][INFO] - Training Epoch: 8/10, step 354/574 completed (loss: 0.06491108238697052, acc: 1.0)
[2024-12-12 03:01:27,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:27,875][root][INFO] - Training Epoch: 8/10, step 355/574 completed (loss: 0.9016527533531189, acc: 0.7472527623176575)
[2024-12-12 03:01:28,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:28,372][root][INFO] - Training Epoch: 8/10, step 356/574 completed (loss: 0.9128289222717285, acc: 0.747826099395752)
[2024-12-12 03:01:28,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:28,695][root][INFO] - Training Epoch: 8/10, step 357/574 completed (loss: 0.5960996747016907, acc: 0.79347825050354)
[2024-12-12 03:01:28,810][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:29,088][root][INFO] - Training Epoch: 8/10, step 358/574 completed (loss: 0.20386354625225067, acc: 0.918367326259613)
[2024-12-12 03:01:29,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:29,471][root][INFO] - Training Epoch: 8/10, step 359/574 completed (loss: 0.003932982217520475, acc: 1.0)
[2024-12-12 03:01:29,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:29,803][root][INFO] - Training Epoch: 8/10, step 360/574 completed (loss: 0.02313622646033764, acc: 1.0)
[2024-12-12 03:01:29,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:30,191][root][INFO] - Training Epoch: 8/10, step 361/574 completed (loss: 0.21143245697021484, acc: 0.9024389982223511)
[2024-12-12 03:01:30,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:30,609][root][INFO] - Training Epoch: 8/10, step 362/574 completed (loss: 0.2743183672428131, acc: 0.9333333373069763)
[2024-12-12 03:01:30,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:30,991][root][INFO] - Training Epoch: 8/10, step 363/574 completed (loss: 0.47099193930625916, acc: 0.8289473652839661)
[2024-12-12 03:01:31,101][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:31,341][root][INFO] - Training Epoch: 8/10, step 364/574 completed (loss: 0.2125389724969864, acc: 0.9268292784690857)
[2024-12-12 03:01:31,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:31,644][root][INFO] - Training Epoch: 8/10, step 365/574 completed (loss: 0.04933479055762291, acc: 1.0)
[2024-12-12 03:01:31,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:31,958][root][INFO] - Training Epoch: 8/10, step 366/574 completed (loss: 0.015641240403056145, acc: 1.0)
[2024-12-12 03:01:32,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:32,285][root][INFO] - Training Epoch: 8/10, step 367/574 completed (loss: 0.007309005130082369, acc: 1.0)
[2024-12-12 03:01:32,364][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:32,591][root][INFO] - Training Epoch: 8/10, step 368/574 completed (loss: 0.078206367790699, acc: 1.0)
[2024-12-12 03:01:32,696][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:32,974][root][INFO] - Training Epoch: 8/10, step 369/574 completed (loss: 0.0154517637565732, acc: 1.0)
[2024-12-12 03:01:33,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:33,588][root][INFO] - Training Epoch: 8/10, step 370/574 completed (loss: 1.1675673723220825, acc: 0.678787887096405)
[2024-12-12 03:01:33,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:34,447][root][INFO] - Training Epoch: 8/10, step 371/574 completed (loss: 0.6903201341629028, acc: 0.7735849022865295)
[2024-12-12 03:01:34,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:34,857][root][INFO] - Training Epoch: 8/10, step 372/574 completed (loss: 0.4002806842327118, acc: 0.8999999761581421)
[2024-12-12 03:01:34,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:35,204][root][INFO] - Training Epoch: 8/10, step 373/574 completed (loss: 0.20767399668693542, acc: 0.9107142686843872)
[2024-12-12 03:01:35,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:35,578][root][INFO] - Training Epoch: 8/10, step 374/574 completed (loss: 0.14821572601795197, acc: 0.9428571462631226)
[2024-12-12 03:01:35,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:35,912][root][INFO] - Training Epoch: 8/10, step 375/574 completed (loss: 0.0061722551472485065, acc: 1.0)
[2024-12-12 03:01:36,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:36,284][root][INFO] - Training Epoch: 8/10, step 376/574 completed (loss: 0.23357447981834412, acc: 0.95652174949646)
[2024-12-12 03:01:36,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:36,643][root][INFO] - Training Epoch: 8/10, step 377/574 completed (loss: 0.2042350172996521, acc: 0.9375)
[2024-12-12 03:01:36,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:37,006][root][INFO] - Training Epoch: 8/10, step 378/574 completed (loss: 0.36362430453300476, acc: 0.8736842274665833)
[2024-12-12 03:01:37,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:37,581][root][INFO] - Training Epoch: 8/10, step 379/574 completed (loss: 0.8542613983154297, acc: 0.7904191613197327)
[2024-12-12 03:01:37,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:38,011][root][INFO] - Training Epoch: 8/10, step 380/574 completed (loss: 0.650005578994751, acc: 0.8496240377426147)
[2024-12-12 03:01:38,513][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:39,302][root][INFO] - Training Epoch: 8/10, step 381/574 completed (loss: 0.9407913088798523, acc: 0.759358286857605)
[2024-12-12 03:01:39,443][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:39,870][root][INFO] - Training Epoch: 8/10, step 382/574 completed (loss: 0.46063247323036194, acc: 0.8468468189239502)
[2024-12-12 03:01:39,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:40,184][root][INFO] - Training Epoch: 8/10, step 383/574 completed (loss: 0.1621173918247223, acc: 0.9642857313156128)
[2024-12-12 03:01:40,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:40,540][root][INFO] - Training Epoch: 8/10, step 384/574 completed (loss: 0.04203013330698013, acc: 0.9642857313156128)
[2024-12-12 03:01:40,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:40,913][root][INFO] - Training Epoch: 8/10, step 385/574 completed (loss: 0.2052585482597351, acc: 0.9375)
[2024-12-12 03:01:41,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:41,257][root][INFO] - Training Epoch: 8/10, step 386/574 completed (loss: 0.032376185059547424, acc: 1.0)
[2024-12-12 03:01:41,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:41,611][root][INFO] - Training Epoch: 8/10, step 387/574 completed (loss: 0.023888498544692993, acc: 1.0)
[2024-12-12 03:01:41,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:41,961][root][INFO] - Training Epoch: 8/10, step 388/574 completed (loss: 0.003633960383012891, acc: 1.0)
[2024-12-12 03:01:42,038][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:42,326][root][INFO] - Training Epoch: 8/10, step 389/574 completed (loss: 0.004278166685253382, acc: 1.0)
[2024-12-12 03:01:42,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:42,700][root][INFO] - Training Epoch: 8/10, step 390/574 completed (loss: 0.05371284484863281, acc: 1.0)
[2024-12-12 03:01:42,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:43,048][root][INFO] - Training Epoch: 8/10, step 391/574 completed (loss: 0.4322463274002075, acc: 0.8703703880310059)
[2024-12-12 03:01:43,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:43,461][root][INFO] - Training Epoch: 8/10, step 392/574 completed (loss: 0.8683465123176575, acc: 0.7184466123580933)
[2024-12-12 03:01:43,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:43,985][root][INFO] - Training Epoch: 8/10, step 393/574 completed (loss: 1.1650300025939941, acc: 0.6985294222831726)
[2024-12-12 03:01:44,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:44,351][root][INFO] - Training Epoch: 8/10, step 394/574 completed (loss: 0.9769526124000549, acc: 0.7333333492279053)
[2024-12-12 03:01:44,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:44,729][root][INFO] - Training Epoch: 8/10, step 395/574 completed (loss: 1.0712974071502686, acc: 0.6736111044883728)
[2024-12-12 03:01:44,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:45,105][root][INFO] - Training Epoch: 8/10, step 396/574 completed (loss: 0.4601283371448517, acc: 0.8837209343910217)
[2024-12-12 03:01:45,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:45,462][root][INFO] - Training Epoch: 8/10, step 397/574 completed (loss: 0.004526364617049694, acc: 1.0)
[2024-12-12 03:01:45,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:45,784][root][INFO] - Training Epoch: 8/10, step 398/574 completed (loss: 0.42163002490997314, acc: 0.9069767594337463)
[2024-12-12 03:01:45,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:46,089][root][INFO] - Training Epoch: 8/10, step 399/574 completed (loss: 0.054168786853551865, acc: 1.0)
[2024-12-12 03:01:46,226][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:46,620][root][INFO] - Training Epoch: 8/10, step 400/574 completed (loss: 0.4579026699066162, acc: 0.8529411554336548)
[2024-12-12 03:01:46,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:46,989][root][INFO] - Training Epoch: 8/10, step 401/574 completed (loss: 0.3747817575931549, acc: 0.8133333325386047)
[2024-12-12 03:01:47,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:47,328][root][INFO] - Training Epoch: 8/10, step 402/574 completed (loss: 0.08068115264177322, acc: 1.0)
[2024-12-12 03:01:47,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:47,644][root][INFO] - Training Epoch: 8/10, step 403/574 completed (loss: 0.058738552033901215, acc: 1.0)
[2024-12-12 03:01:47,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:48,002][root][INFO] - Training Epoch: 8/10, step 404/574 completed (loss: 0.20981259644031525, acc: 0.9354838728904724)
[2024-12-12 03:01:48,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:48,357][root][INFO] - Training Epoch: 8/10, step 405/574 completed (loss: 0.02794458530843258, acc: 1.0)
[2024-12-12 03:01:48,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:48,737][root][INFO] - Training Epoch: 8/10, step 406/574 completed (loss: 0.025264225900173187, acc: 1.0)
[2024-12-12 03:01:48,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:49,120][root][INFO] - Training Epoch: 8/10, step 407/574 completed (loss: 0.05885264649987221, acc: 1.0)
[2024-12-12 03:01:49,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:49,504][root][INFO] - Training Epoch: 8/10, step 408/574 completed (loss: 0.09236511588096619, acc: 0.9629629850387573)
[2024-12-12 03:01:49,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:49,892][root][INFO] - Training Epoch: 8/10, step 409/574 completed (loss: 0.053215596824884415, acc: 0.9615384340286255)
[2024-12-12 03:01:49,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:50,216][root][INFO] - Training Epoch: 8/10, step 410/574 completed (loss: 0.1533375233411789, acc: 0.931034505367279)
[2024-12-12 03:01:50,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:50,536][root][INFO] - Training Epoch: 8/10, step 411/574 completed (loss: 0.08015622943639755, acc: 0.9642857313156128)
[2024-12-12 03:01:50,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:50,869][root][INFO] - Training Epoch: 8/10, step 412/574 completed (loss: 0.160038024187088, acc: 0.9666666388511658)
[2024-12-12 03:01:50,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:51,193][root][INFO] - Training Epoch: 8/10, step 413/574 completed (loss: 0.27428823709487915, acc: 0.939393937587738)
[2024-12-12 03:01:51,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:51,566][root][INFO] - Training Epoch: 8/10, step 414/574 completed (loss: 0.013687037862837315, acc: 1.0)
[2024-12-12 03:01:52,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:52,604][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:52,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:53,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:53,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:54,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:54,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:54,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:55,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:55,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:55,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:56,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:56,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:57,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:57,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:57,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:58,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:58,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:58,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:59,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:59,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:01:59,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:00,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:00,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:00,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:01,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:01,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:01,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:02,322][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:02,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:03,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:03,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:03,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:03,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:04,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:04,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:05,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:05,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:05,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:06,197][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:06,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:06,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:07,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:07,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:07,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:08,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:08,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:08,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:09,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:09,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:09,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:10,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:10,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:10,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:11,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:11,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:11,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:12,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:12,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:13,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:13,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:13,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:14,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:14,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:15,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:15,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:15,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:16,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:16,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:17,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:17,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:17,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:18,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:18,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:18,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:19,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:19,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:19,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:20,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:20,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:20,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:21,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:21,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:21,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:22,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:22,811][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.6499, device='cuda:0') eval_epoch_loss=tensor(1.5368, device='cuda:0') eval_epoch_acc=tensor(0.6604, device='cuda:0')
[2024-12-12 03:02:22,812][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:02:22,812][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:02:23,063][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_8_step_415_loss_1.5368496179580688/model.pt
[2024-12-12 03:02:23,066][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 03:02:23,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:23,485][root][INFO] - Training Epoch: 8/10, step 415/574 completed (loss: 0.5616664290428162, acc: 0.8235294222831726)
[2024-12-12 03:02:23,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:23,859][root][INFO] - Training Epoch: 8/10, step 416/574 completed (loss: 0.2759047746658325, acc: 0.8846153616905212)
[2024-12-12 03:02:23,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:24,235][root][INFO] - Training Epoch: 8/10, step 417/574 completed (loss: 0.10415790975093842, acc: 0.9444444179534912)
[2024-12-12 03:02:24,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:24,604][root][INFO] - Training Epoch: 8/10, step 418/574 completed (loss: 0.36632952094078064, acc: 0.8500000238418579)
[2024-12-12 03:02:24,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:24,941][root][INFO] - Training Epoch: 8/10, step 419/574 completed (loss: 0.04592863842844963, acc: 1.0)
[2024-12-12 03:02:25,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:25,278][root][INFO] - Training Epoch: 8/10, step 420/574 completed (loss: 0.028504598885774612, acc: 1.0)
[2024-12-12 03:02:25,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:25,606][root][INFO] - Training Epoch: 8/10, step 421/574 completed (loss: 0.13178953528404236, acc: 0.9666666388511658)
[2024-12-12 03:02:25,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:25,918][root][INFO] - Training Epoch: 8/10, step 422/574 completed (loss: 0.07708974927663803, acc: 1.0)
[2024-12-12 03:02:26,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:26,301][root][INFO] - Training Epoch: 8/10, step 423/574 completed (loss: 0.19708415865898132, acc: 0.9166666865348816)
[2024-12-12 03:02:26,407][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:26,643][root][INFO] - Training Epoch: 8/10, step 424/574 completed (loss: 0.18222978711128235, acc: 0.9259259104728699)
[2024-12-12 03:02:26,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:26,995][root][INFO] - Training Epoch: 8/10, step 425/574 completed (loss: 0.18844348192214966, acc: 0.939393937587738)
[2024-12-12 03:02:27,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:27,355][root][INFO] - Training Epoch: 8/10, step 426/574 completed (loss: 0.02617671713232994, acc: 1.0)
[2024-12-12 03:02:27,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:27,725][root][INFO] - Training Epoch: 8/10, step 427/574 completed (loss: 0.293773353099823, acc: 0.9459459185600281)
[2024-12-12 03:02:27,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:28,056][root][INFO] - Training Epoch: 8/10, step 428/574 completed (loss: 0.11427439004182816, acc: 0.9629629850387573)
[2024-12-12 03:02:28,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:28,370][root][INFO] - Training Epoch: 8/10, step 429/574 completed (loss: 0.04025085270404816, acc: 0.95652174949646)
[2024-12-12 03:02:28,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:28,716][root][INFO] - Training Epoch: 8/10, step 430/574 completed (loss: 0.00274736643768847, acc: 1.0)
[2024-12-12 03:02:28,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:29,089][root][INFO] - Training Epoch: 8/10, step 431/574 completed (loss: 0.018004192039370537, acc: 1.0)
[2024-12-12 03:02:29,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:29,417][root][INFO] - Training Epoch: 8/10, step 432/574 completed (loss: 0.0028422218747437, acc: 1.0)
[2024-12-12 03:02:29,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:29,827][root][INFO] - Training Epoch: 8/10, step 433/574 completed (loss: 0.2317444533109665, acc: 0.9444444179534912)
[2024-12-12 03:02:29,936][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:30,200][root][INFO] - Training Epoch: 8/10, step 434/574 completed (loss: 0.012693625874817371, acc: 1.0)
[2024-12-12 03:02:30,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:30,576][root][INFO] - Training Epoch: 8/10, step 435/574 completed (loss: 0.07978233695030212, acc: 0.9696969985961914)
[2024-12-12 03:02:30,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:30,950][root][INFO] - Training Epoch: 8/10, step 436/574 completed (loss: 0.20967751741409302, acc: 0.9444444179534912)
[2024-12-12 03:02:31,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:31,317][root][INFO] - Training Epoch: 8/10, step 437/574 completed (loss: 0.13108620047569275, acc: 0.9545454382896423)
[2024-12-12 03:02:31,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:31,686][root][INFO] - Training Epoch: 8/10, step 438/574 completed (loss: 0.015373589470982552, acc: 1.0)
[2024-12-12 03:02:31,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:32,012][root][INFO] - Training Epoch: 8/10, step 439/574 completed (loss: 0.14010119438171387, acc: 0.9487179517745972)
[2024-12-12 03:02:32,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:32,472][root][INFO] - Training Epoch: 8/10, step 440/574 completed (loss: 0.4802829325199127, acc: 0.8939393758773804)
[2024-12-12 03:02:32,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:33,304][root][INFO] - Training Epoch: 8/10, step 441/574 completed (loss: 1.2581197023391724, acc: 0.6399999856948853)
[2024-12-12 03:02:33,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:33,711][root][INFO] - Training Epoch: 8/10, step 442/574 completed (loss: 0.9866980910301208, acc: 0.7096773982048035)
[2024-12-12 03:02:33,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:34,362][root][INFO] - Training Epoch: 8/10, step 443/574 completed (loss: 1.0766874551773071, acc: 0.6965174078941345)
[2024-12-12 03:02:34,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:34,698][root][INFO] - Training Epoch: 8/10, step 444/574 completed (loss: 0.3603016436100006, acc: 0.9245283007621765)
[2024-12-12 03:02:34,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:35,133][root][INFO] - Training Epoch: 8/10, step 445/574 completed (loss: 0.09939200431108475, acc: 0.9772727489471436)
[2024-12-12 03:02:35,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:35,514][root][INFO] - Training Epoch: 8/10, step 446/574 completed (loss: 0.3103850781917572, acc: 0.8260869383811951)
[2024-12-12 03:02:35,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:35,880][root][INFO] - Training Epoch: 8/10, step 447/574 completed (loss: 0.17806078493595123, acc: 0.9615384340286255)
[2024-12-12 03:02:35,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:36,204][root][INFO] - Training Epoch: 8/10, step 448/574 completed (loss: 0.05521440505981445, acc: 0.9642857313156128)
[2024-12-12 03:02:36,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:36,500][root][INFO] - Training Epoch: 8/10, step 449/574 completed (loss: 0.14111526310443878, acc: 0.9701492786407471)
[2024-12-12 03:02:36,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:36,866][root][INFO] - Training Epoch: 8/10, step 450/574 completed (loss: 0.25645530223846436, acc: 0.9444444179534912)
[2024-12-12 03:02:36,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:37,245][root][INFO] - Training Epoch: 8/10, step 451/574 completed (loss: 0.3198457956314087, acc: 0.8913043737411499)
[2024-12-12 03:02:37,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:37,630][root][INFO] - Training Epoch: 8/10, step 452/574 completed (loss: 0.2632063925266266, acc: 0.8974359035491943)
[2024-12-12 03:02:37,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:38,007][root][INFO] - Training Epoch: 8/10, step 453/574 completed (loss: 0.26170045137405396, acc: 0.9078947305679321)
[2024-12-12 03:02:38,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:38,343][root][INFO] - Training Epoch: 8/10, step 454/574 completed (loss: 0.14499309659004211, acc: 0.9795918464660645)
[2024-12-12 03:02:38,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:38,656][root][INFO] - Training Epoch: 8/10, step 455/574 completed (loss: 0.0226436797529459, acc: 1.0)
[2024-12-12 03:02:38,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:39,025][root][INFO] - Training Epoch: 8/10, step 456/574 completed (loss: 0.8232499957084656, acc: 0.7525773048400879)
[2024-12-12 03:02:39,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:39,417][root][INFO] - Training Epoch: 8/10, step 457/574 completed (loss: 0.24487994611263275, acc: 0.9428571462631226)
[2024-12-12 03:02:39,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:39,832][root][INFO] - Training Epoch: 8/10, step 458/574 completed (loss: 0.8218854069709778, acc: 0.7732558250427246)
[2024-12-12 03:02:39,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:40,197][root][INFO] - Training Epoch: 8/10, step 459/574 completed (loss: 0.24662257730960846, acc: 0.9464285969734192)
[2024-12-12 03:02:40,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:40,583][root][INFO] - Training Epoch: 8/10, step 460/574 completed (loss: 0.4778324067592621, acc: 0.8765432238578796)
[2024-12-12 03:02:40,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:40,953][root][INFO] - Training Epoch: 8/10, step 461/574 completed (loss: 0.2164512723684311, acc: 0.9722222089767456)
[2024-12-12 03:02:41,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:41,315][root][INFO] - Training Epoch: 8/10, step 462/574 completed (loss: 0.04216434806585312, acc: 1.0)
[2024-12-12 03:02:41,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:41,696][root][INFO] - Training Epoch: 8/10, step 463/574 completed (loss: 0.23587197065353394, acc: 0.9230769276618958)
[2024-12-12 03:02:41,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:42,084][root][INFO] - Training Epoch: 8/10, step 464/574 completed (loss: 0.21494141221046448, acc: 0.95652174949646)
[2024-12-12 03:02:42,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:42,436][root][INFO] - Training Epoch: 8/10, step 465/574 completed (loss: 0.23777489364147186, acc: 0.9285714030265808)
[2024-12-12 03:02:42,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:42,718][root][INFO] - Training Epoch: 8/10, step 466/574 completed (loss: 0.4150257706642151, acc: 0.8795180916786194)
[2024-12-12 03:02:42,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:43,129][root][INFO] - Training Epoch: 8/10, step 467/574 completed (loss: 0.4428982436656952, acc: 0.8558558821678162)
[2024-12-12 03:02:43,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:43,513][root][INFO] - Training Epoch: 8/10, step 468/574 completed (loss: 0.6456166505813599, acc: 0.8349514603614807)
[2024-12-12 03:02:43,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:43,898][root][INFO] - Training Epoch: 8/10, step 469/574 completed (loss: 0.5395702719688416, acc: 0.8536585569381714)
[2024-12-12 03:02:44,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:44,209][root][INFO] - Training Epoch: 8/10, step 470/574 completed (loss: 0.04156501963734627, acc: 1.0)
[2024-12-12 03:02:44,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:44,531][root][INFO] - Training Epoch: 8/10, step 471/574 completed (loss: 0.03073074109852314, acc: 1.0)
[2024-12-12 03:02:44,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:44,929][root][INFO] - Training Epoch: 8/10, step 472/574 completed (loss: 0.829113781452179, acc: 0.7647058963775635)
[2024-12-12 03:02:45,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:45,271][root][INFO] - Training Epoch: 8/10, step 473/574 completed (loss: 1.1688252687454224, acc: 0.6419214010238647)
[2024-12-12 03:02:45,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:45,653][root][INFO] - Training Epoch: 8/10, step 474/574 completed (loss: 0.4875603914260864, acc: 0.8229166865348816)
[2024-12-12 03:02:45,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:46,023][root][INFO] - Training Epoch: 8/10, step 475/574 completed (loss: 0.8949716091156006, acc: 0.7300613522529602)
[2024-12-12 03:02:46,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:46,338][root][INFO] - Training Epoch: 8/10, step 476/574 completed (loss: 0.7154247164726257, acc: 0.8057553768157959)
[2024-12-12 03:02:46,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:46,722][root][INFO] - Training Epoch: 8/10, step 477/574 completed (loss: 1.066973328590393, acc: 0.7185929417610168)
[2024-12-12 03:02:46,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:47,121][root][INFO] - Training Epoch: 8/10, step 478/574 completed (loss: 0.09295020252466202, acc: 0.9722222089767456)
[2024-12-12 03:02:47,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:47,452][root][INFO] - Training Epoch: 8/10, step 479/574 completed (loss: 0.24486590921878815, acc: 0.939393937587738)
[2024-12-12 03:02:47,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:47,798][root][INFO] - Training Epoch: 8/10, step 480/574 completed (loss: 0.35855528712272644, acc: 0.8888888955116272)
[2024-12-12 03:02:47,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:48,183][root][INFO] - Training Epoch: 8/10, step 481/574 completed (loss: 0.2789943516254425, acc: 0.949999988079071)
[2024-12-12 03:02:48,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:48,510][root][INFO] - Training Epoch: 8/10, step 482/574 completed (loss: 0.08617101609706879, acc: 1.0)
[2024-12-12 03:02:48,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:48,886][root][INFO] - Training Epoch: 8/10, step 483/574 completed (loss: 0.40708211064338684, acc: 0.8965517282485962)
[2024-12-12 03:02:48,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:49,150][root][INFO] - Training Epoch: 8/10, step 484/574 completed (loss: 0.05239712819457054, acc: 1.0)
[2024-12-12 03:02:49,242][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:49,487][root][INFO] - Training Epoch: 8/10, step 485/574 completed (loss: 0.02494400180876255, acc: 1.0)
[2024-12-12 03:02:49,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:49,858][root][INFO] - Training Epoch: 8/10, step 486/574 completed (loss: 0.0907009169459343, acc: 0.9629629850387573)
[2024-12-12 03:02:49,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:50,249][root][INFO] - Training Epoch: 8/10, step 487/574 completed (loss: 0.3456069827079773, acc: 0.9523809552192688)
[2024-12-12 03:02:50,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:50,583][root][INFO] - Training Epoch: 8/10, step 488/574 completed (loss: 0.16168975830078125, acc: 0.9090909361839294)
[2024-12-12 03:02:50,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:50,930][root][INFO] - Training Epoch: 8/10, step 489/574 completed (loss: 0.5648635625839233, acc: 0.8153846263885498)
[2024-12-12 03:02:51,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:51,268][root][INFO] - Training Epoch: 8/10, step 490/574 completed (loss: 0.23856188356876373, acc: 0.9666666388511658)
[2024-12-12 03:02:51,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:51,597][root][INFO] - Training Epoch: 8/10, step 491/574 completed (loss: 0.06208820268511772, acc: 1.0)
[2024-12-12 03:02:51,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:51,958][root][INFO] - Training Epoch: 8/10, step 492/574 completed (loss: 0.3628486096858978, acc: 0.843137264251709)
[2024-12-12 03:02:52,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:52,293][root][INFO] - Training Epoch: 8/10, step 493/574 completed (loss: 0.16343486309051514, acc: 0.9655172228813171)
[2024-12-12 03:02:52,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:52,584][root][INFO] - Training Epoch: 8/10, step 494/574 completed (loss: 0.051663197576999664, acc: 1.0)
[2024-12-12 03:02:52,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:52,940][root][INFO] - Training Epoch: 8/10, step 495/574 completed (loss: 0.10938595235347748, acc: 0.9473684430122375)
[2024-12-12 03:02:53,047][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:53,292][root][INFO] - Training Epoch: 8/10, step 496/574 completed (loss: 0.9513983726501465, acc: 0.7678571343421936)
[2024-12-12 03:02:53,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:53,699][root][INFO] - Training Epoch: 8/10, step 497/574 completed (loss: 0.5376981496810913, acc: 0.8314606547355652)
[2024-12-12 03:02:53,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:54,069][root][INFO] - Training Epoch: 8/10, step 498/574 completed (loss: 0.7391735315322876, acc: 0.7415730357170105)
[2024-12-12 03:02:54,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:54,464][root][INFO] - Training Epoch: 8/10, step 499/574 completed (loss: 1.2798339128494263, acc: 0.609929084777832)
[2024-12-12 03:02:54,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:54,857][root][INFO] - Training Epoch: 8/10, step 500/574 completed (loss: 0.9151042699813843, acc: 0.72826087474823)
[2024-12-12 03:02:54,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:55,205][root][INFO] - Training Epoch: 8/10, step 501/574 completed (loss: 0.08567193895578384, acc: 0.9599999785423279)
[2024-12-12 03:02:55,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:55,548][root][INFO] - Training Epoch: 8/10, step 502/574 completed (loss: 0.038726381957530975, acc: 1.0)
[2024-12-12 03:02:55,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:55,890][root][INFO] - Training Epoch: 8/10, step 503/574 completed (loss: 0.03027965873479843, acc: 1.0)
[2024-12-12 03:02:56,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:56,241][root][INFO] - Training Epoch: 8/10, step 504/574 completed (loss: 0.06956411898136139, acc: 1.0)
[2024-12-12 03:02:56,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:56,545][root][INFO] - Training Epoch: 8/10, step 505/574 completed (loss: 0.11897092312574387, acc: 0.9622641801834106)
[2024-12-12 03:02:56,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:56,934][root][INFO] - Training Epoch: 8/10, step 506/574 completed (loss: 0.5512309074401855, acc: 0.8620689511299133)
[2024-12-12 03:02:57,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:57,531][root][INFO] - Training Epoch: 8/10, step 507/574 completed (loss: 0.9241010546684265, acc: 0.7117117047309875)
[2024-12-12 03:02:57,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:57,968][root][INFO] - Training Epoch: 8/10, step 508/574 completed (loss: 0.608699381351471, acc: 0.8450704216957092)
[2024-12-12 03:02:58,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:58,279][root][INFO] - Training Epoch: 8/10, step 509/574 completed (loss: 0.061223387718200684, acc: 0.949999988079071)
[2024-12-12 03:02:58,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:58,625][root][INFO] - Training Epoch: 8/10, step 510/574 completed (loss: 0.01398684922605753, acc: 1.0)
[2024-12-12 03:02:58,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:02:58,954][root][INFO] - Training Epoch: 8/10, step 511/574 completed (loss: 0.006294958293437958, acc: 1.0)
[2024-12-12 03:03:00,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:01,785][root][INFO] - Training Epoch: 8/10, step 512/574 completed (loss: 1.3672057390213013, acc: 0.6285714507102966)
[2024-12-12 03:03:01,988][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:02,551][root][INFO] - Training Epoch: 8/10, step 513/574 completed (loss: 0.6826037764549255, acc: 0.761904776096344)
[2024-12-12 03:03:02,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:02,907][root][INFO] - Training Epoch: 8/10, step 514/574 completed (loss: 0.19336692988872528, acc: 0.9285714030265808)
[2024-12-12 03:03:03,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:03,196][root][INFO] - Training Epoch: 8/10, step 515/574 completed (loss: 0.2654494345188141, acc: 0.8999999761581421)
[2024-12-12 03:03:03,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:03,893][root][INFO] - Training Epoch: 8/10, step 516/574 completed (loss: 0.5282241106033325, acc: 0.8194444179534912)
[2024-12-12 03:03:04,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:04,247][root][INFO] - Training Epoch: 8/10, step 517/574 completed (loss: 0.0027207492385059595, acc: 1.0)
[2024-12-12 03:03:04,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:04,562][root][INFO] - Training Epoch: 8/10, step 518/574 completed (loss: 0.1048908531665802, acc: 0.9354838728904724)
[2024-12-12 03:03:04,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:04,892][root][INFO] - Training Epoch: 8/10, step 519/574 completed (loss: 0.13549496233463287, acc: 0.949999988079071)
[2024-12-12 03:03:04,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:05,253][root][INFO] - Training Epoch: 8/10, step 520/574 completed (loss: 0.3874746859073639, acc: 0.8888888955116272)
[2024-12-12 03:03:05,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:06,246][root][INFO] - Training Epoch: 8/10, step 521/574 completed (loss: 1.4233125448226929, acc: 0.5805084705352783)
[2024-12-12 03:03:06,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:06,614][root][INFO] - Training Epoch: 8/10, step 522/574 completed (loss: 0.7119781970977783, acc: 0.8059701323509216)
[2024-12-12 03:03:06,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:06,986][root][INFO] - Training Epoch: 8/10, step 523/574 completed (loss: 0.775749146938324, acc: 0.7664233446121216)
[2024-12-12 03:03:07,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:07,554][root][INFO] - Training Epoch: 8/10, step 524/574 completed (loss: 1.1252614259719849, acc: 0.6899999976158142)
[2024-12-12 03:03:07,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:07,899][root][INFO] - Training Epoch: 8/10, step 525/574 completed (loss: 0.10799503326416016, acc: 0.9814814925193787)
[2024-12-12 03:03:08,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:08,244][root][INFO] - Training Epoch: 8/10, step 526/574 completed (loss: 0.15797211229801178, acc: 0.9615384340286255)
[2024-12-12 03:03:08,321][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:08,524][root][INFO] - Training Epoch: 8/10, step 527/574 completed (loss: 0.2994365394115448, acc: 0.9047619104385376)
[2024-12-12 03:03:08,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:08,911][root][INFO] - Training Epoch: 8/10, step 528/574 completed (loss: 0.47440221905708313, acc: 0.868852436542511)
[2024-12-12 03:03:09,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:09,238][root][INFO] - Training Epoch: 8/10, step 529/574 completed (loss: 0.4278049170970917, acc: 0.8474576473236084)
[2024-12-12 03:03:09,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:09,599][root][INFO] - Training Epoch: 8/10, step 530/574 completed (loss: 0.9513968825340271, acc: 0.7209302186965942)
[2024-12-12 03:03:09,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:09,968][root][INFO] - Training Epoch: 8/10, step 531/574 completed (loss: 0.2887159287929535, acc: 0.8863636255264282)
[2024-12-12 03:03:10,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:10,315][root][INFO] - Training Epoch: 8/10, step 532/574 completed (loss: 0.40393462777137756, acc: 0.9056603908538818)
[2024-12-12 03:03:10,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:10,735][root][INFO] - Training Epoch: 8/10, step 533/574 completed (loss: 0.18519824743270874, acc: 0.9545454382896423)
[2024-12-12 03:03:10,862][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:11,102][root][INFO] - Training Epoch: 8/10, step 534/574 completed (loss: 0.21089692413806915, acc: 0.9200000166893005)
[2024-12-12 03:03:11,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:11,467][root][INFO] - Training Epoch: 8/10, step 535/574 completed (loss: 0.2119121253490448, acc: 0.8999999761581421)
[2024-12-12 03:03:11,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:11,810][root][INFO] - Training Epoch: 8/10, step 536/574 completed (loss: 0.010522856377065182, acc: 1.0)
[2024-12-12 03:03:11,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:12,218][root][INFO] - Training Epoch: 8/10, step 537/574 completed (loss: 0.259871244430542, acc: 0.9384615421295166)
[2024-12-12 03:03:12,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:12,607][root][INFO] - Training Epoch: 8/10, step 538/574 completed (loss: 0.2615252733230591, acc: 0.890625)
[2024-12-12 03:03:12,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:13,015][root][INFO] - Training Epoch: 8/10, step 539/574 completed (loss: 0.17504450678825378, acc: 0.96875)
[2024-12-12 03:03:13,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:13,333][root][INFO] - Training Epoch: 8/10, step 540/574 completed (loss: 0.38928595185279846, acc: 0.9696969985961914)
[2024-12-12 03:03:13,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:13,695][root][INFO] - Training Epoch: 8/10, step 541/574 completed (loss: 0.055619288235902786, acc: 1.0)
[2024-12-12 03:03:13,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:14,028][root][INFO] - Training Epoch: 8/10, step 542/574 completed (loss: 0.021345781162381172, acc: 1.0)
[2024-12-12 03:03:14,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:14,300][root][INFO] - Training Epoch: 8/10, step 543/574 completed (loss: 0.2180992215871811, acc: 0.9130434989929199)
[2024-12-12 03:03:14,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:14,642][root][INFO] - Training Epoch: 8/10, step 544/574 completed (loss: 0.07492712140083313, acc: 0.9666666388511658)
[2024-12-12 03:03:14,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:15,029][root][INFO] - Training Epoch: 8/10, step 545/574 completed (loss: 0.13366512954235077, acc: 0.9512194991111755)
[2024-12-12 03:03:15,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:15,411][root][INFO] - Training Epoch: 8/10, step 546/574 completed (loss: 0.011986429803073406, acc: 1.0)
[2024-12-12 03:03:15,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:15,779][root][INFO] - Training Epoch: 8/10, step 547/574 completed (loss: 0.12848396599292755, acc: 0.9473684430122375)
[2024-12-12 03:03:15,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:16,116][root][INFO] - Training Epoch: 8/10, step 548/574 completed (loss: 0.17362214624881744, acc: 0.9354838728904724)
[2024-12-12 03:03:16,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:16,464][root][INFO] - Training Epoch: 8/10, step 549/574 completed (loss: 0.006174926180392504, acc: 1.0)
[2024-12-12 03:03:16,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:16,814][root][INFO] - Training Epoch: 8/10, step 550/574 completed (loss: 0.12664493918418884, acc: 0.9696969985961914)
[2024-12-12 03:03:16,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:17,223][root][INFO] - Training Epoch: 8/10, step 551/574 completed (loss: 0.11244001239538193, acc: 0.9750000238418579)
[2024-12-12 03:03:17,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:17,540][root][INFO] - Training Epoch: 8/10, step 552/574 completed (loss: 0.2376471906900406, acc: 0.8999999761581421)
[2024-12-12 03:03:17,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:17,888][root][INFO] - Training Epoch: 8/10, step 553/574 completed (loss: 0.5407025218009949, acc: 0.8613138794898987)
[2024-12-12 03:03:18,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:18,294][root][INFO] - Training Epoch: 8/10, step 554/574 completed (loss: 0.5436290502548218, acc: 0.8206896781921387)
[2024-12-12 03:03:18,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:18,635][root][INFO] - Training Epoch: 8/10, step 555/574 completed (loss: 0.7286590933799744, acc: 0.7714285850524902)
[2024-12-12 03:03:18,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:18,994][root][INFO] - Training Epoch: 8/10, step 556/574 completed (loss: 0.7210920453071594, acc: 0.7814569473266602)
[2024-12-12 03:03:19,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:19,354][root][INFO] - Training Epoch: 8/10, step 557/574 completed (loss: 0.38875776529312134, acc: 0.8461538553237915)
[2024-12-12 03:03:20,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:20,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:20,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:21,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:21,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:21,631][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:21,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:22,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:22,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:22,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:23,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:23,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:24,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:24,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:24,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:25,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:25,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:25,895][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:26,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:26,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:27,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:27,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:27,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:27,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:28,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:28,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:29,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:29,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:29,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:30,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:30,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:30,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:31,062][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:31,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:31,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:32,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:32,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:32,763][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:33,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:33,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:33,731][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:34,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:34,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:34,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:35,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:35,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:35,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:36,266][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:36,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:36,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:37,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:37,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:37,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:38,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:38,496][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:38,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:39,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:39,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:39,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:40,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:40,352][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:40,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:41,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:41,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:41,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:42,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:42,538][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:42,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:43,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:43,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:44,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:44,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:44,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:45,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:45,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:45,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:46,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:46,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:46,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:47,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:47,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:47,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:48,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:48,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:48,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:49,494][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.2617, device='cuda:0') eval_epoch_loss=tensor(1.4497, device='cuda:0') eval_epoch_acc=tensor(0.6975, device='cuda:0')
[2024-12-12 03:03:49,496][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:03:49,496][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:03:49,741][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_8_step_558_loss_1.4496753215789795/model.pt
[2024-12-12 03:03:49,744][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 03:03:49,876][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:50,132][root][INFO] - Training Epoch: 8/10, step 558/574 completed (loss: 0.009261876344680786, acc: 1.0)
[2024-12-12 03:03:50,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:50,443][root][INFO] - Training Epoch: 8/10, step 559/574 completed (loss: 0.2445622980594635, acc: 0.9615384340286255)
[2024-12-12 03:03:50,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:50,763][root][INFO] - Training Epoch: 8/10, step 560/574 completed (loss: 0.011685466393828392, acc: 1.0)
[2024-12-12 03:03:50,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:51,120][root][INFO] - Training Epoch: 8/10, step 561/574 completed (loss: 0.31496885418891907, acc: 0.9230769276618958)
[2024-12-12 03:03:51,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:51,522][root][INFO] - Training Epoch: 8/10, step 562/574 completed (loss: 0.4000353217124939, acc: 0.9111111164093018)
[2024-12-12 03:03:51,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:51,869][root][INFO] - Training Epoch: 8/10, step 563/574 completed (loss: 0.25586333870887756, acc: 0.9220778942108154)
[2024-12-12 03:03:51,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:52,258][root][INFO] - Training Epoch: 8/10, step 564/574 completed (loss: 0.19380466639995575, acc: 0.9166666865348816)
[2024-12-12 03:03:52,357][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:52,615][root][INFO] - Training Epoch: 8/10, step 565/574 completed (loss: 0.21967722475528717, acc: 0.9137930870056152)
[2024-12-12 03:03:52,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:52,985][root][INFO] - Training Epoch: 8/10, step 566/574 completed (loss: 0.31065306067466736, acc: 0.9047619104385376)
[2024-12-12 03:03:53,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:53,361][root][INFO] - Training Epoch: 8/10, step 567/574 completed (loss: 0.030227556824684143, acc: 1.0)
[2024-12-12 03:03:53,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:53,722][root][INFO] - Training Epoch: 8/10, step 568/574 completed (loss: 0.023772012442350388, acc: 1.0)
[2024-12-12 03:03:53,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:54,131][root][INFO] - Training Epoch: 8/10, step 569/574 completed (loss: 0.7983295917510986, acc: 0.7967914342880249)
[2024-12-12 03:03:54,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:54,516][root][INFO] - Training Epoch: 8/10, step 570/574 completed (loss: 0.1308792382478714, acc: 0.9516128897666931)
[2024-12-12 03:03:54,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:54,860][root][INFO] - Training Epoch: 8/10, step 571/574 completed (loss: 0.31115278601646423, acc: 0.8974359035491943)
[2024-12-12 03:03:54,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:55,193][root][INFO] - Training Epoch: 8/10, step 572/574 completed (loss: 1.0908584594726562, acc: 0.6887755393981934)
[2024-12-12 03:03:55,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:55,546][root][INFO] - Training Epoch: 8/10, step 573/574 completed (loss: 0.7882729172706604, acc: 0.7358490824699402)
[2024-12-12 03:03:55,962][slam_llm.utils.train_utils][INFO] - Epoch 8: train_perplexity=1.5106, train_epoch_loss=0.4125, epoch time 358.6894986741245s
[2024-12-12 03:03:55,963][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 03:03:55,963][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2024-12-12 03:03:55,963][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 03:03:55,963][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 22
[2024-12-12 03:03:55,963][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 03:03:56,510][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:56,845][root][INFO] - Training Epoch: 9/10, step 0/574 completed (loss: 0.18269261717796326, acc: 0.9629629850387573)
[2024-12-12 03:03:56,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:57,210][root][INFO] - Training Epoch: 9/10, step 1/574 completed (loss: 0.07597919553518295, acc: 1.0)
[2024-12-12 03:03:57,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:57,555][root][INFO] - Training Epoch: 9/10, step 2/574 completed (loss: 0.16894850134849548, acc: 0.9459459185600281)
[2024-12-12 03:03:57,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:57,918][root][INFO] - Training Epoch: 9/10, step 3/574 completed (loss: 0.14837659895420074, acc: 0.9736841917037964)
[2024-12-12 03:03:58,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:58,327][root][INFO] - Training Epoch: 9/10, step 4/574 completed (loss: 0.22081972658634186, acc: 0.8918918967247009)
[2024-12-12 03:03:58,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:58,710][root][INFO] - Training Epoch: 9/10, step 5/574 completed (loss: 0.13554692268371582, acc: 0.9642857313156128)
[2024-12-12 03:03:58,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:59,065][root][INFO] - Training Epoch: 9/10, step 6/574 completed (loss: 0.3873211145401001, acc: 0.8775510191917419)
[2024-12-12 03:03:59,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:59,429][root][INFO] - Training Epoch: 9/10, step 7/574 completed (loss: 0.04474881663918495, acc: 1.0)
[2024-12-12 03:03:59,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:03:59,791][root][INFO] - Training Epoch: 9/10, step 8/574 completed (loss: 0.008080524392426014, acc: 1.0)
[2024-12-12 03:03:59,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:00,177][root][INFO] - Training Epoch: 9/10, step 9/574 completed (loss: 0.004293876700103283, acc: 1.0)
[2024-12-12 03:04:00,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:00,505][root][INFO] - Training Epoch: 9/10, step 10/574 completed (loss: 0.06939899176359177, acc: 0.9629629850387573)
[2024-12-12 03:04:00,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:00,836][root][INFO] - Training Epoch: 9/10, step 11/574 completed (loss: 0.16195978224277496, acc: 0.8974359035491943)
[2024-12-12 03:04:00,947][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:01,185][root][INFO] - Training Epoch: 9/10, step 12/574 completed (loss: 0.07678292691707611, acc: 0.9696969985961914)
[2024-12-12 03:04:01,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:01,522][root][INFO] - Training Epoch: 9/10, step 13/574 completed (loss: 0.13044001162052155, acc: 0.95652174949646)
[2024-12-12 03:04:01,605][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:01,830][root][INFO] - Training Epoch: 9/10, step 14/574 completed (loss: 0.15180760622024536, acc: 0.9215686321258545)
[2024-12-12 03:04:01,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:02,192][root][INFO] - Training Epoch: 9/10, step 15/574 completed (loss: 0.14797504246234894, acc: 0.9591836929321289)
[2024-12-12 03:04:02,289][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:02,526][root][INFO] - Training Epoch: 9/10, step 16/574 completed (loss: 0.21369221806526184, acc: 0.9473684430122375)
[2024-12-12 03:04:02,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:02,884][root][INFO] - Training Epoch: 9/10, step 17/574 completed (loss: 0.046487513929605484, acc: 1.0)
[2024-12-12 03:04:02,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:03,218][root][INFO] - Training Epoch: 9/10, step 18/574 completed (loss: 0.11380484700202942, acc: 1.0)
[2024-12-12 03:04:03,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:03,573][root][INFO] - Training Epoch: 9/10, step 19/574 completed (loss: 0.07242780923843384, acc: 1.0)
[2024-12-12 03:04:03,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:03,927][root][INFO] - Training Epoch: 9/10, step 20/574 completed (loss: 0.2052479088306427, acc: 0.8846153616905212)
[2024-12-12 03:04:04,043][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:04,305][root][INFO] - Training Epoch: 9/10, step 21/574 completed (loss: 0.3766708970069885, acc: 0.8965517282485962)
[2024-12-12 03:04:04,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:04,649][root][INFO] - Training Epoch: 9/10, step 22/574 completed (loss: 0.26150625944137573, acc: 0.9599999785423279)
[2024-12-12 03:04:04,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:05,015][root][INFO] - Training Epoch: 9/10, step 23/574 completed (loss: 0.606277346611023, acc: 0.8571428656578064)
[2024-12-12 03:04:05,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:05,417][root][INFO] - Training Epoch: 9/10, step 24/574 completed (loss: 0.004334578290581703, acc: 1.0)
[2024-12-12 03:04:05,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:05,763][root][INFO] - Training Epoch: 9/10, step 25/574 completed (loss: 0.25816527009010315, acc: 0.9056603908538818)
[2024-12-12 03:04:05,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:06,130][root][INFO] - Training Epoch: 9/10, step 26/574 completed (loss: 0.746968686580658, acc: 0.7534246444702148)
[2024-12-12 03:04:06,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:07,386][root][INFO] - Training Epoch: 9/10, step 27/574 completed (loss: 1.6164063215255737, acc: 0.5533596873283386)
[2024-12-12 03:04:07,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:07,750][root][INFO] - Training Epoch: 9/10, step 28/574 completed (loss: 0.31444287300109863, acc: 0.9069767594337463)
[2024-12-12 03:04:07,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:08,152][root][INFO] - Training Epoch: 9/10, step 29/574 completed (loss: 0.634957492351532, acc: 0.8313252925872803)
[2024-12-12 03:04:08,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:08,550][root][INFO] - Training Epoch: 9/10, step 30/574 completed (loss: 0.43512141704559326, acc: 0.8888888955116272)
[2024-12-12 03:04:08,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:08,910][root][INFO] - Training Epoch: 9/10, step 31/574 completed (loss: 0.37811240553855896, acc: 0.8928571343421936)
[2024-12-12 03:04:09,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:09,266][root][INFO] - Training Epoch: 9/10, step 32/574 completed (loss: 0.12624605000019073, acc: 0.9259259104728699)
[2024-12-12 03:04:09,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:09,626][root][INFO] - Training Epoch: 9/10, step 33/574 completed (loss: 0.01710510440170765, acc: 1.0)
[2024-12-12 03:04:09,739][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:10,033][root][INFO] - Training Epoch: 9/10, step 34/574 completed (loss: 0.6414912939071655, acc: 0.7899159789085388)
[2024-12-12 03:04:10,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:10,387][root][INFO] - Training Epoch: 9/10, step 35/574 completed (loss: 0.23716960847377777, acc: 0.9344262480735779)
[2024-12-12 03:04:10,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:10,791][root][INFO] - Training Epoch: 9/10, step 36/574 completed (loss: 0.4193156659603119, acc: 0.8730158805847168)
[2024-12-12 03:04:10,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:11,161][root][INFO] - Training Epoch: 9/10, step 37/574 completed (loss: 0.35625600814819336, acc: 0.8813559412956238)
[2024-12-12 03:04:11,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:11,525][root][INFO] - Training Epoch: 9/10, step 38/574 completed (loss: 0.5020037889480591, acc: 0.8620689511299133)
[2024-12-12 03:04:11,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:11,905][root][INFO] - Training Epoch: 9/10, step 39/574 completed (loss: 0.05542604252696037, acc: 1.0)
[2024-12-12 03:04:12,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:12,289][root][INFO] - Training Epoch: 9/10, step 40/574 completed (loss: 0.2509802579879761, acc: 0.9230769276618958)
[2024-12-12 03:04:12,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:12,706][root][INFO] - Training Epoch: 9/10, step 41/574 completed (loss: 0.37957462668418884, acc: 0.8648648858070374)
[2024-12-12 03:04:12,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:13,094][root][INFO] - Training Epoch: 9/10, step 42/574 completed (loss: 0.48905932903289795, acc: 0.8153846263885498)
[2024-12-12 03:04:13,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:13,523][root][INFO] - Training Epoch: 9/10, step 43/574 completed (loss: 0.974936306476593, acc: 0.7171717286109924)
[2024-12-12 03:04:13,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:13,964][root][INFO] - Training Epoch: 9/10, step 44/574 completed (loss: 0.48498278856277466, acc: 0.876288652420044)
[2024-12-12 03:04:14,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:14,391][root][INFO] - Training Epoch: 9/10, step 45/574 completed (loss: 0.8419963717460632, acc: 0.75)
[2024-12-12 03:04:14,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:14,769][root][INFO] - Training Epoch: 9/10, step 46/574 completed (loss: 0.12969690561294556, acc: 0.9230769276618958)
[2024-12-12 03:04:14,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:15,133][root][INFO] - Training Epoch: 9/10, step 47/574 completed (loss: 0.04409944266080856, acc: 1.0)
[2024-12-12 03:04:15,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:15,540][root][INFO] - Training Epoch: 9/10, step 48/574 completed (loss: 0.2746996581554413, acc: 0.9642857313156128)
[2024-12-12 03:04:15,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:15,924][root][INFO] - Training Epoch: 9/10, step 49/574 completed (loss: 0.07040788978338242, acc: 0.9722222089767456)
[2024-12-12 03:04:16,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:16,280][root][INFO] - Training Epoch: 9/10, step 50/574 completed (loss: 0.4405873417854309, acc: 0.8421052694320679)
[2024-12-12 03:04:16,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:16,618][root][INFO] - Training Epoch: 9/10, step 51/574 completed (loss: 0.5074397325515747, acc: 0.8095238208770752)
[2024-12-12 03:04:16,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:16,961][root][INFO] - Training Epoch: 9/10, step 52/574 completed (loss: 0.6511128544807434, acc: 0.8591549396514893)
[2024-12-12 03:04:17,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:17,430][root][INFO] - Training Epoch: 9/10, step 53/574 completed (loss: 1.411334753036499, acc: 0.6000000238418579)
[2024-12-12 03:04:17,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:17,820][root][INFO] - Training Epoch: 9/10, step 54/574 completed (loss: 0.23772066831588745, acc: 0.9189189076423645)
[2024-12-12 03:04:17,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:18,232][root][INFO] - Training Epoch: 9/10, step 55/574 completed (loss: 0.25847652554512024, acc: 0.9230769276618958)
[2024-12-12 03:04:19,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:21,217][root][INFO] - Training Epoch: 9/10, step 56/574 completed (loss: 1.477024793624878, acc: 0.6075085401535034)
[2024-12-12 03:04:21,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:22,557][root][INFO] - Training Epoch: 9/10, step 57/574 completed (loss: 1.8373656272888184, acc: 0.501089334487915)
[2024-12-12 03:04:22,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:23,183][root][INFO] - Training Epoch: 9/10, step 58/574 completed (loss: 1.2102524042129517, acc: 0.6534090638160706)
[2024-12-12 03:04:23,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:23,751][root][INFO] - Training Epoch: 9/10, step 59/574 completed (loss: 0.8479240536689758, acc: 0.7647058963775635)
[2024-12-12 03:04:23,933][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:24,325][root][INFO] - Training Epoch: 9/10, step 60/574 completed (loss: 1.0439375638961792, acc: 0.6594203114509583)
[2024-12-12 03:04:24,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:24,744][root][INFO] - Training Epoch: 9/10, step 61/574 completed (loss: 0.7582621574401855, acc: 0.7250000238418579)
[2024-12-12 03:04:24,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:25,119][root][INFO] - Training Epoch: 9/10, step 62/574 completed (loss: 0.13293616473674774, acc: 0.970588207244873)
[2024-12-12 03:04:25,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:25,514][root][INFO] - Training Epoch: 9/10, step 63/574 completed (loss: 0.31228482723236084, acc: 0.8611111044883728)
[2024-12-12 03:04:25,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:25,877][root][INFO] - Training Epoch: 9/10, step 64/574 completed (loss: 0.20358027517795563, acc: 0.9375)
[2024-12-12 03:04:25,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:26,193][root][INFO] - Training Epoch: 9/10, step 65/574 completed (loss: 0.08926620334386826, acc: 0.9655172228813171)
[2024-12-12 03:04:26,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:26,580][root][INFO] - Training Epoch: 9/10, step 66/574 completed (loss: 0.3781115710735321, acc: 0.875)
[2024-12-12 03:04:26,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:26,962][root][INFO] - Training Epoch: 9/10, step 67/574 completed (loss: 0.2390815168619156, acc: 0.949999988079071)
[2024-12-12 03:04:27,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:27,349][root][INFO] - Training Epoch: 9/10, step 68/574 completed (loss: 0.09017129987478256, acc: 0.9599999785423279)
[2024-12-12 03:04:27,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:27,696][root][INFO] - Training Epoch: 9/10, step 69/574 completed (loss: 0.06550682336091995, acc: 0.9722222089767456)
[2024-12-12 03:04:27,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:28,034][root][INFO] - Training Epoch: 9/10, step 70/574 completed (loss: 0.11316605657339096, acc: 0.9696969985961914)
[2024-12-12 03:04:28,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:28,340][root][INFO] - Training Epoch: 9/10, step 71/574 completed (loss: 0.9382632970809937, acc: 0.6838235259056091)
[2024-12-12 03:04:28,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:28,715][root][INFO] - Training Epoch: 9/10, step 72/574 completed (loss: 0.7186399102210999, acc: 0.7857142686843872)
[2024-12-12 03:04:28,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:29,114][root][INFO] - Training Epoch: 9/10, step 73/574 completed (loss: 1.4183253049850464, acc: 0.5743589997291565)
[2024-12-12 03:04:29,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:29,389][root][INFO] - Training Epoch: 9/10, step 74/574 completed (loss: 0.5397868752479553, acc: 0.8163265585899353)
[2024-12-12 03:04:29,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:29,755][root][INFO] - Training Epoch: 9/10, step 75/574 completed (loss: 1.0855579376220703, acc: 0.6940298676490784)
[2024-12-12 03:04:29,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:30,138][root][INFO] - Training Epoch: 9/10, step 76/574 completed (loss: 1.534313440322876, acc: 0.5985401272773743)
[2024-12-12 03:04:30,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:30,496][root][INFO] - Training Epoch: 9/10, step 77/574 completed (loss: 0.021710563451051712, acc: 1.0)
[2024-12-12 03:04:30,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:30,827][root][INFO] - Training Epoch: 9/10, step 78/574 completed (loss: 0.04535601660609245, acc: 0.9583333134651184)
[2024-12-12 03:04:30,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:31,205][root][INFO] - Training Epoch: 9/10, step 79/574 completed (loss: 0.03224576637148857, acc: 1.0)
[2024-12-12 03:04:31,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:31,535][root][INFO] - Training Epoch: 9/10, step 80/574 completed (loss: 0.004725490231066942, acc: 1.0)
[2024-12-12 03:04:31,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:31,839][root][INFO] - Training Epoch: 9/10, step 81/574 completed (loss: 0.2732239365577698, acc: 0.9038461446762085)
[2024-12-12 03:04:31,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:32,206][root][INFO] - Training Epoch: 9/10, step 82/574 completed (loss: 0.3654666244983673, acc: 0.9038461446762085)
[2024-12-12 03:04:32,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:32,590][root][INFO] - Training Epoch: 9/10, step 83/574 completed (loss: 0.05541539937257767, acc: 1.0)
[2024-12-12 03:04:32,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:32,979][root][INFO] - Training Epoch: 9/10, step 84/574 completed (loss: 0.3568362593650818, acc: 0.9130434989929199)
[2024-12-12 03:04:33,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:33,334][root][INFO] - Training Epoch: 9/10, step 85/574 completed (loss: 0.24299286305904388, acc: 0.9399999976158142)
[2024-12-12 03:04:33,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:33,704][root][INFO] - Training Epoch: 9/10, step 86/574 completed (loss: 0.03999294713139534, acc: 1.0)
[2024-12-12 03:04:33,874][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:34,168][root][INFO] - Training Epoch: 9/10, step 87/574 completed (loss: 0.35747838020324707, acc: 0.8999999761581421)
[2024-12-12 03:04:34,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:34,581][root][INFO] - Training Epoch: 9/10, step 88/574 completed (loss: 0.5250758528709412, acc: 0.8155339956283569)
[2024-12-12 03:04:34,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:35,732][root][INFO] - Training Epoch: 9/10, step 89/574 completed (loss: 0.9369931221008301, acc: 0.7572815418243408)
[2024-12-12 03:04:35,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:36,554][root][INFO] - Training Epoch: 9/10, step 90/574 completed (loss: 1.1908133029937744, acc: 0.6559139490127563)
[2024-12-12 03:04:36,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:37,361][root][INFO] - Training Epoch: 9/10, step 91/574 completed (loss: 1.2468901872634888, acc: 0.6681034564971924)
[2024-12-12 03:04:37,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:38,106][root][INFO] - Training Epoch: 9/10, step 92/574 completed (loss: 0.6205836534500122, acc: 0.7894737124443054)
[2024-12-12 03:04:38,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:39,098][root][INFO] - Training Epoch: 9/10, step 93/574 completed (loss: 0.844276487827301, acc: 0.7029703259468079)
[2024-12-12 03:04:39,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:39,480][root][INFO] - Training Epoch: 9/10, step 94/574 completed (loss: 0.45421841740608215, acc: 0.8548387289047241)
[2024-12-12 03:04:39,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:39,875][root][INFO] - Training Epoch: 9/10, step 95/574 completed (loss: 0.26617345213890076, acc: 0.9275362491607666)
[2024-12-12 03:04:39,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:40,210][root][INFO] - Training Epoch: 9/10, step 96/574 completed (loss: 0.8231711387634277, acc: 0.7478991746902466)
[2024-12-12 03:04:40,320][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:40,597][root][INFO] - Training Epoch: 9/10, step 97/574 completed (loss: 0.8136153817176819, acc: 0.7403846383094788)
[2024-12-12 03:04:40,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:41,020][root][INFO] - Training Epoch: 9/10, step 98/574 completed (loss: 1.0761733055114746, acc: 0.6788321137428284)
[2024-12-12 03:04:41,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:41,396][root][INFO] - Training Epoch: 9/10, step 99/574 completed (loss: 0.34006303548812866, acc: 0.8656716346740723)
[2024-12-12 03:04:41,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:41,772][root][INFO] - Training Epoch: 9/10, step 100/574 completed (loss: 0.4769513010978699, acc: 0.8999999761581421)
[2024-12-12 03:04:41,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:42,160][root][INFO] - Training Epoch: 9/10, step 101/574 completed (loss: 0.04350988566875458, acc: 0.9545454382896423)
[2024-12-12 03:04:42,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:42,496][root][INFO] - Training Epoch: 9/10, step 102/574 completed (loss: 0.009851256385445595, acc: 1.0)
[2024-12-12 03:04:42,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:42,865][root][INFO] - Training Epoch: 9/10, step 103/574 completed (loss: 0.013826495036482811, acc: 1.0)
[2024-12-12 03:04:42,998][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:43,240][root][INFO] - Training Epoch: 9/10, step 104/574 completed (loss: 0.23151376843452454, acc: 0.9137930870056152)
[2024-12-12 03:04:43,351][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:43,613][root][INFO] - Training Epoch: 9/10, step 105/574 completed (loss: 0.10041649639606476, acc: 0.9534883499145508)
[2024-12-12 03:04:43,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:44,008][root][INFO] - Training Epoch: 9/10, step 106/574 completed (loss: 0.040986720472574234, acc: 1.0)
[2024-12-12 03:04:44,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:44,339][root][INFO] - Training Epoch: 9/10, step 107/574 completed (loss: 0.022342870011925697, acc: 1.0)
[2024-12-12 03:04:44,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:44,721][root][INFO] - Training Epoch: 9/10, step 108/574 completed (loss: 0.0018426507012918591, acc: 1.0)
[2024-12-12 03:04:44,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:45,065][root][INFO] - Training Epoch: 9/10, step 109/574 completed (loss: 0.12971454858779907, acc: 0.9523809552192688)
[2024-12-12 03:04:45,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:45,454][root][INFO] - Training Epoch: 9/10, step 110/574 completed (loss: 0.37228816747665405, acc: 0.9076923131942749)
[2024-12-12 03:04:45,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:45,890][root][INFO] - Training Epoch: 9/10, step 111/574 completed (loss: 0.36188045144081116, acc: 0.8947368264198303)
[2024-12-12 03:04:46,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:46,261][root][INFO] - Training Epoch: 9/10, step 112/574 completed (loss: 0.3593981862068176, acc: 0.9122806787490845)
[2024-12-12 03:04:46,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:46,621][root][INFO] - Training Epoch: 9/10, step 113/574 completed (loss: 0.25875207781791687, acc: 0.9487179517745972)
[2024-12-12 03:04:46,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:46,992][root][INFO] - Training Epoch: 9/10, step 114/574 completed (loss: 0.27281948924064636, acc: 0.918367326259613)
[2024-12-12 03:04:47,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:47,284][root][INFO] - Training Epoch: 9/10, step 115/574 completed (loss: 0.02128412015736103, acc: 1.0)
[2024-12-12 03:04:47,381][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:47,615][root][INFO] - Training Epoch: 9/10, step 116/574 completed (loss: 0.42116910219192505, acc: 0.8730158805847168)
[2024-12-12 03:04:47,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:48,034][root][INFO] - Training Epoch: 9/10, step 117/574 completed (loss: 0.5979977250099182, acc: 0.8130081295967102)
[2024-12-12 03:04:48,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:48,441][root][INFO] - Training Epoch: 9/10, step 118/574 completed (loss: 0.38331595063209534, acc: 0.8870967626571655)
[2024-12-12 03:04:48,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:49,308][root][INFO] - Training Epoch: 9/10, step 119/574 completed (loss: 1.3651596307754517, acc: 0.6159695982933044)
[2024-12-12 03:04:49,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:49,704][root][INFO] - Training Epoch: 9/10, step 120/574 completed (loss: 0.31382179260253906, acc: 0.8933333158493042)
[2024-12-12 03:04:49,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:50,124][root][INFO] - Training Epoch: 9/10, step 121/574 completed (loss: 0.46233636140823364, acc: 0.8269230723381042)
[2024-12-12 03:04:50,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:50,451][root][INFO] - Training Epoch: 9/10, step 122/574 completed (loss: 0.01881406269967556, acc: 1.0)
[2024-12-12 03:04:50,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:50,765][root][INFO] - Training Epoch: 9/10, step 123/574 completed (loss: 0.23139213025569916, acc: 0.8947368264198303)
[2024-12-12 03:04:50,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:51,144][root][INFO] - Training Epoch: 9/10, step 124/574 completed (loss: 1.098705530166626, acc: 0.6687116622924805)
[2024-12-12 03:04:51,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:51,533][root][INFO] - Training Epoch: 9/10, step 125/574 completed (loss: 0.8653817772865295, acc: 0.7291666865348816)
[2024-12-12 03:04:51,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:51,876][root][INFO] - Training Epoch: 9/10, step 126/574 completed (loss: 0.8209924697875977, acc: 0.7166666388511658)
[2024-12-12 03:04:52,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:52,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:53,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:53,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:53,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:54,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:54,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:55,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:55,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:55,726][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:56,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:56,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:56,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:57,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:57,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:57,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:58,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:58,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:58,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:59,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:59,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:04:59,774][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:00,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:00,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:00,728][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:01,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:01,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:01,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:02,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:02,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:02,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:03,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:03,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:03,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:04,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:04,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:04,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:05,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:05,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:05,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:06,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:06,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:06,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:07,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:07,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:07,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:08,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:08,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:08,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:09,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:09,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:09,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:10,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:10,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:10,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:11,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:11,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:11,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:12,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:12,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:12,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:13,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:13,838][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:14,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:14,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:14,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:15,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:15,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:15,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:16,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:16,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:16,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:17,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:17,586][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:17,861][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:18,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:18,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:18,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:19,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:19,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:19,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:20,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:20,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:20,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:21,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:21,654][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.0457, device='cuda:0') eval_epoch_loss=tensor(1.3977, device='cuda:0') eval_epoch_acc=tensor(0.7018, device='cuda:0')
[2024-12-12 03:05:21,655][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:05:21,656][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:05:21,878][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_9_step_127_loss_1.3976527452468872/model.pt
[2024-12-12 03:05:21,881][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 03:05:21,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:22,344][root][INFO] - Training Epoch: 9/10, step 127/574 completed (loss: 1.0204522609710693, acc: 0.6785714030265808)
[2024-12-12 03:05:22,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:22,720][root][INFO] - Training Epoch: 9/10, step 128/574 completed (loss: 0.869516909122467, acc: 0.7230769395828247)
[2024-12-12 03:05:22,855][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:23,123][root][INFO] - Training Epoch: 9/10, step 129/574 completed (loss: 0.8629459142684937, acc: 0.7279411554336548)
[2024-12-12 03:05:23,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:23,445][root][INFO] - Training Epoch: 9/10, step 130/574 completed (loss: 0.09984246641397476, acc: 0.9615384340286255)
[2024-12-12 03:05:23,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:23,794][root][INFO] - Training Epoch: 9/10, step 131/574 completed (loss: 0.045700155198574066, acc: 1.0)
[2024-12-12 03:05:23,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:24,155][root][INFO] - Training Epoch: 9/10, step 132/574 completed (loss: 0.2098240852355957, acc: 0.9375)
[2024-12-12 03:05:24,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:24,495][root][INFO] - Training Epoch: 9/10, step 133/574 completed (loss: 0.1251925528049469, acc: 0.95652174949646)
[2024-12-12 03:05:24,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:24,852][root][INFO] - Training Epoch: 9/10, step 134/574 completed (loss: 0.10991232842206955, acc: 0.9428571462631226)
[2024-12-12 03:05:24,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:25,182][root][INFO] - Training Epoch: 9/10, step 135/574 completed (loss: 0.026170561090111732, acc: 1.0)
[2024-12-12 03:05:25,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:25,538][root][INFO] - Training Epoch: 9/10, step 136/574 completed (loss: 0.316483736038208, acc: 0.8571428656578064)
[2024-12-12 03:05:25,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:25,851][root][INFO] - Training Epoch: 9/10, step 137/574 completed (loss: 0.5880977511405945, acc: 0.8666666746139526)
[2024-12-12 03:05:25,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:26,166][root][INFO] - Training Epoch: 9/10, step 138/574 completed (loss: 0.5400325059890747, acc: 0.95652174949646)
[2024-12-12 03:05:26,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:26,522][root][INFO] - Training Epoch: 9/10, step 139/574 completed (loss: 0.07687818259000778, acc: 1.0)
[2024-12-12 03:05:26,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:26,893][root][INFO] - Training Epoch: 9/10, step 140/574 completed (loss: 0.02553344890475273, acc: 1.0)
[2024-12-12 03:05:26,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:27,270][root][INFO] - Training Epoch: 9/10, step 141/574 completed (loss: 0.1695973128080368, acc: 0.9354838728904724)
[2024-12-12 03:05:27,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:27,607][root][INFO] - Training Epoch: 9/10, step 142/574 completed (loss: 0.36678165197372437, acc: 0.837837815284729)
[2024-12-12 03:05:27,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:28,129][root][INFO] - Training Epoch: 9/10, step 143/574 completed (loss: 0.5940789580345154, acc: 0.8157894611358643)
[2024-12-12 03:05:28,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:28,527][root][INFO] - Training Epoch: 9/10, step 144/574 completed (loss: 0.6509155035018921, acc: 0.8059701323509216)
[2024-12-12 03:05:28,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:28,937][root][INFO] - Training Epoch: 9/10, step 145/574 completed (loss: 0.5661731958389282, acc: 0.8265306353569031)
[2024-12-12 03:05:29,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:29,371][root][INFO] - Training Epoch: 9/10, step 146/574 completed (loss: 0.6912540793418884, acc: 0.7340425252914429)
[2024-12-12 03:05:29,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:29,724][root][INFO] - Training Epoch: 9/10, step 147/574 completed (loss: 0.5687646865844727, acc: 0.800000011920929)
[2024-12-12 03:05:29,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:30,094][root][INFO] - Training Epoch: 9/10, step 148/574 completed (loss: 0.28369924426078796, acc: 0.8928571343421936)
[2024-12-12 03:05:30,194][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:30,438][root][INFO] - Training Epoch: 9/10, step 149/574 completed (loss: 0.10870382189750671, acc: 0.95652174949646)
[2024-12-12 03:05:30,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:30,777][root][INFO] - Training Epoch: 9/10, step 150/574 completed (loss: 0.34469708800315857, acc: 0.931034505367279)
[2024-12-12 03:05:30,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:31,114][root][INFO] - Training Epoch: 9/10, step 151/574 completed (loss: 0.22071805596351624, acc: 0.9347826242446899)
[2024-12-12 03:05:31,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:31,477][root][INFO] - Training Epoch: 9/10, step 152/574 completed (loss: 0.3906470537185669, acc: 0.8474576473236084)
[2024-12-12 03:05:31,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:31,824][root][INFO] - Training Epoch: 9/10, step 153/574 completed (loss: 0.31482282280921936, acc: 0.9122806787490845)
[2024-12-12 03:05:31,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:32,167][root][INFO] - Training Epoch: 9/10, step 154/574 completed (loss: 0.30881860852241516, acc: 0.9054054021835327)
[2024-12-12 03:05:32,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:32,472][root][INFO] - Training Epoch: 9/10, step 155/574 completed (loss: 0.10336370021104813, acc: 0.9285714030265808)
[2024-12-12 03:05:32,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:32,859][root][INFO] - Training Epoch: 9/10, step 156/574 completed (loss: 0.13065150380134583, acc: 0.9130434989929199)
[2024-12-12 03:05:32,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:33,198][root][INFO] - Training Epoch: 9/10, step 157/574 completed (loss: 0.46456125378608704, acc: 0.8421052694320679)
[2024-12-12 03:05:33,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:34,857][root][INFO] - Training Epoch: 9/10, step 158/574 completed (loss: 0.5593740344047546, acc: 0.837837815284729)
[2024-12-12 03:05:34,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:35,141][root][INFO] - Training Epoch: 9/10, step 159/574 completed (loss: 1.0253159999847412, acc: 0.6851851940155029)
[2024-12-12 03:05:35,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:35,536][root][INFO] - Training Epoch: 9/10, step 160/574 completed (loss: 0.5618069767951965, acc: 0.8255813717842102)
[2024-12-12 03:05:35,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:36,123][root][INFO] - Training Epoch: 9/10, step 161/574 completed (loss: 0.4715190529823303, acc: 0.8588235378265381)
[2024-12-12 03:05:36,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:36,684][root][INFO] - Training Epoch: 9/10, step 162/574 completed (loss: 0.7149558067321777, acc: 0.7640449404716492)
[2024-12-12 03:05:36,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:37,011][root][INFO] - Training Epoch: 9/10, step 163/574 completed (loss: 0.4040820896625519, acc: 0.9090909361839294)
[2024-12-12 03:05:37,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:37,376][root][INFO] - Training Epoch: 9/10, step 164/574 completed (loss: 0.07207140326499939, acc: 0.9523809552192688)
[2024-12-12 03:05:37,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:37,747][root][INFO] - Training Epoch: 9/10, step 165/574 completed (loss: 0.29222050309181213, acc: 0.8965517282485962)
[2024-12-12 03:05:37,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:38,077][root][INFO] - Training Epoch: 9/10, step 166/574 completed (loss: 0.0989111065864563, acc: 0.9795918464660645)
[2024-12-12 03:05:38,162][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:38,416][root][INFO] - Training Epoch: 9/10, step 167/574 completed (loss: 0.4157125949859619, acc: 0.8799999952316284)
[2024-12-12 03:05:38,530][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:38,870][root][INFO] - Training Epoch: 9/10, step 168/574 completed (loss: 0.3743910491466522, acc: 0.9027777910232544)
[2024-12-12 03:05:38,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:39,261][root][INFO] - Training Epoch: 9/10, step 169/574 completed (loss: 0.8376817107200623, acc: 0.7156862616539001)
[2024-12-12 03:05:39,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:40,292][root][INFO] - Training Epoch: 9/10, step 170/574 completed (loss: 1.4988958835601807, acc: 0.5616438388824463)
[2024-12-12 03:05:40,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:40,621][root][INFO] - Training Epoch: 9/10, step 171/574 completed (loss: 0.22221732139587402, acc: 0.9583333134651184)
[2024-12-12 03:05:40,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:40,997][root][INFO] - Training Epoch: 9/10, step 172/574 completed (loss: 0.22603948414325714, acc: 0.9259259104728699)
[2024-12-12 03:05:41,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:41,315][root][INFO] - Training Epoch: 9/10, step 173/574 completed (loss: 0.04191455617547035, acc: 1.0)
[2024-12-12 03:05:41,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:41,853][root][INFO] - Training Epoch: 9/10, step 174/574 completed (loss: 0.6741963028907776, acc: 0.8230088353157043)
[2024-12-12 03:05:41,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:42,148][root][INFO] - Training Epoch: 9/10, step 175/574 completed (loss: 0.40001019835472107, acc: 0.8695651888847351)
[2024-12-12 03:05:42,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:42,499][root][INFO] - Training Epoch: 9/10, step 176/574 completed (loss: 0.46985912322998047, acc: 0.8863636255264282)
[2024-12-12 03:05:42,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:43,414][root][INFO] - Training Epoch: 9/10, step 177/574 completed (loss: 1.4795308113098145, acc: 0.6030534505844116)
[2024-12-12 03:05:43,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:44,086][root][INFO] - Training Epoch: 9/10, step 178/574 completed (loss: 0.9293695688247681, acc: 0.7629629373550415)
[2024-12-12 03:05:44,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:44,433][root][INFO] - Training Epoch: 9/10, step 179/574 completed (loss: 0.2984616458415985, acc: 0.9016393423080444)
[2024-12-12 03:05:44,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:44,759][root][INFO] - Training Epoch: 9/10, step 180/574 completed (loss: 0.06779921054840088, acc: 0.9583333134651184)
[2024-12-12 03:05:44,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:45,075][root][INFO] - Training Epoch: 9/10, step 181/574 completed (loss: 0.03827899321913719, acc: 1.0)
[2024-12-12 03:05:45,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:45,377][root][INFO] - Training Epoch: 9/10, step 182/574 completed (loss: 0.013889709487557411, acc: 1.0)
[2024-12-12 03:05:45,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:45,694][root][INFO] - Training Epoch: 9/10, step 183/574 completed (loss: 0.3684267997741699, acc: 0.8658536672592163)
[2024-12-12 03:05:45,777][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:46,004][root][INFO] - Training Epoch: 9/10, step 184/574 completed (loss: 0.9517650008201599, acc: 0.712990939617157)
[2024-12-12 03:05:46,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:46,363][root][INFO] - Training Epoch: 9/10, step 185/574 completed (loss: 1.2352159023284912, acc: 0.6628242135047913)
[2024-12-12 03:05:46,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:46,851][root][INFO] - Training Epoch: 9/10, step 186/574 completed (loss: 1.159607172012329, acc: 0.6656249761581421)
[2024-12-12 03:05:46,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:47,379][root][INFO] - Training Epoch: 9/10, step 187/574 completed (loss: 1.4124456644058228, acc: 0.6135084629058838)
[2024-12-12 03:05:47,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:47,789][root][INFO] - Training Epoch: 9/10, step 188/574 completed (loss: 0.9812843799591064, acc: 0.7153024673461914)
[2024-12-12 03:05:47,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:48,136][root][INFO] - Training Epoch: 9/10, step 189/574 completed (loss: 0.020402872934937477, acc: 1.0)
[2024-12-12 03:05:48,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:48,685][root][INFO] - Training Epoch: 9/10, step 190/574 completed (loss: 0.958687961101532, acc: 0.7441860437393188)
[2024-12-12 03:05:48,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:49,509][root][INFO] - Training Epoch: 9/10, step 191/574 completed (loss: 1.0462037324905396, acc: 0.682539701461792)
[2024-12-12 03:05:49,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:50,428][root][INFO] - Training Epoch: 9/10, step 192/574 completed (loss: 0.9147593975067139, acc: 0.6969696879386902)
[2024-12-12 03:05:50,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:51,173][root][INFO] - Training Epoch: 9/10, step 193/574 completed (loss: 0.4689660966396332, acc: 0.8588235378265381)
[2024-12-12 03:05:51,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:52,251][root][INFO] - Training Epoch: 9/10, step 194/574 completed (loss: 0.8085218071937561, acc: 0.7716049551963806)
[2024-12-12 03:05:52,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:53,206][root][INFO] - Training Epoch: 9/10, step 195/574 completed (loss: 0.4455380141735077, acc: 0.8709677457809448)
[2024-12-12 03:05:53,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:53,532][root][INFO] - Training Epoch: 9/10, step 196/574 completed (loss: 0.0896172747015953, acc: 0.9642857313156128)
[2024-12-12 03:05:53,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:53,878][root][INFO] - Training Epoch: 9/10, step 197/574 completed (loss: 0.13472478091716766, acc: 0.949999988079071)
[2024-12-12 03:05:53,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:54,216][root][INFO] - Training Epoch: 9/10, step 198/574 completed (loss: 0.3072073459625244, acc: 0.9117646813392639)
[2024-12-12 03:05:54,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:54,570][root][INFO] - Training Epoch: 9/10, step 199/574 completed (loss: 0.7032427787780762, acc: 0.7867646813392639)
[2024-12-12 03:05:54,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:54,934][root][INFO] - Training Epoch: 9/10, step 200/574 completed (loss: 0.5367363095283508, acc: 0.8389830589294434)
[2024-12-12 03:05:55,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:55,307][root][INFO] - Training Epoch: 9/10, step 201/574 completed (loss: 0.6643027067184448, acc: 0.7985074520111084)
[2024-12-12 03:05:55,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:55,671][root][INFO] - Training Epoch: 9/10, step 202/574 completed (loss: 0.7138959169387817, acc: 0.8058252334594727)
[2024-12-12 03:05:55,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:55,957][root][INFO] - Training Epoch: 9/10, step 203/574 completed (loss: 0.3500559628009796, acc: 0.920634925365448)
[2024-12-12 03:05:56,060][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:56,287][root][INFO] - Training Epoch: 9/10, step 204/574 completed (loss: 0.22668910026550293, acc: 0.9120879173278809)
[2024-12-12 03:05:56,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:56,680][root][INFO] - Training Epoch: 9/10, step 205/574 completed (loss: 0.7825173139572144, acc: 0.7757847309112549)
[2024-12-12 03:05:56,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:57,094][root][INFO] - Training Epoch: 9/10, step 206/574 completed (loss: 0.873720645904541, acc: 0.748031497001648)
[2024-12-12 03:05:57,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:57,420][root][INFO] - Training Epoch: 9/10, step 207/574 completed (loss: 0.7709422707557678, acc: 0.7931034564971924)
[2024-12-12 03:05:57,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:57,779][root][INFO] - Training Epoch: 9/10, step 208/574 completed (loss: 0.8820616006851196, acc: 0.7644927501678467)
[2024-12-12 03:05:57,901][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:58,181][root][INFO] - Training Epoch: 9/10, step 209/574 completed (loss: 0.7707089781761169, acc: 0.7704280018806458)
[2024-12-12 03:05:58,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:58,525][root][INFO] - Training Epoch: 9/10, step 210/574 completed (loss: 0.448255330324173, acc: 0.8478260636329651)
[2024-12-12 03:05:58,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:58,859][root][INFO] - Training Epoch: 9/10, step 211/574 completed (loss: 0.008524816483259201, acc: 1.0)
[2024-12-12 03:05:58,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:59,166][root][INFO] - Training Epoch: 9/10, step 212/574 completed (loss: 0.180430606007576, acc: 0.9642857313156128)
[2024-12-12 03:05:59,293][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:05:59,582][root][INFO] - Training Epoch: 9/10, step 213/574 completed (loss: 0.15334372222423553, acc: 0.957446813583374)
[2024-12-12 03:05:59,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:00,272][root][INFO] - Training Epoch: 9/10, step 214/574 completed (loss: 0.41989246010780334, acc: 0.8692307472229004)
[2024-12-12 03:06:00,362][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:00,565][root][INFO] - Training Epoch: 9/10, step 215/574 completed (loss: 0.1676558405160904, acc: 0.9594594836235046)
[2024-12-12 03:06:00,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:00,920][root][INFO] - Training Epoch: 9/10, step 216/574 completed (loss: 0.28544190526008606, acc: 0.8837209343910217)
[2024-12-12 03:06:01,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:01,455][root][INFO] - Training Epoch: 9/10, step 217/574 completed (loss: 0.46142759919166565, acc: 0.8288288116455078)
[2024-12-12 03:06:01,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:01,865][root][INFO] - Training Epoch: 9/10, step 218/574 completed (loss: 0.25645047426223755, acc: 0.9111111164093018)
[2024-12-12 03:06:01,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:02,225][root][INFO] - Training Epoch: 9/10, step 219/574 completed (loss: 0.10580573976039886, acc: 0.9696969985961914)
[2024-12-12 03:06:02,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:02,581][root][INFO] - Training Epoch: 9/10, step 220/574 completed (loss: 0.012042615562677383, acc: 1.0)
[2024-12-12 03:06:02,677][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:02,911][root][INFO] - Training Epoch: 9/10, step 221/574 completed (loss: 0.010119500569999218, acc: 1.0)
[2024-12-12 03:06:03,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:03,323][root][INFO] - Training Epoch: 9/10, step 222/574 completed (loss: 0.23466365039348602, acc: 0.942307710647583)
[2024-12-12 03:06:03,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:04,111][root][INFO] - Training Epoch: 9/10, step 223/574 completed (loss: 0.6458495259284973, acc: 0.79347825050354)
[2024-12-12 03:06:04,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:04,651][root][INFO] - Training Epoch: 9/10, step 224/574 completed (loss: 0.7628650665283203, acc: 0.7840909361839294)
[2024-12-12 03:06:04,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:05,083][root][INFO] - Training Epoch: 9/10, step 225/574 completed (loss: 0.47543230652809143, acc: 0.8297872543334961)
[2024-12-12 03:06:05,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:05,434][root][INFO] - Training Epoch: 9/10, step 226/574 completed (loss: 0.19152407348155975, acc: 0.9622641801834106)
[2024-12-12 03:06:05,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:05,804][root][INFO] - Training Epoch: 9/10, step 227/574 completed (loss: 0.319578617811203, acc: 0.9166666865348816)
[2024-12-12 03:06:05,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:06,191][root][INFO] - Training Epoch: 9/10, step 228/574 completed (loss: 0.2418372631072998, acc: 0.9534883499145508)
[2024-12-12 03:06:06,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:06,540][root][INFO] - Training Epoch: 9/10, step 229/574 completed (loss: 0.23727130889892578, acc: 0.9333333373069763)
[2024-12-12 03:06:06,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:06,923][root][INFO] - Training Epoch: 9/10, step 230/574 completed (loss: 0.7438336610794067, acc: 0.7473683953285217)
[2024-12-12 03:06:07,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:07,262][root][INFO] - Training Epoch: 9/10, step 231/574 completed (loss: 0.6382002234458923, acc: 0.8222222328186035)
[2024-12-12 03:06:07,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:07,696][root][INFO] - Training Epoch: 9/10, step 232/574 completed (loss: 0.7590622901916504, acc: 0.8277778029441833)
[2024-12-12 03:06:07,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:08,190][root][INFO] - Training Epoch: 9/10, step 233/574 completed (loss: 1.226858139038086, acc: 0.6559633016586304)
[2024-12-12 03:06:08,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:08,679][root][INFO] - Training Epoch: 9/10, step 234/574 completed (loss: 0.6866682767868042, acc: 0.800000011920929)
[2024-12-12 03:06:08,790][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:09,056][root][INFO] - Training Epoch: 9/10, step 235/574 completed (loss: 0.27118268609046936, acc: 0.8421052694320679)
[2024-12-12 03:06:09,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:09,424][root][INFO] - Training Epoch: 9/10, step 236/574 completed (loss: 0.09750297665596008, acc: 0.9583333134651184)
[2024-12-12 03:06:09,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:09,801][root][INFO] - Training Epoch: 9/10, step 237/574 completed (loss: 0.11757567524909973, acc: 0.9545454382896423)
[2024-12-12 03:06:09,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:10,182][root][INFO] - Training Epoch: 9/10, step 238/574 completed (loss: 0.3314436078071594, acc: 0.9629629850387573)
[2024-12-12 03:06:10,282][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:10,515][root][INFO] - Training Epoch: 9/10, step 239/574 completed (loss: 0.15939822793006897, acc: 0.9428571462631226)
[2024-12-12 03:06:10,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:10,912][root][INFO] - Training Epoch: 9/10, step 240/574 completed (loss: 0.16400298476219177, acc: 0.9545454382896423)
[2024-12-12 03:06:11,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:11,244][root][INFO] - Training Epoch: 9/10, step 241/574 completed (loss: 0.32978934049606323, acc: 0.9090909361839294)
[2024-12-12 03:06:11,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:11,833][root][INFO] - Training Epoch: 9/10, step 242/574 completed (loss: 0.4348781108856201, acc: 0.8225806355476379)
[2024-12-12 03:06:11,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:12,371][root][INFO] - Training Epoch: 9/10, step 243/574 completed (loss: 0.21856480836868286, acc: 0.9318181872367859)
[2024-12-12 03:06:12,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:12,704][root][INFO] - Training Epoch: 9/10, step 244/574 completed (loss: 0.15127481520175934, acc: 0.9523809552192688)
[2024-12-12 03:06:12,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:13,055][root][INFO] - Training Epoch: 9/10, step 245/574 completed (loss: 0.04192984476685524, acc: 1.0)
[2024-12-12 03:06:13,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:13,378][root][INFO] - Training Epoch: 9/10, step 246/574 completed (loss: 0.020403876900672913, acc: 1.0)
[2024-12-12 03:06:13,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:13,700][root][INFO] - Training Epoch: 9/10, step 247/574 completed (loss: 0.1784660667181015, acc: 0.8999999761581421)
[2024-12-12 03:06:13,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:14,039][root][INFO] - Training Epoch: 9/10, step 248/574 completed (loss: 0.22825390100479126, acc: 0.9459459185600281)
[2024-12-12 03:06:14,114][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:14,400][root][INFO] - Training Epoch: 9/10, step 249/574 completed (loss: 0.19669201970100403, acc: 0.9729729890823364)
[2024-12-12 03:06:14,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:14,729][root][INFO] - Training Epoch: 9/10, step 250/574 completed (loss: 0.17520134150981903, acc: 0.9729729890823364)
[2024-12-12 03:06:14,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:15,081][root][INFO] - Training Epoch: 9/10, step 251/574 completed (loss: 0.21418820321559906, acc: 0.9117646813392639)
[2024-12-12 03:06:15,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:15,451][root][INFO] - Training Epoch: 9/10, step 252/574 completed (loss: 0.05620971694588661, acc: 1.0)
[2024-12-12 03:06:15,581][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:15,828][root][INFO] - Training Epoch: 9/10, step 253/574 completed (loss: 0.005223496351391077, acc: 1.0)
[2024-12-12 03:06:15,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:16,153][root][INFO] - Training Epoch: 9/10, step 254/574 completed (loss: 0.0044724647887051105, acc: 1.0)
[2024-12-12 03:06:16,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:16,490][root][INFO] - Training Epoch: 9/10, step 255/574 completed (loss: 0.06578634679317474, acc: 0.9677419066429138)
[2024-12-12 03:06:16,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:16,855][root][INFO] - Training Epoch: 9/10, step 256/574 completed (loss: 0.11650732904672623, acc: 0.9473684430122375)
[2024-12-12 03:06:16,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:17,233][root][INFO] - Training Epoch: 9/10, step 257/574 completed (loss: 0.18650947511196136, acc: 0.9571428298950195)
[2024-12-12 03:06:17,331][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:17,573][root][INFO] - Training Epoch: 9/10, step 258/574 completed (loss: 0.30113935470581055, acc: 0.9210526347160339)
[2024-12-12 03:06:17,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:18,139][root][INFO] - Training Epoch: 9/10, step 259/574 completed (loss: 0.5282378196716309, acc: 0.8584905862808228)
[2024-12-12 03:06:18,315][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:18,721][root][INFO] - Training Epoch: 9/10, step 260/574 completed (loss: 0.8653491139411926, acc: 0.7583333253860474)
[2024-12-12 03:06:18,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:19,057][root][INFO] - Training Epoch: 9/10, step 261/574 completed (loss: 0.10828263312578201, acc: 0.9722222089767456)
[2024-12-12 03:06:19,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:19,387][root][INFO] - Training Epoch: 9/10, step 262/574 completed (loss: 0.08698978275060654, acc: 0.9677419066429138)
[2024-12-12 03:06:19,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:19,789][root][INFO] - Training Epoch: 9/10, step 263/574 completed (loss: 0.5530345439910889, acc: 0.8533333539962769)
[2024-12-12 03:06:19,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:20,191][root][INFO] - Training Epoch: 9/10, step 264/574 completed (loss: 0.3425854444503784, acc: 0.8958333134651184)
[2024-12-12 03:06:20,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:21,024][root][INFO] - Training Epoch: 9/10, step 265/574 completed (loss: 1.2782368659973145, acc: 0.6159999966621399)
[2024-12-12 03:06:21,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:21,344][root][INFO] - Training Epoch: 9/10, step 266/574 completed (loss: 0.6786583662033081, acc: 0.7977527976036072)
[2024-12-12 03:06:21,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:21,689][root][INFO] - Training Epoch: 9/10, step 267/574 completed (loss: 0.5437634587287903, acc: 0.8243243098258972)
[2024-12-12 03:06:21,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:22,135][root][INFO] - Training Epoch: 9/10, step 268/574 completed (loss: 0.439321905374527, acc: 0.8965517282485962)
[2024-12-12 03:06:22,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:22,495][root][INFO] - Training Epoch: 9/10, step 269/574 completed (loss: 0.013145760633051395, acc: 1.0)
[2024-12-12 03:06:23,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:23,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:23,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:24,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:24,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:25,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:25,393][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:25,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:26,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:26,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:26,891][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:27,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:27,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:27,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:28,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:28,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:28,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:29,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:29,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:30,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:30,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:30,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:30,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:31,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:31,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:32,006][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:32,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:32,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:33,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:33,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:33,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:34,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:34,662][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:35,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:35,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:35,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:36,145][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:36,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:36,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:37,187][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:37,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:37,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:38,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:38,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:38,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:39,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:39,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:39,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:40,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:40,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:40,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:41,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:41,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:41,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:42,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:42,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:43,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:43,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:43,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:44,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:44,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:44,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:45,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:45,656][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:46,065][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:46,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:46,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:47,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:47,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:48,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:48,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:48,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:49,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:49,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:49,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:50,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:50,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:50,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:51,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:51,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:51,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:52,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:52,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:52,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:53,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:53,756][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.0940, device='cuda:0') eval_epoch_loss=tensor(1.4095, device='cuda:0') eval_epoch_acc=tensor(0.6999, device='cuda:0')
[2024-12-12 03:06:53,757][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:06:53,757][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:06:53,973][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_9_step_270_loss_1.4095174074172974/model.pt
[2024-12-12 03:06:53,977][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 03:06:54,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:54,372][root][INFO] - Training Epoch: 9/10, step 270/574 completed (loss: 0.025766372680664062, acc: 1.0)
[2024-12-12 03:06:54,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:54,745][root][INFO] - Training Epoch: 9/10, step 271/574 completed (loss: 0.12730157375335693, acc: 0.96875)
[2024-12-12 03:06:54,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:55,112][root][INFO] - Training Epoch: 9/10, step 272/574 completed (loss: 0.027704386040568352, acc: 1.0)
[2024-12-12 03:06:55,232][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:55,492][root][INFO] - Training Epoch: 9/10, step 273/574 completed (loss: 0.25680938363075256, acc: 0.9166666865348816)
[2024-12-12 03:06:55,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:55,842][root][INFO] - Training Epoch: 9/10, step 274/574 completed (loss: 0.11995767056941986, acc: 0.96875)
[2024-12-12 03:06:55,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:56,164][root][INFO] - Training Epoch: 9/10, step 275/574 completed (loss: 0.05937739089131355, acc: 1.0)
[2024-12-12 03:06:56,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:56,536][root][INFO] - Training Epoch: 9/10, step 276/574 completed (loss: 0.48350486159324646, acc: 0.9655172228813171)
[2024-12-12 03:06:56,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:56,928][root][INFO] - Training Epoch: 9/10, step 277/574 completed (loss: 0.023601457476615906, acc: 1.0)
[2024-12-12 03:06:57,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:57,301][root][INFO] - Training Epoch: 9/10, step 278/574 completed (loss: 0.3117713928222656, acc: 0.8936170339584351)
[2024-12-12 03:06:57,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:57,686][root][INFO] - Training Epoch: 9/10, step 279/574 completed (loss: 0.22181856632232666, acc: 0.9375)
[2024-12-12 03:06:57,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:58,053][root][INFO] - Training Epoch: 9/10, step 280/574 completed (loss: 0.18195639550685883, acc: 0.9545454382896423)
[2024-12-12 03:06:58,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:58,475][root][INFO] - Training Epoch: 9/10, step 281/574 completed (loss: 0.7043962478637695, acc: 0.7710843086242676)
[2024-12-12 03:06:58,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:58,845][root][INFO] - Training Epoch: 9/10, step 282/574 completed (loss: 1.0961313247680664, acc: 0.7222222089767456)
[2024-12-12 03:06:58,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:59,172][root][INFO] - Training Epoch: 9/10, step 283/574 completed (loss: 0.15520359575748444, acc: 0.9210526347160339)
[2024-12-12 03:06:59,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:59,509][root][INFO] - Training Epoch: 9/10, step 284/574 completed (loss: 0.17024965584278107, acc: 0.9117646813392639)
[2024-12-12 03:06:59,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:06:59,811][root][INFO] - Training Epoch: 9/10, step 285/574 completed (loss: 0.13331225514411926, acc: 0.949999988079071)
[2024-12-12 03:06:59,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:00,159][root][INFO] - Training Epoch: 9/10, step 286/574 completed (loss: 0.7658218145370483, acc: 0.7265625)
[2024-12-12 03:07:00,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:00,522][root][INFO] - Training Epoch: 9/10, step 287/574 completed (loss: 0.7338811755180359, acc: 0.8080000281333923)
[2024-12-12 03:07:00,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:00,845][root][INFO] - Training Epoch: 9/10, step 288/574 completed (loss: 0.5787637233734131, acc: 0.8461538553237915)
[2024-12-12 03:07:00,960][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:01,189][root][INFO] - Training Epoch: 9/10, step 289/574 completed (loss: 0.9017390012741089, acc: 0.7515528202056885)
[2024-12-12 03:07:01,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:01,548][root][INFO] - Training Epoch: 9/10, step 290/574 completed (loss: 0.8998754620552063, acc: 0.7268041372299194)
[2024-12-12 03:07:01,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:01,878][root][INFO] - Training Epoch: 9/10, step 291/574 completed (loss: 0.00882918294519186, acc: 1.0)
[2024-12-12 03:07:01,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:02,247][root][INFO] - Training Epoch: 9/10, step 292/574 completed (loss: 0.3037870228290558, acc: 0.8809523582458496)
[2024-12-12 03:07:02,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:02,607][root][INFO] - Training Epoch: 9/10, step 293/574 completed (loss: 0.4809810519218445, acc: 0.8793103694915771)
[2024-12-12 03:07:02,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:03,069][root][INFO] - Training Epoch: 9/10, step 294/574 completed (loss: 0.33432626724243164, acc: 0.8909090757369995)
[2024-12-12 03:07:03,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:03,628][root][INFO] - Training Epoch: 9/10, step 295/574 completed (loss: 0.9691494703292847, acc: 0.7268041372299194)
[2024-12-12 03:07:03,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:03,953][root][INFO] - Training Epoch: 9/10, step 296/574 completed (loss: 0.4426577687263489, acc: 0.8448275923728943)
[2024-12-12 03:07:04,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:04,344][root][INFO] - Training Epoch: 9/10, step 297/574 completed (loss: 0.08362165838479996, acc: 0.9629629850387573)
[2024-12-12 03:07:04,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:04,689][root][INFO] - Training Epoch: 9/10, step 298/574 completed (loss: 0.31119951605796814, acc: 0.9210526347160339)
[2024-12-12 03:07:04,822][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:05,072][root][INFO] - Training Epoch: 9/10, step 299/574 completed (loss: 0.17847846448421478, acc: 0.9464285969734192)
[2024-12-12 03:07:05,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:05,444][root][INFO] - Training Epoch: 9/10, step 300/574 completed (loss: 0.36797404289245605, acc: 0.96875)
[2024-12-12 03:07:05,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:05,786][root][INFO] - Training Epoch: 9/10, step 301/574 completed (loss: 0.19050820171833038, acc: 0.9811320900917053)
[2024-12-12 03:07:05,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:06,167][root][INFO] - Training Epoch: 9/10, step 302/574 completed (loss: 0.17449143528938293, acc: 0.9622641801834106)
[2024-12-12 03:07:06,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:06,497][root][INFO] - Training Epoch: 9/10, step 303/574 completed (loss: 0.06356862187385559, acc: 1.0)
[2024-12-12 03:07:06,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:06,809][root][INFO] - Training Epoch: 9/10, step 304/574 completed (loss: 0.06734498590230942, acc: 1.0)
[2024-12-12 03:07:06,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:07,161][root][INFO] - Training Epoch: 9/10, step 305/574 completed (loss: 0.4345918297767639, acc: 0.8196721076965332)
[2024-12-12 03:07:07,277][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:07,527][root][INFO] - Training Epoch: 9/10, step 306/574 completed (loss: 0.08522416651248932, acc: 0.9666666388511658)
[2024-12-12 03:07:07,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:07,883][root][INFO] - Training Epoch: 9/10, step 307/574 completed (loss: 0.0288546122610569, acc: 1.0)
[2024-12-12 03:07:08,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:08,271][root][INFO] - Training Epoch: 9/10, step 308/574 completed (loss: 0.39376944303512573, acc: 0.9275362491607666)
[2024-12-12 03:07:08,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:08,683][root][INFO] - Training Epoch: 9/10, step 309/574 completed (loss: 0.3460736870765686, acc: 0.9166666865348816)
[2024-12-12 03:07:08,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:09,051][root][INFO] - Training Epoch: 9/10, step 310/574 completed (loss: 0.44037798047065735, acc: 0.8795180916786194)
[2024-12-12 03:07:09,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:09,377][root][INFO] - Training Epoch: 9/10, step 311/574 completed (loss: 0.526654064655304, acc: 0.8333333134651184)
[2024-12-12 03:07:09,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:09,719][root][INFO] - Training Epoch: 9/10, step 312/574 completed (loss: 0.4184299111366272, acc: 0.8571428656578064)
[2024-12-12 03:07:09,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:10,025][root][INFO] - Training Epoch: 9/10, step 313/574 completed (loss: 0.003362940391525626, acc: 1.0)
[2024-12-12 03:07:10,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:10,352][root][INFO] - Training Epoch: 9/10, step 314/574 completed (loss: 0.06360531598329544, acc: 0.9583333134651184)
[2024-12-12 03:07:10,451][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:10,688][root][INFO] - Training Epoch: 9/10, step 315/574 completed (loss: 0.0457797534763813, acc: 1.0)
[2024-12-12 03:07:10,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:10,980][root][INFO] - Training Epoch: 9/10, step 316/574 completed (loss: 0.30659034848213196, acc: 0.9354838728904724)
[2024-12-12 03:07:11,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:11,355][root][INFO] - Training Epoch: 9/10, step 317/574 completed (loss: 0.16388113796710968, acc: 0.9402984976768494)
[2024-12-12 03:07:11,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:11,744][root][INFO] - Training Epoch: 9/10, step 318/574 completed (loss: 0.48334550857543945, acc: 0.8461538553237915)
[2024-12-12 03:07:11,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:12,112][root][INFO] - Training Epoch: 9/10, step 319/574 completed (loss: 0.12616468966007233, acc: 0.9333333373069763)
[2024-12-12 03:07:12,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:12,474][root][INFO] - Training Epoch: 9/10, step 320/574 completed (loss: 0.12587235867977142, acc: 0.9677419066429138)
[2024-12-12 03:07:12,564][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:12,830][root][INFO] - Training Epoch: 9/10, step 321/574 completed (loss: 0.06648670881986618, acc: 0.9800000190734863)
[2024-12-12 03:07:12,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:13,206][root][INFO] - Training Epoch: 9/10, step 322/574 completed (loss: 0.3245169222354889, acc: 0.8888888955116272)
[2024-12-12 03:07:13,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:13,609][root][INFO] - Training Epoch: 9/10, step 323/574 completed (loss: 0.22726839780807495, acc: 0.9714285731315613)
[2024-12-12 03:07:13,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:14,001][root][INFO] - Training Epoch: 9/10, step 324/574 completed (loss: 0.3129158914089203, acc: 0.9230769276618958)
[2024-12-12 03:07:14,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:14,369][root][INFO] - Training Epoch: 9/10, step 325/574 completed (loss: 0.3652666509151459, acc: 0.9024389982223511)
[2024-12-12 03:07:14,469][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:14,705][root][INFO] - Training Epoch: 9/10, step 326/574 completed (loss: 0.14225777983665466, acc: 0.9736841917037964)
[2024-12-12 03:07:14,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:15,066][root][INFO] - Training Epoch: 9/10, step 327/574 completed (loss: 0.07449429482221603, acc: 0.9473684430122375)
[2024-12-12 03:07:15,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:15,431][root][INFO] - Training Epoch: 9/10, step 328/574 completed (loss: 0.009866003878414631, acc: 1.0)
[2024-12-12 03:07:15,535][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:15,851][root][INFO] - Training Epoch: 9/10, step 329/574 completed (loss: 0.018396781757473946, acc: 1.0)
[2024-12-12 03:07:15,949][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:16,176][root][INFO] - Training Epoch: 9/10, step 330/574 completed (loss: 0.07007613778114319, acc: 1.0)
[2024-12-12 03:07:16,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:16,558][root][INFO] - Training Epoch: 9/10, step 331/574 completed (loss: 0.4350236654281616, acc: 0.8709677457809448)
[2024-12-12 03:07:16,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:16,930][root][INFO] - Training Epoch: 9/10, step 332/574 completed (loss: 0.2786830961704254, acc: 0.9473684430122375)
[2024-12-12 03:07:17,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:17,290][root][INFO] - Training Epoch: 9/10, step 333/574 completed (loss: 0.10602200776338577, acc: 1.0)
[2024-12-12 03:07:17,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:17,633][root][INFO] - Training Epoch: 9/10, step 334/574 completed (loss: 0.13143864274024963, acc: 1.0)
[2024-12-12 03:07:17,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:18,004][root][INFO] - Training Epoch: 9/10, step 335/574 completed (loss: 0.029825348407030106, acc: 1.0)
[2024-12-12 03:07:18,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:18,388][root][INFO] - Training Epoch: 9/10, step 336/574 completed (loss: 0.25210654735565186, acc: 0.8999999761581421)
[2024-12-12 03:07:18,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:18,781][root][INFO] - Training Epoch: 9/10, step 337/574 completed (loss: 0.6948658227920532, acc: 0.8275862336158752)
[2024-12-12 03:07:18,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:19,120][root][INFO] - Training Epoch: 9/10, step 338/574 completed (loss: 0.6672778725624084, acc: 0.7872340679168701)
[2024-12-12 03:07:19,213][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:19,441][root][INFO] - Training Epoch: 9/10, step 339/574 completed (loss: 0.7292685508728027, acc: 0.7831325531005859)
[2024-12-12 03:07:19,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:19,820][root][INFO] - Training Epoch: 9/10, step 340/574 completed (loss: 0.008414525538682938, acc: 1.0)
[2024-12-12 03:07:19,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:20,143][root][INFO] - Training Epoch: 9/10, step 341/574 completed (loss: 0.07073453068733215, acc: 1.0)
[2024-12-12 03:07:20,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:20,469][root][INFO] - Training Epoch: 9/10, step 342/574 completed (loss: 0.48005321621894836, acc: 0.891566276550293)
[2024-12-12 03:07:20,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:20,867][root][INFO] - Training Epoch: 9/10, step 343/574 completed (loss: 0.25933077931404114, acc: 0.8867924809455872)
[2024-12-12 03:07:20,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:21,250][root][INFO] - Training Epoch: 9/10, step 344/574 completed (loss: 0.2421257644891739, acc: 0.9113923907279968)
[2024-12-12 03:07:21,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:21,599][root][INFO] - Training Epoch: 9/10, step 345/574 completed (loss: 0.14807723462581635, acc: 0.9607843160629272)
[2024-12-12 03:07:21,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:21,954][root][INFO] - Training Epoch: 9/10, step 346/574 completed (loss: 0.3279902935028076, acc: 0.89552241563797)
[2024-12-12 03:07:22,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:22,335][root][INFO] - Training Epoch: 9/10, step 347/574 completed (loss: 0.028084691613912582, acc: 1.0)
[2024-12-12 03:07:22,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:22,684][root][INFO] - Training Epoch: 9/10, step 348/574 completed (loss: 0.012648616917431355, acc: 1.0)
[2024-12-12 03:07:22,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:23,063][root][INFO] - Training Epoch: 9/10, step 349/574 completed (loss: 0.22505412995815277, acc: 0.9444444179534912)
[2024-12-12 03:07:23,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:23,424][root][INFO] - Training Epoch: 9/10, step 350/574 completed (loss: 0.6078325510025024, acc: 0.8139534592628479)
[2024-12-12 03:07:23,563][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:23,824][root][INFO] - Training Epoch: 9/10, step 351/574 completed (loss: 0.15697523951530457, acc: 0.9743589758872986)
[2024-12-12 03:07:23,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:24,222][root][INFO] - Training Epoch: 9/10, step 352/574 completed (loss: 0.3264548182487488, acc: 0.9111111164093018)
[2024-12-12 03:07:24,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:24,553][root][INFO] - Training Epoch: 9/10, step 353/574 completed (loss: 0.1319710910320282, acc: 0.95652174949646)
[2024-12-12 03:07:24,666][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:24,931][root][INFO] - Training Epoch: 9/10, step 354/574 completed (loss: 0.04040294885635376, acc: 1.0)
[2024-12-12 03:07:25,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:25,331][root][INFO] - Training Epoch: 9/10, step 355/574 completed (loss: 0.7601666450500488, acc: 0.8021978139877319)
[2024-12-12 03:07:25,475][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:25,829][root][INFO] - Training Epoch: 9/10, step 356/574 completed (loss: 0.8115197420120239, acc: 0.7652173638343811)
[2024-12-12 03:07:25,917][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:26,152][root][INFO] - Training Epoch: 9/10, step 357/574 completed (loss: 0.34731343388557434, acc: 0.8804348111152649)
[2024-12-12 03:07:26,243][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:26,470][root][INFO] - Training Epoch: 9/10, step 358/574 completed (loss: 0.1957905888557434, acc: 0.8775510191917419)
[2024-12-12 03:07:26,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:26,808][root][INFO] - Training Epoch: 9/10, step 359/574 completed (loss: 0.01853868179023266, acc: 1.0)
[2024-12-12 03:07:26,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:27,153][root][INFO] - Training Epoch: 9/10, step 360/574 completed (loss: 0.05230998620390892, acc: 1.0)
[2024-12-12 03:07:27,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:27,460][root][INFO] - Training Epoch: 9/10, step 361/574 completed (loss: 0.15179821848869324, acc: 0.9268292784690857)
[2024-12-12 03:07:27,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:27,826][root][INFO] - Training Epoch: 9/10, step 362/574 completed (loss: 0.1310974806547165, acc: 0.9555555582046509)
[2024-12-12 03:07:27,939][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:28,206][root][INFO] - Training Epoch: 9/10, step 363/574 completed (loss: 0.17074114084243774, acc: 0.9473684430122375)
[2024-12-12 03:07:28,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:28,554][root][INFO] - Training Epoch: 9/10, step 364/574 completed (loss: 0.07207486033439636, acc: 0.9756097793579102)
[2024-12-12 03:07:28,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:28,881][root][INFO] - Training Epoch: 9/10, step 365/574 completed (loss: 0.14695654809474945, acc: 0.939393937587738)
[2024-12-12 03:07:28,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:29,260][root][INFO] - Training Epoch: 9/10, step 366/574 completed (loss: 0.0033318104688078165, acc: 1.0)
[2024-12-12 03:07:29,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:29,629][root][INFO] - Training Epoch: 9/10, step 367/574 completed (loss: 0.007536234334111214, acc: 1.0)
[2024-12-12 03:07:29,757][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:30,019][root][INFO] - Training Epoch: 9/10, step 368/574 completed (loss: 0.05727012827992439, acc: 1.0)
[2024-12-12 03:07:30,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:30,403][root][INFO] - Training Epoch: 9/10, step 369/574 completed (loss: 0.06150151044130325, acc: 1.0)
[2024-12-12 03:07:30,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:31,003][root][INFO] - Training Epoch: 9/10, step 370/574 completed (loss: 1.0295792818069458, acc: 0.7090908885002136)
[2024-12-12 03:07:31,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:31,864][root][INFO] - Training Epoch: 9/10, step 371/574 completed (loss: 0.5614143013954163, acc: 0.8113207817077637)
[2024-12-12 03:07:31,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:32,226][root][INFO] - Training Epoch: 9/10, step 372/574 completed (loss: 0.22077004611492157, acc: 0.9444444179534912)
[2024-12-12 03:07:32,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:32,587][root][INFO] - Training Epoch: 9/10, step 373/574 completed (loss: 0.16562595963478088, acc: 0.9464285969734192)
[2024-12-12 03:07:32,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:32,935][root][INFO] - Training Epoch: 9/10, step 374/574 completed (loss: 0.1606290340423584, acc: 0.9428571462631226)
[2024-12-12 03:07:33,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:33,303][root][INFO] - Training Epoch: 9/10, step 375/574 completed (loss: 0.003191727213561535, acc: 1.0)
[2024-12-12 03:07:33,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:33,612][root][INFO] - Training Epoch: 9/10, step 376/574 completed (loss: 0.0015473709208890796, acc: 1.0)
[2024-12-12 03:07:33,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:33,920][root][INFO] - Training Epoch: 9/10, step 377/574 completed (loss: 0.03038218431174755, acc: 1.0)
[2024-12-12 03:07:34,014][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:34,245][root][INFO] - Training Epoch: 9/10, step 378/574 completed (loss: 0.39355286955833435, acc: 0.8736842274665833)
[2024-12-12 03:07:34,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:34,830][root][INFO] - Training Epoch: 9/10, step 379/574 completed (loss: 0.7428126931190491, acc: 0.8143712282180786)
[2024-12-12 03:07:34,948][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:35,237][root][INFO] - Training Epoch: 9/10, step 380/574 completed (loss: 0.48479679226875305, acc: 0.8571428656578064)
[2024-12-12 03:07:35,725][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:36,506][root][INFO] - Training Epoch: 9/10, step 381/574 completed (loss: 0.7435570955276489, acc: 0.8128342032432556)
[2024-12-12 03:07:36,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:37,074][root][INFO] - Training Epoch: 9/10, step 382/574 completed (loss: 0.3298419117927551, acc: 0.9009009003639221)
[2024-12-12 03:07:37,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:37,401][root][INFO] - Training Epoch: 9/10, step 383/574 completed (loss: 0.01905534230172634, acc: 1.0)
[2024-12-12 03:07:37,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:37,766][root][INFO] - Training Epoch: 9/10, step 384/574 completed (loss: 0.03220539540052414, acc: 0.9642857313156128)
[2024-12-12 03:07:37,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:38,117][root][INFO] - Training Epoch: 9/10, step 385/574 completed (loss: 0.024572717025876045, acc: 1.0)
[2024-12-12 03:07:38,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:38,449][root][INFO] - Training Epoch: 9/10, step 386/574 completed (loss: 0.013198919594287872, acc: 1.0)
[2024-12-12 03:07:38,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:38,785][root][INFO] - Training Epoch: 9/10, step 387/574 completed (loss: 0.015262559987604618, acc: 1.0)
[2024-12-12 03:07:38,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:39,156][root][INFO] - Training Epoch: 9/10, step 388/574 completed (loss: 0.00960275437682867, acc: 1.0)
[2024-12-12 03:07:39,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:39,497][root][INFO] - Training Epoch: 9/10, step 389/574 completed (loss: 0.0019991889130324125, acc: 1.0)
[2024-12-12 03:07:39,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:39,770][root][INFO] - Training Epoch: 9/10, step 390/574 completed (loss: 0.05100875347852707, acc: 1.0)
[2024-12-12 03:07:39,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:40,090][root][INFO] - Training Epoch: 9/10, step 391/574 completed (loss: 0.461991548538208, acc: 0.8333333134651184)
[2024-12-12 03:07:40,196][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:40,465][root][INFO] - Training Epoch: 9/10, step 392/574 completed (loss: 0.5749452710151672, acc: 0.8252426981925964)
[2024-12-12 03:07:40,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:40,986][root][INFO] - Training Epoch: 9/10, step 393/574 completed (loss: 0.9584687352180481, acc: 0.720588207244873)
[2024-12-12 03:07:41,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:41,388][root][INFO] - Training Epoch: 9/10, step 394/574 completed (loss: 0.6710583567619324, acc: 0.7599999904632568)
[2024-12-12 03:07:41,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:41,785][root][INFO] - Training Epoch: 9/10, step 395/574 completed (loss: 0.8528149127960205, acc: 0.7361111044883728)
[2024-12-12 03:07:41,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:42,139][root][INFO] - Training Epoch: 9/10, step 396/574 completed (loss: 0.35903000831604004, acc: 0.930232584476471)
[2024-12-12 03:07:42,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:42,526][root][INFO] - Training Epoch: 9/10, step 397/574 completed (loss: 0.0578780435025692, acc: 0.9583333134651184)
[2024-12-12 03:07:42,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:42,899][root][INFO] - Training Epoch: 9/10, step 398/574 completed (loss: 0.41623368859291077, acc: 0.9069767594337463)
[2024-12-12 03:07:43,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:43,247][root][INFO] - Training Epoch: 9/10, step 399/574 completed (loss: 0.014103234745562077, acc: 1.0)
[2024-12-12 03:07:43,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:43,782][root][INFO] - Training Epoch: 9/10, step 400/574 completed (loss: 0.30236369371414185, acc: 0.9264705777168274)
[2024-12-12 03:07:43,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:44,112][root][INFO] - Training Epoch: 9/10, step 401/574 completed (loss: 0.2544775605201721, acc: 0.8799999952316284)
[2024-12-12 03:07:44,191][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:44,414][root][INFO] - Training Epoch: 9/10, step 402/574 completed (loss: 0.18960191309452057, acc: 0.9696969985961914)
[2024-12-12 03:07:44,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:44,704][root][INFO] - Training Epoch: 9/10, step 403/574 completed (loss: 0.04261462762951851, acc: 0.9696969985961914)
[2024-12-12 03:07:44,791][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:45,022][root][INFO] - Training Epoch: 9/10, step 404/574 completed (loss: 0.00970437005162239, acc: 1.0)
[2024-12-12 03:07:45,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:45,393][root][INFO] - Training Epoch: 9/10, step 405/574 completed (loss: 0.055516332387924194, acc: 0.9629629850387573)
[2024-12-12 03:07:45,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:45,741][root][INFO] - Training Epoch: 9/10, step 406/574 completed (loss: 0.045108646154403687, acc: 0.9599999785423279)
[2024-12-12 03:07:45,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:46,120][root][INFO] - Training Epoch: 9/10, step 407/574 completed (loss: 0.003970203921198845, acc: 1.0)
[2024-12-12 03:07:46,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:46,496][root][INFO] - Training Epoch: 9/10, step 408/574 completed (loss: 0.01669606752693653, acc: 1.0)
[2024-12-12 03:07:46,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:46,824][root][INFO] - Training Epoch: 9/10, step 409/574 completed (loss: 0.05303915590047836, acc: 0.9615384340286255)
[2024-12-12 03:07:46,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:47,134][root][INFO] - Training Epoch: 9/10, step 410/574 completed (loss: 0.08013907819986343, acc: 0.982758641242981)
[2024-12-12 03:07:47,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:47,481][root][INFO] - Training Epoch: 9/10, step 411/574 completed (loss: 0.0499008409678936, acc: 0.9642857313156128)
[2024-12-12 03:07:47,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:47,887][root][INFO] - Training Epoch: 9/10, step 412/574 completed (loss: 0.054928623139858246, acc: 0.9666666388511658)
[2024-12-12 03:07:48,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:48,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:49,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:49,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:50,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:50,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:50,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:51,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:51,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:51,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:52,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:52,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:52,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:52,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:53,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:53,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:53,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:54,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:54,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:54,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:55,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:55,529][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:55,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:56,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:56,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:57,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:57,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:57,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:58,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:58,585][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:58,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:59,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:07:59,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:00,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:00,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:01,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:01,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:01,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:02,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:02,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:03,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:03,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:03,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:04,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:04,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:04,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:04,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:05,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:05,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:05,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:06,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:06,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:06,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:07,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:07,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:08,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:08,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:08,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:09,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:09,383][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:09,686][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:10,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:10,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:11,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:11,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:11,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:12,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:12,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:12,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:13,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:13,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:14,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:14,375][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:14,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:15,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:15,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:15,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:16,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:16,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:16,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:17,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:17,611][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:17,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:18,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:18,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:19,106][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.9915, device='cuda:0') eval_epoch_loss=tensor(1.3842, device='cuda:0') eval_epoch_acc=tensor(0.7108, device='cuda:0')
[2024-12-12 03:08:19,107][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:08:19,107][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:08:19,365][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_9_step_413_loss_1.3841755390167236/model.pt
[2024-12-12 03:08:19,369][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 03:08:19,369][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 9 is 0.7108194231987
[2024-12-12 03:08:19,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:19,772][root][INFO] - Training Epoch: 9/10, step 413/574 completed (loss: 0.05209142342209816, acc: 0.9696969985961914)
[2024-12-12 03:08:19,888][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:20,143][root][INFO] - Training Epoch: 9/10, step 414/574 completed (loss: 0.005957385525107384, acc: 1.0)
[2024-12-12 03:08:20,244][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:20,524][root][INFO] - Training Epoch: 9/10, step 415/574 completed (loss: 0.2261696755886078, acc: 0.9019607901573181)
[2024-12-12 03:08:20,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:20,892][root][INFO] - Training Epoch: 9/10, step 416/574 completed (loss: 0.055410437285900116, acc: 1.0)
[2024-12-12 03:08:21,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:21,242][root][INFO] - Training Epoch: 9/10, step 417/574 completed (loss: 0.07273164391517639, acc: 1.0)
[2024-12-12 03:08:21,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:21,567][root][INFO] - Training Epoch: 9/10, step 418/574 completed (loss: 0.09176915138959885, acc: 1.0)
[2024-12-12 03:08:21,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:21,864][root][INFO] - Training Epoch: 9/10, step 419/574 completed (loss: 0.22378131747245789, acc: 0.949999988079071)
[2024-12-12 03:08:21,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:22,231][root][INFO] - Training Epoch: 9/10, step 420/574 completed (loss: 0.011598341166973114, acc: 1.0)
[2024-12-12 03:08:22,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:22,606][root][INFO] - Training Epoch: 9/10, step 421/574 completed (loss: 0.007884358055889606, acc: 1.0)
[2024-12-12 03:08:22,707][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:22,904][root][INFO] - Training Epoch: 9/10, step 422/574 completed (loss: 0.04863545671105385, acc: 1.0)
[2024-12-12 03:08:22,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:23,218][root][INFO] - Training Epoch: 9/10, step 423/574 completed (loss: 0.057309482246637344, acc: 1.0)
[2024-12-12 03:08:23,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:23,559][root][INFO] - Training Epoch: 9/10, step 424/574 completed (loss: 0.07525563985109329, acc: 1.0)
[2024-12-12 03:08:23,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:23,894][root][INFO] - Training Epoch: 9/10, step 425/574 completed (loss: 0.1750185638666153, acc: 0.939393937587738)
[2024-12-12 03:08:24,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:24,216][root][INFO] - Training Epoch: 9/10, step 426/574 completed (loss: 0.06602361798286438, acc: 0.95652174949646)
[2024-12-12 03:08:24,333][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:24,592][root][INFO] - Training Epoch: 9/10, step 427/574 completed (loss: 0.10000930726528168, acc: 1.0)
[2024-12-12 03:08:24,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:24,947][root][INFO] - Training Epoch: 9/10, step 428/574 completed (loss: 0.1570587009191513, acc: 0.9259259104728699)
[2024-12-12 03:08:25,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:25,256][root][INFO] - Training Epoch: 9/10, step 429/574 completed (loss: 0.052150458097457886, acc: 0.95652174949646)
[2024-12-12 03:08:25,340][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:25,572][root][INFO] - Training Epoch: 9/10, step 430/574 completed (loss: 0.0031568275298923254, acc: 1.0)
[2024-12-12 03:08:25,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:25,857][root][INFO] - Training Epoch: 9/10, step 431/574 completed (loss: 0.005543472245335579, acc: 1.0)
[2024-12-12 03:08:25,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:26,198][root][INFO] - Training Epoch: 9/10, step 432/574 completed (loss: 0.0024076050613075495, acc: 1.0)
[2024-12-12 03:08:26,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:26,578][root][INFO] - Training Epoch: 9/10, step 433/574 completed (loss: 0.3012624979019165, acc: 0.8611111044883728)
[2024-12-12 03:08:26,675][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:26,900][root][INFO] - Training Epoch: 9/10, step 434/574 completed (loss: 0.14574939012527466, acc: 0.9599999785423279)
[2024-12-12 03:08:26,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:27,275][root][INFO] - Training Epoch: 9/10, step 435/574 completed (loss: 0.03431595116853714, acc: 1.0)
[2024-12-12 03:08:27,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:27,610][root][INFO] - Training Epoch: 9/10, step 436/574 completed (loss: 0.08698581159114838, acc: 0.9722222089767456)
[2024-12-12 03:08:27,693][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:27,923][root][INFO] - Training Epoch: 9/10, step 437/574 completed (loss: 0.03386767953634262, acc: 1.0)
[2024-12-12 03:08:28,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:28,279][root][INFO] - Training Epoch: 9/10, step 438/574 completed (loss: 0.0010627120500430465, acc: 1.0)
[2024-12-12 03:08:28,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:28,677][root][INFO] - Training Epoch: 9/10, step 439/574 completed (loss: 0.10586736351251602, acc: 0.9487179517745972)
[2024-12-12 03:08:28,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:29,140][root][INFO] - Training Epoch: 9/10, step 440/574 completed (loss: 0.3991389572620392, acc: 0.8484848737716675)
[2024-12-12 03:08:29,360][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:29,855][root][INFO] - Training Epoch: 9/10, step 441/574 completed (loss: 1.043582797050476, acc: 0.656000018119812)
[2024-12-12 03:08:29,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:30,305][root][INFO] - Training Epoch: 9/10, step 442/574 completed (loss: 0.8846588134765625, acc: 0.725806474685669)
[2024-12-12 03:08:30,464][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:30,959][root][INFO] - Training Epoch: 9/10, step 443/574 completed (loss: 1.037462830543518, acc: 0.7164179086685181)
[2024-12-12 03:08:31,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:31,303][root][INFO] - Training Epoch: 9/10, step 444/574 completed (loss: 0.17154887318611145, acc: 0.9622641801834106)
[2024-12-12 03:08:31,458][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:31,742][root][INFO] - Training Epoch: 9/10, step 445/574 completed (loss: 0.16973941028118134, acc: 0.9545454382896423)
[2024-12-12 03:08:31,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:32,110][root][INFO] - Training Epoch: 9/10, step 446/574 completed (loss: 0.04090253636240959, acc: 1.0)
[2024-12-12 03:08:32,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:32,445][root][INFO] - Training Epoch: 9/10, step 447/574 completed (loss: 0.06661687791347504, acc: 1.0)
[2024-12-12 03:08:32,532][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:32,759][root][INFO] - Training Epoch: 9/10, step 448/574 completed (loss: 0.012590530328452587, acc: 1.0)
[2024-12-12 03:08:32,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:33,136][root][INFO] - Training Epoch: 9/10, step 449/574 completed (loss: 0.21961864829063416, acc: 0.9402984976768494)
[2024-12-12 03:08:33,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:33,480][root][INFO] - Training Epoch: 9/10, step 450/574 completed (loss: 0.1884545087814331, acc: 0.9305555820465088)
[2024-12-12 03:08:33,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:33,793][root][INFO] - Training Epoch: 9/10, step 451/574 completed (loss: 0.2622044086456299, acc: 0.9239130616188049)
[2024-12-12 03:08:33,907][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:34,145][root][INFO] - Training Epoch: 9/10, step 452/574 completed (loss: 0.1807439774274826, acc: 0.9230769276618958)
[2024-12-12 03:08:34,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:34,422][root][INFO] - Training Epoch: 9/10, step 453/574 completed (loss: 0.1317744255065918, acc: 0.9605262875556946)
[2024-12-12 03:08:34,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:34,745][root][INFO] - Training Epoch: 9/10, step 454/574 completed (loss: 0.14428476989269257, acc: 0.9591836929321289)
[2024-12-12 03:08:34,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:35,122][root][INFO] - Training Epoch: 9/10, step 455/574 completed (loss: 0.12896913290023804, acc: 0.939393937587738)
[2024-12-12 03:08:35,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:35,473][root][INFO] - Training Epoch: 9/10, step 456/574 completed (loss: 0.6573759913444519, acc: 0.7938144207000732)
[2024-12-12 03:08:35,572][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:35,872][root][INFO] - Training Epoch: 9/10, step 457/574 completed (loss: 0.210863396525383, acc: 0.9142857193946838)
[2024-12-12 03:08:36,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:36,275][root][INFO] - Training Epoch: 9/10, step 458/574 completed (loss: 0.7240025997161865, acc: 0.8023256063461304)
[2024-12-12 03:08:36,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:36,603][root][INFO] - Training Epoch: 9/10, step 459/574 completed (loss: 0.1739770472049713, acc: 0.9464285969734192)
[2024-12-12 03:08:36,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:36,921][root][INFO] - Training Epoch: 9/10, step 460/574 completed (loss: 0.3209916949272156, acc: 0.9259259104728699)
[2024-12-12 03:08:37,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:37,257][root][INFO] - Training Epoch: 9/10, step 461/574 completed (loss: 0.09305751323699951, acc: 0.9722222089767456)
[2024-12-12 03:08:37,359][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:37,617][root][INFO] - Training Epoch: 9/10, step 462/574 completed (loss: 0.09566697478294373, acc: 0.96875)
[2024-12-12 03:08:37,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:37,941][root][INFO] - Training Epoch: 9/10, step 463/574 completed (loss: 0.45507729053497314, acc: 0.9615384340286255)
[2024-12-12 03:08:38,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:38,311][root][INFO] - Training Epoch: 9/10, step 464/574 completed (loss: 0.2192872315645218, acc: 0.9347826242446899)
[2024-12-12 03:08:38,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:38,671][root][INFO] - Training Epoch: 9/10, step 465/574 completed (loss: 0.25025781989097595, acc: 0.9047619104385376)
[2024-12-12 03:08:38,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:38,999][root][INFO] - Training Epoch: 9/10, step 466/574 completed (loss: 0.20762358605861664, acc: 0.9277108311653137)
[2024-12-12 03:08:39,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:39,410][root][INFO] - Training Epoch: 9/10, step 467/574 completed (loss: 0.32338234782218933, acc: 0.9099099040031433)
[2024-12-12 03:08:39,526][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:39,802][root][INFO] - Training Epoch: 9/10, step 468/574 completed (loss: 0.5786095857620239, acc: 0.844660222530365)
[2024-12-12 03:08:39,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:40,154][root][INFO] - Training Epoch: 9/10, step 469/574 completed (loss: 0.46942371129989624, acc: 0.9024389982223511)
[2024-12-12 03:08:40,234][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:40,462][root][INFO] - Training Epoch: 9/10, step 470/574 completed (loss: 0.07093361020088196, acc: 0.9583333134651184)
[2024-12-12 03:08:40,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:40,808][root][INFO] - Training Epoch: 9/10, step 471/574 completed (loss: 0.011976763606071472, acc: 1.0)
[2024-12-12 03:08:40,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:41,222][root][INFO] - Training Epoch: 9/10, step 472/574 completed (loss: 0.7544261813163757, acc: 0.7941176295280457)
[2024-12-12 03:08:41,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:41,627][root][INFO] - Training Epoch: 9/10, step 473/574 completed (loss: 1.0507699251174927, acc: 0.7030567526817322)
[2024-12-12 03:08:41,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:42,002][root][INFO] - Training Epoch: 9/10, step 474/574 completed (loss: 0.33621442317962646, acc: 0.8854166865348816)
[2024-12-12 03:08:42,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:42,395][root][INFO] - Training Epoch: 9/10, step 475/574 completed (loss: 0.7189843058586121, acc: 0.7852760553359985)
[2024-12-12 03:08:42,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:42,760][root][INFO] - Training Epoch: 9/10, step 476/574 completed (loss: 0.5305590629577637, acc: 0.8561151027679443)
[2024-12-12 03:08:42,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:43,126][root][INFO] - Training Epoch: 9/10, step 477/574 completed (loss: 0.8743956089019775, acc: 0.7386934757232666)
[2024-12-12 03:08:43,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:43,471][root][INFO] - Training Epoch: 9/10, step 478/574 completed (loss: 0.1506693810224533, acc: 0.9722222089767456)
[2024-12-12 03:08:43,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:43,844][root][INFO] - Training Epoch: 9/10, step 479/574 completed (loss: 0.12665961682796478, acc: 0.9696969985961914)
[2024-12-12 03:08:43,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:44,217][root][INFO] - Training Epoch: 9/10, step 480/574 completed (loss: 0.02227400802075863, acc: 1.0)
[2024-12-12 03:08:44,312][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:44,594][root][INFO] - Training Epoch: 9/10, step 481/574 completed (loss: 0.09133453667163849, acc: 0.949999988079071)
[2024-12-12 03:08:44,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:44,946][root][INFO] - Training Epoch: 9/10, step 482/574 completed (loss: 0.11646506935358047, acc: 0.949999988079071)
[2024-12-12 03:08:45,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:45,317][root][INFO] - Training Epoch: 9/10, step 483/574 completed (loss: 0.3154449164867401, acc: 0.9137930870056152)
[2024-12-12 03:08:45,399][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:45,674][root][INFO] - Training Epoch: 9/10, step 484/574 completed (loss: 0.17885471880435944, acc: 0.9354838728904724)
[2024-12-12 03:08:45,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:46,035][root][INFO] - Training Epoch: 9/10, step 485/574 completed (loss: 0.23997071385383606, acc: 0.9473684430122375)
[2024-12-12 03:08:46,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:46,371][root][INFO] - Training Epoch: 9/10, step 486/574 completed (loss: 0.1570359170436859, acc: 0.9629629850387573)
[2024-12-12 03:08:46,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:46,715][root][INFO] - Training Epoch: 9/10, step 487/574 completed (loss: 0.035308174788951874, acc: 1.0)
[2024-12-12 03:08:46,801][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:47,020][root][INFO] - Training Epoch: 9/10, step 488/574 completed (loss: 0.18273048102855682, acc: 0.9545454382896423)
[2024-12-12 03:08:47,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:47,351][root][INFO] - Training Epoch: 9/10, step 489/574 completed (loss: 0.3378404974937439, acc: 0.9384615421295166)
[2024-12-12 03:08:47,426][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:47,661][root][INFO] - Training Epoch: 9/10, step 490/574 completed (loss: 0.026516513898968697, acc: 1.0)
[2024-12-12 03:08:47,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:47,989][root][INFO] - Training Epoch: 9/10, step 491/574 completed (loss: 0.1646726429462433, acc: 0.931034505367279)
[2024-12-12 03:08:48,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:48,360][root][INFO] - Training Epoch: 9/10, step 492/574 completed (loss: 0.1238396018743515, acc: 0.9607843160629272)
[2024-12-12 03:08:48,488][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:48,730][root][INFO] - Training Epoch: 9/10, step 493/574 completed (loss: 0.041056908667087555, acc: 1.0)
[2024-12-12 03:08:48,834][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:49,055][root][INFO] - Training Epoch: 9/10, step 494/574 completed (loss: 0.04384739696979523, acc: 1.0)
[2024-12-12 03:08:49,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:49,397][root][INFO] - Training Epoch: 9/10, step 495/574 completed (loss: 0.03310525417327881, acc: 1.0)
[2024-12-12 03:08:49,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:49,779][root][INFO] - Training Epoch: 9/10, step 496/574 completed (loss: 0.5652382969856262, acc: 0.8303571343421936)
[2024-12-12 03:08:49,919][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:50,196][root][INFO] - Training Epoch: 9/10, step 497/574 completed (loss: 0.3272410035133362, acc: 0.8876404762268066)
[2024-12-12 03:08:50,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:50,547][root][INFO] - Training Epoch: 9/10, step 498/574 completed (loss: 0.6870443224906921, acc: 0.7415730357170105)
[2024-12-12 03:08:50,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:50,891][root][INFO] - Training Epoch: 9/10, step 499/574 completed (loss: 1.1208356618881226, acc: 0.6879432797431946)
[2024-12-12 03:08:50,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:51,222][root][INFO] - Training Epoch: 9/10, step 500/574 completed (loss: 0.7401404976844788, acc: 0.77173912525177)
[2024-12-12 03:08:51,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:51,487][root][INFO] - Training Epoch: 9/10, step 501/574 completed (loss: 0.012497450225055218, acc: 1.0)
[2024-12-12 03:08:51,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:51,789][root][INFO] - Training Epoch: 9/10, step 502/574 completed (loss: 0.02976991981267929, acc: 1.0)
[2024-12-12 03:08:51,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:52,161][root][INFO] - Training Epoch: 9/10, step 503/574 completed (loss: 0.013882803730666637, acc: 1.0)
[2024-12-12 03:08:52,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:52,523][root][INFO] - Training Epoch: 9/10, step 504/574 completed (loss: 0.3598579168319702, acc: 0.9629629850387573)
[2024-12-12 03:08:52,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:52,873][root][INFO] - Training Epoch: 9/10, step 505/574 completed (loss: 0.343639075756073, acc: 0.9056603908538818)
[2024-12-12 03:08:52,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:53,221][root][INFO] - Training Epoch: 9/10, step 506/574 completed (loss: 0.4563467800617218, acc: 0.8965517282485962)
[2024-12-12 03:08:53,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:53,836][root][INFO] - Training Epoch: 9/10, step 507/574 completed (loss: 0.7722156047821045, acc: 0.7837837934494019)
[2024-12-12 03:08:53,975][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:54,269][root][INFO] - Training Epoch: 9/10, step 508/574 completed (loss: 0.3656518757343292, acc: 0.8873239159584045)
[2024-12-12 03:08:54,349][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:54,577][root][INFO] - Training Epoch: 9/10, step 509/574 completed (loss: 0.009524418041110039, acc: 1.0)
[2024-12-12 03:08:54,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:54,871][root][INFO] - Training Epoch: 9/10, step 510/574 completed (loss: 0.08199705183506012, acc: 0.9333333373069763)
[2024-12-12 03:08:54,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:55,162][root][INFO] - Training Epoch: 9/10, step 511/574 completed (loss: 0.08793836086988449, acc: 0.9615384340286255)
[2024-12-12 03:08:56,650][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:57,949][root][INFO] - Training Epoch: 9/10, step 512/574 completed (loss: 1.1834393739700317, acc: 0.6714285612106323)
[2024-12-12 03:08:58,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:58,720][root][INFO] - Training Epoch: 9/10, step 513/574 completed (loss: 0.532387375831604, acc: 0.817460298538208)
[2024-12-12 03:08:58,807][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:59,032][root][INFO] - Training Epoch: 9/10, step 514/574 completed (loss: 0.13486440479755402, acc: 0.9285714030265808)
[2024-12-12 03:08:59,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:08:59,377][root][INFO] - Training Epoch: 9/10, step 515/574 completed (loss: 0.16622145473957062, acc: 0.9333333373069763)
[2024-12-12 03:08:59,551][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:00,067][root][INFO] - Training Epoch: 9/10, step 516/574 completed (loss: 0.39025530219078064, acc: 0.8333333134651184)
[2024-12-12 03:09:00,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:00,372][root][INFO] - Training Epoch: 9/10, step 517/574 completed (loss: 0.0035063824616372585, acc: 1.0)
[2024-12-12 03:09:00,446][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:00,681][root][INFO] - Training Epoch: 9/10, step 518/574 completed (loss: 0.09106873720884323, acc: 0.9677419066429138)
[2024-12-12 03:09:00,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:01,065][root][INFO] - Training Epoch: 9/10, step 519/574 completed (loss: 0.036081187427043915, acc: 1.0)
[2024-12-12 03:09:01,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:01,443][root][INFO] - Training Epoch: 9/10, step 520/574 completed (loss: 0.306980699300766, acc: 0.9259259104728699)
[2024-12-12 03:09:01,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:02,454][root][INFO] - Training Epoch: 9/10, step 521/574 completed (loss: 1.303269863128662, acc: 0.6016949415206909)
[2024-12-12 03:09:02,541][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:02,787][root][INFO] - Training Epoch: 9/10, step 522/574 completed (loss: 0.543053150177002, acc: 0.8283582329750061)
[2024-12-12 03:09:02,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:03,127][root][INFO] - Training Epoch: 9/10, step 523/574 completed (loss: 0.5875460505485535, acc: 0.8321167826652527)
[2024-12-12 03:09:03,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:03,698][root][INFO] - Training Epoch: 9/10, step 524/574 completed (loss: 0.9033982753753662, acc: 0.6899999976158142)
[2024-12-12 03:09:03,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:04,026][root][INFO] - Training Epoch: 9/10, step 525/574 completed (loss: 0.17007961869239807, acc: 0.9629629850387573)
[2024-12-12 03:09:04,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:04,386][root][INFO] - Training Epoch: 9/10, step 526/574 completed (loss: 0.16438010334968567, acc: 0.942307710647583)
[2024-12-12 03:09:04,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:04,757][root][INFO] - Training Epoch: 9/10, step 527/574 completed (loss: 0.10328961908817291, acc: 0.9523809552192688)
[2024-12-12 03:09:04,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:05,107][root][INFO] - Training Epoch: 9/10, step 528/574 completed (loss: 0.5525075197219849, acc: 0.8196721076965332)
[2024-12-12 03:09:05,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:05,463][root][INFO] - Training Epoch: 9/10, step 529/574 completed (loss: 0.19308792054653168, acc: 0.9491525292396545)
[2024-12-12 03:09:05,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:05,858][root][INFO] - Training Epoch: 9/10, step 530/574 completed (loss: 0.4354667663574219, acc: 0.8604651093482971)
[2024-12-12 03:09:05,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:06,195][root][INFO] - Training Epoch: 9/10, step 531/574 completed (loss: 0.20701947808265686, acc: 0.9318181872367859)
[2024-12-12 03:09:06,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:06,526][root][INFO] - Training Epoch: 9/10, step 532/574 completed (loss: 0.30757805705070496, acc: 0.8867924809455872)
[2024-12-12 03:09:06,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:06,849][root][INFO] - Training Epoch: 9/10, step 533/574 completed (loss: 0.14920325577259064, acc: 0.9545454382896423)
[2024-12-12 03:09:06,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:07,143][root][INFO] - Training Epoch: 9/10, step 534/574 completed (loss: 0.03139904886484146, acc: 1.0)
[2024-12-12 03:09:07,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:07,469][root][INFO] - Training Epoch: 9/10, step 535/574 completed (loss: 0.012075078673660755, acc: 1.0)
[2024-12-12 03:09:07,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:07,814][root][INFO] - Training Epoch: 9/10, step 536/574 completed (loss: 0.024072982370853424, acc: 1.0)
[2024-12-12 03:09:07,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:08,203][root][INFO] - Training Epoch: 9/10, step 537/574 completed (loss: 0.2165459841489792, acc: 0.9538461565971375)
[2024-12-12 03:09:08,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:08,579][root][INFO] - Training Epoch: 9/10, step 538/574 completed (loss: 0.4245030879974365, acc: 0.84375)
[2024-12-12 03:09:08,703][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:08,968][root][INFO] - Training Epoch: 9/10, step 539/574 completed (loss: 0.14203816652297974, acc: 0.96875)
[2024-12-12 03:09:09,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:09,319][root][INFO] - Training Epoch: 9/10, step 540/574 completed (loss: 0.26524797081947327, acc: 0.939393937587738)
[2024-12-12 03:09:09,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:09,687][root][INFO] - Training Epoch: 9/10, step 541/574 completed (loss: 0.056056223809719086, acc: 0.9375)
[2024-12-12 03:09:09,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:10,051][root][INFO] - Training Epoch: 9/10, step 542/574 completed (loss: 0.20324474573135376, acc: 0.9032257795333862)
[2024-12-12 03:09:10,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:10,381][root][INFO] - Training Epoch: 9/10, step 543/574 completed (loss: 0.0034321951679885387, acc: 1.0)
[2024-12-12 03:09:10,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:10,690][root][INFO] - Training Epoch: 9/10, step 544/574 completed (loss: 0.27540528774261475, acc: 0.8999999761581421)
[2024-12-12 03:09:10,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:11,066][root][INFO] - Training Epoch: 9/10, step 545/574 completed (loss: 0.2596203088760376, acc: 0.9268292784690857)
[2024-12-12 03:09:11,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:11,417][root][INFO] - Training Epoch: 9/10, step 546/574 completed (loss: 0.03694780170917511, acc: 1.0)
[2024-12-12 03:09:11,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:11,709][root][INFO] - Training Epoch: 9/10, step 547/574 completed (loss: 0.032380782067775726, acc: 1.0)
[2024-12-12 03:09:11,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:12,060][root][INFO] - Training Epoch: 9/10, step 548/574 completed (loss: 0.07436168938875198, acc: 1.0)
[2024-12-12 03:09:12,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:12,362][root][INFO] - Training Epoch: 9/10, step 549/574 completed (loss: 0.005375968292355537, acc: 1.0)
[2024-12-12 03:09:12,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:12,735][root][INFO] - Training Epoch: 9/10, step 550/574 completed (loss: 0.35167792439460754, acc: 0.8787878751754761)
[2024-12-12 03:09:12,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:13,075][root][INFO] - Training Epoch: 9/10, step 551/574 completed (loss: 0.08208781480789185, acc: 0.949999988079071)
[2024-12-12 03:09:13,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:13,469][root][INFO] - Training Epoch: 9/10, step 552/574 completed (loss: 0.20958635210990906, acc: 0.9428571462631226)
[2024-12-12 03:09:13,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:13,868][root][INFO] - Training Epoch: 9/10, step 553/574 completed (loss: 0.42913901805877686, acc: 0.8905109763145447)
[2024-12-12 03:09:13,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:14,221][root][INFO] - Training Epoch: 9/10, step 554/574 completed (loss: 0.40659022331237793, acc: 0.8758620619773865)
[2024-12-12 03:09:14,311][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:14,537][root][INFO] - Training Epoch: 9/10, step 555/574 completed (loss: 0.4843820333480835, acc: 0.8428571224212646)
[2024-12-12 03:09:15,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:15,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:15,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:16,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:16,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:17,004][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:17,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:17,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:17,852][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:18,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:18,719][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:19,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:19,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:19,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:20,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:20,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:20,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:21,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:21,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:21,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:22,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:22,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:23,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:23,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:23,745][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:24,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:24,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:24,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:25,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:25,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:25,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:26,279][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:26,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:26,978][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:27,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:27,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:27,973][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:28,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:28,634][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:28,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:29,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:29,589][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:29,925][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:30,313][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:30,632][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:30,966][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:31,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:31,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:31,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:32,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:32,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:32,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:33,238][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:33,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:33,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:34,201][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:34,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:34,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:35,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:35,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:35,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:36,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:36,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:37,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:37,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:37,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:38,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:38,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:38,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:39,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:39,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:40,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:40,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:40,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:41,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:41,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:41,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:42,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:42,505][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:42,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:43,106][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:43,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:43,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:44,146][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:44,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:44,980][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.3563, device='cuda:0') eval_epoch_loss=tensor(1.4716, device='cuda:0') eval_epoch_acc=tensor(0.6884, device='cuda:0')
[2024-12-12 03:09:44,981][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:09:44,981][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:09:45,179][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_9_step_556_loss_1.4716157913208008/model.pt
[2024-12-12 03:09:45,184][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 03:09:45,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:45,591][root][INFO] - Training Epoch: 9/10, step 556/574 completed (loss: 0.6068233251571655, acc: 0.7814569473266602)
[2024-12-12 03:09:45,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:45,938][root][INFO] - Training Epoch: 9/10, step 557/574 completed (loss: 0.21720142662525177, acc: 0.9230769276618958)
[2024-12-12 03:09:46,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:46,315][root][INFO] - Training Epoch: 9/10, step 558/574 completed (loss: 0.1127709448337555, acc: 0.9200000166893005)
[2024-12-12 03:09:46,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:46,678][root][INFO] - Training Epoch: 9/10, step 559/574 completed (loss: 0.06448549032211304, acc: 1.0)
[2024-12-12 03:09:46,770][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:46,997][root][INFO] - Training Epoch: 9/10, step 560/574 completed (loss: 0.0891386941075325, acc: 0.9615384340286255)
[2024-12-12 03:09:47,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:47,291][root][INFO] - Training Epoch: 9/10, step 561/574 completed (loss: 0.1371314525604248, acc: 0.9487179517745972)
[2024-12-12 03:09:47,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:47,672][root][INFO] - Training Epoch: 9/10, step 562/574 completed (loss: 0.3416209816932678, acc: 0.9111111164093018)
[2024-12-12 03:09:47,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:48,036][root][INFO] - Training Epoch: 9/10, step 563/574 completed (loss: 0.12525369226932526, acc: 0.9740259647369385)
[2024-12-12 03:09:48,134][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:48,372][root][INFO] - Training Epoch: 9/10, step 564/574 completed (loss: 0.1529766172170639, acc: 0.9791666865348816)
[2024-12-12 03:09:48,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:48,732][root][INFO] - Training Epoch: 9/10, step 565/574 completed (loss: 0.10964646935462952, acc: 0.9482758641242981)
[2024-12-12 03:09:48,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:49,065][root][INFO] - Training Epoch: 9/10, step 566/574 completed (loss: 0.28579068183898926, acc: 0.8928571343421936)
[2024-12-12 03:09:49,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:49,465][root][INFO] - Training Epoch: 9/10, step 567/574 completed (loss: 0.09243196994066238, acc: 0.9736841917037964)
[2024-12-12 03:09:49,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:49,818][root][INFO] - Training Epoch: 9/10, step 568/574 completed (loss: 0.13489577174186707, acc: 0.9629629850387573)
[2024-12-12 03:09:49,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:50,232][root][INFO] - Training Epoch: 9/10, step 569/574 completed (loss: 0.6122196912765503, acc: 0.855614960193634)
[2024-12-12 03:09:50,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:50,624][root][INFO] - Training Epoch: 9/10, step 570/574 completed (loss: 0.18465928733348846, acc: 0.9354838728904724)
[2024-12-12 03:09:50,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:50,967][root][INFO] - Training Epoch: 9/10, step 571/574 completed (loss: 0.19128386676311493, acc: 0.94017094373703)
[2024-12-12 03:09:51,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:51,273][root][INFO] - Training Epoch: 9/10, step 572/574 completed (loss: 0.8638104200363159, acc: 0.7448979616165161)
[2024-12-12 03:09:51,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:51,603][root][INFO] - Training Epoch: 9/10, step 573/574 completed (loss: 0.664482057094574, acc: 0.8113207817077637)
[2024-12-12 03:09:51,953][slam_llm.utils.train_utils][INFO] - Epoch 9: train_perplexity=1.4030, train_epoch_loss=0.3386, epoch time 355.988571934402s
[2024-12-12 03:09:51,953][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 03:09:51,953][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-12-12 03:09:51,953][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 03:09:51,953][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 25
[2024-12-12 03:09:51,954][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 03:09:52,461][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:52,748][root][INFO] - Training Epoch: 10/10, step 0/574 completed (loss: 0.06566120684146881, acc: 0.9629629850387573)
[2024-12-12 03:09:52,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:53,023][root][INFO] - Training Epoch: 10/10, step 1/574 completed (loss: 0.022732192650437355, acc: 1.0)
[2024-12-12 03:09:53,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:53,400][root][INFO] - Training Epoch: 10/10, step 2/574 completed (loss: 0.28556403517723083, acc: 0.8918918967247009)
[2024-12-12 03:09:53,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:53,766][root][INFO] - Training Epoch: 10/10, step 3/574 completed (loss: 0.2624336779117584, acc: 0.9736841917037964)
[2024-12-12 03:09:53,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:54,163][root][INFO] - Training Epoch: 10/10, step 4/574 completed (loss: 0.12336881458759308, acc: 0.9729729890823364)
[2024-12-12 03:09:54,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:54,515][root][INFO] - Training Epoch: 10/10, step 5/574 completed (loss: 0.07510840147733688, acc: 0.9642857313156128)
[2024-12-12 03:09:54,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:54,811][root][INFO] - Training Epoch: 10/10, step 6/574 completed (loss: 0.16175313293933868, acc: 0.9591836929321289)
[2024-12-12 03:09:54,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:55,120][root][INFO] - Training Epoch: 10/10, step 7/574 completed (loss: 0.035832252353429794, acc: 1.0)
[2024-12-12 03:09:55,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:55,502][root][INFO] - Training Epoch: 10/10, step 8/574 completed (loss: 0.002838348038494587, acc: 1.0)
[2024-12-12 03:09:55,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:55,909][root][INFO] - Training Epoch: 10/10, step 9/574 completed (loss: 0.004136769101023674, acc: 1.0)
[2024-12-12 03:09:55,992][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:56,266][root][INFO] - Training Epoch: 10/10, step 10/574 completed (loss: 0.045038167387247086, acc: 0.9629629850387573)
[2024-12-12 03:09:56,374][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:56,617][root][INFO] - Training Epoch: 10/10, step 11/574 completed (loss: 0.09447789192199707, acc: 0.9487179517745972)
[2024-12-12 03:09:56,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:57,002][root][INFO] - Training Epoch: 10/10, step 12/574 completed (loss: 0.029599571600556374, acc: 1.0)
[2024-12-12 03:09:57,116][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:57,402][root][INFO] - Training Epoch: 10/10, step 13/574 completed (loss: 0.0608583465218544, acc: 0.97826087474823)
[2024-12-12 03:09:57,504][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:57,750][root][INFO] - Training Epoch: 10/10, step 14/574 completed (loss: 0.27375882863998413, acc: 0.9411764740943909)
[2024-12-12 03:09:57,865][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:58,122][root][INFO] - Training Epoch: 10/10, step 15/574 completed (loss: 0.14441964030265808, acc: 0.9387755393981934)
[2024-12-12 03:09:58,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:58,508][root][INFO] - Training Epoch: 10/10, step 16/574 completed (loss: 0.12088188529014587, acc: 0.9473684430122375)
[2024-12-12 03:09:58,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:58,868][root][INFO] - Training Epoch: 10/10, step 17/574 completed (loss: 0.06593199819326401, acc: 0.9583333134651184)
[2024-12-12 03:09:58,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:59,211][root][INFO] - Training Epoch: 10/10, step 18/574 completed (loss: 0.10376434028148651, acc: 0.9722222089767456)
[2024-12-12 03:09:59,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:59,549][root][INFO] - Training Epoch: 10/10, step 19/574 completed (loss: 0.008807501755654812, acc: 1.0)
[2024-12-12 03:09:59,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:09:59,857][root][INFO] - Training Epoch: 10/10, step 20/574 completed (loss: 0.1317245364189148, acc: 0.9615384340286255)
[2024-12-12 03:09:59,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:00,176][root][INFO] - Training Epoch: 10/10, step 21/574 completed (loss: 0.11046944558620453, acc: 0.9655172228813171)
[2024-12-12 03:10:00,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:00,548][root][INFO] - Training Epoch: 10/10, step 22/574 completed (loss: 0.20024287700653076, acc: 0.9200000166893005)
[2024-12-12 03:10:00,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:00,940][root][INFO] - Training Epoch: 10/10, step 23/574 completed (loss: 0.07885248214006424, acc: 0.9523809552192688)
[2024-12-12 03:10:01,037][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:01,225][root][INFO] - Training Epoch: 10/10, step 24/574 completed (loss: 0.11058755964040756, acc: 0.9375)
[2024-12-12 03:10:01,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:01,527][root][INFO] - Training Epoch: 10/10, step 25/574 completed (loss: 0.281462699174881, acc: 0.8867924809455872)
[2024-12-12 03:10:01,618][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:01,892][root][INFO] - Training Epoch: 10/10, step 26/574 completed (loss: 0.17382653057575226, acc: 0.9589040875434875)
[2024-12-12 03:10:02,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:03,163][root][INFO] - Training Epoch: 10/10, step 27/574 completed (loss: 1.2850301265716553, acc: 0.6482213735580444)
[2024-12-12 03:10:03,269][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:03,523][root][INFO] - Training Epoch: 10/10, step 28/574 completed (loss: 0.16045312583446503, acc: 0.930232584476471)
[2024-12-12 03:10:03,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:03,897][root][INFO] - Training Epoch: 10/10, step 29/574 completed (loss: 0.23968085646629333, acc: 0.9397590160369873)
[2024-12-12 03:10:04,005][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:04,258][root][INFO] - Training Epoch: 10/10, step 30/574 completed (loss: 0.35543110966682434, acc: 0.8888888955116272)
[2024-12-12 03:10:04,377][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:04,618][root][INFO] - Training Epoch: 10/10, step 31/574 completed (loss: 0.05972827598452568, acc: 0.9642857313156128)
[2024-12-12 03:10:04,713][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:04,992][root][INFO] - Training Epoch: 10/10, step 32/574 completed (loss: 0.040642231702804565, acc: 1.0)
[2024-12-12 03:10:05,098][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:05,367][root][INFO] - Training Epoch: 10/10, step 33/574 completed (loss: 0.004799313843250275, acc: 1.0)
[2024-12-12 03:10:05,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:05,770][root][INFO] - Training Epoch: 10/10, step 34/574 completed (loss: 0.591201663017273, acc: 0.831932783126831)
[2024-12-12 03:10:05,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:06,147][root][INFO] - Training Epoch: 10/10, step 35/574 completed (loss: 0.2426985204219818, acc: 0.9344262480735779)
[2024-12-12 03:10:06,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:06,537][root][INFO] - Training Epoch: 10/10, step 36/574 completed (loss: 0.3073945641517639, acc: 0.920634925365448)
[2024-12-12 03:10:06,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:06,900][root][INFO] - Training Epoch: 10/10, step 37/574 completed (loss: 0.21017436683177948, acc: 0.9491525292396545)
[2024-12-12 03:10:07,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:07,272][root][INFO] - Training Epoch: 10/10, step 38/574 completed (loss: 0.3750493824481964, acc: 0.8620689511299133)
[2024-12-12 03:10:07,355][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:07,627][root][INFO] - Training Epoch: 10/10, step 39/574 completed (loss: 0.17084425687789917, acc: 0.9523809552192688)
[2024-12-12 03:10:07,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:08,009][root][INFO] - Training Epoch: 10/10, step 40/574 completed (loss: 0.4351538419723511, acc: 0.9615384340286255)
[2024-12-12 03:10:08,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:08,392][root][INFO] - Training Epoch: 10/10, step 41/574 completed (loss: 0.34847110509872437, acc: 0.8783783912658691)
[2024-12-12 03:10:08,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:08,775][root][INFO] - Training Epoch: 10/10, step 42/574 completed (loss: 0.31176161766052246, acc: 0.9076923131942749)
[2024-12-12 03:10:08,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:09,200][root][INFO] - Training Epoch: 10/10, step 43/574 completed (loss: 0.535007119178772, acc: 0.8282828330993652)
[2024-12-12 03:10:09,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:09,614][root][INFO] - Training Epoch: 10/10, step 44/574 completed (loss: 0.3556373715400696, acc: 0.8865979313850403)
[2024-12-12 03:10:09,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:10,011][root][INFO] - Training Epoch: 10/10, step 45/574 completed (loss: 0.8555520176887512, acc: 0.6985294222831726)
[2024-12-12 03:10:10,089][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:10,321][root][INFO] - Training Epoch: 10/10, step 46/574 completed (loss: 0.017233343794941902, acc: 1.0)
[2024-12-12 03:10:10,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:10,617][root][INFO] - Training Epoch: 10/10, step 47/574 completed (loss: 0.05930301547050476, acc: 0.9629629850387573)
[2024-12-12 03:10:10,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:10,992][root][INFO] - Training Epoch: 10/10, step 48/574 completed (loss: 0.03858562558889389, acc: 1.0)
[2024-12-12 03:10:11,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:11,374][root][INFO] - Training Epoch: 10/10, step 49/574 completed (loss: 0.020072827115654945, acc: 1.0)
[2024-12-12 03:10:11,492][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:11,743][root][INFO] - Training Epoch: 10/10, step 50/574 completed (loss: 0.3251460790634155, acc: 0.8947368264198303)
[2024-12-12 03:10:11,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:12,137][root][INFO] - Training Epoch: 10/10, step 51/574 completed (loss: 0.2795291244983673, acc: 0.9047619104385376)
[2024-12-12 03:10:12,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:12,463][root][INFO] - Training Epoch: 10/10, step 52/574 completed (loss: 0.564115583896637, acc: 0.8732394576072693)
[2024-12-12 03:10:12,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:12,908][root][INFO] - Training Epoch: 10/10, step 53/574 completed (loss: 1.3475570678710938, acc: 0.5733333230018616)
[2024-12-12 03:10:13,001][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:13,226][root][INFO] - Training Epoch: 10/10, step 54/574 completed (loss: 0.11562726646661758, acc: 0.9459459185600281)
[2024-12-12 03:10:13,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:13,610][root][INFO] - Training Epoch: 10/10, step 55/574 completed (loss: 0.013708108104765415, acc: 1.0)
[2024-12-12 03:10:15,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:16,610][root][INFO] - Training Epoch: 10/10, step 56/574 completed (loss: 1.4102873802185059, acc: 0.6348122954368591)
[2024-12-12 03:10:17,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:17,967][root][INFO] - Training Epoch: 10/10, step 57/574 completed (loss: 1.8461891412734985, acc: 0.4945533871650696)
[2024-12-12 03:10:18,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:18,600][root][INFO] - Training Epoch: 10/10, step 58/574 completed (loss: 1.0887264013290405, acc: 0.7329545617103577)
[2024-12-12 03:10:18,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:19,173][root][INFO] - Training Epoch: 10/10, step 59/574 completed (loss: 0.6643500328063965, acc: 0.8161764740943909)
[2024-12-12 03:10:19,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:19,737][root][INFO] - Training Epoch: 10/10, step 60/574 completed (loss: 0.7850295901298523, acc: 0.760869562625885)
[2024-12-12 03:10:19,835][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:20,138][root][INFO] - Training Epoch: 10/10, step 61/574 completed (loss: 0.6494067907333374, acc: 0.8125)
[2024-12-12 03:10:20,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:20,481][root][INFO] - Training Epoch: 10/10, step 62/574 completed (loss: 0.023538822308182716, acc: 1.0)
[2024-12-12 03:10:20,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:20,869][root][INFO] - Training Epoch: 10/10, step 63/574 completed (loss: 0.042806509882211685, acc: 1.0)
[2024-12-12 03:10:20,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:21,252][root][INFO] - Training Epoch: 10/10, step 64/574 completed (loss: 0.19591230154037476, acc: 0.890625)
[2024-12-12 03:10:21,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:21,634][root][INFO] - Training Epoch: 10/10, step 65/574 completed (loss: 0.052851881831884384, acc: 0.9655172228813171)
[2024-12-12 03:10:21,746][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:21,983][root][INFO] - Training Epoch: 10/10, step 66/574 completed (loss: 0.27200204133987427, acc: 0.8928571343421936)
[2024-12-12 03:10:22,073][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:22,306][root][INFO] - Training Epoch: 10/10, step 67/574 completed (loss: 0.25306421518325806, acc: 0.8999999761581421)
[2024-12-12 03:10:22,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:22,682][root][INFO] - Training Epoch: 10/10, step 68/574 completed (loss: 0.02368779666721821, acc: 1.0)
[2024-12-12 03:10:22,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:23,048][root][INFO] - Training Epoch: 10/10, step 69/574 completed (loss: 0.16813962161540985, acc: 0.9166666865348816)
[2024-12-12 03:10:23,144][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:23,390][root][INFO] - Training Epoch: 10/10, step 70/574 completed (loss: 0.06123179942369461, acc: 1.0)
[2024-12-12 03:10:23,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:23,764][root][INFO] - Training Epoch: 10/10, step 71/574 completed (loss: 0.8998145461082458, acc: 0.7573529481887817)
[2024-12-12 03:10:23,863][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:24,078][root][INFO] - Training Epoch: 10/10, step 72/574 completed (loss: 0.5899173617362976, acc: 0.7777777910232544)
[2024-12-12 03:10:24,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:24,449][root][INFO] - Training Epoch: 10/10, step 73/574 completed (loss: 1.2671793699264526, acc: 0.5948718190193176)
[2024-12-12 03:10:24,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:24,786][root][INFO] - Training Epoch: 10/10, step 74/574 completed (loss: 0.4484748840332031, acc: 0.8163265585899353)
[2024-12-12 03:10:24,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:25,107][root][INFO] - Training Epoch: 10/10, step 75/574 completed (loss: 0.8316903710365295, acc: 0.7313432693481445)
[2024-12-12 03:10:25,210][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:25,485][root][INFO] - Training Epoch: 10/10, step 76/574 completed (loss: 1.5154098272323608, acc: 0.5875912308692932)
[2024-12-12 03:10:25,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:25,804][root][INFO] - Training Epoch: 10/10, step 77/574 completed (loss: 0.03237377479672432, acc: 1.0)
[2024-12-12 03:10:25,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:26,175][root][INFO] - Training Epoch: 10/10, step 78/574 completed (loss: 0.002959616482257843, acc: 1.0)
[2024-12-12 03:10:26,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:26,543][root][INFO] - Training Epoch: 10/10, step 79/574 completed (loss: 0.13977518677711487, acc: 0.939393937587738)
[2024-12-12 03:10:26,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:26,901][root][INFO] - Training Epoch: 10/10, step 80/574 completed (loss: 0.009207597933709621, acc: 1.0)
[2024-12-12 03:10:27,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:27,284][root][INFO] - Training Epoch: 10/10, step 81/574 completed (loss: 0.22767826914787292, acc: 0.9038461446762085)
[2024-12-12 03:10:27,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:27,677][root][INFO] - Training Epoch: 10/10, step 82/574 completed (loss: 0.18813124299049377, acc: 0.9807692170143127)
[2024-12-12 03:10:27,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:27,986][root][INFO] - Training Epoch: 10/10, step 83/574 completed (loss: 0.07923753559589386, acc: 0.96875)
[2024-12-12 03:10:28,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:28,308][root][INFO] - Training Epoch: 10/10, step 84/574 completed (loss: 0.2639504075050354, acc: 0.9130434989929199)
[2024-12-12 03:10:28,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:28,647][root][INFO] - Training Epoch: 10/10, step 85/574 completed (loss: 0.19090570509433746, acc: 0.9200000166893005)
[2024-12-12 03:10:28,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:28,956][root][INFO] - Training Epoch: 10/10, step 86/574 completed (loss: 0.04053704813122749, acc: 1.0)
[2024-12-12 03:10:29,084][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:29,423][root][INFO] - Training Epoch: 10/10, step 87/574 completed (loss: 0.2232944816350937, acc: 0.9200000166893005)
[2024-12-12 03:10:29,509][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:29,757][root][INFO] - Training Epoch: 10/10, step 88/574 completed (loss: 0.39969879388809204, acc: 0.844660222530365)
[2024-12-12 03:10:30,128][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:30,895][root][INFO] - Training Epoch: 10/10, step 89/574 completed (loss: 0.89114910364151, acc: 0.762135922908783)
[2024-12-12 03:10:31,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:31,726][root][INFO] - Training Epoch: 10/10, step 90/574 completed (loss: 0.9964669942855835, acc: 0.7204301357269287)
[2024-12-12 03:10:31,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:32,531][root][INFO] - Training Epoch: 10/10, step 91/574 completed (loss: 1.148299217224121, acc: 0.6724137663841248)
[2024-12-12 03:10:32,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:33,277][root][INFO] - Training Epoch: 10/10, step 92/574 completed (loss: 0.44363540410995483, acc: 0.8842105269432068)
[2024-12-12 03:10:33,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:34,271][root][INFO] - Training Epoch: 10/10, step 93/574 completed (loss: 0.6785402297973633, acc: 0.7821782231330872)
[2024-12-12 03:10:34,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:34,564][root][INFO] - Training Epoch: 10/10, step 94/574 completed (loss: 0.47776374220848083, acc: 0.9032257795333862)
[2024-12-12 03:10:34,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:34,940][root][INFO] - Training Epoch: 10/10, step 95/574 completed (loss: 0.3318081498146057, acc: 0.8840579986572266)
[2024-12-12 03:10:35,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:35,328][root][INFO] - Training Epoch: 10/10, step 96/574 completed (loss: 0.6326219439506531, acc: 0.7899159789085388)
[2024-12-12 03:10:35,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:35,724][root][INFO] - Training Epoch: 10/10, step 97/574 completed (loss: 0.6087362170219421, acc: 0.7884615659713745)
[2024-12-12 03:10:35,841][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:36,108][root][INFO] - Training Epoch: 10/10, step 98/574 completed (loss: 0.6377642750740051, acc: 0.8175182342529297)
[2024-12-12 03:10:36,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:36,481][root][INFO] - Training Epoch: 10/10, step 99/574 completed (loss: 0.2010851502418518, acc: 0.9701492786407471)
[2024-12-12 03:10:36,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:36,837][root][INFO] - Training Epoch: 10/10, step 100/574 completed (loss: 0.030241599306464195, acc: 1.0)
[2024-12-12 03:10:36,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:37,188][root][INFO] - Training Epoch: 10/10, step 101/574 completed (loss: 0.0029880476649850607, acc: 1.0)
[2024-12-12 03:10:37,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:37,566][root][INFO] - Training Epoch: 10/10, step 102/574 completed (loss: 0.01773211546242237, acc: 1.0)
[2024-12-12 03:10:37,695][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:37,965][root][INFO] - Training Epoch: 10/10, step 103/574 completed (loss: 0.0294291153550148, acc: 1.0)
[2024-12-12 03:10:38,072][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:38,329][root][INFO] - Training Epoch: 10/10, step 104/574 completed (loss: 0.07383327931165695, acc: 0.9655172228813171)
[2024-12-12 03:10:38,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:38,668][root][INFO] - Training Epoch: 10/10, step 105/574 completed (loss: 0.04219534620642662, acc: 1.0)
[2024-12-12 03:10:38,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:38,993][root][INFO] - Training Epoch: 10/10, step 106/574 completed (loss: 0.009657489135861397, acc: 1.0)
[2024-12-12 03:10:39,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:39,314][root][INFO] - Training Epoch: 10/10, step 107/574 completed (loss: 0.00261161127127707, acc: 1.0)
[2024-12-12 03:10:39,390][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:39,664][root][INFO] - Training Epoch: 10/10, step 108/574 completed (loss: 0.0037857042625546455, acc: 1.0)
[2024-12-12 03:10:39,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:40,066][root][INFO] - Training Epoch: 10/10, step 109/574 completed (loss: 0.09830454736948013, acc: 0.9523809552192688)
[2024-12-12 03:10:40,173][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:40,456][root][INFO] - Training Epoch: 10/10, step 110/574 completed (loss: 0.2618328630924225, acc: 0.9384615421295166)
[2024-12-12 03:10:40,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:40,850][root][INFO] - Training Epoch: 10/10, step 111/574 completed (loss: 0.22799736261367798, acc: 0.9473684430122375)
[2024-12-12 03:10:40,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:41,232][root][INFO] - Training Epoch: 10/10, step 112/574 completed (loss: 0.46037647128105164, acc: 0.859649121761322)
[2024-12-12 03:10:41,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:41,579][root][INFO] - Training Epoch: 10/10, step 113/574 completed (loss: 0.20907051861286163, acc: 0.9743589758872986)
[2024-12-12 03:10:41,679][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:41,952][root][INFO] - Training Epoch: 10/10, step 114/574 completed (loss: 0.2371634989976883, acc: 0.918367326259613)
[2024-12-12 03:10:42,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:42,313][root][INFO] - Training Epoch: 10/10, step 115/574 completed (loss: 0.007760252337902784, acc: 1.0)
[2024-12-12 03:10:42,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:42,710][root][INFO] - Training Epoch: 10/10, step 116/574 completed (loss: 0.41925516724586487, acc: 0.8571428656578064)
[2024-12-12 03:10:42,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:43,105][root][INFO] - Training Epoch: 10/10, step 117/574 completed (loss: 0.3903154730796814, acc: 0.8861788511276245)
[2024-12-12 03:10:43,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:43,496][root][INFO] - Training Epoch: 10/10, step 118/574 completed (loss: 0.1100543960928917, acc: 0.9516128897666931)
[2024-12-12 03:10:43,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:44,384][root][INFO] - Training Epoch: 10/10, step 119/574 completed (loss: 1.0880393981933594, acc: 0.6882129311561584)
[2024-12-12 03:10:44,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:44,766][root][INFO] - Training Epoch: 10/10, step 120/574 completed (loss: 0.1814395934343338, acc: 0.9333333373069763)
[2024-12-12 03:10:44,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:45,181][root][INFO] - Training Epoch: 10/10, step 121/574 completed (loss: 0.33846017718315125, acc: 0.9038461446762085)
[2024-12-12 03:10:45,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:45,542][root][INFO] - Training Epoch: 10/10, step 122/574 completed (loss: 0.06392363458871841, acc: 0.9583333134651184)
[2024-12-12 03:10:45,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:45,920][root][INFO] - Training Epoch: 10/10, step 123/574 completed (loss: 0.005813738331198692, acc: 1.0)
[2024-12-12 03:10:46,018][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:46,272][root][INFO] - Training Epoch: 10/10, step 124/574 completed (loss: 0.8941640257835388, acc: 0.7055214643478394)
[2024-12-12 03:10:47,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:47,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:47,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:48,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:48,519][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:48,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:49,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:49,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:49,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:50,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:50,641][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:51,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:51,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:51,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:52,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:52,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:52,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:53,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:53,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:53,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:54,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:54,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:54,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:55,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:55,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:55,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:56,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:56,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:56,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:57,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:57,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:57,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:58,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:58,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:58,816][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:59,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:59,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:10:59,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:00,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:00,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:00,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:01,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:01,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:01,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:02,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:02,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:02,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:03,027][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:03,341][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:03,668][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:04,009][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:04,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:04,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:04,887][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:05,233][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:05,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:05,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:06,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:06,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:06,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:07,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:07,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:08,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:08,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:08,965][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:09,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:09,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:09,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:10,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:10,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:11,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:11,441][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:11,779][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:12,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:12,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:12,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:13,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:13,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:13,697][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:14,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:14,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:14,579][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:14,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:15,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:15,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:16,165][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.1904, device='cuda:0') eval_epoch_loss=tensor(1.4328, device='cuda:0') eval_epoch_acc=tensor(0.7070, device='cuda:0')
[2024-12-12 03:11:16,166][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:11:16,167][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:11:16,360][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_10_step_125_loss_1.432788610458374/model.pt
[2024-12-12 03:11:16,363][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 03:11:16,447][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:16,761][root][INFO] - Training Epoch: 10/10, step 125/574 completed (loss: 0.7062100172042847, acc: 0.8125)
[2024-12-12 03:11:16,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:17,094][root][INFO] - Training Epoch: 10/10, step 126/574 completed (loss: 0.6173182129859924, acc: 0.7749999761581421)
[2024-12-12 03:11:17,185][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:17,446][root][INFO] - Training Epoch: 10/10, step 127/574 completed (loss: 0.8589505553245544, acc: 0.7142857313156128)
[2024-12-12 03:11:17,577][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:17,828][root][INFO] - Training Epoch: 10/10, step 128/574 completed (loss: 0.7191489338874817, acc: 0.7589743733406067)
[2024-12-12 03:11:17,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:18,218][root][INFO] - Training Epoch: 10/10, step 129/574 completed (loss: 0.719154953956604, acc: 0.8088235259056091)
[2024-12-12 03:11:18,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:18,575][root][INFO] - Training Epoch: 10/10, step 130/574 completed (loss: 0.037269920110702515, acc: 1.0)
[2024-12-12 03:11:18,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:18,917][root][INFO] - Training Epoch: 10/10, step 131/574 completed (loss: 0.07663654536008835, acc: 1.0)
[2024-12-12 03:11:19,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:19,258][root][INFO] - Training Epoch: 10/10, step 132/574 completed (loss: 0.047348491847515106, acc: 1.0)
[2024-12-12 03:11:19,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:19,567][root][INFO] - Training Epoch: 10/10, step 133/574 completed (loss: 0.03182860463857651, acc: 1.0)
[2024-12-12 03:11:19,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:19,923][root][INFO] - Training Epoch: 10/10, step 134/574 completed (loss: 0.38209268450737, acc: 0.8857142925262451)
[2024-12-12 03:11:20,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:20,249][root][INFO] - Training Epoch: 10/10, step 135/574 completed (loss: 0.11608319729566574, acc: 0.9615384340286255)
[2024-12-12 03:11:20,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:20,553][root][INFO] - Training Epoch: 10/10, step 136/574 completed (loss: 0.42199066281318665, acc: 0.9523809552192688)
[2024-12-12 03:11:20,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:20,865][root][INFO] - Training Epoch: 10/10, step 137/574 completed (loss: 0.4687206447124481, acc: 0.8333333134651184)
[2024-12-12 03:11:20,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:21,143][root][INFO] - Training Epoch: 10/10, step 138/574 completed (loss: 0.010777901858091354, acc: 1.0)
[2024-12-12 03:11:21,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:21,443][root][INFO] - Training Epoch: 10/10, step 139/574 completed (loss: 0.152923122048378, acc: 0.9523809552192688)
[2024-12-12 03:11:21,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:21,786][root][INFO] - Training Epoch: 10/10, step 140/574 completed (loss: 0.11204886436462402, acc: 0.9615384340286255)
[2024-12-12 03:11:21,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:22,128][root][INFO] - Training Epoch: 10/10, step 141/574 completed (loss: 0.07656490057706833, acc: 1.0)
[2024-12-12 03:11:22,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:22,507][root][INFO] - Training Epoch: 10/10, step 142/574 completed (loss: 0.27080386877059937, acc: 0.9459459185600281)
[2024-12-12 03:11:22,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:23,029][root][INFO] - Training Epoch: 10/10, step 143/574 completed (loss: 0.5910682678222656, acc: 0.8157894611358643)
[2024-12-12 03:11:23,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:23,360][root][INFO] - Training Epoch: 10/10, step 144/574 completed (loss: 0.6718180775642395, acc: 0.7910447716712952)
[2024-12-12 03:11:23,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:23,762][root][INFO] - Training Epoch: 10/10, step 145/574 completed (loss: 0.6117057800292969, acc: 0.8775510191917419)
[2024-12-12 03:11:23,884][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:24,198][root][INFO] - Training Epoch: 10/10, step 146/574 completed (loss: 0.6942052841186523, acc: 0.7553191781044006)
[2024-12-12 03:11:24,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:24,516][root][INFO] - Training Epoch: 10/10, step 147/574 completed (loss: 0.3749639689922333, acc: 0.8428571224212646)
[2024-12-12 03:11:24,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:24,889][root][INFO] - Training Epoch: 10/10, step 148/574 completed (loss: 0.11347101628780365, acc: 0.9285714030265808)
[2024-12-12 03:11:24,985][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:25,243][root][INFO] - Training Epoch: 10/10, step 149/574 completed (loss: 0.026292741298675537, acc: 1.0)
[2024-12-12 03:11:25,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:25,575][root][INFO] - Training Epoch: 10/10, step 150/574 completed (loss: 0.07024892419576645, acc: 1.0)
[2024-12-12 03:11:25,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:25,920][root][INFO] - Training Epoch: 10/10, step 151/574 completed (loss: 0.1698925942182541, acc: 0.95652174949646)
[2024-12-12 03:11:26,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:26,245][root][INFO] - Training Epoch: 10/10, step 152/574 completed (loss: 0.2761039435863495, acc: 0.9322034120559692)
[2024-12-12 03:11:26,367][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:26,617][root][INFO] - Training Epoch: 10/10, step 153/574 completed (loss: 0.30543428659439087, acc: 0.9122806787490845)
[2024-12-12 03:11:26,749][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:26,993][root][INFO] - Training Epoch: 10/10, step 154/574 completed (loss: 0.3030729591846466, acc: 0.9189189076423645)
[2024-12-12 03:11:27,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:27,345][root][INFO] - Training Epoch: 10/10, step 155/574 completed (loss: 0.2380455881357193, acc: 0.9285714030265808)
[2024-12-12 03:11:27,430][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:27,659][root][INFO] - Training Epoch: 10/10, step 156/574 completed (loss: 0.6345310807228088, acc: 0.8695651888847351)
[2024-12-12 03:11:27,740][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:27,981][root][INFO] - Training Epoch: 10/10, step 157/574 completed (loss: 0.26083824038505554, acc: 0.8947368264198303)
[2024-12-12 03:11:28,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:29,624][root][INFO] - Training Epoch: 10/10, step 158/574 completed (loss: 0.39128807187080383, acc: 0.8783783912658691)
[2024-12-12 03:11:29,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:30,017][root][INFO] - Training Epoch: 10/10, step 159/574 completed (loss: 0.43085938692092896, acc: 0.8703703880310059)
[2024-12-12 03:11:30,164][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:30,439][root][INFO] - Training Epoch: 10/10, step 160/574 completed (loss: 0.510704517364502, acc: 0.8604651093482971)
[2024-12-12 03:11:30,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:31,028][root][INFO] - Training Epoch: 10/10, step 161/574 completed (loss: 0.46731239557266235, acc: 0.8823529481887817)
[2024-12-12 03:11:31,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:31,582][root][INFO] - Training Epoch: 10/10, step 162/574 completed (loss: 0.7124453186988831, acc: 0.7640449404716492)
[2024-12-12 03:11:31,664][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:31,932][root][INFO] - Training Epoch: 10/10, step 163/574 completed (loss: 0.14897458255290985, acc: 0.9090909361839294)
[2024-12-12 03:11:32,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:32,274][root][INFO] - Training Epoch: 10/10, step 164/574 completed (loss: 0.5251420140266418, acc: 0.8571428656578064)
[2024-12-12 03:11:32,395][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:32,661][root][INFO] - Training Epoch: 10/10, step 165/574 completed (loss: 0.349712610244751, acc: 0.931034505367279)
[2024-12-12 03:11:32,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:33,054][root][INFO] - Training Epoch: 10/10, step 166/574 completed (loss: 0.05888024717569351, acc: 1.0)
[2024-12-12 03:11:33,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:33,406][root][INFO] - Training Epoch: 10/10, step 167/574 completed (loss: 0.07111386954784393, acc: 0.9800000190734863)
[2024-12-12 03:11:33,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:33,822][root][INFO] - Training Epoch: 10/10, step 168/574 completed (loss: 0.35656309127807617, acc: 0.8611111044883728)
[2024-12-12 03:11:33,916][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:34,200][root][INFO] - Training Epoch: 10/10, step 169/574 completed (loss: 0.6287970542907715, acc: 0.8039215803146362)
[2024-12-12 03:11:34,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:35,231][root][INFO] - Training Epoch: 10/10, step 170/574 completed (loss: 1.1021919250488281, acc: 0.6780821681022644)
[2024-12-12 03:11:35,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:35,535][root][INFO] - Training Epoch: 10/10, step 171/574 completed (loss: 0.0552305169403553, acc: 1.0)
[2024-12-12 03:11:35,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:35,883][root][INFO] - Training Epoch: 10/10, step 172/574 completed (loss: 0.058824725449085236, acc: 1.0)
[2024-12-12 03:11:35,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:36,166][root][INFO] - Training Epoch: 10/10, step 173/574 completed (loss: 0.14343999326229095, acc: 0.9642857313156128)
[2024-12-12 03:11:36,295][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:36,703][root][INFO] - Training Epoch: 10/10, step 174/574 completed (loss: 0.5489938259124756, acc: 0.8407079577445984)
[2024-12-12 03:11:36,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:37,054][root][INFO] - Training Epoch: 10/10, step 175/574 completed (loss: 0.3098219037055969, acc: 0.9275362491607666)
[2024-12-12 03:11:37,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:37,454][root][INFO] - Training Epoch: 10/10, step 176/574 completed (loss: 0.48290106654167175, acc: 0.8636363744735718)
[2024-12-12 03:11:37,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:38,378][root][INFO] - Training Epoch: 10/10, step 177/574 completed (loss: 1.3158785104751587, acc: 0.6717557311058044)
[2024-12-12 03:11:38,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:39,050][root][INFO] - Training Epoch: 10/10, step 178/574 completed (loss: 0.7476497888565063, acc: 0.770370364189148)
[2024-12-12 03:11:39,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:39,385][root][INFO] - Training Epoch: 10/10, step 179/574 completed (loss: 0.3670315742492676, acc: 0.868852436542511)
[2024-12-12 03:11:39,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:39,750][root][INFO] - Training Epoch: 10/10, step 180/574 completed (loss: 0.015723081305623055, acc: 1.0)
[2024-12-12 03:11:39,875][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:40,139][root][INFO] - Training Epoch: 10/10, step 181/574 completed (loss: 0.02882027067244053, acc: 1.0)
[2024-12-12 03:11:40,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:40,518][root][INFO] - Training Epoch: 10/10, step 182/574 completed (loss: 0.04165274277329445, acc: 1.0)
[2024-12-12 03:11:40,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:40,914][root][INFO] - Training Epoch: 10/10, step 183/574 completed (loss: 0.21750180423259735, acc: 0.9512194991111755)
[2024-12-12 03:11:41,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:41,302][root][INFO] - Training Epoch: 10/10, step 184/574 completed (loss: 0.900642991065979, acc: 0.7794561982154846)
[2024-12-12 03:11:41,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:41,652][root][INFO] - Training Epoch: 10/10, step 185/574 completed (loss: 1.099419116973877, acc: 0.6945244669914246)
[2024-12-12 03:11:41,767][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:42,132][root][INFO] - Training Epoch: 10/10, step 186/574 completed (loss: 1.0312492847442627, acc: 0.703125)
[2024-12-12 03:11:42,267][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:42,660][root][INFO] - Training Epoch: 10/10, step 187/574 completed (loss: 1.2027060985565186, acc: 0.6547842621803284)
[2024-12-12 03:11:42,756][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:43,058][root][INFO] - Training Epoch: 10/10, step 188/574 completed (loss: 0.9418635964393616, acc: 0.7295373678207397)
[2024-12-12 03:11:43,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:43,401][root][INFO] - Training Epoch: 10/10, step 189/574 completed (loss: 0.06306207925081253, acc: 1.0)
[2024-12-12 03:11:43,554][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:43,951][root][INFO] - Training Epoch: 10/10, step 190/574 completed (loss: 0.5139220356941223, acc: 0.8488371968269348)
[2024-12-12 03:11:44,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:44,747][root][INFO] - Training Epoch: 10/10, step 191/574 completed (loss: 0.9861143827438354, acc: 0.7222222089767456)
[2024-12-12 03:11:45,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:45,666][root][INFO] - Training Epoch: 10/10, step 192/574 completed (loss: 0.7175902724266052, acc: 0.7803030014038086)
[2024-12-12 03:11:45,866][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:46,414][root][INFO] - Training Epoch: 10/10, step 193/574 completed (loss: 0.4400465190410614, acc: 0.8352941274642944)
[2024-12-12 03:11:46,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:47,490][root][INFO] - Training Epoch: 10/10, step 194/574 completed (loss: 0.6850425601005554, acc: 0.7839506268501282)
[2024-12-12 03:11:47,758][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:48,443][root][INFO] - Training Epoch: 10/10, step 195/574 completed (loss: 0.316780149936676, acc: 0.8548387289047241)
[2024-12-12 03:11:48,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:48,828][root][INFO] - Training Epoch: 10/10, step 196/574 completed (loss: 0.0419749915599823, acc: 1.0)
[2024-12-12 03:11:48,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:49,168][root][INFO] - Training Epoch: 10/10, step 197/574 completed (loss: 0.04503234103322029, acc: 1.0)
[2024-12-12 03:11:49,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:49,543][root][INFO] - Training Epoch: 10/10, step 198/574 completed (loss: 0.1341225504875183, acc: 0.9558823704719543)
[2024-12-12 03:11:49,637][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:49,918][root][INFO] - Training Epoch: 10/10, step 199/574 completed (loss: 0.5348984003067017, acc: 0.8529411554336548)
[2024-12-12 03:11:50,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:50,325][root][INFO] - Training Epoch: 10/10, step 200/574 completed (loss: 0.411602646112442, acc: 0.8983050584793091)
[2024-12-12 03:11:50,440][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:50,699][root][INFO] - Training Epoch: 10/10, step 201/574 completed (loss: 0.5618664622306824, acc: 0.8208954930305481)
[2024-12-12 03:11:50,818][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:51,058][root][INFO] - Training Epoch: 10/10, step 202/574 completed (loss: 0.3731387257575989, acc: 0.9126213788986206)
[2024-12-12 03:11:51,170][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:51,421][root][INFO] - Training Epoch: 10/10, step 203/574 completed (loss: 0.18490712344646454, acc: 0.9365079402923584)
[2024-12-12 03:11:51,511][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:51,753][root][INFO] - Training Epoch: 10/10, step 204/574 completed (loss: 0.18908923864364624, acc: 0.9560439586639404)
[2024-12-12 03:11:51,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:52,150][root][INFO] - Training Epoch: 10/10, step 205/574 completed (loss: 0.6067661643028259, acc: 0.8251121044158936)
[2024-12-12 03:11:52,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:52,566][root][INFO] - Training Epoch: 10/10, step 206/574 completed (loss: 0.7691987752914429, acc: 0.7913385629653931)
[2024-12-12 03:11:52,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:52,992][root][INFO] - Training Epoch: 10/10, step 207/574 completed (loss: 0.6258460879325867, acc: 0.8146551847457886)
[2024-12-12 03:11:53,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:53,381][root][INFO] - Training Epoch: 10/10, step 208/574 completed (loss: 0.7284589409828186, acc: 0.8224637508392334)
[2024-12-12 03:11:53,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:53,750][root][INFO] - Training Epoch: 10/10, step 209/574 completed (loss: 0.6133557558059692, acc: 0.8249027132987976)
[2024-12-12 03:11:53,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:54,100][root][INFO] - Training Epoch: 10/10, step 210/574 completed (loss: 0.35827285051345825, acc: 0.8913043737411499)
[2024-12-12 03:11:54,221][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:54,481][root][INFO] - Training Epoch: 10/10, step 211/574 completed (loss: 0.019166134297847748, acc: 1.0)
[2024-12-12 03:11:54,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:54,858][root][INFO] - Training Epoch: 10/10, step 212/574 completed (loss: 0.14898577332496643, acc: 0.9642857313156128)
[2024-12-12 03:11:54,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:55,247][root][INFO] - Training Epoch: 10/10, step 213/574 completed (loss: 0.1360376477241516, acc: 0.957446813583374)
[2024-12-12 03:11:55,412][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:55,927][root][INFO] - Training Epoch: 10/10, step 214/574 completed (loss: 0.4162006080150604, acc: 0.8769230842590332)
[2024-12-12 03:11:56,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:56,288][root][INFO] - Training Epoch: 10/10, step 215/574 completed (loss: 0.17500105500221252, acc: 0.9459459185600281)
[2024-12-12 03:11:56,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:56,685][root][INFO] - Training Epoch: 10/10, step 216/574 completed (loss: 0.2467980533838272, acc: 0.9186046719551086)
[2024-12-12 03:11:56,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:57,220][root][INFO] - Training Epoch: 10/10, step 217/574 completed (loss: 0.3718279004096985, acc: 0.8828828930854797)
[2024-12-12 03:11:57,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:57,599][root][INFO] - Training Epoch: 10/10, step 218/574 completed (loss: 0.2152349054813385, acc: 0.8999999761581421)
[2024-12-12 03:11:57,676][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:57,909][root][INFO] - Training Epoch: 10/10, step 219/574 completed (loss: 0.048204731196165085, acc: 1.0)
[2024-12-12 03:11:58,017][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:58,290][root][INFO] - Training Epoch: 10/10, step 220/574 completed (loss: 0.001124367117881775, acc: 1.0)
[2024-12-12 03:11:58,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:58,661][root][INFO] - Training Epoch: 10/10, step 221/574 completed (loss: 0.0035392052959650755, acc: 1.0)
[2024-12-12 03:11:58,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:59,053][root][INFO] - Training Epoch: 10/10, step 222/574 completed (loss: 0.07263855636119843, acc: 0.9807692170143127)
[2024-12-12 03:11:59,250][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:11:59,831][root][INFO] - Training Epoch: 10/10, step 223/574 completed (loss: 0.4461931586265564, acc: 0.8478260636329651)
[2024-12-12 03:11:59,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:00,374][root][INFO] - Training Epoch: 10/10, step 224/574 completed (loss: 0.7111250162124634, acc: 0.8011363744735718)
[2024-12-12 03:12:00,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:00,803][root][INFO] - Training Epoch: 10/10, step 225/574 completed (loss: 0.39923757314682007, acc: 0.8404255509376526)
[2024-12-12 03:12:00,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:01,150][root][INFO] - Training Epoch: 10/10, step 226/574 completed (loss: 0.0856863409280777, acc: 0.9811320900917053)
[2024-12-12 03:12:01,247][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:01,505][root][INFO] - Training Epoch: 10/10, step 227/574 completed (loss: 0.16536615788936615, acc: 0.9666666388511658)
[2024-12-12 03:12:01,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:01,870][root][INFO] - Training Epoch: 10/10, step 228/574 completed (loss: 0.21164463460445404, acc: 0.930232584476471)
[2024-12-12 03:12:01,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:02,202][root][INFO] - Training Epoch: 10/10, step 229/574 completed (loss: 0.20578645169734955, acc: 0.8999999761581421)
[2024-12-12 03:12:02,327][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:02,618][root][INFO] - Training Epoch: 10/10, step 230/574 completed (loss: 0.5073503851890564, acc: 0.9052631855010986)
[2024-12-12 03:12:02,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:02,957][root][INFO] - Training Epoch: 10/10, step 231/574 completed (loss: 0.4371680021286011, acc: 0.8777777552604675)
[2024-12-12 03:12:03,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:03,368][root][INFO] - Training Epoch: 10/10, step 232/574 completed (loss: 0.6951596736907959, acc: 0.7888888716697693)
[2024-12-12 03:12:03,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:03,856][root][INFO] - Training Epoch: 10/10, step 233/574 completed (loss: 1.1369907855987549, acc: 0.6834862232208252)
[2024-12-12 03:12:03,976][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:04,332][root][INFO] - Training Epoch: 10/10, step 234/574 completed (loss: 0.5973790884017944, acc: 0.7923076748847961)
[2024-12-12 03:12:04,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:04,667][root][INFO] - Training Epoch: 10/10, step 235/574 completed (loss: 0.04360293596982956, acc: 1.0)
[2024-12-12 03:12:04,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:05,027][root][INFO] - Training Epoch: 10/10, step 236/574 completed (loss: 0.05504113808274269, acc: 1.0)
[2024-12-12 03:12:05,131][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:05,350][root][INFO] - Training Epoch: 10/10, step 237/574 completed (loss: 0.10183843970298767, acc: 0.9545454382896423)
[2024-12-12 03:12:05,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:05,673][root][INFO] - Training Epoch: 10/10, step 238/574 completed (loss: 0.5585964918136597, acc: 0.8888888955116272)
[2024-12-12 03:12:05,797][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:06,030][root][INFO] - Training Epoch: 10/10, step 239/574 completed (loss: 0.2521962821483612, acc: 0.9142857193946838)
[2024-12-12 03:12:06,148][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:06,427][root][INFO] - Training Epoch: 10/10, step 240/574 completed (loss: 0.12770135700702667, acc: 0.9772727489471436)
[2024-12-12 03:12:06,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:06,788][root][INFO] - Training Epoch: 10/10, step 241/574 completed (loss: 0.19011864066123962, acc: 0.9318181872367859)
[2024-12-12 03:12:06,937][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:07,367][root][INFO] - Training Epoch: 10/10, step 242/574 completed (loss: 0.21319924294948578, acc: 0.9677419066429138)
[2024-12-12 03:12:07,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:07,892][root][INFO] - Training Epoch: 10/10, step 243/574 completed (loss: 0.1930403709411621, acc: 0.9545454382896423)
[2024-12-12 03:12:07,959][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:08,200][root][INFO] - Training Epoch: 10/10, step 244/574 completed (loss: 0.0016326202312484384, acc: 1.0)
[2024-12-12 03:12:08,317][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:08,556][root][INFO] - Training Epoch: 10/10, step 245/574 completed (loss: 0.26975250244140625, acc: 0.9615384340286255)
[2024-12-12 03:12:08,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:08,896][root][INFO] - Training Epoch: 10/10, step 246/574 completed (loss: 0.14567486941814423, acc: 0.9354838728904724)
[2024-12-12 03:12:08,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:09,222][root][INFO] - Training Epoch: 10/10, step 247/574 completed (loss: 0.03462209552526474, acc: 1.0)
[2024-12-12 03:12:09,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:09,563][root][INFO] - Training Epoch: 10/10, step 248/574 completed (loss: 0.04310329258441925, acc: 1.0)
[2024-12-12 03:12:09,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:09,876][root][INFO] - Training Epoch: 10/10, step 249/574 completed (loss: 0.2586767077445984, acc: 0.9189189076423645)
[2024-12-12 03:12:09,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:10,225][root][INFO] - Training Epoch: 10/10, step 250/574 completed (loss: 0.11585943400859833, acc: 0.9729729890823364)
[2024-12-12 03:12:10,339][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:10,601][root][INFO] - Training Epoch: 10/10, step 251/574 completed (loss: 0.08033734560012817, acc: 1.0)
[2024-12-12 03:12:10,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:10,950][root][INFO] - Training Epoch: 10/10, step 252/574 completed (loss: 0.03560924157500267, acc: 1.0)
[2024-12-12 03:12:11,054][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:11,245][root][INFO] - Training Epoch: 10/10, step 253/574 completed (loss: 0.00814023893326521, acc: 1.0)
[2024-12-12 03:12:11,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:11,593][root][INFO] - Training Epoch: 10/10, step 254/574 completed (loss: 0.00308216386474669, acc: 1.0)
[2024-12-12 03:12:11,709][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:11,975][root][INFO] - Training Epoch: 10/10, step 255/574 completed (loss: 0.01846383698284626, acc: 1.0)
[2024-12-12 03:12:12,077][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:12,363][root][INFO] - Training Epoch: 10/10, step 256/574 completed (loss: 0.056552350521087646, acc: 0.9649122953414917)
[2024-12-12 03:12:12,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:12,726][root][INFO] - Training Epoch: 10/10, step 257/574 completed (loss: 0.10048362612724304, acc: 0.9571428298950195)
[2024-12-12 03:12:12,820][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:13,090][root][INFO] - Training Epoch: 10/10, step 258/574 completed (loss: 0.15721848607063293, acc: 0.9736841917037964)
[2024-12-12 03:12:13,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:13,667][root][INFO] - Training Epoch: 10/10, step 259/574 completed (loss: 0.41579264402389526, acc: 0.8962264060974121)
[2024-12-12 03:12:13,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:14,252][root][INFO] - Training Epoch: 10/10, step 260/574 completed (loss: 0.7304906249046326, acc: 0.8416666388511658)
[2024-12-12 03:12:14,330][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:14,603][root][INFO] - Training Epoch: 10/10, step 261/574 completed (loss: 0.054760295897722244, acc: 0.9722222089767456)
[2024-12-12 03:12:14,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:14,981][root][INFO] - Training Epoch: 10/10, step 262/574 completed (loss: 0.28120338916778564, acc: 0.9032257795333862)
[2024-12-12 03:12:15,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:15,334][root][INFO] - Training Epoch: 10/10, step 263/574 completed (loss: 0.4440242052078247, acc: 0.8799999952316284)
[2024-12-12 03:12:15,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:15,681][root][INFO] - Training Epoch: 10/10, step 264/574 completed (loss: 0.3633316457271576, acc: 0.8333333134651184)
[2024-12-12 03:12:15,899][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:16,515][root][INFO] - Training Epoch: 10/10, step 265/574 completed (loss: 0.8983494639396667, acc: 0.7039999961853027)
[2024-12-12 03:12:16,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:16,891][root][INFO] - Training Epoch: 10/10, step 266/574 completed (loss: 0.6153969764709473, acc: 0.7977527976036072)
[2024-12-12 03:12:17,008][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:17,263][root][INFO] - Training Epoch: 10/10, step 267/574 completed (loss: 0.3741765022277832, acc: 0.8648648858070374)
[2024-12-12 03:12:18,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:18,358][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:18,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:19,094][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:19,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:19,803][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:20,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:20,613][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:21,028][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:21,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:21,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:22,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:22,536][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:22,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:23,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:23,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:24,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:24,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:24,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:25,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:25,610][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:25,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:26,309][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:26,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:27,000][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:27,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:27,647][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:27,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:28,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:28,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:29,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:29,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:29,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:29,984][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:30,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:30,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:30,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:31,227][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:31,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:31,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:32,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:32,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:32,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:33,075][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:33,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:33,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:34,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:34,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:34,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:35,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:35,522][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:35,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:36,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:36,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:36,951][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:37,334][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:37,665][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:37,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:38,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:38,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:38,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:39,249][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:39,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:40,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:40,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:40,806][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:41,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:41,574][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:41,990][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:42,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:42,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:43,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:43,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:43,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:44,152][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:44,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:44,784][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:45,155][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:45,428][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:45,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:46,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:46,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:46,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:47,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:47,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:48,072][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.3473, device='cuda:0') eval_epoch_loss=tensor(1.4696, device='cuda:0') eval_epoch_acc=tensor(0.7031, device='cuda:0')
[2024-12-12 03:12:48,074][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:12:48,074][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:12:48,278][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_10_step_268_loss_1.469560146331787/model.pt
[2024-12-12 03:12:48,283][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 03:12:48,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:48,767][root][INFO] - Training Epoch: 10/10, step 268/574 completed (loss: 0.40566232800483704, acc: 0.8620689511299133)
[2024-12-12 03:12:48,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:49,080][root][INFO] - Training Epoch: 10/10, step 269/574 completed (loss: 0.09467752277851105, acc: 0.9090909361839294)
[2024-12-12 03:12:49,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:49,438][root][INFO] - Training Epoch: 10/10, step 270/574 completed (loss: 0.021668560802936554, acc: 1.0)
[2024-12-12 03:12:49,555][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:49,807][root][INFO] - Training Epoch: 10/10, step 271/574 completed (loss: 0.02714414708316326, acc: 1.0)
[2024-12-12 03:12:49,918][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:50,144][root][INFO] - Training Epoch: 10/10, step 272/574 completed (loss: 0.010164727456867695, acc: 1.0)
[2024-12-12 03:12:50,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:50,508][root][INFO] - Training Epoch: 10/10, step 273/574 completed (loss: 0.3402014374732971, acc: 0.9333333373069763)
[2024-12-12 03:12:50,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:50,862][root][INFO] - Training Epoch: 10/10, step 274/574 completed (loss: 0.06821255385875702, acc: 0.96875)
[2024-12-12 03:12:50,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:51,186][root][INFO] - Training Epoch: 10/10, step 275/574 completed (loss: 0.09879051148891449, acc: 0.9666666388511658)
[2024-12-12 03:12:51,303][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:51,545][root][INFO] - Training Epoch: 10/10, step 276/574 completed (loss: 0.02321559749543667, acc: 1.0)
[2024-12-12 03:12:51,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:51,876][root][INFO] - Training Epoch: 10/10, step 277/574 completed (loss: 0.1036834716796875, acc: 0.9599999785423279)
[2024-12-12 03:12:51,980][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:52,211][root][INFO] - Training Epoch: 10/10, step 278/574 completed (loss: 0.18216823041439056, acc: 0.936170220375061)
[2024-12-12 03:12:52,325][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:52,578][root][INFO] - Training Epoch: 10/10, step 279/574 completed (loss: 0.22634534537792206, acc: 0.9166666865348816)
[2024-12-12 03:12:52,678][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:52,918][root][INFO] - Training Epoch: 10/10, step 280/574 completed (loss: 0.07896494120359421, acc: 0.9772727489471436)
[2024-12-12 03:12:53,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:53,330][root][INFO] - Training Epoch: 10/10, step 281/574 completed (loss: 0.3873837888240814, acc: 0.9036144614219666)
[2024-12-12 03:12:53,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:53,680][root][INFO] - Training Epoch: 10/10, step 282/574 completed (loss: 0.7687095999717712, acc: 0.75)
[2024-12-12 03:12:53,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:54,031][root][INFO] - Training Epoch: 10/10, step 283/574 completed (loss: 0.18047398328781128, acc: 0.9473684430122375)
[2024-12-12 03:12:54,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:54,349][root][INFO] - Training Epoch: 10/10, step 284/574 completed (loss: 0.06503486633300781, acc: 0.970588207244873)
[2024-12-12 03:12:54,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:54,654][root][INFO] - Training Epoch: 10/10, step 285/574 completed (loss: 0.04748627543449402, acc: 1.0)
[2024-12-12 03:12:54,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:54,962][root][INFO] - Training Epoch: 10/10, step 286/574 completed (loss: 0.3608440160751343, acc: 0.8984375)
[2024-12-12 03:12:55,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:55,280][root][INFO] - Training Epoch: 10/10, step 287/574 completed (loss: 0.4094306230545044, acc: 0.8560000061988831)
[2024-12-12 03:12:55,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:55,587][root][INFO] - Training Epoch: 10/10, step 288/574 completed (loss: 0.22257064282894135, acc: 0.9450549483299255)
[2024-12-12 03:12:55,698][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:55,935][root][INFO] - Training Epoch: 10/10, step 289/574 completed (loss: 0.6367588043212891, acc: 0.8447204828262329)
[2024-12-12 03:12:56,066][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:56,317][root][INFO] - Training Epoch: 10/10, step 290/574 completed (loss: 0.8012188076972961, acc: 0.7680412530899048)
[2024-12-12 03:12:56,389][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:56,603][root][INFO] - Training Epoch: 10/10, step 291/574 completed (loss: 0.0039623575285077095, acc: 1.0)
[2024-12-12 03:12:56,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:56,888][root][INFO] - Training Epoch: 10/10, step 292/574 completed (loss: 0.06507550179958344, acc: 0.976190447807312)
[2024-12-12 03:12:57,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:57,288][root][INFO] - Training Epoch: 10/10, step 293/574 completed (loss: 0.4658008813858032, acc: 0.8620689511299133)
[2024-12-12 03:12:57,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:57,746][root][INFO] - Training Epoch: 10/10, step 294/574 completed (loss: 0.20078496634960175, acc: 0.9454545378684998)
[2024-12-12 03:12:57,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:58,310][root][INFO] - Training Epoch: 10/10, step 295/574 completed (loss: 0.7894077897071838, acc: 0.7628865838050842)
[2024-12-12 03:12:58,435][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:58,689][root][INFO] - Training Epoch: 10/10, step 296/574 completed (loss: 0.1025872677564621, acc: 0.9655172228813171)
[2024-12-12 03:12:58,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:59,070][root][INFO] - Training Epoch: 10/10, step 297/574 completed (loss: 0.030216047540307045, acc: 1.0)
[2024-12-12 03:12:59,165][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:59,393][root][INFO] - Training Epoch: 10/10, step 298/574 completed (loss: 0.22086946666240692, acc: 0.8947368264198303)
[2024-12-12 03:12:59,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:12:59,744][root][INFO] - Training Epoch: 10/10, step 299/574 completed (loss: 0.07489322870969772, acc: 0.9821428656578064)
[2024-12-12 03:12:59,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:00,076][root][INFO] - Training Epoch: 10/10, step 300/574 completed (loss: 0.19063793122768402, acc: 0.96875)
[2024-12-12 03:13:00,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:00,439][root][INFO] - Training Epoch: 10/10, step 301/574 completed (loss: 0.055321261286735535, acc: 0.9811320900917053)
[2024-12-12 03:13:00,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:00,814][root][INFO] - Training Epoch: 10/10, step 302/574 completed (loss: 0.10201837122440338, acc: 0.9811320900917053)
[2024-12-12 03:13:00,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:01,168][root][INFO] - Training Epoch: 10/10, step 303/574 completed (loss: 0.05734233185648918, acc: 1.0)
[2024-12-12 03:13:01,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:01,485][root][INFO] - Training Epoch: 10/10, step 304/574 completed (loss: 0.1398979276418686, acc: 0.96875)
[2024-12-12 03:13:01,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:01,857][root][INFO] - Training Epoch: 10/10, step 305/574 completed (loss: 0.13498470187187195, acc: 0.9344262480735779)
[2024-12-12 03:13:01,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:02,218][root][INFO] - Training Epoch: 10/10, step 306/574 completed (loss: 0.026332413777709007, acc: 1.0)
[2024-12-12 03:13:02,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:02,539][root][INFO] - Training Epoch: 10/10, step 307/574 completed (loss: 0.0015254197642207146, acc: 1.0)
[2024-12-12 03:13:02,674][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:02,927][root][INFO] - Training Epoch: 10/10, step 308/574 completed (loss: 0.26539766788482666, acc: 0.9275362491607666)
[2024-12-12 03:13:03,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:03,341][root][INFO] - Training Epoch: 10/10, step 309/574 completed (loss: 0.2851276993751526, acc: 0.9166666865348816)
[2024-12-12 03:13:03,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:03,663][root][INFO] - Training Epoch: 10/10, step 310/574 completed (loss: 0.38527911901474, acc: 0.9036144614219666)
[2024-12-12 03:13:03,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:04,021][root][INFO] - Training Epoch: 10/10, step 311/574 completed (loss: 0.48350587487220764, acc: 0.8205128312110901)
[2024-12-12 03:13:04,139][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:04,389][root][INFO] - Training Epoch: 10/10, step 312/574 completed (loss: 0.2504235804080963, acc: 0.918367326259613)
[2024-12-12 03:13:04,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:04,732][root][INFO] - Training Epoch: 10/10, step 313/574 completed (loss: 0.04753895103931427, acc: 0.9583333134651184)
[2024-12-12 03:13:04,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:05,083][root][INFO] - Training Epoch: 10/10, step 314/574 completed (loss: 0.029451884329319, acc: 1.0)
[2024-12-12 03:13:05,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:05,450][root][INFO] - Training Epoch: 10/10, step 315/574 completed (loss: 0.01621004194021225, acc: 1.0)
[2024-12-12 03:13:05,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:05,766][root][INFO] - Training Epoch: 10/10, step 316/574 completed (loss: 0.4214700162410736, acc: 0.8709677457809448)
[2024-12-12 03:13:05,858][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:06,125][root][INFO] - Training Epoch: 10/10, step 317/574 completed (loss: 0.14489124715328217, acc: 0.9701492786407471)
[2024-12-12 03:13:06,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:06,501][root][INFO] - Training Epoch: 10/10, step 318/574 completed (loss: 0.2856055498123169, acc: 0.9134615659713745)
[2024-12-12 03:13:06,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:06,776][root][INFO] - Training Epoch: 10/10, step 319/574 completed (loss: 0.0197181086987257, acc: 1.0)
[2024-12-12 03:13:06,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:07,040][root][INFO] - Training Epoch: 10/10, step 320/574 completed (loss: 0.10303547978401184, acc: 0.9677419066429138)
[2024-12-12 03:13:07,125][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:07,358][root][INFO] - Training Epoch: 10/10, step 321/574 completed (loss: 0.13615252077579498, acc: 0.9800000190734863)
[2024-12-12 03:13:07,487][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:07,726][root][INFO] - Training Epoch: 10/10, step 322/574 completed (loss: 0.266674667596817, acc: 0.8888888955116272)
[2024-12-12 03:13:07,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:08,105][root][INFO] - Training Epoch: 10/10, step 323/574 completed (loss: 0.21777968108654022, acc: 0.9428571462631226)
[2024-12-12 03:13:08,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:08,436][root][INFO] - Training Epoch: 10/10, step 324/574 completed (loss: 0.17355148494243622, acc: 0.9487179517745972)
[2024-12-12 03:13:08,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:08,760][root][INFO] - Training Epoch: 10/10, step 325/574 completed (loss: 0.37714993953704834, acc: 0.9024389982223511)
[2024-12-12 03:13:08,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:09,045][root][INFO] - Training Epoch: 10/10, step 326/574 completed (loss: 0.22378680109977722, acc: 0.9210526347160339)
[2024-12-12 03:13:09,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:09,384][root][INFO] - Training Epoch: 10/10, step 327/574 completed (loss: 0.08603058010339737, acc: 0.9473684430122375)
[2024-12-12 03:13:09,486][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:09,741][root][INFO] - Training Epoch: 10/10, step 328/574 completed (loss: 0.01515579130500555, acc: 1.0)
[2024-12-12 03:13:09,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:10,130][root][INFO] - Training Epoch: 10/10, step 329/574 completed (loss: 0.30837684869766235, acc: 0.8888888955116272)
[2024-12-12 03:13:10,255][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:10,509][root][INFO] - Training Epoch: 10/10, step 330/574 completed (loss: 0.032692939043045044, acc: 1.0)
[2024-12-12 03:13:10,643][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:10,890][root][INFO] - Training Epoch: 10/10, step 331/574 completed (loss: 0.25509634613990784, acc: 0.9193548560142517)
[2024-12-12 03:13:11,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:11,262][root][INFO] - Training Epoch: 10/10, step 332/574 completed (loss: 0.24108435213565826, acc: 0.8947368264198303)
[2024-12-12 03:13:11,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:11,586][root][INFO] - Training Epoch: 10/10, step 333/574 completed (loss: 0.21290212869644165, acc: 0.9375)
[2024-12-12 03:13:11,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:11,947][root][INFO] - Training Epoch: 10/10, step 334/574 completed (loss: 0.0643673688173294, acc: 0.9666666388511658)
[2024-12-12 03:13:12,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:12,320][root][INFO] - Training Epoch: 10/10, step 335/574 completed (loss: 0.015197121538221836, acc: 1.0)
[2024-12-12 03:13:12,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:12,670][root][INFO] - Training Epoch: 10/10, step 336/574 completed (loss: 0.08745473623275757, acc: 0.9800000190734863)
[2024-12-12 03:13:12,764][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:13,042][root][INFO] - Training Epoch: 10/10, step 337/574 completed (loss: 0.43917036056518555, acc: 0.8505747318267822)
[2024-12-12 03:13:13,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:13,430][root][INFO] - Training Epoch: 10/10, step 338/574 completed (loss: 0.5624862313270569, acc: 0.8617021441459656)
[2024-12-12 03:13:13,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:13,803][root][INFO] - Training Epoch: 10/10, step 339/574 completed (loss: 0.4819420576095581, acc: 0.8433734774589539)
[2024-12-12 03:13:13,896][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:14,167][root][INFO] - Training Epoch: 10/10, step 340/574 completed (loss: 0.011122974567115307, acc: 1.0)
[2024-12-12 03:13:14,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:14,525][root][INFO] - Training Epoch: 10/10, step 341/574 completed (loss: 0.08933716267347336, acc: 0.9743589758872986)
[2024-12-12 03:13:14,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:14,930][root][INFO] - Training Epoch: 10/10, step 342/574 completed (loss: 0.19220148026943207, acc: 0.9277108311653137)
[2024-12-12 03:13:15,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:15,268][root][INFO] - Training Epoch: 10/10, step 343/574 completed (loss: 0.28391900658607483, acc: 0.9056603908538818)
[2024-12-12 03:13:15,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:15,640][root][INFO] - Training Epoch: 10/10, step 344/574 completed (loss: 0.14292846620082855, acc: 0.949367105960846)
[2024-12-12 03:13:15,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:15,989][root][INFO] - Training Epoch: 10/10, step 345/574 completed (loss: 0.07845243066549301, acc: 0.9803921580314636)
[2024-12-12 03:13:16,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:16,332][root][INFO] - Training Epoch: 10/10, step 346/574 completed (loss: 0.26945552229881287, acc: 0.9402984976768494)
[2024-12-12 03:13:16,448][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:16,691][root][INFO] - Training Epoch: 10/10, step 347/574 completed (loss: 0.0020982821006327868, acc: 1.0)
[2024-12-12 03:13:16,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:17,028][root][INFO] - Training Epoch: 10/10, step 348/574 completed (loss: 0.03739408403635025, acc: 1.0)
[2024-12-12 03:13:17,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:17,403][root][INFO] - Training Epoch: 10/10, step 349/574 completed (loss: 0.050915010273456573, acc: 1.0)
[2024-12-12 03:13:17,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:17,706][root][INFO] - Training Epoch: 10/10, step 350/574 completed (loss: 0.3923344612121582, acc: 0.9069767594337463)
[2024-12-12 03:13:17,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:18,028][root][INFO] - Training Epoch: 10/10, step 351/574 completed (loss: 0.08975578099489212, acc: 1.0)
[2024-12-12 03:13:18,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:18,391][root][INFO] - Training Epoch: 10/10, step 352/574 completed (loss: 0.11602238565683365, acc: 1.0)
[2024-12-12 03:13:18,472][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:18,734][root][INFO] - Training Epoch: 10/10, step 353/574 completed (loss: 0.0034844926558434963, acc: 1.0)
[2024-12-12 03:13:18,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:19,100][root][INFO] - Training Epoch: 10/10, step 354/574 completed (loss: 0.11764935404062271, acc: 0.9615384340286255)
[2024-12-12 03:13:19,200][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:19,461][root][INFO] - Training Epoch: 10/10, step 355/574 completed (loss: 0.5224008560180664, acc: 0.8571428656578064)
[2024-12-12 03:13:19,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:19,966][root][INFO] - Training Epoch: 10/10, step 356/574 completed (loss: 0.6523488759994507, acc: 0.834782600402832)
[2024-12-12 03:13:20,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:20,316][root][INFO] - Training Epoch: 10/10, step 357/574 completed (loss: 0.2804587781429291, acc: 0.8913043737411499)
[2024-12-12 03:13:20,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:20,690][root][INFO] - Training Epoch: 10/10, step 358/574 completed (loss: 0.17426562309265137, acc: 0.918367326259613)
[2024-12-12 03:13:20,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:21,042][root][INFO] - Training Epoch: 10/10, step 359/574 completed (loss: 0.09117856621742249, acc: 0.9583333134651184)
[2024-12-12 03:13:21,158][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:21,404][root][INFO] - Training Epoch: 10/10, step 360/574 completed (loss: 0.04450283944606781, acc: 1.0)
[2024-12-12 03:13:21,518][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:21,786][root][INFO] - Training Epoch: 10/10, step 361/574 completed (loss: 0.09483447670936584, acc: 0.9512194991111755)
[2024-12-12 03:13:21,911][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:22,179][root][INFO] - Training Epoch: 10/10, step 362/574 completed (loss: 0.04511775076389313, acc: 1.0)
[2024-12-12 03:13:22,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:22,548][root][INFO] - Training Epoch: 10/10, step 363/574 completed (loss: 0.16413220763206482, acc: 0.9736841917037964)
[2024-12-12 03:13:22,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:22,898][root][INFO] - Training Epoch: 10/10, step 364/574 completed (loss: 0.07124154269695282, acc: 0.9756097793579102)
[2024-12-12 03:13:22,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:23,246][root][INFO] - Training Epoch: 10/10, step 365/574 completed (loss: 0.1466951072216034, acc: 0.939393937587738)
[2024-12-12 03:13:23,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:23,613][root][INFO] - Training Epoch: 10/10, step 366/574 completed (loss: 0.016328757628798485, acc: 1.0)
[2024-12-12 03:13:23,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:23,934][root][INFO] - Training Epoch: 10/10, step 367/574 completed (loss: 0.2677466869354248, acc: 0.9130434989929199)
[2024-12-12 03:13:24,013][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:24,232][root][INFO] - Training Epoch: 10/10, step 368/574 completed (loss: 0.3965294361114502, acc: 0.9285714030265808)
[2024-12-12 03:13:24,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:24,583][root][INFO] - Training Epoch: 10/10, step 369/574 completed (loss: 0.10019230097532272, acc: 0.96875)
[2024-12-12 03:13:24,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:25,235][root][INFO] - Training Epoch: 10/10, step 370/574 completed (loss: 0.6405515074729919, acc: 0.7939394116401672)
[2024-12-12 03:13:25,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:26,128][root][INFO] - Training Epoch: 10/10, step 371/574 completed (loss: 0.3538656532764435, acc: 0.8962264060974121)
[2024-12-12 03:13:26,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:26,467][root][INFO] - Training Epoch: 10/10, step 372/574 completed (loss: 0.3120979368686676, acc: 0.8999999761581421)
[2024-12-12 03:13:26,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:26,794][root][INFO] - Training Epoch: 10/10, step 373/574 completed (loss: 0.10782628506422043, acc: 0.9642857313156128)
[2024-12-12 03:13:26,881][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:27,107][root][INFO] - Training Epoch: 10/10, step 374/574 completed (loss: 0.17626796662807465, acc: 0.9428571462631226)
[2024-12-12 03:13:27,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:27,426][root][INFO] - Training Epoch: 10/10, step 375/574 completed (loss: 0.012818809598684311, acc: 1.0)
[2024-12-12 03:13:27,557][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:27,847][root][INFO] - Training Epoch: 10/10, step 376/574 completed (loss: 0.0034389463253319263, acc: 1.0)
[2024-12-12 03:13:27,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:28,246][root][INFO] - Training Epoch: 10/10, step 377/574 completed (loss: 0.06833440810441971, acc: 0.9791666865348816)
[2024-12-12 03:13:28,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:28,636][root][INFO] - Training Epoch: 10/10, step 378/574 completed (loss: 0.25091204047203064, acc: 0.9157894849777222)
[2024-12-12 03:13:28,792][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:29,226][root][INFO] - Training Epoch: 10/10, step 379/574 completed (loss: 0.5241515040397644, acc: 0.8622754216194153)
[2024-12-12 03:13:29,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:29,628][root][INFO] - Training Epoch: 10/10, step 380/574 completed (loss: 0.377067506313324, acc: 0.8947368264198303)
[2024-12-12 03:13:30,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:30,935][root][INFO] - Training Epoch: 10/10, step 381/574 completed (loss: 0.6380852460861206, acc: 0.8288770318031311)
[2024-12-12 03:13:31,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:31,499][root][INFO] - Training Epoch: 10/10, step 382/574 completed (loss: 0.2807803153991699, acc: 0.9189189076423645)
[2024-12-12 03:13:31,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:31,803][root][INFO] - Training Epoch: 10/10, step 383/574 completed (loss: 0.04073287919163704, acc: 1.0)
[2024-12-12 03:13:31,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:32,094][root][INFO] - Training Epoch: 10/10, step 384/574 completed (loss: 0.003438579151406884, acc: 1.0)
[2024-12-12 03:13:32,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:32,447][root][INFO] - Training Epoch: 10/10, step 385/574 completed (loss: 0.03576729819178581, acc: 1.0)
[2024-12-12 03:13:32,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:32,830][root][INFO] - Training Epoch: 10/10, step 386/574 completed (loss: 0.05943546071648598, acc: 0.9722222089767456)
[2024-12-12 03:13:32,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:33,185][root][INFO] - Training Epoch: 10/10, step 387/574 completed (loss: 0.005712089594453573, acc: 1.0)
[2024-12-12 03:13:33,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:33,536][root][INFO] - Training Epoch: 10/10, step 388/574 completed (loss: 0.0237860269844532, acc: 1.0)
[2024-12-12 03:13:33,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:33,913][root][INFO] - Training Epoch: 10/10, step 389/574 completed (loss: 0.013148590922355652, acc: 1.0)
[2024-12-12 03:13:34,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:34,290][root][INFO] - Training Epoch: 10/10, step 390/574 completed (loss: 0.15998205542564392, acc: 0.9523809552192688)
[2024-12-12 03:13:34,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:34,617][root][INFO] - Training Epoch: 10/10, step 391/574 completed (loss: 0.27451398968696594, acc: 0.9259259104728699)
[2024-12-12 03:13:34,710][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:34,926][root][INFO] - Training Epoch: 10/10, step 392/574 completed (loss: 0.5443949103355408, acc: 0.7961165308952332)
[2024-12-12 03:13:35,040][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:35,451][root][INFO] - Training Epoch: 10/10, step 393/574 completed (loss: 0.6928597092628479, acc: 0.7867646813392639)
[2024-12-12 03:13:35,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:35,826][root][INFO] - Training Epoch: 10/10, step 394/574 completed (loss: 0.669849693775177, acc: 0.7933333516120911)
[2024-12-12 03:13:35,946][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:36,214][root][INFO] - Training Epoch: 10/10, step 395/574 completed (loss: 0.7116345763206482, acc: 0.7916666865348816)
[2024-12-12 03:13:36,297][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:36,540][root][INFO] - Training Epoch: 10/10, step 396/574 completed (loss: 0.19940924644470215, acc: 0.9767441749572754)
[2024-12-12 03:13:36,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:36,875][root][INFO] - Training Epoch: 10/10, step 397/574 completed (loss: 0.004197065718472004, acc: 1.0)
[2024-12-12 03:13:36,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:37,240][root][INFO] - Training Epoch: 10/10, step 398/574 completed (loss: 0.4421905279159546, acc: 0.8837209343910217)
[2024-12-12 03:13:37,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:37,564][root][INFO] - Training Epoch: 10/10, step 399/574 completed (loss: 0.006413304712623358, acc: 1.0)
[2024-12-12 03:13:37,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:38,098][root][INFO] - Training Epoch: 10/10, step 400/574 completed (loss: 0.3813810646533966, acc: 0.8823529481887817)
[2024-12-12 03:13:38,188][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:38,442][root][INFO] - Training Epoch: 10/10, step 401/574 completed (loss: 0.12510384619235992, acc: 0.9733333587646484)
[2024-12-12 03:13:38,553][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:38,779][root][INFO] - Training Epoch: 10/10, step 402/574 completed (loss: 0.05840203911066055, acc: 1.0)
[2024-12-12 03:13:38,868][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:39,108][root][INFO] - Training Epoch: 10/10, step 403/574 completed (loss: 0.13530877232551575, acc: 0.9696969985961914)
[2024-12-12 03:13:39,220][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:39,450][root][INFO] - Training Epoch: 10/10, step 404/574 completed (loss: 0.015167917124927044, acc: 1.0)
[2024-12-12 03:13:39,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:39,814][root][INFO] - Training Epoch: 10/10, step 405/574 completed (loss: 0.005489659961313009, acc: 1.0)
[2024-12-12 03:13:39,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:40,183][root][INFO] - Training Epoch: 10/10, step 406/574 completed (loss: 0.012178456410765648, acc: 1.0)
[2024-12-12 03:13:40,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:40,498][root][INFO] - Training Epoch: 10/10, step 407/574 completed (loss: 0.04712994396686554, acc: 1.0)
[2024-12-12 03:13:40,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:40,825][root][INFO] - Training Epoch: 10/10, step 408/574 completed (loss: 0.008516745641827583, acc: 1.0)
[2024-12-12 03:13:40,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:41,189][root][INFO] - Training Epoch: 10/10, step 409/574 completed (loss: 0.00247042253613472, acc: 1.0)
[2024-12-12 03:13:41,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:41,567][root][INFO] - Training Epoch: 10/10, step 410/574 completed (loss: 0.04410160705447197, acc: 0.982758641242981)
[2024-12-12 03:13:42,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:42,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:42,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:43,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:43,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:43,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:44,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:44,592][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:44,986][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:45,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:45,826][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:46,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:46,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:46,944][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:47,405][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:47,737][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:48,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:48,356][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:48,705][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:49,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:49,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:49,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:49,956][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:50,343][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:50,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:51,090][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:51,391][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:51,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:52,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:52,406][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:52,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:53,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:53,542][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:53,877][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:54,260][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:54,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:55,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:55,483][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:55,851][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:56,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:56,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:56,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:57,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:57,454][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:57,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:58,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:58,396][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:58,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:59,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:59,379][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:13:59,681][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:00,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:00,369][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:00,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:01,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:01,308][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:01,602][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:01,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:02,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:02,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:02,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:03,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:03,870][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:04,205][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:04,583][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:04,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:05,287][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:05,653][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:05,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:06,409][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:06,871][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:07,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:07,561][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:07,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:08,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:08,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:08,814][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:09,142][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:09,473][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:09,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:10,127][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:10,521][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:10,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:11,280][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:11,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:12,236][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.2125, device='cuda:0') eval_epoch_loss=tensor(1.4381, device='cuda:0') eval_epoch_acc=tensor(0.6954, device='cuda:0')
[2024-12-12 03:14:12,237][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:14:12,237][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:14:12,436][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_10_step_411_loss_1.4380615949630737/model.pt
[2024-12-12 03:14:12,439][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 03:14:12,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:12,833][root][INFO] - Training Epoch: 10/10, step 411/574 completed (loss: 0.01652480475604534, acc: 1.0)
[2024-12-12 03:14:12,972][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:13,220][root][INFO] - Training Epoch: 10/10, step 412/574 completed (loss: 0.032727405428886414, acc: 1.0)
[2024-12-12 03:14:13,338][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:13,575][root][INFO] - Training Epoch: 10/10, step 413/574 completed (loss: 0.06999816000461578, acc: 0.9696969985961914)
[2024-12-12 03:14:13,670][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:13,933][root][INFO] - Training Epoch: 10/10, step 414/574 completed (loss: 0.017372580245137215, acc: 1.0)
[2024-12-12 03:14:14,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:14,296][root][INFO] - Training Epoch: 10/10, step 415/574 completed (loss: 0.21807743608951569, acc: 0.9215686321258545)
[2024-12-12 03:14:14,388][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:14,608][root][INFO] - Training Epoch: 10/10, step 416/574 completed (loss: 0.09382018446922302, acc: 0.9615384340286255)
[2024-12-12 03:14:14,704][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:14,956][root][INFO] - Training Epoch: 10/10, step 417/574 completed (loss: 0.049845073372125626, acc: 1.0)
[2024-12-12 03:14:15,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:15,310][root][INFO] - Training Epoch: 10/10, step 418/574 completed (loss: 0.07962222397327423, acc: 1.0)
[2024-12-12 03:14:15,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:15,696][root][INFO] - Training Epoch: 10/10, step 419/574 completed (loss: 0.015906166285276413, acc: 1.0)
[2024-12-12 03:14:15,804][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:16,034][root][INFO] - Training Epoch: 10/10, step 420/574 completed (loss: 0.004974245093762875, acc: 1.0)
[2024-12-12 03:14:16,121][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:16,402][root][INFO] - Training Epoch: 10/10, step 421/574 completed (loss: 0.014419355429708958, acc: 1.0)
[2024-12-12 03:14:16,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:16,747][root][INFO] - Training Epoch: 10/10, step 422/574 completed (loss: 0.02818428911268711, acc: 1.0)
[2024-12-12 03:14:16,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:17,088][root][INFO] - Training Epoch: 10/10, step 423/574 completed (loss: 0.019592098891735077, acc: 1.0)
[2024-12-12 03:14:17,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:17,408][root][INFO] - Training Epoch: 10/10, step 424/574 completed (loss: 0.057646628469228745, acc: 0.9629629850387573)
[2024-12-12 03:14:17,546][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:17,790][root][INFO] - Training Epoch: 10/10, step 425/574 completed (loss: 0.020191682502627373, acc: 1.0)
[2024-12-12 03:14:17,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:18,143][root][INFO] - Training Epoch: 10/10, step 426/574 completed (loss: 0.0024576822761446238, acc: 1.0)
[2024-12-12 03:14:18,253][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:18,494][root][INFO] - Training Epoch: 10/10, step 427/574 completed (loss: 0.03204837068915367, acc: 1.0)
[2024-12-12 03:14:18,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:18,813][root][INFO] - Training Epoch: 10/10, step 428/574 completed (loss: 0.01255878247320652, acc: 1.0)
[2024-12-12 03:14:18,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:19,153][root][INFO] - Training Epoch: 10/10, step 429/574 completed (loss: 0.11528733372688293, acc: 0.95652174949646)
[2024-12-12 03:14:19,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:19,494][root][INFO] - Training Epoch: 10/10, step 430/574 completed (loss: 0.0007010003901086748, acc: 1.0)
[2024-12-12 03:14:19,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:19,851][root][INFO] - Training Epoch: 10/10, step 431/574 completed (loss: 0.04652000963687897, acc: 0.9629629850387573)
[2024-12-12 03:14:19,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:20,217][root][INFO] - Training Epoch: 10/10, step 432/574 completed (loss: 0.004213723819702864, acc: 1.0)
[2024-12-12 03:14:20,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:20,602][root][INFO] - Training Epoch: 10/10, step 433/574 completed (loss: 0.11161785572767258, acc: 0.9166666865348816)
[2024-12-12 03:14:20,711][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:20,981][root][INFO] - Training Epoch: 10/10, step 434/574 completed (loss: 0.0035948234144598246, acc: 1.0)
[2024-12-12 03:14:21,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:21,351][root][INFO] - Training Epoch: 10/10, step 435/574 completed (loss: 0.013889273628592491, acc: 1.0)
[2024-12-12 03:14:21,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:21,692][root][INFO] - Training Epoch: 10/10, step 436/574 completed (loss: 0.09050414711236954, acc: 0.9722222089767456)
[2024-12-12 03:14:21,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:22,079][root][INFO] - Training Epoch: 10/10, step 437/574 completed (loss: 0.023342011496424675, acc: 1.0)
[2024-12-12 03:14:22,184][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:22,456][root][INFO] - Training Epoch: 10/10, step 438/574 completed (loss: 0.0012015241663902998, acc: 1.0)
[2024-12-12 03:14:22,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:22,835][root][INFO] - Training Epoch: 10/10, step 439/574 completed (loss: 0.1427677422761917, acc: 0.9487179517745972)
[2024-12-12 03:14:22,997][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:23,319][root][INFO] - Training Epoch: 10/10, step 440/574 completed (loss: 0.21691766381263733, acc: 0.9242424368858337)
[2024-12-12 03:14:23,566][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:24,075][root][INFO] - Training Epoch: 10/10, step 441/574 completed (loss: 0.8772239089012146, acc: 0.7120000123977661)
[2024-12-12 03:14:24,209][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:24,492][root][INFO] - Training Epoch: 10/10, step 442/574 completed (loss: 0.6736417412757874, acc: 0.774193525314331)
[2024-12-12 03:14:24,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:25,145][root][INFO] - Training Epoch: 10/10, step 443/574 completed (loss: 0.9216455221176147, acc: 0.7412935495376587)
[2024-12-12 03:14:25,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:25,518][root][INFO] - Training Epoch: 10/10, step 444/574 completed (loss: 0.24903680384159088, acc: 0.9433962106704712)
[2024-12-12 03:14:25,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:25,942][root][INFO] - Training Epoch: 10/10, step 445/574 completed (loss: 0.09191662818193436, acc: 0.9545454382896423)
[2024-12-12 03:14:26,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:26,273][root][INFO] - Training Epoch: 10/10, step 446/574 completed (loss: 0.008555082604289055, acc: 1.0)
[2024-12-12 03:14:26,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:26,629][root][INFO] - Training Epoch: 10/10, step 447/574 completed (loss: 0.011395932175219059, acc: 1.0)
[2024-12-12 03:14:26,744][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:26,998][root][INFO] - Training Epoch: 10/10, step 448/574 completed (loss: 0.08874224126338959, acc: 0.9642857313156128)
[2024-12-12 03:14:27,110][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:27,379][root][INFO] - Training Epoch: 10/10, step 449/574 completed (loss: 0.035339951515197754, acc: 1.0)
[2024-12-12 03:14:27,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:27,781][root][INFO] - Training Epoch: 10/10, step 450/574 completed (loss: 0.1016249805688858, acc: 0.9444444179534912)
[2024-12-12 03:14:27,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:28,105][root][INFO] - Training Epoch: 10/10, step 451/574 completed (loss: 0.16540849208831787, acc: 0.945652186870575)
[2024-12-12 03:14:28,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:28,452][root][INFO] - Training Epoch: 10/10, step 452/574 completed (loss: 0.22424456477165222, acc: 0.9358974099159241)
[2024-12-12 03:14:28,575][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:28,816][root][INFO] - Training Epoch: 10/10, step 453/574 completed (loss: 0.13905039429664612, acc: 0.9473684430122375)
[2024-12-12 03:14:28,926][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:29,163][root][INFO] - Training Epoch: 10/10, step 454/574 completed (loss: 0.08577126264572144, acc: 0.9591836929321289)
[2024-12-12 03:14:29,272][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:29,531][root][INFO] - Training Epoch: 10/10, step 455/574 completed (loss: 0.23705309629440308, acc: 0.939393937587738)
[2024-12-12 03:14:29,645][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:29,897][root][INFO] - Training Epoch: 10/10, step 456/574 completed (loss: 0.4682925045490265, acc: 0.8453608155250549)
[2024-12-12 03:14:30,015][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:30,294][root][INFO] - Training Epoch: 10/10, step 457/574 completed (loss: 0.10165265947580338, acc: 0.9714285731315613)
[2024-12-12 03:14:30,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:30,674][root][INFO] - Training Epoch: 10/10, step 458/574 completed (loss: 0.5950546860694885, acc: 0.8255813717842102)
[2024-12-12 03:14:30,802][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:31,037][root][INFO] - Training Epoch: 10/10, step 459/574 completed (loss: 0.12209387868642807, acc: 0.9821428656578064)
[2024-12-12 03:14:31,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:31,355][root][INFO] - Training Epoch: 10/10, step 460/574 completed (loss: 0.29837486147880554, acc: 0.9135802388191223)
[2024-12-12 03:14:31,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:31,681][root][INFO] - Training Epoch: 10/10, step 461/574 completed (loss: 0.0632861778140068, acc: 0.9722222089767456)
[2024-12-12 03:14:31,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:32,021][root][INFO] - Training Epoch: 10/10, step 462/574 completed (loss: 0.37723252177238464, acc: 0.9375)
[2024-12-12 03:14:32,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:32,365][root][INFO] - Training Epoch: 10/10, step 463/574 completed (loss: 0.008351060561835766, acc: 1.0)
[2024-12-12 03:14:32,465][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:32,703][root][INFO] - Training Epoch: 10/10, step 464/574 completed (loss: 0.10550045967102051, acc: 0.95652174949646)
[2024-12-12 03:14:32,778][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:32,998][root][INFO] - Training Epoch: 10/10, step 465/574 completed (loss: 0.08433076739311218, acc: 0.976190447807312)
[2024-12-12 03:14:33,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:33,326][root][INFO] - Training Epoch: 10/10, step 466/574 completed (loss: 0.2719401717185974, acc: 0.9036144614219666)
[2024-12-12 03:14:33,438][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:33,699][root][INFO] - Training Epoch: 10/10, step 467/574 completed (loss: 0.23226629197597504, acc: 0.9459459185600281)
[2024-12-12 03:14:33,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:34,050][root][INFO] - Training Epoch: 10/10, step 468/574 completed (loss: 0.35551148653030396, acc: 0.9223300814628601)
[2024-12-12 03:14:34,154][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:34,409][root][INFO] - Training Epoch: 10/10, step 469/574 completed (loss: 0.30913713574409485, acc: 0.8943089246749878)
[2024-12-12 03:14:34,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:34,779][root][INFO] - Training Epoch: 10/10, step 470/574 completed (loss: 0.053762149065732956, acc: 0.9583333134651184)
[2024-12-12 03:14:34,897][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:35,164][root][INFO] - Training Epoch: 10/10, step 471/574 completed (loss: 0.024204950779676437, acc: 1.0)
[2024-12-12 03:14:35,305][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:35,564][root][INFO] - Training Epoch: 10/10, step 472/574 completed (loss: 0.6770222783088684, acc: 0.7647058963775635)
[2024-12-12 03:14:35,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:35,962][root][INFO] - Training Epoch: 10/10, step 473/574 completed (loss: 0.918842613697052, acc: 0.7379912734031677)
[2024-12-12 03:14:36,083][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:36,338][root][INFO] - Training Epoch: 10/10, step 474/574 completed (loss: 0.2675052583217621, acc: 0.9166666865348816)
[2024-12-12 03:14:36,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:36,716][root][INFO] - Training Epoch: 10/10, step 475/574 completed (loss: 0.4301530420780182, acc: 0.8834356069564819)
[2024-12-12 03:14:36,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:37,079][root][INFO] - Training Epoch: 10/10, step 476/574 completed (loss: 0.387869268655777, acc: 0.8705036044120789)
[2024-12-12 03:14:37,192][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:37,439][root][INFO] - Training Epoch: 10/10, step 477/574 completed (loss: 0.6763912439346313, acc: 0.8140703439712524)
[2024-12-12 03:14:37,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:37,803][root][INFO] - Training Epoch: 10/10, step 478/574 completed (loss: 0.05211286246776581, acc: 0.9722222089767456)
[2024-12-12 03:14:37,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:38,113][root][INFO] - Training Epoch: 10/10, step 479/574 completed (loss: 0.06080593541264534, acc: 1.0)
[2024-12-12 03:14:38,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:38,474][root][INFO] - Training Epoch: 10/10, step 480/574 completed (loss: 0.1174464151263237, acc: 0.9629629850387573)
[2024-12-12 03:14:38,594][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:38,853][root][INFO] - Training Epoch: 10/10, step 481/574 completed (loss: 0.013075652532279491, acc: 1.0)
[2024-12-12 03:14:38,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:39,182][root][INFO] - Training Epoch: 10/10, step 482/574 completed (loss: 0.16481032967567444, acc: 0.8999999761581421)
[2024-12-12 03:14:39,326][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:39,570][root][INFO] - Training Epoch: 10/10, step 483/574 completed (loss: 0.1582372933626175, acc: 0.9655172228813171)
[2024-12-12 03:14:39,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:39,884][root][INFO] - Training Epoch: 10/10, step 484/574 completed (loss: 0.02271156758069992, acc: 1.0)
[2024-12-12 03:14:39,958][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:40,210][root][INFO] - Training Epoch: 10/10, step 485/574 completed (loss: 0.09948694705963135, acc: 0.9473684430122375)
[2024-12-12 03:14:40,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:40,578][root][INFO] - Training Epoch: 10/10, step 486/574 completed (loss: 0.1735326498746872, acc: 0.9259259104728699)
[2024-12-12 03:14:40,683][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:40,951][root][INFO] - Training Epoch: 10/10, step 487/574 completed (loss: 0.4888879358768463, acc: 0.9523809552192688)
[2024-12-12 03:14:41,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:41,297][root][INFO] - Training Epoch: 10/10, step 488/574 completed (loss: 0.29293105006217957, acc: 0.9545454382896423)
[2024-12-12 03:14:41,402][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:41,630][root][INFO] - Training Epoch: 10/10, step 489/574 completed (loss: 0.17721468210220337, acc: 0.9538461565971375)
[2024-12-12 03:14:41,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:41,935][root][INFO] - Training Epoch: 10/10, step 490/574 completed (loss: 0.04391303285956383, acc: 1.0)
[2024-12-12 03:14:42,035][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:42,272][root][INFO] - Training Epoch: 10/10, step 491/574 completed (loss: 0.01014647725969553, acc: 1.0)
[2024-12-12 03:14:42,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:42,615][root][INFO] - Training Epoch: 10/10, step 492/574 completed (loss: 0.09137795865535736, acc: 0.9803921580314636)
[2024-12-12 03:14:42,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:42,995][root][INFO] - Training Epoch: 10/10, step 493/574 completed (loss: 0.03989729657769203, acc: 1.0)
[2024-12-12 03:14:43,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:43,366][root][INFO] - Training Epoch: 10/10, step 494/574 completed (loss: 0.02941477857530117, acc: 1.0)
[2024-12-12 03:14:43,471][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:43,691][root][INFO] - Training Epoch: 10/10, step 495/574 completed (loss: 0.10832462459802628, acc: 0.9473684430122375)
[2024-12-12 03:14:43,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:44,083][root][INFO] - Training Epoch: 10/10, step 496/574 completed (loss: 0.4848015308380127, acc: 0.875)
[2024-12-12 03:14:44,217][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:44,490][root][INFO] - Training Epoch: 10/10, step 497/574 completed (loss: 0.2665501832962036, acc: 0.898876428604126)
[2024-12-12 03:14:44,596][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:44,874][root][INFO] - Training Epoch: 10/10, step 498/574 completed (loss: 0.32835644483566284, acc: 0.9101123809814453)
[2024-12-12 03:14:44,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:45,279][root][INFO] - Training Epoch: 10/10, step 499/574 completed (loss: 0.8445613980293274, acc: 0.7517730593681335)
[2024-12-12 03:14:45,408][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:45,660][root][INFO] - Training Epoch: 10/10, step 500/574 completed (loss: 0.46512115001678467, acc: 0.9021739363670349)
[2024-12-12 03:14:45,787][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:46,033][root][INFO] - Training Epoch: 10/10, step 501/574 completed (loss: 0.002661564387381077, acc: 1.0)
[2024-12-12 03:14:46,132][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:46,361][root][INFO] - Training Epoch: 10/10, step 502/574 completed (loss: 0.0011193102691322565, acc: 1.0)
[2024-12-12 03:14:46,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:46,668][root][INFO] - Training Epoch: 10/10, step 503/574 completed (loss: 0.0485931932926178, acc: 0.9629629850387573)
[2024-12-12 03:14:46,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:46,976][root][INFO] - Training Epoch: 10/10, step 504/574 completed (loss: 0.01143725123256445, acc: 1.0)
[2024-12-12 03:14:47,052][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:47,301][root][INFO] - Training Epoch: 10/10, step 505/574 completed (loss: 0.08706049621105194, acc: 0.9811320900917053)
[2024-12-12 03:14:47,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:47,688][root][INFO] - Training Epoch: 10/10, step 506/574 completed (loss: 0.09723755717277527, acc: 0.9655172228813171)
[2024-12-12 03:14:47,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:48,279][root][INFO] - Training Epoch: 10/10, step 507/574 completed (loss: 0.6234229207038879, acc: 0.8288288116455078)
[2024-12-12 03:14:48,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:48,715][root][INFO] - Training Epoch: 10/10, step 508/574 completed (loss: 0.3044273555278778, acc: 0.8732394576072693)
[2024-12-12 03:14:48,795][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:49,018][root][INFO] - Training Epoch: 10/10, step 509/574 completed (loss: 0.0005572145455516875, acc: 1.0)
[2024-12-12 03:14:49,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:49,367][root][INFO] - Training Epoch: 10/10, step 510/574 completed (loss: 0.035429175943136215, acc: 0.9666666388511658)
[2024-12-12 03:14:49,470][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:49,705][root][INFO] - Training Epoch: 10/10, step 511/574 completed (loss: 0.014008878730237484, acc: 1.0)
[2024-12-12 03:14:51,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:52,466][root][INFO] - Training Epoch: 10/10, step 512/574 completed (loss: 1.0433049201965332, acc: 0.6714285612106323)
[2024-12-12 03:14:52,667][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:53,229][root][INFO] - Training Epoch: 10/10, step 513/574 completed (loss: 0.2603192627429962, acc: 0.9285714030265808)
[2024-12-12 03:14:53,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:53,556][root][INFO] - Training Epoch: 10/10, step 514/574 completed (loss: 0.3152158260345459, acc: 0.9285714030265808)
[2024-12-12 03:14:53,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:53,893][root][INFO] - Training Epoch: 10/10, step 515/574 completed (loss: 0.15065868198871613, acc: 0.9166666865348816)
[2024-12-12 03:14:54,068][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:54,583][root][INFO] - Training Epoch: 10/10, step 516/574 completed (loss: 0.3038727641105652, acc: 0.8888888955116272)
[2024-12-12 03:14:54,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:54,948][root][INFO] - Training Epoch: 10/10, step 517/574 completed (loss: 0.005591085180640221, acc: 1.0)
[2024-12-12 03:14:55,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:55,313][root][INFO] - Training Epoch: 10/10, step 518/574 completed (loss: 0.022864317521452904, acc: 1.0)
[2024-12-12 03:14:55,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:55,673][root][INFO] - Training Epoch: 10/10, step 519/574 completed (loss: 0.014936181716620922, acc: 1.0)
[2024-12-12 03:14:55,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:56,088][root][INFO] - Training Epoch: 10/10, step 520/574 completed (loss: 0.12369705736637115, acc: 0.9259259104728699)
[2024-12-12 03:14:56,337][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:57,076][root][INFO] - Training Epoch: 10/10, step 521/574 completed (loss: 1.2945291996002197, acc: 0.5847457647323608)
[2024-12-12 03:14:57,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:57,489][root][INFO] - Training Epoch: 10/10, step 522/574 completed (loss: 0.4799613356590271, acc: 0.8208954930305481)
[2024-12-12 03:14:57,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:57,880][root][INFO] - Training Epoch: 10/10, step 523/574 completed (loss: 0.49252453446388245, acc: 0.8613138794898987)
[2024-12-12 03:14:58,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:58,442][root][INFO] - Training Epoch: 10/10, step 524/574 completed (loss: 0.8739229440689087, acc: 0.7450000047683716)
[2024-12-12 03:14:58,543][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:58,833][root][INFO] - Training Epoch: 10/10, step 525/574 completed (loss: 0.07171331346035004, acc: 0.9814814925193787)
[2024-12-12 03:14:58,962][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:59,210][root][INFO] - Training Epoch: 10/10, step 526/574 completed (loss: 0.07363852113485336, acc: 1.0)
[2024-12-12 03:14:59,316][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:59,537][root][INFO] - Training Epoch: 10/10, step 527/574 completed (loss: 0.10987374931573868, acc: 0.9523809552192688)
[2024-12-12 03:14:59,663][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:14:59,903][root][INFO] - Training Epoch: 10/10, step 528/574 completed (loss: 0.21431945264339447, acc: 0.9180327653884888)
[2024-12-12 03:14:59,977][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:00,199][root][INFO] - Training Epoch: 10/10, step 529/574 completed (loss: 0.3459796905517578, acc: 0.8983050584793091)
[2024-12-12 03:15:00,275][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:00,503][root][INFO] - Training Epoch: 10/10, step 530/574 completed (loss: 0.46900150179862976, acc: 0.8837209343910217)
[2024-12-12 03:15:00,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:00,811][root][INFO] - Training Epoch: 10/10, step 531/574 completed (loss: 0.3090566098690033, acc: 0.9090909361839294)
[2024-12-12 03:15:00,898][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:01,133][root][INFO] - Training Epoch: 10/10, step 532/574 completed (loss: 0.23760543763637543, acc: 0.9245283007621765)
[2024-12-12 03:15:01,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:01,458][root][INFO] - Training Epoch: 10/10, step 533/574 completed (loss: 0.15308664739131927, acc: 0.9545454382896423)
[2024-12-12 03:15:01,523][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:01,746][root][INFO] - Training Epoch: 10/10, step 534/574 completed (loss: 0.03188004344701767, acc: 1.0)
[2024-12-12 03:15:01,854][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:02,087][root][INFO] - Training Epoch: 10/10, step 535/574 completed (loss: 0.011519080959260464, acc: 1.0)
[2024-12-12 03:15:02,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:02,431][root][INFO] - Training Epoch: 10/10, step 536/574 completed (loss: 0.013617322780191898, acc: 1.0)
[2024-12-12 03:15:02,547][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:02,824][root][INFO] - Training Epoch: 10/10, step 537/574 completed (loss: 0.23465408384799957, acc: 0.9384615421295166)
[2024-12-12 03:15:02,940][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:03,215][root][INFO] - Training Epoch: 10/10, step 538/574 completed (loss: 0.122113898396492, acc: 0.953125)
[2024-12-12 03:15:03,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:03,607][root][INFO] - Training Epoch: 10/10, step 539/574 completed (loss: 0.06400787085294724, acc: 0.96875)
[2024-12-12 03:15:03,715][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:03,951][root][INFO] - Training Epoch: 10/10, step 540/574 completed (loss: 0.16278541088104248, acc: 0.9696969985961914)
[2024-12-12 03:15:04,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:04,304][root][INFO] - Training Epoch: 10/10, step 541/574 completed (loss: 0.09444046765565872, acc: 0.9375)
[2024-12-12 03:15:04,394][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:04,642][root][INFO] - Training Epoch: 10/10, step 542/574 completed (loss: 0.02402670867741108, acc: 1.0)
[2024-12-12 03:15:04,768][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:05,018][root][INFO] - Training Epoch: 10/10, step 543/574 completed (loss: 0.05102359876036644, acc: 0.95652174949646)
[2024-12-12 03:15:05,136][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:05,376][root][INFO] - Training Epoch: 10/10, step 544/574 completed (loss: 0.13416168093681335, acc: 0.9333333373069763)
[2024-12-12 03:15:05,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:05,759][root][INFO] - Training Epoch: 10/10, step 545/574 completed (loss: 0.06541519612073898, acc: 0.9756097793579102)
[2024-12-12 03:15:05,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:06,093][root][INFO] - Training Epoch: 10/10, step 546/574 completed (loss: 0.013913814909756184, acc: 1.0)
[2024-12-12 03:15:06,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:06,417][root][INFO] - Training Epoch: 10/10, step 547/574 completed (loss: 0.08208499103784561, acc: 0.9473684430122375)
[2024-12-12 03:15:06,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:06,695][root][INFO] - Training Epoch: 10/10, step 548/574 completed (loss: 0.03465734422206879, acc: 0.9677419066429138)
[2024-12-12 03:15:06,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:07,050][root][INFO] - Training Epoch: 10/10, step 549/574 completed (loss: 0.0007759071886539459, acc: 1.0)
[2024-12-12 03:15:07,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:07,409][root][INFO] - Training Epoch: 10/10, step 550/574 completed (loss: 0.2076168805360794, acc: 0.8787878751754761)
[2024-12-12 03:15:07,512][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:07,745][root][INFO] - Training Epoch: 10/10, step 551/574 completed (loss: 0.3298960030078888, acc: 0.8999999761581421)
[2024-12-12 03:15:07,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:08,094][root][INFO] - Training Epoch: 10/10, step 552/574 completed (loss: 0.03613543510437012, acc: 1.0)
[2024-12-12 03:15:08,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:08,430][root][INFO] - Training Epoch: 10/10, step 553/574 completed (loss: 0.45488449931144714, acc: 0.8467153310775757)
[2024-12-12 03:15:09,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:09,556][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:09,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:10,102][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:10,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:10,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:11,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:11,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:11,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:12,080][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:12,442][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:12,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:13,135][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:13,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:13,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:14,211][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:14,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:14,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:15,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:15,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:16,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:16,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:16,688][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:16,969][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:17,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:17,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:18,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:18,300][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:18,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:18,963][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:19,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:19,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:19,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:20,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:20,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:20,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:21,212][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:21,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:21,923][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:22,223][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:22,549][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:22,889][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:23,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:23,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:23,905][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:24,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:24,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:24,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:25,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:25,376][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:25,655][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:25,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:26,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:26,624][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:26,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:27,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:27,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:28,100][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:28,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:28,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:29,076][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:29,482][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:29,957][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:30,262][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:30,603][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:30,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:31,263][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:31,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:31,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:32,404][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:32,755][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:33,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:33,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:33,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:34,143][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:34,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:34,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:35,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:35,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:35,805][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:36,208][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:36,571][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:36,909][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:37,206][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:37,514][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:38,066][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(4.7170, device='cuda:0') eval_epoch_loss=tensor(1.5512, device='cuda:0') eval_epoch_acc=tensor(0.6950, device='cuda:0')
[2024-12-12 03:15:38,067][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-12-12 03:15:38,068][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-12-12 03:15:38,263][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4/asr_epoch_10_step_554_loss_1.5511672496795654/model.pt
[2024-12-12 03:15:38,266][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_ds_rate_4 directory
[2024-12-12 03:15:38,346][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:38,660][root][INFO] - Training Epoch: 10/10, step 554/574 completed (loss: 0.317771852016449, acc: 0.9103448390960693)
[2024-12-12 03:15:38,782][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:39,040][root][INFO] - Training Epoch: 10/10, step 555/574 completed (loss: 0.37189650535583496, acc: 0.9071428775787354)
[2024-12-12 03:15:39,147][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:39,375][root][INFO] - Training Epoch: 10/10, step 556/574 completed (loss: 0.5661757588386536, acc: 0.8344370722770691)
[2024-12-12 03:15:39,481][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:39,713][root][INFO] - Training Epoch: 10/10, step 557/574 completed (loss: 0.14850623905658722, acc: 0.9572649598121643)
[2024-12-12 03:15:39,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:40,056][root][INFO] - Training Epoch: 10/10, step 558/574 completed (loss: 0.0034831517841666937, acc: 1.0)
[2024-12-12 03:15:40,140][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:40,358][root][INFO] - Training Epoch: 10/10, step 559/574 completed (loss: 0.023916838690638542, acc: 1.0)
[2024-12-12 03:15:40,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:40,719][root][INFO] - Training Epoch: 10/10, step 560/574 completed (loss: 0.005937004461884499, acc: 1.0)
[2024-12-12 03:15:40,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:41,112][root][INFO] - Training Epoch: 10/10, step 561/574 completed (loss: 0.013783602975308895, acc: 1.0)
[2024-12-12 03:15:41,245][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:41,502][root][INFO] - Training Epoch: 10/10, step 562/574 completed (loss: 0.24738295376300812, acc: 0.9111111164093018)
[2024-12-12 03:15:41,606][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:41,832][root][INFO] - Training Epoch: 10/10, step 563/574 completed (loss: 0.07374551147222519, acc: 0.9740259647369385)
[2024-12-12 03:15:41,910][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:42,176][root][INFO] - Training Epoch: 10/10, step 564/574 completed (loss: 0.03311920911073685, acc: 1.0)
[2024-12-12 03:15:42,288][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:42,541][root][INFO] - Training Epoch: 10/10, step 565/574 completed (loss: 0.23109066486358643, acc: 0.931034505367279)
[2024-12-12 03:15:42,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:42,903][root][INFO] - Training Epoch: 10/10, step 566/574 completed (loss: 0.3511768877506256, acc: 0.9166666865348816)
[2024-12-12 03:15:43,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:43,278][root][INFO] - Training Epoch: 10/10, step 567/574 completed (loss: 0.1863882839679718, acc: 0.9736841917037964)
[2024-12-12 03:15:43,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:43,595][root][INFO] - Training Epoch: 10/10, step 568/574 completed (loss: 0.007006845436990261, acc: 1.0)
[2024-12-12 03:15:43,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:43,975][root][INFO] - Training Epoch: 10/10, step 569/574 completed (loss: 0.591105043888092, acc: 0.8235294222831726)
[2024-12-12 03:15:44,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:44,336][root][INFO] - Training Epoch: 10/10, step 570/574 completed (loss: 0.10812095552682877, acc: 0.9677419066429138)
[2024-12-12 03:15:44,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:44,724][root][INFO] - Training Epoch: 10/10, step 571/574 completed (loss: 0.09857191145420074, acc: 0.9743589758872986)
[2024-12-12 03:15:44,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:45,055][root][INFO] - Training Epoch: 10/10, step 572/574 completed (loss: 0.5567271709442139, acc: 0.8469387888908386)
[2024-12-12 03:15:45,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-12 03:15:45,395][root][INFO] - Training Epoch: 10/10, step 573/574 completed (loss: 0.49213942885398865, acc: 0.8427672982215881)
[2024-12-12 03:15:45,786][slam_llm.utils.train_utils][INFO] - Epoch 10: train_perplexity=1.3020, train_epoch_loss=0.2639, epoch time 353.8316517993808s
[2024-12-12 03:15:45,787][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2024-12-12 03:15:45,787][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2024-12-12 03:15:45,787][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2024-12-12 03:15:45,787][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 28
[2024-12-12 03:15:45,787][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-12-12 03:15:45,791][root][INFO] - Key: avg_train_prep, Value: 4.204151630401611
[2024-12-12 03:15:45,793][root][INFO] - Key: avg_train_loss, Value: 1.054636001586914
[2024-12-12 03:15:45,793][root][INFO] - Key: avg_train_acc, Value: 0.7246569395065308
[2024-12-12 03:15:45,793][root][INFO] - Key: avg_eval_prep, Value: 5.7936930656433105
[2024-12-12 03:15:45,793][root][INFO] - Key: avg_eval_loss, Value: 1.6779638528823853
[2024-12-12 03:15:45,793][root][INFO] - Key: avg_eval_acc, Value: 0.6010887026786804
[2024-12-12 03:15:45,793][root][INFO] - Key: avg_epoch_time, Value: 356.72442822530866
[2024-12-12 03:15:45,794][root][INFO] - Key: avg_checkpoint_time, Value: 0.2325395175255835
