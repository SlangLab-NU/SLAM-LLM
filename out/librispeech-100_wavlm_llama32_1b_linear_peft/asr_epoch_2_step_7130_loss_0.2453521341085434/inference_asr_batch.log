[2024-12-17 03:37:39,369][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2024-12-17 03:37:39,369][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-17 03:37:39,369][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-17 03:37:40,872][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-17 03:37:46,682][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-17 03:37:46,684][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-17 03:37:46,686][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-17 03:37:46,687][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-17 03:37:51,163][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-17 03:37:51,164][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-17 03:37:51,165][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-17 03:37:51,284][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-17 03:37:51,286][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-17 03:37:51,390][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-17 03:37:51,390][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-12-17 03:37:51,390][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.2453521341085434/model.pt
[2024-12-17 03:37:51,494][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-17 03:37:51,498][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-12-17 03:37:53,282][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-17 03:37:54,017][root][INFO] - --> Training Set Length = 2620
[2024-12-17 03:37:54,018][root][INFO] - =====================================
[2024-12-17 03:37:55,760][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:37:58,929][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:00,742][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:02,368][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:04,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:06,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:08,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:09,996][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:11,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:12,912][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:14,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:15,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:16,964][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:19,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:21,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:22,830][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:24,290][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:25,348][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:26,203][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:26,961][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:28,981][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:31,096][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:33,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:35,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:36,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:38,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:39,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:42,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:44,157][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:46,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:48,079][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:49,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:51,085][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:53,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:54,849][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:38:57,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:00,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:01,229][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:02,843][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:05,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:07,775][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:09,982][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:13,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:18,120][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:20,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:23,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:25,453][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:26,122][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:27,251][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:28,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:29,999][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:31,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:33,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:34,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:35,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:36,799][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:37,853][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:39,216][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:40,382][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:41,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:43,570][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:45,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:47,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:48,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:49,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:51,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:52,708][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:55,214][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:39:57,609][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:00,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:02,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:03,700][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:05,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:06,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:09,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:10,921][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:12,569][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:15,636][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:18,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:20,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:24,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:26,844][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:27,480][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:28,474][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:29,439][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:31,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:33,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:36,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:38,150][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:38,821][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:39,869][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:40,848][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:42,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:43,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:45,129][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:47,044][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:48,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:50,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:51,931][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:54,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:55,738][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:40:57,156][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:01,258][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:03,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:07,070][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:08,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:09,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:12,265][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:13,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:16,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:17,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:18,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:19,680][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:21,403][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:22,800][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:24,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:25,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:28,840][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:31,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:32,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:33,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:35,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:37,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:39,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:41,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:44,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:46,759][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:49,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:52,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:53,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:55,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:41:58,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:00,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:01,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:03,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:04,886][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:08,354][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:10,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:13,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:14,323][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:15,384][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:16,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:19,477][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:25,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:26,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:27,817][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:29,051][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:30,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:32,175][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:33,497][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:34,398][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:35,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:36,411][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:37,694][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:39,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:42,236][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:44,011][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:46,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:48,109][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:49,648][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:52,276][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:55,590][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:57,608][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:42:59,850][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:03,026][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:05,983][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:08,690][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:10,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:10,880][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:13,815][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:15,414][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:16,729][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:19,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:19,819][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:21,130][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:22,673][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:24,271][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:26,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:28,702][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:30,808][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:32,506][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:35,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:36,615][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:38,524][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:40,041][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:43,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:44,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:48,149][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:49,578][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:50,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:51,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:52,762][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:53,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:55,347][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:56,537][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:58,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:43:59,622][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:00,945][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:02,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:04,601][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:05,425][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:07,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:08,720][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:10,418][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:11,928][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:13,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:15,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:17,182][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:18,525][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:19,722][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:21,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:22,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:24,623][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:26,950][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:28,413][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:30,938][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:32,445][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:35,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:38,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:39,595][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:42,416][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:44,003][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:48,189][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:51,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:44:53,748][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:00,318][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:01,023][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:02,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:02,987][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:03,824][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:04,754][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:05,736][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:06,484][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:08,171][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:09,264][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:10,078][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:11,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:12,019][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:13,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:14,224][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:15,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:16,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:17,195][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:18,071][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:20,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:21,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:23,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:24,621][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:26,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:29,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:30,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:32,600][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:34,104][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:36,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:37,099][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:38,734][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:40,654][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:42,046][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:43,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:45,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:46,467][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:47,934][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:48,717][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:50,237][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:51,345][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:52,064][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:53,133][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:54,151][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:55,285][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:56,635][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:57,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:58,616][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:45:59,385][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:00,867][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:02,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:03,507][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:04,691][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:05,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:06,846][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:07,788][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:10,123][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:12,502][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:14,685][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:16,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:17,306][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:19,794][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:21,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:22,533][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:24,646][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:26,105][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:27,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:28,812][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:30,074][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:32,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:34,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:35,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:36,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:37,828][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:39,387][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:40,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:42,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:44,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:45,420][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:46,392][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:47,434][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:48,231][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:49,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:50,669][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:52,097][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:53,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:54,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:56,324][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:46:58,771][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:01,735][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:04,607][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:07,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:10,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:12,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:14,410][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:16,599][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:20,056][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:23,580][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:26,370][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:28,625][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:30,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:33,108][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:35,468][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:37,783][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:40,059][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:42,092][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:44,831][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:46,386][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:48,278][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:52,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:53,741][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:55,649][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:58,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:47:59,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:00,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:02,153][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:04,235][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:05,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:08,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:10,055][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:11,832][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:14,560][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:18,016][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:19,485][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:20,552][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:23,528][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:25,914][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:29,022][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:30,723][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:32,284][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:34,137][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:36,177][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:37,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:39,283][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:40,239][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:41,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:43,943][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:46,490][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:48,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:51,193][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:54,111][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:48:57,699][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:01,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:04,845][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:06,856][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:07,501][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:09,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:10,517][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:12,661][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:15,344][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:18,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:21,118][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:24,163][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:25,181][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:26,716][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:27,550][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:31,508][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:32,565][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:33,499][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:35,421][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:41,672][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:45,301][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:46,361][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:47,495][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:49,906][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:51,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:52,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:53,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:54,452][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:56,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:58,429][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:49:59,400][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:00,180][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:01,548][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:02,796][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:04,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:06,859][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:08,617][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:12,124][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:15,178][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:17,582][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:22,030][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:23,890][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:26,380][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:28,630][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:30,644][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:32,112][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:33,298][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:35,776][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:37,431][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:39,873][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:43,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:46,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:50,007][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:53,450][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:54,342][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:56,883][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:50:59,261][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:00,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:01,494][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:05,378][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:06,727][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:08,659][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:11,491][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:13,268][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:15,422][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:18,025][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:20,228][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:21,922][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:23,769][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:25,329][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:26,652][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:28,082][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:29,994][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:32,036][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:34,363][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:36,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:37,335][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:38,350][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:39,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:42,167][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:44,993][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:48,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:50,567][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:52,365][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:55,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:57,924][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:51:59,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:02,328][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:04,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:06,955][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:09,086][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:11,456][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:13,857][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:16,126][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:18,712][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:20,813][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:23,299][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:26,930][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:30,498][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:31,733][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:34,012][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:35,638][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:37,045][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:37,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:38,954][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:40,168][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:41,520][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:42,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:43,687][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:44,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:45,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:46,259][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:47,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:48,747][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:51,053][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:53,463][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:55,248][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:56,953][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:58,190][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:52:59,081][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:00,307][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:01,628][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:02,633][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:04,186][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:05,793][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:07,033][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:08,257][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:09,031][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:09,979][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:10,837][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:12,459][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:13,701][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:14,927][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:16,119][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:17,222][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:18,117][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:18,894][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:20,294][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:21,373][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:22,433][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:23,424][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:24,063][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:25,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:26,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:27,809][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:29,010][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:30,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:31,371][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:33,230][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:35,423][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:37,246][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:39,516][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:40,160][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:41,256][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:44,204][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:45,559][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:47,591][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:49,558][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:50,619][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:52,302][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:56,568][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:53:59,332][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:01,207][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:02,732][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:04,176][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:07,913][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:09,115][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:11,576][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:13,159][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:14,872][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:16,049][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:17,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:18,743][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:20,067][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:23,058][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:25,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:27,766][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:29,241][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:30,970][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:33,107][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:35,254][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:36,024][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:36,833][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:38,198][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:39,432][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:40,401][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:41,415][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:42,651][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:43,593][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:44,991][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:45,765][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:47,002][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:47,902][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:49,095][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:50,620][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:51,479][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:52,629][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:54,292][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:55,172][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:56,138][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:57,489][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:58,417][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:54:59,829][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:01,048][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:02,034][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:03,274][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:05,304][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:07,751][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:09,050][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:11,366][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:12,714][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:14,103][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:16,199][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:18,270][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:21,612][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:23,971][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:26,166][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:28,427][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:30,539][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:33,627][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:35,319][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:38,091][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:39,029][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:39,785][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:40,584][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:42,534][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:44,967][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:45,935][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:46,750][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:47,466][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:49,169][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:50,273][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:51,588][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:53,314][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:54,780][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:55,718][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:56,642][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:55:58,730][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:00,455][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:01,847][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:03,179][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:04,781][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:06,353][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:08,219][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:10,069][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:12,562][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:14,437][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:16,500][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:18,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:21,057][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:23,215][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:26,174][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:27,493][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:29,942][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:33,952][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:35,839][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:37,202][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:38,310][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:39,842][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:42,772][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:43,786][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:45,336][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:46,989][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:48,161][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:49,885][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:51,811][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:52,932][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:53,658][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:54,864][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:55,752][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:57,879][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:56:59,544][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:02,240][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:03,753][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:04,598][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:05,689][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:06,527][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:07,436][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:08,660][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:12,218][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:13,626][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:14,904][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:15,789][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:16,825][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:17,460][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:19,444][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:21,692][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:23,706][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:24,900][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-17 03:57:26,823][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-18 22:22:39,910][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': True}
[2024-12-18 22:22:39,912][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-18 22:22:39,922][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-18 22:22:42,492][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-18 22:22:49,449][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-18 22:22:49,456][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-18 22:22:49,459][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-18 22:22:49,461][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-18 22:22:58,591][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-18 22:22:58,599][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-18 22:22:58,602][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-18 22:22:58,819][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-18 22:22:58,822][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-18 22:22:59,034][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-18 22:22:59,034][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-12-18 22:22:59,034][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.2453521341085434/model.pt
[2024-12-18 22:22:59,468][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-18 22:22:59,473][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-12-18 22:22:59,503][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/test_small.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-18 22:23:02,096][root][INFO] - --> Training Set Length = 1
[2024-12-18 22:23:02,096][root][INFO] - =====================================
[2024-12-18 22:23:17,573][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-19 19:13:07,678][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': True}
[2024-12-19 19:13:07,678][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-19 19:13:07,678][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-19 19:13:09,202][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-19 19:13:15,122][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-19 19:13:15,125][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-19 19:13:15,130][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-19 19:13:15,137][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-19 19:13:21,228][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-19 19:13:21,231][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-19 19:13:21,233][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-19 19:13:21,376][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-19 19:13:21,379][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-19 19:23:33,404][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': True}
[2024-12-19 19:23:33,404][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-19 19:23:33,409][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-19 19:23:35,123][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-19 19:23:41,080][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-19 19:23:41,083][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-19 19:23:41,091][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-19 19:23:41,095][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-19 19:23:46,225][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-19 19:23:46,228][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-19 19:23:46,234][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-19 19:23:46,377][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-19 19:23:46,379][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-19 19:23:46,505][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-19 19:23:46,506][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-12-19 19:23:46,506][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.2453521341085434/model.pt
[2024-12-19 19:23:46,750][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-19 19:23:46,756][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-12-19 19:23:46,774][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/test_small.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-19 19:23:48,689][root][INFO] - --> Training Set Length = 1
[2024-12-19 19:23:48,691][root][INFO] - =====================================
[2024-12-19 19:24:02,039][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-24 00:06:30,949][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': True}
[2024-12-24 00:06:30,949][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-24 00:06:30,949][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-12-24 00:06:32,952][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-24 00:06:40,007][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-24 00:06:40,010][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-24 00:06:40,019][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-24 00:06:40,021][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-24 00:06:46,419][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-24 00:06:46,421][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-24 00:06:46,428][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-24 00:06:46,598][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-24 00:06:46,601][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-24 00:06:46,733][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-24 00:06:46,734][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-12-24 00:06:46,734][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.2453521341085434/model.pt
[2024-12-24 00:06:46,948][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-24 00:06:46,953][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-12-24 00:06:46,978][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/test_small.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-24 00:06:49,258][root][INFO] - --> Training Set Length = 2
[2024-12-24 00:06:49,259][root][INFO] - =====================================
[2024-12-24 00:07:18,798][slam_llm.models.slam_model][INFO] - modality encoder
[2024-12-29 23:13:41,972][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': True}
[2024-12-29 23:13:41,972][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-12-29 23:13:41,972][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'librispeech-100_wavlm_llama32_1b_linear_peft'}
[2024-12-29 23:13:43,317][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-12-29 23:13:49,612][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-29 23:13:49,614][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-12-29 23:13:49,619][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-12-29 23:13:49,620][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-12-29 23:13:53,939][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-29 23:13:53,940][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-12-29 23:13:53,944][slam_llm.models.slam_model][INFO] - setup peft...
[2024-12-29 23:13:54,090][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-12-29 23:13:54,092][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-12-29 23:13:54,215][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-12-29 23:13:54,216][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-12-29 23:13:54,216][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.2453521341085434/model.pt
[2024-12-29 23:13:54,654][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-12-29 23:13:54,660][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2024-12-29 23:13:54,681][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/test_small.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-12-29 23:13:55,345][root][INFO] - --> Training Set Length = 2
[2024-12-29 23:13:55,346][root][INFO] - =====================================
[2024-12-29 23:14:22,837][slam_llm.models.slam_model][INFO] - modality encoder
