{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test.jsonl:  38%|███▊      | 4603/12232 [01:27<02:25, 52.57it/s]\n",
      "Processing Files:   0%|          | 0/3 [01:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m input_folder \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Update this to your input folder path\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m# Process all JSON files\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m process_json_files(input_folder)\n",
      "Cell \u001b[0;32mIn[1], line 56\u001b[0m, in \u001b[0;36mprocess_json_files\u001b[0;34m(input_folder)\u001b[0m\n\u001b[1;32m     54\u001b[0m target_sentence \u001b[39m=\u001b[39m data_dict\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Get the target (transcription) sentence\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m target_sentence:\n\u001b[0;32m---> 56\u001b[0m     phoneme_transcript \u001b[39m=\u001b[39m get_phonemes(target_sentence)  \u001b[39m# Get phoneme transcript\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     data_dict[\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m phoneme_transcript  \u001b[39m# Update the 'target' field\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# Update the 'prompt' field\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mget_phonemes\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_phonemes\u001b[39m(sentence):\n\u001b[1;32m     23\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convert sentence to phonemes using G2P and clean the result.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     phonemes_list \u001b[39m=\u001b[39m [transducer(word)\u001b[39m.\u001b[39moutput_string \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m re\u001b[39m.\u001b[39mfindall(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+\u001b[39m\u001b[39m'\u001b[39m, sentence)]\n\u001b[1;32m     25\u001b[0m     phonemes \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(phonemes_list)\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m clean_phonemes(phonemes)\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_phonemes\u001b[39m(sentence):\n\u001b[1;32m     23\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convert sentence to phonemes using G2P and clean the result.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     phonemes_list \u001b[39m=\u001b[39m [transducer(word)\u001b[39m.\u001b[39moutput_string \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m re\u001b[39m.\u001b[39mfindall(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+\u001b[39m\u001b[39m'\u001b[39m, sentence)]\n\u001b[1;32m     25\u001b[0m     phonemes \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(phonemes_list)\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m clean_phonemes(phonemes)\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/g2p/transducer/__init__.py:1224\u001b[0m, in \u001b[0;36mTokenizingTransducer.__call__\u001b[0;34m(self, to_convert)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[39mif\u001b[39;00m token[\u001b[39m\"\u001b[39m\u001b[39mis_word\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m   1223\u001b[0m     word_tg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transducer(token[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 1224\u001b[0m     tg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m word_tg\n\u001b[1;32m   1225\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1226\u001b[0m     non_word_tg \u001b[39m=\u001b[39m TransductionGraph(token[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/g2p/transducer/__init__.py:1117\u001b[0m, in \u001b[0;36mCompositeTransductionGraph.__iadd__\u001b[0;34m(self, tg)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iadd__\u001b[39m(\u001b[39mself\u001b[39m, tg):\n\u001b[0;32m-> 1117\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mappend(tg)\n\u001b[1;32m   1118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/g2p/transducer/__init__.py:1110\u001b[0m, in \u001b[0;36mCompositeTransductionGraph.append\u001b[0;34m(self, tg)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tiers) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(tg\u001b[39m.\u001b[39m_tiers)\n\u001b[1;32m   1109\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tiers)):\n\u001b[0;32m-> 1110\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tiers[i]\u001b[39m.\u001b[39mappend(copy\u001b[39m.\u001b[39;49mdeepcopy(tg\u001b[39m.\u001b[39;49mtiers[i]))\n\u001b[1;32m   1111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1112\u001b[0m     \u001b[39mfor\u001b[39;00m tier \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tiers:\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/copy.py:206\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    204\u001b[0m append \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mappend\n\u001b[1;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m x:\n\u001b[0;32m--> 206\u001b[0m     append(deepcopy(a, memo))\n\u001b[1;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/copy.py:211\u001b[0m, in \u001b[0;36m_deepcopy_tuple\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_deepcopy_tuple\u001b[39m(x, memo, deepcopy\u001b[39m=\u001b[39mdeepcopy):\n\u001b[0;32m--> 211\u001b[0m     y \u001b[39m=\u001b[39m [deepcopy(a, memo) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m x]\n\u001b[1;32m    212\u001b[0m     \u001b[39m# We're not going to put the tuple in the memo, but it's still important we\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[39m# check for it, in case the tuple contains recursive mutable structures.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/copy.py:211\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_deepcopy_tuple\u001b[39m(x, memo, deepcopy\u001b[39m=\u001b[39mdeepcopy):\n\u001b[0;32m--> 211\u001b[0m     y \u001b[39m=\u001b[39m [deepcopy(a, memo) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m x]\n\u001b[1;32m    212\u001b[0m     \u001b[39m# We're not going to put the tuple in the memo, but it's still important we\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[39m# check for it, in case the tuple contains recursive mutable structures.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from g2p import make_g2p\n",
    "\n",
    "# Initialize G2P converter for English ARPAbet\n",
    "transducer = make_g2p('eng', 'eng-arpabet')\n",
    "\n",
    "def clean_phonemes(phonemes):\n",
    "    \"\"\"\n",
    "    Cleans the phonemes string by:\n",
    "    1. Removing special characters like \"'\" and '\"'.\n",
    "    2. Removing trailing and multiple spaces.\n",
    "    \"\"\"\n",
    "    # Remove special characters\n",
    "    cleaned = re.sub(r\"[^\\w\\s]\", \"\", phonemes)\n",
    "    # Remove extra spaces\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "def get_phonemes(sentence):\n",
    "    \"\"\"Convert sentence to phonemes using G2P and clean the result.\"\"\"\n",
    "    phonemes_list = [transducer(word).output_string for word in re.findall(r'\\S+', sentence)]\n",
    "    phonemes = \" \".join(phonemes_list)\n",
    "    return clean_phonemes(phonemes)  # Clean the phoneme transcript\n",
    "\n",
    "def process_json_files(input_folder):\n",
    "    \"\"\"Process all JSON files in the input folder and save the corresponding updated JSON files to the output folder.\"\"\"\n",
    "    # Dynamically create the output folder name by appending '_phoneme'\n",
    "    output_folder = f\"{input_folder.rstrip(os.sep)}_phoneme\"\n",
    "    \n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through all files in the input folder\n",
    "    for file_name in tqdm(os.listdir(input_folder), desc=\"Processing Files\"):\n",
    "        if file_name.endswith('.jsonl'):  # Only process .jsonl files\n",
    "            input_file_path = os.path.join(input_folder, file_name)\n",
    "            output_file_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "            # Open the input JSON file\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                data = file.readlines()\n",
    "\n",
    "            # Initialize list to store processed data\n",
    "            updated_data = []\n",
    "\n",
    "            # Iterate through each JSON line, process and update with phoneme transcript\n",
    "            for line in tqdm(data, desc=f\"Processing {file_name}\"):\n",
    "                data_dict = json.loads(line.strip())  # Load the JSON object\n",
    "\n",
    "                # Update the 'target' field with the phoneme transcription\n",
    "                target_sentence = data_dict.get('target', '')  # Get the target (transcription) sentence\n",
    "                if target_sentence:\n",
    "                    phoneme_transcript = get_phonemes(target_sentence)  # Get phoneme transcript\n",
    "                    data_dict['target'] = phoneme_transcript  # Update the 'target' field\n",
    "\n",
    "                # Update the 'prompt' field\n",
    "                data_dict['prompt'] = (\n",
    "                    \"Transcribe speech to Phonemes. Output the transcription directly without redundant content. \"\n",
    "                    \"Ensure that the output is not duplicated.\"\n",
    "                )\n",
    "\n",
    "                updated_data.append(data_dict)  # Store the updated object\n",
    "\n",
    "            # Save the updated data back to the corresponding output JSON file\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                for updated_record in updated_data:\n",
    "                    json.dump(updated_record, output_file)\n",
    "                    output_file.write('\\n')\n",
    "\n",
    "# Define input folder\n",
    "input_folder = \"/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia\"  # Update this to your input folder path\n",
    "\n",
    "# Process all JSON files\n",
    "process_json_files(input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Background processes not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[39m.\u001b[39;49msystem(\u001b[39m'\u001b[39;49m\u001b[39mfilename=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m08_convert_phoneme\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m && nohup python \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m$\u001b[39;49m\u001b[39m{filename}\u001b[39;49;00m\u001b[39m.py\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m > \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m$\u001b[39;49m\u001b[39m{filename}\u001b[39;49;00m\u001b[39m.log\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m 2>&1 &\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/ipykernel/zmqshell.py:641\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39mif\u001b[39;00m cmd\u001b[39m.\u001b[39mrstrip()\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m&\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    635\u001b[0m     \u001b[39m# this is *far* from a rigorous test\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[39m# We do not support backgrounding processes because we either use\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[39m# pexpect or pipes to read from.  Users can always just call\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     \u001b[39m# os.system() or use ip.system=ip.system_raw\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     \u001b[39m# if they really want a background process.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBackground processes not supported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[1;32m    643\u001b[0m \u001b[39m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[39m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[39m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[39m# Also, protect system call from UNC paths on Windows here too\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[39m# as is done in InteractiveShell.system_raw\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mplatform \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwin32\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: Background processes not supported."
     ]
    }
   ],
   "source": [
    "!filename=\"08_convert_phoneme\" && nohup python \"${filename}.py\" > \"${filename}.log\" 2>&1 &"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
