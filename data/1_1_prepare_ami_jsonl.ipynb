{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"edinburghcstr/ami\", \"ihm\", \n",
    "    cache_dir='/work/van-speech-nlp/temp',\n",
    "    use_auth_token='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'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meeting_id': 'EN2001a',\n",
       " 'audio_id': 'AMI_EN2001a_H04_MEO069_0330297_0330718',\n",
       " 'text': 'IF YOU IF YOU S. S. H. AND THEY HAVE THIS BIG WARNING ABOUT DOING NOTHING AT ALL IN THE GATEWAY MACHINE',\n",
       " 'audio': {'path': '/work/van-speech-nlp/temp/downloads/extracted/e3fafc840ba9066f036441b51a5b9abf81cefeea4d4e326ecc8968181644b826/EN2001a/train_ami_en2001a_h04_meo069_0330297_0330718.wav',\n",
       "  'array': array([ 0.00231934, -0.00183105, -0.00543213, ..., -0.00238037,\n",
       "         -0.00244141, -0.00219727]),\n",
       "  'sampling_rate': 16000},\n",
       " 'begin_time': 3302.969970703125,\n",
       " 'end_time': 3307.179931640625,\n",
       " 'microphone_id': 'H04',\n",
       " 'speaker_id': 'MEO069'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e4150b73a44c3dab34dfd82dc0e696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af07192c5c7d4dee97e49fe186c86659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13098 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4cd53d95be4a6eaad9ff0059938a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12643 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e78bf2e20544899ef5b6032ed74354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/108502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f3e5868a934ac59b651d89d2b425c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/13098 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfc6cfe1b4e4d62a320db4c63c3254d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12643 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Audio\n",
    "import logging\n",
    "\n",
    "# Load the AMI dataset\n",
    "dataset = load_dataset(\n",
    "    \"edinburghcstr/ami\", \"ihm\", \n",
    "    cache_dir='/work/van-speech-nlp/temp',\n",
    "    use_auth_token='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'\n",
    ")\n",
    "\n",
    "# Define the min and max input lengths in seconds\n",
    "min_input_length_in_sec = 1.0\n",
    "max_input_length_in_sec = 10.0\n",
    "\n",
    "# Calculate input_length as the difference between end_time and begin_time\n",
    "dataset = dataset.map(\n",
    "    lambda x: {'input_length': x['end_time'] - x['begin_time']}\n",
    ")\n",
    "\n",
    "# Filter audio samples based on the calculated input_length\n",
    "dataset = dataset.filter(\n",
    "    lambda x: min_input_length_in_sec < x['input_length'] < max_input_length_in_sec\n",
    ")\n",
    "\n",
    "# Log the number of data points after filtering\n",
    "logging.info(\n",
    "    \"After filtering audio within a certain length, the number of data in each dataset is:\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:       66698/66698 (100%)\n",
      "Validation:  8351/8351 (100%)\n"
     ]
    }
   ],
   "source": [
    "# Assuming the dataset is split into train/validation sets, log each count\n",
    "if 'train' in dataset:\n",
    "    original_data_count_train = len(dataset['train'])\n",
    "    print(f'Train:       {len(dataset[\"train\"])}/{original_data_count_train} ({len(dataset[\"train\"]) * 100 // original_data_count_train}%)')\n",
    "else:\n",
    "    print(f'Train:       0/0 (0%)')\n",
    "\n",
    "if 'validation' in dataset:\n",
    "    original_data_count_validation = len(dataset['validation'])\n",
    "    print(f'Validation:  {len(dataset[\"validation\"])}/{original_data_count_validation} ({len(dataset[\"validation\"]) * 100 // original_data_count_validation}%)')\n",
    "else:\n",
    "    print(f'Validation:  0/0 (0%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['meeting_id', 'audio_id', 'text', 'audio', 'begin_time', 'end_time', 'microphone_id', 'speaker_id', 'input_length'],\n",
       "    num_rows: 66698\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train split:   0%|          | 50/66698 [00:00<02:14, 494.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train split: 100%|██████████| 66698/66698 [01:33<00:00, 716.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated /work/van-speech-nlp/jindaznb/jslpnb/hypo/Hypo2Trans/data/ami/ami_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation split: 100%|██████████| 8351/8351 [00:11<00:00, 728.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated /work/van-speech-nlp/jindaznb/jslpnb/hypo/Hypo2Trans/data/ami/ami_validation.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test split: 100%|██████████| 7546/7546 [00:10<00:00, 720.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated /work/van-speech-nlp/jindaznb/jslpnb/hypo/Hypo2Trans/data/ami/ami_test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "identifier = \"ami\"\n",
    "\n",
    "def create_jsonl_file(dataset, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        jsonl_path = os.path.join(output_dir, f\"{identifier}_{split}.jsonl\")\n",
    "\n",
    "        if os.path.exists(jsonl_path):\n",
    "            os.remove(jsonl_path)\n",
    "\n",
    "        with open(jsonl_path, 'w') as jsonl_file:\n",
    "            for sample in tqdm(dataset[split], desc=f\"Processing {split} split\"):\n",
    "                audio_id = sample['audio_id']\n",
    "                audio_path = sample['audio']['path']  \n",
    "                transcription = sample['text'].lower()\n",
    "\n",
    "                json_data = {\n",
    "                    \"key\": audio_id,\n",
    "                    \"source\": audio_path,\n",
    "                    \"target\": transcription\n",
    "                }\n",
    "\n",
    "                jsonl_file.write(json.dumps(json_data) + \"\\n\")\n",
    "\n",
    "        print(f\"Generated {jsonl_path}\")\n",
    "\n",
    "output_directory = \"/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/data/ami\"\n",
    "\n",
    "create_jsonl_file(dataset, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
