{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Audio\n",
    "import logging\n",
    "\n",
    "# Load the AMI dataset\n",
    "dataset = load_dataset(\n",
    "    \"edinburghcstr/ami\", \"ihm\", \n",
    "    cache_dir='/work/van-speech-nlp/temp',\n",
    "    use_auth_token='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'\n",
    ")\n",
    "\n",
    "# Define the min and max input lengths in seconds\n",
    "min_input_length_in_sec = 1.0\n",
    "max_input_length_in_sec = 10.0\n",
    "\n",
    "# Calculate input_length as the difference between end_time and begin_time\n",
    "dataset = dataset.map(\n",
    "    lambda x: {'input_length': x['end_time'] - x['begin_time']}\n",
    ")\n",
    "\n",
    "# Filter audio samples based on the calculated input_length\n",
    "dataset = dataset.filter(\n",
    "    lambda x: min_input_length_in_sec < x['input_length'] < max_input_length_in_sec\n",
    ")\n",
    "\n",
    "# Log the number of data points after filtering\n",
    "logging.info(\n",
    "    \"After filtering audio within a certain length, the number of data in each dataset is:\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the dataset is split into train/validation sets, log each count\n",
    "if 'train' in dataset:\n",
    "    original_data_count_train = len(dataset['train'])\n",
    "    print(f'Train:       {len(dataset[\"train\"])}/{original_data_count_train} ({len(dataset[\"train\"]) * 100 // original_data_count_train}%)')\n",
    "else:\n",
    "    print(f'Train:       0/0 (0%)')\n",
    "\n",
    "if 'validation' in dataset:\n",
    "    original_data_count_validation = len(dataset['validation'])\n",
    "    print(f'Validation:  {len(dataset[\"validation\"])}/{original_data_count_validation} ({len(dataset[\"validation\"]) * 100 // original_data_count_validation}%)')\n",
    "else:\n",
    "    print(f'Validation:  0/0 (0%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "identifier = \"ami\"\n",
    "\n",
    "def create_jsonl_file(dataset, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        jsonl_path = os.path.join(output_dir, f\"{identifier}_{split}.jsonl\")\n",
    "\n",
    "        if os.path.exists(jsonl_path):\n",
    "            os.remove(jsonl_path)\n",
    "\n",
    "        with open(jsonl_path, 'w') as jsonl_file:\n",
    "            for sample in tqdm(dataset[split], desc=f\"Processing {split} split\"):\n",
    "                audio_id = sample['audio_id']\n",
    "                audio_path = sample['audio']['path']  \n",
    "                transcription = sample['text'].lower()\n",
    "\n",
    "                json_data = {\n",
    "                    \"key\": audio_id,\n",
    "                    \"source\": audio_path,\n",
    "                    \"target\": transcription\n",
    "                }\n",
    "\n",
    "                jsonl_file.write(json.dumps(json_data) + \"\\n\")\n",
    "\n",
    "        print(f\"Generated {jsonl_path}\")\n",
    "\n",
    "output_directory = \"/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/ami\"\n",
    "\n",
    "create_jsonl_file(dataset, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
