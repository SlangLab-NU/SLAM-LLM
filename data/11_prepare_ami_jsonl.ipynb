{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering, the number of data in each dataset split is:\n",
      "train: 108502\n",
      "validation: 13098\n",
      "test: 12643\n",
      "\n",
      "After filtering audio within a certain length, the number of data in each dataset split is:\n",
      "train: 66698\n",
      "validation: 8351\n",
      "test: 7546\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Audio\n",
    "import logging\n",
    "\n",
    "# Enable logging for the datasets library to see detailed information\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the AMI dataset\n",
    "dataset = load_dataset(\n",
    "    \"edinburghcstr/ami\", \"ihm\",\n",
    "    cache_dir='/work/van-speech-nlp/temp',\n",
    "    use_auth_token='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'\n",
    ")\n",
    "\n",
    "# Print the number of data points in each split before filtering\n",
    "print(\"Before filtering, the number of data in each dataset split is:\")\n",
    "for split, data in dataset.items():\n",
    "    print(f\"{split}: {len(data)}\")\n",
    "\n",
    "# Define the min and max input lengths in seconds\n",
    "min_input_length_in_sec = 1.0\n",
    "max_input_length_in_sec = 10.0\n",
    "\n",
    "# Calculate input_length as the difference between end_time and begin_time\n",
    "dataset = dataset.map(\n",
    "    lambda x: {'input_length': x['end_time'] - x['begin_time']}\n",
    ")\n",
    "\n",
    "# Filter audio samples based on the calculated input_length\n",
    "dataset = dataset.filter(\n",
    "    lambda x: min_input_length_in_sec < x['input_length'] < max_input_length_in_sec\n",
    ")\n",
    "\n",
    "# Print the number of data points in each split after filtering\n",
    "print(\"\\nAfter filtering audio within a certain length, the number of data in each dataset split is:\")\n",
    "for split, data in dataset.items():\n",
    "    print(f\"{split}: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "identifier = \"ami\"\n",
    "\n",
    "def create_jsonl_file(dataset, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        jsonl_path = os.path.join(output_dir, f\"{identifier}_{split}.jsonl\")\n",
    "\n",
    "        if os.path.exists(jsonl_path):\n",
    "            os.remove(jsonl_path)\n",
    "\n",
    "        with open(jsonl_path, 'w') as jsonl_file:\n",
    "            for sample in tqdm(dataset[split], desc=f\"Processing {split} split\"):\n",
    "                audio_id = sample['audio_id']\n",
    "                audio_path = sample['audio']['path']  \n",
    "                transcription = sample['text'].lower()\n",
    "\n",
    "                json_data = {\n",
    "                    \"key\": audio_id,\n",
    "                    \"source\": audio_path,\n",
    "                    \"target\": transcription\n",
    "                }\n",
    "\n",
    "                jsonl_file.write(json.dumps(json_data) + \"\\n\")\n",
    "\n",
    "        print(f\"Generated {jsonl_path}\")\n",
    "\n",
    "output_directory = \"/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/ami\"\n",
    "\n",
    "create_jsonl_file(dataset, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
